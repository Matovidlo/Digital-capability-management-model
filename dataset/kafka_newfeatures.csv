Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Child-Issue),Outward issue link (Cloners),Outward issue link (Completes),Outward issue link (Completes),Outward issue link (Container),Outward issue link (Container),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Required),Outward issue link (Supercedes),Outward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Tags),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
"Followup : KAFKA-9445(Allow fetching a key from a single partition); addressing code review comments",KAFKA-9487,13282565,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,NaviBrar,NaviBrar,NaviBrar,31/Jan/20 09:37,12/Feb/20 03:25,12/Jan/21 10:06,12/Feb/20 03:25,,,,,,,,2.5.0,,,streams,,,,,,0,,,,,"A few code review comments are left to be addressed from Kafka 9445, which I will be addressing in this PR.",,bchen225242,githubbot,mjsax,NaviBrar,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-01-31 18:50:33.099,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 10 18:09:32 UTC 2020,,,,,,,"0|z0b19k:",9223372036854775807,,,,,,,,,,,,,,,,"31/Jan/20 18:50;mjsax;We merge the original feature PR to meet the feature freeze deadline – marking this ticket as a blocker for 2.5 to make sure we get it done in time.","02/Feb/20 06:56;githubbot;brary commented on pull request #8033: [KAFKA-9487] : Follow-up PR of Kafka-9445
URL: https://github.com/apache/kafka/pull/8033
 
 
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","03/Feb/20 05:24;bchen225242;Could we be more specific about the change in the ticket title? ","10/Feb/20 18:09;githubbot;vvcephei commented on pull request #8033: KAFKA-9487: Follow-up PR of Kafka-9445
URL: https://github.com/apache/kafka/pull/8033
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vulnerabilities found for jackson-databind-2.9.9,KAFKA-8952,13259195,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ijuma,namratakokate,namratakokate,27/Sep/19 06:52,29/Sep/19 17:28,12/Jan/21 10:06,29/Sep/19 17:28,2.3.0,,,,,,,2.2.2,2.3.1,2.4.0,,,,,,,0,,,,,"I am currently using apache kafka latest version-2.3.0, however When I deployed the binary on the containers, I can see the vulnerability reported for the two jars - jackson-databind-2.9.9.jar and  guava-20.0.jar (filed KAFKA-8959 for the Guava part).",,githubbot,ijuma,namratakokate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-09-29 15:53:46.977,,,false,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,Sun Sep 29 17:26:19 UTC 2019,,,,,,,"0|z072nk:",9223372036854775807,,,,,,,,,,,,,,,,"29/Sep/19 15:53;githubbot;ijuma commented on pull request #7411: KAFKA-8952: Update Jackson to 2.10.0
URL: https://github.com/apache/kafka/pull/7411
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","29/Sep/19 17:26;ijuma;Since the Guava issue is more complicated, I filed KAFKA-8959 for that. This issue is just about the Jackson dependency bump.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a request timeout to NetworkClient,KAFKA-2120,12820639,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,mgharat,becket_qin,becket_qin,13/Apr/15 21:13,26/Dec/16 22:37,12/Jan/21 10:06,29/Sep/15 17:52,,,,,,,,0.9.0.0,,,,,,,,,2,,,,,"Currently NetworkClient does not have a timeout setting for requests. So if no response is received for a request due to reasons such as broker is down, the request will never be completed.
Request timeout will also be used as implicit timeout for some methods such as KafkaProducer.flush() and kafkaProducer.close().
KIP-19 is created for this public interface change.
https://cwiki.apache.org/confluence/display/KAFKA/KIP-19+-+Add+a+request+timeout+to+NetworkClient",,becket_qin,dapengsun,enothereska,ewencp,githubbot,glenn-sontheimer,guozhang,hachikuji,hongyu.bi,ijuma,jjkoshy,mauzhang,mgharat,padalianitin,sslavic,stevenz3wu,sureshms,tapasi_paul2004,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1835,KAFKA-1788,,,,,,,,,,,,,,"27/Jul/15 21:09;mgharat;KAFKA-2120.patch;https://issues.apache.org/jira/secure/attachment/12747419/KAFKA-2120.patch","27/Jul/15 22:31;mgharat;KAFKA-2120_2015-07-27_15:31:19.patch;https://issues.apache.org/jira/secure/attachment/12747440/KAFKA-2120_2015-07-27_15%3A31%3A19.patch","29/Jul/15 22:57;mgharat;KAFKA-2120_2015-07-29_15:57:02.patch;https://issues.apache.org/jira/secure/attachment/12747875/KAFKA-2120_2015-07-29_15%3A57%3A02.patch","11/Aug/15 02:55;mgharat;KAFKA-2120_2015-08-10_19:55:18.patch;https://issues.apache.org/jira/secure/attachment/12749751/KAFKA-2120_2015-08-10_19%3A55%3A18.patch","12/Aug/15 17:59;mgharat;KAFKA-2120_2015-08-12_10:59:09.patch;https://issues.apache.org/jira/secure/attachment/12750133/KAFKA-2120_2015-08-12_10%3A59%3A09.patch","03/Sep/15 22:12;mgharat;KAFKA-2120_2015-09-03_15:12:02.patch;https://issues.apache.org/jira/secure/attachment/12754096/KAFKA-2120_2015-09-03_15%3A12%3A02.patch","05/Sep/15 00:49;mgharat;KAFKA-2120_2015-09-04_17:49:01.patch;https://issues.apache.org/jira/secure/attachment/12754305/KAFKA-2120_2015-09-04_17%3A49%3A01.patch","09/Sep/15 23:46;mgharat;KAFKA-2120_2015-09-09_16:45:44.patch;https://issues.apache.org/jira/secure/attachment/12755028/KAFKA-2120_2015-09-09_16%3A45%3A44.patch","10/Sep/15 01:56;mgharat;KAFKA-2120_2015-09-09_18:56:18.patch;https://issues.apache.org/jira/secure/attachment/12755045/KAFKA-2120_2015-09-09_18%3A56%3A18.patch","11/Sep/15 04:39;mgharat;KAFKA-2120_2015-09-10_21:38:55.patch;https://issues.apache.org/jira/secure/attachment/12755325/KAFKA-2120_2015-09-10_21%3A38%3A55.patch","11/Sep/15 21:54;mgharat;KAFKA-2120_2015-09-11_14:54:15.patch;https://issues.apache.org/jira/secure/attachment/12755499/KAFKA-2120_2015-09-11_14%3A54%3A15.patch","16/Sep/15 01:57;mgharat;KAFKA-2120_2015-09-15_18:57:20.patch;https://issues.apache.org/jira/secure/attachment/12756138/KAFKA-2120_2015-09-15_18%3A57%3A20.patch","19/Sep/15 02:28;mgharat;KAFKA-2120_2015-09-18_19:27:48.patch;https://issues.apache.org/jira/secure/attachment/12761250/KAFKA-2120_2015-09-18_19%3A27%3A48.patch","28/Sep/15 23:13;mgharat;KAFKA-2120_2015-09-28_16:13:02.patch;https://issues.apache.org/jira/secure/attachment/12764128/KAFKA-2120_2015-09-28_16%3A13%3A02.patch",,,14.0,,,,,,,,,,,,,,,,,,,,2015-04-16 17:27:22.582,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 26 22:37:55 UTC 2016,,,,,,,"0|i2d75b:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,"16/Apr/15 17:27;sureshms;KAFKA-1788 which has been in the work for some time now addresses this, right? ","17/Apr/15 01:14;ewencp;[~sureshms] They're related, but not the same thing. This JIRA is about the network requests made by the NetworkClient class, and is really about an internal implementation detail - we want to make sure the request times out so the client can make progress, even if it's never going to get a response. KAFKA-1788 is about timeouts for producer.send() calls which can never even make a network request because they can't make a connection to the broker. It's a user-facing issue.

See the mailing list discussion thread linked in the KIP for more details.","17/Apr/15 04:12;sureshms;[~ewencp], thanks for the explanation. ","04/Jun/15 21:46;hachikuji;The new consumer will also be able to take advantage of a request timeout in NetworkClient. Currently the consumer just blocks indefinitely on some requests, but we will probably need a timeout to resolve KAFKA-1894.","22/Jun/15 17:07;hachikuji;Hey [~mgharat] and [~becket_qin], how is this feature coming along? I'd be happy to help out if you don't have time for it.","22/Jun/15 17:22;mgharat;Hi [~jasong35], we have began work on this from today. Will try to get this done asap.

Thanks,

Mayuresh","27/Jul/15 21:09;mgharat;Created reviewboard https://reviews.apache.org/r/36858/diff/
 against branch origin/trunk","27/Jul/15 22:31;mgharat;Updated reviewboard https://reviews.apache.org/r/36858/diff/
 against branch origin/trunk","29/Jul/15 22:57;mgharat;Updated reviewboard https://reviews.apache.org/r/36858/diff/
 against branch origin/trunk","06/Aug/15 16:18;hachikuji;[~mgharat] It occurred to me that the global request timeout doesn't work so well for the consumer. In particular, join group requests can take as long as the session timeout (which we encourage people to set high), but it would be unfortunate if we had to use that as the timeout for all requests. I wonder if it would be a good idea to have per-request timeouts?","06/Aug/15 16:18;glenn-sontheimer;Hello,
﻿I am out of the office and will return on August 10.
Thanks.
-Glenn
","06/Aug/15 16:40;mgharat;[~jasong35] I am not too familiar with the consumer code right now. From your comment above I get a feeling that the consumer and producer have different requirements. Also I see that consumer uses a ConsumerNetworkClient and we can have a separate timeout config for Consumer that can be passed to NetworkClient. 
By per-requests timeout, do you mean that different types of request can have different timeouts configured? If yes, it can be done. But then we might degrade the user experience wherein the user has to now configure different timeouts for different kinds of requests, which means that he has to have internal knowledge of different kinds of requests that clients are making to brokers. 
I might be wrong here since I am not fully aware of the consumer code base and I am still starting with it. Let me know your thoughts on this. 
","06/Aug/15 16:47;hachikuji;[~mgharat] Yeah, that's what I mean. I think the consumer has three different cases where a different timeout should be used:

1. Join group requests: The timeout should be based on the session timeout.
2. Fetch requests: The timeout should be based on the fetch max wait time.
3. All others: This includes heartbeats and metadata requests, and can use the default ""request.timeout.ms"" from this patch.

So we don't actually need any new configuration, but it seems like we need to ability to set timeouts differently depending on the request type.","06/Aug/15 16:52;mgharat;Hi [~jasong35],

Cool. So what we can do is :
1) Let this patch go through since this is important for the new producer to be used in production.
2) I will create separate ticket for #1 and #2 that you mentioned and we can work on that separately from this patch.
Does that work?

Thanks,

Mayuresh ","06/Aug/15 16:56;hachikuji;[~mgharat] Sounds reasonable to me. Thanks!","06/Aug/15 17:14;becket_qin;[~hachikuji] [~mgharat], the request time is for exception handling purpose, the request timeout should be the highest timeout across all the timeout. Can we just sanity check the configurations user provides? The rules should be:
Request timeout > session timeout.
Request timeout > fetch.max.wait.timeout
request timeout won't kick in before the other timeout reached.
The sanity check patch should be relatively simple. We can either do it in this patch or in a separate patch.","06/Aug/15 17:23;mgharat;[~becket_qin] yes we can do that. I will create a separate ticket and work on that. I think we should not include that in this patch if possible since this patch is specifically for KIP-19. Let us wait and think on this a bit and then I will send out the jira links for the new tickets.

Thanks,

Mayuresh","06/Aug/15 17:28;hachikuji;[~becket_qin] That is what I was thinking when I first looked at this patch, and it's still an option. However, since we are recommending session timeouts on the order of 60 seconds, it seems unfortunate to have to apply the same timeout on all requests (even those that should return immediately). Allowing a lower timeout for other requests means that the consumer could recover from worst-case failures much quicker. That being said, since the sanity check is easy, I think it's a good idea to add it to this patch.","11/Aug/15 02:55;mgharat;Updated reviewboard https://reviews.apache.org/r/36858/diff/
 against branch origin/trunk","11/Aug/15 19:43;guozhang;[~hachikuji] assigning to you for reviews. Please feel free to re-assign.","12/Aug/15 17:59;mgharat;Updated reviewboard https://reviews.apache.org/r/36858/diff/
 against branch origin/trunk","03/Sep/15 22:12;mgharat;Updated reviewboard https://reviews.apache.org/r/36858/diff/
 against branch origin/trunk","04/Sep/15 22:58;tapasi_paul2004;Hello folks,

    We are facing an issue with the networkproducerclient(rls 8.2.1) spinning issue as reported in this Jira , when kafka is down. Looks like there is a patch available and in the distribution I see rls 8.2.2 is now available. Can you please tell me if this fix for the timeout is there in 8.2.2 ?
If not, when it would be available ?

Thanks","05/Sep/15 00:49;mgharat;Updated reviewboard https://reviews.apache.org/r/36858/diff/
 against branch origin/trunk","09/Sep/15 00:34;mgharat;Hi [~junrao], I have uploaded a new patch addressing the concerns that you raised and also explained my earlier approach. Thanks a lot for all the comments. Would you mind taking another look?

Thanks,

Mayuresh","09/Sep/15 23:46;mgharat;Updated reviewboard https://reviews.apache.org/r/36858/diff/
 against branch origin/trunk","10/Sep/15 01:56;mgharat;Updated reviewboard https://reviews.apache.org/r/36858/diff/
 against branch origin/trunk","10/Sep/15 15:11;mgharat;Hi [~junrao],

Thanks for the comments. I have uploaded a new patch and also made some improvements as per [~jjkoshy]'s comments. Would you mind taking another look. 

Thanks,

Mayuresh","10/Sep/15 22:39;jjkoshy;Thanks for the updated patch - I will review again later today","11/Sep/15 04:39;mgharat;Updated reviewboard https://reviews.apache.org/r/36858/diff/
 against branch origin/trunk","11/Sep/15 21:54;mgharat;Updated reviewboard https://reviews.apache.org/r/36858/diff/
 against branch origin/trunk","16/Sep/15 01:57;mgharat;Updated reviewboard https://reviews.apache.org/r/36858/diff/
 against branch origin/trunk","16/Sep/15 17:42;jjkoshy;Thanks [~mgharat] for the patch, and thanks to everyone who helped with reviews.","16/Sep/15 17:48;mgharat;Thanks a lot everyone who provided great reviews and insights. 

","16/Sep/15 18:49;ijuma;The Jenkins builds failed with 11 failures, seems higher than normal, but could be a coincidence. Worth double-checking though.","16/Sep/15 20:09;jjkoshy;Yes I did note failures in the RB, but those were on trunk as well and transient on my laptop. I believe the failures are Jenkins slowness, but I'm going to rerun tests locally to verify","16/Sep/15 23:18;jjkoshy;Hmm.. actually after rerunning tests I'm not sure this patch is not to blame. [~mgharat] can you also look into this. I'm going to do a few more runs with an earlier revision, but if it appears that this patch is making things worse I would be in favor of reverting if we cannot figure it out by EOD today.","16/Sep/15 23:27;mgharat;Yep. Looking in to this. ","17/Sep/15 16:54;mgharat;I checked if the failures are related to this patch and I found that the failures occur with commits earlier to this patch.

Here is what I did:
1) Checked out the commit :
commit b658e25207174578c26ea94a75e5b0ea10f06ba5
Author: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>
Date:   Mon Sep 14 15:02:32 2015 -0700

2) This is one commit before this patch. I ran ./gradlew test. 

3) Got following errors, in different runs :

*RUN 1 :*

kafka.server.LogRecoveryTest > testHWCheckpointNoFailuresMultipleLogSegments FAILED
    java.lang.AssertionError: Failed to update high watermark for follower after timeout
        at org.junit.Assert.fail(Assert.java:88)
        at kafka.utils.TestUtils$.waitUntilTrue(TestUtils.scala:654)
        at kafka.server.LogRecoveryTest.testHWCheckpointNoFailuresMultipleLogSegments(LogRecoveryTest.scala:173)


kafka.api.SSLConsumerTest > testGroupConsumption FAILED
    java.lang.IllegalStateException: Failed to consume the expected records after 301 iterations.
        at kafka.api.SSLConsumerTest.consumeRecords(SSLConsumerTest.scala:226)
        at kafka.api.SSLConsumerTest.testGroupConsumption(SSLConsumerTest.scala:165)



kafka.integration.UncleanLeaderElectionTest > testCleanLeaderElectionDisabledByTopicOverride FAILED
    java.lang.AssertionError: expected:<List(first, second, third)> but was:<List()>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:743)
        at org.junit.Assert.assertEquals(Assert.java:118)
        at org.junit.Assert.assertEquals(Assert.java:144)
        at kafka.integration.UncleanLeaderElectionTest.verifyUncleanLeaderElectionDisabled(UncleanLeaderElectionTest.scala:253)
        at kafka.integration.UncleanLeaderElectionTest.testCleanLeaderElectionDisabledByTopicOverride(UncleanLeaderElectionTest.scala:155)



kafka.api.ConsumerTest > testPatternSubscription FAILED
    java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
        at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:56)
        at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:43)
        at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:25)
        at kafka.api.ConsumerTest$$anonfun$sendRecords$1.apply(ConsumerTest.scala:449)
        at kafka.api.ConsumerTest$$anonfun$sendRecords$1.apply(ConsumerTest.scala:449)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at kafka.api.ConsumerTest.sendRecords(ConsumerTest.scala:449)
        at kafka.api.ConsumerTest.testPatternSubscription(ConsumerTest.scala:132)

        Caused by:
        org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.


kafka.api.ProducerBounceTest > testBrokerFailure FAILED
    java.lang.AssertionError
        at org.junit.Assert.fail(Assert.java:86)
        at org.junit.Assert.assertTrue(Assert.java:41)
        at org.junit.Assert.assertTrue(Assert.java:52)
        at kafka.api.ProducerBounceTest.testBrokerFailure(ProducerBounceTest.scala:117)

kafka.api.ConsumerTest > testPatternSubscription FAILED
    java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
        at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:56)
        at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:43)
        at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:25)
        at kafka.api.ConsumerTest$$anonfun$sendRecords$1.apply(ConsumerTest.scala:449)
        at kafka.api.ConsumerTest$$anonfun$sendRecords$1.apply(ConsumerTest.scala:449)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at kafka.api.ConsumerTest.sendRecords(ConsumerTest.scala:449)
        at kafka.api.ConsumerTest.testPatternSubscription(ConsumerTest.scala:110)

        Caused by:
        org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.


*RUN 2 :*

kafka.server.LogRecoveryTest > testHWCheckpointNoFailuresMultipleLogSegments FAILED
    java.lang.AssertionError: Failed to update high watermark for follower after timeout
        at org.junit.Assert.fail(Assert.java:88)
        at kafka.utils.TestUtils$.waitUntilTrue(TestUtils.scala:654)
        at kafka.server.LogRecoveryTest.testHWCheckpointNoFailuresMultipleLogSegments(LogRecoveryTest.scala:173)


kafka.server.LogRecoveryTest > testHWCheckpointWithFailuresMultipleLogSegments FAILED
    java.lang.AssertionError: expected:<4> but was:<3>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:743)
        at org.junit.Assert.assertEquals(Assert.java:118)
        at org.junit.Assert.assertEquals(Assert.java:555)
        at org.junit.Assert.assertEquals(Assert.java:542)
        at kafka.server.LogRecoveryTest.testHWCheckpointWithFailuresMultipleLogSegments(LogRecoveryTest.scala:229)


*RUN 3 :*

kafka.server.PlaintextReplicaFetchTest > testReplicaFetcherThread FAILED
    kafka.common.FailedToSendMessageException: Failed to send messages after 5 tries.



kafka.integration.UncleanLeaderElectionTest > testUncleanLeaderElectionEnabled FAILED
    java.lang.AssertionError: expected:<List(first)> but was:<List(first, first)>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:743)
        at org.junit.Assert.assertEquals(Assert.java:118)
        at org.junit.Assert.assertEquals(Assert.java:144)
        at kafka.integration.UncleanLeaderElectionTest.verifyUncleanLeaderElectionEnabled(UncleanLeaderElectionTest.scala:185)
        at kafka.integration.UncleanLeaderElectionTest.testUncleanLeaderElectionEnabled(UncleanLeaderElectionTest.scala:110)


kafka.integration.UncleanLeaderElectionTest > testCleanLeaderElectionDisabledByTopicOverride FAILED
    java.lang.AssertionError: expected:<List(first, second, third)> but was:<List()>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:743)
        at org.junit.Assert.assertEquals(Assert.java:118)
        at org.junit.Assert.assertEquals(Assert.java:144)
        at kafka.integration.UncleanLeaderElectionTest.verifyUncleanLeaderElectionDisabled(UncleanLeaderElectionTest.scala:253)
        at kafka.integration.UncleanLeaderElectionTest.testCleanLeaderElectionDisabledByTopicOverride(UncleanLeaderElectionTest.scala:155)




kafka.integration.UncleanLeaderElectionTest > testUncleanLeaderElectionDisabled FAILED
    java.lang.AssertionError: expected:<List(first)> but was:<List(first, first, first)>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:743)
        at org.junit.Assert.assertEquals(Assert.java:118)
        at org.junit.Assert.assertEquals(Assert.java:144)
        at kafka.integration.UncleanLeaderElectionTest.verifyUncleanLeaderElectionDisabled(UncleanLeaderElectionTest.scala:220)
        at kafka.integration.UncleanLeaderElectionTest.testUncleanLeaderElectionDisabled(UncleanLeaderElectionTest.scala:123)


kafka.api.ProducerBounceTest > testBrokerFailure FAILED
    java.lang.AssertionError
        at org.junit.Assert.fail(Assert.java:86)
        at org.junit.Assert.assertTrue(Assert.java:41)
        at org.junit.Assert.assertTrue(Assert.java:52)
        at kafka.api.ProducerBounceTest.testBrokerFailure(ProducerBounceTest.scala:117)

kafka.api.ConsumerTest > testPatternSubscription FAILED
    java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.
        at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:56)
        at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:43)
        at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:25)
        at kafka.api.ConsumerTest$$anonfun$sendRecords$1.apply(ConsumerTest.scala:449)
        at kafka.api.ConsumerTest$$anonfun$sendRecords$1.apply(ConsumerTest.scala:449)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at kafka.api.ConsumerTest.sendRecords(ConsumerTest.scala:449)
        at kafka.api.ConsumerTest.testPatternSubscription(ConsumerTest.scala:110)

        Caused by:
        org.apache.kafka.common.errors.NetworkException: The server disconnected before a response was received.


4) Than I ran ./gradlew test with commit :
commit b7d4043d8da1d296519faf321e9f0f7188d7d67a
Author: Parth Brahmbhatt <brahmbhatt.parth@gmail.com>
Date:   Sat Sep 12 21:58:34 2015 -0700

5) This is 4 commits behind this patch. All tests pass for this.
 ","17/Sep/15 17:13;ijuma;[~mgharat], are you getting consistent failures or are they transient? By the way, the ducktape system tests failed this morning, but were passing previously:

http://jenkins.confluent.io/job/kafka_system_tests/

I started a new run in case the failure was transient.","17/Sep/15 17:15;jjkoshy;[~mgharat] yes I'm seeing the same thing. i.e., I ran tests against the revision just before your patch and I'm seeing various failures as well as hanging tests.","17/Sep/15 17:15;ijuma;Also, it doesn't look like there's any significant change between b7d4043d8da1d296519faf321e9f0f7188d7d67a (where tests passed) and the latest in trunk, or am I missing something?","17/Sep/15 17:46;jjkoshy;I see test failures on b7d4043d8da1d296519faf321e9f0f7188d7d67a as well","17/Sep/15 17:54;mgharat;[~ijuma] failures are consistent. I ran it multiple times.
","17/Sep/15 18:00;ijuma;Tests pass for me when I run b658e25207174578c26ea94a75e5b0ea10f06ba5 (I know there are tests that have transient failures, but they happen rarely for me). With the latest code from trunk, I got at least one failure in 3 attempts with no successful run. This seems quite suspicious.

Looking at the code, I noticed that we are passing the request timeout for the broker code (ControllerChannelManager, KafkaServer and `ReplicaFetcherThread`). These classes were already handling the timeout themselves and I wonder if doing it in two places is causing some overlapping timeouts, which can delay things enough to cause test failures. Is it possible to just not set a request timeout in those cases?","17/Sep/15 18:04;ijuma;Also, there are already failures in the system tests that I relaunched:

http://jenkins.confluent.io/job/kafka_system_tests/77/console

They had been successful for several days before this morning (they run once a day).","17/Sep/15 18:17;mgharat;""Looking at the code, I noticed that we are passing the request timeout for the broker code (ControllerChannelManager, KafkaServer and `ReplicaFetcherThread`)"". This might be true, I am not sure because anything that uses NetworkClient will use the new configured requestTimeout for sent requests. 

Also since both me and Joel saw failures with earlier commits, I am not sure if they existed with earlier commits and this patch is making them more prominent. 

Thanks,

Mayuresh","17/Sep/15 18:51;ijuma;[~mgharat], it is hard to tell for sure given the number of tests that are sensitive to timing issues. The fact that the system tests are now failing does give credence to the fact that the latest changes have had some sort of effect though (although it could be a coincidence, of course):

http://jenkins.confluent.io/job/kafka_system_tests/buildTimeTrend","17/Sep/15 21:43;jjkoshy;Actually some of the confusion on my side is because I was running tests with JDK 7u51 which has been failing consistently for a while now (wonder what's up with that) - with JDK 8 the previous revision does seem to go through unit tests without failures. So in the interest of keeping trunk somewhat stable I have reverted the patch for now - sorry about the inconvenience.","19/Sep/15 02:28;mgharat;Updated reviewboard https://reviews.apache.org/r/36858/diff/
 against branch origin/trunk","28/Sep/15 23:13;mgharat;Updated reviewboard https://reviews.apache.org/r/36858/diff/
 against branch origin/trunk","29/Sep/15 17:52;jjkoshy;+1 and committed to trunk.","05/Oct/15 21:06;githubbot;GitHub user guozhangwang opened a pull request:

    https://github.com/apache/kafka/pull/278

    TRIVIAL: remove TODO in KafkaConsumer after KAFKA-2120

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/guozhangwang/kafka TL-KafkaConsumer

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/278.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #278
    
----
commit 3c84b3d081962dfd29dccd4e053d05adca581ede
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2015-10-05T21:04:08Z

    TRIVIAL: remove TODO in KafkaConsumer after KAFKA-2120

----
","05/Oct/15 21:08;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/278
","14/Oct/15 03:21;guozhang;[~mgharat] Your commit seems contains some debugging code in KafkaProducer like

{code}
remainingTime = checkMaybeGetRemainingTime(startTime);
{code}

Could you remove them?","15/Oct/15 16:18;mgharat;[~guozhang] The check is required to be done at each stage to see if we have crossed the maxBlockTime : 
1) after fetching metadata.
2) after serialization of key.
3) after serialization of value.
4) after partitioning.
","15/Oct/15 16:32;guozhang;I see, that makes sense. Could we only define remainingTime in line 437 then and not getting return values for previous calls?","15/Oct/15 16:43;mgharat;[~guozhang] Yes sure. Will submit a PR for it. ","15/Oct/15 21:42;githubbot;GitHub user MayureshGharat opened a pull request:

    https://github.com/apache/kafka/pull/320

    KAFKA-2120

    Trivial fix to get rid of unused statements in kafkaProducer.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/MayureshGharat/kafka kafka-2120-followup

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/320.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #320
    
----
commit 2d28c58f79a7207729aaa92e0e255bbd7c566ba4
Author: Mayuresh Gharat <mgharat@mgharat-ld1.linkedin.biz>
Date:   2015-10-15T21:32:31Z

    Trivial fix to get rid of unused statements in kafkaProducer

----
","16/Oct/15 16:41;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/320
","09/Jan/16 06:35;padalianitin;Is this resolution backported to older versions; i.e.; 0.8.2 also?","14/Mar/16 19:04;githubbot;GitHub user christian-posta opened a pull request:

    https://github.com/apache/kafka/pull/1061

    TRIVIAL: remove TODO in ConsumerNetworkClient after KAFKA-2120

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/christian-posta/kafka ceposta-trivial-remove-todo-after-KAFKA-2120

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1061.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1061
    
----
commit e5baf7cae7028c3840195328111fd45b4a403616
Author: Christian Posta <christian.posta@gmail.com>
Date:   2016-03-14T19:03:46Z

    TRIVIAL: remove TODO in ConsumerNetworkClient after KAFKA-2120

----
","26/Dec/16 22:37;githubbot;Github user pono closed the pull request at:

    https://github.com/apache/kafka/pull/1061
"
support exclude.internal.topics in new consumer,KAFKA-2832,12912895,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,vahid,junrao,junrao,13/Nov/15 17:03,14/Apr/16 22:05,12/Jan/21 10:06,17/Mar/16 19:34,,,,,,,,0.10.0.0,,,clients,,,,,,0,,,,,"The old consumer supports exclude.internal.topics that prevents internal topics from being consumed by default. It would be useful to add that in the new consumer, especially when wildcards are used.",,ecomar,githubbot,gwenshap,ijuma,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-3306,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-18 16:26:59.41,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 14 22:05:47 UTC 2016,,,,,,,"0|i2oddj:",9223372036854775807,,gwenshap,,,,,,,,,,,,,,"18/Feb/16 16:26;githubbot;GitHub user vahidhashemian opened a pull request:

    https://github.com/apache/kafka/pull/932

    KAFKA-2832: Add a consumer config option to exclude internal topics

    A new consumer config option 'exclude.internal.topics' was added to allow excluding internal topics when wildcards are used to specify consumers.
    The new option takes a boolean value, with a default 'false' value (i.e. no exclusion).
    
    This patch is co-authored with @rajinisivaram.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vahidhashemian/kafka KAFKA-2832

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/932.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #932
    
----
commit 558e49a39dca3004203707b791dbd371fad0d5db
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2016-02-18T15:53:39Z

    KAFKA-2832: Add a consumer config option to exclude internal topics
    
    A new consumer config option 'exclude.internal.topics' was added to allow excluding internal topics when wildcards are used to specify consumers.
    The new option takes a boolean value, with a default 'false' value (i.e. no exclusion).
    
    This patch is co-authored with @rajinisivaram.

----
","19/Feb/16 10:51;githubbot;Github user vahidhashemian closed the pull request at:

    https://github.com/apache/kafka/pull/932
","19/Feb/16 10:51;githubbot;GitHub user vahidhashemian reopened a pull request:

    https://github.com/apache/kafka/pull/932

    KAFKA-2832: Add a consumer config option to exclude internal topics

    A new consumer config option 'exclude.internal.topics' was added to allow excluding internal topics when wildcards are used to specify consumers.
    The new option takes a boolean value, with a default 'false' value (i.e. no exclusion).
    
    This patch is co-authored with @rajinisivaram.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vahidhashemian/kafka KAFKA-2832

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/932.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #932
    
----
commit ca4c47fce1a2c6615bf41a68249ffc7e315ea12f
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2016-02-18T15:53:39Z

    KAFKA-2832: Add a consumer config option to exclude internal topics
    
    A new consumer config option 'exclude.internal.topics' was added to allow excluding internal topics when wildcards are used to specify consumers.
    The new option takes a boolean value, with a default of 'true' (i.e. exclude internal topics).
    
    This patch is co-authored with @rajinisivaram.

----
","23/Feb/16 22:50;githubbot;Github user vahidhashemian closed the pull request at:

    https://github.com/apache/kafka/pull/932
","23/Feb/16 22:50;githubbot;GitHub user vahidhashemian reopened a pull request:

    https://github.com/apache/kafka/pull/932

    KAFKA-2832: Add a consumer config option to exclude internal topics

    A new consumer config option 'exclude.internal.topics' was added to allow excluding internal topics when wildcards are used to specify consumers.
    The new option takes a boolean value, with a default 'false' value (i.e. no exclusion).
    
    This patch is co-authored with @rajinisivaram.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vahidhashemian/kafka KAFKA-2832

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/932.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #932
    
----
commit 1b03ad9d1e8dd0a40adb3272ff857c48289b1e05
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2016-02-18T15:53:39Z

    KAFKA-2832: Add a consumer config option to exclude internal topics
    
    A new consumer config option 'exclude.internal.topics' was added to allow excluding internal topics when wildcards are used to specify consumers.
    The new option takes a boolean value, with a default of 'true' (i.e. exclude internal topics).
    
    This patch is co-authored with @rajinisivaram.

----
","08/Mar/16 15:58;granthenke;In the patch review it was decided that it would be best to utilize the server to send an indicator if a topic was ""internal"".","09/Mar/16 09:40;ijuma;Tentatively marking this as ""Blocker"" for now as we may want to change the default and it would be hard to do later (as suggested by [~ewencp])","16/Mar/16 15:03;ecomar;Hi all, Vahid is on vacation so I am creating a new pull request to accommodate the changes discussed so far - and added a unit test for the new option.","16/Mar/16 15:03;githubbot;GitHub user edoardocomar opened a pull request:

    https://github.com/apache/kafka/pull/1082

    KAFKA-2832: Add a consumer config option to exclude internal topics

    A new consumer config option 'exclude.internal.topics' was added to
    allow excluding internal topics when wildcards are used to specify
    consumers.
    The new option takes a boolean value, with a default 'false' value (i.e.
    no exclusion).
    
    This patch is co-authored with @rajinisivaram @edoardocomar @mimaison

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/edoardocomar/kafka KAFKA-2832

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1082.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1082
    
----
commit ec62a3ad5e9076880b0e1c53891949f90b06c112
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2016-03-09T21:48:12Z

    KAFKA-2832: Add a consumer config option to exclude internal topics
    
    A new consumer config option 'exclude.internal.topics' was added to
    allow excluding internal topics when wildcards are used to specify
    consumers.
    The new option takes a boolean value, with a default 'false' value (i.e.
    no exclusion).
    
    This patch is co-authored with @rajinisivaram @edoardocomar @mimaison

----
","17/Mar/16 19:34;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1082
","17/Mar/16 19:34;gwenshap;Issue resolved by pull request 1082
[https://github.com/apache/kafka/pull/1082]","14/Apr/16 22:05;githubbot;Github user vahidhashemian closed the pull request at:

    https://github.com/apache/kafka/pull/932
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add delete topic support ,KAFKA-330,12550245,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,09/Apr/12 16:54,01/Mar/15 21:47,12/Jan/21 10:06,07/Feb/14 04:23,0.8.0,0.8.1,,,,,,0.8.1,,,controller,log,replication,,,,5,features,project,,,One proposal of this API is here - https://cwiki.apache.org/confluence/display/KAFKA/Kafka+replication+detailed+design+V2#KafkareplicationdetaileddesignV2-Deletetopic,,adenysenko,clehene,fullung,gwenshap,jbrosenberg,jkreps,junrao,mcandre,mrlabbe,nehanarkhede,prashanth.menon,smeder,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-50,KAFKA-1074,,,,,,,,,,,,,,,,KAFKA-784,KAFKA-1177,KAFKA-1443,,,,,"28/Jan/14 19:03;nehanarkhede;KAFKA-330.patch;https://issues.apache.org/jira/secure/attachment/12625630/KAFKA-330.patch","28/Jan/14 23:19;nehanarkhede;KAFKA-330_2014-01-28_15:19:20.patch;https://issues.apache.org/jira/secure/attachment/12625765/KAFKA-330_2014-01-28_15%3A19%3A20.patch","29/Jan/14 06:01;nehanarkhede;KAFKA-330_2014-01-28_22:01:16.patch;https://issues.apache.org/jira/secure/attachment/12625797/KAFKA-330_2014-01-28_22%3A01%3A16.patch","31/Jan/14 22:19;nehanarkhede;KAFKA-330_2014-01-31_14:19:14.patch;https://issues.apache.org/jira/secure/attachment/12626391/KAFKA-330_2014-01-31_14%3A19%3A14.patch","01/Feb/14 01:45;nehanarkhede;KAFKA-330_2014-01-31_17:45:25.patch;https://issues.apache.org/jira/secure/attachment/12626437/KAFKA-330_2014-01-31_17%3A45%3A25.patch","01/Feb/14 19:31;nehanarkhede;KAFKA-330_2014-02-01_11:30:32.patch;https://issues.apache.org/jira/secure/attachment/12626488/KAFKA-330_2014-02-01_11%3A30%3A32.patch","01/Feb/14 22:58;nehanarkhede;KAFKA-330_2014-02-01_14:58:31.patch;https://issues.apache.org/jira/secure/attachment/12626499/KAFKA-330_2014-02-01_14%3A58%3A31.patch","05/Feb/14 17:31;nehanarkhede;KAFKA-330_2014-02-05_09:31:30.patch;https://issues.apache.org/jira/secure/attachment/12627158/KAFKA-330_2014-02-05_09%3A31%3A30.patch","06/Feb/14 15:49;nehanarkhede;KAFKA-330_2014-02-06_07:48:40.patch;https://issues.apache.org/jira/secure/attachment/12627358/KAFKA-330_2014-02-06_07%3A48%3A40.patch","06/Feb/14 17:42;nehanarkhede;KAFKA-330_2014-02-06_09:42:38.patch;https://issues.apache.org/jira/secure/attachment/12627378/KAFKA-330_2014-02-06_09%3A42%3A38.patch","06/Feb/14 18:29;nehanarkhede;KAFKA-330_2014-02-06_10:29:31.patch;https://issues.apache.org/jira/secure/attachment/12627388/KAFKA-330_2014-02-06_10%3A29%3A31.patch","06/Feb/14 19:38;nehanarkhede;KAFKA-330_2014-02-06_11:37:48.patch;https://issues.apache.org/jira/secure/attachment/12627400/KAFKA-330_2014-02-06_11%3A37%3A48.patch","08/Feb/14 19:07;nehanarkhede;KAFKA-330_2014-02-08_11:07:37.patch;https://issues.apache.org/jira/secure/attachment/12627828/KAFKA-330_2014-02-08_11%3A07%3A37.patch","03/Apr/13 10:34;swapnilghike;kafka-330-v1.patch;https://issues.apache.org/jira/secure/attachment/12576760/kafka-330-v1.patch","04/Apr/13 18:32;swapnilghike;kafka-330-v2.patch;https://issues.apache.org/jira/secure/attachment/12577028/kafka-330-v2.patch",,15.0,,,,,,,,,,,,,,,,,,,,2012-07-31 15:40:06.189,,,false,,,,,,,,,,,,,,,,,,235110,,,Sun Mar 01 21:47:03 UTC 2015,,,,,,,"0|i029e7:",11123,,,,,,,,,,,,,,,,"31/Jul/12 15:40;junrao;During controller failover, we need to remove unneeded leaderAndISRPath that the previous controller didn't get a chance to remove.","06/Dec/12 18:07;junrao;The delete topic logic can follow the same logic in partition reassignment.
1. Create a ZK path to indicate that we want to delete a topic.
2. The controller registers a listener to the deleteTopic path and when the watcher is triggered:
2.1 Send stopReplica requests to each relevant broker.
2.2 Each broker then delete the local log directory.
2.3 Once the stopReplica request completes, the controller deletes the deleteTopic path and the delete topic command completes.","12/Dec/12 20:25;smeder;I've been doing a lot of manual resetting of data in Kafka and one thing I noticed is that clients don't always behave so well when I do that. So when you implement this you should probably also make sure that the current kafka clients behave well when a topic is removed, i.e. error or reset as appropriate.","18/Dec/12 12:38;prashanth.menon;I'll take this on, hoping to get a patch in this weekend.","08/Jan/13 05:32;junrao;Prashanth, any progress on this jira?","13/Jan/13 15:51;prashanth.menon;Apologies, I began work on this jira before going on break.  Now that I'm back, I should be able to wrap it up.","22/Jan/13 17:59;nehanarkhede;Hey Prashanth, how's this JIRA coming along ?","22/Feb/13 21:35;smeder;Any news on this?","25/Feb/13 18:08;nehanarkhede;Prashanth, mind if I take a look at this ? I have some time this week.","26/Feb/13 14:39;prashanth.menon;Apologies for not getting to this.  Neha, go ahead and run with it.","27/Feb/13 23:54;nehanarkhede;Here is a broad description of how delete topic can work in Kafka -

1. The delete topic tool writes to a /delete_topics/[topic] path
2. The controller's delete topic listener fires and does the following -
2.1 List the partitions for the topic to be deleted
2.2 For each partition, do the following -
2.2.1 Move the partition to OfflinePartition state. Take the leader offline. From this point on, all produce/consume requests for this partition will start failing 
2.2.2 For every replica for a partition, first move it to OfflineReplica state (it is removed from isr) then to NonExistentReplica (send stop-replica request with delete flag on to each replica)
2.3 Delete the /brokers/topics/[topic] path from zookeeper
2.4 Delete the /delete_topics/[topic] path to signify completion of the delete operation
","05/Mar/13 23:14;nehanarkhede;We should check if KAFKA-784 is still an issue after adding delete topic support to Kafka 0.8","07/Mar/13 00:18;jbrosenberg;I'd also like to see an auto-delete feature, where by a topic can be automatically be deleted, after it has been garbage collected, and has no more messages.  This could be set to happen automatically, after an expiration time.  This may require exposing an api on each broker so a broker can ask if any other brokers have messages pending for a topic, before deciding the topic should be removed.","21/Mar/13 02:47;swapnilghike;Delete topic admin path schema updated at https://cwiki.apache.org/confluence/display/KAFKA/Kafka+data+structures+in+Zookeeper","28/Mar/13 18:21;nehanarkhede;It makes sense to include the delete topic feature in the 0.8 beta release since most people might create test topics that would require cleanup","03/Apr/13 10:26;swapnilghike;Patch v1 attached. 

How topics are deleted: 
1. The DeleteTopicsCommand writes to /admin/delete_topics in zk and exits.
2. The DeleteTopicsCommand complains if a topic that is being deleted is absent in zookeeper. It won't run even if at least one of the topics specified is actually present in the zookeeper. 
3. A DeleteTopicsListener is triggered in controller. It moves the replicas and partitions to Offline->NonExistent states, deletes the partitions from controller's memory, sends StopReplicaRequests with deletePartition=true.
4. Brokers on receiving the StopReplicaRequest remove the partition from their own memory and delete the logs.
5. If all the partitions were successfully deleted, the topic path is deleted from zookeeper.
6. Controller always deletes the admin/delete_topics path at the end. It checks in removeFromTopicsBeingDeleted() whether each topic has been deleted from zookeeper, at which point it declares victory or logs a warning of shame.


How to validate that the topics have been deleted:
1. Rerun the DeleteTopicsCommand, it should complain that the topics are absent in zookeeper.


Special comments:
A. TopicChangeListener:
1. I think that we should not handle deleted topics here. We should rather modify the controller's memory in NonExistentPartition state change. This is because the controller will release its lock between DeleteTopics listener and TopicChangeListener, we should want the controller's memory to be up-to-date when the lock is released with the completion of DeleteTopics listener.
2. Probably there is no need to add the new topics' partititon-replica assignment to controllerContext.partitionReplicaAssignment, because onNewTopicCreation() will do that. I put a TODO there. Please correct if I am wrong.


Handling failures:

A. What happens when controller fails:
1. Before OfflineReplica state change: New controller context will be initialized and initializeAndMaybeTriggerTopicDeletion() will delete the topics.
2. After OfflineReplica state change and before OfflinePartition state change: Initialization of controller context will re-insert replicas into ISR, and initializeAndMaybeTriggerTopicDeletion() will delete the topics.
3. After OfflinePartition state change and before NonExistentReplica state change: Ditto as 2.
4. After NonExistentReplica state change and before NonExistentPartition state change: The replicas that were deleted will be restarted on individual brokers, then the topics will be deleted.
5. After NonExistentPartition state change and before deleting topics from zk: Ditto as 3. (The NonExistentPartition state change in partition state machine currently does not delete the partitions from zk, it assumes that the controller will delete them, which is similar to what we do for some other state changes as of now).
I think the deletion should proceed smoothly even if the controller fails over in the middle of 1,2,3,4 or 5 above.

B. What happens if a topic is deleted when a broker that has a replica of that topic's partition is down? =>
i. When the broker comes back up and the topic has been deleted from zk, the controller can only tell the broker which topics are currently alive. The broker should delete the dead logs when it receives the first leaderAndIsr request. This can be done just before starting the hw checkpointing thread. 
ii. This will also be useful in replica reassignment for a partition. When the replica reassignment algorithms sends a StopReplica request with delete=true, the receiving broker could be down. After the broker is back up, it will realize that it needs to delete the logs for certain partitions that are no longer assigned to it.


Possible corner cases:
1. What happens to hw checkpointing for deleted partitions? => checkpointHighWatermarks() reads the current allPartitions() on a broker and writes the hw. So the hw for deleted partitions will disappear.

2. What happens to Produce/Fetch requests in purgatory? => 
i. After the topics have been deleted, produce requests in purgatory will expire because there will no fetchers, fetch requests will expire because producer requests would fail in appendToLocalLog() and no more data will be appended.
ii. Expiration of producer requests is harmless. 
iii. Expiration of fetch requests will try to send whatever data is remaining, but it will not be able to send any data because the replica would be dead. We could think of forcing the delayed fetch requests to expire before the replica is deleted and remove the expired requests from the delayed queue, but that would probably require synchronizing on the delayed queue. Thoughts?


Other unrelated changes: 
A. ReplicaStateMachine
1. Moved NonExistentReplica to the bottom of cases to maintain the same order as PartitionStateMachine.
2. Deleted a redundant replicaState.put(replica,OnlineReplica) statement.
3. Even if a replica is not in the ISR, it should always be moved to OfflineReplica state.

B. Utils.scala:
1. Bug fix in seqToJson().  

Testing done:
1. Bring up one broker, create topics, delete topics, verify zk, verify that logs are gone. 
2. Bring up two brokers, create topics, delete topics, verify zk, verify that logs are gone from both brokers.
3. Repeat the above 1 and 2 with more than one partition per topic.
4. Write to admin/delete_paths, bring up the controller, watch the topic and logs get deleted.
5. Bring up two brokers, create two topics with replication factor of two, verify that the logs get created. Now, shut down broker 1 and delete a topic. Verify that the topic disappears from zk and logs of broker 0. Bring up broker 1, verify that the topic disappears from the logs of broker 1 because controller (broker 0) will send leaderAndIsr request for the remaining topic.
6. Validate error inputs.
7. Validate that the tool prints error when a non-existent topic is being deleted.

Is it ok if I write unit tests after this patch is checked in, in case there are modifications?","03/Apr/13 11:30;swapnilghike;Actually scratch 5 in ""How topics are deleted"". Topics are always deleted from zk.","03/Apr/13 19:00;nehanarkhede;Thanks for the patch! Some suggestions -

1. In controller, it is important to not let a long delete topics operation block critical state changes like elect leader. To make this possible, relinquish the lock between the deletes for individual topics
2. If you do relinquish the lock like I suggested above, you need to now take care of avoid leader elections for partitions being deleted
3. Since now you will handle topic deletion for individual topics, it might be worth changing the zookeeper structure for delete topics so status on individual topic deletes gets reported accordingly. One way to do this is to introduce a path to indicate that the admin tool has initiated delete operation for some topics (/admin/delete_topics_updated), and create child nodes under /admin/delete_topics, one per topic. As you complete individual topic deletion, you delete the /admin/delete_topics/<topic> path. Admin tool creates the /admin/delete_topics/<topic> path and updates /admin/delete_topics_updated. Controller only registers a data change watcher on /admin/delete_topics_updated. When this watcher fires, it reads the children of /admin/delete_topics and starts topic deletion. 
4. On startup/failover, the controller registers a data change watch on /admin/delete_topics_updated, and then reads the list of topics under /admin/delete_topics.
5. Admin tool never errors out since it just adds to the list of deleted topics

On the broker side, there are a few things to be done correctly -

1. KafkaApis
After receiving stop replica request, request handler should reject produce/fetch requests for partitions to be deleted by returning PartitionBeingDeleted error code. Once the delete is complete, the partition can be removed from this list. In that case, it will return UnknownTopicOrPartition error code

2. ReplicaManager
2.1 Remove unused variable leaderBrokerId from makeFollower()
2.2 Fix the comment inside recordFollowerPosition to say ""partition hasn't been created or has been deleted""
2.3 Let the partition do the delete() operation. This will ensure that the leaderAndIsrUpdateLock is acquired for the duration of the delete. This will avoid interleaving leader/isr requests with stop replica requests and simplify the reasoning of log truncate/highwatermark update operations

3. Partition - Introduce a new delete() API that works like this -
1. Acquire leaderIsrUpdateLock so that create log does not interfere with delete log. Also remove/add fetcher does not interfere with delete log.
2. Removes fetcher for the partition
3. Invoke delete() on the log. Be careful how current read/write requests will be affected.

4. LogManager
1. When deleteLogs() is invoked, remove logs from allLogs. This will prevent flush being invoked on the log to be deleted.
2. Invoke log.delete() on every individual log.
3. log.markDeletedWhile(_ => true) will leave an extra rolled over segment in the in memory segment list

5. Log
1. Log delete should acquire ""lock"" to prevent interleaving with append/truncate/roll/flush etc
Following steps need to be taken during log.delete()
2. Invoke log.close()
3. Invoke segmentList.delete(), where SegmentList.delete() only does contents.set(new Array[T](0))
4. Invoke segment.delete()
5. Update a flag deleted = true

Few questions to be thought about -

- Are any changes required to roll(). If deleted flag is true, then skip roll().
- Are any changes required to markDeletedWhile(). Same as roll. If deleted flag is true, skip
- Are any changes required to flush() ? This can be invoked either during roll or by append. It cannot be invoked by the flush thread since that is disabled for logs to be deleted. This needs to be handled by using lastOption. 
- See what to do with truncateTo(). This is used during make follower in Partition. This won't interfere with delete since Partition's delete acquires the leaderIsrUpdateLock. Another place that uses truncateTo() is the handleOffsetOutOfRange on the follower. This won't interleave since the replica fetcher was already removed before attempting to delete the log
- See what to do with truncateAndStartWithNewOffset(). This won't interleave with delete log since the replica fetcher was already removed before attempting to delete the log
- What if the broker is writing from the log when stop replica is deleting it ? Since log.delete() acquires the ""lock"", either append starts before or after the delete. If it starts after, then the changed mentioned in #7 and #9 should be made. 
- What if the broker is about to write to the log that is under deletion ? Same as above
- What if the broker is reading from the log that is being deleted ? It will get a ClosedChannelException, I think. This needs to be conformed. The test can run a consumer that is consuming data from beginning of a log and you can invoke delete topic. 
- What if the broker about to read from the log that is being deleted ? It will try reading from a file channel that is closed. This will run into ClosedChannelException. Should we catch ClosedChannelException and log an appropriate error and send PartitionDeleted error code when that happens ?
- What happens to the partition entry from the high watermark file when it is being deleted ? When partition is removed from allPartitions, the next high watermark checkpoint removes the partition's entry from the high watermark file.
- What happens to requests in the purgatory when partition has been deleted ? When a partition has been removed from allPartitions, then the requests in the purgatory will send UnknownTopicOrPartitionCode back to the client.

6. Log.read()
val first = view.head.start
This needs to change to headOption. Return empty message set when this returns None

7. Log.flush()
segments.view.last.flush()
Need to change the above to segments.view.lastOption. If that returns None, then return without flushing. 

8. SegmentList.delete()
contents.set(new Array[T](0))

9. Log.append()
Fix this to use lastOption - val segment = maybeRoll(segments.view.last)
If None, then return (-2,-2) to signify that the log was deleted




","03/Apr/13 22:07;swapnilghike;Replying to a few comments, will follow up with changes according to others:

On the controller side: 
1. I think that the delete topics command will not take too long to complete, in any case it won't take any longer than Preferred Replica Election command. Both commands write to /admin zk path and trigger listeners that may send send some requests and update some zk paths. I believed that the reason for relinquishing the lock in ReassignPartitions listeners after every partition reassignment was that the controller waits for the new replicas to join the ISR, which could take long.
2. Hence I think that we should not relinquish the lock between deletion of two topics.
3. So maybe we don't need to use two separate zk paths? If we rerun the DeleteTopicsCommand, it should complain that the topics are absent in zookeeper if the topics were successfully deleted.

On the broker side:
4. LogManager: 
1. deleteLogs() indeed removes the logs from allLogs.
2. delete() is invoked on every individual log.
3. Yes, following up on this.

5. Log: 
1. The lock is acquired by all these functions, but I will double check if it needs to be acquired at the top level for our purpose.
3. Well, log.delete() takes care of deleting the individual segments. 

Will make modifications to Log*, hopefully they will address all your comments.
","03/Apr/13 22:47;nehanarkhede;Let's do some zookeeper math here to see how long it takes to delete one topic, 8 partitions from a 6 node kafka cluster -

# of zk ops                  operation during delete topic
1                                    val partitionAssignment = ZkUtils.getPartitionAssignmentForTopics(zkClient, topics.toSeq)        
7                                    val brokers = ZkUtils.getAllBrokersInCluster(zkClient)										
1                                    ZkUtils.getAllReplicasOnBroker(zkClient, topics.toSeq, brokers.map(_.id))	        (This is a redundant read from zookeeper, so reuse the info read in step 1)
2                                    removeReplicaFromIsr -> getLeaderIsrAndEpochForPartition, conditionalUpdatePersistentPath
9                                    removeFromTopicsBeingDeleted -> readDataMaybeNull (1), deletePath (8)

20 zookeeper ops. With 10ms per op, (which is a what a zookeeper cluster that kafka consumers and brokers share does in best case), that is 200ms per topic
With 50 such topics, it is 10 seconds. That is the amount of time you are starving other partitions from being available! 
What you can do, for simplicity purposes, is keep the existing long lock on the controller side for this patch. We can improve it later or in 0.8.1

Also, the log side of your patch does not acquire the lock. You used the delete APIs that were used by unit tests so far. So they don't deal with the issues I've mentioned above in my comments.
Regarding LogManager - Let's look at the modified version of your patch and see if that solves the problems I've outlined above wrt to interleaving other operations with delete log.
","04/Apr/13 06:14;swapnilghike;Thanks for the excellent explanation. Some of these zk operations will not be repeated for every topic, for example, ZkUtils.getAllBrokersInCluster(zkClient) or removeFromTopicsBeingDeleted. But anyways, it seems that the cost of ZK operations is even worse because removeReplicaFromIsr() makes 2 Zk operations for each replica, which would be responsible for 2*50*8*3(repl-factor) = 2400 zk operations. 

I agree with you, let's optimize this after log deletion works correctly. 

Similarly, preferred replica election will suffer from a very high number of zk operations since the callbacks will elect leader for every partition. So, we could relinquish the lock in preferred replica election too.","04/Apr/13 18:29;junrao;Thanks for the patch. Even though the patch is not big, it touches quite a few critical components such as controller, replica manager, and log. It will take some time to stabilize this. We probably should consider pushing this out of 0.8 so that we don't delay the 0,8 release too much. One quick comment:

1. KafkaControler.onTopicsDeletion(): Why do we need to read things like partitionAssignment and brokers from ZK? Could we just use the cached data in controller context?","04/Apr/13 18:32;swapnilghike;Yes, I agree with you Jun. Attaching a temporary patch v2 for the records, which needs testing. Patch v2 reads the cached data from the controller context. We don't need to review this patch since Log has significantly changed in trunk, so I will need to rework that part.","18/Apr/13 04:20;junrao;move to 0.8.1 to reduce the remaining work in 0.8.0.","28/Jan/14 19:03;nehanarkhede;Created reviewboard https://reviews.apache.org/r/17460/
 against branch trunk","28/Jan/14 19:29;nehanarkhede;Delete topic is a pretty tricky feature and there are multiple ways to solve it. I will list the various approaches with the tradeoffs here. Few things to think about that make delete topic tricky -

1. How do you handle resuming delete topics during controller failover?
2. How do you handle re-creating topics if brokers that host a subset of the replicas are down?
3. If a broker fails during delete topic, how does it know which version of the topic it has logs for, when it restarts? This is relevant if we allow re-creating topics while a broker is down

Will address these one by one. 

#1 is pretty straightforward to handle and can be achieved in a way similar to partition reassignment (through an admin path in zookeeper indicating a topic deletion that has not finished)

#2 is an important policy decision that can affect the complexity of the design for this feature. If you allow topics to be deleted while brokers are down, the broker needs a way to know that it's version of the topic is too old. This is mainly an issue since a topic can be re-created and written to, while a broker is down. We need to ensure that a broker does not join the quorum with an older version of the log. There are 2 ways to solve this problem that I could think off -
   1. Do not allow topic deletion to succeed if a broker hosting a replica is down. Here, the controller keeps track of the state of each replica during topic deletion    (TopicDeletionStarted, TopicDeletionSuccessful, TopicDeletionFailed) and only marks the topic as deleted if all replicas for all partitions of that topic are successfully deleted. 
   2. Allow a topic to be deleted while a broker is down and keep track of the ""generation"" of the topic in a fault tolerant, highly available and consistent log. This log can either be zookeeper or a Kafka topic. The main issue here is how many generations would we have to keep track off for a topic. In other words, can this ""generation"" information ever be garbage collected. There isn't a good bound on this since it is unclear when the failed broker will come back online and when a topic will be re-created. That would mean keeping this generation information for potentially a very long time and incurring overhead during recovery or bootstrap of generation information during controller or broker fail overs. This is especially a problem for use cases or tests that keep creating and deleting a lot of short lived topics. Essentially, this solution is not scalable unless we figure out an intuitive way to garbage collect this topic metadata. It would require us to introduce a config for controlling when a topic's generation metadata can be garbage collected. Note that this config is different from the topic TTL feature which controls when a topic, that is currently not in use, can be deleted. Overall, this alternative is unnecessarily complex for the benefit of deleting topics while a broker is down.

#3 is related to the policy decision made about #2. If a topic is not marked deleted successfully while a broker is down, the controller will automatically resume topic deletion when a broker restarts. 

This patch follows the previous approach of not calling a topic deletion successful until all replicas have confirmed the deletion of local state for that topic. This requires the following changes -
1. TopicCommand issues topic deletion by creating a new admin path /admin/delete_topics/<topic>

2. The controller listens for child changes on /admin/delete_topic and starts topic deletion for the respective topics

3. The controller has a background thread that handles topic deletion. The purpose of having this background thread is to accommodate the TTL feature, when we have it. This thread is signaled whenever deletion for a topic needs to be started or resumed. Currently, a topic's deletion can be started only by the onPartitionDeletion callback on the controller. In the future, it can be triggered based on the configured TTL for the topic. A topic's deletion will be halted in the following scenarios -
* broker hosting one of the replicas for that topic goes down
* partition reassignment for partitions of that topic is in progress
* preferred replica election for partitions of that topic is in progress (though this is not strictly required since it holds the controller lock for the entire duration from start to end)

4. Topic deletion is resumed when -
* broker hosting one of the replicas for that topic is started
* preferred replica election for partitions of that topic completes
* partition reassignment for partitions of that topic completes 

5. Every replica for a topic being deleted is in either of the 3 states - 
* TopicDeletionStarted (Replica enters TopicDeletionStarted phase when the onPartitionDeletion callback is invoked. This happens when the child change watch for /admin/delete_topics fires on the controller. As part of this state change, the controller sends StopReplicaRequests to all replicas. It registers a callback for the StopReplicaResponse when deletePartition=true thereby invoking a callback when a response for delete replica is received from every replica)
* TopicDeletionSuccessful (deleteTopicStopReplicaCallback() moves replicas from TopicDeletionStarted->TopicDeletionSuccessful depending on the error codes in StopReplicaResponse)
* TopicDeletionFailed. (deleteTopicStopReplicaCallback() moves replicas from TopicDeletionStarted->TopicDeletionSuccessful depending on the error codes in StopReplicaResponse. In general, if a broker dies and if it hosted replicas for topics being deleted, the controller marks the respective replicas in TopicDeletionFailed state in the onBrokerFailure callback. The reason is that if a broker fails before the request is sent and after the replica is in TopicDeletionStarted state, it is possible that the replica will mistakenly remain in TopicDeletionStarted state and topic deletion will not be retried when the broker comes back up.)

6. The delete topic thread marks a topic successfully deleted only if all replicas are in TopicDeletionSuccessful state and it starts the topic deletion teardown mode where it deletes all topic state from the controllerContext as well as from zookeeper. This is the only time the /brokers/topics/<topic> path gets deleted. 
On the other hand, if no replica is in TopicDeletionStarted state and at least one replica is in TopicDeletionFailed state, then it marks the topic for deletion retry. 

7. I've introduced callbacks for controller-broker communication. Ideally, every callback should be of the following format (RequestOrResponse) => Unit. BUT since StopReplicaResponse doesn't carry the replica id, this is handled in a somewhat hacky manner in the patch. The purpose is to fix the approach of upgrading controller-broker protocols in a reasonable way before having delete topic upgrade StopReplica request in a one-off way. Will file a JIRA for that.

Several integration tests added for delete topic -

1. Topic deletion when all replica brokers are alive
2. Halt and resume topic deletion after a follower replica is restarted
3. Halt and resume topic deletion after a controller failover
4. Request handling during topic deletion
5. Topic deletion and partition reassignment in parallel
6. Topic deletion and preferred replica election in parallel
7. Topic deletion and per topic config changes in parallel","28/Jan/14 23:19;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/17460/
 against branch trunk","29/Jan/14 06:01;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/17460/
 against branch trunk","31/Jan/14 22:19;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/17460/
 against branch trunk","01/Feb/14 01:45;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/17460/
 against branch trunk","01/Feb/14 22:58;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/17460/ against branch trunk.

Asking for a more detailed review as the patch is somewhat tested and refactored to make the topic deletion logic easier to maintain and understand. ","05/Feb/14 17:31;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/17460/
 against branch trunk","06/Feb/14 15:49;nehanarkhede;Updated reviewboard  against branch trunk","06/Feb/14 17:42;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/17460/
 against branch trunk","06/Feb/14 18:29;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/17460/
 against branch trunk","06/Feb/14 19:38;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/17460/
 against branch trunk","07/Feb/14 04:23;nehanarkhede;Thanks for the reviews. This is a big patch, please do submit your review even after checkin, I will fix the issues in follow up JIRAs.","07/Feb/14 15:31;sriramsub;Can we have https://issues.apache.org/jira/secure/attachment/12625445/KAFKA-930_2014-01-27_13%3A28%3A51.patch
this merged now that delete support is in?","07/Feb/14 15:41;junrao;Sriram,

You can check in that patch now. You probably would have to add an additional check to see whether a partition whose leader is to be moved to the preferred replica is in a topic to be deleted, while holding the controller lock. If so, skip leader balancing.","07/Feb/14 17:20;nehanarkhede;[~sriramsub] It will not be enough to just drop the partitions that belong to topics being deleted from the preferred replica list. In addition that, I think we may also have to leave them out while computing what the preferred replica imbalance factor is.","08/Feb/14 19:07;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/17460/
 against branch trunk","27/Feb/15 19:50;mcandre;Could the default config please set {code}delete.topic.enable=true{code} by default, so that kafka behaves more intuitively out of the box?","27/Feb/15 20:35;jkreps;+1 on enabling by default.","01/Mar/15 21:47;gwenshap;created KAFKA-1993 to track this suggestion.",,,,,,,,,,,,,,,,,,,
Consumer offset checker should show the offset manager and offsets partition,KAFKA-1785,12756290,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Duplicate,mgharat,jjkoshy,jjkoshy,19/Nov/14 02:47,13/Jan/15 22:01,12/Jan/21 10:06,13/Jan/15 22:01,,,,,,,,0.9.0.0,,,,,,,,,0,,,,,"This is trivial, extremely useful to have and can be done as part of the offset client patch.",,donnchadh,jjkoshy,junrao,mgharat,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-12-29 22:01:47.249,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 13 17:57:07 UTC 2015,,,,,,,"0|i22jdb:",9223372036854775807,,,,,,,,,,,,,,,,"29/Dec/14 22:01;nehanarkhede;This looks like something that can be pushed to 0.8.3. [~jjkoshy], thoughts?","05/Jan/15 22:11;nehanarkhede;Ping. [~jjkoshy], [~mgharat]","06/Jan/15 01:09;mgharat;Talked to Joel about it last time. Joel was of the opinion that its trivial and suggested that it would be better if we get it done. But in any case its not a blocker for the release as per our discussion. Will try to get it done but it can be moved in worst case.","13/Jan/15 17:57;junrao;Moving this to 0.8.3.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement a ConsumerOffsetClient library,KAFKA-1784,12756234,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Duplicate,mgharat,jjkoshy,jjkoshy,18/Nov/14 21:54,05/Jan/15 22:06,12/Jan/21 10:06,05/Jan/15 22:06,,,,,,,,0.8.2.0,,,,,,,,,0,,,,,I think it would be useful to provide an offset client library. It would make the documentation a lot simpler. Right now it is non-trivial to commit/fetch offsets to/from kafka.,,donnchadh,jjkoshy,junrao,mgharat,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1013,,,,,,,,,,,,,,,"07/Dec/14 19:43;mgharat;KAFKA-1784.patch;https://issues.apache.org/jira/secure/attachment/12685628/KAFKA-1784.patch",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2014-12-02 02:43:16.307,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 05 22:06:44 UTC 2015,,,,,,,"0|i22j13:",9223372036854775807,,nehanarkhede,,,,,,,,,,,,,,"02/Dec/14 02:43;junrao;Joel, does this need to be a blocker for 0.8.2?","02/Dec/14 20:00;jjkoshy;It does not need to be a blocker but would be good to get it in as it makes the offset management APIs a lot easier to use. This is mostly done in KAFKA-1013 but I would like to separate out the client in that patch. If this ends up being the last blocker we can move it out but if we have enough cycles to get it in we should.","02/Dec/14 20:07;nehanarkhede;[~mgharat] Are you actively working on this? How long do you think this will take given that we are shooting for end of the month for the 0.8.2 release?","02/Dec/14 20:25;mgharat;Separating out the client will not be a big change. I suppose it is already their in the earlier patch in KAFKA-1013. I will discuss this with Joel today. I think I should be able to complete this before the deadline.","02/Dec/14 21:30;nehanarkhede;Thanks [~mgharat] and [~jjkoshy]","07/Dec/14 19:43;mgharat;Created reviewboard https://reviews.apache.org/r/28793/diff/
 against branch origin/trunk","09/Dec/14 00:11;nehanarkhede;[~mgharat] Thanks for the patch. Can you add a little more context on the intent and usage of this library? It is meant for admin usage only or do you intend to refactor the existing KafkaApis to use this new library?","09/Dec/14 00:12;nehanarkhede;Assigning to myself for review since [~jjkoshy] is on vacation.","09/Dec/14 00:23;mgharat;Yes. We are planning to do that. Actually the patch for KAFKA-1013 has those. Currently this patch can be used for admin usage and then we will get back to KAFKA-1013 to make it work with KafkaApis and Consumer.","29/Dec/14 22:19;nehanarkhede;[~mgharat] Did you get a chance to address my review comments and use the helper methods directly and just add the admin tool script for now? This is currently marked as a blocker for 0.8.2","29/Dec/14 22:48;mgharat;I suppose we don't require this ticket. KAFKA-1013 takes care of this. I have uploaded a patch for KAFKA-1013 after incorporating the new comments that Joel had given. I have also incorporated your comments on merging the the import and export offsets tool in to a single tool. 
That patch has an API for fetching and committing offsets that this ticket describes. So I suppose this can be closed as duplicate. ","05/Jan/15 22:06;nehanarkhede;Closing as duplicate of KAFKA-1013, as advised by [~mgharat]. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a Performance Suite for the Log subsystem,KAFKA-545,12610379,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,jkreps,jkreps,04/Oct/12 19:36,05/Dec/12 22:09,12/Jan/21 10:06,15/Nov/12 22:55,0.8.0,,,,,,,0.8.0,,,,,,,,,0,features,,,,"We have had several performance concerns or potential improvements for the logging subsystem. To conduct these in a data-driven way, it would be good to have a single-machine performance test that isolated the performance of the log.

The performance optimizations we would like to evaluate include
- Special casing appends in a follower which already have the correct offset to avoid decompression and recompression
- Memory mapping either all or some of the segment files to improve the performance of small appends and lookups
- Supporting multiple data directories and avoiding RAID

Having a standalone tool is nice to isolate the component and makes profiling more intelligible.

This test would drive load against Log/LogManager controlled by a set of command line options. These command line program could then be scripted up into a suite of tests that covered variations in message size, message set size, compression, number of partitions, etc.

Here is a proposed usage for the tool:

./bin/kafka-log-perf-test.sh
Option                   Description                            
------                       -----------                            
--partitions             The number of partitions to write to
--dir                       The directory in which to write the log
--message-size      The size of the messages
--set-size               The number of messages per write
--compression        Compression alg
--messages            The number of messages to write
--readers                The number of reader threads reading the data

The tool would capture latency and throughput for the append() and read() operations.",,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/12 22:21;jkreps;KAFKA-545-draft.patch;https://issues.apache.org/jira/secure/attachment/12551424/KAFKA-545-draft.patch","05/Nov/12 21:17;jkreps;KAFKA-545-v2.patch;https://issues.apache.org/jira/secure/attachment/12552163/KAFKA-545-v2.patch","05/Nov/12 21:45;jkreps;KAFKA-545-v3.patch;https://issues.apache.org/jira/secure/attachment/12552169/KAFKA-545-v3.patch","05/Nov/12 21:13;jkreps;KAFKA-545.patch;https://issues.apache.org/jira/secure/attachment/12552161/KAFKA-545.patch",,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2012-11-01 17:12:55.658,,,false,,,,,,,,,,,,,,,,,,241112,,,Thu Nov 15 22:13:42 UTC 2012,,,,,,,"0|i0190n:",5230,,,,,,,,,,,,,,,,"30/Oct/12 22:21;jkreps;Attaching wip that has this command. Usage is:

jkreps-mn:kafka-git jkreps$ ./bin/kafka-run-class.sh kafka.perf.LogPerformance --help
Option                                  Description                            
------                                  -----------                            
--batch-size <Integer: size>            Number of messages to write in a       
                                          single batch. (default: 200)         
--compression-codec <Integer:           If set, messages are sent compressed   
  compression codec >                     (default: 0)                         
--date-format <date format>             The date format to use for formatting  
                                          the time field. See java.text.       
                                          SimpleDateFormat for options.        
                                          (default: yyyy-MM-dd HH:mm:ss:SSS)   
--dir <path>                            The log directory. (default:           
                                          /var/folders/wV/wVHRnnYrEX0ZFMG7ypsUXE+++TM/-
                                          Tmp-/kafka-8193339)                  
--flush-interval <Integer:              The number of messages in a partition  
  num_messages>                           between flushes. (default:           
                                          2147483647)                          
--flush-time <Integer: ms>              The time between flushes. (default:    
                                          2147483647)                          
--help                                  Print usage.                           
--hide-header                           If set, skips printing the header for  
                                          the stats                            
--index-interval <Integer: bytes>       The number of bytes in between index   
                                          entries. (default: 4096)             
--message-size <Integer: size>          The size of each message. (default:    
                                          100)                                 
--messages <Long: count>                The number of messages to send or      
                                          consume (default:                    
                                          9223372036854775807)                 
--partitions <Integer: num_partitions>  The number of partitions. (default: 1) 
--reader-batch-size <Integer:           The number of messages to write at     
  num_messages>                           once. (default: 200)                 
--readers <Integer: num_threads>        The number of reader threads.          
                                          (default: 1)                         
--reporting-interval <Integer: size>    Interval at which to print progress    
                                          info. (default: 5000)                
--show-detailed-stats                   If set, stats are reported for each    
                                          reporting interval as configured by  
                                          reporting-interval                   
--topic <topic>                         REQUIRED: The topic to consume from.   
--writer-batch-size <Integer:           The number of messages to write at     
  num_messages>                           once. (default: 200)                 
--writers <Integer: num_threads>        The number of writer threads.          
                                          (default: 1)     ","31/Oct/12 23:53;jkreps;The initial set of results show the following:

Big impacts on the write path are 
1. flush
2. FileChannel overhead
3. CRC calculation

Big impacts on the read path are
1. FileMessageSet.searchFor
2. MessageSet iteration (mostly an artifact of the test)

Notable is that the index lookup doesn't show up at all.

Since our read path is an order of magnitude faster than our write path, it makes sense to focus on writes first. The most important thing here is to handle flush better.

The important thing to know is that on Linux fsync holds a global lock on writes to the file, so effectively flush blocks all appends (even though we only really want to flush the data already written). We are effectively targeting linux here since it is so common.

Originally we thought that we could just disable flush and depend on replication for durability with the flush to disk happening via Linux's background pdflush process. However recovery demands a ""known stable point"" from which to recover. Currently we guarantee that all but the last log segment have been flushed. The problem with this is that disabling the time or size based flush effectively just prolongs the time of the global lock until the segment is full, but under high load this could mean suddenly blocking on writing out 2GB of data--a huge multi-second latency spike.

There are a number of possible solutions to this dilemma:
1. Memory map the FileMessageSet. It is possible that the locking characteristics for mmap/msync are different than fsync. This would have the advantage of also getting rid of the overhead of the write call which in java is pretty high and thus making FileMessageSet.searchFor and iterator much faster. There are two disadvantages of this. First we would have to pre-allocate file segments. This would likely confuse people a bit. We are already doing it with index files, but those are smaller. Second memory map eats up process address space. This would likely mean that running on a 32 bit OS would be infeasible (since you would only have 2GB).
2. Change the recovery to recover from an arbitrary point in the log, and write out a ""high water mark"" in a similar way to how we do for the ISR. This would let us just avoid syncing the active segment.
3. Move the flush into a background thread. This wouldn't help if one was using a time-based or size-based flush but would help for the flush at the time of segment roll since at that point there is guaranteed to be no more writes on the segment. This would require recovering the last two segments. This is kind of hacky but easy to implement.

So there are two questions this raises:
1. Does mmap/msync actually have different locking than fsync? Mmap might be the preferable solution since it solves lots of performance bottlenecks all at once.
2. Does fsync block reads? This is a question Jun raised. We don't actually care to much about adding a little bit of latency to consumers, but with synchronous replication fetch latency is a lower bound on produce latency. So blocking reads may be as bad as blocking writes. It is unlikely that an in-sync replica would be a full segment behind, but nonetheless.

I started to investigate these two questions.

I started with (2). It doesn't seem that fsync blocks reads. This is intuitive. To test this I wrote a test that has one thread continually append to a file, one thread call fsync every 2 seconds, and one thread do random reads. I measured the max read time over 100k reads. Here is a trace of the output:
flushing
flush completed in 0.35281
0.226049
1.347807
0.02298
1.562114
0.041638
2.119723
0.027258
5.329019
flushing
8.32156
flush completed in 81.123215
0.04145
1.473818
0.06444
1.733412
0.050216
1.437777
0.04984
1.612728
0.026001
1.858957
0.041096
0.390903
flushing
12.527883
flush completed in 67.416953
0.055656
1.450987
0.029861
1.469376
0.047733
1.313674
0.024264
1.71214
0.027112
0.023717
1.228905
0.029688
1.215998
flushing
flush completed in 59.289193
18.217726
1.549095
0.029295
1.367316
0.047124
1.389574
0.034049
0.030982
1.129182
0.048443
1.070381
0.040149
1.07179
flushing
flush completed in 59.340792
18.668898
0.933095
0.041071
1.197376
0.035512
1.34228
0.042432
0.024397
0.835786
0.026552
1.496774
0.036751
1.148597
flushing
flush completed in 59.308117
16.34416
0.854841
0.053005
1.013405
0.08081
0.051634
1.218344
0.015086
1.447114
0.019883
1.128675
0.041854
1.148591
flushing
15.110585
flush completed in 47.303732
1.018977
0.015041
0.036324
1.293796
0.051184
1.291538
0.013544
1.211112
0.014241
1.520512
0.027815
1.246593
flushing
0.016121
flush completed in 59.38031
22.635984
0.051233
0.054701
0.712837
0.01345
1.004364
0.017261
1.216081
0.019825
^C[jkreps@jkreps-ld kafka-jbod]$ java -server -Xmx128M -Xms128M -XX:+UseConcMarkSweepGC -cp project/boot/scala-2.8.0/lib/scala-library.jar:core/target/scala_2.8.0/test-classes kafka.TestFileChannelReadLocking 100000
flushing
flush completed in 0.528026
0.217155
1.620644
0.034775
0.032913
1.867401
0.026142
2.367951
0.05301
flushing
3.941371
flush completed in 252.11533
17.678379
1.226629
0.043844
2.254627
0.05084
1.654637
0.028077
0.029319
1.217127
0.029779
1.251271
0.281076
0.919776
flushing
0.050014
1.156282
flush completed in 238.852776
5.953432
0.038438
0.966883
0.048641
0.907416
0.037052
1.595778
0.023821
0.923264
0.047909
0.921312
0.058346
0.058062
flushing
0.906503
0.090477
flush completed in 239.282906
1.504453
0.014805
1.276596
0.051536
1.200947
0.052367
0.068161
1.585333
0.051904
1.052337
0.063165
1.502294
0.01266
flushing
0.839178
0.048978
flush completed in 290.738005
1.414586
0.069402
0.056503
1.0008
0.050265
0.955949
0.050486
1.014454
0.048935
1.210959
0.054599
1.313663
0.058152
flushing
0.062286
1.036941
flush completed in 242.879275
11.401024
1.390022
0.07374
0.92633
0.013332
1.015606
0.04448
1.687692
0.014552
0.018272
1.339258
0.051723
^C

As you can see there is some locking happening. But it is not for the duration of the flush. I tried varying the amount of data being written and the max read time remains constant. My guess is that what is happening is that the locking is at the page level, which is what we see with pdflush. This should be acceptable as the latency is bounded to the time to flush one page regardless of the flush size.

I am working on testing mmap.","01/Nov/12 16:56;jkreps;Same test now with 20 seconds worth of data accumulating:
[jkreps@jkreps-ld kafka-jbod]$ java -server -Xmx128M -Xms128M -XX:+UseConcMarkSweepGC -cp project/boot/scala-2.8.0/lib/scala-library.jar:core/target/scala_2.8.0/test-classes kafka.TestFileChannelReadLocking 500000
flushing
flush completed in 0.497006
2.271428
11.766812
1.660411
1.861596
2.039938
1.278876
1.407181
1.130133
1.192209
1.663374
1.658432
1.124757
1.254995
1.848904
1.861381
1.158326
1.414888
1.240507
1.542315
1.543492
1.395788
1.128224
1.244737
1.323254
1.004004
1.508619
1.294839
1.237147
1.369261
1.500938
1.098796
1.140933
1.195621
0.825858
1.21719
flushing
1.187579
1.234125
0.981985
0.999659
1.05744
1.171083
flush completed in 2488.938675
1.219635
1.240126
1.192422
1.604653
1.412199
1.89463
1.282256
1.08756
1.360199
0.947128
1.130891
0.782065
1.453711
1.225088
1.704001
1.110982
1.155404
1.297822
1.450305
1.224275
1.272652
1.280408
1.23271
1.144039
1.273127
1.302072
1.408974
1.348525
1.556987
1.193373
1.407276
1.722947
1.443469
1.751133
flushing
1.288651
flush completed in 608.099163
1.520736
1.233443
1.553179
1.627624
1.613462
1.534873
1.508163
1.538743
1.489821
1.318509
1.537813
1.385722
1.06104
1.31107
1.232484
1.621071
1.63272
1.800139
1.311899
1.315283
1.552909
1.518307
1.384089
1.520744
1.762693
1.467796
1.699609
1.159155
1.469895
1.187978
1.830385
flushing
1.669841
1.341722
1.52613
flush completed in 2040.207681
1.202133
1.400995
1.077904
1.69022
1.055655
1.145438
1.535375
1.281362
1.168067
0.989543
1.162816
1.531742
1.296389
1.065467

Again, more or less constant time even though now we have 2 second flushes.","01/Nov/12 17:12;nehanarkhede;Makes sense, are the reads in your test using NIO ?","01/Nov/12 17:52;jkreps;Yes, for the above data everything is done on a FileChannel.","01/Nov/12 20:05;jkreps;Okay, wrote some tests for lockin MappedByteBuffer and FileChannel to see how writes block reads. I think these things work the same in both cases. Again the results are a series of max times, this time over 5M writes. For both cases I see behavior similar to the below--high max times while a flush is occuring but no hard locking. I am not sure the exact cause of this, but it is safe to say that mmap is no panacea here.

[jkreps@jkreps-ld kafka-jbod]$ java -server -Xmx128M -Xms128M -XX:+UseConcMarkSweepGC -cp project/boot/scala-2.8.0/lib/scala-library.jar:core/target/scala_2.8.0/test-classes kafka.TestMmapLocking $((2*1024*1024*1024-1)) 5000 5000000 1
flushing
flush completed in 49.601333 ms
10.077477
2.214208
1.712157
1.895483
1.798951
1.934366
1.738388
flushing
13.097689
262.515081
flush completed in 1752.944685 ms
2.044426
1.655329
2.063751
1.55256
2.429741
1.717703
1.477672
9.815928
flushing
368.168449
flush completed in 1928.963959 ms
240.966127
1.600222
1.191583
1.750381
2.09028
1.694696
1.88224
2.122531
flushing
168.749728
385.088614
flush completed in 2160.281323 ms
241.846812
1.745029
1.82718
1.756801
1.822239
1.689906
1.708812
1.633651
flushing
183.764496
368.315579
flush completed in 2180.277532 ms
276.418226
1.737839
1.730913
1.711507
1.540686
2.011486
1.937501
1.834844
flushing
95.983117
129.890815
flush completed in 2695.239899 ms
1288.338521
1.828685
1.613301
1.63822
1.725626
","05/Nov/12 21:13;jkreps; - Improve the existing TestLinearWriteSpeed.scala that does linear writes to a file. Now this prints latency/throughput info at a configurable period and allows writing to many files at once as well as using either mmap or write. This is a great test because it gives a bound on fs performance without any Kafka code involved.
 - Added a test program that drives the LogManager level. Can write to any number of partitions and controls various configs.
 - Add three test programs to test read and write locking in the OS. These are a bit hacky and I am fine leaving them on my machine too though they may come in handy again. (TestFileChannelLocking.scala, TestFileChannelReadLocking.scala, TestMmapLocking.scala)
Misc things
 - Remove the reference to flushInterval in many tests. Lots of tests were setting this to 1 to commit messages immediately. That is no longer necessary in 0.8.
 - Added a new configuration replica.highwatermark.checkpoint.ms to control the checkpointing of hwm. Previously we used the config for log sync, but that doesn't make sense.
 - Added Log.toString method for convenience
 - Added explicit check for maxSize < 0 and startOffset > endOffset in LogSegment.read
","05/Nov/12 21:17;jkreps;Oops, wrong base revision for that patch, fixed it in v2.","05/Nov/12 21:45;jkreps;One more tweak.","15/Nov/12 22:13;nehanarkhede;+1.. its great to have this test checked in !",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement KIP-145 transformations ,KAFKA-7472,13188872,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,,loicmdivad,rhauch,rhauch,02/Oct/18 14:15,05/May/20 20:22,12/Jan/21 10:06,,1.1.0,,,,,,,,,,KafkaConnect,,,,,,0,,,,,"As part of [KIP-145|https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect], several SMTs were described and approved. However, they were never implemented.",,loicmdivad,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8863,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-05-04 20:08:03.167,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 05 20:22:36 UTC 2020,,,,,,,"0|i3yqif:",9223372036854775807,,,,,,,,,,,,,,,,"04/May/20 20:08;loicmdivad;Hi, I would like to suggest the PR: [https://github.com/apache/kafka/pull/8614]

It comes in addition to the work brought in KAFKA-5142 and KAFKA-8863. ","04/May/20 21:54;rhauch;Thanks, [~loicmdivad]. I'll take a look shortly.

I took the liberty to add you to AK's ""contributor"" group in Jira, which means you can self-assign other issues in the future.","05/May/20 20:22;loicmdivad;Thank you [~rhauch]! :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Block getAssignments(),KAFKA-9882,13298858,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,,,eljefe6aa,eljefe6aa,16/Apr/20 16:12,21/Apr/20 19:48,12/Jan/21 10:06,,2.5.0,,,,,,,,,,clients,,,,,,0,,,,,"In 2.0, the KafkaConsumer poll(long) was deprecated and replaced with a poll(Duration). The poll(Duration) does not block for consumer assignments.

Now, there isn't a blocking method that can get consumer assignments.

A new KafkaConsumer method needs to be added that blocks while getting consumer assignments.

The current workaround is to poll for a short amount of time in a while loop and check the size of assignment(). This isn't a great method of verifying the consumer assignment.",,bchen225242,eljefe6aa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-04-20 21:10:44.171,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 19:48:34 UTC 2020,,,,,,,"0|z0dprk:",9223372036854775807,,,,,,,,,,,,,,,,"20/Apr/20 21:10;bchen225242;Thanks for the ticket. Are there any concrete use cases for blocking on the assignment?","21/Apr/20 01:28;eljefe6aa;The use case is whenever you have to call a KafkaConsumer method that requires a java.util.Set<[TopicPartition|https://kafka.apache.org/20/javadoc/org/apache/kafka/common/TopicPartition.html]>. These would be methods such as seekToEnd and seekToBeginning.

To use either of these methods with a newly created KafkaConsumer instance, you have to poll and then call assignment() in a while loop and check for the size.

The reality is that calling poll() to get assignment() to be non-zero doesn't make sense either.","21/Apr/20 17:23;bchen225242;I checked back on the original KIP [https://cwiki.apache.org/confluence/display/KAFKA/KIP-266%3A+Fix+consumer+indefinite+blocking+behavior]
So it seems the original intention was to resolve the indefinite blocking. For your case, have you considered using
`public void subscribe(Pattern pattern, ConsumerRebalanceListener listener)` which can plug in a user-callback to wait for assignment result?","21/Apr/20 17:45;eljefe6aa;subscribe() doesn't get a partition assignment until poll() is called. I double-checked this just now.

Using this method would lead to quite a bit of complicated code. The listener code would have to maintain a boolean to check if this is the first time the onPartitionsAssigned method was called. It would have to flip the first time it was called.

I think a more straightforward approach to getting partition assignments would result in more readable and less buggy code.","21/Apr/20 17:52;bchen225242;I see, to make the discussion more effective, could you draft a sample code skeleton assuming we have introduced a new blocking API for the above use case?","21/Apr/20 18:02;eljefe6aa;{code:java}
// Create KafkaConsumer and subscribe

// Call blocking getter for partition assignment
java.util.Set<TopicPartition> assignment = consumer.getAssignment();

// Seek to end of topic
consumer.seekToEnd​(assignment);

// Seek to beginning of topic
consumer.seekToBeginning​(assignment);

// Start polling{code}","21/Apr/20 19:32;bchen225242;So you are trying to reset offset every time when you restart the consumer? If that's the case, we could get a config like `init.offset.reset` which enforces the reset every time we restart the consumer.","21/Apr/20 19:48;eljefe6aa;Seeking each time was just an example. Adding a config wouldn't handle all cases. For example, it would handle seeking to a specific offset based on which partition it was assigned.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Fetcher to return Future for Metadata requests,KAFKA-3416,12951214,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,,,john.warner,john.warner,17/Mar/16 14:05,12/Jan/20 06:01,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,"Update Fetcher so that requests to get metadata return the Future, rather than waiting for the Future to finish. This enables non-blocking code to use these requests.",,high.lee,john.warner,Yohan123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-01-12 06:01:51.026,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 12 06:01:51 UTC 2020,,,,,,,"0|i2utv3:",9223372036854775807,,,,,,,,,,,,,,,,"12/Jan/20 06:01;high.lee;[~john.warner]

Is this ticket valid?

If so, can I do it?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metadata should include number of state stores for task,KAFKA-6583,13140255,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Duplicate,Yohan123,Yohan123,Yohan123,22/Feb/18 17:18,24/Jul/18 20:23,12/Jan/21 10:06,02/Jul/18 17:17,0.10.2.0,0.11.0.0,,,,,,,,,streams,,,,,,0,needs-kip,,,,"Currently, in the need for clients to be more evenly balanced, stateful tasks should be distributed in such a manner that it will be spread equally. However, for such an awareness to be implemented during task assignment, it would require the need for the present rebalance protocol metadata to also contain the number of state stores in a particular task. This way, it will allow us to ""weight"" tasks during assignment. ",,astubbs,mjsax,Yohan123,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-4969,KAFKA-4696,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-02-23 00:59:23.625,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 02 17:18:18 UTC 2018,,,,,,,"0|i3qhbz:",9223372036854775807,,,,,,,,,,,,,,,,"23/Feb/18 00:59;mjsax;Thanks for putting the comment. It's better to ""Link"" Jiras to each other directly. You can find ""Link"" with ""More"" button.","24/Feb/18 04:13;Yohan123;[~mjsax] When updating the rebalance protocol's metadata, is {{SubscriptionInfo}} the only class that is changed? Because from my knowledge, their are also other variable instances in {{StreamPartitionAssignor}} which are called ""metadata"". (e.g. variable of type {{Cluster}} in {{StreamPartitionAssignor#assign(}}{{Cluster metadata, Map<String, Subscription> subscriptions}}{{)}})  It appears that only {{SubscriptionInfo}} contains any data which is useful for assignment, but I just need to confirm that updating metadata would not involve any other classes.","14/Jun/18 15:31;Yohan123;[~mjsax] Did you finish the upgrade path for metadata? If so, we probably could restart this PR.","14/Jun/18 18:53;mjsax;Yes. KIP-268 is done.","02/Jul/18 02:56;Yohan123;This issue will be resolved by KAFKA-4696's PR which is currently ongoing.","02/Jul/18 15:37;yuzhihong@gmail.com;Shouldn't this be marked Resolved when KAFKA-4696 (Open as of now) is integrated ?","02/Jul/18 17:18;mjsax;If we close as duplicate, we can resolve immediately. But you are right, it should not say ""fixed"" but ""duplicate"". Corrected this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Record Headers,KAFKA-4208,13006934,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,michael.andre.pearce,michael.andre.pearce,michael.andre.pearce,22/Sep/16 19:19,09/May/18 14:58,12/Jan/21 10:06,29/Apr/17 02:40,,,,,,,,0.11.0.0,,,clients,core,,,,,9,,,,,"Currently headers are not natively supported unlike many transport and messaging platforms or standard, this is to add support for headers to kafka

This JIRA is related to KIP found here:
https://cwiki.apache.org/confluence/display/KAFKA/KIP-82+-+Add+Record+Headers
",,abhishek.agarwal,akilman,ashish.mahendru,asridhar,githubbot,hachikuji,hj.d.chen@gmail.com,michael.andre.pearce,pallavi.rao,teabot,TimoMeijer,zkobza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CAMEL-11935,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-30 19:27:50.504,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 21 21:35:40 UTC 2017,,,,,,,"0|i33ywn:",9223372036854775807,,,,,,,,,,,,,,,,"30/Mar/17 19:27;githubbot;GitHub user michaelandrepearce opened a pull request:

    https://github.com/apache/kafka/pull/2772

    KAFKA-4208: Add Record Headers

    As per KIP-82
    
    Adding record headers api to ProducerRecord, ConsumerRecord
    Support to convert from protocol to api added Kafka Producer, Kafka Fetcher (Consumer)
    Updated MirrorMaker, ConsoleConsumer and scala BaseConsumer
    Add RecordHeaders and RecordHeader implementation of the interfaces Headers and Header
    
    Some bits using are reverted to being Java 7 compatible, for the moment until KIP-118 is implemented.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/IG-Group/kafka KIP-82

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2772.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2772
    
----
commit f70c094ce5dcbd8be4d348c6dafc170ae94678dd
Author: Michael Andre Pearce <michael.andre.pearce@me.com>
Date:   2017-03-30T19:24:48Z

    KAFKA-4208: Add Record Headers
    
    As per KIP-82
    
    Adding record headers api to ProducerRecord, ConsumerRecord
    Support to convert from protocol to api added Kafka Producer, Kafka Fetcher (Consumer)
    Updated MirrorMaker, ConsoleConsumer and scala BaseConsumer
    Add RecordHeaders and RecordHeader implementation of the interfaces Headers and Header

----
","29/Apr/17 02:40;hachikuji;Issue resolved by pull request 2772
[https://github.com/apache/kafka/pull/2772]","29/Apr/17 02:41;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2772
","07/May/17 16:33;githubbot;GitHub user michaelandrepearce opened a pull request:

    https://github.com/apache/kafka/pull/2991

    KAFKA-4208: Add Record Headers

    Update upgrade.html
    
    Raising this now, as KIP-118 is pulled from release as such submitting this without java 8 changes.
    
    As per remaining review comment from https://github.com/apache/kafka/pull/2772, updating the upgrade notes.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/IG-Group/kafka KIP-82

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2991.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2991
    
----
commit 326a781cba94030c744c3a0cf26355b5d72f9282
Author: Michael Andre Pearce <michael.andre.pearce@me.com>
Date:   2017-05-07T16:30:29Z

    KAFKA-4208: Add Record Headers
    
    Update upgrade.html

----
","21/May/17 21:35;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2991
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-4: Command line and centralized operations,KAFKA-1694,12747005,New Feature,In Progress,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,,granthenke,joestein,joestein,09/Oct/14 12:06,04/Jan/18 19:13,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,8,,,,,https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Command+Line+and+Related+Improvements,,abiletskyi,alanlee,ankon,aziada,baluchicken,benoyantony,ehelleren,felixgv,guozhang,hammer,ijuma,jeffwidman,jeromatron,jholoman,joestein,jthakrar,junrao,ktoy,liqusha,mherstine,mikewiesner,nemon,otis,richardatcloudera,roczei,singhashish,sslavic,stephane.maarek@gmail.com,tgraves,tscoville2012,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-4327,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/14 08:57;abiletskyi;KAFKA-1694.patch;https://issues.apache.org/jira/secure/attachment/12688626/KAFKA-1694.patch","24/Dec/14 19:22;abiletskyi;KAFKA-1694_2014-12-24_21:21:51.patch;https://issues.apache.org/jira/secure/attachment/12689064/KAFKA-1694_2014-12-24_21%3A21%3A51.patch","12/Jan/15 13:29;abiletskyi;KAFKA-1694_2015-01-12_15:28:41.patch;https://issues.apache.org/jira/secure/attachment/12691644/KAFKA-1694_2015-01-12_15%3A28%3A41.patch","12/Jan/15 16:55;abiletskyi;KAFKA-1694_2015-01-12_18:54:48.patch;https://issues.apache.org/jira/secure/attachment/12691672/KAFKA-1694_2015-01-12_18%3A54%3A48.patch","13/Jan/15 17:30;abiletskyi;KAFKA-1694_2015-01-13_19:30:11.patch;https://issues.apache.org/jira/secure/attachment/12691981/KAFKA-1694_2015-01-13_19%3A30%3A11.patch","14/Jan/15 13:42;abiletskyi;KAFKA-1694_2015-01-14_15:42:12.patch;https://issues.apache.org/jira/secure/attachment/12692221/KAFKA-1694_2015-01-14_15%3A42%3A12.patch","14/Jan/15 16:08;abiletskyi;KAFKA-1694_2015-01-14_18:07:39.patch;https://issues.apache.org/jira/secure/attachment/12692242/KAFKA-1694_2015-01-14_18%3A07%3A39.patch","12/Mar/15 11:05;abiletskyi;KAFKA-1694_2015-03-12_13:04:37.patch;https://issues.apache.org/jira/secure/attachment/12704135/KAFKA-1694_2015-03-12_13%3A04%3A37.patch","08/Dec/14 12:39;abiletskyi;KAFKA-1772_1802_1775_1774_v2.patch;https://issues.apache.org/jira/secure/attachment/12685743/KAFKA-1772_1802_1775_1774_v2.patch",,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,2014-12-07 21:41:11.912,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 04 19:13:57 UTC 2018,,,,,,,"0|i20zg7:",9223372036854775807,,,,,,,,,,,,,,,,"07/Dec/14 21:41;abiletskyi;I've added a single patch that covers all currently implemented functionality (Admin message + basis shell functionality with TopicCommand) to receive some initial feedback.

To start the Shell please follow the instructions under 
https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Command+Line+Tool+Installation","13/Dec/14 01:35;junrao;Thanks for the patch. A few comments.

10. I still don't think having a single admin request/response is a good idea. Since the args for different sub admin requests are quite different, it's very hard to reason about what the format in args is. Ideally, we should be able to figure out the request format just from the protocol definition. It seems to me it's cleaner to just create the following standalone requests/responses.

1. create topic
2. alter topic
3. delete topic
4. list topic
5. describe topic (this will be used to replace TopicMetadataRequest eventually)
6. describe cluster (return all brokers and the controller)

11. We can reuse the java request objects in the scala request (see HeartbeatRequestAndHeader.scala as an example).

12. describe topic : I am not sure that we need to include the following options in the request. We can just return the info (replicas, isrs, etc) of all partitions and let the client decide what to do with them.
reportUnderReplicatedPartitions
reportUnavailablePartitions

Since this is a large patch, could you submit an RB using the patch review tool?","13/Dec/14 08:31;joestein;+1 multiple admin request/response messages","13/Dec/14 11:11;abiletskyi;[~junrao]: Thank you for you comments.

I agree with all you remarks, we also think about breaking AdminRequest into separate requests as a preferable solution.
I created and uploaded this patch to see whether we go at least right direction in building shell for admin requests, as I understand there are no major concerns for now about the structure/approach so I'll submit this all with patch review tool and we can follow our usual procedure.","22/Dec/14 08:57;abiletskyi;Created reviewboard https://reviews.apache.org/r/29301/diff/
 against branch origin/trunk","24/Dec/14 19:22;abiletskyi;Updated reviewboard https://reviews.apache.org/r/29301/diff/
 against branch origin/trunk","12/Jan/15 13:29;abiletskyi;Updated reviewboard https://reviews.apache.org/r/29301/diff/
 against branch origin/trunk","12/Jan/15 16:55;abiletskyi;Updated reviewboard https://reviews.apache.org/r/29301/diff/
 against branch origin/trunk","13/Jan/15 17:30;abiletskyi;Updated reviewboard https://reviews.apache.org/r/29301/diff/
 against branch origin/trunk","14/Jan/15 13:42;abiletskyi;Updated reviewboard https://reviews.apache.org/r/29301/diff/
 against branch origin/trunk","14/Jan/15 16:08;abiletskyi;Updated reviewboard https://reviews.apache.org/r/29301/diff/
 against branch origin/trunk","17/Feb/15 19:42;guozhang;[~abiletskyi], I am wondering if we can split the current RBs into multiple ones following the subtasks structure. Concerns with having one big RB is that it is hard to review, and also very hard to commit: as there will likely have some hidden issues with such big changes, and upon finding them after we commit, if it is not easily fix-forwardable we have to revert the whole thing. What do you think?","18/Feb/15 16:47;abiletskyi;[~guozhang], yes, initially I did split it to several patches. The problem is that almost all tickets depend on each other. Also I believe that single patch lets you better understand the whole picture (and the use case - CLI) so people can comment, argue in mail list.
But I totally agree with you this way we can miss some hidden issues. I would prefer to collect some feedback to be sure we more or less agree on approach and after that I can split this big feature to sub-patches.
","19/Feb/15 16:23;guozhang;Sounds great, thanks!","12/Mar/15 11:05;abiletskyi;Updated reviewboard https://reviews.apache.org/r/29301/diff/
 against branch origin/trunk","12/Mar/15 21:04;abiletskyi;Hi,

The list of changes from the latest patch:
- removed MaybeOf type and changed RQ/RP protocol
- switched over to java protocol definitions (removed scala classes)
- removed workaround for KAFKA-1867 (since it was fixed and committed)
- code review fixes

What's left (pending discussion on mailing list):
- Batching Admin Operations
- Remove Cluster Metadata, evolve TopicMetadata to V1 instead
- Introduce fine-grained error codes instead of single AdminRequestFailedError

I also update KIP-4 accordingly and added proposed changes to cover all pending items.","30/Oct/15 21:32;granthenke;[~abiletskyi], are you still able to work on this? I would like to pick the work up if not. I am looking forward to getting this functionality into Kafka as soon as possible.","02/Nov/15 09:17;abiletskyi;[~granthenke], it was decided to split the work into 3 phases: api, admin client, cli. The Phase 1 was implemented under KAFKA-2229, the patch was moved to github (https://github.com/apache/kafka/pull/223). There were some minor comments under this pull request, they were fixed, though not rebased. IMO it lacks some deeper review and maybe testing. In short, you can pickup this ticket. I'm happy to help to close this issue asap too. If anything is needed (i.e. rebase) let me know.","17/Jul/17 13:10;sslavic;Thanks for providing Java AdminClient. While migrating from Scala {{AdminUtils}} to Java {{AdminClient}} I noticed that in the new Java one there is no {{topicExists}} API support other than listing all topics and doing the search locally in memory.

Are there plans already and if not would it make sense to add {{topicExists}} to the Java AdminClient and matching Kafka broker API?","17/Jul/17 13:17;ijuma;[~sslavic], `AdminUtils` is communicating with ZooKeeper directly. The broker doesn't currently expose a protocol API that just checks if a topic exists. We are currently planning to make `listTopics` more efficient, but no immediate plans for a topic exists protocol API. Is there a use case where this is important?","17/Jul/17 16:18;sslavic;Here's one use case: with auto topic creation disabled, when trying to publish with KafkaProducer to a topic that doesn't exist, one will get a timeout back - only way to differentiate is the timeout caused by topic does not exist condition or any other issue I do now by issuing topicExists check as followup to failed on timeout publish request.

Since auto topic creation can be disabled, all CRUD operations should be supported as minimum through client APIs. Yes, this kind of check can already be done with list support and in memory exist check. I see topicExists check as an optimization. With lots of topics, it's handy to have this optimization available in Kafka's Java client APIs, and should be easy to support so please consider adding it to Broker API and Java client.","19/Jul/17 13:08;ijuma;Is the intent to create the topic if it doesn't exist? In that case, you can just call createTopics and if the error states that it already exists, you're good.","04/Jan/18 19:13;felixgv;[~ijuma],

Because delete topic is asynchronous, it is useful to have a way of knowing when the deletion process is completed properly. Currently, we do an extensive series of checks to verify this, but a lightweight approach would be preferable. Obviously, calling createTopics is not an appropriate way of checking if delete topic finished. This particular issue could be side-stepped by providing a first-class synchronous topic deletion API, but at the end of the day, it's probably useful to have a proper topic existence API regardless. The challenge, I suppose, is that this is not a binary state, but rather a gradient. The valid responses might be something along the lines of:

# EXISTS
# DOES_NOT_EXIST
# CREATION_IN_PROGRESS
# DELETION_IN_PROGRESS.

-F",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-146: Support per-connector/per-task classloaders in Connect,KAFKA-3487,12954853,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,kkonstantine,ewencp,ewencp,31/Mar/16 04:03,18/May/17 17:40,12/Jan/21 10:06,18/May/17 17:39,0.10.0.0,,,,,,,0.11.0.0,,,KafkaConnect,,,,,,2,needs-kip,,,,"Currently we just use the default ClassLoader in Connect. However, this limits how we can compatibly load conflicting connector plugins. Ideally we would use a separate class loader per connector/task that is instantiated to avoid potential conflicts.

Note that this also opens up options for other ways to provide jars to instantiate connectors. For example, Spark uses this to dynamically publish classes defined in the REPL and load them via URL: https://ardoris.wordpress.com/2014/03/30/how-spark-does-class-loading/ But much simpler examples (include URL in the connector class instead of just class name) are also possible and could be a nice way to more support dynamic sets of connectors, multiple versions of the same connector, etc.
",,criccomini,ewencp,githubbot,rhauch,shikhar,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-17 14:58:35.884,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 18 17:40:36 UTC 2017,,,,,,,"0|i2vfun:",9223372036854775807,,ewencp,,,,,,,,,,,,,,"17/Jun/16 14:58;rhauch;I hope this can move toward an approach that can eventually support the Jigsaw module system that appears to be coming in JDK9. Jigsaw relies upon the standard Java service loader mechanism (added in JDK6) to find all implementations of an interface, and Jigsaw will then properly handle all dependencies and class loading.

A pre-JDK9 approach is to use a configuration parameter for the workers (e.g., a connector module path) that simply lists the directories in which Kafka Connect can find each connector ""module"". The connector module's directory would contain all of the JARs required by the connector. Upon startup, Kafka Connect could iterate through this list of paths, and for each: create a URL classloader (that inherits the parent classpath), pass the URL classloader to the service loader method to load the connector implementation class (without having to know its name), create a {{ConnectorModule}} object with the URL classloader and reference to the connector class, and then load the {{ConnectorModule}} into an internal registry keyed by name. Then, the rest of Kafka Connect would simply use the registry.

Really, this same registry could also be used to find all implementations available on the current classloader, meaning it would work with connectors that don't define a {{META-INF/services/org.apache.kafka.connect.connector.Connector}} file for the service loader.

And, to add support for JDK9, upon startup Kafka Connect would simply use the service loader to locate all {{Connector}} implementation classes, and populate the same registry, using a different {{ConnectorModule}} implementation that simply instantiates the class and relying upon JDK9 modules to properly handle all of the classloading and isolation. The rest of the Kafka Connect remains unchanged.

The only change to connector implementations is to add support for the service loader, and since that's going to be required by JDK9 it might be worth doing now. And, doing this now would greatly simply the implementation.
","20/Jun/16 00:56;ewencp;[~rhauch] Definitely agree that we want to come up with a forward-looking solution. We still need to support JDK7 (although I think some discussion has been kicked off around moving to baseline of JDK8), so we definitely need to make sure we come up with a compatible solution for JDK8 and JDK9, but if we can set things up to be cleaner in the future, that'd be ideal. The basics (classloader per directory) should definitely be easy to make work. There's lots of other stuff you can do in this area too, e.g. URLClassLaoders that can just pull the jars from the network, or even more awesome would be a version that works from Central/Maven repos and can just auto-resolve all transitive dependencies in some variant of a URLClassLoader and allow all the connectors to be loaded completely dynamically (no more restarts to install new connectors!).","20/Jun/16 02:54;rhauch;[~ewencp] wrote:

{quote}
There's lots of other stuff you can do in this area too, e.g. URLClassLaoders that can just pull the jars from the network, or even more awesome would be a version that works from Central/Maven repos and can just auto-resolve all transitive dependencies in some variant of a URLClassLoader and allow all the connectors to be loaded completely dynamically (no more restarts to install new connectors!)
{quote}

Autoresolving classloaders work fine in many situations, but not all. Some environments do not have access to a Maven class loader, and it's not always repeatable/reliable. Yet not having to restart to install new connectors is still doable even with file-based JARs.","20/Jun/16 05:30;ewencp;[~rhauch] Absolutely, did not mean to imply it was the be-all end-all, just that for some use cases its extremely convenient. I wouldn't rely on Central being up for any sort of production use case :)","12/May/17 00:31;githubbot;GitHub user kkonstantine opened a pull request:

    https://github.com/apache/kafka/pull/3028

    KAFKA-3487: Support classloading isolation in Connect.

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/kkonstantine/kafka KAFKA-3487-Support-classloading-isolation-in-Connect

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3028.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3028
    
----
commit 6e3e92cfadfa7c03090476a0f025b5a6b7989d5c
Author: Konstantine Karantasis <konstantine@confluent.io>
Date:   2017-05-05T00:00:29Z

    KAFKA-3487: Support classloading isolation in Connect.
    
      * Add module.path in worker config.

commit 84967f43712dd55523ca33e855fa0252f9cdb677
Author: Konstantine Karantasis <konstantine@confluent.io>
Date:   2017-05-09T23:10:08Z

    Add isolation package.
    
      * Add a delegating class loader
      * Add per module class loaders
      * Add module factories

commit 7e1169b081c451d0f4a3920e75e0bf1961ee84e8
Author: Konstantine Karantasis <konstantine@confluent.io>
Date:   2017-05-12T00:24:08Z

    Add config property only for standalone currently.

commit d8cd6330c6304267a5d7b07c8ef48aa201d95ac8
Author: Konstantine Karantasis <konstantine@confluent.io>
Date:   2017-05-09T23:19:24Z

    Add maven-artifact dependency for module versioning.

commit 50c8a1b79366ce04231b7c88b9abc36871afc143
Author: Konstantine Karantasis <konstantine@confluent.io>
Date:   2017-05-09T23:21:15Z

    Replace connector factory with new module factory.

commit df22a391e920b2f24ad795df929968cd0d5e2dd1
Author: Konstantine Karantasis <konstantine@confluent.io>
Date:   2017-05-11T18:19:39Z

    Consolidating Modules factory class.

commit ba3a78dbcf10a94a2080d7f7ef87585826c27ea8
Author: Konstantine Karantasis <konstantine@confluent.io>
Date:   2017-05-12T00:20:41Z

    Setting thread context class loader to a modules loader.

----
","18/May/17 17:39;ewencp;Issue resolved by pull request 3028
[https://github.com/apache/kafka/pull/3028]","18/May/17 17:40;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3028
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add API to Start and Stop Stream Threads,KAFKA-10500,13328213,New Feature,Patch Available,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,wcarlson5,cadonna,cadonna,18/Sep/20 09:47,11/Jan/21 20:38,12/Jan/21 10:06,,,,,,,,,2.8.0,,,streams,,,,,,0,kip,,,,"Currently, there is no possibility in Kafka Streams to increase or decrease the number of stream threads after the Kafka Streams client has been started. Uncaught exceptions thrown in a stream thread kill the stream thread leaving the Kafka Streams client with less stream threads for processing than when the client was started. The only way to replace the killed stream thread is to restart the whole Kafka Streams client. For transient errors, it might make sense to replace a killed stream thread with a new one while users try to find the root cause of the error. That could be accomplished by starting a new stream thread in the uncaught exception handler of the killed stream thread.

Part of KIP-663 [https://cwiki.apache.org/confluence/display/KAFKA/KIP-663%3A+API+to+Start+and+Shut+Down+Stream+Threads] ",,ableegoldman,cadonna,mjsax,wcarlson5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10015,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-09-18 09:47:48.0,,,,,,,"0|z0ipoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add wall clock TimeDefinition for suppression of intermediate events,KAFKA-7748,13205003,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,jonathanpdx,jonathanpdx,17/Dec/18 22:02,10/Jan/21 08:53,12/Jan/21 10:06,,2.1.0,,,,,,,,,,streams,,,,,,22,needs-kip,,,,"Currently, Kafka Streams offers the ability to suppress intermediate events based on either RecordTime or WindowEndTime, which are in turn defined by stream time:

{{Suppressed.untilTimeLimit(final Duration timeToWaitForMoreEvents, final BufferConfig bufferConfig)}}

It would be helpful to have another option that would allow suppression of intermediate events based on wall clock time. This would allow us to only produce a limited number of aggregates independent of their stream time (which in our case is event time).

For reference, here's the relevant KIP:

[https://cwiki.apache.org/confluence/display/KAFKA/KIP-328%3A+Ability+to+suppress+updates+for+KTables#KIP-328:AbilitytosuppressupdatesforKTables-Best-effortratelimitperkey]

And here's the relevant Confluent Slack thread:

https://confluentcommunity.slack.com/archives/C48AHTCUQ/p1544468349201700

 ",,Alalem,alissonsales,andreybratus,arnaud.villevieille,astubbs,callaertanthony,ckillmar,donald-dh,dongjin,guozhang,jalaziz,jbfletch,jonathanpdx,kennyjwilli,kobynet,kvileid@yahoo.com,maatdeamon,mathieude,miture,mjsax,mmajis,nikuis,pdebuitlear,sergioazevedo,sumek,theirix,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-01-10 16:47:03.149,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 10 08:53:28 UTC 2021,,,,,,,"0|u002g0:",9223372036854775807,,,,,,,,,,,,,,,,"10/Jan/19 16:47;vvcephei;This has been a big stumbling block for users of Suppress. It would be good to get it implemented ASAP.

This feature requires a KIP, but it would be a very small one.","14/Jan/19 06:08;jonathanpdx;[~vvcephei] It doesn't appear I have perms to create a KIP. Is that something you were hoping I would do or are you planning on taking that on yourself?","14/Jan/19 19:25;vvcephei;Hi [~jonathanpdx], if you're wiling, it would be very helpful for you to write the KIP.

FYI the deadline to get the KIP voted on and accepted in Jan 24th ([https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=100827512).]

[~mjsax] or [~guozhang], can you give [~jonathanpdx] the required permissions?

Thanks,

-John","15/Jan/19 01:30;guozhang;[~jonathanpdx] I've added you to the wiki space.","05/Jun/19 09:22;miture;Any updates on this?","26/Mar/20 13:27;mathieude;What's the status on this ?","30/Apr/20 05:21;maatdeamon;is that feature still being implemented ?","30/Apr/20 07:03;mjsax;The KIP discussion got silent...","30/Apr/20 11:33;maatdeamon;Arffff what a shame. Do we know why ? Do we need more vote on this, or it reach a dead end ?","30/Apr/20 20:38;mjsax;It's not dead if anybody picks it up again... The current proposal was not yet convincing and we still would need to figure out the design of this feature.

Sometime people start a KIP but drop it for unclear reasons.","28/May/20 08:36;pdebuitlear;Any update on this?","28/May/20 16:11;vvcephei;Hi [~pdebuitlear] ,

It seems like [~jonathanpdx] was unable to complete the KIP, so it's available for any interested party to pick it up. As far as I know, no one is currently working on it.

If you are interested, I can help bring you up to speed.

Thanks,

-John","10/Jan/21 08:53;Alalem;Hi [~vvcephei],

I'm interested in having this in, I believe it will help a lot of the suppress api users including me.

How can we get started with this?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support mutual TLS authentication for SASL_SSL listeners,KAFKA-10700,13339489,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,rsivaram,rsivaram,rsivaram,09/Nov/20 10:38,07/Jan/21 05:18,12/Jan/21 10:06,,,,,,,,,2.8.0,,,security,,,,,,0,,,,,See https://cwiki.apache.org/confluence/display/KAFKA/KIP-684+-+Support+mutual+TLS+authentication+on+SASL_SSL+listeners for details,,johanneszinn,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-11-09 10:38:36.0,,,,,,,"0|z0ke9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Topology#connectSourceStoreAndTopic as a public method,KAFKA-10892,13348302,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,joker1007,joker1007,29/Dec/20 12:50,29/Dec/20 20:58,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,need-kip,,,,"I want Topology#connectSourceStoreAndTopic.

Because I want to use a topic as a source topic directly without a redundant changelog topic for not only KeyValueStore but also WindowStore.

This issue is similar to [KAFKA-6840], but is a suggestion for a more general approach
{code:java}
    public synchronized Topology connectSourceStoreAndTopic(final String sourceStoreName,
                                                            final String topic) {
        internalTopologyBuilder.connectSourceStoreAndTopic(sourceStoreName, topic);
        return this;
    }
{code}
h3. Background

I want to use a topic as a source topic for WindowStore because using WindowStore is suitable for the feature that I'm implementing.
 The records stored in the topic are aggregated with a time window by another application. The size of the topic is over 10TB.
 I want to use the topic as a source topic for WindowStore directly.
 But, I cannot do so on the current interface. 
 I need a redundant topic only for storing the records into WindowStore.

If this API is public, I can use topics incoming from other applications (not only Kafka Streams applications) as source topics for any StateStore implementation without redundant changelog topics.

Of course, I need to implement a processor for storing incoming records in such a case.
 But I think it's not difficult.

Please consider this.",,ableegoldman,joker1007,t2y,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-12-29 12:50:18.0,,,,,,,"0|z0lwkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automatic broker version detection to initialize stream client,KAFKA-9689,13290694,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,feyman,bchen225242,bchen225242,10/Mar/20 02:10,22/Dec/20 23:15,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,"Eventually we shall deprecate the flag to suppress EOS thread producer feature, instead we take version detection approach on broker to decide which semantic to use.",,bchen225242,feyman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-12-22 23:15:08.071,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 22 23:15:08 UTC 2020,,,,,,,"0|z0cctc:",9223372036854775807,,,,,,,,,,,,,,,,"22/Dec/20 23:15;feyman;Checked with [~bchen225242] offline, I will pick up this task~",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker side filtering,KAFKA-6020,13107619,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,malejpavouk,malejpavouk,06/Oct/17 20:43,14/Dec/20 10:36,12/Jan/21 10:06,,,,,,,,,,,,consumer,,,,,,5,needs-kip,,,,"Currently, it is not possible to filter messages on broker side. Filtering messages on broker side is convenient for filter with very low selectivity (one message in few thousands). In my case it means to transfer several GB of data to consumer, throw it away, take one message and do it again...

While I understand that filtering by message body is not feasible (for performance reasons), I propose to filter just by message key prefix. This can be achieved even without any deserialization, as the prefix to be matched can be passed as an array (hence the broker would do just array prefix compare).
",,adupriez,bulbfreeman,chetanm,danielnicoletti,DeanJ,flavio_livide,geoffray,hakim.acharifi,jeffwidman,kerbylane,malejpavouk,tkram01,vvcephei,yuzhihong@gmail.com,yzang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-10-06 20:51:59.182,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 14 10:26:38 UTC 2020,,,,,,,"0|i3kzdb:",9223372036854775807,,,,,,,,,,,,,,,,"06/Oct/17 20:51;yuzhihong@gmail.com;This needs a KIP, right ?","30/Aug/18 12:52;geoffray;Hello. 
I strongly support the idea on message filtering on the broker side.


I am using kafka as a message streaming system. I am not committing anything and I am not even keeping track of the offsets.
On the other hands, I have to deal with a vast variety of distinct objects. Hundreds of thousands, up to millions.
Obviously, I cannot have that many topics, but with broker side filtering, I could get get some sort of sub-topics.

To me, the idea would be, on a fetch request to send a mask to test a custom header.
If either the custom header is not present or if it does not match the mask, the message is not sent as part of the reply to the fetch request.
The concept seems simple but I have no clue how much work it is to implement.","07/Mar/19 00:10;yzang;Any updates for this?

We have smilier needs on our side, strongly support this idea on broker-side filtering. 

Our use case comes from N-DC replication. Basically imagine if you have 5 data centers and you need to replicate data to everywhere, typically you'll have to run N*(N-1) which is 20 mirror-maker jobs in order replicate messages in each local data center to all remote data centers. Each mirror maker will have to read the whole 5 copies of events, do some processing and only replicate one fifth of the events. This is a huge waste of network bandwidth and cpu resources. If we can have a way to pre filter the events on broker side, mirror maker doesn't need to read all 5 copies of events any more, which can be a huge amount of savings when we have even more data centers in the future.","30/Jan/20 18:35;DeanJ;its been almost an year and we are looking forward to this feature, any updates, just let us know even if there is any plan in near future for this ???

 

I liked this approach, any thoughts on adopting something similar ??

[https://github.com/flipkart-incubator/kafka-filtering]

 ","14/Dec/20 10:26;flavio_livide;We have use cases where some consumers need the whole topic and others only a small subset. The set up is quite dynamic so setting up topics on purpose for consumers becomes quite complicated to manage. 
 A very simple form of broker side filter would make a big improvement. Could be a key prefix, header based, or as brutal as letting a producer set a number 1 to 1024 and use a bitmask to filter for clients (if I can reduce traffic by three orders of magnitude I can do the rest of the filtering client side).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pluggable standby tasks assignor for Kafka Streams,KAFKA-10686,13339022,New Feature,In Progress,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,lkokhreidze,lkokhreidze,lkokhreidze,05/Nov/20 13:31,09/Dec/20 12:37,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,needs-kip,,,,"In production, Kafka Streams instances often run across different clusters and availability zones. In order to guarantee high availability of the Kafka Streams deployments, users would need more granular control over which instances standby tasks can be created. 

Idea of this ticket is to expose interface for Kafka Streams which can be implemented by the users to control where standby tasks can be created.

Kafka Streams can have RackAware assignment as a default implementation that will take into account `rack.id` of the application and make sure that standby tasks are created on different racks. 

Point of this ticket though is to give more flexibility to users on standby task creation, in cases where just rack awareness is not enough. ",,ableegoldman,cadonna,lkokhreidze,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6718,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-11-05 13:55:37.087,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 05 16:55:48 UTC 2020,,,,,,,"0|z0kbdk:",9223372036854775807,,,,,,,,,,,,,,,,"05/Nov/20 13:32;lkokhreidze;If this ticket makes sense, I'd be keen to work on this and provide KIP.

Would love to incorporate https://issues.apache.org/jira/browse/KAFKA-6718 as part of this KIP.","05/Nov/20 13:55;cadonna;Hi [~lkokhreidze], I guess this ticket makes sense. Looking forward to your KIP!

You might want to look into KIP-441 to see how a Kafka Streams client passes its task lags to the assignor. I can imagine that you want to use a similar mechanism to pass information about a Kafka Streams client to the assignor.","05/Nov/20 16:55;lkokhreidze;Hi [~cadonna]!

Yes, that's exactly right. I'll definitely look into the KIP-441 to get some inspiration. 
 Thanks for the tip!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not trigger REBALANCING when specific exceptions occur in Kafka Streams ,KAFKA-9638,13289136,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,lkokhreidze,lkokhreidze,03/Mar/20 08:16,09/Dec/20 02:33,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,,,,,"As of now, when StreamThread encounters exception in Kafka Streams application, it will result in REBALANCING of all the tasks that were responsibility of the given thread. Problem with that is, if the exception was, lets say some logical exception, like NPE, REBALANCING is pretty much useless, cause all other threads will also die with the same NPE. This kind of mute rebalancing gives extra costs in terms of network traffic, IOPS, etc in case of large stateful applications.

In addition, this behaviour causes global outage of the Kafka Streams application, instead of localized outage of the certain tasks. Would be great if Kafka Streams users could specify via some interface, exceptions that must not trigger rebalancing of the tasks. StreamThread may still die, but in this case, we would have isolated incident.",,ableegoldman,bchen225242,cadonna,lkokhreidze,mjsax,wcarlson5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-03 18:36:32.873,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 09 02:33:57 UTC 2020,,,,,,,"0|z0c3ww:",9223372036854775807,,,,,,,,,,,,,,,,"03/Mar/20 18:36;bchen225242;Thanks for the ticket. Could you provide a more detailed example where rebalance could cause massive shutdown than necessary? AFAIK, if one thread fails with NPE, it shouldn't affect other threads, or the other threads should already be on the edge of falling down.","04/Mar/20 10:09;lkokhreidze;Hi [~bchen225242]

Not sure I understand. Maybe I am missing something but whenever stream thread encounters unrecoverable exception, all its assigned partitions with corresponding state will be migrated to other thread (not necessarily on the same node).

Lets have a concrete example, if we have topology similar to this with 6 threads span across 2 physical machines (3 threads on each machine). And for simplicity, lets say that `input-topic` has also 6 partitions.

 
{code:java}
streamBuilder.stream(""input-topic"")
  .selectKey((key, value) -> value.newKey())
  .groupByKey()
  .aggregate(0d, (key, value, aggr) -> aggr + value.invoiceValue(), Materialized.as(""my-store"")); 
{code}
 

In the aggregator function I have a bug, `value.inoiceValue()` may sometime return null, and code above will throw NPE. As a result, stream thread will die, and it's partition will be re-assigned (with corresponding state) to another thread, maybe to another node altogether. Since there was an error, offset for problematic event won't be committed and other threads are also doomed to die with the same exception. So all the rebalancing, state migration, etc is pretty much useless in this case.

Instead, would be great if Kafka streams would kill the thread for the partition where problematic event is, instead of rebalancing it across different threads. In this case, we would accumulate lag on a single partition but all other threads would still be processing data. So instead of global downtime on 6 partitions, we would have downtime only on a single one.

 

Does this make sense?

 

Regards,

Levani

 ","05/Mar/20 05:14;mjsax;I don't think it would be possible to implement this – the underlying consumer group management does not really allow this. If a thread dies, it will stop to send consumer heartbeats and thus the broker side group coordinator will remove the member from the group and trigger a rebalance.

Not sure if static group membership would help. You can also try to increase corresponding timeouts, but this may have other undesired side effects.

Hence, if you really want to support anything like this, it is required to change how consumer groups works. It's not something that can be addressed at the Streams layer.","09/Dec/20 02:33;wcarlson5;There is a kip that might effect how this issues behaves. With the new handler (KIP-671) there will be an option to just shutdown the application on such failures. This should prevent unnecessary rebalances in this case. It would be left in state ERROR which the following kip is making terminal.

 

[https://cwiki.apache.org/confluence/x/lCvZCQ]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consider to throw exception for failed fetch requests,KAFKA-10315,13319633,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,showuon,mjsax,mjsax,27/Jul/20 22:58,24/Nov/20 14:43,12/Jan/21 10:06,,,,,,,,,,,,consumer,,,,,,0,need-kip,,,,"The current `Consumer#poll(Duration)` method is designed to block until data is available or the provided poll timeout expires. This implies, that if fetch requests fail the consumer retries them internally and eventually returns an empty set of records. – Thus, from a user point of view, returning an empty set of records can mean that no data is available broker side or that the broker cannot be reached.

For Kafka Streams, this behavior is problematic as its runtime would like to distinguish both cases, to apply its own timeouts (cf https://issues.apache.org/jira/browse/KAFKA-9274).

One idea to address this issue is to add a new method `Consumer#pollOnce()` that would throw an exception if a fetch request fails instead of retrying internally.",,mjsax,ramkrish1489,showuon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-07-28 04:28:25.409,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 14:43:37 UTC 2020,,,,,,,"0|z0h8x4:",9223372036854775807,,,,,,,,,,,,,,,,"28/Jul/20 04:10;mjsax;[~showuon] thanks for you interest in this ticket. Before you spent a lot of time on it, I just want to point out, that this ticket is meant as a ""placeholder / reminder"" and it's not really clear if the proposal is what we actually want/need.

We should discuss/brainstorm on the Jira before we start an actually KIP. I am sure that [~guozhang] [~vvcephei] and [~hachikuji] have some thoughts about this idea and we should hear them out first.","28/Jul/20 04:28;showuon;[~mjsax], Thanks for reminding. I'll wait and see what other team member's thought first.","24/Nov/20 14:43;ramkrish1489;[~showuon] [~mjsax] this is an important feature that is required even for a normal consumer to be able to distinguish between no records been fetched and n/w failure to the broker. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add throttling of IPs by connection rate,KAFKA-10749,13341618,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,david.mao,david.mao,david.mao,19/Nov/20 15:59,19/Nov/20 15:59,12/Jan/21 10:06,,,,,,,,,2.8.0,,,core,network,,,,,0,,,,,"This tracks the completion of IP connection rate throttling as detailed in

[https://cwiki.apache.org/confluence/display/KAFKA/KIP-612%3A+Ability+to+Limit+Connection+Creation+Rate+on+Brokers]

 ",,david.mao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-11-19 15:59:53.0,,,,,,,"0|z0kre8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Raft Protocol for Metadata Quorum,KAFKA-9876,13298661,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,hachikuji,hachikuji,hachikuji,15/Apr/20 21:26,18/Nov/20 06:40,12/Jan/21 10:06,,,,,,,,,,,,core,,,,,,1,,,,,"This tracks the completion of the Raft Protocol specified in KIP-595: https://cwiki.apache.org/confluence/display/KAFKA/KIP-595%3A+A+Raft+Protocol+for+the+Metadata+Quorum. If/when the KIP is approved by the community, we will create smaller sub-tasks to track overall prgress.",,adupriez,dajac,feyman,hachikuji,higebu,touchida,unmeshjoshi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-11-18 06:40:55.358,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 06:40:55 UTC 2020,,,,,,,"0|z0dojs:",9223372036854775807,,,,,,,,,,,,,,,,"18/Nov/20 06:40;feyman;Hi, [~hachikuji], I'm interested in Raft and also KIP-595, I would like to get involved in this, is their any sub-task that I can pick up?

I'm currently reading the KIP document/discussion and also the implementation to get familiar. Checked with [~bchen225242] offline, he mentioned that some discussion are not finalized, so I just ask here~

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rack Aware Stand-by Task Assignment for Kafka Streams,KAFKA-6718,13148216,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,_deepakgoyal,_deepakgoyal,27/Mar/18 13:02,05/Nov/20 17:31,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,needs-kip,,,,"|Machines in data centre are sometimes grouped in racks. Racks provide isolation as each rack may be in a different physical location and has its own power source. When tasks are properly replicated across racks, it provides fault tolerance in that if a rack goes down, the remaining racks can continue to serve traffic.
  
 This feature is already implemented at Kafka [KIP-36|https://cwiki.apache.org/confluence/display/KAFKA/KIP-36+Rack+aware+replica+assignment] but we needed similar for task assignments at Kafka Streams Application layer. 
  
 This features enables replica tasks to be assigned on different racks for fault-tolerance.
 NUM_STANDBY_REPLICAS = x
 totalTasks = x+1 (replica + active)
 # If there are no rackID provided: Cluster will behave rack-unaware
 # If same rackId is given to all the nodes: Cluster will behave rack-unaware
 # If (totalTasks <= number of racks), then Cluster will be rack aware i.e. each replica task is each assigned to a different rack.
 # Id (totalTasks > number of racks), then it will first assign tasks on different racks, further tasks will be assigned to least loaded node, cluster wide.|

We have added another config in StreamsConfig called ""RACK_ID_CONFIG"" which helps StickyPartitionAssignor to assign tasks in such a way that no two replica tasks are on same rack if possible.
 Post that it also helps to maintain stickyness with-in the rack.|",,_deepakgoyal,ableegoldman,asurana,githubbot,guozhang,jbfletch,lkokhreidze,miguno,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6642,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-03-27 17:34:31.51,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 18 11:59:07 UTC 2019,,,,,,,"0|i3rtwf:",9223372036854775807,,asurana,,,,,,,,,,,,,,"27/Mar/18 17:34;guozhang;[~_deepakgoyal] thanks for creating the KIP! I've assigned the JIRA to you.

About the KIP itself, please note that if you are enhancing the rebalance protocol to encode the rack id information, these two KIPs are correlated and I'd recommend you read about them first:

Any protocol changes will need to consider a smooth upgrade path:
https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade

We'd like to encode some other information into the metadata to enhance partition assignor's workload balance awareness:
https://cwiki.apache.org/confluence/display/KAFKA/KIP-262%3A+Metadata+should+include+number+of+state+stores+for+task","28/Mar/18 11:06;_deepakgoyal;Meanwhile, please look at the PR: [https://github.com/apache/kafka/pull/4785] ","10/Nov/19 07:04;githubbot;mjsax commented on pull request #4785: KAFKA-6718: Rack Aware Replica Task Assignment
URL: https://github.com/apache/kafka/pull/4785
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","10/Nov/19 07:06;mjsax;The corresponding PR was abandoned. Also, there is no KIP yet as required. Unassigning this ticket due to inactivity.

[~_deepakgoyal]: feel free to pick this up again at any time. Note, that this ticket requires a KIP.","23/Nov/19 17:00;lkokhreidze;I'm interested in picking up this ticket. At TransferWise we already see need for this feature on clustered Kubernetes deployments. If there're no objections I would like to continue working on this.

cc [~mjsax] [~_deepakgoyal]","24/Nov/19 23:26;mjsax;Sure. I guess [~vvcephei] who is working on KIP-441 atm might have some thought on this ticket?","03/Dec/19 08:36;lkokhreidze;Hi [~vvcephei] 

Would appreciate your thoughts on this ticket if there is anything specific you think we must take into account while implementing this new feature.

KIP-441 has a lot of work with standby tasks, wondering if we should unify this in the KIP-441 (pretty sure we shouldn't, thinking out loud), should we wait for KIP-441, or they can be done in parallel?

 ","03/Dec/19 21:39;vvcephei;Hey [~lkokhreidze]. I know Matthias pinged me a while back... I've just been re-queuing the task to look at this ticket every day to the next day. Sorry about that.

I actually don't think that there's anything specific to worry about with respect to KIP-441. In my opinion, you can just design your feature against the current state of Streams, and whoever loses the race would just have to deal with adjusting the implementation to take the other feature into account.

For what it's worth, though, I don't think there's too much semantic overlap, just the practical overlap that both efforts affect the same protocol and module. That's why I don't think we need to wait for each other _or_ try to unify the efforts.

I don't know if you have any specific ideas, and I haven't had a chance to look into the details of the old work on this ticket. (This is what I haven't been able to find time to do). But sharing my immediate thoughts, I've used the ""rack awareness"" feature in Elasticsearch, and felt that it was a pretty straightforward and reasonable way to implement it. See https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-awareness.html for more information.

Basically, they let you add some arbitrary ""tags""/""attrs"" to each instance (in the config), and then there's a ""rack awareness"" config that takes a list of ""tags"" to consider. It's a pretty simple, but also powerful, arrangement. You might also want to consider the ""forced awareness"" section, which is in response to a concern that also applies to us.

Thanks for picking it up!","18/Dec/19 11:59;lkokhreidze;Thanks for the ideas [~vvcephei] will definitely look into it. My initial idea was around how Kubernetes [affinity rules|#affinity-and-anti-affinity]] work. It's similar to the elasticsearch mechanisms you have provided (tbh, I think most of the distributed systems share same idea behind rack awareness and standby tasks). I think with this task we have a possibility to let users specify something similar to affinity/anti-affinity rules based on some interface implementation which can have some default implementation out of the box (rack awareness maybe by default) but maybe can be extended to some other metrics? Would be amazing to have disk space awareness as well, but since disk space can't be static value, like rack.id it maybe challenging to implement it, but definitely it would be useful to have such feature.

Actually, required disk space can be estimated for standby tasks, so in theory it can be encoded (taking disk space as an example can be any other reasonable metric as well) to the assignment so interface that defines affinity rule, would get Standby tasks with additional encoded metrics and either accept the tasks, or reject them. If rejected, it will be routed to other Kafka Streams instance, until succeeds. If appropriate Kafka Streams instance won't be found that corresponds to `num.stanby.replicas` config, we can log warning like it's right now when num(Kafka Streams instance) is less than `num.stanby.replicas`. This is rough idea, which may not work at all since I've not looked in details how standby tasks work atm in Kafka Streams :) Sorry if this all doesn't make sense. But would definitely love to try to make this interface as extendable as possible so we won't be limited to only rack awareness.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorMaker 2.0 (KIP-382),KAFKA-7500,13191101,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ryannedolan,ryannedolan,ryannedolan,12/Oct/18 02:36,05/Nov/20 06:24,12/Jan/21 10:06,07/Oct/19 08:31,2.4.0,,,,,,,2.4.0,,,KafkaConnect,mirrormaker,,,,,17,pull-request-available,ready-to-commit,,,"Implement a drop-in replacement for MirrorMaker leveraging the Connect framework.

[https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0]

[https://github.com/apache/kafka/pull/6295]",,88manpreet,abellemare,adupriez,ahadjidj,almaz,amuraru,apriceq,astubbs,attawaykl,Aun,barbaros.alp,briancop,chaffelson,chridtian.hagel,cloudfrog,cmichaelgraham,ecomar,ejpearson,ella92,enothereska,frvabe,har5havardhan,Hau,ivanyu,JAS71,krisden,kurs,Lothar,mikeharper,mrsrinivas,Munamala,myloginid@gmail.com,OlegOsipov,omkreddy,pdavidson,PhilGrayson,qihong,qq619618919,roczei,ryannedolan,saritago,satyacool,saup007,sdeokulecluster,shoxdid,terence.yi,tewarir,tspann,twbecker,vgovindarajulu,vpernin,whammond,wushujames,wy96f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10128,,,,,,KAFKA-395,,,"11/Jul/19 17:34;Munamala;Active-Active XDCR setup.png;https://issues.apache.org/jira/secure/attachment/12974437/Active-Active+XDCR+setup.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2019-04-24 01:50:11.873,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 05 06:09:05 UTC 2020,,,,,,,"0|i3z45r:",9223372036854775807,,,,,,,,,,,,,,,,"24/Apr/19 01:50;terence.yi;What's the time schedule of MM2. 

 ","24/Apr/19 03:48;ryannedolan;[~terence.yi] MM2 is currently being tested by several companies based on the PR here: [https://github.com/apache/kafka/pull/6295]

I intend to mark the PR as ready-for-review in the coming days, and I'm hoping it will get merged by the 2.3 release.","24/Apr/19 05:29;terence.yi;That's great!

Do you have a test case set for reference? I wanted to start the evaluation in coming days also.","24/Apr/19 17:04;ryannedolan;[~terence.yi] the most trivial scenario would be something like this:

 

 

{{clusters = primary, secondary}}

{{primary.bootstrap.servers = primary1.host:9092, primary2.host:9092, primary3.host:9092}}

{{secondary.bootstrap.servers = secondary1.host:9092, secondary2.host:9092, secondary3.host:9092}}

{{primary->secondary.enabled = true}}

{{}}{{secondary->primary.enabled = true}}

{{primary->secondary.topics = .*}}

{{secondary->primary.topics = .*}}

 

... where there are two clusters replicating each other. Then some simple things you can test include:
 # create a topic on either cluster; a remote topic should show up on the other
 # send records to a topic on one cluster and verify the same records appear on the other
 # change a topic's configuration, e.g. retention.policy=compact, and verify the config is sync'd to the other
 # change a topic's ACL and verify the ACL is sync'd to the other
 # consume from a topic in one cluster and verify that checkpoints are emitted for the group to the other
 # use RemoteClusterUtils to translate a consumer's offsets from one cluster to the other and verify no records are skipped

etc

I'm putting together documentation for more advanced scenarios and will share when ready.","09/May/19 02:31;cloudfrog;I'm excited to test this as well.  We've been fighting with duplicates for a long time, and wasting a lot of resources dealing with duplicate detection and remediation.

I have a few questions about the initial release.
 # Is this expected to work with kafka 1.1.1 clusters?  This is currently the max version we can upgrade to without breaking support for vertica integration.
 # KIP-382 mentions that you will prefix remote topic names to avoid replication loops.  Would this be configurable?  I am already using a similar approach by adding the local datacenter as a suffix to my topics although it requires the producer to be datacenter aware, and additionally prevents the ability to create aggregate topics without help of app.  Would be great to see options to transform destination topic names.","09/May/19 03:30;ryannedolan;[~cloudfrog], glad you are taking a look. I'm looking forward to hearing about your experience.

> Is this expected to work with kafka 1.1.1 clusters?

Yes, I believe it will work with 0.11.0.0 or higher, but maybe you can test it to verify :)

> will prefix remote topic names ... be configurable? 

Yes, you can implement your own ReplicationPolicy to define remote topics however you like:

 
{code:java}
replication.policy.class = my.SuffixReplicationPolicy
{code}
Also, MM2 doesn't care how existing source topics are named. If your topics are suffixed with their local DC (a common pattern), you can leave them as-is without breaking anything. By default you'd get topics like ""dc1.topic1-dc1"", so you might consider implementing a ReplicationPolicy that strips the suffix during replication so you get just ""dc1.topic1"".

Ryanne

 ","09/May/19 15:55;ryannedolan;[https://github.com/apache/kafka/pull/6295]","23/May/19 21:09;Munamala;[~ryannedolan],

I configured connect mirrormaker  for replicating topics between two clusters on  kafka 1.1.1  from source to target. Can you please provide your input to the following.
 # The topics are created with a replication factor 1 in the target cluster even though the source cluster has replication factor 4.  The default replication factor of the target cluster is 4. Obviously, the topic data is failing to replicate with the error: org.apache.kafka.common.errors.NotEnoughReplicasException: Number of insync replicas for partition target.TEST_TOPIC is [1], below required minimum [2]
 
 Replication works fine , if I manually create the topics in the target cluster with replication factor 4 before starting the replication.
 # I see the following lines in the log:
 
 [2019-05-23 20:50:39,033] WARN [Producer clientId=producer-9] Error while fetching metadata with correlation id 2079 : \{source.checkpoints.internal=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient:1023)
 [2019-05-23 20:50:39,039] WARN [Producer clientId=producer-8] Error while fetching metadata with correlation id 2175 : \{heartbeats=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient:1023)
 
 How are the topics created: source.checkpoints.internal and heartbeats?

      3. Is this the right forum to ask questions during the evaluation?

Thanks!","24/May/19 15:15;ryannedolan;[~Munamala] thanks for trying it out!

> The topics are created with a replication factor 1

There is a property `replication.factor` that you can change as needed to prevent this issue. This can be set across all clusters or for a specific cluster, e.g. `my-cluster.replication.factor = 4`, and MM2 will use this value when creating topics.

[~arunmathew88] suggested we bump the default replication factor to 2. It would be cool if MM2 could figure this value out based on the downstream cluster's min ISRs -- maybe a future KIP?

> How are the topics created: source.checkpoints.internal and heartbeats?

Currently they are not being actively created -- we sorta assume the downstream brokers are configured to auto-create topics, or that the internal topics are manually created. It's a good idea to fix this, thanks.

> Is this the right forum to ask questions during the evaluation?

Yes, thanks!

Ryanne","20/Jun/19 13:55;abellemare;[~ryannedolan] - Thanks for driving this forward, it looks extremely useful.

I'm currently evaluating this for my company's needs. I have a few questions.

1) Are the offset translation topics included in the patch you have above? I ask because I see that https://issues.apache.org/jira/browse/KAFKA-7815 is still open, and I am not sure if SourceTask is the provider of this info and to the overall status of the offset translations work.

2) How do you go about switching a consumer from one cluster to another, say in the case of a total cluster failure? I have noted that there exists the offsets topics that indicate the source offset and equivalent destination offset for a consumer group. Aside from the mechanisms of redirecting the consumer to the replica cluster, what do you do in terms of ensuring that the consumer is able to read from this destination cluster? My first instinct is to have a service that consumes from the offsets-sync topic and sets the offsets for each consumer group as the new sync events come in. If this is up to date with the latest events, then it should be possible to restart the consumer from the checkpoint in the destination cluster under the same consumer group id.

3) Do you use timestamps at all for failing over from one cluster to another? This is a bit of an extension to #2 above, and my current understanding is that it is not necessary to do so. I seem to recall that in previous versions of MM it was more common to use timestamps than offsets, since the guarantee for the offset wasn't present.

 

Anyways, thanks very much for your work here. I am eager to see this get into 2.4 because I believe it will make multi-cluster much more approachable for smaller businesses.","21/Jun/19 22:36;ryannedolan;[~abellemare] Thanks for trying it out.

> 1) Are the offset translation topics included... KAFKA-7915

Yes, I've included the required changes to SourceTask in PR-6295.

> 2) ...switching a consumer from one cluster to another...

So glad you asked :)

The key is the RemoteClusterUtils.translateOffsets() method. This consumes from the checkpoint topic (not the offset-sync topic directly), which has both upstream and downstream offsets for each consumer group. The downstream offsets are calculated based on the offset-sync stream, of course, but MirrorCheckpointConnector does the translation for you. This makes the translateOffsets() method rather straightforward -- it just finds the most recent checkpoint for a given group.

The translateOffsets() method works both ways: you can translate from a source topic (""topic1"") to a remote topic (""us-east.topic1"") and vice versa, which means your failover and failback logic is identical. In both cases you just migrate all your consumer groups from one cluster to another.

Also note that migration only requires information that is already stored on the target cluster (the checkpoints), so you do not need to connect to a failed cluster in order to failover from it. Obviously that would defeat the purpose!

Based on translateOffsets(), you can do several nifty things wrt failover/failback, e.g. build scripts that bulk-migrate consumer groups from one cluster to another, or add consumer client logic that automatically failsover to a secondary cluster as needed.

In the former case, you can use kafka-consumer-groups.sh to ""reset offsets"" to those returned by translateOffsets(). This will cause consumer state to be transferred to the target cluster, in effect. In the latter, you can use translateOffsets() with KafkaConsumer.seek().

There are more advanced operations and architectures you can build using MM2 as well. Some are outlined in the following talk (by me): https://www.slideshare.net/ConfluentInc/disaster-recovery-with-mirrormaker-20-ryanne-dolan-cloudera-kafka-summit-london-2019

> 3) Do you use timestamps at all for failing over from one cluster to another?

MM2 preserves the timestamps of replicated records, but otherwise does not care about timestamps. Nor does failover need to involve any timestamps.

Ryanne","03/Jul/19 22:45;Munamala;[~ryannedolan],   Thanks for all your efforts on MM2.  I am also eagerly looking forward MM2  to be on 2.4.0.

Appreciate your input on the following scenario:

I have setup two clusters K1 and K2 and configured MM2 replication for TOPIC1. My consumer group is ""TOPIC_GROUP"". 
 
 1. To start with K1 cluster is active and ""TOPIC_GROUP"" is actively consuming TOPIC1 from K1. 
 2. Now, the failover to K2 is triggered and the ""TOPIC_GROUP"" starts consuming from K2. I have used RemoteClusterUtils.translateOffsets to seek to the correct offset for K1.TOPIC1.
 3. During failover, there were #N messages in TOPIC1 in K1 that were not consumed by ""TOPIC_GROUP"" but were replicated in K2 and consumed by ""TOPIC_GROUP"" in K2 as K1.TOPIC1

In this scenario per my testing, when K1 comes back online and TOPIC_GROUP restarts consuming from K1, the #N messages will be consumed again from TOPIC1. Is this expected? Is there a way to do consumer.seek on the TOPIC1 partitions in K1, based on the K1.TOPIC1 consumer offsets in K2?

Thanks!","04/Jul/19 04:31;ryannedolan;[~Munamala] Yes, the translateOffsets() works both ways. When K1 comes back online, you can ""failback"" from K2 to K1 once a checkpoint for TOPIC_GROUP is emitted upstream to K1. The checkpoint will have offsets for TOPIC1 on K1 (translated from K1.TOPIC1 on K2). You can then seek() to skip over the records TOPIC_GROUP already consumed in K2.

The tricky part here is that you need to make sure MM2 is configured to emit checkpoints both from K1->K2 and K2->K1. Configure the whitelists like:

K1->K2.topics = TOPIC1, K2.TOPIC1
K2->K1.topics = TOPIC1, K1.TOPIC1
K1->K2.groups = TOPIC_GROUP
K2->K1.groups = TOPIC_GROUP

Otherwise, you won't see checkpoints for TOPIC1 going from K2 to K1.

Ryanne","11/Jul/19 17:38;Munamala;[~ryannedolan]  translateOffsets() has worked for both failover and fallback scenarios. Thanks for the info.  Another request, For the  XDCR with Active/Active layout as per one of your slides, (attached here)  can you suggest any preferred way of  deploying MM2 clusters to support the replication. Thanks again. !Active-Active XDCR setup.png!","12/Jul/19 22:05;sdeokulecluster;Hello, 

Tentatively when is 2.4.0 expected to be  released ? 

Will  Kafka version 0.10.2.1 work with MM 2.0 or we should plan to upgrade to 0.11.0 at the minimum ?

Thanks","16/Jul/19 16:11;apriceq;Hi, Sorry for what is probably a stupid question. I'm interested in trying this out. Are there any pre-releases anywhere? Or is the proper way to test this to pull and build the referenced PR branch? 

If the pull and build is the path where is the best place to find instructions for generating a ""release"" from it?

Thanks!

 

edit - Thanks [~ryannedolan] for the reply! (assuming the last line was meant to be a gradle command) but otherwise, looking forward to trying it out!","16/Jul/19 22:19;ryannedolan;[~Munamala] I discuss this a bit in the readme: https://github.com/apache/kafka/blob/cae2a5e1f0779a0889f6cb43b523ebc8a812f4c2/connect/mirror/README.md

Please let me know if anything is unclear.","16/Jul/19 22:26;ryannedolan;[~sdeokulecluster] Kafka releases are time-based, every 4 months. MM2 has been tested with several versions of Kafka and should work with 0.10.2.1, though I have not tested that specific version.","16/Jul/19 22:33;ryannedolan;[~apriceq] I would recommend pulling down the PR and building it. Something along these lines should work:

    $ git clone github.com/ryannedolan/kafka
    $ git checkout KIP-382
    $ gradle :connect:mirror:build

Of course, you can also fork my repo. I welcome PRs!","01/Aug/19 14:20;shoxdid;Hey,

I use kubernettes to spin up 2 kafka clusters locally (3 brokers each). And then I run MM2 locally as well to sync topic messages. When I send message to source topic, source.topic is created in the sink cluster, but message is not delivered. An exception is thrown in the console (see below). When I restart MM2, message arrives in the source.topic. Anyone recognize this error? Moreover, when I move one of the kafka cluster to a different machine, everything works again. I tried to increase network/io threads in the local setup, it still doesn't solve the issue.

 

[2019-08-01 10:18:29,033] INFO WorkerSourceTask\{id=MirrorSourceConnector-0} flushing 21 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:418)
[2019-08-01 10:18:29,072] INFO WorkerSourceTask\{id=MirrorSourceConnector-0} Finished commitOffsets successfully in 39 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:500)
[2019-08-01 10:18:29,072] ERROR WorkerSourceTask\{id=MirrorSourceConnector-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:179)
{color:#FF0000}java.lang.NullPointerException{color}
{color:#FF0000} at org.apache.kafka.connect.mirror.MirrorSourceTask.poll(MirrorSourceTask.java:140){color}
{color:#FF0000} at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:245){color}
{color:#FF0000} at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:221){color}
{color:#FF0000} at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:177){color}
{color:#FF0000} at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:227){color}
{color:#FF0000} at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515){color}
{color:#FF0000} at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264){color}
{color:#FF0000} at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128){color}
{color:#FF0000} at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628){color}
 at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-01 10:18:29,073] ERROR WorkerSourceTask\{id=MirrorSourceConnector-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:180)
[2019-08-01 10:18:29,073] INFO [Producer clientId=connector-producer-MirrorSourceConnector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1153)
[2019-08-01 10:18:29,080] INFO [Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1153)
[2019-08-01 10:18:44,602] INFO [Worker clientId=connect-2, groupId=sec-mm2] Attempt to heartbeat failed since coordinator localhost:31000 (id: 2147483647 rack: null) is either not started or not valid. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:931)
[2019-08-01 10:18:44,602] INFO [Worker clientId=connect-2, groupId=sec-mm2] Group coordinator localhost:31000 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:780)
[2019-08-01 10:18:44,612] INFO [Worker clientId=connect-2, groupId=sec-mm2] Discovered group coordinator localhost:31002 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:728)","21/Aug/19 19:56;ryannedolan;[~shoxdid] This is now fixed, thanks for reporting!","11/Sep/19 17:56;qihong;[~ryannedolan] Thanks for all your efforts on MM2. Appreciate your input on the following questions.

I'm trying to replicate topics from one cluster to another, include topic data and related consumers' offset. But I only see the topic data was replicated, not consumer offsets.

Here's my mm2.properties
{code:java}
clusters = dr1, dr2

dr1.bootstrap.servers = 10.1.0.4:9092,10.1.0.5:9092,10.1.0.6:9092
dr2.bootstrap.servers = 10.2.0.4:9092,10.2.0.5:9092,10.2.0.6:9092

# only allow replication dr1 -> dr2
dr1->dr2.enabled = true
dr1->dr2.topics = test.*
dr1->dr2.groups = test.*
dr1->dr2.emit.heartbeats.enabled = false

dr2->dr1.enabled = false
dr2->dr1.emit.heartbeats.enabled = false
{code}
Here's how I started MM2 cluster (dr2 as the nearby cluster)
{code:java}
nohup bin/connect-mirror-maker.sh mm2.properties --clusters dr2 > mm2.log 2>&1 &
{code}
On dr1, there's topic *test1*, and consumer group *test1grp* for topic test1.

On dr2, I found following topics
{code:java}
__consumer_offsets
dr1.checkpoints.internal
dr1.test1
heartbeats
mm2-configs.dr1.internal
mm2-offsets.dr1.internal
mm2-status.dr1.internal
 {code}
But couldn't find any consumer groups on dr2 related to consumer group *test1grp*.

Could you please let me know in detail how to migrate consumer group *test1grp* from dr1 to dr2?, i.e. what command(s) need to run to set up the offset for *test1grp* on dr2 before consume topic *dr1.test1* ?

 

By the way, how to set up and run this in a Kafka connect cluster? i.e., how to set up 
 MirrorSourceConnector, MirrorCheckpointConnector in a connect cluster? Is there document about this?
  ","11/Sep/19 20:45;ryannedolan;[~qihong] thanks for the questions.

> But couldn't find any consumer groups on dr2 related to consumer group test1grp.

MM2 does not create consumer groups for you or attempt to keep them in sync -- it only produces checkpoints (dr1.checkpoints.internal) that encode the state of remote consumer groups. You must then do something with these checkpoints, depending on your use-case. The RemoteClusterUtils class will read checkpoints for you, which you can then use in interesting ways.

For example, you can use RemoteClusterUtils.translateOffsets() and the kafka-consumer-groups.sh --reset-offsets tool to create a consumer group in dr2 based on MM2's checkpoints from dr1. Or, you can use RemoteClusterUtils in your Consumer code to failover/failback automatically. Both require a bit of code, but nothing too sophisticated.

Looking ahead a bit, this will be a ton easier when KIP-396 is merged (KAFKA-7689). Once consumer offsets can be controlled from the Admin API, it will be possible to consume checkpoints and update offsets directly. That will enable the behavior you were expecting.

> By the way, how to set up and run this in a Kafka connect cluster?

MM2's Connectors are just plain-old Connectors. You can run them with connect-standalone.sh or connect-distributed.sh as with any other Connector. To do so, you need a worker config and a connector config as usual. The worker config must include whatever client settings are required to connect to the _target_ cluster (i.e. bootstrap servers, security settings), since the Worker is what is actually producing downstream records. The connector configs, on the other hand, need connection settings for _both_ source and target clusters (e.g. source.cluster.bootstrap.servers, target.cluster.bootstrap.servers). The Connectors use both source and target clusters when syncing topic configuration etc.

There is an example Connector configuration here: https://github.com/apache/kafka/pull/6295#issuecomment-522074048","13/Sep/19 09:08;chridtian.hagel;Dear [~ryannedolan] , thanks also from my side for your efforts pushing MM2.
 I was trying out MM2 in order to mirror one cluster completely to a downstream one in a distributed connector setup.
 When running the MM2, I saw at the beginning lot's of 
{code:java}
WorkerSourceTask{id=mirror-maker-49} Failed to flush, timed out while waiting for producer to flush outstanding 5942 messages
WorkerSourceTask{id=mirror-maker-49} Failed to commit offsets{code}
Error messages.
 I would guess they are happening when the MM2 replicates faster, than it can write on the connect.offsets.topic. This topic is configured with 25 partitions, which I thought should provide enough throughput to handle the load.

In order to test the robustness of the MM2, we also replaced the containers several times during the replication and observed a surprising amount of duplicated messages, which I guess happen because of the lagging offset commit.

Any ideas, how the setup can be improved so it becomes more robust.","13/Sep/19 16:45;ryannedolan;[~chridtian.hagel] thanks for giving it a spin. The ""failed to flush"" errors are probably due to WorkerSourceTask being unable to send the 5942 messages within the default flush timeout, which I believe is 5 seconds. There are various reasons this might be the case:

- tasks.max could be 1 (the default), which means a single Producer is sending records across the entire Herder. Try increasing this considerably. This can be as high as the total number of partitions being replicated, at the cost of more overhead per partition, obviously. If you configure this too high, MM2 just uses one task per partition.
- The producer lag may be high, which is detrimental to throughput. Make sure the MM2 driver is running close to the target cluster to minimize this latency. If you are replicating between multiple DCs, consider running a few MM2 nodes in each DC, with `--clusters` argument to hint which clusters are nearby. That way, drivers will consume from other DCs but only produce locally.
- You may need to use more MM2 nodes.
- You may need to increase the 5 second flush timeout.

Re: duplicated messages, you are correct that MM2 will send dupes if containers are bounced like that. Generally, this is okay -- occasional dupes are a fact of life in most Kafka pipelines. That said, I am working on a PoC and KIP for exactly-once replication with MM2, which will eliminate these dupes.

","17/Sep/19 13:42;chridtian.hagel;[~ryannedolan] increasing the number of tasks and playing around with the producer.buffer.memory variable helped a lot to get rid of the above-mentioned error.

 

However, I think I stumbled over a bug with the topic config syncing. When starting the mm2 with the following config:
{code:java}
{
    ""connector.class"": ""org.apache.kafka.connect.mirror.MirrorSourceConnector"",
    ""source.cluster.alias"": """",
    ""replication.policy.separator"": """",
    ""target.cluster.alias"": ""B"",
    ""source.cluster.bootstrap.servers"": ""ip:9092"",
    ""target.cluster.bootstrap.servers"": ""ip:9093"",
    ""sync.topic.acls.enabled"": ""false"",
    ""replication.factor"": ""1"",
    ""internal.topic.replication.factor"": ""1"",
    ""topics"": "".*"",
    ""enabled"": ""true"",
    ""rename.topics"": ""false"",
    ""replication.factor"": 1,
    ""refresh.topics"": ""true"",
    ""refresh.groups"": ""true"",
    ""sync.topic.configs"": ""true""
  }
{code}
it indeed does create all topics from the source cluster in the target cluster, with the correct number of partitions.

Other config parameters like cleanup.policy remain in the cluster default. I thought at first insufficient acls might be the cause, but I replicated the behavior also with a simple docker-compose setup. Did I miss a configuration? Or is this even the intended behavior","17/Sep/19 16:03;ryannedolan;[~chridtian.hagel] we try to be smart about what config properties are replicated and which are left as defaults:

- we have a config property blacklist: https://github.com/apache/kafka/pull/6295/files#diff-1ae39c06c52ded296030121b13d4b791R33
- we don't replicate a config property that is inherited from the cluster default or from the static broker config, i.e. we only replicate properties that are explicitly set for a topic.
- we don't replicate read-only or sensitive properties for obvious reasons.

cleanup.policy is one that _should_ be replicated, generally, unless you are expecting the default value to be replicated. Also, be advised that config sync is only periodic, with a default interval of 10 minutes.","17/Sep/19 18:42;qihong;Hi [~ryannedolan], just listened your Kafka Power Chat talk, Thanks!

Have a follow up question about the last question (from me) you answered in the talk. You said you prefer dedicated MM2 cluster over running MM2 in connect cluster since you can use less number of clusters to do replications among multiple Kafka clusters.

But there's no REST Api for a dedicated MM2 cluster that can provide the status of the replication streams, nor updating the replication configuration. Any changes to the configuration meaning update the config files and restart all MM2 instances, is that right? Or I missed it that dedicated MM2 cluster does provide REST API for admin and monitoring, if so, where is it?

If my understanding is correct, we can archive the same thing with MM2 in connect cluster. Assume there are 3 Kafka clusters: A, B, and C. Set up a connect cluster against C (meaning all topics for connectors' data and states go to cluster C), then set up MM2 connectors to replicate data and metadata  A -> B and B -> A. If this is correct, we can use the Kafka cluster C plus the connect cluster that running against Kafka cluster C to replicate data among more Kafka clusters, like A, B, and D, and even more. Of course, this needs more complicated configuration, which requires deeper understanding how the MM2 connectors work. In this scenario, the connect cluster provides REST API to admin and monitoring all the connectors. This will be useful for people can't use Stream Replication Manager from Cloudera or Kafka replicator from Confluent for some reason. Is this right?","17/Sep/19 19:57;ryannedolan;[~qihong] That's all correct :)

In fact, KIP-382 mentions a ""primary"" cluster (in your case ""C"") and a single Connect cluster being used to replicate data between any number of other clusters. One way to do this is with a SinkConnector (part of the KIP and coming later) s.t. records travel _through_ the primary cluster to other clusters. I believe you could also override the Connect Worker's internal Producer config to write directly to another cluster, s.t., as you say, state is stored in one cluster but records go to another. I've never tried that, ymmv, but I suspect it'd work as you've described.

Notice that, generally, you will end up with multiple Connect clusters anyway -- one in each DC -- for performance and HA reasons. At that point you are back to managing multiple Connectors on multiple Connect clusters. MM2's top-level driver manages that complexity for you by automatically spinning up and configuring all the required Workers.

re a REST API, the MM2 driver essentially turns off the REST service inside each Herder. This is because the current Connect REST API doesn't support having multiple Herders or Worker configs, so we'd need to sorta abuse the Connect REST API to get it to work. However, there was much discussion around KIP-382 re an MM2 REST API, and there are several good ideas floating around. These were ultimately deferred, but not ruled out.

Also, fwiw, I have successfully run an MM2 cluster with the Connect REST API turned on, with each Herder's endpoints wrapped in a higher-level API. I have done this successfully with a reverse proxy as well as with a fork of Connect's RestServer. This enables you to start/stop/configure individual connectors within the MM2 cluster, if you so wish. 

And finally: MM2 is very pluggable. For example, you can drop in a TopicFilter that grabs dynamic whitelists from somewhere. I happen to know this works very well :)","18/Sep/19 16:23;qihong;Hi [~ryannedolan], Thanks for your quick response (y)(y)(y)

Now I know what ""primary"" means. According to blog [A look inside Kafka MirrorMaker2|https://blog.cloudera.com/a-look-inside-kafka-mirrormaker-2/] by Renu Tewari,
{quote}In MM2 only one connect cluster is needed for all the cross-cluster replications between a pair of datacenters. Now if we simply take a Kafka source and sink connector and deploy them in tandem to do replication, the data would need to hop through an intermediate Kafka cluster. MM2 avoids this unnecessary data copying by a direct passthrough from source to sink.  
{quote}
 This is exactly what I want! Do you have release schedule for *SinkConnector*, and *direct passthrough from source to sink* feature?

 ","18/Sep/19 16:50;ryannedolan;[~qihong] the direct passthrough described there is what MM2 currently does -- there are dedicated workers consuming from each source cluster and writing to each target cluster, without requiring a bunch of Connect clusters, and without requiring hops through an intermediate Kafka cluster. I think the confusion here is that ""sink"" does not imply ""SinkConnector"". Sink in this context is the target cluster.","20/Sep/19 08:25;Aun;Hello Ryanne, I have executed few performance tests for MM2 as part of evaluation and sharing results here. Tests are specifically focused towards

1. Over all latency in replication
 2. System resource usage - CPU and Memory

*Hardware*

All the tests are executed in nodes with below hardware configurations

44 core CPU
 256 GB RAM
 1.25 GBps (10GBps shared link)

*Kafka Cluster:*

Two 3 node kafka clusters (namely primary, backup)
 Replication factor is 3 by default
 Keberos authentication enabled

*MM2 setup:*
 3 instances of MM2
 8 GB RAM

 
||Test 1||Test 2||
|one topic, 1000 partitions
 duration: 1 hour
 Message Size - 10KB
  
 MM2 conf: tasks.max = 100
  
 Replication: 
 primary -> backup cluster
  
 60000 messages/2 second
 600 MB/2 second
  
 * Latency: Avg. 4 seconds
 * MM2 process CPU usage took 80-100% on avg throughout the run duration
 * Memory usage is between 3-3.5 GB|two topics, 1000 partitions
 duration: 1 hour
 Message Size - 1KB
  
 MM2 conf: tasks.max = 100
  
 Replication: 
 primary -> backup
 backup -> primary
  
 120000 messages/2 second (to each topic)
 120 MB/2 second (to each topic)
  
 * Latency: Avg. 8 seconds
 * MM2 process CPU usage took 100-140% on avg throughout the execution 
 * Memory usage is between 3-4 GB|

 

*Test 3:*
 
6 topics, 6 partitions (each)
 duration: 1 hour
 Message Size - 1KB
  
 MM2 conf: tasks.max = 20
  
 Replication: 
 primary -> backup 
 backup -> primary
  
20000 messages/2 second (to each topic)
 20 MB/2 second (to each topic)
 * Latency: Avg. 150 milliseconds
 * Memory usage is between 3-4 GB

 

 Based on the test results,
 * Number of tasks has direct impact on latency. More tasks -> less latency
 * Message size does not have any impacts on replication latency
 * In case of shared cluster (other services are also running), CPU cores >=32 yielded good results (latency in seconds)
 * It is good to have >= 8GB memory dedicated to MM2 instance
 * Higher Number of messages per time interval to one topic causes more latency than the same number of messages to multiple topics
 * When number of partitions increases (also tasks increased), latency decreases for that topic

Thanks for all your efforts on MM2. Happy to provide more details if required.","07/Oct/19 08:31;omkreddy;Issue resolved by pull request 6295
[https://github.com/apache/kafka/pull/6295]","12/Apr/20 10:28;Lothar;Hi [~ryannedolan], Thanks for your great job.
I have a question about the failover in the scenario Srikala described. 
Let's say that we have two clusters K1 and K2 and configured MM2 replication for TOPIC1. The consumer group is ""TOPIC_GROUP"".
We configured the whitelist as you saied:
K1->K2.topics = TOPIC1, K2.TOPIC1
K2->K1.topics = TOPIC1, K1.TOPIC1
K1->K2.groups = TOPIC_GROUP
K2->K1.groups = TOPIC_GROUP
In the beginning the conusmers consume Topic1 from K1. Then K1 out of service, failover happens. Consumers start to consume K1.TOPIC1 from K2.
And producers switch to producing records to K1.TOPIC1 on K2 cluster. The record will sync from K2 to K1 now. The question is that which target topic the records will be sync to on K1 cluser? TOPIC1 or K2.K1.TOPIC1? ","13/Apr/20 20:13;ryannedolan;[~Lothar] you have a small misunderstanding: after failover producers do _not_ produce to K1.TOPIC1, as that is a remote topic and by definition has only _replicated_ records from some other cluster. Instead they just produce to TOPIC1 (a normal topic) on the new cluster, which is then replicated back to K1 as K2.TOPIC1.","16/Apr/20 15:29;Lothar;[~ryannedolan] I got your point. Thanks for your reply.","21/May/20 02:25;qq619618919;[~ryannedolan] I have two distinct kafka clusters located in different data centers - DC1 and DC2. How to organize kafka producer failover between two DCs? If primary kafka cluster (DC1) becomes unavailable, I want producer to switch to failover kafka cluster (DC2) and continue publishing to it? Producer also should be able to switch back to primary cluster, once it is available. Any good patterns, existing libs, approaches, code examples?","22/May/20 18:23;ryannedolan;[~qq619618919] I usually recommend using a load balancers and health checks for this purpose, but you can also get away with just two VIPs (dc1-kafka, dc2-kafka) and KIP-302.","27/May/20 13:08;ella92;Hi [~ryannedolan]. Now I can't find class LegacyReplicationPolicy.class. Could you describe where is it? I can't find it:(","05/Jun/20 11:25;barbaros.alp;[~ryannedolan] I couldn't find any information regarding `MirrorMakerMessageHandler`. Is there another filter that is capable of providing the same features as MirrorMakerMessageHandler?
  ","19/Jun/20 02:40;har5havardhan;Hi,

Does Mirror maker 2 support Kafka version 0.10.0.0 ?","19/Jun/20 21:52;ryannedolan;[~har5havardhan] I think that is unlikely, given all the missing Admin APIs. You can certainly disable offending features (e.g. topic config sync) and get MM2 to work with older versions of Kafka, but I have not tried with 0.10.0.0 specifically.","18/Sep/20 07:49;OlegOsipov;Hello! 

I have a question about the cross-cluster exactly-once delivery guarantee.

Was it implemented?  I've got two M2M within two DC, it looks like I have no any duplicates. But I can't find any mention of it. But for example, here [A look inside Kafka Mirrormaker 2|https://blog.cloudera.com/a-look-inside-kafka-mirrormaker-2/] stated ""this feature will be coming soon in MM2’s next iteration.""","02/Nov/20 13:05;saritago;Hi [~ryannedolan]

We are trying to set up MM2 with connection distributed setup. 

Worker configs are as follows

 

 

 
{quote}quote

bootstrap.servers=abc-broker:9093
 security.protocol=SASL_SSL
 sasl.mechanism=PLAIN
 sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=""superuser"" password=""superuser_password"";
 group.id=connect-tails
 key.converter=org.apache.kafka.connect.json.JsonConverter
 value.converter=org.apache.kafka.connect.json.JsonConverter
 key.converter.schemas.enable=false
 value.converter.schemas.enable=false
 internal.key.converter=org.apache.kafka.connect.json.JsonConverter
 internal.value.converter=org.apache.kafka.connect.json.JsonConverter
 internal.key.converter.schemas.enable=false
 internal.value.converter.schemas.enable=false
 offset.storage.topic=connect-offsets-test
 config.storage.topic=connect-configs-test
 status.storage.topic=connect-status-test
 offset.flush.interval.ms=300000
 producer.buffer.memory=1234
 producer.ssl.truststore.password=truststore_password
 producer.ssl.truststore.location=/opt/projects/confluent/wildcard.kafka.iggroup.local.jks
 producer.ssl.keystore.password=keystore_password
 producer.ssl.keystore.location=/opt/projects/confluent/wildcard.kafka.iggroup.local.jks

 

 

quote
{quote}
 

 

 

When i run below command, I can see worker connector created:

 

 
{quote}{{}}}}
 \{{ ```nohup sh connect-distributed ../etc/kafka/connect-distributed-1.properties &``` }}

 

{{
{quote}
 

When I try to create a connector using POST call, I start seeing message as 

 

 
{quote}{{}}}}

```WARN [Producer clientId=connector-producer-MM9-0] Bootstrap broker abc-broker:9093 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient:1037)``` 

 

{{
{quote}
 

json content for POST call is as below

 

 
{quote}{{  ""name"": ""MM9"",
 ""config"": { ""connector.class"": ""org.apache.kafka.connect.mirror.MirrorSourceConnector"", 
""tasks.max"": 3,
 ""topics"": ""messaging_ops_mm8"",
 ""errors.log.enable"": true,
 ""errors.log.include.messages"": true,
 ""key.converter"": ""org.apache.kafka.connect.converters.ByteArrayConverter"", ""value.converter"": ""org.apache.kafka.connect.converters.ByteArrayConverter"", ""clusters"": ""peach, tails"",
 ""source.cluster.alias"": ""peach"", 
""target.cluster.alias"": ""tails"", 
""source.cluster.bootstrap.servers"": ""xyz-broker:9093"", ""source.cluster.producer.bootstrap.servers"": ""xyz-broker:9093"",
 ""sync.topic.acls.enabled"": ""false"",
 ""emit.checkpoints.interval.seconds"": ""1"",
 ""groups"": ""consumer-group-.*"",
 ""source.cluster.admin.bootstrap.servers"": ""xyz-broker:9093"", ""source.cluster.consumer.bootstrap.servers"": ""xyz-broker:9093"", ""source.cluster.security.protocol"": ""SASL_SSL"", 
""source.cluster.sasl.mechanism"": ""PLAIN"", 
""source.cluster.sasl.jaas.config"": ""org.apache.kafka.common.security.plain.PlainLoginModule required username='superuser' password='superuser-password';"",
 ""source.cluster.ssl.truststore.password"" : ""truststorepassword"",        ""source.cluster.ssl.truststore.location"" : ""/opt/projects/confluent/wildcard.kafka.iggroup.local.jks"",        ""source.cluster.ssl.keystore.password"" : ""keystorepassword"",
  ""source.cluster.ssl.keystore.location"" : ""/opt/projects/confluent/wildcard.kafka.iggroup.local.jks"", 
""target.cluster.bootstrap.servers"": ""abc-broker:9093"", ""target.cluster.producer.bootstrap.servers"": ""abc-broker:9093"", 
""enabled"": ""true"",  
""target.cluster.admin.bootstrap.servers"": ""abc-broker:9093"", ""target.cluster.consumer.bootstrap.servers"": ""abc-broker:9093"", 
""name"": ""MirrorSourceConnector"", 
""emit.heartbeats.interval.seconds"": ""1"",
 ""target.cluster.security.protocol"": ""SASL_SSL"",
 ""target.cluster.sasl.mechanism"": ""PLAIN"", 
""target.cluster.sasl.jaas.config"": ""org.apache.kafka.common.security.plain.PlainLoginModule required username='superuser' password='superuser-password';"", ""target.cluster.ssl.truststore.password"" : ""truststorepassword"",        ""target.cluster.ssl.truststore.location"" : ""/opt/projects/confluent/wildcard.kafka.iggroup.local.jks"",        ""target.cluster.ssl.keystore.password"" : ""keystore-password"",        ""target.cluster.ssl.keystore.location"" : ""/opt/projects/confluent/wildcard.kafka.iggroup.local.jks"" }}
{quote}
 

Note the bootstrap-server added in connect-distributed.properties file and the target cluster bootstrap-server are same(abc-broker).

I have added all the SASL credentials, did a swap of source and destination cluster in the json but I continue to get this broker disconnected WARNING. 

We are on kafka version 0.11.0.3. 

What is it that we are missing?","05/Nov/20 06:09;saritago;Was able to fix the bootstrap-server disconnect issue. Went through all logs and noticed that the sasl mechanism being used by producer/consumer/adminclinet/mirrormakerconnectors was not same. Had to manually set sasl mechanism for each of these configs to fix the issue. Below are the details of worker configs and connector configs.




Worker configs are as follows:

```
bootstrap.servers=abc-broker-1:9093
security.protocol=SASL_SSL
sasl.mechanism=PLAIN
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=""abc-broker-superuser"" password=""abc-broker-superuser-password"";

producer.security.protocol=SASL_SSL
producer.sasl.mechanism=PLAIN
producer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=""abc-broker-superuser"" password=""abc-broker-superuser-password"";


group.id=connect-tails

key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter

key.converter.schemas.enable=false
value.converter.schemas.enable=false


internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter
internal.key.converter.schemas.enable=false
internal.value.converter.schemas.enable=false

offset.storage.topic=connect-offsets-test


config.storage.topic=connect-configs-test

status.storage.topic=connect-status-test


producer.ssl.truststore.password=truststore_password
producer.ssl.truststore.location=/opt/projects/confluent/wildcard.kafka.iggroup.local.jks
producer.ssl.keystore.password=keystore_password
producer.ssl.keystore.location=/opt/projects/confluent/wildcard.kafka.iggroup.local.jks

```

 

 

 

 

Connector configs:
```
{
 ""name"": ""MM9"",
 ""config"": {
 ""connector.class"": ""org.apache.kafka.connect.mirror.MirrorSourceConnector"",
 ""producer.sasl.jaas.config"": ""org.apache.kafka.common.security.plain.PlainLoginModule required username='xyz-broker-superuser' password='xyz-broker-superuser-password';"",
 ""errors.log.include.messages"": ""true"",
 ""target.cluster.sasl.jaas.config"": ""org.apache.kafka.common.security.plain.PlainLoginModule required username='abc-broker-superuser' password='abc-broker-superuser-password';"",
 ""sync.topic.acls.enabled"": ""false"",
 ""tasks.max"": ""3"",
 ""source.cluster.producer.security.protocol"": ""SASL_SSL"",
 ""emit.checkpoints.interval.seconds"": ""1"",
 ""source.cluster.alias"": ""xyz-broker"",
 ""target.cluster.producer.sasl.jaas.config"": ""org.apache.kafka.common.security.plain.PlainLoginModule required username='abc-broker-superuser' password='abc-broker-superuser-password';"",
 ""source.cluster.producer.sasl.mechanism"": ""PLAIN"",
 ""target.cluster.producer.bootstrap.servers"": ""abc-broker-3:9093,abc-broker-2:9093"",
 ""enabled"": ""true"",
 ""target.cluster.admin.bootstrap.servers"": ""abc-broker-3:9093,abc-broker-2:9093"",
 ""target.cluster.producer.security.protocol"": ""SASL_SSL"",
 ""target.cluster.security.protocol"": ""SASL_SSL"",
 ""target.cluster.consumer.sasl.mechanism"": ""PLAIN"",
 ""value.converter"": ""org.apache.kafka.connect.converters.ByteArrayConverter"",
 ""errors.log.enable"": ""true"",
 ""source.cluster.admin.bootstrap.servers"": ""xyz-broker-1:9093,xyz-broker-2:9093"",
 ""key.converter"": ""org.apache.kafka.connect.converters.ByteArrayConverter"",
 ""clusters"": ""xyz-broker, abc-broker"",
 ""source.cluster.producer.bootstrap.servers"": ""xyz-broker-1:9093,xyz-broker-2:9093"",
 ""target.cluster.consumer.sasl.jaas.config"": ""org.apache.kafka.common.security.plain.PlainLoginModule required username='abc-broker-superuser' password='abc-broker-superuser-password';"",
 ""producer.security.protocol"": ""SASL_SSL"",
 ""topics"": ""messaging_ops_mm8"",
 ""source.cluster.producer.sasl.jaas.config"": ""org.apache.kafka.common.security.plain.PlainLoginModule required username='xyz-broker-superuser' password='xyz-broker-superuser-password';"",
 ""target.cluster.sasl.mechanism"": ""PLAIN"",
 ""source.cluster.consumer.security.protocol"": ""SASL_SSL"",
 ""groups"": ""consumer-group-.*"",
 ""source.cluster.consumer.sasl.mechanism"": ""PLAIN"",
 ""source.cluster.sasl.jaas.config"": ""org.apache.kafka.common.security.plain.PlainLoginModule required username='xyz-broker-superuser' password='xyz-broker-superuser-password';"",
 ""source.cluster.bootstrap.servers"": ""xyz-broker-1:9093,xyz-broker-2:9093"",
 ""source.cluster.sasl.mechanism"": ""PLAIN"",
 ""producer.sasl.mechanism"": ""PLAIN"",
 ""target.cluster.alias"": ""abc-broker"",
 ""target.cluster.consumer.security.protocol"": ""SASL_SSL"",
 ""task.class"": ""org.apache.kafka.connect.mirror.MirrorSourceTask"",
 ""target.cluster.consumer.bootstrap.servers"": ""abc-broker-3:9093,abc-broker-2:9093"",
 ""name"": ""MM9"",
 ""target.cluster.bootstrap.servers"": ""abc-broker-3:9093,abc-broker-2:9093"",
 ""emit.heartbeats.interval.seconds"": ""1"",
 ""task.assigned.partitions"": ""heartbeats-0"",
 ""source.cluster.security.protocol"": ""SASL_SSL"",
 ""target.cluster.producer.sasl.mechanism"": ""PLAIN"",
 ""source.cluster.consumer.bootstrap.servers"": ""xyz-broker-1:9093,xyz-broker-2:9093""
 }
 }
```",,,,,,,,,,,,,,,,,,
Add a Hash SMT transformer,KAFKA-10299,13318697,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,brbrown35,brbrown35,22/Jul/20 12:10,03/Nov/20 16:41,12/Jan/21 10:06,,,,,,,,,,,,KafkaConnect,,,,,,0,,,,,"A previous contribution to [https://github.com/aiven/aiven-kafka-connect-transforms] was suggested as by a member of confluent as being a nice addition to the out of the box Kafka Connect SMTs. The discussion is here [https://github.com/aiven/aiven-kafka-connect-transforms/issues/9#issuecomment-662378057]

This change would add a new SMT which allows for either a hashing a key or a value using the configured hashing function.",,brbrown35,ivanyu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-07-22 12:10:49.0,,,,,,,"0|z0h35c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"WorkerSinkTask: IllegalStateException cased by consumer.seek(tp, offsets) when (tp, offsets) are supplied by WorkerSinkTaskContext",KAFKA-10370,13321423,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Not A Bug,yangguo1220,yangguo1220,yangguo1220,07/Aug/20 02:03,27/Oct/20 20:45,12/Jan/21 10:06,21/Aug/20 14:53,2.5.0,,,,,,,,,,KafkaConnect,,,,,,0,,,,,"In [WorkerSinkTask.java|https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java], when we want the consumer to consume from certain offsets, rather than from the last committed offset, [WorkerSinkTaskContext|https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTaskContext.java#L63-L66] provided a way to supply the offsets from external (e.g. implementation of SinkTask) to rewind the consumer. 

In the [poll() method|https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L312], it first call [rewind()|https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L615-L633] to (1) read the offsets from WorkerSinkTaskContext, if the offsets are not empty, (2) consumer.seek(tp, offset) to rewind the consumer.

As a part of [WorkerSinkTask initialization|https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L290-L307], when the [SinkTask starts|https://github.com/apache/kafka/blob/trunk/connect/api/src/main/java/org/apache/kafka/connect/sink/SinkTask.java#L83-L88], we can supply the specific offsets by +""context.offset(supplied_offsets);+"" in start() method, so that when the consumer does the first poll, it should rewind to the specific offsets in rewind() method. However in practice, we saw the following IllegalStateException when running consumer.seek(tp, offsets);

{code:java}
[2020-08-07 23:53:55,752] INFO WorkerSinkTask{id=MirrorSinkConnector-0} Rewind test-1 to offset 3 (org.apache.kafka.connect.runtime.WorkerSinkTask:648)
[2020-08-07 23:53:55,752] INFO [Consumer clientId=connector-consumer-MirrorSinkConnector-0, groupId=connect-MirrorSinkConnector] Seeking to offset 3 for partition test-1 (org.apache.kafka.clients.consumer.KafkaConsumer:1592)
[2020-08-07 23:53:55,752] ERROR WorkerSinkTask{id=MirrorSinkConnector-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:187)
java.lang.IllegalStateException: No current assignment for partition test-1
        at org.apache.kafka.clients.consumer.internals.SubscriptionState.assignedState(SubscriptionState.java:368)
        at org.apache.kafka.clients.consumer.internals.SubscriptionState.seekUnvalidated(SubscriptionState.java:385)
        at org.apache.kafka.clients.consumer.KafkaConsumer.seek(KafkaConsumer.java:1597)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.rewind(WorkerSinkTask.java:649)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:334)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:229)
        at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:198)
        at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
        at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
[2020-08-07 23:53:55,752] ERROR WorkerSinkTask{id=MirrorSinkConnector-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:188)
{code}

As suggested in https://stackoverflow.com/questions/41008610/kafkaconsumer-0-10-java-api-error-message-no-current-assignment-for-partition/41010594, the resolution (that has been initially verified) proposed in the attached PR is to use *consumer.assign* with *consumer.seek* , instead of *consumer.subscribe*, to handle the initial position of the consumer, when specific offsets are provided by external through WorkerSinkTaskContext",,ryannedolan,yangguo1220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10339,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-08-17 20:49:43.795,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 21 14:52:31 UTC 2020,,,,,,,"0|z0hjvs:",9223372036854775807,,rhauch,,,,,,,,,,,,,,"10/Aug/20 17:48;yangguo1220;Hi [~rhauch], when you have a chance, I would like to get your initial feedback / advice on this issue and proposed solution. Thanks cc [~ryannedolan]","17/Aug/20 20:49;ryannedolan;Thanks [~yangguo1220]. I believe that using assign() instead of subscribe() will have unexpected side effects, e.g. no rebalances and no auto-detection of new topics/partitions. As you know, in MirrorSourceTask, we explicitly avoided using subscribe() and instead handle rebalances and new topic-partitions explicitly, for efficiency purposes. (Rebalances were a major problem in production deployments of MM1.) But I'm not sure it would be appropriate to change the behavior of WorkerSinkTask here, especially as a side-effect of calling SinkTaskContext.offset().

IIRC, the root cause of the exception is actually that subscribe() results in partitions being assigned asynchronously, so if you subscribe() and then seek() you'll likely have zero assignments at that point. I believe the correct way to deal with this is to register a RebalanceListener, which can rewind the offsets of a partition _after_ the partition is assigned.

It may be possible for WorkerSinkTask to do this automatically. There are basically two scenarios:

- SinkTaskContext.offset() is called _after_ a partition is assigned, in which case the existing implementation should work. Unfortunately, it seems impossible for a SinkTask to know whether the partition is assigned or not. This to me seems like a bug in the API.
- SinkTaskContext.offset() is called _before_ a partition is assigned, which would result in the exception you're seeing. In this case, WorkerSinkTask could store the offsets and seek() asynchronously using a RebalanceListener. This essentially defers the seek() until _after_ the partition is actually assigned, thus avoiding the exception.

It's possible this bug only exists in the edge-case of calling WorkerSinkTask.offsets() within the SinktTask.start() method. We could possibly handle that case specially: if offsets() is called during start(), WorkerSinkTask could use the RebalanceListener to defer the seek() until the partitions are actually assigned.","18/Aug/20 14:46;yangguo1220;Hey [~ryannedolan] I updated the PR with the following code in `onPartitionsAssigned`. From my initial testing, it works well with `MirrorSinkTask`. Since the partition assignment is now driven by Consumer (as we use `consumer.subscribe()`), the `offsets` that passed into `context.offsets(offsets)` is all <topic,partition> associated with consumer group, rather than letting `MirrorSinkTask` to do the consuming task assignment.

Appreciate for your above thoughts and definitely expecting more feedback!

https://github.com/apache/kafka/pull/9145/files#diff-9d27e74bcdc892150367aed9a4cf499eR617-R698","21/Aug/20 14:52;yangguo1220;After looking at HDFS Sink connector https://github.com/confluentinc/kafka-connect-hdfs/blob/master/src/main/java/io/confluent/connect/hdfs/HdfsSinkTask.java, it seems that the correct resolution is to add the following method to the SinkTask implementation.

   @Override
    public void open(Collection<TopicPartition> partitions) {
    	    loadContextOffsets();
    }

* loadContextOffsets() is the method to load consumer group offsets from target cluster and put them into context.offsets()

Therefore, no change is needed and I am closing this ticket",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Re-design KStream.process() and K*.transform*() operations,KAFKA-10603,13335112,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,vvcephei,vvcephei,13/Oct/20 03:12,22/Oct/20 04:03,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,needs-kip,,,,"After the implementation of KIP-478, we have the ability to reconsider all these APIs, and maybe just replace them with
{code:java}
// KStream
KStream<KOut, VOut> process(ProcessorSupplier<KIn, VIn, KOut, VOut>) 

// KTable
KTable<KOut, VOut> process(ProcessorSupplier<KIn, VIn, KOut, VOut>){code}
 

but it needs more thought and a KIP for sure.

 

This ticket probably supercedes https://issues.apache.org/jira/browse/KAFKA-8396",,mjsax,sagarrao,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-10536,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-10-22 04:03:52.978,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 22 04:03:52 UTC 2020,,,,,,,"0|z0jnag:",9223372036854775807,,,,,,,,,,,,,,,,"22/Oct/20 04:03;sagarrao;Hey [~vvcephei], I know this task requires a understanding of the new changes in the KIP.. Just curious, is it something that I can pick up?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorMaker2 Exactly-once Semantics,KAFKA-10339,13320712,New Feature,Patch Available,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,yangguo1220,yangguo1220,yangguo1220,03/Aug/20 17:40,18/Oct/20 00:54,12/Jan/21 10:06,,,,,,,,,,,,KafkaConnect,mirrormaker,,,,,0,needs-kip,,,,"MirrorMaker2 is currently implemented on Kafka Connect Framework, more specifically the Source Connector / Task, which do not provide exactly-once semantics (EOS) out-of-the-box, as discussed in https://github.com/confluentinc/kafka-connect-jdbc/issues/461,  https://github.com/apache/kafka/pull/5553, https://issues.apache.org/jira/browse/KAFKA-6080  and https://issues.apache.org/jira/browse/KAFKA-3821. Therefore MirrorMaker2 currently does not provide EOS.",,ivanyu,jitabc,mimaison,ryannedolan,yangguo1220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-08-03 19:29:51.051,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 18 00:54:01 UTC 2020,,,,,,,"0|z0hfk8:",9223372036854775807,,,,,,,,,,,,,,,,"03/Aug/20 17:43;yangguo1220;[~ryannedolan] [~mimaison] Here is the KIP I would like to get your thoughts on, thanks so much for any feedback or input
https://cwiki.apache.org/confluence/display/KAFKA/KIP-653%3A+MirrorMaker2+Exactly-once+Semantics","03/Aug/20 19:29;ryannedolan;[~yangguo1220] this is awesome, thanks! My team has looked into this multiple times, and we came to the same conclusion around WorkerSourceTask requiring a lot of changes to support transactions. I believe it would be easier to start from scratch with a new SourceWorker than to adapt it to support transactions, and we'd probably need to deprecate a number of APIs in the process. I wouldn't rule it out, but it would be difficult.

Love how your KIP ""kills two birds with one stone"" -- we get a MirrorSinkConnector _and_ EOS.

","03/Aug/20 20:22;yangguo1220;Thanks [~ryannedolan] for your quick feedback :) if MirrorSinkConnector and transactional producer in MirrorSinkTask sounds the right path for you initially, I will spend more times this week on refining the above KIP, meanwhile waiting for the initial feedback from [~mimaison]. 

Also the migration may be interesting if MirrorSinkConnector is the right path to go.","03/Aug/20 20:56;ryannedolan;Yeah, I guess it would be possible to switch from MirrorSourceConnector to MirrorSinkConnector in the connect-mirror-maker ""driver"", but external tooling would notice (e.g. JMX metrics would change), so we'd need to put that behind a flag or something so users could opt-in to the SinkConnector in order to get EOS. But even without changing the ""driver', an EOS MirrorSinkConnector would be very useful to many organizations that run MM2 on existing Connect clusters.","05/Aug/20 03:25;yangguo1220;Technically, transaction can not be done across clusters, so the above KIP is not valid. Need to look for other alternatives","05/Aug/20 14:00;ryannedolan;I think what you mean is that a read-process-write loop cannot span clusters, since a transaction coordinator on one cluster cannot commit offsets on another cluster. But I don't think we actually need that -- we can just store offsets on the target cluster instead.

I think what we need is something along these lines:

- we manage offsets ourselves -- we don't rely on Connect's internal offsets tracking or __consumer_offsets on the source cluster.
- we only write to the target cluster.
- offsets are stored on the target cluster using a ""fake"" consumer group. I say ""fake"" because there would be no actual records being consumed by the group, just offsets being stored in __consumer_offsets topic.
- we write all records in a transaction, just as the KIP currently describes.
- in addition, we call addOffsetsToTransaction in order to commit offsets to the ""fake"" consumer group on the target cluster.
- when MirrorSourceTask starts, it loads initial offsets from __consumer_offsets on the target cluster.

Result:
- if the transaction succeeds, the __consumer_offsets topic on the target cluster is updated.
- if the transaction aborts, all data records are dropped, and the __consumer_offsets topic is not updated.
- when MirrorSourceTask starts/restarts, it resumes at the last committed offsets, as recorded in the target cluster.

Thoughts?","05/Aug/20 16:44;yangguo1220;thanks for the input. I think that sounds a working plan. Here is my follow-up thoughts

_""when MirrorSourceTask starts, it loads initial offsets from __consumer_offsets on the target cluster.""_

As the consumer is configured to pull data from source cluster, I am thinking we probably need to:
(1) add a new API (called ""Map<TopicPartition, Long> loadOffsets()"") to [SinkTask.java|https://github.com/apache/kafka/blob/trunk/connect/api/src/main/java/org/apache/kafka/connect/sink/SinkTask.java] 
(2) in MirrorSinkTask.java, implement/override loadOffsets() to supply the consumer offsets loaded from target cluster.
(3) in [WorkerSinkTask.java|https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L295], when initialize the consumer, if `task.loadOffsets()` returns non empty, use the returned offsets for the consumer in WorkerSinkTask as the starting point.

_""in addition, we call addOffsetsToTransaction in order to commit offsets to the ""fake"" consumer group on the target cluster.""_

I fully agree with the ""fake"" consumer group on the target cluster. I am thinking if ""addOffsetsToTransaction"" has been taken care by *producer.sendOffsetsToTransaction(offsetsMap, consumerGroupId);*?","05/Aug/20 17:33;ryannedolan;I believe your proposed task.loadOffsets() is taken care of by the existing SinkTaskContext.offsets() method actually. This mechanism is used in other SinkTasks where the offsets are stored in the downstream system. I think we may already have all the interfaces required to make this work.","05/Aug/20 18:22;yangguo1220;it is true that SinkTaskContext.offsets() has been taken covered. I will test this updated idea out in my local and the extra step is to create a ""fake"" consumer group on target cluster.","21/Sep/20 15:10;yangguo1220;[~mimaison] When possible, very appreciated if KAFKA-10483 and KAFKA-10304 could be reviewed first, which are will make this task easier to test and implement. If there may be other folks who are more appropriate to review, please nominate and I will contact. Thanks","21/Sep/20 16:42;mimaison;Thanks [~yangguo1220]. I've seen your review requests, they are in my open source todo list =)
I'd like to get https://github.com/apache/kafka/pull/8730 merged first. Then I'll take a look at your PRs","21/Sep/20 17:31;yangguo1220;Thanks [~mimaison], once the above PR 8730 is merged, I will rebase from it immediately and https://issues.apache.org/jira/browse/KAFKA-10304 could be prioritized. ","18/Oct/20 00:54;yangguo1220;https://github.com/apache/kafka/pull/9451",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Throttle Create Topic, Create Partition and Delete Topic Operations",KAFKA-9915,13300812,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dajac,dajac,dajac,24/Apr/20 14:07,15/Oct/20 07:59,12/Jan/21 10:06,15/Oct/20 07:59,,,,,,,,2.7.0,,,core,,,,,,2,,,,,"This tracks the completion of the KIP-599: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-599%3A+Throttle+Create+Topic%2C+Create+Partition+and+Delete+Topic+Operations]. If/when the KIP is approved by the community, we will create smaller sub-tasks to track overall progress.",,bbejeck,dajac,dhanesh,ijuma,MarkC0x,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-10-12 23:38:54.038,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 15 07:59:27 UTC 2020,,,,,,,"0|z0e1go:",9223372036854775807,,,,,,,,,,,,,,,,"12/Oct/20 23:38;ijuma;[~dajac] should this KIP be in the release plan? The Jira seems to be there, but the KIP isn't.","13/Oct/20 06:49;dajac;[~ijuma] Are you referring to [this one|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=158872629]? The KIP is listed in the `Planned KIP Content` section.","14/Oct/20 17:57;bbejeck;[~dajac] can we resolve this issue? Looks like it's completed.","14/Oct/20 18:05;ijuma;[~dajac] Sounds good, I missed it when I was looking before.","15/Oct/20 07:59;dajac;[~bbejeck] Sure. Let me resolve it. I will update the doc next week.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Raft Snapshot,KAFKA-10310,13319414,New Feature,In Progress,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,jagsancio,jagsancio,jagsancio,27/Jul/20 00:03,13/Oct/20 21:48,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,Tracking issue for KIP-630: Kafka Raft Snapshot,,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-07-27 00:03:33.0,,,,,,,"0|z0h7kg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rack aware task assignment in kafka streams,KAFKA-6642,13144613,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,,asurana,asurana,13/Mar/18 04:31,12/Oct/20 18:17,12/Jan/21 10:06,04/Apr/18 23:01,,,,,,,,,,,streams,,,,,,0,,,,,"We have rack aware replica assignment in kafka broker ([KIP-36 Rack aware replica assignment|https://cwiki.apache.org/confluence/display/KAFKA/KIP-36+Rack+aware+replica+assignment]).

This request is to have a similar feature for kafka streams applications. Standby tasks/standby replica assignment in kafka streams is currently not rack aware, and this request is to make it rack aware for better availability.",,ableegoldman,asurana,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-03-28 17:31:48.16,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 12 18:17:31 UTC 2020,,,,,,,"0|i3r7qn:",9223372036854775807,,,,,,,,,,,,,,,,"28/Mar/18 04:06;asurana;Current task assignor is sticky, and it can be made rack-aware with few changes. Where we ensure that same tasks (active & replicas) are assigned on different racks as much as possible.

Approach
 # RACK_ID can be added in StreamsConfig file, and needs to be passed while starting kafka-streams application. All the processes having same RACK_ID are considered in the same rack.
 # No changes in partition to task assignment

 

Assignment of tasks to instances:
 # We assign active tasks to the instances where same task was running as active previously.
 # Active Tasks which couldn't be assigned in first step are assigned to the instances where same task was running as standby previously
 # Active tasks that still couldn't be assigned, are assigned to instances in round-robin way starting from least-loaded instance
 # Above 3 steps are same as StickyAssignor as there is only one active task for any task_id so no extra rack aware logic is required in assigning active tasks.
 # Now we have to assign standy-task, and here we assign these to instances running in racks other than the one with its active task. If we run out of racks then we can assign standby-tasks in same rack but on different instances.
 # This makes the assignment rack-aware but more of a best effort and doesn't guarantee anything. This is because we might not have capacity left in some racks or we might have more number of replicas than number of racks etc

Note: Here we are making current StickyTaskAssignor rack-aware, but doesn't change the logic drastically.

Scenario#1
----
When RACK_ID is not passed in any of the stream instances.

In this case, assignment will happen as it's happening currently by StickyTaskAssignor. For all the instances for whom RACK_ID is not passed are considered to be part of single default-rack.

 

Scenario#2
----
When RACK_ID is passed in all the stream instances.

In this case, all instances belong to one or the other rack, and assignment is rack-aware as per above approach.

 

Scenario#3
----
When RACK_ID is passed in some stream instances but not in all.

In this case, all the instances with RACK_ID will belong to the provided racks. All the instances for whom RACK_ID were not passed, will be considered to be part of single default-rack.

 

Please let us know what you guys think about approach.","28/Mar/18 17:31;guozhang;[~asurana] Thanks for creating this ticket, I was not aware that it has been created some time ago. There is another ticket created recently as https://issues.apache.org/jira/browse/KAFKA-6718. Could you take a look into the proposed PR https://github.com/apache/kafka/pull/4785  and see if that aligns with your proposals, and if you could collaborate with [~_deepakgoyal] on that PR?","30/Mar/18 10:08;asurana;Hi Wang,

Sure, I am working with Deepak on this. PR is very similar to my proposal.","04/Apr/18 23:01;mjsax;I am closing this a duplicate of KAFKA-6718 -- let us know if you disagree.","12/Oct/20 03:45;asurana;Since the other ticket is abandoned, should I create KIP for this?","12/Oct/20 18:17;mjsax;This ticket is close as duplicate already. If you want to work on rack awareness, you should assign KAFKA-6718 to yourself.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Consumer Record Latency Metric,KAFKA-8656,13244411,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,seglo,seglo,seglo,11/Jul/19 16:24,12/Oct/20 14:08,12/Jan/21 10:06,,,,,,,,,,,,metrics,,,,,,3,,,,,"Consumer lag is a useful metric to monitor how many records are queued to be processed.  We can look at individual lag per partition or we may aggregate metrics. For example, we may want to monitor what the maximum lag of any particular partition in our consumer subscription so we can identify hot partitions, caused by an insufficient producing partitioning strategy.  We may want to monitor a sum of lag across all partitions so we have a sense as to our total backlog of messages to consume. Lag in offsets is useful when you have a good understanding of your messages and processing characteristics, but it doesn’t tell us how far behind _in time_ we are.  This is known as wait time in queueing theory, or more informally it’s referred to as latency.

The latency of a message can be defined as the difference between when that message was first produced to when the message is received by a consumer.  The latency of records in a partition correlates with lag, but a larger lag doesn’t necessarily mean a larger latency. For example, a topic consumed by two separate application consumer groups A and B may have similar lag, but different latency per partition.  Application A is a consumer which performs CPU intensive business logic on each message it receives. It’s distributed across many consumer group members to handle the load quickly enough, but since its processing time is slower, it takes longer to process each message per partition.  Meanwhile, Application B is a consumer which performs a simple ETL operation to land streaming data in another system, such as HDFS. It may have similar lag to Application A, but because it has a faster processing time its latency per partition is significantly less.

If the Kafka Consumer reported a latency metric it would be easier to build Service Level Agreements (SLAs) based on non-functional requirements of the streaming system.  For example, the system must never have a latency of greater than 10 minutes. This SLA could be used in monitoring alerts or as input to automatic scaling solutions.

[KIP-488|https://cwiki.apache.org/confluence/display/KAFKA/488%3A+Kafka+Consumer+Record+Latency+Metric]

 ",,clement@unportant.info,ikar43,jon@cybus.co.uk,seglo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-10-12 14:08:02.612,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 12 14:08:02 UTC 2020,,,,,,,"0|z04kyg:",9223372036854775807,,,,,,,,,,,,,,,,"12/Oct/20 14:08;jon@cybus.co.uk;KIP link should be [https://cwiki.apache.org/confluence/display/KAFKA/489%3A+Kafka+Consumer+Record+Latency+Metric] (i.e. 489 instead of 488)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Tiered Storage,KAFKA-7739,13204565,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,sriharsha,sriharsha,sriharsha,14/Dec/18 19:51,12/Oct/20 06:31,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,15,,,,,More detais are in the KIP https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage,,adupriez,allenxwang,almaz,anoop.hbase,Avinash_Bhat,badam,barakm,cameronbraid,dhruvilshah,domsj,f.pompermaier,githubbot,gumartinm,halkazzar,iksaif,jghoman,leyncl,lkokhreidze,mattramsey00@gmail.com,mpcdt,Munamala,rchartier,rodarvus,SaryeHaddadi,satish.duggana,sharp-pixel,SmithJohnTaylor,sriharsha,stevenz3wu,Suman268,tombentley,tsindotg@gmail.com,vale68,vlad2017,yupeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7374,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-10-19 17:55:51.517,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 19 17:55:51 UTC 2019,,,,,,,"0|s01jiw:",9223372036854775807,,,,,,,,,,,,,,,,"19/Oct/19 17:55;githubbot;satishd commented on pull request #7561: [WIP] KAFKA-7739: Tiered storage
URL: https://github.com/apache/kafka/pull/7561
 
 
   [WIP] This is the initial **draft** version of the KIP-405. It includes the initial set of changes required for plugging in a RemoteStorageManager. We will update the KIP and this PR in the next few days with more details.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support PEM format for SSL certificates and private key,KAFKA-10338,13320634,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,03/Aug/20 10:10,07/Oct/20 17:34,12/Jan/21 10:06,06/Oct/20 18:14,,,,,,,,2.7.0,,,security,,,,,,3,,,,,"We currently support only file-based JKS/PKCS12 format for SSL key stores and trust stores. It will be good to add support for PEM as configuration values that fits better with config externalization.

KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-651+-+Support+PEM+format+for+SSL+certificates+and+private+key",,aakashgupta96,lulf,psmolinski,rsivaram,tbnguyen1407,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-08-03 10:10:18.0,,,,,,,"0|z0hf2w:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ability to filter events at Kafka broker based on Kafka header value,KAFKA-10581,13334182,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,bhukailas448@gmail.com,bhukailas448@gmail.com,07/Oct/20 13:05,07/Oct/20 13:05,12/Jan/21 10:06,,,,,,,,,,,,clients,consumer,,,,,0,,,,,Provide an ability to filter kafka message events at Kafka broker based on consumer's interest,,bhukailas448@gmail.com,ivanyu,psmolinski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-10-07 13:05:35.0,,,,,,,"0|z0jhk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement KIP-649: Dynamic Client Configuration,KAFKA-10325,13320071,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,rdielhenn,rdielhenn,29/Jul/20 22:14,02/Oct/20 19:03,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,Implement KIP-649: Dynamic Client Configuration,,rdielhenn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-07-29 22:14:54.0,,,,,,,"0|z0hbm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Choose ip address while producing messages to kafka broker.,KAFKA-10567,13330548,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,shri_22ram,shri_22ram,02/Oct/20 11:53,02/Oct/20 11:55,12/Jan/21 10:06,,2.5.0,,,,,,,,,,producer ,,,,,,0,,,,,"I am using camel KAFKA inside Apache KARAF and producing messages to KAFKA server. I have multiple interfaces configured in my system and sending(producing) messages will happen only by using the first interface **managmentserver1_local_interface** always. Same thing i tried with camel-ftp in which i can choose the network interface by using the name ""**managmentserver1_traffic_interface**"" and binding through the **bindAddress** api which is introduced after 2.23. Is there any way to configure the camel kafka component to select network nterfaces? Currently my machine has 2 network interfaces.

127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
 192.198.10.1 testserver1 managementserver1_local_interface
 192.199.11.2 testserver1 managementserver1_traffic_interface


Gone through the source code of kafka and found that internally it is creating a socket which is used for transferring which will provide the option to bind source address.

private void configureSocketChannel(SocketChannel socketChannel, int sendBufferSize, int receiveBufferSize)
 throws IOException {
 socketChannel.configureBlocking(false);
 Socket socket = socketChannel.socket();
 socket.setKeepAlive(true);
 if (sendBufferSize != Selectable.USE_DEFAULT_BUFFER_SIZE)
 socket.setSendBufferSize(sendBufferSize);
 if (receiveBufferSize != Selectable.USE_DEFAULT_BUFFER_SIZE)
 socket.setReceiveBufferSize(receiveBufferSize);
 socket.setTcpNoDelay(true);
 }

Note: There is an option to choose the network interface in camel-ftp through an attribute bindAddress which allows us to bind the source addressees(in case of multiple interfaces). We are using camel 2.23. Because we are planning to handle the traffic only through traffic interface and oam activities through only management server local interface.

 

The reason i am posting here is Camel is packaging the apache kafka clients with their package. So please don't take this as a camel post.

 

Please find the SO link

https://stackoverflow.com/questions/63898065/choose-source-ip-address-while-sending-messages-in-camel-kafka",,shri_22ram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 02 11:55:48 UTC 2020,,,,,,,"0|z0j42w:",9223372036854775807,,,,,,,,,,,,,,,,"02/Oct/20 11:55;shri_22ram;Gone through the API's which doesn't have any api to bind the source address. Raising this because we have multiple interfaces. i can manage with static routes in this case but in production we are not allowed to access the system unless there is a problem. Also it will affect the other oam scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extend Admin API to support dynamic application log levels,KAFKA-7800,13208418,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,enether,enether,enether,08/Jan/19 22:25,24/Sep/20 16:20,12/Jan/21 10:06,07/Oct/19 09:18,,,,,,,,2.4.0,,,,,,,,,0,,,,,"[https://cwiki.apache.org/confluence/display/KAFKA/KIP-412%3A+Extend+Admin+API+to+support+dynamic+application+log+levels]



Logging is a critical part of any system's infrastructure. It is the most direct way of observing what is happening with a system. In the case of issues, it helps us diagnose the problem quickly which in turn helps lower the [MTTR|http://enterprisedevops.org/article/devops-metric-mean-time-to-recovery-mttr-definition-and-reasoning].

Kafka supports application logging via the log4j library and outputs messages in various log levels (TRACE, DEBUG, INFO, WARN, ERROR). Log4j is a rich library that supports fine-grained logging configurations (e.g use INFO-level logging in {{kafka.server.ReplicaManager}} and use DEBUG-level in {{kafka.server.KafkaApis}}).
This is statically configurable through the [log4j.properties|https://github.com/apache/kafka/blob/trunk/config/log4j.properties] file which gets read once at broker start-up.

A problem with this static configuration is that we cannot alter the log levels when a problem arises. It is severely impractical to edit a properties file and restart all brokers in order to gain visibility of a problem taking place in production.
It would be very useful if we support dynamically altering the log levels at runtime without needing to restart the Kafka process.

Log4j itself supports dynamically altering the log levels in a programmatic way and Kafka exposes a [JMX API|https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/utils/Log4jController.scala] that lets you alter them. This allows users to change the log levels via a GUI (jconsole) or a CLI (jmxterm) that uses JMX.

There is one problem with changing log levels through JMX that we hope to address and that is *Ease of Use*:
 * Establishing a connection - Connecting to a remote process via JMX requires configuring and exposing multiple JMX ports to the outside world. This is a burden on users, as most production deployments may stand behind layers of firewalls and have policies against opening ports. This makes opening the ports and connections in the middle of an incident even more burdensome
 * Security - JMX and tools around it support authentication and authorization but it is an additional hassle to set up credentials for another system.

 * Manual process - Changing the whole cluster's log level requires manually connecting to each broker. In big deployments, this is severely impractical and forces users to build tooling around it.

h4. Proposition

Ideally, Kafka would support dynamically changing log levels and address all of the aforementioned concerns out of the box.
We propose extending the IncrementalAlterConfig/DescribeConfig Admin API with functionality for dynamically altering the broker's log level.
This approach would also pave the way for even finer-grained logging logic (e.g log DEBUG level only for a certain topic) and would allow us to leverage the existing *AlterConfigPolicy* for custom user-defined validation of log-level changes.
These log-level changes will be *temporary* and reverted on broker restart - we will not persist them anywhere.",,enether,githubbot,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-08-02 18:54:11.506,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 08 13:49:13 UTC 2019,,,,,,,"0|u00n9c:",9223372036854775807,,,,,,,,,,,,,,,,"12/Jun/19 12:26;enether;For some reason, the Github bot did not catch the opened PR. Here it is - [https://github.com/apache/kafka/pull/6903]","02/Aug/19 18:54;githubbot;gwenshap commented on pull request #6903: KAFKA-7800: Dynamic log levels admin API (KIP-412)
URL: https://github.com/apache/kafka/pull/6903
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","08/Aug/19 13:49;githubbot;stanislavkozlovski commented on pull request #7180: MINOR: Fix flaky tests introduced by KAFKA-7800
URL: https://github.com/apache/kafka/pull/7180
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add audit of Kafka cluster,KAFKA-9413,13279044,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,viktorsomogyi,ledostuff,ledostuff,13/Jan/20 11:14,07/Sep/20 10:18,12/Jan/21 10:06,,2.4.0,,,,,,,,,,core,logging,,,,,4,,,,,"Sometimes it could be necessary to know who add/edit/delete some cluster resource. E.g. create topic, add ACL or delete its. I suggest adding default implementation of this functionality which send audit records into separate log file. And other users could implement another logic(by implementing common API in plugins), e.g. send audit events to another cluster, or database or anywhere else by changing some property's default value.",,alex.dunayevsky,ivanyu,kindkid,ledostuff,RSBiryukov,Umka7789,vvberunenko,xjecht,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-01-13 11:14:55.0,,,,,,,"0|z0afxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-554: Add Broker-side SCRAM Config API,KAFKA-10259,13315974,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rndgstn,rndgstn,rndgstn,09/Jul/20 18:48,04/Sep/20 20:13,12/Jan/21 10:06,04/Sep/20 20:13,,,,,,,,2.7.0,,,,,,,,,0,,,,,,,rndgstn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-07-09 18:48:59.0,,,,,,,"0|z0gmd4:",9223372036854775807,,cmccabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Externalize Secrets for Kafka Connect Configurations,KAFKA-6886,13158008,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rayokota,rayokota,rayokota,08/May/18 20:08,30/Aug/20 16:49,12/Jan/21 10:06,30/May/18 21:43,,,,,,,,2.0.0,,,KafkaConnect,,,,,,0,,,,,"Kafka Connect's connector configurations have plaintext passwords, and Connect stores these in cleartext either on the filesystem (for standalone mode) or in internal topics (for distributed mode). 

Connect should not store or transmit cleartext passwords in connector configurations. Secrets in stored connector configurations should be allowed to be replaced with references to values stored in external secret management systems. Connect should provide an extension point for adding customized integrations, as well as provide a file-based extension as an example. Second, a Connect runtime should be allowed to be configured to use one or more of these extensions, and allow connector configurations to use placeholders that will be resolved by the runtime before passing the complete connector configurations to connectors. This will allow existing connectors to not see any difference in the configurations that Connect provides to them at startup. And third, Connect's API should be changed to allow a connector to obtain the latest connector configuration at any time.

",,ewencp,githubbot,rayokota,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-23 00:06:52.408,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 30 21:45:04 UTC 2018,,,,,,,"0|i3thmf:",9223372036854775807,,,,,,,,,,,,,,,,"23/May/18 00:06;githubbot;rayokota opened a new pull request #5068: KAFKA-6886 Externalize secrets from Connect configs
URL: https://github.com/apache/kafka/pull/5068
 
 
   This commit allows secrets in Connect configs to be externalized and replaced with variable references of the form `${provider:[path:]key}`.  
   
   There are 2 main additions to `org.apache.kafka.common.config`: a `ConfigProvider` and a `ConfigTransformer`.  The `ConfigProvider` is an interface that allows key-value pairs to be provided by an external source for a given ""path"".  An a TTL can be associated with the key-value pairs returned from a ""path"".  The `ConfigTransformer` will use instances of `ConfigProvider` to replace variable references in a set of configuration values.
   
   In the Connect framework, `ConfigProvider` classes can be specified in the worker config, and then variable references can be used in the connector config.  In addition, the herder can be configured to restart connectors (or not) based on the TTL returned from a `ConfigProvider`.  The main class that performs restarts and transformations is `WorkerConfigTransformer`.  
   
   Finally, a `configs()` method has been added to both `SourceTaskContext` and `SinkTaskContext`.  This allows connectors to get configs with variables replaced by the latest values from instances of `ConfigProvider`.
   
   Most of the other changes in the Connect framework are threading various objects through classes to enable the above functionality.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","30/May/18 21:43;ewencp;Issue resolved by pull request 5068
[https://github.com/apache/kafka/pull/5068]","30/May/18 21:45;githubbot;ewencp closed pull request #5068: KAFKA-6886 Externalize secrets from Connect configs
URL: https://github.com/apache/kafka/pull/5068
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/checkstyle/suppressions.xml b/checkstyle/suppressions.xml
index ba48c38cb28..5bf69b6b65f 100644
--- a/checkstyle/suppressions.xml
+++ b/checkstyle/suppressions.xml
@@ -83,7 +83,7 @@
               files=""(KafkaConfigBackingStore|RequestResponseTest|WorkerSinkTaskTest).java""/>
 
     <suppress checks=""ParameterNumber""
-              files=""WorkerSourceTask.java""/>
+              files=""(WorkerSinkTask|WorkerSourceTask).java""/>
     <suppress checks=""ParameterNumber""
               files=""WorkerCoordinator.java""/>
     <suppress checks=""ParameterNumber""
diff --git a/clients/src/main/java/org/apache/kafka/common/config/ConfigChangeCallback.java b/clients/src/main/java/org/apache/kafka/common/config/ConfigChangeCallback.java
new file mode 100644
index 00000000000..d4c9948bc93
--- /dev/null
+++ b/clients/src/main/java/org/apache/kafka/common/config/ConfigChangeCallback.java
@@ -0,0 +1,31 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.common.config;
+
+/**
+ * A callback passed to {@link ConfigProvider} for subscribing to changes.
+ */
+public interface ConfigChangeCallback {
+
+    /**
+     * Performs an action when configuration data changes.
+     *
+     * @param path the path at which the data resides
+     * @param data the configuration data
+     */
+    void onChange(String path, ConfigData data);
+}
diff --git a/clients/src/main/java/org/apache/kafka/common/config/ConfigData.java b/clients/src/main/java/org/apache/kafka/common/config/ConfigData.java
new file mode 100644
index 00000000000..2bd0ff6b06a
--- /dev/null
+++ b/clients/src/main/java/org/apache/kafka/common/config/ConfigData.java
@@ -0,0 +1,66 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.common.config;
+
+import java.util.Map;
+
+/**
+ * Configuration data from a {@link ConfigProvider}.
+ */
+public class ConfigData {
+
+    private final Map<String, String> data;
+    private final Long ttl;
+
+    /**
+     * Creates a new ConfigData with the given data and TTL (in milliseconds).
+     *
+     * @param data a Map of key-value pairs
+     * @param ttl the time-to-live of the data in milliseconds, or null if there is no TTL
+     */
+    public ConfigData(Map<String, String> data, Long ttl) {
+        this.data = data;
+        this.ttl = ttl;
+    }
+
+    /**
+     * Creates a new ConfigData with the given data.
+     *
+     * @param data a Map of key-value pairs
+     */
+    public ConfigData(Map<String, String> data) {
+        this(data, null);
+    }
+
+    /**
+     * Returns the data.
+     *
+     * @return data a Map of key-value pairs
+     */
+    public Map<String, String> data() {
+        return data;
+    }
+
+    /**
+     * Returns the TTL (in milliseconds).
+     *
+     * @return ttl the time-to-live (in milliseconds) of the data, or null if there is no TTL
+     */
+    public Long ttl() {
+        return ttl;
+    }
+}
diff --git a/clients/src/main/java/org/apache/kafka/common/config/ConfigProvider.java b/clients/src/main/java/org/apache/kafka/common/config/ConfigProvider.java
new file mode 100644
index 00000000000..7133baaebd0
--- /dev/null
+++ b/clients/src/main/java/org/apache/kafka/common/config/ConfigProvider.java
@@ -0,0 +1,78 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.common.config;
+
+import org.apache.kafka.common.Configurable;
+
+import java.io.Closeable;
+import java.util.Set;
+
+/**
+ * A provider of configuration data, which may optionally support subscriptions to configuration changes.
+ */
+public interface ConfigProvider extends Configurable, Closeable {
+
+    /**
+     * Retrieves the data at the given path.
+     *
+     * @param path the path where the data resides
+     * @return the configuration data
+     */
+    ConfigData get(String path);
+
+    /**
+     * Retrieves the data with the given keys at the given path.
+     *
+     * @param path the path where the data resides
+     * @param keys the keys whose values will be retrieved
+     * @return the configuration data
+     */
+    ConfigData get(String path, Set<String> keys);
+
+    /**
+     * Subscribes to changes for the given keys at the given path (optional operation).
+     *
+     * @param path the path where the data resides
+     * @param keys the keys whose values will be retrieved
+     * @param callback the callback to invoke upon change
+     * @throws {@link UnsupportedOperationException} if the subscribe operation is not supported
+     */
+    default void subscribe(String path, Set<String> keys, ConfigChangeCallback callback) {
+        throw new UnsupportedOperationException();
+    }
+
+    /**
+     * Unsubscribes to changes for the given keys at the given path (optional operation).
+     *
+     * @param path the path where the data resides
+     * @param keys the keys whose values will be retrieved
+     * @param callback the callback to be unsubscribed from changes
+     * @throws {@link UnsupportedOperationException} if the unsubscribe operation is not supported
+     */
+    default void unsubscribe(String path, Set<String> keys, ConfigChangeCallback callback) {
+        throw new UnsupportedOperationException();
+    }
+
+    /**
+     * Clears all subscribers (optional operation).
+     *
+     * @throws {@link UnsupportedOperationException} if the unsubscribeAll operation is not supported
+     */
+    default void unsubscribeAll() {
+        throw new UnsupportedOperationException();
+    }
+}
diff --git a/clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java b/clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java
new file mode 100644
index 00000000000..7c3c516b073
--- /dev/null
+++ b/clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java
@@ -0,0 +1,169 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.common.config;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+/**
+ * This class wraps a set of {@link ConfigProvider} instances and uses them to perform
+ * transformations.
+ *
+ * <p>The default variable pattern is of the form <code>${provider:[path:]key}</code>,
+ * where the <code>provider</code> corresponds to a {@link ConfigProvider} instance, as passed to
+ * {@link ConfigTransformer#ConfigTransformer(Map)}.  The pattern will extract a set
+ * of paths (which are optional) and keys and then pass them to {@link ConfigProvider#get(String, Set)} to obtain the
+ * values with which to replace the variables.
+ *
+ * <p>For example, if a Map consisting of an entry with a provider name ""file"" and provider instance
+ * {@link FileConfigProvider} is passed to the {@link ConfigTransformer#ConfigTransformer(Map)}, and a Properties
+ * file with contents
+ * <pre>
+ * fileKey=someValue
+ * </pre>
+ * resides at the path ""/tmp/properties.txt"", then when a configuration Map which has an entry with a key ""someKey"" and
+ * a value ""${file:/tmp/properties.txt:fileKey}"" is passed to the {@link #transform(Map)} method, then the transformed
+ * Map will have an entry with key ""someKey"" and a value ""someValue"".
+ *
+ * <p>This class only depends on {@link ConfigProvider#get(String, Set)} and does not depend on subscription support
+ * in a {@link ConfigProvider}, such as the {@link ConfigProvider#subscribe(String, Set, ConfigChangeCallback)} and
+ * {@link ConfigProvider#unsubscribe(String, Set, ConfigChangeCallback)} methods.
+ */
+public class ConfigTransformer {
+    private static final Pattern DEFAULT_PATTERN = Pattern.compile(""\\$\\{(.*?):((.*?):)?(.*?)\\}"");
+    private static final String EMPTY_PATH = """";
+
+    private final Map<String, ConfigProvider> configProviders;
+
+    /**
+     * Creates a ConfigTransformer with the default pattern, of the form <code>${provider:[path:]key}</code>.
+     *
+     * @param configProviders a Map of provider names and {@link ConfigProvider} instances.
+     */
+    public ConfigTransformer(Map<String, ConfigProvider> configProviders) {
+        this.configProviders = configProviders;
+    }
+
+    /**
+     * Transforms the given configuration data by using the {@link ConfigProvider} instances to
+     * look up values to replace the variables in the pattern.
+     *
+     * @param configs the configuration values to be transformed
+     * @return an instance of {@link ConfigTransformerResult}
+     */
+    public ConfigTransformerResult transform(Map<String, String> configs) {
+        Map<String, Map<String, Set<String>>> keysByProvider = new HashMap<>();
+        Map<String, Map<String, Map<String, String>>> lookupsByProvider = new HashMap<>();
+
+        // Collect the variables from the given configs that need transformation
+        for (Map.Entry<String, String> config : configs.entrySet()) {
+            List<ConfigVariable> vars = getVars(config.getKey(), config.getValue(), DEFAULT_PATTERN);
+            for (ConfigVariable var : vars) {
+                Map<String, Set<String>> keysByPath = keysByProvider.computeIfAbsent(var.providerName, k -> new HashMap<>());
+                Set<String> keys = keysByPath.computeIfAbsent(var.path, k -> new HashSet<>());
+                keys.add(var.variable);
+            }
+        }
+
+        // Retrieve requested variables from the ConfigProviders
+        Map<String, Long> ttls = new HashMap<>();
+        for (Map.Entry<String, Map<String, Set<String>>> entry : keysByProvider.entrySet()) {
+            String providerName = entry.getKey();
+            ConfigProvider provider = configProviders.get(providerName);
+            Map<String, Set<String>> keysByPath = entry.getValue();
+            if (provider != null && keysByPath != null) {
+                for (Map.Entry<String, Set<String>> pathWithKeys : keysByPath.entrySet()) {
+                    String path = pathWithKeys.getKey();
+                    Set<String> keys = new HashSet<>(pathWithKeys.getValue());
+                    ConfigData configData = provider.get(path, keys);
+                    Map<String, String> data = configData.data();
+                    Long ttl = configData.ttl();
+                    if (ttl != null && ttl >= 0) {
+                        ttls.put(path, ttl);
+                    }
+                    Map<String, Map<String, String>> keyValuesByPath =
+                            lookupsByProvider.computeIfAbsent(providerName, k -> new HashMap<>());
+                    keyValuesByPath.put(path, data);
+                }
+            }
+        }
+
+        // Perform the transformations by performing variable replacements
+        Map<String, String> data = new HashMap<>(configs);
+        for (Map.Entry<String, String> config : configs.entrySet()) {
+            data.put(config.getKey(), replace(lookupsByProvider, config.getValue(), DEFAULT_PATTERN));
+        }
+        return new ConfigTransformerResult(data, ttls);
+    }
+
+    private static List<ConfigVariable> getVars(String key, String value, Pattern pattern) {
+        List<ConfigVariable> configVars = new ArrayList<>();
+        Matcher matcher = pattern.matcher(value);
+        while (matcher.find()) {
+            configVars.add(new ConfigVariable(matcher));
+        }
+        return configVars;
+    }
+
+    private static String replace(Map<String, Map<String, Map<String, String>>> lookupsByProvider,
+                                  String value,
+                                  Pattern pattern) {
+        Matcher matcher = pattern.matcher(value);
+        StringBuilder builder = new StringBuilder();
+        int i = 0;
+        while (matcher.find()) {
+            ConfigVariable configVar = new ConfigVariable(matcher);
+            Map<String, Map<String, String>> lookupsByPath = lookupsByProvider.get(configVar.providerName);
+            if (lookupsByPath != null) {
+                Map<String, String> keyValues = lookupsByPath.get(configVar.path);
+                String replacement = keyValues.get(configVar.variable);
+                builder.append(value, i, matcher.start());
+                if (replacement == null) {
+                    // No replacements will be performed; just return the original value
+                    builder.append(matcher.group(0));
+                } else {
+                    builder.append(replacement);
+                }
+                i = matcher.end();
+            }
+        }
+        builder.append(value, i, value.length());
+        return builder.toString();
+    }
+
+    private static class ConfigVariable {
+        final String providerName;
+        final String path;
+        final String variable;
+
+        ConfigVariable(Matcher matcher) {
+            this.providerName = matcher.group(1);
+            this.path = matcher.group(3) != null ? matcher.group(3) : EMPTY_PATH;
+            this.variable = matcher.group(4);
+        }
+
+        public String toString() {
+            return ""("" + providerName + "":"" + (path != null ? path + "":"" : """") + variable + "")"";
+        }
+    }
+}
diff --git a/clients/src/main/java/org/apache/kafka/common/config/ConfigTransformerResult.java b/clients/src/main/java/org/apache/kafka/common/config/ConfigTransformerResult.java
new file mode 100644
index 00000000000..df7bea62f37
--- /dev/null
+++ b/clients/src/main/java/org/apache/kafka/common/config/ConfigTransformerResult.java
@@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.common.config;
+
+import java.util.Map;
+
+/**
+ * The result of a transformation from {@link ConfigTransformer}.
+ */
+public class ConfigTransformerResult {
+
+    private Map<String, Long> ttls;
+    private Map<String, String> data;
+
+    /**
+     * Creates a new ConfigTransformerResult with the given data and TTL values for a set of paths.
+     *
+     * @param data a Map of key-value pairs
+     * @param ttls a Map of path and TTL values (in milliseconds)
+     */
+    public ConfigTransformerResult(Map<String, String> data, Map<String, Long> ttls) {
+        this.data = data;
+        this.ttls = ttls;
+    }
+
+    /**
+     * Returns the transformed data, with variables replaced with corresponding values from the
+     * ConfigProvider instances if found.
+     *
+     * <p>Modifying the transformed data that is returned does not affect the {@link ConfigProvider} nor the
+     * original data that was used as the source of the transformation.
+     *
+     * @return data a Map of key-value pairs
+     */
+    public Map<String, String> data() {
+        return data;
+    }
+
+    /**
+     * Returns the TTL values (in milliseconds) returned from the ConfigProvider instances for a given set of paths.
+     *
+     * @return data a Map of path and TTL values
+     */
+    public Map<String, Long> ttls() {
+        return ttls;
+    }
+}
diff --git a/clients/src/main/java/org/apache/kafka/common/config/FileConfigProvider.java b/clients/src/main/java/org/apache/kafka/common/config/FileConfigProvider.java
new file mode 100644
index 00000000000..fefc93566f3
--- /dev/null
+++ b/clients/src/main/java/org/apache/kafka/common/config/FileConfigProvider.java
@@ -0,0 +1,101 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.common.config;
+
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.Reader;
+import java.nio.charset.StandardCharsets;
+import java.util.Enumeration;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Properties;
+import java.util.Set;
+
+/**
+ * An implementation of {@link ConfigProvider} that represents a Properties file.
+ * All property keys and values are stored as cleartext.
+ */
+public class FileConfigProvider implements ConfigProvider {
+
+    public void configure(Map<String, ?> configs) {
+    }
+
+    /**
+     * Retrieves the data at the given Properties file.
+     *
+     * @param path the file where the data resides
+     * @return the configuration data
+     */
+    public ConfigData get(String path) {
+        Map<String, String> data = new HashMap<>();
+        if (path == null || path.isEmpty()) {
+            return new ConfigData(data);
+        }
+        try (Reader reader = reader(path)) {
+            Properties properties = new Properties();
+            properties.load(reader);
+            Enumeration<Object> keys = properties.keys();
+            while (keys.hasMoreElements()) {
+                String key = keys.nextElement().toString();
+                String value = properties.getProperty(key);
+                if (value != null) {
+                    data.put(key, value);
+                }
+            }
+            return new ConfigData(data);
+        } catch (IOException e) {
+            throw new ConfigException(""Could not read properties from file "" + path);
+        }
+    }
+
+    /**
+     * Retrieves the data with the given keys at the given Properties file.
+     *
+     * @param path the file where the data resides
+     * @param keys the keys whose values will be retrieved
+     * @return the configuration data
+     */
+    public ConfigData get(String path, Set<String> keys) {
+        Map<String, String> data = new HashMap<>();
+        if (path == null || path.isEmpty()) {
+            return new ConfigData(data);
+        }
+        try (Reader reader = reader(path)) {
+            Properties properties = new Properties();
+            properties.load(reader);
+            for (String key : keys) {
+                String value = properties.getProperty(key);
+                if (value != null) {
+                    data.put(key, value);
+                }
+            }
+            return new ConfigData(data);
+        } catch (IOException e) {
+            throw new ConfigException(""Could not read properties from file "" + path);
+        }
+    }
+
+    // visible for testing
+    protected Reader reader(String path) throws IOException {
+        return new InputStreamReader(new FileInputStream(path), StandardCharsets.UTF_8);
+    }
+
+    public void close() {
+    }
+}
diff --git a/clients/src/test/java/org/apache/kafka/common/config/ConfigTransformerTest.java b/clients/src/test/java/org/apache/kafka/common/config/ConfigTransformerTest.java
new file mode 100644
index 00000000000..7bc74f36e9b
--- /dev/null
+++ b/clients/src/test/java/org/apache/kafka/common/config/ConfigTransformerTest.java
@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.common.config;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+public class ConfigTransformerTest {
+
+    public static final String MY_KEY = ""myKey"";
+    public static final String TEST_INDIRECTION = ""testIndirection"";
+    public static final String TEST_KEY = ""testKey"";
+    public static final String TEST_KEY_WITH_TTL = ""testKeyWithTTL"";
+    public static final String TEST_PATH = ""testPath"";
+    public static final String TEST_RESULT = ""testResult"";
+    public static final String TEST_RESULT_WITH_TTL = ""testResultWithTTL"";
+
+    private ConfigTransformer configTransformer;
+
+    @Before
+    public void setup() {
+        configTransformer = new ConfigTransformer(Collections.singletonMap(""test"", new TestConfigProvider()));
+    }
+
+    @Test
+    public void testReplaceVariable() throws Exception {
+        ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, ""${test:testPath:testKey}""));
+        Map<String, String> data = result.data();
+        Map<String, Long> ttls = result.ttls();
+        assertEquals(TEST_RESULT, data.get(MY_KEY));
+        assertTrue(ttls.isEmpty());
+    }
+
+    @Test
+    public void testReplaceVariableWithTTL() throws Exception {
+        ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, ""${test:testPath:testKeyWithTTL}""));
+        Map<String, String> data = result.data();
+        Map<String, Long> ttls = result.ttls();
+        assertEquals(TEST_RESULT_WITH_TTL, data.get(MY_KEY));
+        assertEquals(1L, ttls.get(TEST_PATH).longValue());
+    }
+
+    @Test
+    public void testReplaceMultipleVariablesInValue() throws Exception {
+        ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, ""hello, ${test:testPath:testKey}; goodbye, ${test:testPath:testKeyWithTTL}!!!""));
+        Map<String, String> data = result.data();
+        assertEquals(""hello, testResult; goodbye, testResultWithTTL!!!"", data.get(MY_KEY));
+    }
+
+    @Test
+    public void testNoReplacement() throws Exception {
+        ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, ""${test:testPath:missingKey}""));
+        Map<String, String> data = result.data();
+        assertEquals(""${test:testPath:missingKey}"", data.get(MY_KEY));
+    }
+
+    @Test
+    public void testSingleLevelOfIndirection() throws Exception {
+        ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, ""${test:testPath:testIndirection}""));
+        Map<String, String> data = result.data();
+        assertEquals(""${test:testPath:testResult}"", data.get(MY_KEY));
+    }
+
+    public static class TestConfigProvider implements ConfigProvider {
+
+        public void configure(Map<String, ?> configs) {
+        }
+
+        public ConfigData get(String path) {
+            return null;
+        }
+
+        public ConfigData get(String path, Set<String> keys) {
+            Map<String, String> data = new HashMap<>();
+            Long ttl = null;
+            if (path.equals(TEST_PATH)) {
+                if (keys.contains(TEST_KEY)) {
+                    data.put(TEST_KEY, TEST_RESULT);
+                }
+                if (keys.contains(TEST_KEY_WITH_TTL)) {
+                    data.put(TEST_KEY_WITH_TTL, TEST_RESULT_WITH_TTL);
+                    ttl = 1L;
+                }
+                if (keys.contains(TEST_INDIRECTION)) {
+                    data.put(TEST_INDIRECTION, ""${test:testPath:testResult}"");
+                }
+            }
+            return new ConfigData(data, ttl);
+        }
+
+        public void close() {
+        }
+    }
+
+}
diff --git a/clients/src/test/java/org/apache/kafka/common/config/FileConfigProviderTest.java b/clients/src/test/java/org/apache/kafka/common/config/FileConfigProviderTest.java
new file mode 100644
index 00000000000..9157e380456
--- /dev/null
+++ b/clients/src/test/java/org/apache/kafka/common/config/FileConfigProviderTest.java
@@ -0,0 +1,95 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.common.config;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.io.Reader;
+import java.io.StringReader;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+public class FileConfigProviderTest {
+
+    private FileConfigProvider configProvider;
+
+    @Before
+    public void setup() {
+        configProvider = new TestFileConfigProvider();
+    }
+
+    @Test
+    public void testGetAllKeysAtPath() throws Exception {
+        ConfigData configData = configProvider.get(""dummy"");
+        Map<String, String> result = new HashMap<>();
+        result.put(""testKey"", ""testResult"");
+        result.put(""testKey2"", ""testResult2"");
+        assertEquals(result, configData.data());
+        assertEquals(null, configData.ttl());
+    }
+
+    @Test
+    public void testGetOneKeyAtPath() throws Exception {
+        ConfigData configData = configProvider.get(""dummy"", Collections.singleton(""testKey""));
+        Map<String, String> result = new HashMap<>();
+        result.put(""testKey"", ""testResult"");
+        assertEquals(result, configData.data());
+        assertEquals(null, configData.ttl());
+    }
+
+    @Test
+    public void testEmptyPath() throws Exception {
+        ConfigData configData = configProvider.get("""", Collections.singleton(""testKey""));
+        assertTrue(configData.data().isEmpty());
+        assertEquals(null, configData.ttl());
+    }
+
+    @Test
+    public void testEmptyPathWithKey() throws Exception {
+        ConfigData configData = configProvider.get("""");
+        assertTrue(configData.data().isEmpty());
+        assertEquals(null, configData.ttl());
+    }
+
+    @Test
+    public void testNullPath() throws Exception {
+        ConfigData configData = configProvider.get(null);
+        assertTrue(configData.data().isEmpty());
+        assertEquals(null, configData.ttl());
+    }
+
+    @Test
+    public void testNullPathWithKey() throws Exception {
+        ConfigData configData = configProvider.get(null, Collections.singleton(""testKey""));
+        assertTrue(configData.data().isEmpty());
+        assertEquals(null, configData.ttl());
+    }
+
+    public static class TestFileConfigProvider extends FileConfigProvider {
+
+        @Override
+        protected Reader reader(String path) throws IOException {
+            return new StringReader(""testKey=testResult\ntestKey2=testResult2"");
+        }
+    }
+}
diff --git a/connect/api/src/main/java/org/apache/kafka/connect/sink/SinkTaskContext.java b/connect/api/src/main/java/org/apache/kafka/connect/sink/SinkTaskContext.java
index 1e214be4b8c..340ef804852 100644
--- a/connect/api/src/main/java/org/apache/kafka/connect/sink/SinkTaskContext.java
+++ b/connect/api/src/main/java/org/apache/kafka/connect/sink/SinkTaskContext.java
@@ -25,6 +25,16 @@
  * Context passed to SinkTasks, allowing them to access utilities in the Kafka Connect runtime.
  */
 public interface SinkTaskContext {
+
+    /**
+     * Get the Task configuration.  This is the latest configuration and may differ from that passed on startup.
+     *
+     * For example, this method can be used to obtain the latest configuration if an external secret has changed,
+     * and the configuration is using variable references such as those compatible with
+     * {@link org.apache.kafka.common.config.ConfigTransformer}.
+     */
+    public Map<String, String> configs();
+
     /**
      * Reset the consumer offsets for the given topic partitions. SinkTasks should use this if they manage offsets
      * in the sink data store rather than using Kafka consumer offsets. For example, an HDFS connector might record
diff --git a/connect/api/src/main/java/org/apache/kafka/connect/source/SourceTaskContext.java b/connect/api/src/main/java/org/apache/kafka/connect/source/SourceTaskContext.java
index 8eec1dfb138..2e87986648f 100644
--- a/connect/api/src/main/java/org/apache/kafka/connect/source/SourceTaskContext.java
+++ b/connect/api/src/main/java/org/apache/kafka/connect/source/SourceTaskContext.java
@@ -18,11 +18,22 @@
 
 import org.apache.kafka.connect.storage.OffsetStorageReader;
 
+import java.util.Map;
+
 /**
  * SourceTaskContext is provided to SourceTasks to allow them to interact with the underlying
  * runtime.
  */
 public interface SourceTaskContext {
+    /**
+     * Get the Task configuration.  This is the latest configuration and may differ from that passed on startup.
+     *
+     * For example, this method can be used to obtain the latest configuration if an external secret has changed,
+     * and the configuration is using variable references such as those compatible with
+     * {@link org.apache.kafka.common.config.ConfigTransformer}.
+     */
+    public Map<String, String> configs();
+
     /**
      * Get the OffsetStorageReader for this SourceTask.
      */
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/cli/ConnectDistributed.java b/connect/runtime/src/main/java/org/apache/kafka/connect/cli/ConnectDistributed.java
index 54854fe4b80..f8c15de8ef4 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/cli/ConnectDistributed.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/cli/ConnectDistributed.java
@@ -21,6 +21,7 @@
 import org.apache.kafka.common.utils.Utils;
 import org.apache.kafka.connect.runtime.Connect;
 import org.apache.kafka.connect.runtime.Worker;
+import org.apache.kafka.connect.runtime.WorkerConfigTransformer;
 import org.apache.kafka.connect.runtime.WorkerInfo;
 import org.apache.kafka.connect.runtime.distributed.DistributedConfig;
 import org.apache.kafka.connect.runtime.distributed.DistributedHerder;
@@ -85,12 +86,16 @@ public static void main(String[] args) throws Exception {
             offsetBackingStore.configure(config);
 
             Worker worker = new Worker(workerId, time, plugins, config, offsetBackingStore);
+            WorkerConfigTransformer configTransformer = worker.configTransformer();
 
             Converter internalValueConverter = worker.getInternalValueConverter();
             StatusBackingStore statusBackingStore = new KafkaStatusBackingStore(time, internalValueConverter);
             statusBackingStore.configure(config);
 
-            ConfigBackingStore configBackingStore = new KafkaConfigBackingStore(internalValueConverter, config);
+            ConfigBackingStore configBackingStore = new KafkaConfigBackingStore(
+                    internalValueConverter,
+                    config,
+                    configTransformer);
 
             DistributedHerder herder = new DistributedHerder(config, time, worker,
                     kafkaClusterId, statusBackingStore, configBackingStore,
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java
index c31568664fc..b5e0ec2c07b 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/AbstractHerder.java
@@ -91,6 +91,7 @@ public AbstractHerder(Worker worker,
                           StatusBackingStore statusBackingStore,
                           ConfigBackingStore configBackingStore) {
         this.worker = worker;
+        this.worker.herder = this;
         this.workerId = workerId;
         this.kafkaClusterId = kafkaClusterId;
         this.statusBackingStore = statusBackingStore;
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java
index a8dd49a4091..c54c160d5ab 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java
@@ -38,6 +38,7 @@
 
 import static org.apache.kafka.common.config.ConfigDef.Range.atLeast;
 import static org.apache.kafka.common.config.ConfigDef.NonEmptyStringWithoutControlChars.nonEmptyStringWithoutControlChars;
+import static org.apache.kafka.common.config.ConfigDef.ValidString.in;
 
 /**
  * <p>
@@ -91,6 +92,20 @@
     private static final String TRANSFORMS_DOC = ""Aliases for the transformations to be applied to records."";
     private static final String TRANSFORMS_DISPLAY = ""Transforms"";
 
+    public static final String CONFIG_RELOAD_ACTION_CONFIG = ""config.action.reload"";
+    private static final String CONFIG_RELOAD_ACTION_DOC =
+            ""The action that Connect should take on the connector when changes in external "" +
+            ""configuration providers result in a change in the connector's configuration properties. "" +
+            ""A value of 'none' indicates that Connect will do nothing. "" +
+            ""A value of 'restart' indicates that Connect should restart/reload the connector with the "" +
+            ""updated configuration properties."" +
+            ""The restart may actually be scheduled in the future if the external configuration provider "" +
+            ""indicates that a configuration value will expire in the future."";
+
+    private static final String CONFIG_RELOAD_ACTION_DISPLAY = ""Reload Action"";
+    public static final String CONFIG_RELOAD_ACTION_NONE = Herder.ConfigReloadAction.NONE.toString();
+    public static final String CONFIG_RELOAD_ACTION_RESTART = Herder.ConfigReloadAction.RESTART.toString();
+
     private final EnrichedConnectorConfig enrichedConfig;
     private static class EnrichedConnectorConfig extends AbstractConfig {
         EnrichedConnectorConfig(ConfigDef configDef, Map<String, String> props) {
@@ -120,7 +135,10 @@ public void ensureValid(String name, Object value) {
                             throw new ConfigException(name, value, ""Duplicate alias provided."");
                         }
                     }
-                }), Importance.LOW, TRANSFORMS_DOC, TRANSFORMS_GROUP, ++orderInGroup, Width.LONG, TRANSFORMS_DISPLAY);
+                }), Importance.LOW, TRANSFORMS_DOC, TRANSFORMS_GROUP, ++orderInGroup, Width.LONG, TRANSFORMS_DISPLAY)
+                .define(CONFIG_RELOAD_ACTION_CONFIG, Type.STRING, CONFIG_RELOAD_ACTION_RESTART,
+                        in(CONFIG_RELOAD_ACTION_NONE, CONFIG_RELOAD_ACTION_RESTART), Importance.LOW,
+                        CONFIG_RELOAD_ACTION_DOC, COMMON_GROUP, ++orderInGroup, Width.MEDIUM, CONFIG_RELOAD_ACTION_DISPLAY);
     }
 
     public ConnectorConfig(Plugins plugins) {
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Herder.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Herder.java
index 855b08a8a7f..5c7cc1429aa 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Herder.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Herder.java
@@ -148,6 +148,12 @@
      */
     void restartTask(ConnectorTaskId id, Callback<Void> cb);
 
+    /**
+     * Get the configuration reload action.
+     * @param connName name of the connector
+     */
+    ConfigReloadAction connectorConfigReloadAction(final String connName);
+
     /**
      * Restart the connector.
      * @param connName name of the connector
@@ -155,6 +161,15 @@
      */
     void restartConnector(String connName, Callback<Void> cb);
 
+    /**
+     * Restart the connector.
+     * @param delayMs delay before restart
+     * @param connName name of the connector
+     * @param cb callback to invoke upon completion
+     * @returns The id of the request
+     */
+    HerderRequest restartConnector(long delayMs, String connName, Callback<Void> cb);
+
     /**
      * Pause the connector. This call will asynchronously suspend processing by the connector and all
      * of its tasks.
@@ -183,6 +198,11 @@
      */
     String kafkaClusterId();
 
+    enum ConfigReloadAction {
+        NONE,
+        RESTART
+    }
+
     class Created<T> {
         private final boolean created;
         private final T result;
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/HerderRequest.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/HerderRequest.java
new file mode 100644
index 00000000000..627da4df823
--- /dev/null
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/HerderRequest.java
@@ -0,0 +1,21 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.connect.runtime;
+
+public interface HerderRequest {
+    void cancel();
+}
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java
index c58eddfb2f7..7a72a0e7b26 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java
@@ -19,6 +19,7 @@
 import org.apache.kafka.clients.producer.KafkaProducer;
 import org.apache.kafka.clients.producer.ProducerConfig;
 import org.apache.kafka.common.MetricName;
+import org.apache.kafka.common.config.ConfigProvider;
 import org.apache.kafka.common.metrics.Sensor;
 import org.apache.kafka.common.metrics.stats.Frequencies;
 import org.apache.kafka.common.metrics.stats.Total;
@@ -30,6 +31,7 @@
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.runtime.ConnectMetrics.LiteralSupplier;
 import org.apache.kafka.connect.runtime.ConnectMetrics.MetricGroup;
+import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;
 import org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter;
 import org.apache.kafka.connect.runtime.errors.ErrorHandlingMetrics;
 import org.apache.kafka.connect.runtime.errors.ErrorReporter;
@@ -76,6 +78,7 @@
 public class Worker {
     private static final Logger log = LoggerFactory.getLogger(Worker.class);
 
+    protected Herder herder;
     private final ExecutorService executor;
     private final Time time;
     private final String workerId;
@@ -91,6 +94,7 @@
     private final ConcurrentMap<String, WorkerConnector> connectors = new ConcurrentHashMap<>();
     private final ConcurrentMap<ConnectorTaskId, WorkerTask> tasks = new ConcurrentHashMap<>();
     private SourceTaskOffsetCommitter sourceTaskOffsetCommitter;
+    private WorkerConfigTransformer workerConfigTransformer;
 
     public Worker(
             String workerId,
@@ -122,6 +126,8 @@ public Worker(
         this.offsetBackingStore = offsetBackingStore;
         this.offsetBackingStore.configure(config);
 
+        this.workerConfigTransformer = initConfigTransformer();
+
         producerProps = new HashMap<>();
         producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, Utils.join(config.getList(WorkerConfig.BOOTSTRAP_SERVERS_CONFIG), "",""));
         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ""org.apache.kafka.common.serialization.ByteArraySerializer"");
@@ -137,6 +143,28 @@ public Worker(
         producerProps.putAll(config.originalsWithPrefix(""producer.""));
     }
 
+    private WorkerConfigTransformer initConfigTransformer() {
+        final List<String> providerNames = config.getList(WorkerConfig.CONFIG_PROVIDERS_CONFIG);
+        Map<String, ConfigProvider> providerMap = new HashMap<>();
+        for (String providerName : providerNames) {
+            ConfigProvider configProvider = plugins.newConfigProvider(
+                    config,
+                    WorkerConfig.CONFIG_PROVIDERS_CONFIG + ""."" + providerName,
+                    ClassLoaderUsage.PLUGINS
+            );
+            providerMap.put(providerName, configProvider);
+        }
+        return new WorkerConfigTransformer(this, providerMap);
+    }
+
+    public WorkerConfigTransformer configTransformer() {
+        return workerConfigTransformer;
+    }
+
+    protected Herder herder() {
+        return herder;
+    }
+
     /**
      * Start worker.
      */
@@ -359,6 +387,7 @@ public boolean isRunning(String connName) {
      */
     public boolean startTask(
             ConnectorTaskId id,
+            ClusterConfigState configState,
             Map<String, String> connProps,
             Map<String, String> taskProps,
             TaskStatus.Listener statusListener,
@@ -419,7 +448,7 @@ public boolean startTask(
                 log.info(""Set up the header converter {} for task {} using the connector config"", headerConverter.getClass(), id);
             }
 
-            workerTask = buildWorkerTask(connConfig, id, task, statusListener, initialState, keyConverter, valueConverter, headerConverter, connectorLoader);
+            workerTask = buildWorkerTask(configState, connConfig, id, task, statusListener, initialState, keyConverter, valueConverter, headerConverter, connectorLoader);
             workerTask.initialize(taskConfig);
             Plugins.compareAndSwapLoaders(savedLoader);
         } catch (Throwable t) {
@@ -444,7 +473,8 @@ public boolean startTask(
         return true;
     }
 
-    private WorkerTask buildWorkerTask(ConnectorConfig connConfig,
+    private WorkerTask buildWorkerTask(ClusterConfigState configState,
+                                       ConnectorConfig connConfig,
                                        ConnectorTaskId id,
                                        Task task,
                                        TaskStatus.Listener statusListener,
@@ -469,13 +499,14 @@ private WorkerTask buildWorkerTask(ConnectorConfig connConfig,
                     internalKeyConverter, internalValueConverter);
             KafkaProducer<byte[], byte[]> producer = new KafkaProducer<>(producerProps);
 
+            // Note we pass the configState as it performs dynamic transformations under the covers
             return new WorkerSourceTask(id, (SourceTask) task, statusListener, initialState, keyConverter, valueConverter,
-                    headerConverter, transformationChain, producer, offsetReader, offsetWriter, config, metrics, loader,
+                    headerConverter, transformationChain, producer, offsetReader, offsetWriter, config, configState, metrics, loader,
                     time, retryWithToleranceOperator);
         } else if (task instanceof SinkTask) {
             TransformationChain<SinkRecord> transformationChain = new TransformationChain<>(connConfig.<SinkRecord>transformations(), retryWithToleranceOperator);
             retryWithToleranceOperator.reporters(sinkTaskReporters(id, connConfig, errorHandlingMetrics));
-            return new WorkerSinkTask(id, (SinkTask) task, statusListener, initialState, config, metrics, keyConverter,
+            return new WorkerSinkTask(id, (SinkTask) task, statusListener, initialState, config, configState, metrics, keyConverter,
                     valueConverter, headerConverter, transformationChain, loader, time,
                     retryWithToleranceOperator);
         } else {
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfig.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfig.java
index 3c76d0fa5f2..355cfbb615b 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfig.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfig.java
@@ -32,6 +32,7 @@
 
 import java.util.ArrayList;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 
@@ -190,6 +191,11 @@
             + ""Examples: plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,""
             + ""/opt/connectors"";
 
+    public static final String CONFIG_PROVIDERS_CONFIG = ""config.providers"";
+    protected static final String CONFIG_PROVIDERS_DOC = ""List of configuration providers. ""
+            + ""This is a comma-separated list of the fully-qualified names of the ConfigProvider implementations, ""
+            + ""in the order they will be created, configured, and used."";
+
     public static final String REST_EXTENSION_CLASSES_CONFIG = ""rest.extension.classes"";
     protected static final String REST_EXTENSION_CLASSES_DOC =
             ""Comma-separated names of <code>ConnectRestExtension</code> classes, loaded and called ""
@@ -262,6 +268,9 @@ protected static ConfigDef baseConfigDef() {
                 .define(HEADER_CONVERTER_CLASS_CONFIG, Type.CLASS,
                         HEADER_CONVERTER_CLASS_DEFAULT,
                         Importance.LOW, HEADER_CONVERTER_CLASS_DOC)
+                .define(CONFIG_PROVIDERS_CONFIG, Type.LIST,
+                        Collections.emptyList(),
+                        Importance.LOW, CONFIG_PROVIDERS_DOC)
                 .define(REST_EXTENSION_CLASSES_CONFIG, Type.LIST, """",
                         Importance.LOW, REST_EXTENSION_CLASSES_DOC);
     }
@@ -334,4 +343,5 @@ public WorkerConfig(ConfigDef definition, Map<String, String> props) {
         super(definition, props);
         logInternalConverterDeprecationWarnings(props);
     }
+
 }
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java
new file mode 100644
index 00000000000..d91411cb8e8
--- /dev/null
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java
@@ -0,0 +1,71 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.connect.runtime;
+
+import org.apache.kafka.common.config.ConfigProvider;
+import org.apache.kafka.common.config.ConfigTransformer;
+import org.apache.kafka.common.config.ConfigTransformerResult;
+
+import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ConcurrentMap;
+
+/**
+ * A wrapper class to perform configuration transformations and schedule reloads for any
+ * retrieved TTL values.
+ */
+public class WorkerConfigTransformer {
+    private final Worker worker;
+    private final ConfigTransformer configTransformer;
+    private final ConcurrentMap<String, Map<String, HerderRequest>> requests = new ConcurrentHashMap<>();
+
+    public WorkerConfigTransformer(Worker worker, Map<String, ConfigProvider> configProviders) {
+        this.worker = worker;
+        this.configTransformer = new ConfigTransformer(configProviders);
+    }
+
+    public Map<String, String> transform(String connectorName, Map<String, String> configs) {
+        ConfigTransformerResult result = configTransformer.transform(configs);
+        scheduleReload(connectorName, result.ttls());
+        return result.data();
+    }
+
+    private void scheduleReload(String connectorName, Map<String, Long> ttls) {
+        for (Map.Entry<String, Long> entry : ttls.entrySet()) {
+            scheduleReload(connectorName, entry.getKey(), entry.getValue());
+        }
+    }
+
+    private void scheduleReload(String connectorName, String path, long ttl) {
+        Herder herder = worker.herder();
+        if (herder.connectorConfigReloadAction(connectorName) == Herder.ConfigReloadAction.RESTART) {
+            Map<String, HerderRequest> connectorRequests = requests.get(connectorName);
+            if (connectorRequests == null) {
+                connectorRequests = new ConcurrentHashMap<>();
+                requests.put(connectorName, connectorRequests);
+            } else {
+                HerderRequest previousRequest = connectorRequests.get(path);
+                if (previousRequest != null) {
+                    // Delete previous request for ttl which is now stale
+                    previousRequest.cancel();
+                }
+            }
+            HerderRequest request = herder.restartConnector(ttl, connectorName, null);
+            connectorRequests.put(path, request);
+        }
+    }
+}
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java
index 3296007b364..47f8529e2d1 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java
@@ -40,6 +40,7 @@
 import org.apache.kafka.connect.header.ConnectHeaders;
 import org.apache.kafka.connect.header.Headers;
 import org.apache.kafka.connect.runtime.ConnectMetrics.MetricGroup;
+import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;
 import org.apache.kafka.connect.runtime.errors.Stage;
 import org.apache.kafka.connect.sink.SinkRecord;
@@ -70,6 +71,7 @@
 
     private final WorkerConfig workerConfig;
     private final SinkTask task;
+    private final ClusterConfigState configState;
     private Map<String, String> taskConfig;
     private final Time time;
     private final Converter keyConverter;
@@ -96,6 +98,7 @@ public WorkerSinkTask(ConnectorTaskId id,
                           TaskStatus.Listener statusListener,
                           TargetState initialState,
                           WorkerConfig workerConfig,
+                          ClusterConfigState configState,
                           ConnectMetrics connectMetrics,
                           Converter keyConverter,
                           Converter valueConverter,
@@ -108,6 +111,7 @@ public WorkerSinkTask(ConnectorTaskId id,
 
         this.workerConfig = workerConfig;
         this.task = task;
+        this.configState = configState;
         this.keyConverter = keyConverter;
         this.valueConverter = valueConverter;
         this.headerConverter = headerConverter;
@@ -133,7 +137,7 @@ public void initialize(TaskConfig taskConfig) {
         try {
             this.taskConfig = taskConfig.originalsStrings();
             this.consumer = createConsumer();
-            this.context = new WorkerSinkTaskContext(consumer, this);
+            this.context = new WorkerSinkTaskContext(consumer, this, configState);
         } catch (Throwable t) {
             log.error(""{} Task failed initialization and will not be started."", this, t);
             onFailure(t);
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTaskContext.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTaskContext.java
index 386f992e82a..3a6b0d6d7b8 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTaskContext.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTaskContext.java
@@ -19,6 +19,7 @@
 import org.apache.kafka.clients.consumer.KafkaConsumer;
 import org.apache.kafka.common.TopicPartition;
 import org.apache.kafka.connect.errors.IllegalWorkerStateException;
+import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;
 import org.apache.kafka.connect.sink.SinkTaskContext;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -37,17 +38,26 @@
     private long timeoutMs;
     private KafkaConsumer<byte[], byte[]> consumer;
     private final WorkerSinkTask sinkTask;
+    private final ClusterConfigState configState;
     private final Set<TopicPartition> pausedPartitions;
     private boolean commitRequested;
 
-    public WorkerSinkTaskContext(KafkaConsumer<byte[], byte[]> consumer, WorkerSinkTask sinkTask) {
+    public WorkerSinkTaskContext(KafkaConsumer<byte[], byte[]> consumer,
+                                 WorkerSinkTask sinkTask,
+                                 ClusterConfigState configState) {
         this.offsets = new HashMap<>();
         this.timeoutMs = -1L;
         this.consumer = consumer;
         this.sinkTask = sinkTask;
+        this.configState = configState;
         this.pausedPartitions = new HashSet<>();
     }
 
+    @Override
+    public Map<String, String> configs() {
+        return configState.taskConfig(sinkTask.id());
+    }
+
     @Override
     public void offset(Map<TopicPartition, Long> offsets) {
         log.debug(""{} Setting offsets for topic partitions {}"", this, offsets);
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java
index e7b92a4d403..70d0cf9d7ae 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java
@@ -34,6 +34,7 @@
 import org.apache.kafka.connect.header.Header;
 import org.apache.kafka.connect.header.Headers;
 import org.apache.kafka.connect.runtime.ConnectMetrics.MetricGroup;
+import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;
 import org.apache.kafka.connect.runtime.errors.Stage;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -66,6 +67,7 @@
 
     private final WorkerConfig workerConfig;
     private final SourceTask task;
+    private final ClusterConfigState configState;
     private final Converter keyConverter;
     private final Converter valueConverter;
     private final HeaderConverter headerConverter;
@@ -103,6 +105,7 @@ public WorkerSourceTask(ConnectorTaskId id,
                             OffsetStorageReader offsetReader,
                             OffsetStorageWriter offsetWriter,
                             WorkerConfig workerConfig,
+                            ClusterConfigState configState,
                             ConnectMetrics connectMetrics,
                             ClassLoader loader,
                             Time time,
@@ -112,6 +115,7 @@ public WorkerSourceTask(ConnectorTaskId id,
 
         this.workerConfig = workerConfig;
         this.task = task;
+        this.configState = configState;
         this.keyConverter = keyConverter;
         this.valueConverter = valueConverter;
         this.headerConverter = headerConverter;
@@ -190,7 +194,7 @@ private synchronized void tryStop() {
     @Override
     public void execute() {
         try {
-            task.initialize(new WorkerSourceTaskContext(offsetReader));
+            task.initialize(new WorkerSourceTaskContext(offsetReader, this, configState));
             task.start(taskConfig);
             log.info(""{} Source task finished initialization and start"", this);
             synchronized (this) {
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTaskContext.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTaskContext.java
index 8f60e57e005..fe1409b282a 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTaskContext.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTaskContext.java
@@ -16,15 +16,29 @@
  */
 package org.apache.kafka.connect.runtime;
 
+import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;
 import org.apache.kafka.connect.source.SourceTaskContext;
 import org.apache.kafka.connect.storage.OffsetStorageReader;
 
+import java.util.Map;
+
 public class WorkerSourceTaskContext implements SourceTaskContext {
 
     private final OffsetStorageReader reader;
+    private final WorkerSourceTask task;
+    private final ClusterConfigState configState;
 
-    public WorkerSourceTaskContext(OffsetStorageReader reader) {
+    public WorkerSourceTaskContext(OffsetStorageReader reader,
+                                   WorkerSourceTask task,
+                                   ClusterConfigState configState) {
         this.reader = reader;
+        this.task = task;
+        this.configState = configState;
+    }
+
+    @Override
+    public Map<String, String> configs() {
+        return configState.taskConfig(task.id());
     }
 
     @Override
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ClusterConfigState.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ClusterConfigState.java
index cac71ddc73b..9507706840f 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ClusterConfigState.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ClusterConfigState.java
@@ -16,6 +16,7 @@
  */
 package org.apache.kafka.connect.runtime.distributed;
 
+import org.apache.kafka.connect.runtime.WorkerConfigTransformer;
 import org.apache.kafka.connect.runtime.TargetState;
 import org.apache.kafka.connect.util.ConnectorTaskId;
 
@@ -24,6 +25,7 @@
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
+import java.util.Objects;
 import java.util.Set;
 import java.util.TreeMap;
 
@@ -46,6 +48,7 @@
     private final Map<String, TargetState> connectorTargetStates;
     private final Map<ConnectorTaskId, Map<String, String>> taskConfigs;
     private final Set<String> inconsistentConnectors;
+    private final WorkerConfigTransformer configTransformer;
 
     public ClusterConfigState(long offset,
                               Map<String, Integer> connectorTaskCounts,
@@ -53,12 +56,29 @@ public ClusterConfigState(long offset,
                               Map<String, TargetState> connectorTargetStates,
                               Map<ConnectorTaskId, Map<String, String>> taskConfigs,
                               Set<String> inconsistentConnectors) {
+        this(offset,
+                connectorTaskCounts,
+                connectorConfigs,
+                connectorTargetStates,
+                taskConfigs,
+                inconsistentConnectors,
+                null);
+    }
+
+    public ClusterConfigState(long offset,
+                              Map<String, Integer> connectorTaskCounts,
+                              Map<String, Map<String, String>> connectorConfigs,
+                              Map<String, TargetState> connectorTargetStates,
+                              Map<ConnectorTaskId, Map<String, String>> taskConfigs,
+                              Set<String> inconsistentConnectors,
+                              WorkerConfigTransformer configTransformer) {
         this.offset = offset;
         this.connectorTaskCounts = connectorTaskCounts;
         this.connectorConfigs = connectorConfigs;
         this.connectorTargetStates = connectorTargetStates;
         this.taskConfigs = taskConfigs;
         this.inconsistentConnectors = inconsistentConnectors;
+        this.configTransformer = configTransformer;
     }
 
     /**
@@ -87,12 +107,19 @@ public boolean contains(String connector) {
     }
 
     /**
-     * Get the configuration for a connector.
+     * Get the configuration for a connector.  The configuration will have been transformed by
+     * {@link org.apache.kafka.common.config.ConfigTransformer} by having all variable
+     * references replaced with the current values from external instances of
+     * {@link org.apache.kafka.common.config.ConfigProvider}, and may include secrets.
      * @param connector name of the connector
      * @return a map containing configuration parameters
      */
     public Map<String, String> connectorConfig(String connector) {
-        return connectorConfigs.get(connector);
+        Map<String, String> configs = connectorConfigs.get(connector);
+        if (configTransformer != null) {
+            configs = configTransformer.transform(connector, configs);
+        }
+        return configs;
     }
 
     /**
@@ -105,12 +132,19 @@ public TargetState targetState(String connector) {
     }
 
     /**
-     * Get the configuration for a task.
+     * Get the configuration for a task.  The configuration will have been transformed by
+     * {@link org.apache.kafka.common.config.ConfigTransformer} by having all variable
+     * references replaced with the current values from external instances of
+     * {@link org.apache.kafka.common.config.ConfigProvider}, and may include secrets.
      * @param task id of the task
      * @return a map containing configuration parameters
      */
     public Map<String, String> taskConfig(ConnectorTaskId task) {
-        return taskConfigs.get(task);
+        Map<String, String> configs = taskConfigs.get(task);
+        if (configTransformer != null) {
+            configs = configTransformer.transform(task.connector(), configs);
+        }
+        return configs;
     }
 
     /**
@@ -184,4 +218,30 @@ public String toString() {
                 "", inconsistentConnectors="" + inconsistentConnectors +
                 '}';
     }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) return true;
+        if (o == null || getClass() != o.getClass()) return false;
+        ClusterConfigState that = (ClusterConfigState) o;
+        return offset == that.offset &&
+                Objects.equals(connectorTaskCounts, that.connectorTaskCounts) &&
+                Objects.equals(connectorConfigs, that.connectorConfigs) &&
+                Objects.equals(connectorTargetStates, that.connectorTargetStates) &&
+                Objects.equals(taskConfigs, that.taskConfigs) &&
+                Objects.equals(inconsistentConnectors, that.inconsistentConnectors) &&
+                Objects.equals(configTransformer, that.configTransformer);
+    }
+
+    @Override
+    public int hashCode() {
+        return Objects.hash(
+                offset,
+                connectorTaskCounts,
+                connectorConfigs,
+                connectorTargetStates,
+                taskConfigs,
+                inconsistentConnectors,
+                configTransformer);
+    }
 }
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java
index 5e9707aa29e..5efb78a93e4 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java
@@ -38,6 +38,7 @@
 import org.apache.kafka.connect.runtime.ConnectMetricsRegistry;
 import org.apache.kafka.connect.runtime.ConnectorConfig;
 import org.apache.kafka.connect.runtime.HerderConnectorContext;
+import org.apache.kafka.connect.runtime.HerderRequest;
 import org.apache.kafka.connect.runtime.SinkConnectorConfig;
 import org.apache.kafka.connect.runtime.SourceConnectorConfig;
 import org.apache.kafka.connect.runtime.TargetState;
@@ -60,6 +61,7 @@
 import java.util.Collections;
 import java.util.HashSet;
 import java.util.List;
+import java.util.Locale;
 import java.util.Map;
 import java.util.NavigableSet;
 import java.util.NoSuchElementException;
@@ -139,7 +141,7 @@
 
     // To handle most external requests, like creating or destroying a connector, we can use a generic request where
     // the caller specifies all the code that should be executed.
-    final NavigableSet<HerderRequest> requests = new ConcurrentSkipListSet<>();
+    final NavigableSet<DistributedHerderRequest> requests = new ConcurrentSkipListSet<>();
     // Config updates can be collected and applied together when possible. Also, we need to take care to rebalance when
     // needed (e.g. task reconfiguration, which requires everyone to coordinate offset commits).
     private Set<String> connectorConfigUpdates = new HashSet<>();
@@ -255,7 +257,7 @@ public void tick() {
         final long now = time.milliseconds();
         long nextRequestTimeoutMs = Long.MAX_VALUE;
         while (true) {
-            final HerderRequest next = peekWithoutException();
+            final DistributedHerderRequest next = peekWithoutException();
             if (next == null) {
                 break;
             } else if (now >= next.at) {
@@ -382,7 +384,7 @@ public void halt() {
 
             // Explicitly fail any outstanding requests so they actually get a response and get an
             // understandable reason for their failure.
-            HerderRequest request = requests.pollFirst();
+            DistributedHerderRequest request = requests.pollFirst();
             while (request != null) {
                 request.callback().onCompletion(new ConnectException(""Worker is shutting down""), null);
                 request = requests.pollFirst();
@@ -640,9 +642,21 @@ else if (!configState.contains(connName))
         );
     }
 
+    @Override
+    public ConfigReloadAction connectorConfigReloadAction(final String connName) {
+        return ConfigReloadAction.valueOf(
+                configState.connectorConfig(connName).get(ConnectorConfig.CONFIG_RELOAD_ACTION_CONFIG)
+                        .toUpperCase(Locale.ROOT));
+    }
+
     @Override
     public void restartConnector(final String connName, final Callback<Void> callback) {
-        addRequest(new Callable<Void>() {
+        restartConnector(0, connName, callback);
+    }
+
+    @Override
+    public HerderRequest restartConnector(final long delayMs, final String connName, final Callback<Void> callback) {
+        return addRequest(delayMs, new Callable<Void>() {
             @Override
             public Void call() throws Exception {
                 if (checkRebalanceNeeded(callback))
@@ -858,6 +872,7 @@ private boolean startTask(ConnectorTaskId taskId) {
         log.info(""Starting task {}"", taskId);
         return worker.startTask(
                 taskId,
+                configState,
                 configState.connectorConfig(taskId.connector()),
                 configState.taskConfig(taskId),
                 this,
@@ -945,7 +960,7 @@ private void reconfigureConnectorTasksWithRetry(final String connName) {
             public void onCompletion(Throwable error, Void result) {
                 // If we encountered an error, we don't have much choice but to just retry. If we don't, we could get
                 // stuck with a connector that thinks it has generated tasks, but wasn't actually successful and therefore
-                // never makes progress. The retry has to run through a HerderRequest since this callback could be happening
+                // never makes progress. The retry has to run through a DistributedHerderRequest since this callback could be happening
                 // from the HTTP request forwarding thread.
                 if (error != null) {
                     log.error(""Failed to reconfigure connector's tasks, retrying after backoff:"", error);
@@ -1041,19 +1056,19 @@ private boolean checkRebalanceNeeded(Callback<?> callback) {
         return false;
     }
 
-    HerderRequest addRequest(Callable<Void> action, Callback<Void> callback) {
+    DistributedHerderRequest addRequest(Callable<Void> action, Callback<Void> callback) {
         return addRequest(0, action, callback);
     }
 
-    HerderRequest addRequest(long delayMs, Callable<Void> action, Callback<Void> callback) {
-        HerderRequest req = new HerderRequest(time.milliseconds() + delayMs, requestSeqNum.incrementAndGet(), action, callback);
+    DistributedHerderRequest addRequest(long delayMs, Callable<Void> action, Callback<Void> callback) {
+        DistributedHerderRequest req = new DistributedHerderRequest(time.milliseconds() + delayMs, requestSeqNum.incrementAndGet(), action, callback);
         requests.add(req);
         if (peekWithoutException() == req)
             member.wakeup();
         return req;
     }
 
-    private HerderRequest peekWithoutException() {
+    private DistributedHerderRequest peekWithoutException() {
         try {
             return requests.isEmpty() ? null : requests.first();
         } catch (NoSuchElementException e) {
@@ -1117,13 +1132,13 @@ public void onConnectorTargetStateChange(String connector) {
         }
     }
 
-    static class HerderRequest implements Comparable<HerderRequest> {
+    class DistributedHerderRequest implements HerderRequest, Comparable<DistributedHerderRequest> {
         private final long at;
         private final long seq;
         private final Callable<Void> action;
         private final Callback<Void> callback;
 
-        public HerderRequest(long at, long seq, Callable<Void> action, Callback<Void> callback) {
+        public DistributedHerderRequest(long at, long seq, Callable<Void> action, Callback<Void> callback) {
             this.at = at;
             this.seq = seq;
             this.action = action;
@@ -1139,7 +1154,12 @@ public HerderRequest(long at, long seq, Callable<Void> action, Callback<Void> ca
         }
 
         @Override
-        public int compareTo(HerderRequest o) {
+        public void cancel() {
+            DistributedHerder.this.requests.remove(this);
+        }
+
+        @Override
+        public int compareTo(DistributedHerderRequest o) {
             final int cmp = Long.compare(at, o.at);
             return cmp == 0 ? Long.compare(seq, o.seq) : cmp;
         }
@@ -1147,9 +1167,9 @@ public int compareTo(HerderRequest o) {
         @Override
         public boolean equals(Object o) {
             if (this == o) return true;
-            if (!(o instanceof HerderRequest))
+            if (!(o instanceof DistributedHerderRequest))
                 return false;
-            HerderRequest other = (HerderRequest) o;
+            DistributedHerderRequest other = (DistributedHerderRequest) o;
             return compareTo(other) == 0;
         }
 
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/DelegatingClassLoader.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/DelegatingClassLoader.java
index b56bd1a7d91..1e598517507 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/DelegatingClassLoader.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/DelegatingClassLoader.java
@@ -16,6 +16,7 @@
  */
 package org.apache.kafka.connect.runtime.isolation;
 
+import org.apache.kafka.common.config.ConfigProvider;
 import org.apache.kafka.connect.components.Versioned;
 import org.apache.kafka.connect.connector.Connector;
 import org.apache.kafka.connect.rest.ConnectRestExtension;
@@ -66,6 +67,7 @@
     private final SortedSet<PluginDesc<Converter>> converters;
     private final SortedSet<PluginDesc<HeaderConverter>> headerConverters;
     private final SortedSet<PluginDesc<Transformation>> transformations;
+    private final SortedSet<PluginDesc<ConfigProvider>> configProviders;
     private final SortedSet<PluginDesc<ConnectRestExtension>> restExtensions;
     private final List<String> pluginPaths;
     private final Map<Path, PluginClassLoader> activePaths;
@@ -80,6 +82,7 @@ public DelegatingClassLoader(List<String> pluginPaths, ClassLoader parent) {
         this.converters = new TreeSet<>();
         this.headerConverters = new TreeSet<>();
         this.transformations = new TreeSet<>();
+        this.configProviders = new TreeSet<>();
         this.restExtensions = new TreeSet<>();
     }
 
@@ -103,6 +106,10 @@ public DelegatingClassLoader(List<String> pluginPaths) {
         return transformations;
     }
 
+    public Set<PluginDesc<ConfigProvider>> configProviders() {
+        return configProviders;
+    }
+
     public Set<PluginDesc<ConnectRestExtension>> restExtensions() {
         return restExtensions;
     }
@@ -236,6 +243,8 @@ private void scanUrlsAndAddPlugins(
             headerConverters.addAll(plugins.headerConverters());
             addPlugins(plugins.transformations(), loader);
             transformations.addAll(plugins.transformations());
+            addPlugins(plugins.configProviders(), loader);
+            configProviders.addAll(plugins.configProviders());
             addPlugins(plugins.restExtensions(), loader);
             restExtensions.addAll(plugins.restExtensions());
         }
@@ -292,6 +301,7 @@ private PluginScanResult scanPluginPath(
                 getPluginDesc(reflections, Converter.class, loader),
                 getPluginDesc(reflections, HeaderConverter.class, loader),
                 getPluginDesc(reflections, Transformation.class, loader),
+                getPluginDesc(reflections, ConfigProvider.class, loader),
                 getServiceLoaderPluginDesc(ConnectRestExtension.class, loader)
         );
     }
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginScanResult.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginScanResult.java
index 6f48e5694bd..87b0b70c503 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginScanResult.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginScanResult.java
@@ -16,6 +16,7 @@
  */
 package org.apache.kafka.connect.runtime.isolation;
 
+import org.apache.kafka.common.config.ConfigProvider;
 import org.apache.kafka.connect.connector.Connector;
 import org.apache.kafka.connect.rest.ConnectRestExtension;
 import org.apache.kafka.connect.storage.Converter;
@@ -29,6 +30,7 @@
     private final Collection<PluginDesc<Converter>> converters;
     private final Collection<PluginDesc<HeaderConverter>> headerConverters;
     private final Collection<PluginDesc<Transformation>> transformations;
+    private final Collection<PluginDesc<ConfigProvider>> configProviders;
     private final Collection<PluginDesc<ConnectRestExtension>> restExtensions;
 
     public PluginScanResult(
@@ -36,12 +38,14 @@ public PluginScanResult(
             Collection<PluginDesc<Converter>> converters,
             Collection<PluginDesc<HeaderConverter>> headerConverters,
             Collection<PluginDesc<Transformation>> transformations,
+            Collection<PluginDesc<ConfigProvider>> configProviders,
             Collection<PluginDesc<ConnectRestExtension>> restExtensions
     ) {
         this.connectors = connectors;
         this.converters = converters;
         this.headerConverters = headerConverters;
         this.transformations = transformations;
+        this.configProviders = configProviders;
         this.restExtensions = restExtensions;
     }
 
@@ -61,6 +65,10 @@ public PluginScanResult(
         return transformations;
     }
 
+    public Collection<PluginDesc<ConfigProvider>> configProviders() {
+        return configProviders;
+    }
+
     public Collection<PluginDesc<ConnectRestExtension>> restExtensions() {
         return restExtensions;
     }
@@ -70,6 +78,7 @@ public boolean isEmpty() {
                && converters().isEmpty()
                && headerConverters().isEmpty()
                && transformations().isEmpty()
+               && configProviders().isEmpty()
                && restExtensions().isEmpty();
     }
 }
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginType.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginType.java
index 918f9d73f56..906b85f7003 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginType.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginType.java
@@ -16,6 +16,7 @@
  */
 package org.apache.kafka.connect.runtime.isolation;
 
+import org.apache.kafka.common.config.ConfigProvider;
 import org.apache.kafka.connect.connector.Connector;
 import org.apache.kafka.connect.rest.ConnectRestExtension;
 import org.apache.kafka.connect.sink.SinkConnector;
@@ -31,6 +32,7 @@
     CONNECTOR(Connector.class),
     CONVERTER(Converter.class),
     TRANSFORMATION(Transformation.class),
+    CONFIGPROVIDER(ConfigProvider.class),
     REST_EXTENSION(ConnectRestExtension.class),
     UNKNOWN(Object.class);
 
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java
index 96074106de0..c89accd3805 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java
@@ -19,6 +19,7 @@
 import org.apache.kafka.common.Configurable;
 import org.apache.kafka.common.KafkaException;
 import org.apache.kafka.common.config.AbstractConfig;
+import org.apache.kafka.common.config.ConfigProvider;
 import org.apache.kafka.common.utils.Utils;
 import org.apache.kafka.connect.components.Versioned;
 import org.apache.kafka.connect.connector.ConnectRecord;
@@ -147,6 +148,10 @@ public DelegatingClassLoader delegatingLoader() {
         return delegatingLoader.transformations();
     }
 
+    public Set<PluginDesc<ConfigProvider>> configProviders() {
+        return delegatingLoader.configProviders();
+    }
+
     public Connector newConnector(String connectorClassOrAlias) {
         Class<? extends Connector> klass;
         try {
@@ -318,6 +323,45 @@ public HeaderConverter newHeaderConverter(AbstractConfig config, String classPro
         return plugin;
     }
 
+    public ConfigProvider newConfigProvider(AbstractConfig config, String providerPrefix, ClassLoaderUsage classLoaderUsage) {
+        String classPropertyName = providerPrefix + "".class"";
+        Map<String, String> originalConfig = config.originalsStrings();
+        if (!originalConfig.containsKey(classPropertyName)) {
+            // This configuration does not define the config provider via the specified property name
+            return null;
+        }
+        ConfigProvider plugin = null;
+        switch (classLoaderUsage) {
+            case CURRENT_CLASSLOADER:
+                // Attempt to load first with the current classloader, and plugins as a fallback.
+                plugin = getInstance(config, classPropertyName, ConfigProvider.class);
+                break;
+            case PLUGINS:
+                // Attempt to load with the plugin class loader, which uses the current classloader as a fallback
+                String configProviderClassOrAlias = originalConfig.get(classPropertyName);
+                Class<? extends ConfigProvider> klass;
+                try {
+                    klass = pluginClass(delegatingLoader, configProviderClassOrAlias, ConfigProvider.class);
+                } catch (ClassNotFoundException e) {
+                    throw new ConnectException(
+                            ""Failed to find any class that implements ConfigProvider and which name matches ""
+                                    + configProviderClassOrAlias + "", available ConfigProviders are: ""
+                                    + pluginNames(delegatingLoader.configProviders())
+                    );
+                }
+                plugin = newPlugin(klass);
+                break;
+        }
+        if (plugin == null) {
+            throw new ConnectException(""Unable to instantiate the ConfigProvider specified in '"" + classPropertyName + ""'"");
+        }
+
+        // Configure the ConfigProvider
+        String configPrefix = providerPrefix + "".param."";
+        Map<String, Object> configProviderConfig = config.originalsWithPrefix(configPrefix);
+        plugin.configure(configProviderConfig);
+        return plugin;
+    }
 
     /**
      * If the given class names are available in the classloader, return a list of new configured
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerder.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerder.java
index 96f8e8767bb..20c6a24d384 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerder.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerder.java
@@ -22,6 +22,7 @@
 import org.apache.kafka.connect.runtime.AbstractHerder;
 import org.apache.kafka.connect.runtime.ConnectorConfig;
 import org.apache.kafka.connect.runtime.HerderConnectorContext;
+import org.apache.kafka.connect.runtime.HerderRequest;
 import org.apache.kafka.connect.runtime.SinkConnectorConfig;
 import org.apache.kafka.connect.runtime.SourceConnectorConfig;
 import org.apache.kafka.connect.runtime.TargetState;
@@ -41,7 +42,14 @@
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.List;
+import java.util.Locale;
 import java.util.Map;
+import java.util.Objects;
+import java.util.concurrent.Executors;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.ScheduledFuture;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicLong;
 
 
 /**
@@ -50,10 +58,17 @@
 public class StandaloneHerder extends AbstractHerder {
     private static final Logger log = LoggerFactory.getLogger(StandaloneHerder.class);
 
+    private final AtomicLong requestSeqNum = new AtomicLong();
+    private final ScheduledExecutorService requestExecutorService;
+
     private ClusterConfigState configState;
 
     public StandaloneHerder(Worker worker, String kafkaClusterId) {
-        this(worker, worker.workerId(), kafkaClusterId, new MemoryStatusBackingStore(), new MemoryConfigBackingStore());
+        this(worker,
+                worker.workerId(),
+                kafkaClusterId,
+                new MemoryStatusBackingStore(),
+                new MemoryConfigBackingStore(worker.configTransformer()));
     }
 
     // visible for testing
@@ -64,6 +79,7 @@ public StandaloneHerder(Worker worker, String kafkaClusterId) {
                      MemoryConfigBackingStore configBackingStore) {
         super(worker, workerId, kafkaClusterId, statusBackingStore, configBackingStore);
         this.configState = ClusterConfigState.EMPTY;
+        this.requestExecutorService = Executors.newSingleThreadScheduledExecutor();
         configBackingStore.setUpdateListener(new ConfigUpdateListener());
     }
 
@@ -77,6 +93,13 @@ public synchronized void start() {
     @Override
     public synchronized void stop() {
         log.info(""Herder stopping"");
+        requestExecutorService.shutdown();
+        try {
+            if (!requestExecutorService.awaitTermination(30, TimeUnit.SECONDS))
+                requestExecutorService.shutdownNow();
+        } catch (InterruptedException e) {
+            // ignore
+        }
 
         // There's no coordination/hand-off to do here since this is all standalone. Instead, we
         // should just clean up the stuff we normally would, i.e. cleanly checkpoint and shutdown all
@@ -229,12 +252,19 @@ public synchronized void restartTask(ConnectorTaskId taskId, Callback<Void> cb)
 
         TargetState targetState = configState.targetState(taskId.connector());
         worker.stopAndAwaitTask(taskId);
-        if (worker.startTask(taskId, connConfigProps, taskConfigProps, this, targetState))
+        if (worker.startTask(taskId, configState, connConfigProps, taskConfigProps, this, targetState))
             cb.onCompletion(null, null);
         else
             cb.onCompletion(new ConnectException(""Failed to start task: "" + taskId), null);
     }
 
+    @Override
+    public ConfigReloadAction connectorConfigReloadAction(final String connName) {
+        return ConfigReloadAction.valueOf(
+                configState.connectorConfig(connName).get(ConnectorConfig.CONFIG_RELOAD_ACTION_CONFIG)
+                        .toUpperCase(Locale.ROOT));
+    }
+
     @Override
     public synchronized void restartConnector(String connName, Callback<Void> cb) {
         if (!configState.contains(connName))
@@ -248,11 +278,24 @@ public synchronized void restartConnector(String connName, Callback<Void> cb) {
             cb.onCompletion(new ConnectException(""Failed to start connector: "" + connName), null);
     }
 
+    @Override
+    public synchronized HerderRequest restartConnector(long delayMs, final String connName, final Callback<Void> cb) {
+        ScheduledFuture<?> future = requestExecutorService.schedule(new Runnable() {
+            @Override
+            public void run() {
+                restartConnector(connName, cb);
+            }
+        }, delayMs, TimeUnit.MILLISECONDS);
+
+        return new StandaloneHerderRequest(requestSeqNum.incrementAndGet(), future);
+    }
+
     private boolean startConnector(Map<String, String> connectorProps) {
         String connName = connectorProps.get(ConnectorConfig.NAME_CONFIG);
         configBackingStore.putConnectorConfig(connName, connectorProps);
+        Map<String, String> connConfigs = configState.connectorConfig(connName);
         TargetState targetState = configState.targetState(connName);
-        return worker.startConnector(connName, connectorProps, new HerderConnectorContext(this, connName), this, targetState);
+        return worker.startConnector(connName, connConfigs, new HerderConnectorContext(this, connName), this, targetState);
     }
 
     private List<Map<String, String>> recomputeTaskConfigs(String connName) {
@@ -270,7 +313,7 @@ private void createConnectorTasks(String connName, TargetState initialState) {
 
         for (ConnectorTaskId taskId : configState.tasks(connName)) {
             Map<String, String> taskConfigMap = configState.taskConfig(taskId);
-            worker.startTask(taskId, connConfigs, taskConfigMap, this, initialState);
+            worker.startTask(taskId, configState, connConfigs, taskConfigMap, this, initialState);
         }
     }
 
@@ -342,4 +385,32 @@ public void onConnectorTargetStateChange(String connector) {
         }
     }
 
+    static class StandaloneHerderRequest implements HerderRequest {
+        private final long seq;
+        private final ScheduledFuture<?> future;
+
+        public StandaloneHerderRequest(long seq, ScheduledFuture<?> future) {
+            this.seq = seq;
+            this.future = future;
+        }
+
+        @Override
+        public void cancel() {
+            future.cancel(false);
+        }
+
+        @Override
+        public boolean equals(Object o) {
+            if (this == o) return true;
+            if (!(o instanceof StandaloneHerderRequest))
+                return false;
+            StandaloneHerderRequest other = (StandaloneHerderRequest) o;
+            return seq == other.seq;
+        }
+
+        @Override
+        public int hashCode() {
+            return Objects.hash(seq);
+        }
+    }
 }
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java b/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java
index e51b365cec6..ea196650c5e 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigBackingStore.java
@@ -35,6 +35,7 @@
 import org.apache.kafka.connect.errors.DataException;
 import org.apache.kafka.connect.runtime.TargetState;
 import org.apache.kafka.connect.runtime.WorkerConfig;
+import org.apache.kafka.connect.runtime.WorkerConfigTransformer;
 import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;
 import org.apache.kafka.connect.runtime.distributed.DistributedConfig;
 import org.apache.kafka.connect.util.Callback;
@@ -221,7 +222,9 @@ public static String COMMIT_TASKS_KEY(String connectorName) {
 
     private final Map<String, TargetState> connectorTargetStates = new HashMap<>();
 
-    public KafkaConfigBackingStore(Converter converter, WorkerConfig config) {
+    private final WorkerConfigTransformer configTransformer;
+
+    public KafkaConfigBackingStore(Converter converter, WorkerConfig config, WorkerConfigTransformer configTransformer) {
         this.lock = new Object();
         this.started = false;
         this.converter = converter;
@@ -232,6 +235,7 @@ public KafkaConfigBackingStore(Converter converter, WorkerConfig config) {
             throw new ConfigException(""Must specify topic for connector configuration."");
 
         configLog = setupAndCreateKafkaBasedLog(this.topic, config);
+        this.configTransformer = configTransformer;
     }
 
     @Override
@@ -270,7 +274,8 @@ public ClusterConfigState snapshot() {
                     new HashMap<>(connectorConfigs),
                     new HashMap<>(connectorTargetStates),
                     new HashMap<>(taskConfigs),
-                    new HashSet<>(inconsistent)
+                    new HashSet<>(inconsistent),
+                    configTransformer
             );
         }
     }
diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/storage/MemoryConfigBackingStore.java b/connect/runtime/src/main/java/org/apache/kafka/connect/storage/MemoryConfigBackingStore.java
index 25891f52354..7e7d62ba3c9 100644
--- a/connect/runtime/src/main/java/org/apache/kafka/connect/storage/MemoryConfigBackingStore.java
+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/storage/MemoryConfigBackingStore.java
@@ -17,6 +17,7 @@
 package org.apache.kafka.connect.storage;
 
 import org.apache.kafka.connect.runtime.TargetState;
+import org.apache.kafka.connect.runtime.WorkerConfigTransformer;
 import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;
 import org.apache.kafka.connect.util.ConnectorTaskId;
 
@@ -32,6 +33,14 @@
 
     private Map<String, ConnectorState> connectors = new HashMap<>();
     private UpdateListener updateListener;
+    private WorkerConfigTransformer configTransformer;
+
+    public MemoryConfigBackingStore() {
+    }
+
+    public MemoryConfigBackingStore(WorkerConfigTransformer configTransformer) {
+        this.configTransformer = configTransformer;
+    }
 
     @Override
     public synchronized void start() {
@@ -63,7 +72,8 @@ public synchronized ClusterConfigState snapshot() {
                 connectorConfigs,
                 connectorTargetStates,
                 taskConfigs,
-                Collections.<String>emptySet());
+                Collections.<String>emptySet(),
+                configTransformer);
     }
 
     @Override
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java
index 0718eb17653..da017e851b7 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/AbstractHerderTest.java
@@ -172,14 +172,14 @@ public void testConfigValidationMissingName() {
         assertEquals(TestSourceConnector.class.getName(), result.name());
         assertEquals(Arrays.asList(ConnectorConfig.COMMON_GROUP, ConnectorConfig.TRANSFORMS_GROUP), result.groups());
         assertEquals(2, result.errorCount());
-        // Base connector config has 7 fields, connector's configs add 2
-        assertEquals(9, result.values().size());
+        // Base connector config has 8 fields, connector's configs add 2
+        assertEquals(10, result.values().size());
         // Missing name should generate an error
         assertEquals(ConnectorConfig.NAME_CONFIG, result.values().get(0).configValue().name());
         assertEquals(1, result.values().get(0).configValue().errors().size());
         // ""required"" config from connector should generate an error
-        assertEquals(""required"", result.values().get(7).configValue().name());
-        assertEquals(1, result.values().get(7).configValue().errors().size());
+        assertEquals(""required"", result.values().get(8).configValue().name());
+        assertEquals(1, result.values().get(8).configValue().errors().size());
 
         verifyAll();
     }
@@ -233,15 +233,15 @@ public void testConfigValidationTransformsExtendResults() {
         );
         assertEquals(expectedGroups, result.groups());
         assertEquals(2, result.errorCount());
-        // Base connector config has 7 fields, connector's configs add 2, 2 type fields from the transforms, and
+        // Base connector config has 8 fields, connector's configs add 2, 2 type fields from the transforms, and
         // 1 from the valid transformation's config
-        assertEquals(12, result.values().size());
+        assertEquals(13, result.values().size());
         // Should get 2 type fields from the transforms, first adds its own config since it has a valid class
-        assertEquals(""transforms.xformA.type"", result.values().get(7).configValue().name());
-        assertTrue(result.values().get(7).configValue().errors().isEmpty());
-        assertEquals(""transforms.xformA.subconfig"", result.values().get(8).configValue().name());
-        assertEquals(""transforms.xformB.type"", result.values().get(9).configValue().name());
-        assertFalse(result.values().get(9).configValue().errors().isEmpty());
+        assertEquals(""transforms.xformA.type"", result.values().get(8).configValue().name());
+        assertTrue(result.values().get(8).configValue().errors().isEmpty());
+        assertEquals(""transforms.xformA.subconfig"", result.values().get(9).configValue().name());
+        assertEquals(""transforms.xformB.type"", result.values().get(10).configValue().name());
+        assertFalse(result.values().get(10).configValue().errors().isEmpty());
 
         verifyAll();
     }
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java
index 5a8bcc5a6cc..b50e7ff0956 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java
@@ -31,6 +31,7 @@
 import org.apache.kafka.connect.data.Struct;
 import org.apache.kafka.connect.errors.RetriableException;
 import org.apache.kafka.connect.json.JsonConverter;
+import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;
 import org.apache.kafka.connect.runtime.errors.ErrorHandlingMetrics;
 import org.apache.kafka.connect.runtime.errors.LogReporter;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;
@@ -369,7 +370,8 @@ private void createSinkTask(TargetState initialState, RetryWithToleranceOperator
 
         workerSinkTask = PowerMock.createPartialMock(
                 WorkerSinkTask.class, new String[]{""createConsumer""},
-                taskId, sinkTask, statusListener, initialState, workerConfig, metrics, converter, converter,
+                taskId, sinkTask, statusListener, initialState, workerConfig,
+                ClusterConfigState.EMPTY, metrics, converter, converter,
                 headerConverter, sinkTransforms, pluginLoader, time, retryWithToleranceOperator);
     }
 
@@ -398,7 +400,8 @@ private void createSourceTask(TargetState initialState, RetryWithToleranceOperat
         workerSourceTask = PowerMock.createPartialMock(
                 WorkerSourceTask.class, new String[]{""commitOffsets"", ""isStopping""},
                 taskId, sourceTask, statusListener, initialState, converter, converter, headerConverter, sourceTransforms,
-                producer, offsetReader, offsetWriter, workerConfig, metrics, pluginLoader, time, retryWithToleranceOperator);
+                producer, offsetReader, offsetWriter, workerConfig,
+                ClusterConfigState.EMPTY, metrics, pluginLoader, time, retryWithToleranceOperator);
     }
 
     private ConsumerRecords<byte[], byte[]> records(ConsumerRecord<byte[], byte[]> record) {
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java
new file mode 100644
index 00000000000..89bba09b0da
--- /dev/null
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java
@@ -0,0 +1,146 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.connect.runtime;
+
+import org.apache.kafka.common.config.ConfigChangeCallback;
+import org.apache.kafka.common.config.ConfigData;
+import org.apache.kafka.common.config.ConfigProvider;
+import org.easymock.EasyMock;
+import org.junit.Before;
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.powermock.api.easymock.PowerMock;
+import org.powermock.api.easymock.annotation.Mock;
+import org.powermock.modules.junit4.PowerMockRunner;
+
+import java.util.Collections;
+import java.util.Map;
+import java.util.Set;
+
+import static org.junit.Assert.assertEquals;
+import static org.powermock.api.easymock.PowerMock.replayAll;
+
+@RunWith(PowerMockRunner.class)
+public class WorkerConfigTransformerTest {
+
+    public static final String MY_KEY = ""myKey"";
+    public static final String MY_CONNECTOR = ""myConnector"";
+    public static final String TEST_KEY = ""testKey"";
+    public static final String TEST_PATH = ""testPath"";
+    public static final String TEST_KEY_WITH_TTL = ""testKeyWithTTL"";
+    public static final String TEST_KEY_WITH_LONGER_TTL = ""testKeyWithLongerTTL"";
+    public static final String TEST_RESULT = ""testResult"";
+    public static final String TEST_RESULT_WITH_TTL = ""testResultWithTTL"";
+    public static final String TEST_RESULT_WITH_LONGER_TTL = ""testResultWithLongerTTL"";
+
+    @Mock private Herder herder;
+    @Mock private Worker worker;
+    @Mock private HerderRequest requestId;
+    private WorkerConfigTransformer configTransformer;
+
+    @Before
+    public void setup() {
+        worker = PowerMock.createMock(Worker.class);
+        herder = PowerMock.createMock(Herder.class);
+        configTransformer = new WorkerConfigTransformer(worker, Collections.singletonMap(""test"", new TestConfigProvider()));
+    }
+
+    @Test
+    public void testReplaceVariable() throws Exception {
+        Map<String, String> result = configTransformer.transform(MY_CONNECTOR, Collections.singletonMap(MY_KEY, ""${test:testPath:testKey}""));
+        assertEquals(TEST_RESULT, result.get(MY_KEY));
+    }
+
+    @Test
+    public void testReplaceVariableWithTTL() throws Exception {
+        EasyMock.expect(worker.herder()).andReturn(herder);
+        EasyMock.expect(herder.connectorConfigReloadAction(MY_CONNECTOR)).andReturn(Herder.ConfigReloadAction.NONE);
+
+        replayAll();
+
+        Map<String, String> result = configTransformer.transform(MY_CONNECTOR, Collections.singletonMap(MY_KEY, ""${test:testPath:testKeyWithTTL}""));
+        assertEquals(TEST_RESULT_WITH_TTL, result.get(MY_KEY));
+    }
+
+    @Test
+    public void testReplaceVariableWithTTLAndScheduleRestart() throws Exception {
+        EasyMock.expect(worker.herder()).andReturn(herder);
+        EasyMock.expect(herder.connectorConfigReloadAction(MY_CONNECTOR)).andReturn(Herder.ConfigReloadAction.RESTART);
+        EasyMock.expect(herder.restartConnector(1L, MY_CONNECTOR, null)).andReturn(requestId);
+
+        replayAll();
+
+        Map<String, String> result = configTransformer.transform(MY_CONNECTOR, Collections.singletonMap(MY_KEY, ""${test:testPath:testKeyWithTTL}""));
+        assertEquals(TEST_RESULT_WITH_TTL, result.get(MY_KEY));
+    }
+
+    @Test
+    public void testReplaceVariableWithTTLFirstCancelThenScheduleRestart() throws Exception {
+        EasyMock.expect(worker.herder()).andReturn(herder);
+        EasyMock.expect(herder.connectorConfigReloadAction(MY_CONNECTOR)).andReturn(Herder.ConfigReloadAction.RESTART);
+        EasyMock.expect(herder.restartConnector(1L, MY_CONNECTOR, null)).andReturn(requestId);
+
+        EasyMock.expect(worker.herder()).andReturn(herder);
+        EasyMock.expect(herder.connectorConfigReloadAction(MY_CONNECTOR)).andReturn(Herder.ConfigReloadAction.RESTART);
+        EasyMock.expectLastCall();
+        requestId.cancel();
+        EasyMock.expectLastCall();
+        EasyMock.expect(herder.restartConnector(10L, MY_CONNECTOR, null)).andReturn(requestId);
+
+        replayAll();
+
+        Map<String, String> result = configTransformer.transform(MY_CONNECTOR, Collections.singletonMap(MY_KEY, ""${test:testPath:testKeyWithTTL}""));
+        assertEquals(TEST_RESULT_WITH_TTL, result.get(MY_KEY));
+
+        result = configTransformer.transform(MY_CONNECTOR, Collections.singletonMap(MY_KEY, ""${test:testPath:testKeyWithLongerTTL}""));
+        assertEquals(TEST_RESULT_WITH_LONGER_TTL, result.get(MY_KEY));
+    }
+
+    public static class TestConfigProvider implements ConfigProvider {
+
+        public void configure(Map<String, ?> configs) {
+        }
+
+        public ConfigData get(String path) {
+            return null;
+        }
+
+        public ConfigData get(String path, Set<String> keys) {
+            if (path.equals(TEST_PATH)) {
+                if (keys.contains(TEST_KEY)) {
+                    return new ConfigData(Collections.singletonMap(TEST_KEY, TEST_RESULT));
+                } else if (keys.contains(TEST_KEY_WITH_TTL)) {
+                    return new ConfigData(Collections.singletonMap(TEST_KEY_WITH_TTL, TEST_RESULT_WITH_TTL), 1L);
+                } else if (keys.contains(TEST_KEY_WITH_LONGER_TTL)) {
+                    return new ConfigData(Collections.singletonMap(TEST_KEY_WITH_LONGER_TTL, TEST_RESULT_WITH_LONGER_TTL), 10L);
+                }
+            }
+            return new ConfigData(Collections.emptyMap());
+        }
+
+        public void subscribe(String path, Set<String> keys, ConfigChangeCallback callback) {
+            throw new UnsupportedOperationException();
+        }
+
+        public void unsubscribe(String path, Set<String> keys) {
+            throw new UnsupportedOperationException();
+        }
+
+        public void close() {
+        }
+    }
+}
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java
index ff8507c0945..d23adbf3d69 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskTest.java
@@ -32,6 +32,7 @@
 import org.apache.kafka.connect.data.SchemaAndValue;
 import org.apache.kafka.connect.errors.RetriableException;
 import org.apache.kafka.connect.runtime.ConnectMetrics.MetricGroup;
+import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;
 import org.apache.kafka.connect.runtime.WorkerSinkTask.SinkTaskMetricsGroup;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;
 import org.apache.kafka.connect.runtime.isolation.PluginClassLoader;
@@ -162,7 +163,7 @@ public void setUp() {
     private void createTask(TargetState initialState) {
         workerTask = PowerMock.createPartialMock(
                 WorkerSinkTask.class, new String[]{""createConsumer""},
-                taskId, sinkTask, statusListener, initialState, workerConfig, metrics,
+                taskId, sinkTask, statusListener, initialState, workerConfig, ClusterConfigState.EMPTY, metrics,
                 keyConverter, valueConverter, headerConverter,
                 transformationChain, pluginLoader, time,
                 RetryWithToleranceOperator.NOOP_OPERATOR);
@@ -1463,5 +1464,4 @@ private void assertMetrics(int minimumPollCountExpected) {
 
     private abstract static class TestSinkTask extends SinkTask  {
     }
-
 }
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java
index 61d8778d11f..800301e05dc 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSinkTaskThreadedTest.java
@@ -28,6 +28,7 @@
 import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.SchemaAndValue;
 import org.apache.kafka.connect.errors.ConnectException;
+import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;
 import org.apache.kafka.connect.runtime.isolation.PluginClassLoader;
 import org.apache.kafka.connect.runtime.standalone.StandaloneConfig;
@@ -137,7 +138,7 @@ public void setup() {
         workerConfig = new StandaloneConfig(workerProps);
         workerTask = PowerMock.createPartialMock(
                 WorkerSinkTask.class, new String[]{""createConsumer""},
-                taskId, sinkTask, statusListener, initialState, workerConfig, metrics, keyConverter,
+                taskId, sinkTask, statusListener, initialState, workerConfig, ClusterConfigState.EMPTY, metrics, keyConverter,
                 valueConverter, headerConverter,
                 new TransformationChain(Collections.emptyList(), RetryWithToleranceOperator.NOOP_OPERATOR),
                 pluginLoader, time, RetryWithToleranceOperator.NOOP_OPERATOR);
@@ -700,5 +701,4 @@ public Object answer() throws Throwable {
 
     private static abstract class TestSinkTask extends SinkTask {
     }
-
 }
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java
index 77f4ad93bad..1482d75b513 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskTest.java
@@ -26,6 +26,7 @@
 import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.runtime.ConnectMetrics.MetricGroup;
 import org.apache.kafka.connect.runtime.WorkerSourceTask.SourceTaskMetricsGroup;
+import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;
 import org.apache.kafka.connect.runtime.isolation.Plugins;
 import org.apache.kafka.connect.runtime.standalone.StandaloneConfig;
@@ -104,6 +105,7 @@
     @Mock private KafkaProducer<byte[], byte[]> producer;
     @Mock private OffsetStorageReader offsetReader;
     @Mock private OffsetStorageWriter offsetWriter;
+    @Mock private ClusterConfigState clusterConfigState;
     private WorkerSourceTask workerTask;
     @Mock private Future<RecordMetadata> sendFuture;
     @MockStrict private TaskStatus.Listener statusListener;
@@ -148,7 +150,7 @@ private void createWorkerTask() {
 
     private void createWorkerTask(TargetState initialState) {
         workerTask = new WorkerSourceTask(taskId, sourceTask, statusListener, initialState, keyConverter, valueConverter, headerConverter,
-                transformationChain, producer, offsetReader, offsetWriter, config, metrics, plugins.delegatingLoader(), Time.SYSTEM,
+                transformationChain, producer, offsetReader, offsetWriter, config, clusterConfigState, metrics, plugins.delegatingLoader(), Time.SYSTEM,
                 RetryWithToleranceOperator.NOOP_OPERATOR);
     }
 
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java
index d29eef5ed69..6fa7ed11b9e 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerTest.java
@@ -34,6 +34,7 @@
 import org.apache.kafka.connect.json.JsonConverterConfig;
 import org.apache.kafka.connect.runtime.ConnectMetrics.MetricGroup;
 import org.apache.kafka.connect.runtime.MockConnectMetrics.MockMetricsReporter;
+import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;
 import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;
 import org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader;
 import org.apache.kafka.connect.runtime.isolation.PluginClassLoader;
@@ -491,6 +492,7 @@ public void testAddRemoveTask() throws Exception {
                 anyObject(OffsetStorageReader.class),
                 anyObject(OffsetStorageWriter.class),
                 EasyMock.eq(config),
+                anyObject(ClusterConfigState.class),
                 anyObject(ConnectMetrics.class),
                 anyObject(ClassLoader.class),
                 anyObject(Time.class),
@@ -547,7 +549,7 @@ public void testAddRemoveTask() throws Exception {
         assertStatistics(worker, 0, 0);
         assertStartupStatistics(worker, 0, 0, 0, 0);
         assertEquals(Collections.emptySet(), worker.taskIds());
-        worker.startTask(TASK_ID, anyConnectorConfigMap(), origProps, taskStatusListener, TargetState.STARTED);
+        worker.startTask(TASK_ID, ClusterConfigState.EMPTY, anyConnectorConfigMap(), origProps, taskStatusListener, TargetState.STARTED);
         assertStatistics(worker, 0, 1);
         assertStartupStatistics(worker, 0, 0, 1, 0);
         assertEquals(new HashSet<>(Arrays.asList(TASK_ID)), worker.taskIds());
@@ -598,7 +600,7 @@ public void testStartTaskFailure() throws Exception {
         assertStatistics(worker, 0, 0);
         assertStartupStatistics(worker, 0, 0, 0, 0);
 
-        assertFalse(worker.startTask(TASK_ID, anyConnectorConfigMap(), origProps, taskStatusListener, TargetState.STARTED));
+        assertFalse(worker.startTask(TASK_ID, ClusterConfigState.EMPTY, anyConnectorConfigMap(), origProps, taskStatusListener, TargetState.STARTED));
         assertStartupStatistics(worker, 0, 0, 1, 1);
 
         assertStatistics(worker, 0, 0);
@@ -629,6 +631,7 @@ public void testCleanupTasksOnStop() throws Exception {
                 anyObject(OffsetStorageReader.class),
                 anyObject(OffsetStorageWriter.class),
                 anyObject(WorkerConfig.class),
+                anyObject(ClusterConfigState.class),
                 anyObject(ConnectMetrics.class),
                 EasyMock.eq(pluginLoader),
                 anyObject(Time.class),
@@ -688,7 +691,7 @@ public void testCleanupTasksOnStop() throws Exception {
         worker = new Worker(WORKER_ID, new MockTime(), plugins, config, offsetBackingStore);
         worker.start();
         assertStatistics(worker, 0, 0);
-        worker.startTask(TASK_ID, anyConnectorConfigMap(), origProps, taskStatusListener, TargetState.STARTED);
+        worker.startTask(TASK_ID, ClusterConfigState.EMPTY, anyConnectorConfigMap(), origProps, taskStatusListener, TargetState.STARTED);
         assertStatistics(worker, 0, 1);
         worker.stop();
         assertStatistics(worker, 0, 0);
@@ -721,6 +724,7 @@ public void testConverterOverrides() throws Exception {
                 anyObject(OffsetStorageReader.class),
                 anyObject(OffsetStorageWriter.class),
                 anyObject(WorkerConfig.class),
+                anyObject(ClusterConfigState.class),
                 anyObject(ConnectMetrics.class),
                 EasyMock.eq(pluginLoader),
                 anyObject(Time.class),
@@ -784,7 +788,7 @@ public void testConverterOverrides() throws Exception {
         connProps.put(""key.converter.extra.config"", ""foo"");
         connProps.put(ConnectorConfig.VALUE_CONVERTER_CLASS_CONFIG, TestConfigurableConverter.class.getName());
         connProps.put(""value.converter.extra.config"", ""bar"");
-        worker.startTask(TASK_ID, connProps, origProps, taskStatusListener, TargetState.STARTED);
+        worker.startTask(TASK_ID, ClusterConfigState.EMPTY, connProps, origProps, taskStatusListener, TargetState.STARTED);
         assertStatistics(worker, 0, 1);
         assertEquals(new HashSet<>(Arrays.asList(TASK_ID)), worker.taskIds());
         worker.stopAndAwaitTask(TASK_ID);
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java
index d7a7d87cb4f..911afe7ec2f 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java
@@ -214,7 +214,7 @@ public void testJoinAssignment() throws Exception {
         EasyMock.expect(worker.isRunning(CONN1)).andReturn(true);
 
         EasyMock.expect(worker.connectorTaskConfigs(CONN1, conn1SinkConfig)).andReturn(TASK_CONFIGS);
-        worker.startTask(EasyMock.eq(TASK1), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
+        worker.startTask(EasyMock.eq(TASK1), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
                 EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));
         PowerMock.expectLastCall().andReturn(true);
         member.poll(EasyMock.anyInt());
@@ -241,7 +241,7 @@ public void testRebalance() throws Exception {
         PowerMock.expectLastCall().andReturn(true);
         EasyMock.expect(worker.isRunning(CONN1)).andReturn(true);
         EasyMock.expect(worker.connectorTaskConfigs(CONN1, conn1SinkConfig)).andReturn(TASK_CONFIGS);
-        worker.startTask(EasyMock.eq(TASK1), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
+        worker.startTask(EasyMock.eq(TASK1), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
                 EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));
         PowerMock.expectLastCall().andReturn(true);
         member.poll(EasyMock.anyInt());
@@ -288,7 +288,7 @@ public void testRebalanceFailedConnector() throws Exception {
         PowerMock.expectLastCall().andReturn(true);
         EasyMock.expect(worker.isRunning(CONN1)).andReturn(true);
         EasyMock.expect(worker.connectorTaskConfigs(CONN1, conn1SinkConfig)).andReturn(TASK_CONFIGS);
-        worker.startTask(EasyMock.eq(TASK1), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
+        worker.startTask(EasyMock.eq(TASK1), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
                 EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));
         PowerMock.expectLastCall().andReturn(true);
         member.poll(EasyMock.anyInt());
@@ -756,7 +756,7 @@ public void testRestartTask() throws Exception {
         expectPostRebalanceCatchup(SNAPSHOT);
         member.poll(EasyMock.anyInt());
         PowerMock.expectLastCall();
-        worker.startTask(EasyMock.eq(TASK0), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
+        worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
                 EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));
         PowerMock.expectLastCall().andReturn(true);
 
@@ -770,7 +770,7 @@ public void testRestartTask() throws Exception {
 
         worker.stopAndAwaitTask(TASK0);
         PowerMock.expectLastCall();
-        worker.startTask(EasyMock.eq(TASK0), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
+        worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
                 EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));
         PowerMock.expectLastCall().andReturn(true);
 
@@ -820,10 +820,10 @@ public void testRestartUnknownTask() throws Exception {
 
     @Test
     public void testRequestProcessingOrder() throws Exception {
-        final DistributedHerder.HerderRequest req1 = herder.addRequest(100, null, null);
-        final DistributedHerder.HerderRequest req2 = herder.addRequest(10, null, null);
-        final DistributedHerder.HerderRequest req3 = herder.addRequest(200, null, null);
-        final DistributedHerder.HerderRequest req4 = herder.addRequest(200, null, null);
+        final DistributedHerder.DistributedHerderRequest req1 = herder.addRequest(100, null, null);
+        final DistributedHerder.DistributedHerderRequest req2 = herder.addRequest(10, null, null);
+        final DistributedHerder.DistributedHerderRequest req3 = herder.addRequest(200, null, null);
+        final DistributedHerder.DistributedHerderRequest req4 = herder.addRequest(200, null, null);
 
         assertEquals(req2, herder.requests.pollFirst()); // lowest delay
         assertEquals(req1, herder.requests.pollFirst()); // next lowest delay
@@ -1080,7 +1080,7 @@ public void testUnknownConnectorPaused() throws Exception {
         // join
         expectRebalance(1, Collections.<String>emptyList(), singletonList(TASK0));
         expectPostRebalanceCatchup(SNAPSHOT);
-        worker.startTask(EasyMock.eq(TASK0), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
+        worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
                 EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));
         PowerMock.expectLastCall().andReturn(true);
         member.poll(EasyMock.anyInt());
@@ -1117,7 +1117,7 @@ public void testConnectorPausedRunningTaskOnly() throws Exception {
         // join
         expectRebalance(1, Collections.<String>emptyList(), singletonList(TASK0));
         expectPostRebalanceCatchup(SNAPSHOT);
-        worker.startTask(EasyMock.eq(TASK0), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
+        worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
                 EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));
         PowerMock.expectLastCall().andReturn(true);
         member.poll(EasyMock.anyInt());
@@ -1157,7 +1157,7 @@ public void testConnectorResumedRunningTaskOnly() throws Exception {
         // join
         expectRebalance(1, Collections.<String>emptyList(), singletonList(TASK0));
         expectPostRebalanceCatchup(SNAPSHOT_PAUSED_CONN1);
-        worker.startTask(EasyMock.eq(TASK0), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
+        worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
                 EasyMock.eq(herder), EasyMock.eq(TargetState.PAUSED));
         PowerMock.expectLastCall().andReturn(true);
         member.poll(EasyMock.anyInt());
@@ -1210,7 +1210,7 @@ public void testTaskConfigAdded() {
         expectRebalance(Collections.<String>emptyList(), Collections.<ConnectorTaskId>emptyList(),
                 ConnectProtocol.Assignment.NO_ERROR, 1, Collections.<String>emptyList(),
                 Arrays.asList(TASK0));
-        worker.startTask(EasyMock.eq(TASK0), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
+        worker.startTask(EasyMock.eq(TASK0), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
                 EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));
         PowerMock.expectLastCall().andReturn(true);
         member.poll(EasyMock.anyInt());
@@ -1250,7 +1250,7 @@ public void testJoinLeaderCatchUpFails() throws Exception {
         PowerMock.expectLastCall().andReturn(true);
         EasyMock.expect(worker.getPlugins()).andReturn(plugins);
         EasyMock.expect(worker.connectorTaskConfigs(CONN1, conn1SinkConfig)).andReturn(TASK_CONFIGS);
-        worker.startTask(EasyMock.eq(TASK1), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
+        worker.startTask(EasyMock.eq(TASK1), EasyMock.<ClusterConfigState>anyObject(), EasyMock.<Map<String, String>>anyObject(), EasyMock.<Map<String, String>>anyObject(),
                 EasyMock.eq(herder), EasyMock.eq(TargetState.STARTED));
         PowerMock.expectLastCall().andReturn(true);
         EasyMock.expect(worker.isRunning(CONN1)).andReturn(true);
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerderTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerderTest.java
index fd330f28009..5372a3a27a5 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerderTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/standalone/StandaloneHerderTest.java
@@ -37,6 +37,7 @@
 import org.apache.kafka.connect.runtime.TaskStatus;
 import org.apache.kafka.connect.runtime.Worker;
 import org.apache.kafka.connect.runtime.WorkerConnector;
+import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;
 import org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader;
 import org.apache.kafka.connect.runtime.isolation.PluginClassLoader;
 import org.apache.kafka.connect.runtime.isolation.Plugins;
@@ -68,6 +69,7 @@
 import java.util.Collection;
 import java.util.Collections;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.ExecutionException;
@@ -325,6 +327,7 @@ public void testRestartConnectorFailureOnStart() throws Exception {
         PowerMock.verifyAll();
     }
 
+
     @Test
     public void testRestartTask() throws Exception {
         ConnectorTaskId taskId = new ConnectorTaskId(CONNECTOR_NAME, 0);
@@ -337,7 +340,14 @@ public void testRestartTask() throws Exception {
         worker.stopAndAwaitTask(taskId);
         EasyMock.expectLastCall();
 
-        worker.startTask(taskId, connectorConfig, taskConfig(SourceSink.SOURCE), herder, TargetState.STARTED);
+        ClusterConfigState configState = new ClusterConfigState(
+                -1,
+                Collections.singletonMap(CONNECTOR_NAME, 1),
+                Collections.singletonMap(CONNECTOR_NAME, connectorConfig),
+                Collections.singletonMap(CONNECTOR_NAME, TargetState.STARTED),
+                Collections.singletonMap(taskId, taskConfig(SourceSink.SOURCE)),
+                new HashSet<>());
+        worker.startTask(taskId, configState, connectorConfig, taskConfig(SourceSink.SOURCE), herder, TargetState.STARTED);
         EasyMock.expectLastCall().andReturn(true);
 
         PowerMock.replayAll();
@@ -363,7 +373,14 @@ public void testRestartTaskFailureOnStart() throws Exception {
         worker.stopAndAwaitTask(taskId);
         EasyMock.expectLastCall();
 
-        worker.startTask(taskId, connectorConfig, taskConfig(SourceSink.SOURCE), herder, TargetState.STARTED);
+        ClusterConfigState configState = new ClusterConfigState(
+                -1,
+                Collections.singletonMap(CONNECTOR_NAME, 1),
+                Collections.singletonMap(CONNECTOR_NAME, connectorConfig),
+                Collections.singletonMap(CONNECTOR_NAME, TargetState.STARTED),
+                Collections.singletonMap(new ConnectorTaskId(CONNECTOR_NAME, 0), taskConfig(SourceSink.SOURCE)),
+                new HashSet<>());
+        worker.startTask(taskId, configState, connectorConfig, taskConfig(SourceSink.SOURCE), herder, TargetState.STARTED);
         EasyMock.expectLastCall().andReturn(false);
 
         PowerMock.replayAll();
@@ -597,7 +614,14 @@ private void expectAdd(SourceSink sourceSink) throws Exception {
         EasyMock.expect(worker.connectorTaskConfigs(CONNECTOR_NAME, connConfig))
             .andReturn(singletonList(generatedTaskProps));
 
-        worker.startTask(new ConnectorTaskId(CONNECTOR_NAME, 0), connectorConfig(sourceSink), generatedTaskProps, herder, TargetState.STARTED);
+        ClusterConfigState configState = new ClusterConfigState(
+                -1,
+                Collections.singletonMap(CONNECTOR_NAME, 1),
+                Collections.singletonMap(CONNECTOR_NAME, connectorConfig(sourceSink)),
+                Collections.singletonMap(CONNECTOR_NAME, TargetState.STARTED),
+                Collections.singletonMap(new ConnectorTaskId(CONNECTOR_NAME, 0), generatedTaskProps),
+                new HashSet<>());
+        worker.startTask(new ConnectorTaskId(CONNECTOR_NAME, 0), configState, connectorConfig(sourceSink), generatedTaskProps, herder, TargetState.STARTED);
         EasyMock.expectLastCall().andReturn(true);
 
         EasyMock.expect(herder.connectorTypeForClass(BogusSourceConnector.class.getName()))
diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaConfigBackingStoreTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaConfigBackingStoreTest.java
index aac1b78c918..ed62d9b6f01 100644
--- a/connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaConfigBackingStoreTest.java
+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/storage/KafkaConfigBackingStoreTest.java
@@ -146,7 +146,7 @@
 
     @Before
     public void setUp() {
-        configStorage = PowerMock.createPartialMock(KafkaConfigBackingStore.class, new String[]{""createKafkaBasedLog""}, converter, DEFAULT_DISTRIBUTED_CONFIG);
+        configStorage = PowerMock.createPartialMock(KafkaConfigBackingStore.class, new String[]{""createKafkaBasedLog""}, converter, DEFAULT_DISTRIBUTED_CONFIG, null);
         Whitebox.setInternalState(configStorage, ""configLog"", storeLog);
         configStorage.setUpdateListener(configUpdateListener);
     }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[KIP-580] Client Exponential Backoff Implementation,KAFKA-9800,13295627,New Feature,In Progress,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,d8tltanc,d8tltanc,d8tltanc,01/Apr/20 20:41,26/Aug/20 14:42,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,KIP-580,,,,"Design:

The main idea is to bookkeep the failed attempt. Currently, the retry backoff has two main usage patterns:
 # Synchronous retires and blocking loop. The thread will sleep in each iteration for retry backoff ms.
 # Async retries. In each polling, the retries do not meet the backoff will be filtered. The data class often maintains a 1:1 mapping to a set of requests which are logically associated. (i.e. a set contains only one initial request and only its retries.)

For type 1, we can utilize a local failure counter of a Java generic data type.

For case 2, I already wrapped the exponential backoff/timeout util class in my KIP-601 [implementation|https://github.com/apache/kafka/pull/8683/files#diff-9ca2b1294653dfa914b9277de62b52e3R28] which takes the number of attempts and returns the backoff/timeout value at the corresponding level. Thus, we can add a new class property to those classes containing retriable data in order to record the number of failed attempts.

 

Changes:

KafkaProducer:
 # Produce request (ApiKeys.PRODUCE). Currently, the backoff applies to each ProducerBatch in Accumulator, which already has an attribute attempts recording the number of failed attempts. So we can let the Accumulator calculate the new retry backoff for each bach when it enqueues them, to avoid instantiate the util class multiple times.
 # Transaction request (ApiKeys..*TXN). TxnRequestHandler will have a new class property of type `Long` to record the number of attempts.

KafkaConsumer:
 # Some synchronous retry use cases. Record the failed attempts in the blocking loop.
 # Partition request (ApiKeys.OFFSET_FOR_LEADER_EPOCH, ApiKeys.LIST_OFFSETS). Though the actual requests are packed for each node, the current implementation is applying backoff to each topic partition, where the backoff value is kept by TopicPartitionState. Thus, TopicPartitionState will have the new property recording the number of attempts.

Metadata:
 #  Metadata lives as a singleton in many clients. Add a new property recording the number of attempts

 AdminClient:
 # AdminClient has its own request abstraction Call. The failed attempts are already kept by the abstraction. So probably clean the Call class logic a bit.

Existing tests:
 # If the tests are testing the retry backoff, add a delta to the assertion, considering the existence of the jitter.
 # If the tests are testing other functionality, we can specify the same value for both `retry.backoff.ms` and `retry.backoff.max.ms` in order to make the retry backoff static. We can use this trick to make the existing tests compatible with the changes.

There're other common usages look like client.poll(timeout), where the timeout passed in is the retry backoff value. We won't change these usages since its underlying logic is nioSelector.select(timeout) and nioSelector.selectNow(), which means if no interested op exists, the client will block retry backoff milliseconds. This is an optimization when there's no request that needs to be sent but the client is waiting for responses. Specifically, if the client fails the inflight requests before the retry backoff milliseconds passed, it still needs to wait until that amount of time passed, unless there's a new request need to be sent.

 ",,d8tltanc,githubbot,ijuma,skaundinya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-04-03 21:14:31.529,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 05:14:31 UTC 2020,,,,,,,"0|z0d6l4:",9223372036854775807,,,,,,,,,,,,,,,,"03/Apr/20 21:14;githubbot;d8tltanc commented on pull request #8421: [WIP]KAFKA-9800: [KIP-580] Admin Client Exponential Backoff Implementation
URL: https://github.com/apache/kafka/pull/8421
 
 
   Refactored Call Class
   Split retry() and fail()
   Hardcoded the retryBackoffMaxMs for now to pass the existing tests
   
   *More detailed description of your change,
   
   *Summary of testing strategy (including rationale)
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","02/Jun/20 06:14;d8tltanc;Recap the discussion in Github. We want to implement a per-request backoff for all types of clients.

 

Let me talk about two of my major concerns and thoughts about implementing the universal client exponential backoff.

 

**AdminClient logic redundancy**

NetworkClient has request timeout handlers. Producer / Consumer are using NetworkClient to help handle timeout but AdminClient doesn’t. The reason, to my understanding, is that AdminClient is implementing the per-request timeout.

For example,

1. Wrapping the request builder into a new class `Call`, (the construction lambda adds tons of lines into the AdminClient.java, which should probably have been living in each AbstractRequest implementation classes files)
2. Re-writing the request queues for different request status, while normal clients are fully using the NetworkClient.

After we add support to the per-request retry backoff to all clients, we can implement the per-request timeout together by the way. Thus we can clean up the redundant request handling logic in AdminClient.

Are we considering refactoring the AdminClient further and remove all the redundant logic which should have belonged to the networking layer and the AbstractRequest implementation classes?

**Flexible backoff modes**

Let's analyze the request backoff demands of all the types of clients at this point. In my opinion, there are simply two:

1. Requests do not need exponential backoff. These requests need to be sent ASAP to avoid dataflow performance degradation, such as the `ProduceRequest` and its related/preceding metadata requests.

2. Request do need exponential backoff. These requests are “second-class citizens” and can be throttled to avoid request storms on the broker side. Such as metadata related requests in AdminClient.

Now the question comes. Even when two requests are of the same request type, one may have to get sent ASAP while the other one may wait, depending on the use case. We need to think deeper about how to make a classification.

But the implementation would be simple. We can utilize the existing builder pattern AbstractRequest and build the request flexibly upon a given retry_backoff mode. For example,

1. AbstractRequest.Builder will interact with a new abstract class specifying the retry_backoff option, static or exponential. 
2. AbstractRequest will have some new interfaces controlling the backoff.

Then, we can control if the request should have a static backoff or an exponential backoff when we construct each implementation instance of AbstractRequest.Builder.


I'll include more details in the Jira ticket and rewrite this PR. Before we talk more about the code details and start the new implementation, please let me know what you think about the AdminClient refactor and static/exponential retry_backoff classification rule.","02/Jun/20 15:31;ijuma;I think you should apply the same backoff strategy for all request types. I don't see much benefit in the more complex approach.","02/Jun/20 21:46;skaundinya;Here are my thoughts:
 1) There should be no changes to the underlying NetworkClient interface. More details on what interfaces should change are outlined in the KIP here: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-580%3A+Exponential+Backoff+for+Kafka+Clients|https://cwiki.apache.org/confluence/display/KAFKA/KIP-580%3A+Exponential+Backoff+for+Kafka+Clients).]. As [~d8tltanc] mentioned, the key difference between the AdminClient and the other clients is that the AdminClient supports a per request timeout whereas the Producer and Consumer are using the NetworkClient to handle the timeouts. In our implementation we should add a per-request support for all clients and reuse the code existing in CallRetryContext to apply to all clients to do this.

2) I agree with [~ijuma], I don't see really much of a benefit for doing a different backoff strategy per client, plus that kind of improvement goes beyond the scope of this KIP.

 

[~d8tltanc] could you shed more light on how to refactor the AdminClient to take out the redundant logic? I want to understand how much this would change and if all this change would still be in the scope of the KIP.","02/Jun/20 22:05;ijuma;For a bit more context, the plan was to add per request timeout support to NetworkClient and migrate AdminClient to use it. I had the start of a PR (([https://github.com/apache/kafka/pull/3503/|https://github.com/apache/kafka/pull/3503/files]), but we then decided to go with a simpler approach that was good enough for what the Consumer needed. Tackling that may increase the scope of this work by quite a bit though. It may be worth checking if we can encapsulate the logic for exponential backoff in separate classes and then use it from where retries are handled today for the relevant clients.

And tackle the consolidation of per request timeouts in a separate JIRA.","03/Jun/20 05:14;d8tltanc;[~skaundinya] [~ijuma]

Thanks for the reply. Let's limit the patch scope to the original proposal. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configurable TCP connection timeout and improve the initial metadata fetch,KAFKA-9893,13299806,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Implemented,d8tltanc,d8tltanc,d8tltanc,20/Apr/20 21:20,26/Aug/20 14:42,12/Jan/21 10:06,01/Jul/20 22:08,2.7.0,,,,,,,2.7.0,,,core,,,,,,0,,,,,"This issue has two parts:
 # Support transportation layer connection timeout described in KIP-601
 # Optimize the logic for NetworkClient.leastLoadedNode()

Changes:
 # Added a new common client configuration parameter socket.connection.setup.timeout.ms to the NetworkClient. Handle potential transportation layer timeout using the same approach as it handling potential request timeout.
 # When no connected channel exists, leastLoadedNode() will now provide a disconnected node that has the least number of failed attempts. 
 # ClusterConnectionStates will keep the connecting node ids. Now it also has several new public methods to provide per connection relavant data.",,d8tltanc,jbfletch,waewoo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-04-20 21:20:51.0,,,,,,,"0|z0dv94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add TRACE-level end-to-end latency metrics to Streams,KAFKA-10054,13307836,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,ableegoldman,ableegoldman,27/May/20 20:17,26/Aug/20 14:42,12/Jan/21 10:06,25/Aug/20 00:49,,,,,,,,2.7.0,,,streams,,,,,,0,,,,,,,ableegoldman,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-05-27 20:17:08.0,,,,,,,"0|z0f8hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snapshotting API for State Stores,KAFKA-9986,13304563,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,nizhikov,nizhikov,nizhikov,13/May/20 08:23,25/Aug/20 15:02,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,need-kip,streams,,,"The parent ticket is KAFKA-3184.

The goal of this ticket is to provide a general snapshotting API for state stores in Streams (not only for in-memory but also for persistent stores), where the snapshot location can be either local disks or remote storage.

Design scope is primarily on:
 # the API design for both checkpointing as well as loading checkpoints into the local state stores
 # the mechanism of the checkpointing, e.g. whether it should be async? whether it should be executed on separate threads? etc.",,ableegoldman,mjsax,nizhikov,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-3184,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-05-28 15:53:57.594,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 07 04:40:33 UTC 2020,,,,,,,"0|z0eoa8:",9223372036854775807,,,,,,,,,,,,,,,,"28/May/20 15:53;vvcephei;Hi [~nizhikov] ,

Thanks for getting involved in this! It's something I've been wanting to see progress on for at least a year, so I'm really happy to see that you're interested in it, too. I know multiple people who would dearly love to see this feature in Streams.

One high-level request: can we call these things ""*Snapshots*"", not ""Checkpoints"" to avoid confusion with respect to the current ""checkpoint"" concept in Streams?

I'm not sure if you have already done it, but I'd highly recommend that you look at two sources of related work:
 # Other stateful data processing systems. Bulk processing systems may not be that relevant, and Streams has some unique features that may render other stream processing systems also not that relevant, but it's still worth taking the time to understand how systems that _do_ allow remote snapshotting actually manage those snapshots.
 # Other distributed databases. This may seem like a strange statement, but if you squint at it, you'll see that state stores in Streams _are_ distributed databases, following the primary/replica pattern, which use the changelog both for replication and for durability. This means that understanding the best snapshot/restore mechanisms from the distributed database world will be deeply helpful in developing a good design for Streams.

Incidentally, we did similar Related Work research while designing KIP-441 ([https://cwiki.apache.org/confluence/display/KAFKA/KIP-441:+Smooth+Scaling+Out+for+Kafka+Streams),] for very similar reasons. You might want to start by looking at the systems on our list.

 

One thing to bear in mind while you look at related systems is that Streams has one feature that renders it unique in both related domains. Namely, the changelog itself. The changelog topic represents a linearized history of the state store, with the offsets representing unique points in the history of the store. The changelog is stored durably external to Streams, and it's compacted with infinite retention.

This has a few important implications:
 * We do not need snapshot/restore to recover from disasters. Typically, this mechanism is used in data systems to recover from the loss of too many nodes in the cluster. However, Streams is already resilient to the loss of the entire cluster. For us, snapshot/restore is purely an optimization: copying filesystem objects to and from remote blob storage is likely to be faster than replaying the changelog for very large stores.
 * We do not need any special algorithms to keep coherent snapshots. As long as we store the snapshot along with the ""checkpoint"" information (the changelog topic, partition, and offset), we can replay any subsequent state updates from the changelog and return the system to _exactly_ where it left off processing its inputs. Other systems need to implement some form of the Chandy/Lamport Distributed Snapshot algorithm in order to capture coherent snapshots, but we get it essentially ""for free"".

 

Another important differentiating factor specifically as you look at distributed databases for comparison is that Streams is more like a distributed database _engine_ than ""just"" a distributed database itself. Most databases just have one storage format. For example, Cassandra stores its data in SSTables, Elasticsearch uses Lucene indices, etc. These systems can craft their snapshot/restore mechanism in full knowledge of the storage format.

On the other hand, Streams allows you to plug in multiple, custom, storage formats. For an efficient snapshot/restore, my impression is that you really need to deal with the low level format. For example, if we just iterate over a whole RocksDB store to copy it into a flat file for every snapshot, it's going to be _way_ slower and more bloated than if we just directly copy around SST files, and only copy the SST files that changed from previous snapshots.

It seems like we would need to design the system with two components, then. One is a way to keep track of the metadata: which snapshots are stored where, what offset they're at, etc. The other is actually performing the snapshot (and recovery), which would have a separate implementation for each store type. So, we'd have one for in-memory stores and another for rocksdb stores, and if people provide custom stores, they should also be able to implement the snapshotting logic for them.

 

Needless to say, the whole scope of this is quite large, and I think a good approach would be to get a general idea of how we want to structure the whole system, and then just implement a part of it. For example, (if you agree with the two part system I described in the last paragraph) maybe the initial implementation of the snapshot metadata component would only handle local filesystem locations, and maybe we would only implement snapshot/restore for in-memory stores. Maybe we wouldn't even bother with incremental snapshots for the first version of in-memory store snapshotting. As we expand the implementation to cover more of the full scope, we might discover that the design needs to be modified, which is perfectly fine.

 

I hope this helps!

-John","07/Jul/20 04:40;mjsax;> where the snapshot location can be either local disks or remote storage

Why local disk? What would be gain by that compared to using StandbyTasks?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide an officially supported Node.js client,KAFKA-10415,13323370,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,madams,madams,18/Aug/20 14:31,18/Aug/20 19:43,12/Jan/21 10:06,,,,,,,,,,,,clients,,,,,,3,,,,,"Please provide an official Node.js client for Kafka at feature parity with all of the other officially supported & provided Kafka clients.

It is extremely confusing when it comes to trying to use Kafka in the Node.js ecosystem.  There are many clients, some look legitimate ([http://kafka.js.org),|http://kafka.js.org%29%2C/] but some are woefully out of date (many listed at [https://cwiki.apache.org/confluence/display/KAFKA/Clients#Clients-Node.js]), and others have confusing relationships among them ([https://github.com/nodefluent/node-sinek] & [https://github.com/nodefluent/kafka-streams]).  Most of them are publicly asking for help.  This leaves teams having to waste time trying to figure out which client has the Kafka features they need (mostly talking about streaming here), and which client has high quality and will be around in the future.  If the client came directly from this project, those decisions would be made and we could get on about our work.

JavaScript is on the of the most popular languages on the planet, and the Node.js user base is huge – big enough that a Node.js client provided directly by the Kafka team is justified.  The list at [https://cwiki.apache.org/confluence/display/KAFKA/Clients#Clients-Node.js] doesn't even mention what is perhaps the most confidence-inducing Node.js client thanks to its documentation, [https://kafka.js.org.|https://kafka.js.org./]  The list at [https://docs.confluent.io/current/clients/index.html#ak-clients] includes an officially-supported Go language client; Go's community is dwarfed by that of Node.js.",,ableegoldman,gladiator77,ijuma,jeqo,madams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-08-18 18:14:18.027,,,false,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,Tue Aug 18 19:43:54 UTC 2020,,,,,,,"0|z0hvuw:",9223372036854775807,,,,,,,,,,,,,,,,"18/Aug/20 18:14;ijuma;Thanks for the JIRA. One clarification, Apache Kafka doesn't have an officially supported Go language client. The link you referenced is from Confluent, not Apache Kafka.

Apache Kafka ships a Java client only.

 ","18/Aug/20 19:43;madams;[~ijuma] I understand.  Should I have filed an issue with Confluent instead, then?  If so, can you point me to an issue tracker for Confluent where I could file this?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide an officially supported Deno client,KAFKA-10416,13323371,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,madams,madams,18/Aug/20 14:33,18/Aug/20 14:33,12/Jan/21 10:06,,,,,,,,,,,,clients,,,,,,1,,,,,"This is a similar request to https://issues.apache.org/jira/browse/KAFKA-10415 for a JavaScript client for Kafka, only packaged for [https://deno.land|https://deno.land/] .",,madams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,2020-08-18 14:33:17.0,,,,,,,"0|z0hvv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New Kafka Connect SMT for plainText => Struct(or Map),KAFKA-9436,13279557,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,whsoul82,whsoul82,15/Jan/20 10:30,14/Aug/20 05:05,12/Jan/21 10:06,,,,,,,,,,,,KafkaConnect,,,,,,0,needs-kip,,,,"I'd like to parse and convert plain text rows to struct(or map) data, and load into documented database such as mongoDB, elasticSearch, etc... with SMT

 

For example

 

1. String parse ( with timemillis )
{code:java}
{
   ""code"" : ""dev_kafka_pc001_1580372261372""
   ,""recode1"" : ""a""
   ,""recode2"" : ""b"" 
}{code}
{code:java}
""transforms"": ""RegexTransform"",
""transforms.RegexTransform.type"": ""org.apache.kafka.connect.transforms.ToStructByRegexTransform$Value"",

""transforms.RegexTransform.struct.field"": ""message"",
""transforms.RegexTransform.regex"": ""^(.{3,4})_(.*)_(pc|mw|ios|and)([0-9]{3})_([0-9]{13})"" ""transforms.RegexTransform.mapping"": ""env,serviceId,device,sequence,datetime:TIMEMILLIS""{code}
 

 

2. plain text apache log
{code:java}
""111.61.73.113 - - [08/Aug/2019:18:15:29 +0900] \""OPTIONS /api/v1/service_config HTTP/1.1\"" 200 - 101989 \""http://local.test.com/\"" \""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36\""""
{code}
SMT connect config with regular expression below can easily transform a plain text to struct (or map) data.

 
{code:java}
""transforms"": ""RegexTransform"",
""transforms.RegexTransform.type"": ""org.apache.kafka.connect.transforms.ToStructByRegexTransform$Value"",

""transforms.RegexTransform.struct.field"": ""message"",
""transforms.RegexTransform.regex"": ""^([\\d.]+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \""(GET|POST|OPTIONS|HEAD|PUT|DELETE|PATCH) (.+?) (.+?)\"" (\\d{3}) ([0-9|-]+) ([0-9|-]+) \""([^\""]+)\"" \""([^\""]+)\""""

""transforms.RegexTransform.mapping"": ""IP,RemoteUser,AuthedRemoteUser,DateTime,Method,Request,Protocol,Response,BytesSent,Ms:NUMBER,Referrer,UserAgent""
{code}
 

I have PR about this",,whsoul82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-01-15 10:30:00.0,,,,,,,"0|z0aj3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add method for getting last record offset in kafka partition,KAFKA-10009,13305359,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,lmnet,lmnet,16/May/20 08:25,08/Aug/20 16:05,12/Jan/21 10:06,,,,,,,,,,,,clients,consumer,,,,,0,,,,,"As far as I understand, at the current moment, there is no reliable way for getting offset of the last record in the partition using java client. There is {{endOffsets}} method in the consumer. And usually {{endOffsets - 1}} works fine. But in the case of transactional producer, topic may contain offsets without a record. And {{endOffsets - 1}} will point to the offset without record.

This feature will help in situations when consumer application wants to consume the whole topic. Checking of beginning and last record offset will give lower and upper bounds for consuming. Of course, it is doable with the current consumer implementation, but I need to check {{position}} after each poll.

Also, I believe that this feature may help with monitoring and operations.",,chia7712,lmnet,wdaehn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-05-16 11:36:16.537,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 08 16:05:29 UTC 2020,,,,,,,"0|z0et74:",9223372036854775807,,,,,,,,,,,,,,,,"16/May/20 11:36;chia7712;If you want to seek to the end offsets in case of transaction, you can set isolation.level to READ_COMMITTED and then call Consumer.seekToEnd or Consumer.endOffsets. Both of them will get last stable offset.","16/May/20 11:42;lmnet;Even with read_committed isolation level {{endOffsets - 1}} will not give you last record offset if producer is transactional. It will point to an offset without a record. I specifically tested this behavior (on 2.4.0). Also, isolation level affects only pending transactions. ","16/May/20 12:47;chia7712;So you want to get end offset which is associated to a true record and the LSO, which maybe a smallest offset of open transaction, is not what you expect.

 ","16/May/20 12:58;lmnet;Exactly. I want an offset of a last true record in a partition.","08/Aug/20 14:40;wdaehn;It is actually worse than that.

There could be log compaction or other reasons why the offset value is not a dense number set.

Without such functionality, how do you reliably know that you have read all data from the topic?
 * Option 1: execute a poll(1second) and if it returns no data, that means there is no more data. But maybe the network was busy, so 1 second is not enough. 10 seconds? 1 minute, 10 minutes? I don't want to wait for ten minutes just to decrease the probability there is more data and I just have not received it yet.
 * Option 2: First call the endOffset(), then you know the high water mark is offset=100. So you poll until you received the record with offset=99 and then you know you have gotten the last record. But what if there is no record with offset=99? Again, you will wait forever.","08/Aug/20 16:05;wdaehn;What would be needed is either
 * endOffsets() returns the last existing offset. A bit dangerous as it might return offset 100, then a log compaction happens and then we start reading.
 * poll() telling that it retrieved the last record. That should be doable. Then we can call poll(100ms) in a loop until it tells us no-more-data via another getter. If I am interested in the current records only I stop polling now, all others will simply continue calling poll(). And the best of it, no side effects and backward compatibility. 

The one thing I don't know is if poll even has a chance to get that information yet or if the broker must be changed as well.

 

I have seen many similar questions and no real solution, so this is a popular request.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce globally consistent checkpoint in Kafka Streams,KAFKA-3672,12965775,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,guozhang,guozhang,06/May/16 23:37,29/Jul/20 20:05,12/Jan/21 10:06,,0.10.0.0,,,,,,,,,,streams,,,,,,0,user-experience,,,,"This is originate from the idea of rethinking about the checkpoint file creation condition:

Today the checkpoint file containing the checkpointed offsets is written upon stream task clean shutdown, and is read and deleted upon stream task (re-)construction. The rationale is that if upon task re-construction, the checkpoint file is missing, it indicates that the underlying persistent state store (rocksDB, for example)'s state may not be consistent with the committed offsets, and hence we'd better to wipe-out the maybe-broken state storage and rebuild from the beginning of the offset.

However, we may able to do better than this setting if we can fully control the persistent store flushing time to be aligned with committing, and hence as long as we commit, we are always guaranteed to get a clear checkpoint.

This may be generalized to a ""global state checkpoint"" mechanism in Kafka Streams, which may also subsume KAFKA-3184 for non persistent stores.",,guozhang,hongyu.bi,leyncl,mjsax,santoshjoshi2003@gmail.com,scosenza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-05-06 23:37:40.0,,,,,,,"0|i2xamf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Support for Quorum-based Producer Acknowledgment,KAFKA-6477,13133269,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,bryan.deng,bryan.deng,24/Jan/18 04:30,26/Jul/20 20:28,12/Jan/21 10:06,,,,,,,,,,,,controller,producer ,,,,,2,,,,,"Hey folks. I would like to add a feature to support the quorum-based acknowledgment for the producer request. We have been running a modified version of Kafka on our testing cluster for weeks, the improvement of P999 is significant with very stable latency. Additionally, I have a proposal to achieve a similar data durability as with the insync.replicas-based acknowledgment through LEO-based leader election.

[https://cwiki.apache.org/confluence/display/KAFKA/KIP-250+Add+Support+for+Quorum-based+Producer+Acknowledge]",,adamhutson,bryan.deng,cchepelov,iksaif,jeffwidman,tmichelet,xmtsui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-11-29 09:04:02.82,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 26 20:28:13 UTC 2020,,,,,,,"0|i3paaf:",9223372036854775807,,,,,,,,,,,,,,,,"29/Nov/18 09:04;iksaif;[~bryan.deng] is this still on your radar ? I'd be interested to try.","30/Nov/18 03:49;bryan.deng;[~iksaif] feel free to take it, there are already some discussions here [https://mail-archives.apache.org/mod_mbox/kafka-dev/201802.mbox/thread?1.] Also, I know community is/was considering the redesign of Kafka controller, you may want to sync-up with them.","30/Nov/18 12:41;iksaif;I meant ""I would be interested to try any patch"" :) I'll follow what is happening in this area and see if I can manage to find some time to help","26/Jul/20 20:28;bryan.deng;I posted my [PR|[https://github.com/mangobatao/kafka/pull/1]] in my repo, which is based on 0.10.2.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Atomic commit of source connector records and offsets,KAFKA-10000,13305074,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,ChrisEgerton,ChrisEgerton,ChrisEgerton,14/May/20 23:19,23/Jul/20 17:27,12/Jan/21 10:06,,,,,,,,,,,,KafkaConnect,,,,,,1,needs-kip,,,,"It'd be nice to be able to configure source connectors such that their offsets are committed if and only if all records up to that point have been ack'd by the producer. This would go a long way towards EOS for source connectors.

 

This differs from https://issues.apache.org/jira/browse/KAFKA-6079, which is marked as {{WONTFIX}} since it only concerns enabling the idempotent producer for source connectors and is not concerned with source connector offsets.

This also differs from https://issues.apache.org/jira/browse/KAFKA-6080, which had a lot of discussion around allowing connector-defined transaction boundaries. The suggestion in this ticket is to only use source connector offset commits as the transaction boundaries for connectors; allowing connector-specified transaction boundaries can be addressed separately.",,ChrisEgerton,yangguo1220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-07-23 17:27:46.974,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 23 17:27:46 UTC 2020,,,,,,,"0|z0erfs:",9223372036854775807,,,,,,,,,,,,,,,,"23/Jul/20 17:27;yangguo1220;Hi Chris, the purpose of this ticket is very interesting. I wonder what is the priority in the overall Kafka Connect backlog, or how is the progress so far (needs-KIP)? Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Native Schema Registry in Kafka,KAFKA-3628,12962815,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,sriharsha,sriharsha,sriharsha,26/Apr/16 23:38,13/Jul/20 13:03,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,1,,,,,Instead of having external schema service. We can use topic config store the schema. I'll write detailed KIP.,,chia7712,florianschneider,marsishandsome,randerzander,sriharsha,sushant sood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-22 19:02:14.802,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 13 13:03:23 UTC 2020,,,,,,,"0|i2wse7:",9223372036854775807,,,,,,,,,,,,,,,,"30/Jun/16 00:15;sriharsha;draft version is here https://cwiki.apache.org/confluence/display/KAFKA/KIP-67+-+Kafka+Schema+Registry .
I'll be updating more detail in next few days.","22/Jan/17 19:02;sushant sood;Hi [~sriharsha] can you please share the updates on this Feature . ","13/Jul/20 13:03;florianschneider;What is the current state of affairs regarding this topic? ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KStream API support for multiple cluster broker,KAFKA-10219,13314187,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,sachinkurle,sachinkurle,30/Jun/20 02:05,05/Jul/20 00:55,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,needs-kip,,,,we are trying to consume from cluster A broker from KStream api and produce to cluster B broker.. we have configuration as boot strap server in consumer and producer configuration but kstream api is picking randomly bootstrap server cluster A or B,,bchen225242,cadonna,mjsax,sachinkurle,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-06-30 08:06:19.225,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 21:06:30 UTC 2020,,,,,,,"0|z0gbiw:",9223372036854775807,,,,,,,,,,,,,,,,"30/Jun/20 08:06;cadonna;[~sachinkurle] The current Kafka Streams documentation states

??Kafka Streams applications can only communicate with a single Kafka cluster specified by this config value.??

The mentioned config is {{bootstrap.servers.}}

See [https://kafka.apache.org/25/documentation/streams/developer-guide/config-streams.html#bootstrap-servers]","30/Jun/20 16:40;bchen225242;Thanks for the proposal, the use case you proposed is reasonable. However, we need to better clarify the feature we are going to introduce and the challenges we are facing, such as:



1. What does ""multiple clusters"" suggest? Do we support all input topics in cluster A and all output topics in cluster B, or a mixing of topics in random cluster A, B, C which needs to be automatically detected by Streams?
2. How do we allocate internal topics? Which cluster should the changelog/repartition topics go to, input topic cluster, or the output one?
3. How do we support Exactly-once? Right now the entire framework assumes a single cluster context. When switching to multiple cluster, we could no longer guarantee exactly-once because we may spam our transaction across multiple clusters, and we don't have a centralized coordinator to track the progress.","30/Jun/20 21:06;sachinkurle;I have simple use-case where I read off source topic from cluster A and write to target topic on cluster B

I am not sure on how transaction going across multiple cluster affects the exactly once.. even in case of single cluster my kstream app deployed on host server have to make network call to kafka cluster change log topic to get transaction details and update the offset topic. May be I am not fully aware of the internal working of kafka stateful transaction in case of Exactly-once",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TLSv1.3 by default and disable some of the older protocols,KAFKA-9320,13275515,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nizhikov,rsivaram,rsivaram,19/Dec/19 14:19,25/Jun/20 08:25,12/Jan/21 10:06,03/Jun/20 07:21,,,,,,,,2.6.0,,,security,,,,,,0,needs-kip,,,,KAFKA-7251 added support for TLSv1.3. We should include this in the list of protocols that are enabled by default. We should also disable some of the older protocols that are not secure. This change requires a KIP.,,nizhikov,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9943,,,,,,,,,,,,KAFKA-9319,,,,,,,"01/Jun/20 08:38;nizhikov;report.txt;https://issues.apache.org/jira/secure/attachment/13004474/report.txt",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2019-12-24 15:35:33.941,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 07:21:40 UTC 2020,,,,,,,"0|z09uhs:",9223372036854775807,,,,,,,,,,,,,,,,"24/Dec/19 15:35;nizhikov;Hello, [~rsivaram].

I wrote a KIP [1] for this issue.
Can you, please, take a look.

[1] https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=142641956","19/Feb/20 09:26;nizhikov;[~rsivaram] 

As you know, I checked system tests with the TLSv1.3 in the KAFKA-9319
It seems we are ready to enable TLSv1.3 by default.

What do you think?
Should I write the KIP and start the discussion?","19/Feb/20 12:00;rsivaram;[~nizhikov] Thanks for running the system tests using TLSv1.3. Yes, please write up the KIP and start the discussion on the mailing list to enable TLSv1.3 by default.","26/Mar/20 07:24;nizhikov;Hello [~rsivaram]

There is no feedback on the dev-list about this improvement.
Do we still want to make it?","01/Jun/20 08:42;nizhikov;I ran the following tests:

* tests/kafkatest/tests/tools/log4j_appender_test.py 
* tests/kafkatest/tests/core/upgrade_test.py 
* tests/kafkatest/tests/core/mirror_maker_test.py 
* tests/kafkatest/tests/core/consumer_group_command_test.py 
* tests/kafkatest/sanity_checks/test_console_consumer.py 
* tests/kafkatest/benchmarks/core/benchmark_test.py


{noformat}
====================================================================================================
SESSION REPORT (ALL TESTS)
ducktape version: 0.7.7
session_id:       2020-05-29--003
run time:         183 minutes 43.112 seconds
tests run:        121
passed:           120
failed:           1
ignored:          0
====================================================================================================
{noformat}

fail:
{noformat}
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.upgrade_test.TestUpgrade.test_upgrade.from_kafka_version=2.0.1.to_message_format_version=None.compression_types=.none
status:     FAIL
run time:   1 minute 21.075 seconds


    Kafka server didn't finish startup in 60 seconds
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 132, in run
    data = self.run_test()
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/tests/runner_client.py"", line 189, in run_test
    return self.test_context.function(self.test)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/mark/_mark.py"", line 428, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/opt/kafka-dev/tests/kafkatest/tests/core/upgrade_test.py"", line 149, in test_upgrade
    self.kafka.start()
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 254, in start
    Service.start(self)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/services/service.py"", line 234, in start
    self.start_node(node)
  File ""/opt/kafka-dev/tests/kafkatest/services/kafka/kafka.py"", line 377, in start_node
    err_msg=""Kafka server didn't finish startup in %d seconds"" % timeout_sec)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/cluster/remoteaccount.py"", line 705, in wait_until
    allow_fail=True) == 0, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/ducktape/utils/util.py"", line 41, in wait_until
    raise TimeoutError(err_msg() if callable(err_msg) else err_msg)
TimeoutError: Kafka server didn't finish startup in 60 seconds

----------------------------------------------------------------------------------------------------
{noformat}","01/Jun/20 08:43;nizhikov;Log from the failed test:

{noformat}
[2020-05-29 18:32:21,839] ERROR Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)
kafka.zookeeper.ZooKeeperClientTimeoutException: Timed out waiting for connection while in state: CONNECTING
        at kafka.zookeeper.ZooKeeperClient.$anonfun$waitUntilConnected$3(ZooKeeperClient.scala:230)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
        at kafka.utils.CoreUtils$.inLock(CoreUtils.scala:251)
        at kafka.zookeeper.ZooKeeperClient.waitUntilConnected(ZooKeeperClient.scala:226)
        at kafka.zookeeper.ZooKeeperClient.<init>(ZooKeeperClient.scala:95)
        at kafka.zk.KafkaZkClient$.apply(KafkaZkClient.scala:1588)
        at kafka.server.KafkaServer.createZkClient$1(KafkaServer.scala:348)
        at kafka.server.KafkaServer.initZkClient(KafkaServer.scala:372)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:202)
        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38)
        at kafka.Kafka$.main(Kafka.scala:75)
        at kafka.Kafka.main(Kafka.scala)
[2020-05-29 18:32:21,841] INFO shutting down (kafka.server.KafkaServer)
[2020-05-29 18:32:21,842] WARN  (kafka.utils.CoreUtils$)
java.lang.NullPointerException
        at kafka.server.KafkaServer.$anonfun$shutdown$6(KafkaServer.scala:579)
        at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
        at kafka.server.KafkaServer.shutdown(KafkaServer.scala:579)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:329)
        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38)
        at kafka.Kafka$.main(Kafka.scala:75)
        at kafka.Kafka.main(Kafka.scala)
[2020-05-29 18:32:21,849] INFO shut down completed (kafka.server.KafkaServer)
[2020-05-29 18:32:21,850] ERROR Exiting Kafka. (kafka.server.KafkaServerStartable)
[2020-05-29 18:32:21,853] INFO shutting down (kafka.server.KafkaServer)
{noformat}","01/Jun/20 12:31;nizhikov;replication_test after last edits.
{noformat}
====================================================================================================
SESSION REPORT (ALL TESTS)
ducktape version: 0.7.7
session_id:       2020-06-01--004
run time:         62 minutes 44.597 seconds
tests run:        31
passed:           31
failed:           0
ignored:          0
====================================================================================================
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.failure_mode=hard_bounce.client_sasl_mechanism=SCRAM-SHA-256.security_protocol=SASL_SSL.broker_type=leader.interbroker_sasl_mechanism=SCRAM-SHA-512
status:     PASS
run time:   3 minutes 52.192 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=clean_bounce.broker_type=controller
status:     PASS
run time:   2 minutes 15.889 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=clean_bounce.broker_type=leader
status:     PASS
run time:   2 minutes 11.561 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=clean_bounce.broker_type=leader.enable_idempotence=True
status:     PASS
run time:   2 minutes 10.796 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=clean_shutdown.broker_type=controller
status:     PASS
run time:   1 minute 9.627 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=clean_shutdown.broker_type=leader
status:     PASS
run time:   1 minute 2.357 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=clean_shutdown.broker_type=leader.enable_idempotence=True
status:     PASS
run time:   1 minute 2.639 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=hard_bounce.broker_type=controller
status:     PASS
run time:   2 minutes 21.234 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=hard_bounce.broker_type=leader
status:     PASS
run time:   2 minutes 28.602 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=hard_bounce.broker_type=leader.enable_idempotence=True
status:     PASS
run time:   2 minutes 24.656 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=hard_shutdown.broker_type=controller
status:     PASS
run time:   1 minute 16.623 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=hard_shutdown.broker_type=leader
status:     PASS
run time:   1 minute 12.983 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=PLAINTEXT.failure_mode=hard_shutdown.broker_type=leader.enable_idempotence=True
status:     PASS
run time:   1 minute 9.099 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.client_sasl_mechanism=PLAIN.failure_mode=hard_bounce.broker_type=leader.interbroker_sasl_mechanism=GSSAPI
status:     PASS
run time:   3 minutes 7.177 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.client_sasl_mechanism=PLAIN.failure_mode=hard_bounce.broker_type=leader.interbroker_sasl_mechanism=PLAIN
status:     PASS
run time:   3 minutes 10.262 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=clean_bounce.broker_type=controller
status:     PASS
run time:   2 minutes 57.467 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=clean_bounce.broker_type=leader
status:     PASS
run time:   2 minutes 48.815 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=clean_shutdown.broker_type=controller
status:     PASS
run time:   1 minute 34.970 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=clean_shutdown.broker_type=leader
status:     PASS
run time:   1 minute 14.040 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=hard_bounce.broker_type=controller
status:     PASS
run time:   3 minutes 23.274 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=hard_bounce.broker_type=leader
status:     PASS
run time:   3 minutes 5.554 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=hard_shutdown.broker_type=controller
status:     PASS
run time:   1 minute 38.441 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.security_protocol=SASL_SSL.failure_mode=hard_shutdown.broker_type=leader
status:     PASS
run time:   1 minute 18.875 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.tls_version=TLSv1.2.security_protocol=PLAINTEXT.broker_type=leader.failure_mode=clean_bounce.compression_type=gzip
status:     PASS
run time:   2 minutes 8.523 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.tls_version=TLSv1.2.security_protocol=PLAINTEXT.broker_type=leader.failure_mode=clean_shutdown.compression_type=gzip
status:     PASS
run time:   1 minute 9.231 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.tls_version=TLSv1.2.security_protocol=PLAINTEXT.broker_type=leader.failure_mode=hard_bounce.compression_type=gzip
status:     PASS
run time:   2 minutes 26.909 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.tls_version=TLSv1.2.security_protocol=PLAINTEXT.broker_type=leader.failure_mode=hard_shutdown.compression_type=gzip
status:     PASS
run time:   1 minute 16.480 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.tls_version=TLSv1.3.security_protocol=PLAINTEXT.broker_type=leader.failure_mode=clean_bounce.compression_type=gzip
status:     PASS
run time:   2 minutes 5.383 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.tls_version=TLSv1.3.security_protocol=PLAINTEXT.broker_type=leader.failure_mode=clean_shutdown.compression_type=gzip
status:     PASS
run time:   1 minute 6.718 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.tls_version=TLSv1.3.security_protocol=PLAINTEXT.broker_type=leader.failure_mode=hard_bounce.compression_type=gzip
status:     PASS
run time:   2 minutes 19.232 seconds
----------------------------------------------------------------------------------------------------
test_id:    kafkatest.tests.core.replication_test.ReplicationTest.test_replication_with_broker_failure.tls_version=TLSv1.3.security_protocol=PLAINTEXT.broker_type=leader.failure_mode=hard_shutdown.compression_type=gzip
status:     PASS
run time:   1 minute 13.884 seconds
----------------------------------------------------------------------------------------------------
{noformat}","03/Jun/20 07:21;nizhikov;Fixed with the https://github.com/apache/kafka/commit/8b22b8159673bfe22d8ac5dcd4e4312d4f2c863c",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read Only Kafka Topics / Clusters,KAFKA-10187,13312513,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,myloginid@gmail.com,myloginid@gmail.com,19/Jun/20 16:09,19/Jun/20 16:38,12/Jan/21 10:06,,,,,,,,,,,,core,,,,,,0,,,,,"Request to support Read Only Kafka Topics / Read Only Kafka Clusters.

The reason so put a topic / cluster in read only mode is for deployments with load balanced multiple Kafka clusters where when a single cluster has to be taken out for maintenance, we want the ingest of new data into these topics / clusters to stop for some time. Consumption should still continue to allow consumers to catch up (including MM2 to replicate the data). With ingestion stopped we can be relatively certain that consumers will catch up in a controlled duration to then take the cluster out for maintenance.

Alternate workarounds exist by modifying LB configuration but they are not elegant and need a LB restart. We can also potentially enable firewalls on the host to block producer IP's but thats also not a nice solution.

Thoughts?",,myloginid@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-19 16:09:29.0,,,,,,,"0|z0g188:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Indicate ""isClosing"" in the SinkTaskContext",KAFKA-6725,13148763,New Feature,Patch Available,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,farmdawgnation,farmdawgnation,farmdawgnation,29/Mar/18 01:58,14/Jun/20 02:15,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,connect,connect-api,,,"Addition of the isClosing method to SinkTaskContext per this KIP.

https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=75977607",,farmdawgnation,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-11 03:47:32.252,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 11 03:51:25 UTC 2018,,,,,,,"0|i3rx9r:",9223372036854775807,,,,,,,,,,,,,,,,"29/Mar/18 16:43;farmdawgnation;I've got some code locally on this that I'm planning on submitting as a patch once it looks like the design is widely agreed upon :)","11/May/18 03:47;githubbot;farmdawgnation opened a new pull request #5002: KAFKA-6725: Addition of isClosing to SinkTaskContext
URL: https://github.com/apache/kafka/pull/5002
 
 
   This PR implements KAFKA-6725 and KIP-275 to provide for the addition of an `isClosing` method on the `SinkTaskContext`. This permits `SinkTask`s to know if their `preCommit` hook is being invoked because the task is about to be shut down. This allows tasks to optionally apply different heuristics in a ""I am about to close"" situation.
   
   For example, a sink configured to archive data to S3 might choose to checkpoint and upload sooner than it would have otherwise.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","11/May/18 03:51;farmdawgnation;This PR represents my work in progress: https://github.com/apache/kafka/pull/5002

Still need to think on testing a bit, but unless I get any better ideas than what I proposed in the PR I'll probably go with that idea. :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for checking binary/source compatibility,KAFKA-1880,12768516,New Feature,Patch Available,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,viktorsomogyi,singhashish,singhashish,19/Jan/15 19:26,11/Jun/20 14:39,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,"Recent discussions around compatibility shows how important compatibility is to users. Kafka should leverage a tool to find, report, and avoid incompatibility issues in public methods.",,ewencp,githubbot,gwenshap,ijuma,singhashish,viktorsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/16 07:47;granthenke;compatibilityReport-only-incompatible.html;https://issues.apache.org/jira/secure/attachment/12801413/compatibilityReport-only-incompatible.html","28/Apr/16 23:32;granthenke;compatibilityReport.html;https://issues.apache.org/jira/secure/attachment/12801343/compatibilityReport.html",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2016-04-28 23:17:38.301,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 07 09:26:46 UTC 2018,,,,,,,"0|i24jav:",9223372036854775807,,,,,,,,,,,,,,,,"28/Apr/16 23:17;granthenke;I have started hacking together a solution for this that is integrated into the build and can check compatibility across different git branches or commits. 

Locally I have a very rough implementation that works and identified KAFKA-3641. I will post a WIP pull request once its cleaned up a bit. ","28/Apr/16 23:32;granthenke;Attaching a sample html compatibility report for all public changes in the clients module between the 0.9 branch and trunk.","29/Apr/16 00:57;gwenshap;It is useful already! Thank you for this cool contribution - can't wait for the PR.","29/Apr/16 01:08;ijuma;This is definitely much needed.","29/Apr/16 01:09;ijuma;That page lists a number of tools. What's the thinking around choosing one?","29/Apr/16 01:25;granthenke;I actually did not use the compatibility checker in the current description (Java API Compliance Checker).  Instead I choose to use [japicmp|https://siom79.github.io/japicmp/]. I will update the description when things are more concrete. 

I evaluated the options with the following criteria:

1. Able to be plugged into our existing build process (That means it needed to be a java dependency I can resolve and use from Gradle)
2. Able to detect source, binary, serialization and annotation incompatibilities
3. Able to filter the checked classes by package (Since we don't use annotations all over already)
4. Able to provide a clear and concise report/overview
5. Bonus: Works with Scala too. (I need to test this yet)

Here are some explanations for tools I considered but didn't choose (mostly taken from https://siom79.github.io/japicmp/) 
- *Java API Compliance Checker*: A Perl script. This approach cannot compare annotations and you need to have Perl installed. Only filters by annotation.
- *Clirr*: Tracking of API changes is implemented only partially, tracking of annotations is not supported. Development has stopped around 2005.
- *JDiff*: A Javadoc doclet that generates an HTML report of all API changes. The source code for both versions has to be available, the differences are not distinguished between binary incompatible or not. Comparison of annotations is not supported.
- *revapi*: An API analysis and change tracking tool that was started about the same time as japicmp. It ships with a maven plugin and an Ant task, but the maven plugin currently (version 0.4.1) only reports changes on the command line.


","29/Apr/16 07:47;granthenke;Adding a more complete sample report that only includes breaking changes. 

I think we need to define more tightly whats ""public"" and whats not. Also what should be serializable. We have a decent number of serialization breaks in common.","29/Apr/16 18:05;githubbot;GitHub user granthenke opened a pull request:

    https://github.com/apache/kafka/pull/1291

    WIP - KAFKA-1880: Add support for checking binary/source compatibility

    This is a WIP pull request to show how I am generating the reports attached to the Jira. I am putting it up now so that we understand what has been changed/broken before the 0.10 release. 
    
    At some point we may want to leverage something like this to break the build too, but I think generating a report is a good start.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/granthenke/kafka api-check

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1291.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1291
    
----
commit 5af41e2ab52bb62a4bf9d13d516b4d2789e357da
Author: Grant Henke <granthenke@gmail.com>
Date:   2016-04-29T18:00:39Z

    WIP - KAFKA-1880: Add support for checking binary/source compatibility

----
","31/Aug/18 15:15;viktorsomogyi;[~granthenke] if you don't mind I've reassigned this to continue your work as something similar has come up regarding KIP-336.","03/Sep/18 11:22;viktorsomogyi;[~ijuma] do you think this is still relevant?","07/Sep/18 09:26;githubbot;viktorsomogyi opened a new pull request #5620: KAFKA-1880: Add support for checking binary/source compatibility
URL: https://github.com/apache/kafka/pull/5620
 
 
   This PR aims to extend the gradle build with a task that can generate a report for API/ABI compatibility.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Warm up new KS instances before migrating tasks - potentially a two phase rebalance,KAFKA-6145,13113091,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,astubbs,astubbs,30/Oct/17 17:39,10/Jun/20 22:27,12/Jan/21 10:06,27/May/20 22:49,,,,,,,,2.6.0,,,streams,,,,,,6,needs-kip,,,,"Currently when expanding the KS cluster, the new node's partitions will be unavailable during the rebalance, which for large states can take a very long time, or for small state stores even more than a few ms can be a deal breaker for micro service use cases.
One workaround would be two execute the rebalance in two phases:
1) start running state store building on the new node
2) once the state store is fully populated on the new node, only then rebalance the tasks - there will still be a rebalance pause, but would be greatly reduced

Relates to: KAFKA-6144 - Allow state stores to serve stale reads during rebalance",,ableegoldman,astubbs,flengronne,githubbot,lkokhreidze,mingaliu,mjsax,NaviBrar,scosenza,shafqat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6039,KAFKA-5578,,,,,,,,,KAFKA-6144,KAFKA-8019,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-02-15 01:58:53.551,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 21:03:44 UTC 2020,,,,,,,"0|i3lvlz:",9223372036854775807,,,,,,,,,,,,,,,,"15/Feb/20 01:58;githubbot;ableegoldman commented on pull request #8121: KAFKA-6145: Pt 1. Bump protocol version and encode task lag map
URL: https://github.com/apache/kafka/pull/8121
 
 
   ""First"" PR for KIP-441: implement the protocol change so we can encode the task lag info in the subscription
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","06/Mar/20 15:19;githubbot;vvcephei commented on pull request #8121: KAFKA-6145: Pt 1. Bump protocol version and encode task lag map
URL: https://github.com/apache/kafka/pull/8121
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","06/Mar/20 23:55;githubbot;ableegoldman commented on pull request #8246: KAFKA-6145: Pt 2. Include offset sums in subscription
URL: https://github.com/apache/kafka/pull/8246
 
 
   KIP-441 Pt. 2: Compute sum of offsets across all stores/changelogs in a task and include them in the subscription.
   
   Previously each thread would just encode every task on disk, but we now need to read the changelog file which is unsafe to do without a lock on the task directory. So, each thread now encodes only its assigned active and standby tasks, and ignores any already-locked tasks.
   
   In some cases there may be unowned and unlocked tasks on disk that were reassigned to another instance and haven't been cleaned up yet by the background thread. Each StreamThread makes a weak effort to lock any such task directories it finds, and if successful is then responsible for computing and reporting that task's offset sum (based on reading the checkpoint file)
   
   This PR therefore also addresses two orthogonal issues:
   1) Prevent background cleaner thread from deleting unowned stores during a rebalance
   2) Deduplicate standby tasks in subscription: each thread used to include every (non-active) task found on disk in its ""standby task"" set, which meant every active, standby, and unowned task was encoded by _every_ thread.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","07/Mar/20 21:08;githubbot;ableegoldman commented on pull request #8252: KAFKA-6145: Pt 2.5 Compute overall task lag per client
URL: https://github.com/apache/kafka/pull/8252
 
 
   **WIP: need to add tests**
   
   Once we have encoded the offset sums per task for each client, we can compute the overall lag during `assign` by fetching the end offsets for all changelog and subtracting.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","10/Mar/20 09:15;githubbot;cadonna commented on pull request #8262: [WIP] KAFKA-6145: Add constrained balanced assignment algorithm
URL: https://github.com/apache/kafka/pull/8262
 
 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","11/Mar/20 22:43;githubbot;ableegoldman commented on pull request #8282: KAFKA-6145: add new assignment configs
URL: https://github.com/apache/kafka/pull/8282
 
 
   For KIP-441 we intend to add 4 new configs:
   
   1. assignment.acceptable.recovery.lag
   2. assignment.balance.factor
   3. assignment.max.extra.replicas
   4. assignment.probing.rebalance.interval.ms
   
   I think we should give them all a common prefix to make it clear they're related, but am open to better suggestions than just ""assignment""
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","14/Mar/20 03:57;githubbot;vvcephei commented on pull request #8246: KAFKA-6145: Pt 2. Include offset sums in subscription
URL: https://github.com/apache/kafka/pull/8246
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","19/Mar/20 18:19;githubbot;vvcephei commented on pull request #8282: KAFKA-6145: add new assignment configs
URL: https://github.com/apache/kafka/pull/8282
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","20/Mar/20 18:51;githubbot;vvcephei commented on pull request #8262: KAFKA-6145: Add constrained balanced assignment algorithm
URL: https://github.com/apache/kafka/pull/8262
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","21/Mar/20 18:40;githubbot;vvcephei commented on pull request #8252: KAFKA-6145: Pt 2.5 Compute overall task lag per client
URL: https://github.com/apache/kafka/pull/8252
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","23/Mar/20 14:54;githubbot;cadonna commented on pull request #8334: KAFKA-6145: Add balanced assignment algorithm
URL: https://github.com/apache/kafka/pull/8334
 
 
   This algorithm assigns tasks to clients and tries to
   - balance the distribution of the  partitions of the
     same input topic over stream threads and clients,
     i.e., data parallel workload balance
   - balance the distribution of work over stream threads.
   The algorithm does not take into account potentially existing states
   on the client.
   
   The assignment is considered balanced when the difference in
   assigned tasks between the stream thread with the most tasks and
   the stream thread with the least tasks does not exceed a given
   balance factor.
   
   The algorithm prioritizes balance over stream threads
   higher than balance over clients.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","24/Mar/20 03:41;githubbot;ableegoldman commented on pull request #8337: KAFKA-6145: Pt. 5 Implement high availability assignment
URL: https://github.com/apache/kafka/pull/8337
 
 
   WIP: still adding tests
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","24/Mar/20 22:49;githubbot;vvcephei commented on pull request #8334: KAFKA-6145: Add balanced assignment algorithm
URL: https://github.com/apache/kafka/pull/8334
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","02/Apr/20 18:36;githubbot;vvcephei commented on pull request #8337: KAFKA-6145: Pt. 5 Implement high availability assignment
URL: https://github.com/apache/kafka/pull/8337
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","03/Apr/20 01:33;githubbot;ableegoldman commented on pull request #8409: KAFKA-6145: KIP-441 Pt. 6 Trigger probing rebalances until group is stable
URL: https://github.com/apache/kafka/pull/8409
 
 
   This KIP is fairly straightforward, and does as the title describes by enforcing a rebalance once the configured `probing.rebalance.interval` has elapsed. However, we have had to modify the original plan in the KIP slightly to handle an edge case with static membership enabled:
   
   Since the group leader can crash and restart without triggering a rebalance, we can't rely on a purely in-memory flag/counter to keep track of these probing rebalances. We can instead rely on the assignment, encoding the upcoming probing rebalance in the `AssignmentInfo`. This is encoded as the time in ms of the next scheduled rebalance, ie it is set to `currentTimeMs + probingRebalanceIntervalMs` when the assignment is being generated. This anchors the probing rebalances to wall clock time, and ensures a pathologically failing member will not prevent the group from ever rebalancing
   
   We leave it up to a single member to be responsible for triggering the probing rebalances, and encode this for a single consumer on the leader's client (chosen arbitrarily). 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","04/Apr/20 04:04;githubbot;ableegoldman commented on pull request #8425: KAFKA-6145: KIP-441 Move tasks with caught-up destination clients right away
URL: https://github.com/apache/kafka/pull/8425
 
 
   If a stateful task is intended to be moved to a client which is already caught-up, we should not create a warmup task for it. Instead, just reassign that task right away.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","07/Apr/20 03:43;githubbot;ableegoldman commented on pull request #8436: KAFKA-6145: KIP-441 avoid unnecessary movement of standbys
URL: https://github.com/apache/kafka/pull/8436
 
 
   Currently we add warmup and standby tasks, meaning we first assign up to max.warmup.replica warmup tasks, and then attempt to assign num.standby copies of each stateful task. This can cause unnecessary transient standbys to pop up for the lifetime of the warmup task, which are presumably not what the user wanted.
   
   Note that we don’t want to simply count all warmups against the configured num.standbys, as this may cause the opposite problem where a standby we intend to keep is temporarily unassigned (which may lead to the cleanup thread deleting it). We should only count this as a standby if the destination client already had this task as a standby; otherwise, the standby already exists on some other client, so we should aim to give it back.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","08/Apr/20 18:02;githubbot;vvcephei commented on pull request #8409: KAFKA-6145: KIP-441 Pt. 6 Trigger probing rebalances until group is stable
URL: https://github.com/apache/kafka/pull/8409
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","09/Apr/20 01:08;githubbot;vvcephei commented on pull request #8425: KAFKA-6145: KIP-441 Move tasks with caught-up destination clients right away
URL: https://github.com/apache/kafka/pull/8425
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","09/Apr/20 22:34;githubbot;vvcephei commented on pull request #8458: KAFKA-6145: KIP-441: Add test scenarios to ensure rebalance convergence
URL: https://github.com/apache/kafka/pull/8458
 
 
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","10/Apr/20 18:43;githubbot;vvcephei commented on pull request #8436: KAFKA-6145: KIP-441 avoid unnecessary movement of standbys
URL: https://github.com/apache/kafka/pull/8436
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","13/Apr/20 13:36;githubbot;vvcephei commented on pull request #8458: KAFKA-6145: KIP-441: Add test scenarios to ensure rebalance convergence
URL: https://github.com/apache/kafka/pull/8458
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","13/Apr/20 13:38;githubbot;vvcephei commented on pull request #8475: [WIP] KAFKA-6145: KIP-441: Add test scenarios to ensure rebalance convergence
URL: https://github.com/apache/kafka/pull/8475
 
 
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","16/Apr/20 05:10;githubbot;ableegoldman commented on pull request #8497: KAFKA-6145: KIP-441 Build state constrained assignment from balanced one
URL: https://github.com/apache/kafka/pull/8497
 
 
   John's awesome `TaskAssignorConvergenceTest` revealed some issues with the current assignor, which he nailed down as being due to the state constrained and balanced assignments not converging.
   
   One way to get an assignment that is as close to the balanced assignment as possible while still being state constrained is of course to start with the balanced assignment, and move tasks around as necessary to satisfy the state constraint. With this basic approach, the converge test is passing.
   
   This PR also includes some semi-orthogonal refactoring, most significantly the removal of the  assignment maps; we now just immediately assign tasks to the `ClientState` rather than first sticking them in an intermediate map.
   
   Also moves `ValidClientsByTaskLoadQueue` to its own file
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","17/Apr/20 21:03;githubbot;vvcephei commented on pull request #8475: KAFKA-6145: KIP-441: Add test scenarios to ensure rebalance convergence
URL: https://github.com/apache/kafka/pull/8475
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Garbage Collect obsolete topics,KAFKA-560,12611032,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,sriharsha,jkreps,jkreps,09/Oct/12 18:13,01/Jun/20 11:31,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,3,project,,,,"Old junk topics tend to accumulate over time. Code may migrate to use new topics leaving the old ones orphaned. Likewise there are some use cases for temporary transient topics. It would be good to have a tool that could delete any topic that had not been written to in a configurable period of time and had no active consumer groups. Something like
   ./bin/delete-unused-topics.sh --last-write [date] --zookeeper [zk_connect]
This requires API support to get the last update time. I think it may be possible to do this through the OffsetRequest now?",,adupriez,cagatayk,carlos.duclos,criccomini,jcreasy,jkreps,nabilgasri,nehanarkhede,sriharsha,sslavic,xakassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-566,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-10-11 04:22:12.579,,,false,,,,,,,,,,,,,,,,,,246210,,,Mon Jun 01 11:31:43 UTC 2020,,,,,,,"0|i07i7j:",41717,,nehanarkhede,,,,,,,,,,,,,,"11/Oct/12 04:22;nehanarkhede;OffsetRequest will not give you the time, but the offsets since a time (approximately). 

However, the original proposal for TopicMetadataRequest had a field for last modified timestamp per partition - 


/**
 * topic (2 bytes + topic.length)
 * number of partitions (4 bytes)
 *
 * partition id (4 bytes)
 *
 * does leader exist (1 byte)
 * leader info (4 + creator.length + host.length + 4 (port) + 4 (id))
 * number of replicas (2 bytes)
 * replica info (4 + creator.length + host.length + 4 (port) + 4 (id))
 * number of in sync replicas (2 bytes)
 * replica info (4 + creator.length + host.length + 4 (port) + 4 (id))
 *
 * does log metadata exist (1 byte)
 * number of log segments (4 bytes)
 * total size of log in bytes (8 bytes)
 *
 * number of log segments (4 bytes)
 * beginning offset (8 bytes)
 * last modified timestamp (8 bytes)
 * size of log segment (8 bytes)
 *
 */

However, we haven't really implemented the last few fields of this request, simply because we couldn't think of a use case. But what you describe seems useful and maybe we should file a JIRA to get the log segment portion of TopicMetadatRequest. 
","11/Oct/12 15:51;jkreps;Filed KAFKA-566 for this.","10/Sep/14 17:30;criccomini;bq. It would be good to have a tool that could delete any topic that had not been written to in a configurable period of time and had no active consumer groups. 

I would prefer not to depend on consumer groups. Samza, for example, doesn't have consumer groups, so doing things like looking at the lsat offset commit of a consumer group in ZK/OffsetManager will not help if the consumer is using Samza (or some other offset checkpoint mechanism). The better approach, to me, seems to be to just have brokers keep track of approximate last-reads for each topic/partition based on FetchRequests.","14/Oct/14 16:24;sriharsha;[~jkreps] [~nehanarkhede] If no one actively working on this JIRA i am interested in taking it. Assigning it to myself please change it if necessary.","20/Jun/19 12:29;carlos.duclos;I guess nothing has happened here, any progress on this?","01/Jun/20 11:31;xakassi;I'm looking for an option on Kafka for auto deletion of old unused topics. There is no such option, right?

I wonder why? I think it's no less important than deleting old messages via retention policy. Because there are collecting lot's of unused topics consuming recources in dev environments. It would be very useful from my point of view to delete such old topics automatically.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Conditionally apply SMTs,KAFKA-9673,13290144,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tombentley,tombentley,tombentley,06/Mar/20 17:37,28/May/20 13:59,12/Jan/21 10:06,28/May/20 13:59,,,,,,,,2.6.0,,,KafkaConnect,,,,,,0,,,,,"KAFKA-7052 ended up using IAE with a message, rather than NPE in the case of a SMT being applied to a record lacking a given field. It's still not possible to apply a SMT conditionally, which is what things like Debezium really need in order to apply transformations only to non-schema change events.

[~rhauch] suggested a mechanism to conditionally apply any SMT but was concerned about the possibility of a naming collision (assuming it was configured by a simple config)

I'd like to propose something which would solve this problem without the possibility of such collisions. The idea is to have a higher-level condition, which applies an arbitrary transformation (or transformation chain) according to some predicate on the record. 

More concretely, it might be configured like this:

{noformat}
  transforms.conditionalExtract.type: Conditional
  transforms.conditionalExtract.transforms: extractInt
  transforms.conditionalExtract.transforms.extractInt.type: org.apache.kafka.connect.transforms.ExtractField$Key
  transforms.conditionalExtract.transforms.extractInt.field: c1
  transforms.conditionalExtract.condition: topic-matches:<someRegexHere>
{noformat}

* The {{Conditional}} SMT is configured with its own list of transforms ({{transforms.conditionalExtract.transforms}}) to apply. This would work just like the top level {{transforms}} config, so subkeys can be used to configure these transforms in the usual way.
* The {{condition}} config defines the predicate for when the transforms are applied to a record using a {{<condition-type>:<parameters>}} syntax

We could initially support three condition types:

*{{topic-matches:<pattern>}}* The transformation would be applied if the record's topic name matched the given regular expression pattern. For example, the following would apply the transformation on records being sent to any topic with a name beginning with ""my-prefix-"":
{noformat}
       transforms.conditionalExtract.condition: topic-matches:my-prefix-.*
{noformat}
   
*{{has-header:<header-name>}}* The transformation would be applied if the record had at least one header with the given name. For example, the following will apply the transformation on records with at least one header with the name ""my-header"":
{noformat}
       transforms.conditionalExtract.condition: has-header:my-header
{noformat}
   
*{{not:<condition-name>}}* This would negate the result of another named condition using the condition config prefix. For example, the following will apply the transformation on records which lack any header with the name my-header:

{noformat}
      transforms.conditionalExtract.condition: not:hasMyHeader
      transforms.conditionalExtract.condition.hasMyHeader: has-header:my-header
{noformat}

I foresee one implementation concern with this approach, which is that currently {{Transformation}} has to return a fixed {{ConfigDef}}, and this proposal would require something more flexible in order to allow the config parameters to depend on the listed transform aliases (and similarly for named predicate used for the {{not:}} predicate). I think this could be done by adding a {{default}} method to {{Transformation}} for getting the ConfigDef given the config, for example.

Obviously this would require a KIP, but before I spend any more time on this I'd be interested in your thoughts [~rhauch], [~rmoff], [~gunnar.morling].",,rhauch,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-05-28 13:59:31.603,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 13:59:31 UTC 2020,,,,,,,"0|z0c9f4:",9223372036854775807,,kkonstantine,,,,,,,,,,,,,,"27/Mar/20 08:34;tombentley;I opened [KIP-585|https://cwiki.apache.org/confluence/display/KAFKA/KIP-585%3A+Conditional+SMT] for discussion.","28/May/20 13:59;rhauch;KIP-585 was approved by the 2.6.0 KIP freeze, and the PR was approved and merged to `trunk` before 2.6.0 feature freeze.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RequestChannel re-design,KAFKA-747,12630405,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,nehanarkhede,jkreps,jkreps,01/Feb/13 17:39,27/May/20 21:01,12/Jan/21 10:06,,,,,,,,,,,,network,,,,,,0,,,,,"We have had some discussion around how to handle queuing requests. There are two competing concerns:
1. We need to maintain request order on a per-socket basis.
2. We want to be able to balance load flexibly over a pool of threads so that if one thread blocks on I/O request processing continues.

Two Approaches We Have Considered

1. Have a global queue of unprocessed requests. All I/O threads read requests off this global queue and process them. To avoid re-ordering have the network layer only read one request at a time.
2. Have a queue per I/O thread and have the network threads statically map sockets to I/O thread request queues.

Problems With These Approaches

In the first case you are not able to get any per-producer parallelism. That is you can't read the next request while the current one is being handled. This seems like it would not be a big deal, but preliminary benchmarks show that it might be. 

In the second case there are two problems. The first is that when an I/O thread gets blocked all request processing for sockets attached to that I/O thread will grind to a halt. If you have 10,000 connections, and  10 I/O threads, then each blockage will stop 1,000 producers. If there is one topic that has long synchronous flush times enabled (or is experiencing fsync locking) this will cause big latency blips for all producers using that I/O thread. The next problem is around backpressure and memory management. Say we use BlockingQueues to feed the I/O threads. And say that one I/O thread stalls. It's request queue will fill up and it will then block ALL network threads, since they will block on inserting into that queue, even though the other I/O threads are unused and have empty queues.

A Proposed Better Solution

The problem with the first solution is that we are not pipelining requests. The problem with the second approach is that we are too constrained in moving work from one I/O thread to another.

Instead we should have a single request queue-like structure, but internally enforce the condition that requests are not re-ordered.

Here are the details. We retain RequestChannel but refactor its internals. Internally we replace the blocking queue with a linked list. We also keep an in-flight-keys array with one entry per I/O thread. When removing a work item from the list we can't just take the first thing. Instead we need to walk the list and look for something with a request key not in the in-flight-keys array. When a response is sent, we remove that key from the in-flight array.

This guarantees that requests for a socket with key K are ordered, but that processing for K can only block requests made by K.",,donnchadh,guozhang,jeffwidman,jkreps,mjsax,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-02-01 17:54:56.448,,,false,,,,,,,,,,,,,,,,,,310900,,,Wed May 27 21:01:36 UTC 2020,,,,,,,"0|i1hnkf:",311245,,,,,,,,,,,,,,,,"01/Feb/13 17:54;sriramsub;I proposed a similar solution but slightly different. We have iothread group instead of one io thread. We have a linked list like structure like you have specified above. Assume the io thread group size is 3. So each group maintains an array of 3 where each slot indicates which client id it is processing currently. We will have one io queue (linked list above) per io group. When a thread in the io group needs to pick up a request it will try to find a client id not being processed by the other threads in the io group. The lookup needs to happen only on an array of size 3 and also there is lesser synchronization. One added benefit is that when a thread in an io group tries to find a request to pick in the queue, it needs to at max look at only 3 slots instead of the total number of io threads which would be longer.","01/Feb/14 20:12;nehanarkhede;Relatively large redesign, moving out of 0.8.1","04/Sep/14 22:28;guozhang;Moving to 0.9","27/May/20 21:01;mjsax;Is this still an issue?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Implement ""Exact Mirroring"" functionality in mirror maker",KAFKA-658,12619157,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jkreps,jkreps,06/Dec/12 16:59,27/May/20 21:00,12/Jan/21 10:06,27/May/20 21:00,,,,,,,,,,,mirrormaker,,,,,,8,project,,,,"There are two ways to implement ""mirroring"" (i.e. replicating a topic from one cluster to another):
1. Do a simple read from the source and write to the destination with no attempt to maintain the same partitioning or offsets in the destination cluster. In this case the destination cluster may have a different number of partitions, and you can even read from many clusters to create a merged cluster. This flexibility is nice. The downside is that since the partitioning and offsets are not the same a consumer of the source cluster has no equivalent position in the destination cluster. This is the style of mirroring we have implemented in the mirror-maker tool and use for datacenter replication today.
2. The second style of replication only would allow creating an exact replica of a source cluster (i.e. all partitions and offsets exactly the same). The nice thing about this is that the offsets and partitions would match exactly. The downside is that it is not possible to merge multiple source clusters this way or have different partitioning. We do not currently support this in mirror maker.

It would be nice to implement the second style as an option in mirror maker as having an exact replica would be a nice option to have in the case where you are replicating a single cluster only.

There are some nuances: In order to maintain the exact offsets it is important to guarantee that the producer never resends a message or loses a message. As a result it would be important to have only a single producer for each destination partition, and check the last produced message on startup (using the getOffsets api) so that in the case of a hard crash messages that are re-consumed are not re-emitted.",,anandriyer,arushkharbanda,boniek,jkreps,justen_walker,kpocius,mihbor,mjsax,mrsrinivas,original-brownbear,paulmw,r4um,ryannedolan,sadineni,thecoop1984,umesh9794@gmail.com,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-10-09 23:30:30.015,,,false,,,,,,,,,,,,,,,,,,296352,,,Wed May 27 21:00:25 UTC 2020,,,,,,,"0|i148qf:",232878,,,,,,,,,,,,,,,,"09/Oct/15 23:30;justen_walker;+1 for Exact Mirroring.  I have a use-case where I'd like to replicate a cluster for DR purposes, and the current system does not maintain ordering, so MirrorMaker cannot be used as-is.","13/Oct/15 20:54;sadineni;+1, I too have exact same use case","13/Oct/15 21:09;wushujames;I would also like this, so that consumers can transition from one cluster to another and be able to resume without missing or duplicating any records.
","14/Aug/17 15:41;paulmw;Does the recent work in exactly-once semantics mean this JIRA is likely to make progress?","26/Oct/18 03:08;ryannedolan;[https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0] (under discussion) aims to support this.","27/May/20 21:00;mjsax;Closing this ticket as fixed, as MM2 is available now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-584: Implement read path for versioning scheme for features,KAFKA-10026,13306139,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,,kprakasam,kprakasam,20/May/20 10:01,20/May/20 10:03,12/Jan/21 10:06,20/May/20 10:03,,,,,,,,,,,,,,,,,0,,,,,"Goal is to implement various classes and integration for the read path of the feature versioning system ([KIP-584|https://cwiki.apache.org/confluence/display/KAFKA/KIP-584%3A+Versioning+scheme+for+features]). The ultimate plan is that the cluster-wide *finalized* features information is going to be stored in ZK under the node {{/feature}}. The read path implemented in this PR is centered around reading this *finalized* features information from ZK, and, processing it inside the Broker.

 

Here is a summary of what's needed for this Jira (a lot of it is *new* classes):
 * A facility is provided in the broker to declare it's supported features, and advertise it's supported features via it's own {{BrokerIdZNode}} under a {{features}} key.
 * A facility is provided in the broker to listen to and propagate cluster-wide *finalized* feature changes from ZK.
 * When new *finalized* features are read from ZK, feature incompatibilities are detected by comparing against the broker's own supported features.
 * {{ApiVersionsResponse}} is now served containing supported and finalized feature information (using the newly added tagged fields).",,kprakasam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 20 10:03:38 UTC 2020,,,,,,,"0|z0ey0o:",9223372036854775807,,,,,,,,,,,,,,,,"20/May/20 10:03;kprakasam;Duplicate of KAFKA-10027",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Priorities for Source Topics,KAFKA-6690,13146538,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,nafshartous,balaprassanna,balaprassanna,20/Mar/18 13:19,19/May/20 11:29,12/Jan/21 10:06,19/May/20 11:29,,,,,,,,,,,consumer,,,,,,10,,,,,"We often encounter use cases where we need to prioritise source topics. If a consumer is listening more than one topic, say, HighPriorityTopic and LowPriorityTopic, it should consume events from LowPriorityTopic only when all the events from HighPriorityTopic are consumed. This is needed in Kafka Streams processor topologies as well.",,astubbs,balaprassanna,barakm,craigtmc,guozhang,marinagre,mjsax,nafshartous,simplyamuthan,sjothi,wenwise,wlsc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-06-29 19:16:10.0,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 11:29:27 UTC 2020,,,,,,,"0|i3rjlz:",9223372036854775807,,,,,,,,,,,,,,,,"29/Jun/18 19:16;nafshartous;This seems like it would require an API change.  For example, if we added a new class
{code}
  TopicPriority(java.lang.String topic, int priority)
{code}

then there would be new {{KafkaConsumer.subscribe}} methods like
{code}
public void subscribe(java.util.List<TopicPriority> topicPriorities)
{code}

Doe this capture the intent ?  ","25/Jul/18 10:01;balaprassanna;Would be helpful if the same option is also present in the following API
{code:java}
org.apache.kafka.streams.Topology.addSource(String name, String[] topics)
{code}
 ","25/Jul/18 16:23;guozhang;[~balaprassanna] Your description on the JIRA seems to be requesting this feature on the Consumer API, while your comment above is suggesting adding it on the Streams API (note the Streams client actually has embedded Consumer client internally). Which type of client are you really thinking about?","27/Jul/18 07:08;balaprassanna;[~guozhang] We would need this in both Consumer API and Streams API","27/Jul/18 22:09;guozhang;If we add it on the consumer API, then Streams may be automatically geared with this feature since it is leveraging on consumers (some different considerata would be done on Streams since today its messaging choosing is purely dependent on timestamp synchronization, but this can be deferred to a follow-up discussion).

Maybe you can start the discussion thread on the mailing list and ask for the community's opinion on this feature request.","29/Jul/18 20:37;nafshartous;[~guozhang] After I asked about how to proceed on the dev list,, [~enether] asked that I write a KIP.
Last Monday I requested permission on the dev list to create a KIP and I didn't see a reply granting permission.  ","30/Jul/18 15:48;guozhang;I've granted you the permission and you should be able to create new KIP pages now.","31/Jul/18 21:58;nafshartous;[~balaprassanna] Please review before I take this to the dev mailing list for discussion

https://cwiki.apache.org/confluence/display/KAFKA/KIP-349%3A+Priorities+for+Source+Topics","01/Aug/18 06:34;balaprassanna;[~nafshartous] I am okay with the KIP","16/Aug/18 12:00;nafshartous;[~guozhang] I called for a VOTE on the dev list a few days ago.  Only one person has voted so far (-1 nonbinding).  Any suggestions on how to proceed, i.e. send a reminder soon or just drop the effort ?  ","29/Aug/18 21:13;nafshartous;There was a comment during the vote that we should look at Samza's {{MessageChoose}} 
 https://samza.apache.org/learn/documentation/0.7.0/api/javadocs/org/apache/samza/system/chooser/MessageChooser.html","30/Aug/18 13:12;mjsax;[~nafshartous] It's your KIP and thus your decision if you want to follow this suggesting or not. It would be good to understand the advantages and disadvantage of your current (simplified?) proposal compared to the suggestion. What you can do/not do with the or the other? Would it be possible to extend your current proposal with something more generic later on?

You can also just continue the VOTE if you are not interested to change the proposal. A single non-binding downvote does not block the KIP. I will also follow up on the mailing list.

Note, that anybody can do a KIP and she/he is in the ""driver seat"" to lead the discussion. Thus, you can agree with any other proposal and change your KIP or argue against it (why do you think it's not necessary etc.). In the end, if you can convince the community that your KIP is valuable you will get it through. Just keep discussion and sharing your point of view on the mailing list, why you want to do what, and lay out the advantages/disadvantages compare to alternative proposals.","05/Sep/18 23:23;nafshartous;[~balaprassanna] There is a question today in the dev list, see subject  

   [DISCUSS] KIP-349 Priorities for Source Topics

about the use-cases and the need for this feature.  Perhaps you could respond and elaborate on your motivational use-cases.  

[~balaprassanna] Or if its easier you could elaborate the use-cases here and I could post on the dev list.  Its important to clearly establish the use-cases in order to gain support for this feature.  
","22/Oct/18 19:41;nafshartous;[~balaprassanna] Just checking again if you can help with elaborating on your use-cases ?  This would make it easier to facilitate a vote on this feature request.  ","22/Oct/18 23:06;balaprassanna;[~nafshartous] Sorry about the delay in reply. We use Kafka to process the asynchronous events of our Document Management System such as preview generation, indexing for search etc. The traffic gets generated via Web and Desktop Sync application. In such cases, we had to prioritize the traffic from web and consume them first. But this might lead to the starvation of events from sync if the consumer speed is slow and the event rate is high from web. A solution to handle the starvation with a timeout after which the events are consumed normally for a specified period of time would be great and help us use our resources effectively.","19/May/20 11:06;wlsc;any news on this?","19/May/20 11:29;nafshartous;After long discussion on the dev list there was not enough support for this feature.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automatic co-partitioning of topics via automatic intermediate topic with matching partitions,KAFKA-6182,13116720,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,astubbs,astubbs,07/Nov/17 16:10,18/May/20 17:48,12/Jan/21 10:06,15/May/20 02:21,1.0.0,,,,,,,,,,streams,,,,,,0,,,,,"Currently it is up to the user to ensure that two input topics for a join have the same number of partitions, and if they don't, manually create an intermediate topic, and send the stream #through that topic first, and then performing the join.
It would be great to have Kafka streams detect this and at least give the user the option to create an intermediate topic automatically with the same number of partitions as the topic being joined with.

See https://docs.confluent.io/current/streams/developer-guide.html#joins-require-co-partitioning-of-the-input-data",,astubbs,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-10-21 23:57:42.287,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 17:48:07 UTC 2020,,,,,,,"0|i3mhzb:",9223372036854775807,,,,,,,,,,,,,,,,"21/Oct/19 23:57;mjsax;Does KIP-221 ([https://cwiki.apache.org/confluence/display/KAFKA/KIP-221%3A+Enhance+DSL+with+Connecting+Topic+Creation+and+Repartition+Hint]) meet the request of this ticket? Repartitioning is not fully automated, but a full automation is rather hard, because at DSL->Topology translation time, the number of topic partitions are not known and hence, a repartition topic cannot be inserted easily.","15/May/20 02:21;mjsax;KIP-221 is implemented now. Closing this ticket.","18/May/20 10:53;astubbs;Ah yes that makes sense. Assuming the exception thrown when the partition mismatch is found at runtime would cause the entire system to fail fast, I think yes, this repartition operation solves the issue.","18/May/20 17:48;mjsax;Failing fast is not covered via KIP-221, but there are other tickets that would address it:  KAFKA-10015 and KAFKA-4748 seem related for a ""fail fast"" feature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Alternative Partitioner to Support ""Always Round-Robin"" partitioning",KAFKA-3333,12946995,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,manmedia@gmail.com,spowis@salesforce.com,spowis@salesforce.com,04/Mar/16 15:03,08/May/20 23:37,12/Jan/21 10:06,09/Jul/19 20:10,,,,,,,,2.4.0,,,clients,,,,,,0,kip,,,,"KIP: [https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=89070828]

Please Look into KAFKA-7358 for the official description **

The [DefaultPartitioner|https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java] typically distributes using the hash of the keybytes, and falls back to round robin if there is no key. But there is currently no way to do Round Robin partitioning if you have keys on your messages without writing your own partitioning implementation.

I think it'd be helpful to have an implementation of straight Round Robin partitioning included with the library.",,cmccabe,githubbot,hachikuji,ijuma,manmedia@gmail.com,spowis@salesforce.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-04 15:15:46.326,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 28 23:20:35 UTC 2020,,,,,,,"0|i2u5s7:",9223372036854775807,,,,,,,,,,,,,,,,"04/Mar/16 15:03;spowis@salesforce.com;Will link to a github pull request shortly.","04/Mar/16 15:15;githubbot;GitHub user Crim opened a pull request:

    https://github.com/apache/kafka/pull/1012

    [KAFKA-3333] - Add RoundRobinPartitioner

    https://issues.apache.org/jira/browse/KAFKA-3333
    
    
    The DefaultPartitioner typically distributes using the hash of the keybytes, and falls back to round robin if there is no key. But there is currently no way to do Round Robin partitioning if you have keys on your messages without writing your own partitioning implementation.
    
    I think it'd be helpful to have an implementation of straight Round Robin partitioning included with the library.


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Crim/kafka KAFKA-3333

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1012.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1012
    
----
commit 7a6781dfdfd1cbb34976a8dc37f0be72c644257b
Author: stephen powis <spowis@salesforce.com>
Date:   2016-03-04T15:14:42Z

    [KAFKA-3333] - Add RoundRobinPartitioner

----
","04/Mar/16 15:16;spowis@salesforce.com;https://github.com/apache/kafka/pull/1012","13/Sep/16 00:55;hachikuji;I think this change probably calls for a KIP since it's a change to the public API. Moving to 0.10.2.0 for now.","13/Sep/16 13:36;spowis@salesforce.com;I'm not advocating for *replacing* the existing implementation, but including this implementation in the library to avoid others from having to put together something custom for what seems like (to me anyhow) a common use case.

I wouldn't think that warrants a ""major improvement"" as defined here: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals  But happy to put something together if you think it does.

Thanks!","13/Sep/16 23:50;hachikuji;Admittedly, the line may not be totally clear, but we have asked for KIPs for similar components on the consumer (see KIP-54 for example). In general, it's a good way to publicize the feature in order to gauge community interest and collect feedback. We've hesitated in the past to include additional partitioners because they can always be made available to users through a separate project. In this case, there might be some concern over the fact that a key can be sent to multiple partitions which breaks the usual binding of key and partition, but perhaps it makes sense for some use cases.","25/Jan/17 16:34;ijuma;This falls under ""Any change that impacts the public interfaces of the project"" from the KIP page. Removing the ""fix version"" for now, we can add it back once the proposal has been voted.","20/May/19 14:32;manmedia@gmail.com;i am taking over since a KIP has been voted and accepted (KIP-369)","20/May/19 14:50;githubbot;mmanna-sapfgl commented on pull request #6771: KAFKA-3333: Adds RoundRobinPartitioner with tests
URL: https://github.com/apache/kafka/pull/6771
 
 
   Adds a new partitioner ""RoundRobinPartitioner"" for users to use. This is to be used when records are expected to be written to partitions in a ""Round Robin"" fashion regardless of a record key.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","20/May/19 18:28;cmccabe;Moving this out of 2.3 and into 2.4, since it missed feature freeze.","09/Jul/19 18:46;githubbot;cmccabe commented on pull request #6771: KAFKA-3333: Adds RoundRobinPartitioner with tests
URL: https://github.com/apache/kafka/pull/6771
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","09/Jul/19 20:10;cmccabe;Committed","28/Mar/20 23:20;githubbot;kkonstantine commented on pull request #1012: [KAFKA-3333] - Add RoundRobinPartitioner
URL: https://github.com/apache/kafka/pull/1012
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add InsertHeader and DropHeaders connect transforms KIP-145,KAFKA-8863,13254406,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,alozano3,alozano3,03/Sep/19 14:02,06/May/20 09:46,12/Jan/21 10:06,,,,,,,,,,,,clients,KafkaConnect,,,,,1,,,,,"[https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect]

Continuing the work done in the PR [https://github.com/apache/kafka/pull/4319] implementing the transforms to work with headers would be awesome.",,alozano3,kiwiandy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 03 15:23:48 UTC 2019,,,,,,,"0|z069zc:",9223372036854775807,,,,,,,,,,,,,,,,"03/Sep/19 15:23;alozano3;I am already working on this issue:
https://github.com/apache/kafka/pull/7284",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add task-level active-process-ratio to Streams metrics,KAFKA-9753,13293618,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,24/Mar/20 19:08,30/Apr/20 16:57,12/Jan/21 10:06,31/Mar/20 23:40,,,,,,,,2.6.0,,,streams,,,,,,0,kip,,,,"This is described as part of KIP-444 (which is mostly done in 2.4 / 2.5).

[https://cwiki.apache.org/confluence/display/KAFKA/KIP-444%3A+Augment+metrics+for+Kafka+Streams]",,githubbot,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-27 00:30:07.102,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 06 22:30:36 UTC 2020,,,,,,,"0|z0cuvc:",9223372036854775807,,,,,,,,,,,,,,,,"27/Mar/20 00:30;githubbot;guozhangwang commented on pull request #8370: KAFKA-9753: Add active tasks process ratio
URL: https://github.com/apache/kafka/pull/8370
 
 
   Measure the percentage ratio the stream thread spent on processing each task among all assigned active tasks.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","27/Mar/20 00:38;githubbot;guozhangwang commented on pull request #8371: KAFKA-9753: A few more metrics to add
URL: https://github.com/apache/kafka/pull/8371
 
 
   Thread-level: 
   
   * avg / max number of records polled from the consumer per runOnce, INFO
   * avg / max number of records processed by the task manager (i.e. across all tasks) per runOnce, INFO
   
   Task-level: 
   
   * number of current buffered records at the moment (i.e. it is just a dynamic gauge), DEBUG.
   
   To be discussed after https://github.com/apache/kafka/pull/8370
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","31/Mar/20 23:39;githubbot;guozhangwang commented on pull request #8370: KAFKA-9753: Add active tasks process ratio
URL: https://github.com/apache/kafka/pull/8370
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","06/Apr/20 22:30;githubbot;guozhangwang commented on pull request #8371: KAFKA-9753: A few more metrics to add
URL: https://github.com/apache/kafka/pull/8371
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Better Scaling Experience for KStream,KAFKA-8019,13218705,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,bchen225242,bchen225242,bchen225242,28/Feb/19 17:53,30/Apr/20 16:56,12/Jan/21 10:06,08/Mar/20 23:51,,,,,,,,,,,streams,,,,,,4,kip,,,,"In our day-to-day work, we found it really hard to scale up a stateful stream application when its state store is very heavy. The caveat is that when the newly spinned hosts take ownership of some active tasks, so that they need to use non-trivial amount of time to restore the state store from changelog topic. The reassigned tasks would be available for unpredicted long time, which is not favorable. Secondly the current global rebalance stops the entire application process, which in a rolling host swap scenario would suggest an infinite resource shuffling without actual progress.

Following the community's [cooperative rebalancing|https://cwiki.apache.org/confluence/display/KAFKA/Incremental+Cooperative+Rebalancing%3A+Support+and+Policies] proposal, we need to build something similar for KStream to better handle the auto scaling experience.",,ableegoldman,astubbs,bchen225242,eivind.aubert,lmontrieux,marcosmaia,mjsax,nfo,rocketraman,shafqat,Yohan123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6145,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-08 23:51:30.14,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 08 23:51:30 UTC 2020,,,,,,,"0|z00788:",9223372036854775807,,,,,,,,,,,,,,,,"08/Mar/20 23:51;mjsax;Closing this a duplicate of KAFKA-6145. Should be address via KIP-441.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Streams should support self-join on streams,KAFKA-7497,13190866,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,ableegoldman,rmoff,rmoff,11/Oct/18 10:22,25/Apr/20 00:17,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,needs-kip,,,,"There are valid reasons to want to join a stream to itself, but Kafka Streams does not currently support this ({{Invalid topology: Topic foo has already been registered by another source.}}).  To perform the join requires creating a second stream as a clone of the first, and then doing a join between the two. This is a clunky workaround and results in unnecessary duplication of data.",,guozhang,mjsax,rmoff,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-10-11 18:46:34.219,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 15 23:26:03 UTC 2019,,,,,,,"0|i3z2pj:",9223372036854775807,,,,,,,,,,,,,,,,"11/Oct/18 18:46;mjsax;It is correct, that a topic cannot be consumed twice (cf. https://issues.apache.org/jira/browse/KAFKA-6687) and I also agree that a self-join operator would be useful.

Once could express a self-joining like this in current Streams API:
{noformat}
KStream stream = builder.stream(...);
stream.join(stream, ...);{noformat}
However, the execution of the join would not be efficient, as two state stores with two changelog topics would be created (both containing the exact same data). Also, and this seems to be the most severs issue, each record would join with itself, what is actually not desired...

I marked this as ""needs-kip"" but I am not 100% sure if we would need a KIP though. Maybe, Kafka Streams could detect internally, that left hand side KStream and right hand side KStream is the same object and just use a different operator implementation (ie, a dedicated self-join processor). This way, no public API change would be required.","11/Oct/18 21:21;guozhang;[~rmoff] Before we dive into the implementation details, could you list the motivations for self-joins in Streams? Note that since stream-join only support key-based join now, self-join should be well covered by just ""enrich"" the stream record via `map` etc.","11/Oct/18 22:46;mjsax;[~guozhang] I cannot follow? A self-join would still have a sliding-join-window, and thus, all records with the same key within the window would be joined. How can a stateless map achieve this? I want to add, that a sliding-window aggregation might allow to compute the same thing – note, though, that Kafka Streams only supports hopping/tumbling windows for aggregations atm, but no sliding windows.

For the use case: this is also unclear to me to be honest though.","12/Oct/18 08:51;rmoff;Here's the use case, inspecting a stream of transactions looking for possible fraud: https://github.com/confluentinc/demo-scene/blob/master/ksql-atm-fraud-detection/ksql-atm-fraud-detection.adoc","12/Oct/18 16:43;guozhang;[~mjsax] You're right, I was only thinking about the use cases that the stream needs some enrichment that are independent of any other streams / tables; I also agree with you that even with sliding window, it seems windowed aggregations should be sufficient still.

[~rmoff] I looked at the use case, and it seems to be a better fit with ""session window"" aggregations of KStream than using KStream self-join: https://kafka.apache.org/20/javadoc/org/apache/kafka/streams/kstream/SessionWindows.html","15/Jan/19 04:12;vvcephei;Note, the use-case reference above is now at: [https://github.com/confluentinc/demo-scene/blob/master/ksql-atm-fraud-detection/ksql-atm-fraud-detection-README.adoc]

 

I _think_ I see the rationale of this ask:

I guess the difference between a windowed aggregation and a self-join is that the windowed aggregation would require you to save all the occurrences of the key in the window and do your computation (which is a pairwise computation) over the collection, whereas the self-join naturally gives you a stream of all the pairwise matches when the same key re-occurs within the window in the stream.

IIUC, you can do the same computation either way, but it's more naturally expressed as a pairwise comparison, so the self-join is more ergonomic? (Although, if there are two fraudulent transactions on the same account, they would show up in the existing program as two independent potential frauds, whereas the aggregation method gives you the opportunity to generate just one fraud report with two occurrences)

 

On the other hand, I'm struggling to see the semantics of this feature clearly.

As I understand it, the semantics of a stream-stream join in general is that you have a stream of unique events U=<u1, u2, u3> and another stream of unique events V=<v1, v2, v3>, and you effectively want to ""zip"" them to produce J=<(u1,v1), (u2,v2), (u3,v3)>. Note that the indices are the identifying key, and they are unique. The standard stream processing complications apply, one side of a pair may be arbitrarily delayed or disordered, which leads to the need for memory on one or both sides of the join.

The use case described in the document linked above seems different; the identifying information isn't unique, in that the same account id appears on an arbitrary number of events in the stream, but the account id is what we use as the joining key. It sounds similar to the ""streaming similarity self-join"" problem, which I found researching this issue: [http://www.vldb.org/pvldb/vol9/p792-defranciscimorales.pdf] . Under that definition, the ""join"" is actually just a cartesian product of every record in both streams, and for each pair, you compute a similarity, keeping (or discarding) only the pairs that have an above-threshold similarity score. There are a number of relaxations/optimizations required to do this efficiently, which in our case means limiting the product to only those records that already share the same account id, and of course limiting the join temporally.

 

I suppose my question, after looking into this is (despite the shared name and the fact that the existing implementation happens to suit both), are these two operations really the same concept? If we say ""yes"", for example, then we may prohibit future optimizations. For example, in the former stream-stream join, you know that the events' keys are unique, so once you produce a pair, you can immediately forget both of the input events for it. But for the similarity-join, you have to remember the input events until the pre-defined join window closes.

 ","15/Jan/19 04:30;mjsax;I disagree ""that you have a stream of unique events"" – the join condition is defined on the record key but the record key is not a primary key for streams: for example, you can have a stream of clicks using the page-id as key. Also note, that each record might join multiple times, not just once.
{quote}one side of a pair may be arbitrarily delayed or disordered, which leads to the need for memory on one or both sides of the join.{quote}
Not sure what you mean by this. If you refer to the join window, I think this is two different thing. ""Delay"" or ""disorder"" seem to refer to wall-clock time, but the join is defined on event-time. Thus, the semantics is to join events that happen temporarily close to each other.

This can be translated to the self-join case too: consider the clickstream example with page-id as key, it mean to return all pages, for which there is more than one click within the time window.

I don't think this is related to similarity joins at all.

 

 ","15/Jan/19 17:12;vvcephei;Thanks [~mjsax],

I see that the ""key"" field in Kafka can be set to anything. My question was about the semantics of a stream-stream join. I've read our javadoc, and all it says is that it does an ""inner equi-join"" restricted by the time window. I guess this means that, given two streams `U=<u1, u2, u3>` and `V=<v1, v2, v3>`, it produces at least one result pair `(ui, vj)` for each pair in the cartesian product of the streams such that `ui.key == vj.key` and `abs(ui.time - vj.time) <= window_size`. Under this definition, if we happen to set V := U, then the operation is still well defined.

It sounds like this is the precise ask, since at the moment, choosing `V := U` throws a runtime error, even though it's not semantically prohibited.

It does seem like part of the scope of work should be to implement it efficiently, that is, to detect that both streams are actually the same at topology-build-time and ensure that we only need one join window store.

If I understand this scoping correctly, there's no public API change, just a behavior change. Also, since it's currently not possible to start a topology with a stream self-join, there's no deprecation or migration plan needed. Therefore no KIP is required.

Sound good?","15/Jan/19 17:42;mjsax;You understanding seems correct. Also not the JavaDocs from `JoinWindows`
{quote}{{* In SQL-style you would express this join as}}
{{* }}
{{* SELECT * FROM stream1, stream2}}
{{* WHERE}}
{{*   stream1.key = stream2.key}}
{{*   AND}}
{{*   stream1.ts - before <= stream2.ts AND stream2.ts <= stream1.ts + after}}
{quote}
I agree, it's not necessarily a public API change. However, it's might still be a major change that we might want to back up with a KIP. Not sure. In the end, it's an optimization to void two state stores, because one state store should be sufficient to compute the self-join.","15/Jan/19 19:30;vvcephei;Ah. I was looking at the javadoc on `KStream#join`. My bad.","15/Jan/19 23:26;guozhang;From the expressiveness of the operators, I think there are cases of stream self-join that cannot be captured with stream aggregations still, since the window is really ""sliding"" (but if we add a sliding window type aggregations, it may equal to the semantics of streams self-join).

From the API point of view, I think allowing stream self join even assuming its use cases can be captured with sliding window aggregations still provides programmability benefits. But the underlying implementation should be different to any of our current internal impls. I think we can still have an umbrella KIP that includes the following:

1. Add sliding window based aggregations.
2. Allow windowed stream self-join; and when detected it convert it to a sliding window based aggregation behind the scene for efficient implementations. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ability to specify a default state store type or factory,KAFKA-6910,13159762,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,,astubbs,astubbs,16/May/18 15:39,10/Apr/20 16:29,12/Jan/21 10:06,10/Apr/20 16:29,1.1.0,1.1.1,,,,,,,,,streams,,,,,,0,,,,,"For large projects, it's a huge pain and not really practically at all to use a custom state store everywhere just to use in memory or avoid rocksdb, for example for running a test suite on windows.

It would be great to be able to set a global config for KS so that it uses a different state store implementation everywhere.

Blocked by KAFKA-4730 - Streams does not have an in-memory windowed store. Also blocked by not having an in-memory session store implementation. A version simple in memory window and session store that's not suitable for production would still be very useful for running test suites.",,astubbs,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-16 15:39:12.0,,,,,,,"0|i3tsdr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add rename topic support,KAFKA-2333,12844791,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,granthenke,granthenke,granthenke,13/Jul/15 21:08,08/Apr/20 19:22,12/Jan/21 10:06,08/Apr/20 19:22,,,,,,,,,,,,,,,,,10,,,,,"Add the ability to change the name of existing topics. 

This likely needs an associated KIP. This Jira will be updated when one is created.",,Blake Atkinson,bruce.szalwinski,eribeiro,gwenshap,idofr,jkreps,kiwiandy,naturallyintelligent1@gmail.com,noslowerdna,sliebau,slowenthal,stephane.maarek@gmail.com,vepo,zackdever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-15 22:35:09.221,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 19:22:45 UTC 2020,,,,,,,"0|i2h7en:",9223372036854775807,,,,,,,,,,,,,,,,"15/Jul/15 22:35;jkreps;Word. Probably does need a KIP, though.

Presumably the interface would be
  bin/kafka-topics.sh --zookeeper xyz --topic old-name --rename new-name

I think the question is what the expected behavior is if there are existing producers and consumers for the topic with the old name when the rename occurs (presumably the answer is either that you get an error or the topic is recreated via auto-create depending on your settings).","15/Jul/15 22:49;gwenshap;In pretty much every database in existence writing to a renamed table will cause errors, so thats expected. 
I'm more concerned about in-flight replications, there's a reason delete took 3 attempts to get right. I hope we can reuse some of that code / tests.

Does it make sense to say that for the future, anything with user-visible name should also have an internal name that will never ever changes?
","15/Jul/15 22:49;granthenke;I am thinking the expected behavior if there are existing producers and consumers should likely be kept uniform with the delete functionality but we can discuss that on a the KIP proposal. ","15/Jul/15 22:59;jkreps;[~gwenshap] Yeah totally, agree it will be tricky. The name is shared across the directory, zk, the checkpoint files, and the offset commit topic. You are probably right that the idea thing would be to have a topic id which is unique and associated name which is just in zk but that itself is a pretty massive change since none of these persistent formats are that way now.","01/Apr/20 11:57;sliebau;Is this something we are willing to investigate further? After almost five years of inactivity I am inclined to close this if no objections are voiced in the next few days.","08/Apr/20 19:22;sliebau;As this has been dormant for a long time and no one reacted to my comment I'll close this for now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide an official Docker Hub image for Kafka,KAFKA-7249,13177006,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,timhigins,timhigins,06/Aug/18 02:51,27/Mar/20 10:21,12/Jan/21 10:06,,1.0.1,1.1.0,1.1.1,2.0.0,,,,,,,build,documentation,packaging,tools,website,,0,build,distribution,docker,packaging,"It would be great if there was an official Docker Hub image for Kafka, supported by the Kafka community, so we knew that the image was trusted and stable for use in production. Many organizations and teams are now using Docker, Kubernetes, and other container systems that make deployment easier. I think Kafka should move into this space and encourage this as an easy way for beginners to get started, but also as a portable and effective way to deploy Kafka in production. 

 

Currently there are only Kafka images maintained by third parties, which seems like a shame for a big Apache project like Kafka. Hope you all consider this.

 

Thanks,

Tim",,cricket007,serge.travin,timhigins,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9774,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-09-15 17:11:39.859,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 15 17:11:39 UTC 2018,,,,,,,"0|i3wppz:",9223372036854775807,,,,,,,,,,,,,,,,"15/Sep/18 17:11;cricket007;Hello [~timhigins], do you have any issues using the confluentinc/cp-kafka and confluentinc/cp-zookeeper images? 

Confluent, as the enterprise support company for Kafka, currently maintains up-to-date production-ready images that closely follow each Apache release, and those images are also supported for Kubernetes deployments from the Confluent Helm Charts. The other components of the Confluent Platform are not required. 

I will point out that the current official Zookeeper image on DockerHub is maintained by a third-party as well, not part of the Zookeeper project release cycle. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor the main loop to process more than one record of one task at a time,KAFKA-9756,13293651,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,24/Mar/20 21:50,27/Mar/20 05:54,12/Jan/21 10:06,27/Mar/20 05:54,,,,,,,,2.6.0,,,streams,,,,,,0,,,,,"Our current main loop is implemented as the following:

1. Loop over all tasks that have records to process, each time process one record at a time.
2. After finish processing one record from each task, check if commit / punctuate / pool etc is needed.

Because we process one record at a time from the task and then moves on to the next task, we are effectively spending lots of time on context switches. Maybe we can first investigate what if we just have each task to be hosted by an individual thread, and see if the context switch cost is is not worse already (which means our current implementation is already a baseline). If that's true we can consider working on one task at a time, and see if it is more efficient.


For num.Iterations:
1. process one record from each of the tasks thread owns.
2. check if commit / punctuate / poll / etc needed.

But in 1) above we process tasks A,B,C,A,B,C,... and effectively we are introducing context switches within the thread as it needs to load the task variables etc for each record processed.

What I was thinking is to process tasks as A,A,A,B,B,B,C,C,C... so that we can reduce the context switches.",,ableegoldman,githubbot,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-26 01:01:36.07,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 05:54:14 UTC 2020,,,,,,,"0|z0cv2o:",9223372036854775807,,,,,,,,,,,,,,,,"26/Mar/20 01:01;githubbot;guozhangwang commented on pull request #8358: KAFKA-9756: Process more than one record of one task at a time
URL: https://github.com/apache/kafka/pull/8358
 
 
   1. Within a single while loop, process the tasks in AAABBBCCC instead of ABCABCABC. This also helps the follow-up PR to time the per-task processing ratio to record less time, hence less overhead.
   
   2. Add thread-level process / punctuate / poll / commit ratio metrics.
   
   3. Fixed a few issues discovered (inline commented).
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","27/Mar/20 05:54;githubbot;guozhangwang commented on pull request #8358: KAFKA-9756: Process more than one record of one task at a time
URL: https://github.com/apache/kafka/pull/8358
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow overriding producer & consumer properties at the connector level,KAFKA-4159,13004670,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,shikhar,shikhar,13/Sep/16 17:47,21/Mar/20 23:27,12/Jan/21 10:06,15/Oct/19 16:42,,,,,,,,,,,KafkaConnect,,,,,,1,needs-kip,,,,"As an example use cases, overriding a sink connector's consumer's partition assignment strategy.",,ewencp,githubbot,hachikuji,kkonstantine,Natengall,rhauch,shikhar,shrijeet,sjdurfey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8265,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-14 17:00:33.147,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 21 23:27:28 UTC 2020,,,,,,,"0|i33kzb:",9223372036854775807,,,,,,,,,,,,,,,,"14/Feb/17 17:00;sjdurfey;I had a use case for this feature. I wanted to be able to group an arbitrary number of connectors under the same consumer group name, without requiring that all connectors have the same group name (which would happen overriding group.id at the worker config). So, this change effectively does the same thing overriding at the worker level does, except allowing it at the connector level. 

I'm not able to assign to this myself. 

https://github.com/apache/kafka/pull/2548","14/Feb/17 17:40;hachikuji;[~sjdurfey] I've added you as a contributor. You should be able to assign this and other Kafka jiras.","22/Mar/17 18:11;ewencp;[~sjdurfey] Allowing configs like this is a public API change, so it'd need a KIP. You can find the instructions for a KIP here: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals It might sound like quite a bit of overhead, but for simple changes (especially additions with no compatibility concerns) it's usually very straightforward. Once that's through a PR could be merged.

That said, I'm curious about the use case. Why would multiple connectors need to share the same consumer group? Why couldn't you just change the number of tasks for the connector? This seems like quite an unusual use case, so I'm not sure it's something that makes sense in the framework. More generally, I personally tend to view the producer and consumer (and their configs) as implementation details -- I think exposing all configs might be masking other issues that could be better addressed by the framework. To me, the motivation section of a KIP would be important in this case. (But that's just my viewpoint, I know other people have asked about this in the past.)","23/Mar/17 16:22;sjdurfey;So, full disclosure, I believe my original desire for overriding the group.id is alleviated once I can upgrade to kafka 0.10.2.0 and use single message transforms. 

The current use case for me was pulling data from a sql server database over jdbc to push into kafka. In the platform the data is organized into topics with a source identifier in the topic name with our own wrapper avro for the payload to provide additional metadata about the data coming from the database. Because the data needed to be wrapped I wasn't able to write directly to these topics with kafka connect, so they needed to be written to different topics. I had additional connectors pulling from those topics to transform the data into the format I needed and had a custom sink connector write out the wrapped paylods. To generate the metadata it was specific to a table, so the design had a transformation connector per topic that kafka connect wrote to. 

I wanted to be able to provide progress of transforming this data through the [kafka offset monitor|https://github.com/quantifind/KafkaOffsetMonitor], but having a connector per topic with a unique group.id per connector really made the offset monitor almost useless, as there were just way too many consumer groups, and as we scaled for more databases, it was only going to get worse. Since all the tables belonged to one source, I wanted to be able to group all the topics for that one source into a single identifier. That way I could just click on that identifier in the offset monitor and view the progress for all topics for that particular source. 

I could've overridden the group.id at the worker level and that would work as well, as long as the connect instance was only ever going to be for that source. However, I think long term that wasn't going to be the case. ","18/Sep/17 22:13;shrijeet;Few other scenarios where this would be useful

* Being able to control `max.poll.records` etc. per connector. I have a custom connector which is slow to process a record compared to others. I don't want to decrease `max.poll.records` for all connectors, just mine.
* Being able to set the desired `auto.offset.reset` behavior per connector ","11/Jul/19 22:38;kkonstantine;This functionality was discussed again in more recent tickets, which were also accompanied by KIPs. Specifically: 
 https://issues.apache.org/jira/browse/KAFKA-6890

and 
 https://issues.apache.org/jira/browse/KAFKA-8265

Given that the latter and most recent was approved and merged, I'd suggest at least closing this ticket here as superseded by https://issues.apache.org/jira/browse/KAFKA-8265 and https://cwiki.apache.org/confluence/display/KAFKA/KIP-458%3A+Connector+Client+Config+Override+Policy

[~rhauch] [~ewencp] thoughts?","15/Oct/19 16:41;rhauch;This has been implemented as part of [KIP-458|https://cwiki.apache.org/confluence/display/KAFKA/KIP-458%3A+Connector+Client+Config+Override+Policy] and KAFKA-8265.","15/Oct/19 16:42;rhauch;This has been implemented as part of [KIP-458|https://cwiki.apache.org/confluence/display/KAFKA/KIP-458%3A+Connector+Client+Config+Override+Policy] and KAFKA-8265.","21/Mar/20 23:27;githubbot;kkonstantine commented on pull request #2548: KAFKA-4159: allow consumer/specific configs at connector level
URL: https://github.com/apache/kafka/pull/2548
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create partition compaction analyzer,KAFKA-1336,12703628,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,,jkreps,jkreps,25/Mar/14 23:24,19/Mar/20 09:48,12/Jan/21 10:06,19/Mar/20 09:48,,,,,,,,,,,,,,,,,1,,,,,"It would be nice to have a tool that given a topic and partition reads the full data and outputs:
1. The percentage of records that are duplicates
2. The percentage of records that have been deleted",,eliasdorneles,jkreps,sliebau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-11 20:50:49.435,,,false,,,,,,,,,,,,,,,,,,381962,,,Thu Mar 19 09:48:03 UTC 2020,,,,,,,"0|i1tt2n:",382237,,,,,,,,,,,,,,,,"11/Mar/20 20:50;sliebau;While I agree in principle that this information might be useful, the age and inactivity of this ticket suggests to me that this is not a priority and might be closed?
","19/Mar/20 09:48;sliebau;Closed due to long inactivity and no comment upon inquiry.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-dump-log.sh to support dumping only head or tail part,KAFKA-9728,13292105,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,weichu,weichu,17/Mar/20 04:21,17/Mar/20 04:21,12/Jan/21 10:06,,,,,,,,,,,,tools,,,,,,0,,,,,"When I use {{kafka-dump-log.sh}} to dump a .log file, its behavior is to traverse through the whole file which is quite heavy.

Even with {{| head}}, the tool will still read through the whole file.

In many cases I just want to inspect the first several messages or last several messages of a log file.

It will be great to have {{kafka-dump-log.sh}} to add options support partial reading.",,weichu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-03-17 04:21:45.0,,,,,,,"0|z0cliw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add rocksdb event listeners in KS,KAFKA-9588,13286649,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,NaviBrar,NaviBrar,21/Feb/20 04:54,12/Mar/20 19:20,12/Jan/21 10:06,,,,,,,,,3.0.0,,,streams,,,,,,0,,,,,"Rocsdb is coming up with the support of event listeners(like onCompactionCompleted) in jni ([https://github.com/facebook/rocksdb/issues/6343]) which would be really helpful in KS to trigger checkpointing on flush completed due to filling up of memtables, rather than doing it periodically etc. This task is currently blocked on https://issues.apache.org/jira/browse/KAFKA-8897.",,ableegoldman,cadonna,lkokhreidze,mjsax,NaviBrar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-02-21 04:54:25.0,,,,,,,"0|z0bq6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
supporting replication.throttled.replicas in dynamic broker configuration,KAFKA-7983,13217504,New Feature,In Progress,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,d8tltanc,junrao,junrao,22/Feb/19 18:11,12/Mar/20 06:34,12/Jan/21 10:06,,,,,,,,,,,,core,,,,,,2,,,,,"In [KIP-226|https://cwiki.apache.org/confluence/display/KAFKA/KIP-226+-+Dynamic+Broker+Configuration#KIP-226-DynamicBrokerConfiguration-DefaultTopicconfigs], we added the support to change broker defaults dynamically. However, it didn't support changing leader.replication.throttled.replicas and follower.replication.throttled.replicas. These 2 configs were introduced in [KIP-73|https://cwiki.apache.org/confluence/display/KAFKA/KIP-73+Replication+Quotas] and controls the set of topic partitions on which replication throttling will be engaged. One useful case is to be able to set a default value for both configs to * to allow throttling to be engaged for all topic partitions. Currently, the static default value for both configs are ignored for replication throttling, it would be useful to fix that as well.",,amironov,d8tltanc,githubbot,huxi_2b,johanl,junrao,trueneu,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-04-04 10:46:31.915,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 06:34:44 UTC 2020,,,,,,,"0|yi1a7c:",9223372036854775807,,,,,,,,,,,,,,,,"04/Apr/19 10:46;huxi_2b;[~junrao] Seems these two configs could be modified dynamically through kafka-configs. ","04/Apr/19 23:03;junrao;[~huxi_2b], leader.replication.throttled.replicas and follower.replication.throttled.replicas are dynamically set through ReplicationQuotaManager.markThrottled() at the topic level. However, these two properties don't exist at the broker level config and BrokerConfigHandler doesn't call ReplicationQuotaManager.markThrottled(). So, currently, we can't set leader.replication.throttled.replicas and follower.replication.throttled.replicas at the broker level either statically or dynamically.","05/Apr/19 11:22;huxi_2b;If specified at broker level, should them follow the format of [partitionId]:[brokerId],[partitionId]:[brokerId],... ?","05/Apr/19 16:20;junrao;Probably. But I think the common broker level setting will be just *.","08/Apr/19 10:21;rahul.mnit;Can you please assign this ticket to me? 

Jira Id: rahul.mnit","08/Apr/19 16:15;junrao;[~rahul.mnit], just added you to the contributor list and assigned the ticket to you. Thanks.","21/May/19 09:10;rahul.mnit;I am working on it. Will submit the patch once ready.","12/Dec/19 11:59;amironov;Hey [~rahul.mnit] any updates on this?","05/Mar/20 20:55;d8tltanc;[~rahul.mnit] I'm going to create a KIP for adding a group of new dynamic configs. Are you still working on this? Thank you.","12/Mar/20 06:31;githubbot;d8tltanc commented on pull request #8283: [WIP] KAFKA-7983: supporting replication.throttled.replicas in dynamic broker configuration
URL: https://github.com/apache/kafka/pull/8283
 
 
   **More detailed description of your change**
   
   > In KIP-226, we added the support to change broker defaults dynamically. However, it didn't support changing leader.replication.throttled.replicas and follower.replication.throttled.replicas. These 2 configs were introduced in KIP-73 and controls the set of topic partitions on which replication throttling will be engaged. One useful case is to be able to set a default value for both configs to * to allow throttling to be engaged for all topic partitions. Currently, the static default value for both configs are ignored for replication throttling, it would be useful to fix that as well.
   
   > leader.replication.throttled.replicas and follower.replication.throttled.replicas are dynamically set through ReplicationQuotaManager.markThrottled() at the topic level. However, these two properties don't exist at the broker level config and BrokerConfigHandler doesn't call ReplicationQuotaManager.markThrottled(). So, currently, we can't set leader.replication.throttled.replicas and follower.replication.throttled.replicas at the broker level either statically or dynamically.
   
   In this patch, we introduced two new dynamic broker configs, both of them are type of boolean:
   
   ""leader.replication.throttled"" (default: false)
   
   ""follower.replication.throttled"" (default: false)
   
   If ""leader.replication.throttled"" is set to ""true"", all leader brokers will be throttled. Similarly, if ""follower.replication.throttled"" is set to ""true"", all follower brokers will be throttled. The throttle mechanism is introduced in KIP-73. 
   
   To implement the broker level throttle, I added a new class variable to ReplicationQuotaManager. The BrokerConfigHandler will call updateBrokerThrottle() and update this class variable upon receiving the config change notification from ZooKeeper. ReplicationQuotaManager::isThrottled()
   
   *Summary of testing strategy (including rationale)
   
   Added ReplicationQuotaManagerTest::shouldBrokerLevelThrottleAffectAllTopicPartition() to test if all topic partitions will be throttled when the broker is throttled.
   
   I'm currently working on adding more tests.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","12/Mar/20 06:34;d8tltanc;[https://github.com/apache/kafka/pull/8283]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka TestKit library,KAFKA-5041,13062471,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,ijuma,ijuma,ijuma,07/Apr/17 16:09,27/Feb/20 17:17,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,3,kip,,,,We should introduce a library that makes it easy for people to write tests that involve a Kafka cluster.,,clouTrix,cryptoe,dthg,ijuma,kordzik,lindong,mjsax,Narendra Kumar,ondrej,smurakozi,t2y,tombentley,vindhya,vvcephei,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-4401,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-10-02 14:46:57.814,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 27 17:17:08 UTC 2020,,,,,,,"0|i3dd2v:",9223372036854775807,,,,,,,,,,,,,,,,"02/Oct/18 14:46;lindong;Moving this to 2.2.0 since PR is not ready yet.","17/Feb/19 19:11;mjsax;Moving all major/minor/trivial tickets that are not merged yet out of 2.2 release.","27/Feb/20 07:46;cryptoe;[~ijuma] are you actively working on this? We have an internal use-case where in we want to create clusters, stop clusters, produce/ consume from topics in a junit test case","27/Feb/20 17:17;vvcephei;Hi all, you may also wish to be aware of this recent thread:

https://lists.apache.org/thread.html/29e6846743bf3b4eb2025ff903fb0a0845e33fb9f277990490b3d8d0%40%3Cdev.kafka.apache.org%3E",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a topic based on the specified brokers,KAFKA-9300,13274555,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,doubleWei,doubleWei,doubleWei,14/Dec/19 10:07,16/Feb/20 00:45,12/Jan/21 10:06,,2.3.0,,,,,,,,,,clients,,,,,28/Feb/20 00:00,0,,,,,"Generally, A Kafka cluster serves multiple businesses. To reduce the impact of businesses, many companies isolate brokers to physically isolate businesses. That is, the topics of certain businesses are created on the specified brokers. The current topic creation script supports only create topic according replica-assignment . This function is not convenient for the service to specify the brokers. Therefore, you need to add this function as follows: Create a topci based on the specified brokers. The replica-assignment-brokers parameter is added to indicate the broker range of the topic distribution. If this parameter is not set, all broker nodes in the cluster are used. For example, kafka-topics.sh --create --topic test06 --partitions 2 --replication-factor 1 --zookeeper zkurl -- --replica-assignment-brokers=1,2.",,doubleWei,huxi_2b,xinzhuxianshenger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,432000,432000,,0%,432000,432000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-12-16 08:22:58.686,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 19 08:06:57 UTC 2019,,,,,,,"0|z09okg:",9223372036854775807,,,,,,,,,,,,,,,,"16/Dec/19 08:22;huxi_2b;This might need a KIP.","16/Dec/19 14:42;doubleWei;[~huxi_2b]  I plan to add  a topic-creating  config 'replica-assignment-brokers' and the underlying layer still invokes AdminUtils.assignReplicasToBrokers( ). I do not think that the public API modification is involved and maybe KIP is not required.","19/Dec/19 08:06;huxi_2b;I mean a new config deserves a KIP to have people discuss whether it should be added:)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zookeeper migration tool support for TLS,KAFKA-8843,13253538,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rndgstn,purbon,purbon,28/Aug/19 16:26,10/Feb/20 02:24,12/Jan/21 10:06,08/Feb/20 15:52,,,,,,,,2.5.0,,,,,,,,,1,,,,,"Currently zookeeper-migration tool works based on SASL authentication. What means only digest and kerberos authentication is supported.

 

With the introduction of ZK 3.5, TLS is added, including a new X509 authentication provider. 

 

To support this great future and utilise the TLS principals, the zookeeper-migration-tool script should support the X509 authentication as well.

 

In my newbie view, this should mean adding a new parameter to allow other ways of authentication around [https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/admin/ZkSecurityMigrator.scala#L65. |https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/admin/ZkSecurityMigrator.scala#L65]

 

If I understand the process correct, this will require a KIP, right?

 ",,githubbot,gquintana,KellySchoenhofen,purbon,rng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-11-18 21:22:55.671,,,false,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,Sat Feb 08 15:47:12 UTC 2020,,,,,,,"0|z064mo:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,"28/Aug/19 16:26;purbon;If possible I would love to work on this with the support of the community.","29/Aug/19 17:10;purbon;working on writing the related KIP right now.","29/Aug/19 18:12;purbon;link to the related KIP: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-515%3A+Enable+ZK+client+to+use+the+new+TLS+supported+authentication]","18/Nov/19 21:22;KellySchoenhofen;Question, does ZK 3.5.6 allow for SSL (TLS, but let's say SSL to keep in line with the documentation) from Kafka? Not SASL_SSL, just plain SSL. Is that what this Jira is for? I have quorum TLS working in ZK 3.5.6, I added a tls-secured listener, but as of yet I can't quite get Kafka to connect to it:

{{[2019-11-18 15:03:11,545] INFO Opening socket connection to server xxx/x.x.x.x:2182. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)}}

is the closest I have come, but I didn't want do to SASL_SSL, I just want to secure the traffic between Kafka and ZooKeeper using TLS 1.2 and a specific class of cipher, like TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, and enforce the CN name on each side to match each other's cert & trusted cert stores (like how ZooKeeper Quorum TLS works). ","24/Jan/20 22:11;githubbot;rondagostino commented on pull request #8003: KAFKA-8843: KIP-515: Zookeeper TLS support
URL: https://github.com/apache/kafka/pull/8003
 
 
   Signed-off-by: Ron Dagostino <rdagostino@confluent.io>
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","27/Jan/20 19:38;gquintana;I am probably dreaming, but it would be great to have _zookeeper-client.properties_ config file on par with producer/consumer.properties config files, containing both TLS and optionnaly JAAS authentication settings:
{code:java}
zookeeper.client.secure=true
zookeeper.sasl.jaas.config=org.apache.zookeeper.server.auth.DigestLoginModule required username=""kafka"" password=""kafkapass"";
zookeeper.ssl.truststore.location=/etc/kafka/truststore.jks
zookeeper.ssl.truststore.password=truststorepass
{code}
As a result, the command line argument could be named _-zk-config-file_ instead of _-zk-tls-config-file_","08/Feb/20 15:47;githubbot;omkreddy commented on pull request #8003: KAFKA-8843: KIP-515: Zookeeper TLS support
URL: https://github.com/apache/kafka/pull/8003
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow fetching a key from a single partition rather than iterating over all the stores on an instance,KAFKA-9445,13279949,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,NaviBrar,NaviBrar,NaviBrar,16/Jan/20 18:01,31/Jan/20 09:38,12/Jan/21 10:06,30/Jan/20 07:46,,,,,,,,2.5.0,,,streams,,,,,,0,KIP-562,,,,"Whenever a call is made to get a particular key from a Kafka Streams instance, currently it returns a Queryable store that contains a list of the stores for all the running and restoring/replica(with KIP-535) on the instance via StreamThreadStateStoreProvider#stores(). This list of stores is then provided to CompositeReadOnlyKeyValueStore#get() which looks into each store one by one. With the changes that went in as a part of KIP-535 since we have access to the information that a key belongs to which partition, we should have a capability to fetch store for that particular partition and look for key in store for that partition only. It would be a good improvement for improving latencies for applications that contain multiple partitions on a single instance and don't have bloom filters enabled internally for Rocksdb.",,ableegoldman,cadonna,githubbot,mjsax,NaviBrar,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9487,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-01-19 10:39:18.809,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 30 07:45:53 UTC 2020,,,,,,,"0|z0al4g:",9223372036854775807,,,,,,,,,,,,,,,,"19/Jan/20 10:39;githubbot;brary commented on pull request #7984: [WIP: KAFKA-9445] Allow adding changes to allow serving from a specific partition
URL: https://github.com/apache/kafka/pull/7984
 
 
   KIP-562 implementation.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","30/Jan/20 07:45;githubbot;mjsax commented on pull request #7984: KAFKA-9445: Allow adding changes to allow serving from a specific partition
URL: https://github.com/apache/kafka/pull/7984
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow serving interactive queries from in-sync Standbys,KAFKA-6144,13113089,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,NaviBrar,astubbs,astubbs,30/Oct/17 17:36,29/Jan/20 17:43,12/Jan/21 10:06,16/Jan/20 22:59,,,,,,,,2.5.0,,,streams,,,,,,6,kip-535,,,,"Currently when expanding the KS cluster, the new node's partitions will be unavailable during the rebalance, which for large states can take a very long time, or for small state stores even more than a few ms can be a deal-breaker for micro service use cases.

One workaround is to allow stale data to be read from the state stores when use case allows. Adding the use case from KAFKA-8994 as it is more descriptive.

""Consider the following scenario in a three node Streams cluster with node A, node S and node R, executing a stateful sub-topology/topic group with 1 partition and `_num.standby.replicas=1_`  
 * *t0*: A is the active instance owning the partition, B is the standby that keeps replicating the A's state into its local disk, R just routes streams IQs to active instance using StreamsMetadata
 * *t1*: IQs pick node R as router, R forwards query to A, A responds back to R which reverse forwards back the results.
 * *t2:* Active A instance is killed and rebalance begins. IQs start failing to A
 * *t3*: Rebalance assignment happens and standby B is now promoted as active instance. IQs continue to fail
 * *t4*: B fully catches up to changelog tail and rewinds offsets to A's last commit position, IQs continue to fail
 * *t5*: IQs to R, get routed to B, which is now ready to serve results. IQs start succeeding again

 

Depending on Kafka consumer group session/heartbeat timeouts, step t2,t3 can take few seconds (~10 seconds based on defaults values). Depending on how laggy the standby B was prior to A being killed, t4 can take few seconds-minutes. 

While this behavior favors consistency over availability at all times, the long unavailability window might be undesirable for certain classes of applications (e.g simple caches or dashboards). 

This issue aims to also expose information about standby B to R, during each rebalance such that the queries can be routed by an application to a standby to serve stale reads, choosing availability over consistency.""",,ableegoldman,aschuraev,astubbs,githubbot,guozhang,marcosmaia,mjsax,NaviBrar,nizhikov,pszymczyk,scosenza,shafqat,vinoth,vladif,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6031,,,,,,,,,,KAFKA-8994,KAFKA-6555,,,,,,"09/Oct/19 15:03;NaviBrar;image-2019-10-09-20-33-37-423.png;https://issues.apache.org/jira/secure/attachment/12982597/image-2019-10-09-20-33-37-423.png","09/Oct/19 15:17;NaviBrar;image-2019-10-09-20-47-38-096.png;https://issues.apache.org/jira/secure/attachment/12982601/image-2019-10-09-20-47-38-096.png",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2017-10-30 17:40:14.197,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 18 18:30:19 UTC 2020,,,,,,,"0|i3lvlj:",9223372036854775807,,,,,,,,,,,,,,,,"30/Oct/17 17:40;yuzhihong@gmail.com;Please prepare a KIP.","30/Oct/17 23:52;mjsax;[~astubbs] Is this a duplicate of KAFKA-6031 ?","01/Nov/17 17:17;astubbs;Yes @mjsax I believe it is, sorry I had a look but didn't find it before. Good to have both the wordings though :)","04/Dec/18 18:37;NaviBrar;Is someone working on a patch for this? If I understand correctly we need to add replica topic partitions in the StreamsMetadata which will be used during get requests in getting host for key(which currently is just looking at active partitions). I have made these changes in my local fork if you want me to send a PR for this, I can do it.","05/Dec/18 16:49;nizhikov;[~NaviBrar] I think you should discuss your patch on the dev-list.

Seems, this ticket requires KIP.
So, prior to the patch review, you should make and discuss KIP.","10/Dec/18 05:40;guozhang;[~NaviBrar] as labeled above I think since it is going to change some public APIs (e.g. the `StreamsMetadata` which is returned from KafkaStreams#metadataForXXX / allMetadata is a public class), I'd suggest you start writing a KIP page to summarize all your thoughts, and also by writing the concrete proposal down it can also help you think about any edge cases like upgrade path, implementation details etc. But it seems you do not have an account for the wiki space yet (https://cwiki.apache.org/confluence/display/KAFKA/Index, it is different from your account in JIRA), please ping me with your account and I can then add you to the permission list so that you can create a KIP: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals

[~NIzhikov] I saw you are assigning this ticket to yourself, are you also working on this ticket in parallel as well? ","10/Dec/18 06:31;NaviBrar;I have made an account by id: navinder_brar@yahoo.com. You can add me and I can start working on KIP. Thanks in advance.","10/Dec/18 06:44;guozhang;Added you as account 

Navinder Pal Singh Brar (navinder_brar)","10/Dec/18 06:50;nizhikov;Hello, [~guozhang].

Right now I working on KAFKA-6970.
After it I planning to pickup this issue.

[~NaviBrar] if you can start working on the issue today, please, go ahead.","10/Dec/18 19:47;NaviBrar;Sure [~NIzhikov] , I will start writing a KIP.","08/Oct/19 21:51;vinoth;[~NaviBrar] curious to know if you got around to the KIP. I am looking into the HA story for interactive queries atm ","09/Oct/19 04:27;NaviBrar;[~vinoth] sorry completely forgot about this one. I had started writing it up and lost track. I will complete this weekend, if it's not done by then you can take it up.","09/Oct/19 14:41;vinoth;[~NaviBrar] sg. Would you have cycles to drive the implementation as well over the next few weeks? I was planning on KAFKA-8994 this week and next, which also involves the assignment metadata change to expose standby information. Wondering if there is room for parallelization.","09/Oct/19 15:05;NaviBrar;Sure [~vinoth] I can do that. In my current project I have already enabled read from replicas, so we can discuss that as well if you want to. For that, I had to make this change in metadata.            !image-2019-10-09-20-47-38-096.png|width=146,height=237!","10/Oct/19 02:31;vinoth;yes. ideally the KIP should cover that as well. I am optimizing for querying streams app from a remote service (much like the Streams Music App). ","13/Oct/19 13:52;NaviBrar;[~vinoth] started a KIP today: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-535%3A+Allow+state+stores+to+serve+stale+reads+during+rebalance]. ","13/Oct/19 18:47;mjsax;[~NaviBrar] Did you read the instruction about the KIP process: [https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals]

Can you move the KIP page to the correct parent page, update the KIP main page (ie, add to the table ""under discussion), follow the KIP template, and start a discussion on the dev mailing list.

Thanks a lot!","13/Oct/19 18:59;NaviBrar;Thanks, [~mjsax]. Did the needful. ","14/Oct/19 20:06;vinoth;[~NaviBrar] Thnx again,. responded on the mailing list. Lets move the conversation there","18/Oct/19 04:50;NaviBrar;Can I assign this to myself as KIP is already created. ","18/Oct/19 16:39;vinoth;yes please. Also consider merging the scope KAFKA-8994 and renaming title to something like ""Allow serving interactive queries from in-sync Standbys"" which captures the essence of the proposal better? ","24/Dec/19 02:23;githubbot;vinothchandar commented on pull request #7868: [WIP] KAFKA-6144: Allow state stores to serve stale reads during rebalance
URL: https://github.com/apache/kafka/pull/7868
 
 
   KIP-535 Implementation
   
   *Summary of testing strategy*
   - [ ] Unit tests for standby metadata
   - [ ] Integration test for querying standbys, during rebalance
   - [ ] Local testing
   - [ ] Unit tests around APIs used for allLocalOffsetLags() API
   - [ ] Integration test for lag APIs
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","14/Jan/20 17:54;vinoth;[~vvcephei] [~NaviBrar]  Added subtasks here.. ","14/Jan/20 18:29;githubbot;vvcephei commented on pull request #7960: [KAFKA-6144]: Add KeyQueryMetadata APIs to KafkaStreams
URL: https://github.com/apache/kafka/pull/7960
 
 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","14/Jan/20 23:18;githubbot;vvcephei commented on pull request #7962: KAFKA-6144: option to query restoring and standby
URL: https://github.com/apache/kafka/pull/7962
 
 
   This is based on a temporary branch, which is mirrored from https://github.com/apache/kafka/pull/7960.
   
   I will delete the temporary branch once #7960 is merged and re-target this PR to trunk.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","15/Jan/20 19:49;githubbot;vvcephei commented on pull request #7962: KAFKA-6144: option to query restoring and standby
URL: https://github.com/apache/kafka/pull/7962
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","18/Jan/20 18:30;githubbot;vinothchandar commented on pull request #7868: KAFKA-6144: Allow state stores to serve stale reads during rebalance
URL: https://github.com/apache/kafka/pull/7868
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Streams: Add Cogroup in the DSL,KAFKA-6049,13108465,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,wcarlson,guozhang,guozhang,11/Oct/17 00:56,28/Jan/20 21:40,12/Jan/21 10:06,08/Jan/20 23:29,,,,,,,,2.5.0,,,streams,,,,,,1,api,kip,user-experience,,"When multiple streams aggregate together to form a single larger object (e.g. a shopping website may have a cart stream, a wish list stream, and a purchases stream. Together they make up a Customer), it is very difficult to accommodate this in the Kafka-Streams DSL: it generally requires you to group and aggregate all of the streams to KTables then make multiple outer join calls to end up with a KTable with your desired object. This will create a state store for each stream and a long chain of ValueJoiners that each new record must go through to get to the final object.

Creating a cogroup method where you use a single state store will:

* Reduce the number of gets from state stores. With the multiple joins when a new value comes into any of the streams a chain reaction happens where the join processor keep calling ValueGetters until we have accessed all state stores.

* Slight performance increase. As described above all ValueGetters are called also causing all ValueJoiners to be called forcing a recalculation of the current joined value of all other streams, impacting performance.",,darion,githubbot,guozhang,lari.hotari@sagire.fi,mjm2tr,mjsax,wushujames,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-10-11 01:02:04.259,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 08 23:28:57 UTC 2020,,,,,,,"0|i3l41r:",9223372036854775807,,,,,,,,,,,,,,,,"11/Oct/17 00:57;guozhang;WIP PR ready at https://github.com/apache/kafka/pull/2975

Needs someone to pick it up, address the left comments, rebase on trunk and push a new PR to continue.","11/Oct/17 00:58;guozhang;This is primarily contributed by [~winkelman.kyle]. I'm leaving the assignor as empty for now to get someone interested in helping complete it off.","11/Oct/17 01:02;yuzhihong@gmail.com;bq. ValueGetters keep calling ValueGetters until 

Is there typo above ?","11/Oct/17 01:04;guozhang;Thanks [~tedyu], fixing now.","05/Mar/18 03:39;githubbot;ConcurrencyPractitioner opened a new pull request #4646: [KAFKA-6049] Kafka Streams: Add Cogroup in the DSL
URL: https://github.com/apache/kafka/pull/4646
 
 
   
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","12/May/18 18:23;githubbot;ConcurrencyPractitioner closed pull request #4646: [KAFKA-6049] Kafka Streams: Add Cogroup in the DSL
URL: https://github.com/apache/kafka/pull/4646
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/streams/src/main/java/org/apache/kafka/streams/kstream/CogroupedKStream.java b/streams/src/main/java/org/apache/kafka/streams/kstream/CogroupedKStream.java
new file mode 100644
index 00000000000..a1aaa1194f5
--- /dev/null
+++ b/streams/src/main/java/org/apache/kafka/streams/kstream/CogroupedKStream.java
@@ -0,0 +1,75 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.kstream;
+
+import org.apache.kafka.common.annotation.InterfaceStability;
+import org.apache.kafka.common.serialization.Serde;
+import org.apache.kafka.streams.KeyValue;
+import org.apache.kafka.streams.processor.StateStoreSupplier;
+import org.apache.kafka.streams.state.KeyValueStore;
+import org.apache.kafka.streams.state.SessionStore;
+import org.apache.kafka.streams.state.WindowStore;
+
+/**
+ * {@code CogroupedKStream} is an abstraction of multiple <i>grouped</i> record streams of {@link KeyValue} pairs.
+ * It is an intermediate representation of one or more {@link KStream}s in order to apply one or more aggregation
+ * operations on the original {@link KStream} records.
+ * <p>
+ * It is an intermediate representation after a grouping of {@link KStream}s, before the aggregations are applied to
+ * the new partitions resulting in a {@link KTable}.
+ * <p>
+ * A {@code CogroupedKStream} must be obtained from a {@link KGroupedStream} via 
+ * {@link KGroupedStream#cogroup(Initializer, Aggregator, org.apache.kafka.common.serialization.Serde, String) cogroup(...)}.
+ *
+ * @param <K> Type of keys
+ * @param <RK> Type of key in table, either K or Windowed&ltK&gt
+ * @param <V> Type of aggregate values
+ * @see KGroupedStream
+ */
+@InterfaceStability.Unstable
+public interface CogroupedKStream<K, V> {
+
+    <T> CogroupedKStream<K, V> cogroup(final KGroupedStream<K, T> groupedStream,
+                                       final Aggregator<? super K, ? super T, V> aggregator);
+
+    KTable<K, V> aggregate(final Initializer<V> initializer,
+                           final Serde<V> valueSerde,
+                           final String storeName);
+    
+    KTable<K, V> aggregate(final Initializer<V> initializer,
+                           final StateStoreSupplier<KeyValueStore> storeSupplier);
+
+    KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,
+                                     final Merger<? super K, V> sessionMerger,
+                                     final SessionWindows sessionWindows,
+                                     final Serde<V> valueSerde,
+                                     final String storeName);
+
+    KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,
+                                     final Merger<? super K, V> sessionMerger,
+                                     final SessionWindows sessionWindows,
+                                     final StateStoreSupplier<SessionStore> storeSupplier);
+
+    <W extends Window> KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,
+                                                        final Windows<W> windows,
+                                                        final Serde<V> valueSerde,
+                                                        final String storeName);
+
+    <W extends Window> KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,
+                                                        final Windows<W> windows,
+                                                        final StateStoreSupplier<WindowStore> storeSupplier);
+}
diff --git a/streams/src/main/java/org/apache/kafka/streams/kstream/KGroupedStream.java b/streams/src/main/java/org/apache/kafka/streams/kstream/KGroupedStream.java
index 29de64c1e67..96154e6b322 100644
--- a/streams/src/main/java/org/apache/kafka/streams/kstream/KGroupedStream.java
+++ b/streams/src/main/java/org/apache/kafka/streams/kstream/KGroupedStream.java
@@ -1594,4 +1594,5 @@
      */
     SessionWindowedKStream<K, V> windowedBy(final SessionWindows windows);
 
+    <T> CogroupedKStream<K, T> cogroup(final Aggregator<? super K, ? super V, T> aggregator);
 }
diff --git a/streams/src/main/java/org/apache/kafka/streams/kstream/KStreamBuilder.java b/streams/src/main/java/org/apache/kafka/streams/kstream/KStreamBuilder.java
index d747ce8d049..ffc307326f5 100644
--- a/streams/src/main/java/org/apache/kafka/streams/kstream/KStreamBuilder.java
+++ b/streams/src/main/java/org/apache/kafka/streams/kstream/KStreamBuilder.java
@@ -50,7 +50,15 @@
 @Deprecated
 public class KStreamBuilder extends org.apache.kafka.streams.processor.TopologyBuilder {
 
-    private final InternalStreamsBuilder internalStreamsBuilder = new InternalStreamsBuilder(super.internalTopologyBuilder);
+    private final InternalStreamsBuilder internalStreamsBuilder;
+    
+    public KStreamBuilder() {
+        internalStreamsBuilder = new InternalStreamsBuilder(super.internalTopologyBuilder);
+    }
+
+    public KStreamBuilder(InternalStreamsBuilder internalStreamsBuilder) {
+        this.internalStreamsBuilder = internalStreamsBuilder;
+    }
 
     private Topology.AutoOffsetReset translateAutoOffsetReset(final org.apache.kafka.streams.processor.TopologyBuilder.AutoOffsetReset resetPolicy) {
         if (resetPolicy == null) {
@@ -1266,4 +1274,7 @@ public String newStoreName(final String prefix) {
         return internalStreamsBuilder.newStoreName(prefix);
     }
 
+    public InternalStreamsBuilder internalStreamsBuilder() {
+        return internalStreamsBuilder;
+    }
 }
diff --git a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImpl.java b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImpl.java
new file mode 100644
index 00000000000..7857a43d043
--- /dev/null
+++ b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImpl.java
@@ -0,0 +1,200 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.kstream.internals;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Set;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.kafka.common.internals.Topic;
+import org.apache.kafka.common.serialization.Serde;
+import org.apache.kafka.streams.kstream.Aggregator;
+import org.apache.kafka.streams.kstream.CogroupedKStream;
+import org.apache.kafka.streams.kstream.Initializer;
+import org.apache.kafka.streams.kstream.KGroupedStream;
+import org.apache.kafka.streams.kstream.KTable;
+import org.apache.kafka.streams.kstream.Merger;
+import org.apache.kafka.streams.kstream.SessionWindows;
+import org.apache.kafka.streams.kstream.Window;
+import org.apache.kafka.streams.kstream.Windowed;
+import org.apache.kafka.streams.kstream.Windows;
+import org.apache.kafka.streams.processor.StateStoreSupplier;
+import org.apache.kafka.streams.state.KeyValueStore;
+import org.apache.kafka.streams.state.SessionStore;
+import org.apache.kafka.streams.state.WindowStore;
+
+class CogroupedKStreamImpl<K, V> implements CogroupedKStream<K, V> {
+
+    private final AtomicInteger index = new AtomicInteger(0);
+    private static final String COGROUP_AGGREGATE_NAME = ""KSTREAM-COGROUP-AGGREGATE-"";
+    private static final String COGROUP_NAME = ""KSTREAM-COGROUP-"";
+    private static enum AggregateType {
+        AGGREGATE,
+        SESSION_WINDOW_AGGREGATE,
+        WINDOW_AGGREGATE
+    }
+
+    private final InternalStreamsBuilder builder;
+    private final Serde<K> keySerde;
+    private final Map<KGroupedStream, Aggregator> pairs = new HashMap<>();
+    private final Map<KGroupedStreamImpl, String> repartitionNames = new HashMap<>();
+
+    <T> CogroupedKStreamImpl(final InternalStreamsBuilder builder,
+                         final KGroupedStream<K, T> groupedStream,
+                         final Serde<K> keySerde,
+                         final Aggregator<? super K, ? super T, V> aggregator) {
+        this.builder = builder;
+        this.keySerde = keySerde;
+        cogroup(groupedStream, aggregator);
+    }
+
+    @Override
+    public <T> CogroupedKStream<K, V> cogroup(final KGroupedStream<K, T> groupedStream,
+                                              final Aggregator<? super K, ? super T, V> aggregator) {
+        Objects.requireNonNull(groupedStream, ""groupedStream can't be null"");
+        Objects.requireNonNull(aggregator, ""aggregator can't be null"");
+        pairs.put(groupedStream, aggregator);
+        return this;
+    }
+
+    @Override
+    public KTable<K, V> aggregate(final Initializer<V> initializer,
+                                  final Serde<V> valueSerde,
+                                  final String storeName) {
+        return aggregate(initializer, AbstractStream.keyValueStore(keySerde, valueSerde, storeName));
+    }
+
+    @Override
+    public KTable<K, V> aggregate(final Initializer<V> initializer,
+                                  final StateStoreSupplier<KeyValueStore> storeSupplier) {
+        Objects.requireNonNull(initializer, ""initializer can't be null"");
+        Objects.requireNonNull(storeSupplier, ""storeSupplier can't be null"");
+        return doAggregate(AggregateType.AGGREGATE, initializer, storeSupplier, null, null, null);
+    }
+
+    @SuppressWarnings(""unchecked"")
+    @Override
+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,
+                                            final Merger<? super K, V> sessionMerger,
+                                            final SessionWindows sessionWindows,
+                                            final Serde<V> valueSerde,
+                                            final String storeName) {
+        Objects.requireNonNull(storeName, ""storeName can't be null"");
+        Topic.validate(storeName);
+        return aggregate(initializer, sessionMerger, sessionWindows, AbstractStream.storeFactory(keySerde, valueSerde, storeName).sessionWindowed(sessionWindows.maintainMs()).build());
+    }
+
+    @SuppressWarnings(""unchecked"")
+    @Override
+    public KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,
+                                            final Merger<? super K, V> sessionMerger,
+                                            final SessionWindows sessionWindows,
+                                            final StateStoreSupplier<SessionStore> storeSupplier) {
+        Objects.requireNonNull(initializer, ""initializer can't be null"");
+        Objects.requireNonNull(sessionMerger, ""sessionMerger can't be null"");
+        Objects.requireNonNull(sessionWindows, ""sessionWindows can't be null"");
+        Objects.requireNonNull(storeSupplier, ""storeSupplier can't be null"");
+        return (KTable<Windowed<K>, V>) doAggregate(AggregateType.SESSION_WINDOW_AGGREGATE, initializer, storeSupplier, sessionMerger, sessionWindows, null);
+    }
+
+    @Override
+    public <W extends Window> KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,
+                                                               final Windows<W> windows,
+                                                               final Serde<V> valueSerde,
+                                                               final String storeName) {
+        return aggregate(initializer, windows, AbstractStream.windowedStore(keySerde, valueSerde, windows, storeName));
+    }
+
+    @SuppressWarnings(""unchecked"")
+    @Override
+    public <W extends Window> KTable<Windowed<K>, V> aggregate(final Initializer<V> initializer,
+                                                               final Windows<W> windows,
+                                                               final StateStoreSupplier<WindowStore> storeSupplier) {
+        Objects.requireNonNull(initializer, ""initializer can't be null"");
+        Objects.requireNonNull(windows, ""windows can't be null"");
+        Objects.requireNonNull(storeSupplier, ""storeSupplier can't be null"");
+        return (KTable<Windowed<K>, V>) doAggregate(AggregateType.WINDOW_AGGREGATE, initializer, storeSupplier, null, null, windows);
+    }
+
+    @SuppressWarnings(""unchecked"")
+    private <W extends Window> KTable<K, V> doAggregate(final AggregateType aggregateType,
+                                                        final Initializer<V> initializer,
+                                                        final StateStoreSupplier storeSupplier,
+                                                        final Merger<? super K, V> sessionMerger,
+                                                        final SessionWindows sessionWindows,
+                                                        final Windows<W> windows) {
+        final Set<String> sourceNodes = new HashSet<>();
+        final Collection<KStreamAggProcessorSupplier> processors = new ArrayList<>();
+        final List<String> processorNames = new ArrayList<>();
+        for (final Map.Entry<KGroupedStream, Aggregator> pair : pairs.entrySet()) {
+            final KGroupedStreamImpl groupedStream = (KGroupedStreamImpl) pair.getKey();
+            final String sourceName = repartitionIfRequired(groupedStream);
+            if (sourceName.equals(groupedStream.name)) {
+                sourceNodes.addAll(groupedStream.sourceNodes);
+            } else {
+                sourceNodes.add(sourceName);
+            }
+
+            final KStreamAggProcessorSupplier processor;
+            switch (aggregateType) {
+                case AGGREGATE:
+                    processor = new KStreamAggregate(storeSupplier.name(), initializer, pair.getValue());
+                    break;
+                case SESSION_WINDOW_AGGREGATE:
+                    processor = new KStreamSessionWindowAggregate(sessionWindows, storeSupplier.name(), initializer, pair.getValue(), sessionMerger);
+                    break;
+                case WINDOW_AGGREGATE:
+                    processor = new KStreamWindowAggregate(windows, storeSupplier.name(), initializer, pair.getValue());
+                    break;
+                default:
+                    throw new IllegalStateException(""Unrecognized AggregateType."");
+            }
+            processors.add(processor);
+            
+            final String processorName = newName(COGROUP_AGGREGATE_NAME);
+            final String[] sourceNames = {sourceName};
+            builder.internalTopologyBuilder.addProcessor(processorName, processor, sourceNames);
+        }
+        final String name = newName(COGROUP_NAME);
+        final KStreamCogroup cogroup = new KStreamCogroup(processors);
+        final String[] processorNamesArray = processorNames.toArray(new String[processorNames.size()]);
+        builder.internalTopologyBuilder.addProcessor(name, cogroup, processorNamesArray);
+        builder.internalTopologyBuilder.addStateStore(storeSupplier, processorNamesArray);
+        builder.internalTopologyBuilder.copartitionSources(sourceNodes);
+        return new KTableImpl<K, String, V>(builder, name, cogroup, sourceNodes, storeSupplier.name(), true);
+    }
+
+    @SuppressWarnings(""rawtypes"")
+    private String repartitionIfRequired(KGroupedStreamImpl groupedStream) {
+        if (repartitionNames.containsKey(groupedStream)) {
+            return repartitionNames.get(groupedStream);
+        }
+        final String sourceName = groupedStream.repartitionIfRequired(null);
+        repartitionNames.put(groupedStream, sourceName);
+        return sourceName;
+    }
+    
+    private String newName(String prefix) {
+        return prefix + String.format(""%010d"", index.getAndIncrement());
+    }
+}
diff --git a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/GroupedStreamAggregateBuilder.java b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/GroupedStreamAggregateBuilder.java
index e4429cc5e88..1528b6ab79a 100644
--- a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/GroupedStreamAggregateBuilder.java
+++ b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/GroupedStreamAggregateBuilder.java
@@ -46,6 +46,10 @@ public Long apply(K aggKey, V value, Long aggregate) {
         }
     };
 
+    public InternalStreamsBuilder internalStreamsBuilder() {
+        return builder;
+    }
+
     GroupedStreamAggregateBuilder(final InternalStreamsBuilder builder,
                                   final Serde<K> keySerde,
                                   final Serde<V> valueSerde,
diff --git a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java
index fa47444c3c8..6395b833244 100644
--- a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java
+++ b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java
@@ -21,6 +21,7 @@
 import org.apache.kafka.streams.kstream.KStream;
 import org.apache.kafka.streams.kstream.KTable;
 import org.apache.kafka.streams.processor.ProcessorSupplier;
+import org.apache.kafka.streams.processor.StateStoreSupplier;
 import org.apache.kafka.streams.processor.internals.InternalTopologyBuilder;
 import org.apache.kafka.streams.state.KeyValueStore;
 import org.apache.kafka.streams.state.StoreBuilder;
@@ -167,10 +168,6 @@ public String newStoreName(final String prefix) {
         return prefix + String.format(KTableImpl.STATE_STORE_NAME + ""%010d"", index.getAndIncrement());
     }
 
-    public synchronized void addStateStore(final StoreBuilder builder) {
-        internalTopologyBuilder.addStateStore(builder);
-    }
-
     public synchronized void addGlobalStore(final StoreBuilder<KeyValueStore> storeBuilder,
                                             final String sourceName,
                                             final String topic,
@@ -204,4 +201,8 @@ public synchronized void addGlobalStore(final StoreBuilder<KeyValueStore> storeB
                        processorName,
                        stateUpdateSupplier);
     }
+    
+    public synchronized void addStateStore(StoreBuilder storeBuilder) {
+        internalTopologyBuilder.addStateStore(storeBuilder);
+    }
 }
diff --git a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KGroupedStreamImpl.java b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KGroupedStreamImpl.java
index 45ae7da7e0f..9fae052c5d2 100644
--- a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KGroupedStreamImpl.java
+++ b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KGroupedStreamImpl.java
@@ -20,6 +20,7 @@
 import org.apache.kafka.common.serialization.Serdes;
 import org.apache.kafka.common.utils.Bytes;
 import org.apache.kafka.streams.kstream.Aggregator;
+import org.apache.kafka.streams.kstream.CogroupedKStream;
 import org.apache.kafka.streams.kstream.Initializer;
 import org.apache.kafka.streams.kstream.KGroupedStream;
 import org.apache.kafka.streams.kstream.KTable;
@@ -192,6 +193,13 @@ private void determineIsQueryable(final String queryableStoreName) {
                            materializedInternal);
 
     }
+    
+    @Override
+    public <VR> CogroupedKStream<K, VR> cogroup(final Aggregator<? super K, ? super V, VR> aggregator) {
+        Objects.requireNonNull(aggregator, ""aggregator should not be null"");
+        return new CogroupedKStreamImpl<>(builder,
+                                          this, keySerde, aggregator);
+    }
 
     @SuppressWarnings(""deprecation"")
     @Override
@@ -515,7 +523,7 @@ public V apply(final K aggKey, final V aggOne, final V aggTwo) {
     /**
      * @return the new sourceName if repartitioned. Otherwise the name of this stream
      */
-    private String repartitionIfRequired(final String queryableStoreName) {
+    String repartitionIfRequired(final String queryableStoreName) {
         if (!repartitionRequired) {
             return this.name;
         }
diff --git a/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamCogroup.java b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamCogroup.java
new file mode 100644
index 00000000000..de800749dda
--- /dev/null
+++ b/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamCogroup.java
@@ -0,0 +1,65 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.kstream.internals;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+
+class KStreamCogroup<K, V> implements KStreamAggProcessorSupplier<K, K, Change<V>, V> {
+
+    private final List<KStreamAggProcessorSupplier> parents;
+
+    private boolean sendOldValues = false;
+
+    KStreamCogroup(Collection<KStreamAggProcessorSupplier> parents) {
+        this.parents = new ArrayList<>(parents);
+    }
+
+    @Override
+    public Processor<K, Change<V>> get() {
+        return new KStreamCogroupProcessor();
+    }
+    
+    private final class KStreamCogroupProcessor extends AbstractProcessor<K, Change<V>> {
+        @Override
+        public void process(K key, Change<V> value) {
+            if (sendOldValues) {
+                context().forward(key, value);
+            } else {
+                context().forward(key, new Change<>(value.newValue, null));
+            }
+        }
+    }
+
+    @SuppressWarnings(""unchecked"")
+    @Override
+    public KTableValueGetterSupplier<K, V> view() {
+        return (KTableValueGetterSupplier<K, V>) parents.get(0).view();
+    }
+
+    @Override
+    public void enableSendingOldValues() {
+        for (KStreamAggProcessorSupplier parent : parents) {
+            parent.enableSendingOldValues();
+        }
+        sendOldValues = true;
+    }
+}
diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java
index 739792f1877..bc5985fe305 100644
--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java
+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java
@@ -24,7 +24,7 @@
 import java.nio.ByteBuffer;
 import java.util.List;
 
-class WindowKeySchema implements RocksDBSegmentedBytesStore.KeySchema {
+public class WindowKeySchema implements RocksDBSegmentedBytesStore.KeySchema {
 
     private static final int SUFFIX_SIZE = WindowStoreUtils.TIMESTAMP_SIZE + WindowStoreUtils.SEQNUM_SIZE;
     private static final byte[] MIN_SUFFIX = new byte[SUFFIX_SIZE];
diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/KStreamCogroupIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamCogroupIntegrationTest.java
new file mode 100644
index 00000000000..0eb15a5d740
--- /dev/null
+++ b/streams/src/test/java/org/apache/kafka/streams/integration/KStreamCogroupIntegrationTest.java
@@ -0,0 +1,356 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.integration;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Properties;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.kafka.clients.consumer.ConsumerConfig;
+import org.apache.kafka.clients.producer.ProducerConfig;
+import org.apache.kafka.common.serialization.LongDeserializer;
+import org.apache.kafka.common.serialization.LongSerializer;
+import org.apache.kafka.common.serialization.Serdes;
+import org.apache.kafka.common.serialization.StringDeserializer;
+import org.apache.kafka.common.serialization.StringSerializer;
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.KeyValue;
+import org.apache.kafka.streams.StreamsBuilder;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;
+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;
+import org.apache.kafka.streams.kstream.Aggregator;
+import org.apache.kafka.streams.kstream.Initializer;
+import org.apache.kafka.streams.kstream.KGroupedStream;
+import org.apache.kafka.streams.kstream.KTable;
+import org.apache.kafka.streams.kstream.KeyValueMapper;
+import org.apache.kafka.streams.kstream.Merger;
+import org.apache.kafka.streams.kstream.ValueJoiner;
+import org.apache.kafka.test.IntegrationTest;
+import org.apache.kafka.test.TestUtils;
+import org.junit.BeforeClass;
+import org.junit.ClassRule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+
+@Category({IntegrationTest.class})
+public class KStreamCogroupIntegrationTest {
+    @ClassRule
+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);
+    private static final String APP_ID = ""cogroup-integration-test-"";
+    private static final String COGROUP_STORE_NAME = ""cogroup"";
+    private static final String TABLE_STORE_NAME = ""table"";
+    private static final String INPUT_TOPIC_1 = ""input-topic-1-"";
+    private static final String INPUT_TOPIC_2 = ""input-topic-2-"";
+    private static final String INPUT_TOPIC_3 = ""input-topic-3-"";
+    private static final String OUTPUT_TOPIC = ""output-topic-"";
+    private static final AtomicInteger TEST_NUMBER = new AtomicInteger();
+    private static final Initializer<String> INITIALIZER = new Initializer<String>() {
+            @Override
+            public String apply() {
+                return """";
+            }
+        };
+    private static final Aggregator<Long, String, String> AGGREGATOR_1 = new Aggregator<Long, String, String>() {
+            @Override
+            public String apply(Long key, String value, String aggregate) {
+                return aggregate + ""1"" + value;
+            }
+        };
+    private static final Aggregator<Long, String, String> AGGREGATOR_2 = new Aggregator<Long, String, String>() {
+            @Override
+            public String apply(Long key, String value, String aggregate) {
+                return aggregate + ""2"" + value;
+            }
+        };
+    private static final Aggregator<Long, String, String> AGGREGATOR_3 = new Aggregator<Long, String, String>() {
+            @Override
+            public String apply(Long key, String value, String aggregate) {
+                return aggregate + ""3"" + value;
+            }
+        };
+    private static final KeyValueMapper<Long, String, Long> GROUP_BY = new KeyValueMapper<Long, String, Long>() {
+            @Override
+            public Long apply(Long key, String value) {
+                return key * 2;
+            }
+        };
+    private static final ValueJoiner<String, String, String> JOINER = new ValueJoiner<String, String, String>() {
+            @Override
+            public String apply(String value1, String value2) {
+                return value1 + ""+"" + value2;
+            }
+        };
+    private static final Merger<Long, String> MERGER = new Merger<Long, String>() {
+            @Override
+            public String apply(Long aggKey, String aggOne, String aggTwo) {
+                return aggOne + aggTwo;
+            }
+        };
+    private static final Properties PRODUCER_CONFIG = new Properties();
+    private static final Properties CONSUMER_CONFIG = new Properties();
+    private static final Properties STREAMS_CONFIG = new Properties();
+    private static final List<Input<Long, String>> INPUTS = Arrays.asList(
+            new Input<>(INPUT_TOPIC_1, 10L, new KeyValue<>(1L, ""a"")),
+            new Input<>(INPUT_TOPIC_2, 10L, new KeyValue<>(2L, ""a"")),
+            new Input<>(INPUT_TOPIC_3, 11L, new KeyValue<>(1L, ""a"")),
+            new Input<>(INPUT_TOPIC_1, 11L, new KeyValue<>(1L, ""b"")),
+            new Input<>(INPUT_TOPIC_2, 12L, new KeyValue<>(2L, ""b"")),
+            new Input<>(INPUT_TOPIC_3, 12L, new KeyValue<>(1L, ""b"")),
+            new Input<>(INPUT_TOPIC_1, 20L, new KeyValue<>(2L, ""c"")),
+            new Input<>(INPUT_TOPIC_2, 20L, new KeyValue<>(1L, ""c"")),
+            new Input<>(INPUT_TOPIC_3, 21L, new KeyValue<>(2L, ""c"")),
+            new Input<>(INPUT_TOPIC_1, 21L, new KeyValue<>(2L, ""a"")),
+            new Input<>(INPUT_TOPIC_2, 22L, new KeyValue<>(1L, ""a"")),
+            new Input<>(INPUT_TOPIC_3, 22L, new KeyValue<>(2L, ""a"")),
+            new Input<>(INPUT_TOPIC_1, 16L, new KeyValue<>(2L, ""b"")),
+            new Input<>(INPUT_TOPIC_2, 16L, new KeyValue<>(1L, ""b"")),
+            new Input<>(INPUT_TOPIC_3, 17L, new KeyValue<>(2L, ""b"")),
+            new Input<>(INPUT_TOPIC_1, 17L, new KeyValue<>(1L, ""c"")),
+            new Input<>(INPUT_TOPIC_2, 18L, new KeyValue<>(2L, ""c"")),
+            new Input<>(INPUT_TOPIC_3, 18L, new KeyValue<>(1L, ""c""))
+        );
+
+    @BeforeClass
+    public static void setupConfigs() throws Exception {
+        PRODUCER_CONFIG.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        PRODUCER_CONFIG.put(ProducerConfig.ACKS_CONFIG, ""all"");
+        PRODUCER_CONFIG.put(ProducerConfig.RETRIES_CONFIG, 0);
+        PRODUCER_CONFIG.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, LongSerializer.class);
+        PRODUCER_CONFIG.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
+
+        CONSUMER_CONFIG.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        CONSUMER_CONFIG.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
+        CONSUMER_CONFIG.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, LongDeserializer.class);
+        CONSUMER_CONFIG.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
+
+        STREAMS_CONFIG.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        STREAMS_CONFIG.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
+        STREAMS_CONFIG.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
+        STREAMS_CONFIG.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.Long().getClass());
+        STREAMS_CONFIG.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
+        STREAMS_CONFIG.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);
+    }
+
+    @Test
+    public void testCogroup() throws InterruptedException, ExecutionException {
+        final int testNumber = TEST_NUMBER.getAndIncrement();
+        final Properties producerConfig = producerConfig();
+        final Properties consumerConfig = consumerConfig(""consumer-"" + testNumber);
+        CLUSTER.createTopic(INPUT_TOPIC_1 + testNumber, 2, 1);
+        CLUSTER.createTopic(INPUT_TOPIC_2 + testNumber, 2, 1);
+        CLUSTER.createTopic(INPUT_TOPIC_3 + testNumber, 2, 1);
+        CLUSTER.createTopic(OUTPUT_TOPIC + testNumber, 2, 1);
+
+        final StreamsBuilder builder = new StreamsBuilder();
+        KGroupedStream<Long, String> stream1 = builder.<Long, String>stream(INPUT_TOPIC_1 + testNumber).groupByKey();
+        KGroupedStream<Long, String> stream2 = builder.<Long, String>stream(INPUT_TOPIC_2 + testNumber).groupByKey();
+        KGroupedStream<Long, String> stream3 = builder.<Long, String>stream(INPUT_TOPIC_3 + testNumber).groupByKey();
+        stream1.cogroup(AGGREGATOR_1)
+                .cogroup(stream2, AGGREGATOR_2)
+                .cogroup(stream3, AGGREGATOR_3)
+                .aggregate(INITIALIZER, null, COGROUP_STORE_NAME)
+                .to(OUTPUT_TOPIC + testNumber);
+
+        final KafkaStreams streams = new KafkaStreams(builder.build(), streamsConfig(APP_ID + testNumber));
+        
+        final List<KeyValue<Long, String>> expecteds = Arrays.asList(
+                KeyValue.pair(1L, ""1a""),
+                KeyValue.pair(2L, ""2a""),
+                KeyValue.pair(1L, ""1a3a""),
+                KeyValue.pair(1L, ""1a3a1b""),
+                KeyValue.pair(2L, ""2a2b""),
+                KeyValue.pair(1L, ""1a3a1b3b""),
+                KeyValue.pair(2L, ""2a2b1c""),
+                KeyValue.pair(1L, ""1a3a1b3b2c""),
+                KeyValue.pair(2L, ""2a2b1c3c""),
+                KeyValue.pair(2L, ""2a2b1c3c1a""),
+                KeyValue.pair(1L, ""1a3a1b3b2c2a""),
+                KeyValue.pair(2L, ""2a2b1c3c1a3a""),
+                KeyValue.pair(2L, ""2a2b1c3c1a3a1b""),
+                KeyValue.pair(1L, ""1a3a1b3b2c2a2b""),
+                KeyValue.pair(2L, ""2a2b1c3c1a3a1b3b""),
+                KeyValue.pair(1L, ""1a3a1b3b2c2a2b1c""),
+                KeyValue.pair(2L, ""2a2b1c3c1a3a1b3b2c""),
+                KeyValue.pair(1L, ""1a3a1b3b2c2a2b1c3c"")
+            );
+
+        try {
+            streams.start();
+
+            final Iterator<KeyValue<Long, String>> expectedsIterator = expecteds.iterator();
+            for (final Input<Long, String> input : INPUTS) {
+                IntegrationTestUtils.produceKeyValuesSynchronously(input.topic + testNumber, Collections.singleton(input.keyValue), producerConfig, CLUSTER.time);
+                List<KeyValue<Long, String>> outputs = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(consumerConfig, OUTPUT_TOPIC + testNumber, 1);
+                assertThat(outputs.get(0), equalTo(expectedsIterator.next()));
+            }
+        } finally {
+            streams.close();
+        }
+    }
+
+    @Test
+    public void testCogroupRepartition() throws InterruptedException, ExecutionException {
+        final int testNumber = TEST_NUMBER.getAndIncrement();
+        final Properties producerConfig = producerConfig();
+        final Properties consumerConfig = consumerConfig(""consumer-"" + testNumber);
+        CLUSTER.createTopic(INPUT_TOPIC_1 + testNumber, 2, 1);
+        CLUSTER.createTopic(INPUT_TOPIC_2 + testNumber, 2, 1);
+        CLUSTER.createTopic(INPUT_TOPIC_3 + testNumber, 2, 1);
+        CLUSTER.createTopic(OUTPUT_TOPIC + testNumber, 2, 1);
+
+        final StreamsBuilder builder = new StreamsBuilder();
+        KGroupedStream<Long, String> stream1 = builder.<Long, String>stream(INPUT_TOPIC_1 + testNumber).groupBy(GROUP_BY);
+        KGroupedStream<Long, String> stream2 = builder.<Long, String>stream(INPUT_TOPIC_2 + testNumber).groupBy(GROUP_BY);
+        KGroupedStream<Long, String> stream3 = builder.<Long, String>stream(INPUT_TOPIC_3 + testNumber).groupBy(GROUP_BY);
+        stream1.cogroup(AGGREGATOR_1)
+                .cogroup(stream2, AGGREGATOR_2)
+                .cogroup(stream3, AGGREGATOR_3)
+                .aggregate(INITIALIZER, null, COGROUP_STORE_NAME)
+                .to(OUTPUT_TOPIC + testNumber);
+
+        final KafkaStreams streams = new KafkaStreams(builder.build(), streamsConfig(APP_ID + testNumber));
+
+        final List<KeyValue<Long, String>> expecteds = Arrays.asList(
+                KeyValue.pair(2L, ""1a""), // Key 1
+                KeyValue.pair(4L, ""2a""), // Key 2
+                KeyValue.pair(2L, ""1a3a""), // Key 1
+                KeyValue.pair(2L, ""1a3a1b""), // Key 1
+                KeyValue.pair(4L, ""2a2b""), // Key 2
+                KeyValue.pair(2L, ""1a3a1b3b""), // Key 1
+                KeyValue.pair(4L, ""2a2b1c""), // Key 2
+                KeyValue.pair(2L, ""1a3a1b3b2c""), // Key 1
+                KeyValue.pair(4L, ""2a2b1c3c""), // Key 2
+                KeyValue.pair(4L, ""2a2b1c3c1a""), // Key 2
+                KeyValue.pair(2L, ""1a3a1b3b2c2a""), // Key 1
+                KeyValue.pair(4L, ""2a2b1c3c1a3a""), // Key 2
+                KeyValue.pair(4L, ""2a2b1c3c1a3a1b""), // Key 2
+                KeyValue.pair(2L, ""1a3a1b3b2c2a2b""), // Key 1
+                KeyValue.pair(4L, ""2a2b1c3c1a3a1b3b""), // Key 2
+                KeyValue.pair(2L, ""1a3a1b3b2c2a2b1c""), // Key 1
+                KeyValue.pair(4L, ""2a2b1c3c1a3a1b3b2c""), // Key 2
+                KeyValue.pair(2L, ""1a3a1b3b2c2a2b1c3c"") // Key 1
+            );
+
+        try {
+            streams.start();
+
+            final Iterator<KeyValue<Long, String>> expectedsIterator = expecteds.iterator();
+            for (final Input<Long, String> input : INPUTS) {
+                IntegrationTestUtils.produceKeyValuesSynchronously(input.topic + testNumber, Collections.singleton(input.keyValue), producerConfig, CLUSTER.time);
+                List<KeyValue<Long, String>> outputs = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(consumerConfig, OUTPUT_TOPIC + testNumber, 1);
+                assertThat(outputs.get(0), equalTo(expectedsIterator.next()));
+            }
+        } finally {
+            streams.close();
+        }
+    }
+
+    @Test
+    public void testCogroupViewAndEnableSendingOldValues() throws InterruptedException, ExecutionException {
+        final int testNumber = TEST_NUMBER.getAndIncrement();
+        final Properties producerConfig = producerConfig();
+        final Properties consumerConfig = consumerConfig(""consumer-"" + testNumber);
+        CLUSTER.createTopic(INPUT_TOPIC_1 + testNumber, 2, 1);
+        CLUSTER.createTopic(INPUT_TOPIC_2 + testNumber, 2, 1);
+        CLUSTER.createTopic(INPUT_TOPIC_3 + testNumber, 2, 1);
+        CLUSTER.createTopic(OUTPUT_TOPIC + testNumber, 2, 1);
+
+        final StreamsBuilder builder = new StreamsBuilder();
+        KGroupedStream<Long, String> stream1 = builder.<Long, String>stream(INPUT_TOPIC_1 + testNumber).groupByKey();
+        KGroupedStream<Long, String> stream2 = builder.<Long, String>stream(INPUT_TOPIC_2 + testNumber).groupByKey();
+        KTable<Long, String> table = builder.table(INPUT_TOPIC_3 + testNumber);
+        stream1.cogroup(AGGREGATOR_1)
+                .cogroup(stream2, AGGREGATOR_2)
+                .aggregate(INITIALIZER, null, COGROUP_STORE_NAME)
+                .outerJoin(table, JOINER)
+                .to(OUTPUT_TOPIC + testNumber);
+
+        final KafkaStreams streams = new KafkaStreams(builder.build(), streamsConfig(APP_ID + testNumber));
+
+        final List<KeyValue<Long, String>> expecteds = Arrays.asList(
+                KeyValue.pair(1L, ""1a+null""),
+                KeyValue.pair(2L, ""2a+null""),
+                KeyValue.pair(1L, ""1a+a""),
+                KeyValue.pair(1L, ""1a1b+a""),
+                KeyValue.pair(2L, ""2a2b+null""),
+                KeyValue.pair(1L, ""1a1b+b""),
+                KeyValue.pair(2L, ""2a2b1c+null""),
+                KeyValue.pair(1L, ""1a1b2c+b""),
+                KeyValue.pair(2L, ""2a2b1c+c""),
+                KeyValue.pair(2L, ""2a2b1c1a+c""),
+                KeyValue.pair(1L, ""1a1b2c2a+b""),
+                KeyValue.pair(2L, ""2a2b1c1a+a""),
+                KeyValue.pair(2L, ""2a2b1c1a1b+a""),
+                KeyValue.pair(1L, ""1a1b2c2a2b+b""),
+                KeyValue.pair(2L, ""2a2b1c1a1b+b""),
+                KeyValue.pair(1L, ""1a1b2c2a2b1c+b""),
+                KeyValue.pair(2L, ""2a2b1c1a1b2c+b""),
+                KeyValue.pair(1L, ""1a1b2c2a2b1c+c"")
+            );
+
+        try {
+            streams.start();
+
+            final Iterator<KeyValue<Long, String>> expectedsIterator = expecteds.iterator();
+            for (final Input<Long, String> input : INPUTS) {
+                IntegrationTestUtils.produceKeyValuesSynchronously(input.topic + testNumber, Collections.singleton(input.keyValue), producerConfig, CLUSTER.time);
+                List<KeyValue<Long, String>> outputs = IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(consumerConfig, OUTPUT_TOPIC + testNumber, 1);
+                assertThat(outputs.get(0), equalTo(expectedsIterator.next()));
+            }
+        } finally {
+            streams.close();
+        }
+    }
+
+    private static final Properties producerConfig() {
+        return PRODUCER_CONFIG;
+    }
+
+    private static final Properties consumerConfig(final String groupId) {
+        final Properties consumerConfig = new Properties();
+        consumerConfig.putAll(CONSUMER_CONFIG);
+        consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
+        return consumerConfig;
+    }
+
+    private static final Properties streamsConfig(final String applicationId) {
+        final Properties streamsConfig = new Properties();
+        streamsConfig.putAll(STREAMS_CONFIG);
+        streamsConfig.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);
+        return streamsConfig;
+    }
+
+    private static final class Input<K, V> {
+        String topic;
+        long timestamp;
+        KeyValue<K, V> keyValue;
+
+        Input(final String topic, final long timestamp, final KeyValue<K, V> keyValue) {
+            this.topic = topic;
+            this.timestamp = timestamp;
+            this.keyValue = keyValue;
+        }
+    }
+}
diff --git a/streams/src/test/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImplTest.java b/streams/src/test/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImplTest.java
new file mode 100644
index 00000000000..d74d73e4a6f
--- /dev/null
+++ b/streams/src/test/java/org/apache/kafka/streams/kstream/internals/CogroupedKStreamImplTest.java
@@ -0,0 +1,144 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.kafka.streams.kstream.internals;
+
+import org.apache.kafka.common.errors.InvalidTopicException;
+import org.apache.kafka.common.serialization.Serdes;
+import org.apache.kafka.streams.kstream.CogroupedKStream;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.kstream.Merger;
+import org.apache.kafka.streams.kstream.SessionWindows;
+import org.apache.kafka.streams.kstream.TimeWindows;
+import org.apache.kafka.test.KStreamTestDriver;
+import org.apache.kafka.test.MockAggregator;
+import org.apache.kafka.test.MockInitializer;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+public class CogroupedKStreamImplTest {
+
+    private static final String TOPIC_1 = ""topic-1"";
+    private static final String TOPIC_2 = ""topic-2"";
+    private static final String INVALID_STORE_NAME = ""~foo bar~"";
+    private final KStreamBuilder builder = new KStreamBuilder();
+    private CogroupedKStream<String, String> cogroupedStream;
+    private KStreamTestDriver driver = null;
+    
+    private final Merger<String, String> sessionMerger = new Merger<String, String>() {
+        @Override
+        public String apply(final String aggKey, final String aggOne, final String aggTwo) {
+            return null;
+        }
+    };
+
+    @Before
+    public void before() {
+        final KStream<String, String> stream = builder.stream(Serdes.String(), Serdes.String(), TOPIC_1);
+        cogroupedStream = stream.groupByKey(Serdes.String(), Serdes.String()).cogroup(MockAggregator.TOSTRING_ADDER);
+    }
+
+    @After
+    public void cleanup() {
+        if (driver != null) {
+            driver.close();
+        }
+        driver = null;
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void shouldNotHaveNullKGroupedStreamOnCogroup() throws Exception {
+        cogroupedStream.cogroup(null, MockAggregator.TOSTRING_ADDER);
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void shouldNotHaveNullAggregatorOnCogroup() throws Exception {
+        cogroupedStream.cogroup(builder.stream(Serdes.String(), Serdes.String(), TOPIC_2).groupByKey(), null);
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void shouldNotHaveNullInitializerOnAggregate() throws Exception {
+        cogroupedStream.aggregate(null, Serdes.String(), ""store"");
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void shouldNotHaveNullStoreNameOnAggregate() throws Exception {
+        cogroupedStream.aggregate(MockInitializer.STRING_INIT, Serdes.String(), null);
+    }
+
+    @Test(expected = InvalidTopicException.class)
+    public void shouldNotHaveInvalidStoreNameOnAggregate() throws Exception {
+        cogroupedStream.aggregate(MockInitializer.STRING_INIT, Serdes.String(), INVALID_STORE_NAME);
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void shouldNotHaveNullInitializerOnWindowedAggregate() throws Exception {
+        cogroupedStream.aggregate(null, TimeWindows.of(10), Serdes.String(), ""store"");
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void shouldNotHaveNullWindowsOnWindowedAggregate() throws Exception {
+        cogroupedStream.aggregate(MockInitializer.STRING_INIT, null, Serdes.String(), ""store"");
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void shouldNotHaveNullStoreNameOnWindowedAggregate() throws Exception {
+        cogroupedStream.aggregate(MockInitializer.STRING_INIT, TimeWindows.of(10), Serdes.String(), null);
+    }
+
+    @Test(expected = InvalidTopicException.class)
+    public void shouldNotHaveInvalidStoreNameOnWindowedAggregate() throws Exception {
+        cogroupedStream.aggregate(MockInitializer.STRING_INIT, TimeWindows.of(10), Serdes.String(), INVALID_STORE_NAME);
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void shouldNotHaveNullStoreSupplierOnWindowedAggregate() throws Exception {
+        cogroupedStream.aggregate(MockInitializer.STRING_INIT, TimeWindows.of(10), null);
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void shouldNotHaveNullInitializerOnSessionWindowedAggregate() throws Exception {
+        cogroupedStream.aggregate(null, sessionMerger, SessionWindows.with(10), Serdes.String(), ""store"");
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void shouldNotHaveNullSessionMergerOnSessionWindowedAggregate() throws Exception {
+        cogroupedStream.aggregate(MockInitializer.STRING_INIT, null, SessionWindows.with(10), Serdes.String(), ""store"");
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void shouldNotHaveNullWindowsOnSessionWindowedAggregate() throws Exception {
+        cogroupedStream.aggregate(MockInitializer.STRING_INIT, sessionMerger, null, Serdes.String(), ""store"");
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void shouldNotHaveNullStoreNameOnSessionWindowedAggregate() throws Exception {
+        cogroupedStream.aggregate(MockInitializer.STRING_INIT, sessionMerger, SessionWindows.with(10), Serdes.String(), null);
+    }
+
+    @Test(expected = InvalidTopicException.class)
+    public void shouldNotHaveInvalidStoreNameOnSessionWindowedAggregate() throws Exception {
+        cogroupedStream.aggregate(MockInitializer.STRING_INIT, sessionMerger, SessionWindows.with(10), Serdes.String(), INVALID_STORE_NAME);
+    }
+
+    @Test(expected = NullPointerException.class)
+    public void shouldNotHaveNullStoreSupplierOnSessionWindowedAggregate() throws Exception {
+        cogroupedStream.aggregate(MockInitializer.STRING_INIT, sessionMerger, SessionWindows.with(10), null);
+    }
+}
diff --git a/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamCogroupTest.java b/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamCogroupTest.java
new file mode 100644
index 00000000000..b65e2977a95
--- /dev/null
+++ b/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamCogroupTest.java
@@ -0,0 +1,117 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.kstream.internals;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.kafka.common.metrics.Metrics;
+import org.apache.kafka.common.serialization.Serdes;
+import org.apache.kafka.common.utils.LogContext;
+import org.apache.kafka.streams.KeyValue;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.internals.MockStreamsMetrics;
+import org.apache.kafka.streams.state.internals.ThreadCache;
+import org.apache.kafka.test.MockProcessorContext;
+import org.apache.kafka.test.NoOpRecordCollector;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+public class KStreamCogroupTest {
+
+    private boolean sendOldValues = false;
+    private final KTableValueGetterSupplier<String, Long> parentValueGetterSupplier = new KTableValueGetterSupplier<String, Long>() {
+            @Override
+            public KTableValueGetter<String, Long> get() {
+                return null;
+            }
+    
+            @Override
+            public String[] storeNames() {
+                return null;
+            }
+        };
+    private final KStreamAggProcessorSupplier parent = new KStreamAggProcessorSupplier<String, String, Change<Long>, Long>() {
+            @Override
+            public Processor<String, Change<Long>> get() {
+                return null;
+            }
+    
+            @Override
+            public KTableValueGetterSupplier<String, Long> view() {
+                return parentValueGetterSupplier;
+            }
+    
+            @Override
+            public void enableSendingOldValues() {
+                sendOldValues = true;
+            }
+        };
+    private final KStreamCogroup<String, Long> cogroup = new KStreamCogroup<String, Long>(Collections.singleton(parent));
+    private final Processor<String, Change<Long>> processor = cogroup.get();
+    private MockProcessorContext context;
+    private List<KeyValue> results = new ArrayList<>();
+
+    @Before
+    public void setup() {
+        context = new MockProcessorContext(null, Serdes.String(), Serdes.Long(), new NoOpRecordCollector(), new ThreadCache(new LogContext(""testCache""), 100000, new MockStreamsMetrics(new Metrics()))) {
+                @Override
+                public <K, V> void forward(final K key, final V value) {
+                    results.add(KeyValue.pair(key, value));
+                }
+            };
+        processor.init(context);
+    }
+
+    @After
+    public void tearDown() {
+        results.clear();
+        sendOldValues = false;
+    }
+
+    @Test
+    public void shouldEnableSendingOldValuesOfParent() {
+        cogroup.enableSendingOldValues();
+        assertTrue(sendOldValues);
+    }
+
+    @Test
+    public void shouldReturnViewOfParent() {
+        final KTableValueGetterSupplier<String, Long> valueGetterSupplier = cogroup.view();
+        assertEquals(parentValueGetterSupplier, valueGetterSupplier);
+    }
+
+    @SuppressWarnings(""unchecked"")
+    @Test
+    public void shouldPassChangeWithOldValueRemoved() {
+        processor.process(""key"", new Change<>(1L, 0L));
+        assertEquals(new KeyValue<>(""key"", new Change<>(1L, null)), (KeyValue<String, Change<Long>>) results.get(0));
+    }
+
+    @SuppressWarnings(""unchecked"")
+    @Test
+    public void shouldPassChangeUnchanged() {
+        cogroup.enableSendingOldValues();
+        processor.process(""key"", new Change<>(1L, 0L));
+        assertEquals(new KeyValue<>(""key"", new Change<>(1L, 0L)), (KeyValue<String, Change<Long>>) results.get(0));
+    }
+}


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","01/Feb/19 22:10;mjsax;Feature freeze deadline for 2.2 was yesterday. Moving this 2.3 release.","01/Dec/19 03:37;githubbot;mjsax commented on pull request #7538: KAFKA-6049: Add non-windowed Cogroup operator (KIP-150)
URL: https://github.com/apache/kafka/pull/7538
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","11/Dec/19 07:51;mjsax;Follow up PRs:
 * time window support: [https://github.com/apache/kafka/pull/7774/]
 * session window support [https://github.com/apache/kafka/pull/7782/]
 * auto-repartitioning: [https://github.com/apache/kafka/pull/7792/]","12/Dec/19 18:52;githubbot;mjsax commented on pull request #7774: KAFKA-6049: Add time window support for cogroup
URL: https://github.com/apache/kafka/pull/7774
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","13/Dec/19 22:07;githubbot;mjsax commented on pull request #7792: KAFKA-6049: Add auto-repartitioning for cogroup
URL: https://github.com/apache/kafka/pull/7792
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","15/Dec/19 19:44;githubbot;mjsax commented on pull request #7782: KAFKA-6049: Add session window support for cogroup
URL: https://github.com/apache/kafka/pull/7782
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","18/Dec/19 02:17;githubbot;mjsax commented on pull request #7847: KAFKA-6049: extend Kafka Streams Scala API for cogroup (KIP-150)
URL: https://github.com/apache/kafka/pull/7847
 
 
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","08/Jan/20 23:28;githubbot;mjsax commented on pull request #7847: KAFKA-6049: extend Kafka Streams Scala API for cogroup (KIP-150)
URL: https://github.com/apache/kafka/pull/7847
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support dynamic gap session window,KAFKA-7325,13180524,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,leyncl,leyncl,22/Aug/18 17:21,27/Jan/20 20:57,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,1,kip,,,,"Currently, Kafka Streams DSL only supports fixed-gap session window. However, in some circumstances, the gap is more dynamic and can vary depending on other factors: the statistical aggregation result, liquidity of the records, etc. In such cases, allowing the user to define a dynamic-gap session is important. [KIP-362|https://cwiki.apache.org/confluence/display/KAFKA/KIP-362%3A+Support+dynamic+gap+session+window] is created to address this.",,leyncl,manas-iu,mjsax,yuxiangqian,zhz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-01-27 06:49:32.849,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 27 20:57:41 UTC 2020,,,,,,,"0|i3xbdz:",9223372036854775807,,,,,,,,,,,,,,,,"27/Jan/20 06:49;manas-iu;Hi, is there any update on this feature?","27/Jan/20 20:57;mjsax;No. There was an initial discussion about the KIP on the mailing list, but nobody is driving the KIP discussion atm. For details, check out the KIP itself and mail archive to catch up on the discussion. Also, feel free to pick up the KIP if you want to drive the discussion and implement this feature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-504: New Java Authorizer API,KAFKA-8865,13254691,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,04/Sep/19 11:51,17/Jan/20 13:23,12/Jan/21 10:06,17/Jan/20 13:23,,,,,,,,2.4.0,,,security,,,,,,0,,,,,Parent task for sub-tasks related to KIP-504,,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-3186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-09-04 11:51:17.0,,,,,,,"0|z06be8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unbalanced assignment of topic-partition to tasks,KAFKA-9352,13277062,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yangguo1220,yangguo1220,yangguo1220,31/Dec/19 22:36,13/Jan/20 18:08,12/Jan/21 10:06,13/Jan/20 18:08,2.4.0,,,,,,,2.5.0,,,mirrormaker,,,,,,0,,,,,"originally, when mirrormaker replicates a group of topics, the assignment between topic-partition and tasks are pretty static. E.g. partitions from the same topic tend to be grouped together as much as possible on the same task. For example, 3 tasks to mirror 3 topics with 8, 2 and 2
partitions respectively. 't1' denotes 'task 1', 't0p5' denotes 'topic 0, partition 5'

The original assignment will look like:

t1 -> [t0p0, t0p1, t0p2, t0p3]
t2 -> [t0p4, t0p5, t0p6, t0p7]
t3 -> [t1p0, t1p2, t2p0, t2p1]

The potential issue of above assignment is: if topic 0 has more traffic than other topics (topic 1, topic 2), t1 and t2 will be loaded more traffic than t3. When the tasks are mapped to the mirrormaker instances (workers) and launched, it will create unbalanced load on the workers. Please see the picture below as an unbalanced example of 2 mirrormaker instances:

!Screen Shot 2019-12-19 at 12.16.02 PM.png!

Given each mirrored topic has different traffic and number of partitions, to balance the load
across all mirrormaker instances (workers), 'roundrobin' helps to evenly assign all
topic-partition to the tasks, then the tasks are further distributed to workers by calling
'ConnectorUtils.groupPartitions()'. For example, 3 tasks to mirror 3 topics with 8, 2 and 2
partitions respectively. 't1' denotes 'task 1', 't0p5' denotes 'topic 0, partition 5'
t1 -> [t0p0, t0p3, t0p6, t1p1]
t2 -> [t0p1, t0p4, t0p7, t2p0]
t3 -> [t0p2, t0p5, t1p0, t2p1]

The improvement of this new above assignment over the original assignment is: the partitions of topic 0, topic 1 and topic 2 are all spread over all tasks, which creates a relatively even load on all workers, after the tasks are mapped to the workers and launched.
Please see the picture below as a balanced example of 4 mirrormaker instances:

!Screen Shot 2019-12-19 at 8.22.17 AM.png!

PR link is: https://github.com/apache/kafka/pull/7880

 ",,githubbot,yangguo1220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Dec/19 22:36;yangguo1220;Screen Shot 2019-12-19 at 12.16.02 PM.png;https://issues.apache.org/jira/secure/attachment/12989759/Screen+Shot+2019-12-19+at+12.16.02+PM.png","31/Dec/19 22:36;yangguo1220;Screen Shot 2019-12-19 at 8.22.17 AM.png;https://issues.apache.org/jira/secure/attachment/12989758/Screen+Shot+2019-12-19+at+8.22.17+AM.png",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2020-01-13 18:06:34.038,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 13 18:06:34 UTC 2020,,,,,,,"0|z0a414:",9223372036854775807,,,,,,,,,,,,,,,,"31/Dec/19 22:38;yangguo1220;pr: https://github.com/apache/kafka/pull/7880","13/Jan/20 18:06;githubbot;mimaison commented on pull request #7880: KAFKA-9352: use 'roundrobin' to assign topic-partition to mirroring tasks
URL: https://github.com/apache/kafka/pull/7880
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support non-key joining in KTable,KAFKA-3705,12968128,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,abellemare,guozhang,guozhang,11/May/16 22:29,09/Jan/20 05:21,12/Jan/21 10:06,03/Oct/19 23:03,,,,,,,,2.4.0,,,streams,,,,,,17,api,kip,,,"KIP-213: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-213+Support+non-key+joining+in+KTable]

Today in Kafka Streams DSL, KTable joins are only based on keys. If users want to join a KTable A by key {{a}} with another KTable B by key {{b}} but with a ""foreign key"" {{a}}, and assuming they are read from two topics which are partitioned on {{a}} and {{b}} respectively, they need to do the following pattern:
{code:java}
tableB' = tableB.groupBy(/* select on field ""a"" */).agg(...); // now tableB' is partitioned on ""a""

tableA.join(tableB', joiner);
{code}
Even if these two tables are read from two topics which are already partitioned on {{a}}, users still need to do the pre-aggregation in order to make the two joining streams to be on the same key. This is a draw-back from programability and we should fix it.",,abellemare,apsaltis,asahib,ashwin153,astubbs,crumley@gmail.com,d3v3l0,githubbot,guozhang,jeqo,jfilipiak,lukejackson,magneh,miguno,mihbor,mjsax,othon2000,samfang,satyacool,srikanth18,thuey100,tymurray,vaughanp@advisory.com,vultron81,Yohan123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-9302,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-21 00:35:08.021,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 15 18:48:02 UTC 2019,,,,,,,"0|i2xp2f:",9223372036854775807,,,,,,,,,,,,,,,,"27/May/16 19:26;guozhang;Please find the discussion and proposal page here: https://cwiki.apache.org/confluence/display/KAFKA/Discussion%3A+Non-key+KTable-KTable+Joins","03/Jun/16 19:24;guozhang;Another thing we may want to consider is to use a different memtable option in RocksDB: https://github.com/facebook/rocksdb/wiki/Hash-based-memtable-implementations","21/Jun/16 00:35;jfilipiak;A few things I came accross building the current implementation based on the processor API.

1. Partitioning
I ended up with the need of passing an additional ValueMapper<K,K1> into the method. I had to use it in the Sinks partitioner to extract the _partition/join-key_ from the key that is used for the repartition topic. It had to be extracted from the key as I still need to be able to pass nullvalues to the correct partition for deletes. This came from not knowing the number of partitions in the processor but only in the partitoner, this made the ""API"" kinda complicated. 

2. Range Select
This ValueMapper mentioned above also had to be passed into the RocksDBIterator. Havin KeyValueIterator<K,V> range(K from, K to) is not ""natural"" for prefix range querries. KeyValueIterator<K,V> range(K1 prefix) where Serde<K1> needs to produce prefixbytes of Serde<K>

3. Key expansion
After a join in this fashion, the key is what I started refering to as widened. Say you have KTable<AK,AV> and it is the table that needs to be repartitioned and KP is the repartition key, then, independently on the other table the new Key of the table must include KP and AK, wich is a wired thing compared to the traditonal relational database way. Imagin having a result table as KTable<Pair<AK,KP>,Pair<AV,XV>> then the used to be unique key AK is not unique anymore, the processor might see the insert in the one partition before the delete in the other (eg when the rows KP was update). I think this should be embrased, because that is how it is. It should just be apparent for the user maybe as it needs to be dealt with in downstream processors.

Unrelated to the topic of joining, the processor api not necessarily comfortable, I appreaceate the beauty of the threading model but stiching graphs together based on processornames and strings is more tricky than I tought. Anyhow really nice stream processing framework. It feels and looks so much better than what is out there spark or storm. Watching their desprate attempts to put state in is a joy. Nice work. As soon as our implementation is hardend in production, Ill probably can share.
","21/Jun/16 01:00;guozhang;Thanks for the feedbacks!

Re 1: Not sure I fully understand this. I thought you can pass a {{StreamPartitioner}} when calling {{addSink}} which should be sufficient?

Re 2: We are aware of this, and as discussed in the wiki our current proposal is that we can use sth. similar to what you mentioned as {{range(K1 prefix)}} and check if {{key.startsWith(prefix)}} to stop iterating. There are some optimizations with prefix seeking in RocksDB but we need to contribute back to RocksDB's JNI to make use of it.

https://cwiki.apache.org/confluence/display/KAFKA/Discussion%3A+Non-key+KTable-KTable+Joins#Discussion:Non-keyKTable-KTableJoins-Simpleapproach:seekwithkeydirectly

Re 3: I think you do not need to keep both the old and new keys for repartitioning if the old values need to be sent as well, but rather send them as two separate records as <null, old> and <new, null> since after the repartitioning, they may be going to two different partitions and hence processed by two different joiners, which is the expected behavior. More precisely, we are going to send the <new, old> pair separately as two record: <PK-new, <AK, AV-new>>, and <PK-new, <AK, AV-old>>, and partition on <PK-new> and <PK-old>. These two records may be sent to two different partitions and hence processed by two different processors.

For example, if you have two KTables {{A}} and {{B}}, with the following schema:

A: {key: a, value: a'}
B: {key: b, value: a, c}

And you want to join them by key {{a}}, now let's say table {{A}} just have two records: 

{a=""a1"", a'=""a1-pre""}, 
{a=""a2"", a'=""a2-pre""}, 

and an incoming record for table {{B}} comes as:

{b=""b"", a=""a1"", c=""c1""}

Then a join result of {a=""a1"", joined = join(""a1-pre"", ""c1"")} should be output.

Later when table {{B}} gets an update on the existing key ""b"":

{b=""b"", a=""a2"", c=""c2""}

Two join results should be output: first negating the previous join result as 

{a=""a1"", joined = null}

Then a new join result on the new re-partitioned key:

{a=""a2"", joined = join(""a2-pre"", ""c2"")}

Does that sound good to you?","28/Jun/16 00:12;jfilipiak;I will just shoot a quick reply now, time somehow became sparse recently. Anyhow. The bottom line of our misunderstandings is always the same thing. My bad that I didn't see the wiki page, if that Range-query interface is addressed that's nice :D.

Point 3 is the one that causes the most confusion I guess. In the repartition case we follow different pathes, where I am not sure that I was able to communicate mine well enough. I <3 the idea of having everything a derived store. ITE all this is beeing used to tail -F mysql-XXXX.bin | kafka | XXX | redis, therefore Redis become a derived store of mysql wich can be used for NoSql style reads. I infact am such a great fan of this concept that I tend to treat everything a derived store. For me this means a repartitioned topic is a derived store of the source topic. This stands in contrast to make a changelog out of it and materialize the changelog in say RocksDb. This leads to the ""problem"" that the changelog topic is not a derived store anymore. Wich gives me a personally bad feeling, it just pushes me out of my comfort zone. Confluent peeps seem to be in their comfort zone with change logging topics. In my narrative shit hits the fan when the property of beeing a derived store is lost. It leads to all the nasty things like beeing in the need of change logging your say RocksDbs as the intermidate topic wont hold stuff forever. 

In contrast to having a change-logging topic that I re-materialize and then changecapture again, I prefer todo the change capturing first and only maintain the state to wich downstream partitions a record is currently published. This works clean and nicely but brings with it what I call ""key widening"". Say I have KTable A and i want to repartition it to A' so that the topic containing A' is a derived store & logcompacted. Then I cant use Key<A> todo this for 2 reasons. The Stream partition, can only access the key to determine the partition to delete from  (deletes come as null values), wich means the fields going to determine the partitions need to be in the key no matter what. Snippet:
{code:java}

		topology.addSink(name, repartitionTopicName, new StreamPartitioner<K, VR>(){
			private Serializer<KL> intermediateSerializer = intermediateSerde.serializer();
			@Override
			public Integer partition(K key, VR value, int numPartitions) {
				KL newKey = intermideateKeyExtractor.apply(key);
				//Copied from Default Partitioner, didn't want to create a CLUSTER object here to reuse it.
				return (Utils.murmur2(intermediateSerializer.serialize(repartitionTopicName, newKey)) % numPartitions )& 0x7fffffff;
			}
			
		}, repartitionProcessorName);
{code}

As you can see the result Key K contains the KL ( the key of the not repatitioned table).

the second reason why this key must be there is that one needs to be able to build a derived stream A''. But since in A' a record can ""move"" from partition X to Y there is a race condition between the ""insert"" in Y and the delete in X. The repartitioner Processor repartitioning for A'' needs to treat them as different keys. If it would be the same key the delete would wipe the new value maybe. This puts downstream consumers of A'' also in the wired position that at any point in time there can be as many A-keys with different values as there are A' partitons -1 or a specific A key might vanish completly and then reappear. Wich is sometimes wanky to work around in the end application. But there is enough strategies to solve at least the multiple Akeys case, not so much for the complete fanish case. I hope this clarrifies stuff. 



","30/Jun/16 19:22;guozhang;Yeah this clarifies a lot. I see that you are not trying to creating a ""table"" inside Kafka Streams, but just want to re-partition the input binlog stream, and you are working on the lower-level APIs  (originally I was confused since you mention ""KTable"" which is only available in the higher-level DSL). And here is my understanding / suggestions:

1. the original Kafka topic that was directly piped from your MySQL bin-log has the form (key -> value)

a -> {a', other-fields}

and you want to repartition it into a new topic on field a', and also log compacted on a'.


2. so instead of representing a delete / update record as ""a -> null"" / ""a -> a'-new, other-fields-new"", you can send the messages in a different format (there are already a few tools including Kafka Connect which allows to represent your binlog entries with such flexibility):

a -> {pair{a'-old, a'-new}, other-fields-current-value }

So that deletion becomes:   a -> {pair{a'-old, null}, other-fields}


3. in this case, you can access the value field to extract a' for partitioning even for deletion cases, and also for an update record, you can then send two records as the following to the re-partition topic:


a'-old -> null,

a'-new -> other-fields.


Will that work for you?

","01/Jul/16 07:27;jfilipiak;Hi, yes that is kinda where I am coming from. I completely understand where you are. 
Doing the change log case ( logging Change<> objects) is just one implementation of this repartitioning and mine is another one. I am very familiar with my approach as I wrote some Samza apps using this approach. It has many benefits that may or may not be of interest. (repartition-topics can also be used to bootstrap, fewer copies of the data (no need to make state HA, see previous) etc.). What we are still missing here is a mutual understanding of what I think keywidening does and how to expose that to users in a non insane manner.

Maybe I try it with your Json syntax. This is the very example we have and where this tickets feature would allow me to build it in the dsl level of the api.

So lets say I have 3 tables. A, B, C, i want to reach a point where I have C => <C,List<Join<A,B>> this will then be read by our application servers and servers them as a faster way to retrieves this than lets say the original mysql. B has foreign keys in A and C.

 
All tables start of as one topic. keyed by this tables primary key
Topic mysq__jadajadajada_A
A.PK => A
Topic mysq_B
B.PK => B
Topic mysq_C
C.PK => C

I am going to repartition B to A.PK now. In the first example without a widened key.
Then it stays B.PK => B but partitioned by A.PK accordingly.

then I can do the join with A and get
B.PK => joined<B,A>

as of your previous comment:
{quote}
Then a join result of
\{a=""a1"", joined = join(""a1-pre"", ""c1"")} 
{quote}
Note the Key stays B.PK (unwindened).
Now I am going to repartition based on C.PK still maintaining
B.PK => joined<B,A>
as the topic layout. 
Now, shit hits the fan. As I am doing my aggregation to become 
C,PK => List<Joined<A,B>>

How would this aggregator looks now?

{code:java}
List<Joined<A,B>> apply(B key, Joined<A,B> value, List<Joined<A,B>> current)
{
   Map m = listToMap(current, bKeyExtractorValueMapper<List<Joined<A,B>,B.PK>);
   if(value == null)
   { 
      m.remove(key)
   }else
   {
     m.put(key,value)
   }
   return m.entrySet.asList

}
{code}

This wouldn't be much different with logged Changes<Joined<A,B>> only the remove and add would be to methods. The problem is, that it doesn't
look wrong. But this code now has race conditions. Think about an update to the A.PK field of a B record that forces it to switch partitions.
(the C.PK value remains) then we publish a delete to the old partition and the new value to the new partition. Then we do the join. then we repartition on the non changed C.PK. This will make out code above see B.PK => null /remove B.PK => Joined<A,B> /add in no particular order. Hence the output is undefined. If we had forcefully by api widened the key to be Joined<A.PK,B.PK> the error would not happen and users would be aware of what happens on repartitioning. I thought this through and it also happens with logging Change<>, as it is really just another implementation.

I hope this finally clarifies that key widening I am talking about. If not, maybe we should have a small skype or something. 
My recommendation is further to not implement this joins as logged Changes<> as it is just more resource intensive and less efficient also making the api more complicated.

PS.: Hive has seen all join types with MapJoins, Skewed Joines, you name it. all these are applicable to streams aswell. Maybe have them in the back of your head.





","01/Jul/16 23:41;guozhang;Yes, this does finally clarify your scenario, thanks!.

I think the change<> pair can still help in your case, because it has the benefit that for aggregations for example, you have the clear information that ""subtract the old value, and add the new value"" instead of depending on whether the returned value is null. For example, the Streams DSL defines the aggregation operator in the following way (note that in your customized implementation you do not need to strictly follow the same pattern, but just to illustrate this idea):

{code}
<T> KTable<K, T> aggregate(Initializer<T> initializer,
                               Aggregator<K, V, T> adder,
                               Aggregator<K, V, T> substractor, ...);
{code}


Let me try again with your example code and the aggregation pattern in the above Streams DSL, and if you do not agree let's have a small skype chat :)


1. Suppose your current value in Table B is

B.PK => B.V.old, which contains A.PK.old, C.PK, etc.

And when you join tables A and B, you repartition the stream B by A.PK while still maintaining the message format as B.BK => B.V, and the join result is in the format of: 

B.PK => join<B.V, A.V>


2. Now suppose you have an update on Table B, as B.PK => B.V.new, which contains A.PK.new, C.PK (same value), etc. And suppose it is represented as a change pair of {old, new}, i.e.

B.PK => {B.V.old, B.V.new}, or more specifically:

B.PK => {<A.PK.old. C.PK, ...>, <A.PK.new, C.PK, ...>}


3. When you repartition it based on A.PK value, this will result in two pairs sending to potentially two different partitions, as:

B.PK => {B.V.old, null}   (sent to partition1)

B.PK => {null, B.V.new}    (sent to partition2)


4. These two records will be joined independently at two processors, each fetching one of the re-partitioned topic partition, and the result is:

B.PK => {joined(B.V.old, A.V.old), null}   (here A.V.old corresponds to the value for key A.PK.old in Table A)

B.PK => {null, joined(B.V.new, A.V.new)}   (here A.V.new corresponds to the value for key A.PK.new in Table A)


and then they will be sent to the second topic that is partitioned on C.PK, and since their C.PK value is the same, they will be sent to the same partition, but in arbitrary order.


5. The aggregation function consumes from the second re-partition topic based on C.PK, and does the aggregation by 1) call a subtract function on the old value of the pair, and then 2) call an add function on the new value of the pair, and if the value is null, skip that call. And more specifically the subtract / add functions look like:

{code}
List<Joined<A,B>> subtractor.apply(C key, Joined<A,B> value, List<Joined<A,B>> current)
{
   current.remove(key)

   return m.entrySet.asList
}

List<Joined<A,B>> adder.apply(C key, Joined<A,B> value, List<Joined<A,B>> current)
{
   current.put(key, value)

   return m.entrySet.asList
}
{code}

And based on the order these two records are received, we will either call {{subtract(C.PK, joined(B.V.old, A.V.old), current)}} first, and then {{add(C.PK, joined(B.V.new, A.V.new), current}}, or vice versa, and either way it is correct, since {{B.V.old}} and {{B.V.new}} are different keys.","07/Jul/16 00:09;guozhang;[~jfilipiak] I am convinced that this combo-key is necessary to avoid out of ordering after talking with you offline. And I have updated the design proposal wiki accordingly: https://cwiki.apache.org/confluence/display/KAFKA/Discussion%3A+Non-key+KTable-KTable+Joins, feel free to take a look.

Just a random thought as for your use case specifically: are relation A, B, and C all need to be captured as a KTable (i.e. the records as binlog / etc from some database table)? If the one with foreign key can be captured just a stream (i.e. KStream), then what you can do is to re-model your computation as {{(stream Join table1) Join table2}}, where {{stream Join table returns a stream}}. And in Kafka Streams DSL you can just do {{stream.selectKey(table1.key).join(table1).selectKey(table2.key).join(table2)}}.","27/Jul/16 11:22;jfilipiak;Something that starts happening to us is that for low cardinality columns on the join, the prefix scan on the rocks can return a big amount of values. That leads to to much time spent between poll() and us loosing group membership. One could check the need for a poll() on the consumer while context.forward() maybe, as we do context.forward() for every row that comes from the prefix scan. The fix with setting session time-out very high, that we are currently using is not that good IMO","27/Jul/16 22:03;guozhang;We are fixing this issue right now on the consumer layer with KIP-62: https://cwiki.apache.org/confluence/display/KAFKA/KIP-62%3A+Allow+consumer+to+send+heartbeats+from+a+background+thread

Expecting to have it in the next minor release.","27/Jul/16 22:06;jfilipiak;The change doesn't seem to be in all the places, I thought about going Ktable<Pair<K1,K2>,T> but also settled with Ktable<K3,T> the other thing would just be generic overkill even though one can hide it from the user by having a PairSerde or something. 

Regarding your idea, I kind of fail to see how an update to table1 or table2 would be reflected in the output with regards to republish, isn't the only option there to have the stream materialized based on a window? Need to take a deeper look though.","27/Jul/16 22:11;guozhang;Yeah currently we do not have a windowed-stream table join, and hence the stream is not materialized but only the table. We can add this join type though in the next release if we feel it is a common request.","17/Jan/17 13:44;miguno;[~jfilipiak]: Now that support for global KTables is around the corner (see KIP-99 at https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=67633649), would that serve some of your needs here?  I am aware that ""non-key joining in KTable"" and ""global KTables"" is not a full overlap, but still the overlap is quite significant.","17/Jan/17 16:21;jfilipiak;Hi, with regard of what I am trying here, the GlobalKTable is not usefull.
 It could be usefull if its source would also emmit change events. (I would still have todo the range lookup but i could save a repartition of my ""bigger"" table) The current design is not usuable here. :'(   ","17/Jan/17 16:28;jfilipiak;AFAICS the table will be also filled by a different thread, IMO that makes it to hard to reason about the execution. If you look into the Table twice while processing a record, it might have different values. :-( GlobalKTable makes me sad ","17/Jan/17 23:13;vaughanp@advisory.com;Healthcare data is intrinsically relational, but the bulk of the data is related to patients in the sense that if the patient did not exist, the data would not exist. For example, these FHIR Resources all depend on a Patient: Encounter, MedicationOrder, Observation, Condition, etc., while Practitioner, Organization, etc. exist independent of patients. This suggests that a good partitioning strategy for processing this data would be to partition by patient in order to get good concurrency with minimal repartitioning/distribution. Data that is independent of the patient would likely need to be distributed to all nodes, but that data is relatively small. 

Processing this data includes checking for referential integrity, dealing with out-of-order data (Encounters that are received before the Patient is received), and re-keying. For example, when an Encounter arrives, a downstream version of that Encounter needs to be created with the patient’s downstream key. Similarly, when a new Patient arrives it should be given a downstream key and any Encounters that reference this patient need to be updated and sent downstream.

But the data is also intrinsically keyed by something other than the patient. For example, an Encounter has a key and it is possible to get duplicate copies of an Encounter, either as corrections or simple duplicates. Thus it is desirable to use the intrinsic key with compacted topics, rather than using the patient as the Kafka topic key. While it is possible to key by one thing and partition by another using explicit partitioners, that seems both error prone and insufficient to keep the data only where it needs to be.

Specifically, the High-level Streaming DSL does not seem to support the latter point. Without the foreign key support discussed here, it is necessary to do aggregation and remapping that cause implicit repartitioning. It seems determined to move the data around. It is not clear to me whether this KIP would eliminate that problem. Note that I found the documentation frustrating in that this repartitioning was not apparent from the documentation – it was most apparent by looking at the set of topics that get implicitly created. 

I would like to see the ability to transform a set of related incoming topics into a set of downstream topics including re-keying and sometimes renormalization using the high-level Streaming DSL. This seems like it is a start towards that, but is it sufficient?
","23/Jan/17 18:16;guozhang;[~jfilipiak] I agree with you the KGlobalTable is not perfect for non-key joins, and that is because of a couple of trade-offs we have to make in the design. Much of it has been discussed in length in KIP-99:

https://cwiki.apache.org/confluence/display/KAFKA/KIP-99%3A+Add+Global+Tables+to+Kafka+Streams

But let me tries to give a very brief summary:

1) Note that different tasks, even if they are hosted by the same stream thread, can be executed at their own pace. For example, task1 could be executing with stream time 100 already, while task2 has not executed even a single record.
2) When adding this globally replicated store, we need to determine whether it is replicated per-instance, per-thread, or per-task. If we want to have time synchronized between them we need to do that per-task or have the store itself to be ""time-series indexed"" so that we can query its different snapshots in time beyond just the most recent status. Both of which will largely increase the storage overhead.
3) That is why we decided to go with the per-instance approach and do not trying to synchronize the update rates between global tables with any of the stream tasks.

If you have some ideas how to improve this scenario, I'd love to hear them. cc [~damianguy]

[~vaughanp@advisory.com] I'm trying to understand your use case a bit better here: there are multiple streams that would be joined by one key ""patient-id"", but these streams' source topics are partitioned by another key ""intrinsic key"" for key-based log compaction. Is that right? If yes, then what you need to do is to repartition the data on the join key before executing the join assuming you are not going to do that as a global KTable - KStream join, and like you mentioned that can be done either explicitly or implicitly:

{code}

encounters = builder.stream(""topic1"");  // keyed and partitioned on intrinsic key
patients = builder.stream(""topic2"");  // keyed and partitioned on intrinsic key

// explicitly

encounters.selectKey(/*patient-id*/).through(""repartition-topic1"").join(patients.groupBy(/*patient-id*/).agg(...))

// implicitly

encounters.selectKey(/*patient-id*/).join(patients.groupBy(/*patient-id*/).agg(...))

{code}

About documentation: thanks for your feedback, we can definitely improve our docs for that: 
","23/Aug/17 00:44;githubbot;GitHub user guozhangwang opened a pull request:

    https://github.com/apache/kafka/pull/3720

    [DO NOT MERGE] KAFKA-3705: non-key joins

    This is just for reviewing the diff easily to see how it is done by @jfillipiak.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/Kaiserchen/kafka KAFKA3705

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3720.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3720
    
----
commit 3da2b8f787a5d30dee2de71cf0f125ab3e57d89b
Author: jfilipiak <jan.filipiak@trivago.com>
Date:   2017-06-30T09:00:39Z

    onetomany join signature to show on mailing list

commit cc9c6f4a68170fb829adb46a6de40ec0fc75716f
Author: jfilipiak <jan.filipiak@trivago.com>
Date:   2017-07-12T14:49:43Z

    stores

commit 807e90aac82d7659310ce92066ac1df6e339068a
Author: jfilipiak <jan.filipiak@trivago.com>
Date:   2017-07-26T06:06:58Z

    just throw in most of the processors, wont build

commit 1a6ff7b01ad35dd7eedf4c69aa534043ab1a8eb8
Author: jfilipiak <jan.filipiak@trivago.com>
Date:   2017-08-18T10:07:34Z

    random clean up

commit ffe9b9496afbdad73bfcb9c014b6045b8ca95e79
Author: jfilipiak <jan.filipiak@trivago.com>
Date:   2017-08-19T19:22:02Z

    clean up as much as possible

----
","24/Oct/17 18:41;vultron81;Does this ticket have a proposed release version?  This feature would make everyone's life so much better that works with [debezium|http://debezium.io] topics from a relational db.","24/Oct/17 21:19;mjsax;It does not -- it's hard to do correctly. You can see that there a prototype PRs from [~jfilipiak] -- maybe he can comment on the current status?","25/Oct/17 08:29;jfilipiak;Just my few sense. In 0.10.0.1 (what we use)  it was easier as every range scan on a rocks DB would just flush it and use rocksDB to handle all the range scanning. With more recent releases The in Memory cache is merged with the rocks DB iterator on the fly. This was a little tricky for me to implement in the first shot. It should be the last missing piece to have the functionality going + probably some processornames beeing off. 

I guess the next thing would open a KIP as I feel that there is still some controversy in the implementation. See my comments in the PR. We should clear them up first I think.


","10/Nov/17 15:14;thuey100;Similar to [~vultron81] mentioned, we're also using debezium, and being unable to do non-key joins is a huge pain point for us. [~jfilipiak], is this the PR you're referring to? https://github.com/apache/kafka/pull/3720. Looking through the comments, it appears there are a bunch of discussion items. I'm not familiar with the Kafka dev process, but is there anything I can do to help move this along?","10/Nov/17 18:36;jfilipiak;[~thuey100] I am glad to see your interest in this. The pull request has discussions regarding the client API. We are currenlty in the process setting this fixed in the KIP https://cwiki.apache.org/confluence/display/KAFKA/KIP-213+Support+non-key+joining+in+KTable. The hardest part in the PR is still outstanding sadly. Merging Cache and Persistent Stores in a prefix scan. We only run our code with 0.10.0.1 it was easier back then. Or we just gonna flush the cache just every time. Feel welcome to get involved!","15/Nov/17 20:34;thuey100;[~jfilipiak] Please bear with me while I try to get caught up. I'm not yet familiar with the Kafka code base. I have a few questions to try to figure out how I can get involved:
1. It seems like we need to get buy-in on your KIP-213? It doesn't seem like there's been much activity on it besides yourself in a while. What's your current plan of attack for getting that approved?
2. I know you said that the most difficult part is yet to be done. Is there some code you can point me toward so I can start digging in and better understand why this is so difficult?
3. This issue has been open since May '16. How far out do you think we are from getting this implemented?","28/May/18 00:55;abellemare;I too am interested, as my org, like many others, have been liberating data out of relational databases into streams, and finding the lack of foreign key joins painful. To me, this is an impediment for adopting Kafka. Many companies may try this out, note that it's not terrible easy to use, and then settle into a pattern of simply using Kafka as a means to transfer data between relational databases. Organizationally, this sort of Jira has huge impacts on how teams can organize and what can be done. Without it, it means that relational data liberated out into the event driven world will continue to provide the same relational pains to all who use it.

There are some expensive ways to mitigate this effect, but it would simply just be much better to have data transformation patterns that match the data patterns produced by tools like Kafka Connect and Debezium.

The last follow up I could find to this was in the mailing list, Feb 16, 2018:

[http://mail-archives.apache.org/mod_mbox/kafka-dev/201802.mbox/%3c5A86D5E7.7000404@trivago.com%3e]

Jan lists four points that they need help with, but it doesn't seem that any of the regular contributors following the PR replied. I suspect that it got lost in the mix. It seems that a lot of work was also done, and that it is in the right direction. I hope that someone familiar with the issue can help kick start it again, as for now all I can really do myself is affirm that this would be an extremely beneficial feature. Thanks for all the work that everyone has done so far, I know that it can often go underappreciated.","17/Aug/18 18:27;githubbot;bellemare opened a new pull request #5527: [DO NOT MERGE] - KAFKA-3705 Added a foreignKeyJoin implementation for KTable.
URL: https://github.com/apache/kafka/pull/5527
 
 
   https://issues.apache.org/jira/browse/KAFKA-3705
   
   Foreign Key Join:
   ================================
   Allows for a KTable to map its value to a given foreign key and join on another KTable keyed on that foreign key. Applies the joiner, then returns the tuples keyed on the original key. This supports updates from both sides of the join.
   
   Design Philosophy:
   ================================
   The intent of this design was to build a totally encapsulated function that operates very similarly to the regular join function. No further work is required by the user to obtain their foreignKeyJoin results after calling the function. That being said, there is increased cost in some of the topology components, especially due to resolving out-of-order arrival due to foreign key changes. I would appreciate any and all feedback on this approach, as my understanding of the Kafka Streams DSL is to provide higher level functionality without requiring the users to know exactly what's going on under the hood. 
   
   Some points of note:
   1) Requires three full materialized State Stores
       One for the prefixScanning of the repartitioned CombinedKey events.
       One for the highwater mark for resolving out-of-order processing of events.
       One for the final materialized sink. This is required because I am sourcing the events back from a repartitioning topic, and so a state store is required to construct a proper Topic Source (see KTableSource).
   
   2) Merging the highwater and final materialized may be possible, but it is unlikely to be useful if we wish for users of this API to be able to specify their own Materialized state store.
   
   3) Caching is disabled on the prefixScan store, same reasons as Jan gave above.
   
   4) ReadOnlyKeyValueStore interface was modified to contain prefixScan. This requires that all implementations support this, but follows an existing precedent where some store functions are already stubbed out with exceptions.
   
   5) Currently limited to Inner Join (can do more join logic in future - just limiting the focus of this KIP).
   
   6) Uses RecordHeaders to address out-of-order processing. The added offset and propagate headers used by the foreignKeyJoin do not persist outside of the function, but they may collide with user-specified header keys.
   
   7) CombinedKeyByForeignKeyPartitioner -> uses a copied + pasted implementation of the DefaultPartitioner. Evaluate breaking this out of the DefaultPartitioner Producer into an accessible function.
   
   8) The large number of variables passed to the KTableKTableForeignKeyJoinNode. Current decision is to leave it as its own node because it doesn't quite fit the patterns of existing nodes. In addition, I am not sure if it each DSL operation should have its own Node type or not.
   
   9) The KTableKTableForeignKeyJoinNode signature (25 parameters! too many, should be < 13)
   
   10) Application Reset does not seem to delete the new internal topics that I have added. (only tested with Kafka 1.0).
   
   
   
   Testing:
   ================================
   Testing is covered by a single integration test that exercises the foreign key join. In addition, it exercises the out-of-order resolution and partitioning strategies by running three streams instances on three partitions. This demonstrates the scalability of the proposed solution.
   
   Note: Because this solution uses the actual offsets of messages to handle out of order resolution, the unit testing framework is not suitable (as far as I can tell) since the offset is not actually provided by the context() access.
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","09/Mar/19 03:55;mjsax;@abellemare Added you to the list on contributors and assigned the ticket to you. You can know also self assign ticket.","12/Apr/19 21:05;abellemare;The PR is now in a draft state - there are a few issues yet to be resolved, but they are detailed within the PR. Please take a look at provide any feedback that you can.

 

https://github.com/apache/kafka/pull/5527","13/Sep/19 17:28;satyacool;How is this change different from ValueJoiner on kstreams","03/Oct/19 22:59;githubbot;bbejeck commented on pull request #5527: [REVIEW NEEDED] - KAFKA-3705 Added a foreignKeyJoin implementation for KTable.
URL: https://github.com/apache/kafka/pull/5527
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","15/Oct/19 18:48;githubbot;guozhangwang commented on pull request #3720: [DO NOT MERGE] KAFKA-3705: non-key joins
URL: https://github.com/apache/kafka/pull/3720
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for TLS 1.3,KAFKA-7251,13177172,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,06/Aug/18 17:34,19/Dec/19 14:20,12/Jan/21 10:06,19/Dec/19 14:20,,,,,,,,2.5.0,,,security,,,,,,0,,,,,"Java 11 adds support for TLS 1.3. We should support this after we add support for Java 11.

Related issues:

[https://bugs.openjdk.java.net/browse/JDK-8206170]

[https://bugs.openjdk.java.net/browse/JDK-8206178]

[https://bugs.openjdk.java.net/browse/JDK-8208538]

[https://bugs.openjdk.java.net/browse/JDK-8207009]

[https://bugs.openjdk.java.net/browse/JDK-8209893]

 

 ",,githubbot,ijuma,mjsax,rsivaram,scholzj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-09-01 16:55:25.923,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 09 16:53:06 UTC 2019,,,,,,,"0|i3wqr3:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,"01/Sep/18 16:55;ijuma;Jetty supports TLS 1.3 with Java 11: https://github.com/eclipse/jetty.project/issues/2711","17/Feb/19 18:48;mjsax;Moving all major/minor/trivial tickets that are not merged yet out of 2.2 release.","09/Dec/19 16:53;githubbot;rajinisivaram commented on pull request #7804: KAFKA-7251; Add support for TLS 1.3
URL: https://github.com/apache/kafka/pull/7804
 
 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement failed message topic to account for processing lag during failure,KAFKA-9285,13272889,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,Yohan123,Yohan123,Yohan123,07/Dec/19 03:36,08/Dec/19 04:05,12/Jan/21 10:06,08/Dec/19 04:05,,,,,,,,,,,consumer,,,,,,0,kip,,,,"Presently, in current Kafka failure schematics, when a consumer crashes, the user is typically responsible for both detecting as well as restarting the failed consumer. Therefore, during this period of time, when the consumer is dead, it would result in a period of inactivity where no records are consumed, hence lag results. Previously, there has been attempts to resolve this problem: when failure is detected by broker, a substitute consumer will be started (the so-called [Rebalance Consumer|[https://cwiki.apache.org/confluence/display/KAFKA/KIP-333%3A+Add+faster+mode+of+rebalancing]]) which will continue processing records in Kafka's stead. 

However, this has complications, as records will only be stored locally, and in case of this consumer failing as well, that data will be lost. Instead, we need to consider how we can still process these records and at the same time effectively _persist_ them. It is here that I propose the concept of a _failed message topic._ At a high level, it works like this. When we find that a consumer has failed, messages which was originally meant to be sent to that consumer would be redirected to this failed messaged topic. The user can choose to assign consumers to this topic, which would consume messages (that would've originally been processed by the failed consumers) from it.

Naturally, records from different topics can not go into the same failed message topic, since we cannot tell which records belong to which consumer.

 ",,Yohan123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Dec 08 04:05:05 UTC 2019,,,,,,,"0|z09ea0:",9223372036854775807,,,,,,,,,,,,,,,,"07/Dec/19 03:37;Yohan123;cc [~bchen225242] You might be interested in this.","08/Dec/19 04:05;Yohan123;Already resolved by Kafka Connect.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add functions to print stream topologies,KAFKA-3858,12979994,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,enothereska,theduderog,theduderog,16/Jun/16 21:06,06/Dec/19 18:31,12/Jan/21 10:06,21/Jul/16 20:12,0.10.1.0,,,,,,,0.10.1.0,,,streams,,,,,,1,,,,,"For debugging and development, it would be very useful to be able to print Kafka streams topologies.  At a minimum, it would be great to be able to see the logical topology including with Kafka topics linked by sub-topologies.  I think that this information does not depend on partitioning.  For more detail, it would be great to be able to print the same logical topology but also showing number of tasks (an perhaps task ids?).  Finally, it would be great to show the physical topology after the tasks have been mapped to JVMs + threads.",,enothereska,githubbot,guozhang,theduderog,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-07-13 14:47:36.417,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 21 20:12:31 UTC 2016,,,,,,,"0|i2zlf3:",9223372036854775807,,,,,,,,,,,,,,,,"13/Jul/16 14:47;githubbot;GitHub user enothereska opened a pull request:

    https://github.com/apache/kafka/pull/1619

    KAFKA-3858: Basic printing of everything

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/enothereska/kafka KAFKA-3858-print-topology

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1619.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1619
    
----
commit 4152616717118ca585d8f0707fb91b9795dd1efd
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-07-13T14:45:58Z

    Basic printing of everything

----
","21/Jul/16 20:12;guozhang;Issue resolved by pull request 1619
[https://github.com/apache/kafka/pull/1619]","21/Jul/16 20:12;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1619
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Retrying leaderEpoch request for partition xxx as the leader reported an error: UNKNOWN_SERVER_ERROR,KAFKA-6522,13135658,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,bookxiao,bookxiao,02/Feb/18 01:21,06/Dec/19 16:57,12/Jan/21 10:06,,1.0.0,,,,,,,,,,core,,,,,,1,,,,,"we have 3 brokers in a kafka cluster(brokerid:401,402,403). The broker-403 fails to fetch data from leader:
{code:java}
[2018-02-02 08:58:26,861] INFO [ReplicaFetcher replicaId=403, leaderId=401, fetcherId=0] Retrying leaderEpoch request for partition sub_payone1hour-0 as the leader reported an error: UNKNOWN_SERVER_ERROR (kafka.server.ReplicaFetcherThread)
[2018-02-02 08:58:26,865] WARN [ReplicaFetcher replicaId=403, leaderId=401, fetcherId=3] Error when sending leader epoch request for Map(sub_myshardSinfo-3 -> -1, sub_myshardUinfo-1 -> -1, sub_videoOnlineResourceType8Test-0 -> -1, pub_videoReportEevent-1 -> 9, sub_StreamNofity-3 -> -1, pub_RsVideoInfo-1 -> -1, pub_lidaTopic3-15 -> -1, pub_lidaTopic3-3 -> -1, sub_zwbtest-1 -> -1, sub_svAdminTagging-5 -> -1, pub_channelinfoupdate-1 -> -1, pub_RsPlayInfo-4 -> -1, sub_tinyVideoWatch-4 -> 14, __consumer_offsets-36 -> -1, pub_ybusAuditorChannel3-2 -> -1, pub_vipPush-4 -> -1, sub_LivingNotifyOnline-3 -> -1, sub_baseonline-4 -> -1, __consumer_offsets-24 -> -1, sub_lidaTopic-3 -> -1, sub_mobileGuessGameReward-0 -> -1, pub_lidaTopic-6 -> -1, sub_NewUserAlgo-0 -> -1, __consumer_offsets-48 -> -1, pub_RsUserBehavior-3 -> -1, sub_channelinfoupdate-0 -> -1, pub_tinyVideoComment-1 -> -1, pub_bulletin-2 -> -1, pub_RecordCompleteNotifition-6 -> -1, sub_lidaTopic2-3 -> -1, smsgateway-10 -> -1, __consumer_offsets-0 -> -1, pub_baseonlinetest-1 -> -1, __consumer_offsets-12 -> -1, pub_myshardUinfo-0 -> -1, pub_baseonline-3 -> -1, smsGatewayMarketDbInfo-6 -> -1, sub_tinyVideoComment-0 -> 14) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 401 was disconnected before the response was read
 at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:95)
 at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:96)
 at kafka.server.ReplicaFetcherThread.fetchEpochsFromLeader(ReplicaFetcherThread.scala:312)
 at kafka.server.AbstractFetcherThread.maybeTruncate(AbstractFetcherThread.scala:130)
 at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:102)
 at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:64){code}
 

on the leader(broker-401) side, the log shows:
{code:java}
[2018-02-02 08:58:26,859] ERROR Closing socket for 192.168.100.101:9099-192.168.100.103:30476 because of error (kafka.network.Processor)
org.apache.kafka.common.errors.InvalidRequestException: Error getting request for apiKey: 23 and apiVersion: 0
Caused by: java.lang.IllegalArgumentException: Unexpected ApiKeys id `23`, it should be between `0` and `20` (inclusive)
 at org.apache.kafka.common.protocol.ApiKeys.forId(ApiKeys.java:73)
 at org.apache.kafka.common.requests.AbstractRequest.getRequest(AbstractRequest.java:39)
 at kafka.network.RequestChannel$Request.liftedTree2$1(RequestChannel.scala:96)
 at kafka.network.RequestChannel$Request.<init>(RequestChannel.scala:91)
 at kafka.network.Processor$$anonfun$processCompletedReceives$1.apply(SocketServer.scala:492)
 at kafka.network.Processor$$anonfun$processCompletedReceives$1.apply(SocketServer.scala:487)
 at scala.collection.Iterator$class.foreach(Iterator.scala:893)
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
 at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
 at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
 at kafka.network.Processor.processCompletedReceives(SocketServer.scala:487)
 at kafka.network.Processor.run(SocketServer.scala:417)
 at java.lang.Thread.run(Thread.java:745){code}",Ubuntu 16.04 LTS 64bit-server,ali.mostafa,bookxiao,huxi_2b,jmarhuen,kirills2006@gmail.com,psby,sravi,virtuus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-02-02 09:33:20.671,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 07 01:16:44 UTC 2018,,,,,,,"0|i3pozz:",9223372036854775807,,,,,,,,,,,,,,,,"02/Feb/18 09:33;huxi_2b;Is it possible that one broker is actually 0.10.2.x?","02/Feb/18 10:32;bookxiao;all brokers use the same binary file (kafka_2.11-1.0.0.tgz). And it recoveries after restarting the broker-403","07/Feb/18 09:51;huxi_2b;[~bookxiao] hmmm.... Based on the stack trace, ApiKeys [Line #73|https://github.com/apache/kafka/blob/f20e4b72d3f5af4539a8c280efcf51b92d6a06af/clients/src/main/java/org/apache/kafka/common/protocol/ApiKeys.java#L73] threw the IllegalArgumentException, that proved it should be from 0.10.2.x codebase. Try to run command below to see if how many apis broker 401 supported:
{code:java}
bin/kafka-broker-api-versions.sh  --bootstrap-server ***{code}
 

 ","06/Nov/18 19:33;jmarhuen;Same happens to me by the end of this tutorial if I follow it without any deviation: [https://data-flair.training/blogs/kafka-cluster/] . It goes away when I restart the nodes. Using in my case \{{kafka_2.11-2.0.0.tgz}} for the three nodes.","06/Nov/18 21:07;jmarhuen;I believe this is happening to me because there were some kafa nodes with the same zookeeper cluster that I wasn't aware of. Which would make sense with what was said two comments before this one.","07/Dec/18 01:15;sravi;[~huxi_2b], We are facing a smiliar problem. we tried the following command 
bin/kafka-broker-api-versions.sh  --bootstrap-server ***
we ended up with the following error: hostname (id: 1 rack: null) -> *ERROR: org.apache.kafka.common.errors.DisconnectException*

We restarted the whole cluster, we even deleted the data directory, this error still prevails making the broker inoperable.

 ","07/Dec/18 01:16;sravi;[~bookxiao], Is this issue resolved for you ? If so, could you share the resolution?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TimeoutException in client side doesn't have stack trace,KAFKA-8520,13238625,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,,zsxwing,zsxwing,10/Jun/19 20:42,08/Nov/19 21:11,12/Jan/21 10:06,08/Nov/19 21:11,,,,,,,,,,,clients,,,,,,0,,,,,"When a TimeoutException is thrown directly in the client side, it doesn't have any stack trace because it inherits ""org.apache.kafka.common.errors.ApiException"". This makes the user hard to debug timeout issues, because it's hard to know which line in the user codes throwing this TimeoutException.

It would be great that adding a new client side TimeoutException which contains the stack trace.",,guozhang,vilo,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7016,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-11-08 08:52:25.918,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 08 21:11:01 UTC 2019,,,,,,,"0|z03lfc:",9223372036854775807,,,,,,,,,,,,,,,,"08/Nov/19 08:52;vilo;Duplicate of https://issues.apache.org/jira/browse/KAFKA-7016","08/Nov/19 21:11;guozhang;Thanks [~vilo] good find.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow KTable bootstrap,KAFKA-4113,13002150,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Invalid,,mjsax,mjsax,01/Sep/16 18:41,22/Oct/19 19:54,12/Jan/21 10:06,22/Oct/19 19:54,,,,,,,,,,,streams,,,,,,9,,,,,"On the mailing list, there are multiple request about the possibility to ""fully populate"" a KTable before actual stream processing start.

Even if it is somewhat difficult to define, when the initial populating phase should end, there are multiple possibilities:

The main idea is, that there is a rarely updated topic that contains the data. Only after this topic got read completely and the KTable is ready, the application should start processing. This would indicate, that on startup, the current partition sizes must be fetched and stored, and after KTable got populated up to those offsets, stream processing can start.

Other discussed ideas are:
1) an initial fixed time period for populating
(it might be hard for a user to estimate the correct value)
2) an ""idle"" period, ie, if no update to a KTable for a certain time is
done, we consider it as populated
3) a timestamp cut off point, ie, all records with an older timestamp
belong to the initial populating phase

The API change is not decided yet, and the API desing is part of this JIRA.

One suggestion (for option (4)) was:
{noformat}
KTable table = builder.table(""topic"", 1000); // populate the table without reading any other topics until see one record with timestamp 1000.
{noformat}",,ableegoldman,astubbs,benstopford,ctoomey,dbreton,donis,edmondo1984,elevy,enothereska,gfodor,ggevay,graphex,guozhang,jkreps,jpalomo,k1th,kiril_p,leyncl,mitch-seymour,mjsax,mrsrinivas,pvazzana,rbrooks982007,scosenza,twbecker,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7458,KAFKA-6542,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-01 18:50:50.182,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 22 19:54:13 UTC 2019,,,,,,,"0|i335fz:",9223372036854775807,,,,,,,,,,,,,,,,"01/Sep/16 18:50;jkreps;Don't you get this naturally out of the message timestamps and the prioritization we already do? People say they want to  ""fully populate"" a table but i think this isn't true. Rather you want the table to be in the same state the associated streams would be in. To see the difference imagine a case where you have a job that is doing a stream-table join and say you lose all your materialized table state and have a job that is down for three hours (for whatever reason--maintenance or something). When it comes back up you don't actually want to catch all the way up on the table because if you do that you will be joining table data from now to stream data from three hours ago. Rather, what you want is to catch up the table to three hours ago and then keep the two roughly aligned so you are joining stream data from time X to the state of the table at time X.

But isn't this exactly what the time stamp prioritization does already? It naturally leads to you catching up on populating the table first if that data is older, right?","01/Sep/16 19:08;mjsax;Two things to add. (1) prioritizing smaller timestamps in processing is ""best effort"" and thus only a weak guarantee. (2) This feature is useful if you have a more ""static table"" that you need to fully populate on *very first Streams application startup* -- this table might get an update every few weeks, but the ""current content"" of the table must be fully available because if I get a first KStream record, I need to enrich it via a join to the KTable and if the KTable record could be missing, the result would be wrong. (Assume you know that there will be a matching KTable record for every KStreams record). Thus, we need to have a way to _guarantee_ that the KTable is first populated completely. Our current ""best effort approach"" would start to process all streams from the beginning (even if it would prioritize KTable and catch up eventually). However, the first KStreams records would be processed incorrectly, as no matching KTable record might be found, and thus the KStream record gets dropped on inner-join (or has missing right-hand side on left-join an is thus corrupted).","01/Sep/16 22:51;wushujames;I wrote a blog post on some other ways to know when you have read everything in a Kafka topic: https://logallthethings.com/2016/06/28/how-to-read-to-the-end-of-a-kafka-topic/

","02/Sep/16 10:21;mjsax;Thanks for pointing out! I read it once. Good post! :)","06/Sep/16 22:27;guozhang;More thoughts regarding this issue after discussing with [~jkreps] offline:

1. Within a single stream, records may not be ordered based on their timestamps. And since Kafka Streams always try to process records based on the source topic's log append ordering, it will result in so-called {{late arrived records}}.

2. When a single Kafka Streams task has multiple input streams, it will try to ""synchronize"" these streams based on their timestamps in a best-effort manner. Therefore we may process a record from one stream with higher timestamp before processing another record from another stream with lower timestamp.

3. For table-stream joins, ideally we want to process records strictly following the timestamp ordering across multiple streams; in that case, I believe users do NOT really need to {{bootstrap a KTable}} since as long as the timestamps of the stream and the table changelog are defined correctly, we are guaranteed to have the correct table snapshot when joining with the stream. For example, say if your table's update traffic is only once-a-week, whose timestamp is defined as the EOW, then when you are (re-)starting the application, it is guaranteed that any stream records whose timestamp is defined before the EOW time will be joined against the ""old"" table snapshot, and any stream records whose timestamp is defined after the EOW time will be joined against the ""updated"" table snapshot, which is the right behavior.

4. However because 1) and 2) above, we are not strictly following the timestamp ordering, the join result is not guaranteed to be ""correct"" or deterministic. In addition, note that the above reasoning assumes that the timestamps on both streams are defined in a consistent manner, for example, if both of these records are generated by the same application who's using the same clock for setting the timestamps; otherwise, for example if the joining streams are not set by the same application of service and there is a time drift between their clocks, then even {{strictly following the timestamp-based ordering}} may still not generate the correct result, and scenarios like [~mjsax] mentioned that a KTable's record may not available when the KStream's record has arrived and is trying to be enriched with the KTable, if its timestamp is indeed defined to be smaller than the corresponding KTable's record timestamp.

5. Therefore, users propose {{bootstrap a KTable}} mainly as a way to give the table's changelog stream's time a bit ""advantage"" over the ordering based on their timestamps so that they are more likely to be processed than the other record stream with the similar timestamps. On the other hand, because of 4) mentioned above I think it is very hard, or even impossible to get absolute ""correct answers"", but just deterministic answers to the best (that is also the motivation of using window retention period in Kafka Streams, or watermarks / hints indicating if there is no late records, along with triggering mechanisms in other frameworks I think).

Following these arguments, here are a list of proposals I'm thinking about to tackle this requirement:

1. Give users the flexibility to define ordering priorities across multiple streams in determining what is the next record to process (i.e. ""synchronize"" them). There are difference ways to expose this API; for example, Samza uses a {{MessageChooser}} user-customizable interface.

2. More restrictive than 1) above, we only allow users to specify an amount of time that one stream should go a little ""in advance"" with other streams such that its records with timestamp {{t + delta}} where {{delta}} is configurable is considered at the same time with other stream's records with time {{t}}. I think most the proposed options in the description of this ticket fall into this category.

3. Different to proposal 1) / 2) above. We change the implementation of KTable-KStream joins to also materialize the KStream based on a sliding window, so that when a record from KStream arrives, it tries to join with the current snapshot of KTable with its backed state store; and when a record from KTable arrives, it tries to join with the KStream's materialized window store with any matching records whose timestamp is smaller than the KTable's update record.

4. This is complementary to 2) / 3) above, that if we do not make the stream synchronization mechanism customizable as proposed in 1), then we can at least consider making it deterministic. So that any join types will generate deterministic results as well.

Thoughts [~mjsax] [~enothereska] [~damianguy]?","19/Oct/16 07:51;gfodor;Having played around with Kafka Streams for a while now, I am still confused (and we still get burned) by this. Let me walk through a case, and see if you guys can find out where I am misunderstanding.

Say we have a topic that's a user table changelog that has user id keys and user records. And we have a clickstream topic that is just a user id to url. For the sake of this example, lets assume our kafka streams job has been running from t = 0 where both topics were empty, so there's no bootstrapping problem.

In the Kafka Streams DSL, I would tap the user table topic via `KStreamBuilder#table`. As best I can tell, this creates a KTable with:
- An unlogged rocksdb state store (which is going to land on disk)
- A new source that is the table topic

After this, I'm going to tap + inner join the click stream as a KStream on user id, and just for this example lets assume I'll sink it all out too to a new topic.

As my node is humming along, it is writing the user id -> user record k/vs to the local rocksdb but is *not* storing the changes to the rocksdb in a topic, because it is not marked as logged. When it reads a record from the KStream, the join is performed by looking for the key in the state store. As mentioned, my understanding is that the join against the stream will wait until the records for the KTable which have earlier timestamps have been consumed. This makes sense.

If I terminate and restart the java process, the kafka consumer for the KTable will pick off at the last committed offset for the user table topic. It may re-consume a few seconds worth of records, and re-write a few keys in the rocks db store, but after that it's still got the full historical state of the topic. So joins against any user id will continue to work.

Where things completely stop making sense for me is if I lose the node. If I lose the node, i lose my rocksdb, which is not logged so is not backed by a changelog topic. When I bring up a new node, my understanding is that the consumer will *not* start at the beginning of the topic used for the KTable, it will just pick up at the last commit. So what I end up with is a rocksdb that only contains the last couple of records from the user table topic. This is obviously really broken, because now my joins will start failling. (And it seems I was lulled into complaency here since I was robust across JVM restarts, but not across node failures.) I believe this problem also happens in a more nefarious way upon rebalances, since if a partition of the KTable gets reassigned, it will also have a partially complete rocksdb store for that partition since it will just consume from the last committed offset. Similarly, and even scarier, if it gets assigned back to the original node, that node now has a rocksdb store with a very small gap, for the key changes that happened during the period where it was assigned to another node.

I am not sure if I am missing something here but this has been the behavior we have seen. The workarounds we have done for this problem are:
- write a routine to let us reset the KTable topics consumer offsets to zero (still doesn't help with a rebalance)
- perform a ""touch"" to the database records we are flushing to kafka, so new copies of all of the records are appended to the topic via kafka connect, and are forced into the rocksdb stores (this works well, but obviously is terrible)
- put a dummy aggregation/reduce after the tap of the KTable topic, which forces things into a logged state store that will be fully materialized on startup if it is missing

Thoughts?","19/Oct/16 08:04;gfodor;I guess what I would argue is that KStreamBuilder#table should have identical semantics to a logged state store backed KTable, except you are specifying the topic and (obv) it's not mutable from the job's POV. It should first check if it has a local, checkpointed rocksdb, and if so, it should just read from the checkpoint forward. If not, it should rematerialize from offset 0 and block the start of the job until it does. On shutdown, it should write the checkpoint file. It seems to me that this might boil down to just having it be ""use this topic for the logged state store backing this KTableImpl.""

I'm sure there are cases I'm missing, but having that be the behavior for KStreamBuilder#table would effectively solve all of our problems as far as I can tell. The semantics + I/O impact of this approach back out to the same exact ones you have when you use a normal user-created persistent state store, but just are managing the topic writes yourself.","20/Oct/16 20:38;guozhang;[~gfodor] The main reason we do not log the source KTable is that the source topic itself is exactly the changelog of this source KTable's materialized state store, and if we log it with another topic, these two topics will have exactly the same messages. Instead, we register the source topic as the changelog of this state store, 

And during the state store restoration process, the store should be brought back to current state by replaying the changelog topic (i.e. the source topic) from offset 0 up to the LEO.  But from your description it seems not the case for you. I checked the source code in trunk and do not have an obvious bug in the {{ProcessorStateManager}} class: i.e. when the KTable is created from a source topic via {{builder.table(...)}} then the source topic should be registered as the changelog and hence used to restore the state upon (re-)initialization. If you are running the current trunk of Kafka then I think there might be some hidden bugs in the code that is not exposed yet.","20/Oct/16 21:07;gfodor;Oh, so it should be doing exactly what makes sense to me -- I am on 0.10.0. Let me verify that there isn't something else going on! Thanks for the info.","01/Nov/16 23:49;gfodor;Hey [~guozhang], I have been able to reproduce a bootstrapping issue on a fresh local node, and I think there might be some stuff I either need clarity on or may even be a bug.

The root cause seems to be here:

https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java#L137

For a completely new node/topology with a KTable topic with existing state, there is no consumer metadata, so this initializes the offset limit to 0, which results in the state restoration loop to basically not consume any records. I've only reproduced this in a local case where I was sinking data to a KTable topic and then initialized the topology for the first time, which is a one-time event, but I'm wondering if this offset limit default of zero could be causing issues later in the lifecycle of the topology as well.","03/Jan/17 15:29;mitch-seymour;I believe I am experiencing the same issue that [~gfodor] describes above (note: I'm using 0.10.0 client libs since we haven't upgraded our brokers yet). If I stream a compacted source topic as a changelog stream using KStreamBuilder#table, with an application id that doesn't have any consumer metadata yet, then the state store does not get initialized as I would expect. This results in failed joins in our application  (as [~mjsax] also noted above). I have been able to get around this issue by creating a dummy consumer before starting the stream, and checking for null meta data using a ConsumerRebalanceListener. If the meta data is null, I do a single poll on the source topic and then exit. I added an example of this method to a public gist, see here:

https://gist.github.com/mitch-seymour/3427ecd0a577cf26b67021e964d6be6c

I am not entirely sure if that approach is safe yet (please feel free to share any feedback or concerns you may have). It seems to be working, but occasionally I get the following error on start up.

Exception in thread ""main"" org.apache.kafka.common.errors.InconsistentGroupProtocolException: The group member's supported protocols are incompatible with those of existing members.

The error goes away after a restart, so I'm not sure what causes it. Anyways, I am happy to help with this issue if needed.","22/Jan/17 23:41;guozhang;Hello [~mitch-seymour], sorry for the late reply. In a recent change we add overloaded functions in {{builder.table/stream}} calls so that users can specify different ""reset policies"" per stream. For example, you can do the following:

{code}

myTable = builder.table(EARLEST, ""topic1"", ..);
myStream = builder.stream(LATEST, ""topic2"", ..);

myStream.join(myTable)...

{code}

If there is no committed offsets (i.e. the application has not executed before), then different streams will use their own reset policies; if it is not specified then the global config value in {{ConsumerConfig}} is used.

Does this solve your use case?","23/Jan/17 04:33;mjsax;[~mitch-seymour] see http://stackoverflow.com/questions/41601771/kafkastreams-inconsistentgroupprotocolexception ","25/Jan/17 13:50;mitch-seymour;[~guozhang] Yes that is perfect. I haven't had a chance to test it yet, but that is the exact behavior that I was looking for. Also, in case anyone else stumbles on this in the future, the relevant ticket with that change is KAFKA-4114, and it looks like it will be going into the 0.10.2.0 release :) Thanks all for the help!","10/Sep/17 18:46;elevy;In response to [~jkreps], in our user case it would be best for the join to be performed against the latest {{KTable}} data.  So yes, we do prefer the {{KTable}} to be fully loaded, even if it's timestamps are far ahead of the {{KStream}}, before the join occurs.","10/Sep/17 22:28;jkreps;But I suppose you must be okay with that not happening too, since in steady state you'll be joining data in the table with data in the stream at about the same timestamp. I think my argument is that if you are okay with this behavior in normal operation you should also be okay with it during bootstrapping (and in many cases joining on future data and producing results that wouldn't occur in steady state operation is not desirable).","11/Sep/17 00:03;elevy;OK in the sense that it is not a fatal failure, but not in the sense that it is the desired behavior.  I concede that it may not be the desirable behavior on all use cases.","13/Jun/18 10:11;benstopford;Whilst I like the 'time-aligned' approach to loading KTables very much, it definitely catches people out. I think this is compounded by the fact that GKTables don't behave like this (they bootstrap themselves on startup rather than being time aligned).

Different use cases actually better suit one or the other (as noted above). So for example, if you're joining Orders to Customers and doing reprocessing you might want the 'as at' version of the customer (say with an old email address) or the latest version of the customer (with their most recent email).

So I think KStreams should support both (a) preloaded or (b) event time ideally in both types of table, letting the user define the behaviour.","14/Jun/18 07:03;donis;Hi Ben, you had a very interesting link in the original comment. Coincidentally we were looking into how to bootstrap KTable/GLobalKTable when you posted this. It's giving 404 now :/
Was there any technical reason for removing it? ","05/Jul/18 23:03;twbecker;Coming from Samza, I find it very surprising that there is no way to do this. If I have 2 topics with existing data, 1 stream and 1 table and write a KafkaStreams application to do a join, it seems very likely that initial records in the result (possible quite many) will not be joined properly, as the corresponding message in the table backing topic has not yet been read.

The timestamp semantics makes sense in that I suppose there are some use-cases where you'd consider the value that was current in the table at the time of some incoming message as ""better"" than the latest value (though I suspect they are a minority). But in reality, the table backing topic is almost certainly log-compacted which means you can't achieve these semantics regardless as these older values are now gone, and worse, the new values have newer timestamps which perpetuate the problem we're talking about.","06/Jul/18 01:27;mjsax;I see you point. I still think, that the timestamp based semantic is superior and I am personally in favor to keep it. However, I agree that a timely decoupled table also has a broad use-case spectrum and we should allow this – however, it should not replace the current KTable, but complement it IMHO. The behavior and semantics would be similar to GlobalKTables.

About, ""table backing topic is almost certainly log-compacted which means you can't achieve these semantics regardless as these older values are now gone"" – if you want to do re-processing I agree. However, the time-synchronization is not just important for re-processing, but provides sound semantics in general. Without it, the computation is inherently non-deterministic (what I believe is not what most people want).

To fix the re-processing case, we would need to ""protect"" the head of the log from compaction: ie, the retention time of the input stream and the non-compacted head of the log must be equally large – there is a config `min.compaction.lag` but I am actually not 100% sure if it can be used for this purpose. Would need to double check. Maybe [~guozhang] knows?

Semantically, it is sound that you cannot do reprocessing if you lost old table state – note that, reprocessing should ensure that you compute the same result (if you don't change the program) than in the original run – if log compaction deletes old data, you can obviously not reprocess it. Using the latest KTable data will result in joining old stream records with ""future"" table data (future in this case is relative future to the stream records of course) and thus produce a different result and would be incorrect, too.","06/Jul/18 16:31;graphex;I think the sad reality of Kafka Streams' behavior right now is even worse than is being portrayed in this ticket. Let's say I have a Kafka cluster with 2 topics, ""events"" and ""users"" and I want to produce a joined ""enriched_events"" topic using Kafka Streams KStream-KTable join. Let's also say that my Kafka cluster currently has 1 billion user records in the users topic, and 100 million event records in the events topic, with about 100k new events coming in per second. We can even further say that the timestamps for every message in the ""users"" topic is before any of the timestamps in the ""events"" topic (and of course both topics are keyed with the user id and partitioned the same).

What I would expect from my new Kafka Streams app is that I would be able to make a KTable out of ""users"" and do a leftJoin with events, and be able to see messages with events+users flow right in to enriched_events, after waiting for the users KTable to populate the RocksDB database on each instance of my app. Unfortunately, what actually happens is that the app quickly processes the 100 million events while slowly populating the RocksDB instance, so enriched_events receives almost no enrichment from corresponding values in my KTable, regardless of any timestamp management.

Only after the app has burned through the 100 million event backlog that existed when it started, and further continued to process 100k events per second for a really long time, will the local RocksDB even be mostly populated and we'll see a reasonable number of successful joins with users flowing into ""enriched_events"".

The only time I've seen behavior remotely similar to what is described here as the best effort is when i restart a Kafka Streams with the same Application ID after it has been running in steady state for a long time. In that case, though it is difficult to actually see what is going on, there appears to be some degree of KTable preloading occurring.

The only workable solution I've been able to find that avoids ""enriched_events"" being filled with a bunch of un-enriched events, is to make a new topic, ""events_controlled"" to use as my Kafka Streams KStream and keep it completely empty, then start up my app, manually watch the lag of my app reading the ""users"" topic until it gets all the way to 0, and then start a separate application to copy messages from ""events"" to ""events_controlled"". This is a pretty high touch solution and is far from ideal in any scenario.

I really don't think the data model of a compacted topic is sufficient to reasonably even attempt to provide ""join-at-the-time"" semantics as described in this ticket. If you need historical joining, use a database with a history for each record. If you're using a compacted Kafka topic as a KTable, you're joining to, your source of truth is, by definition, only intended to contain the latest value for each key in your dataset. This is very much at odds with the ""best effort"" timestamp alignment strategy that acts as your only option for KStream-KTable match semantics, and which doesn't even appear to provide any effort during first run to preload anything into the KTable.","06/Jul/18 17:11;mjsax;[~graphex] What you report is a bug and tracked via KAFKA-3514 – it's on the top list of things we want to fix! I completely agree that KAFKA-3514 breaks the expected behavior that a KTable with older record timestamps should be loaded before processing stream records starts. The difference is, that it's a bug, while the request discussed here is a semantic change request – thus, I think it's best to discuss both separately.","06/Jul/18 18:45;twbecker;[~mjsax] has any thought been given to making the strategy for choosing which topics to process from pluggable? I feel like the current timestamp behavior is one such strategy, but for some other use-cases I feel that a simple topic-level prioritization would be sufficient. For example, in the case where the table backing topic receives way less traffic than the stream topic, I think it could be reasonable to always prefer messages from the table topic over the stream topic. Such a scheme could work for a lot of cases and is quite a bit easier to reason about and implement.","06/Jul/18 18:56;graphex;Thanks for pointing out KAFKA-3514, after being frustrated with both these issues for over a year, I'm surprised I missed it. It would be better for all my use cases to have the option for the preloading that global KTables have, rather than even a working best effort timestamp alignment; sorry if my previous comment didn't make that very clear.

Seems like this feature would be much easier to implement, test, and document than 3514 as well. I've spent weeks artificially backdating my compacted KTable topics only to see no benefit, when what I really want is just a flag to ensure that my KTable is kept as up to date as possible through whatever non-timestamp-related semantics are easiest to implement. Something that is already available for global KTables, but won't scale for my use cases.","06/Jul/18 20:33;mjsax;Thanks for the feedback [~twbecker] and [~graphex]! We prioritize feature based on user feedback. In the beginning there was not much complaint about it.

For GlobalKTable the behavior is different by design, because GlobalKTables are designed for ""static"" data. From my point of view, the design space has two dimensions: partitioned vs broadcasted data, and timestamp-alignment or non-alignment. Currently, we only offer partitions plus aligned (KTable) and broadcasted plus non-aligned (GlobalKTable). Thus, we are missing two more.

Bootstrapping/pre-loading only makes sense for the non-aligned cases IMHO.

We brainstormed about making the strategy plugable at some point – but never pushed it forward so far. I see some more additional use-cases for which this might make sense. It's all about feature prioritization and how much we can get done... Of course, it's an open-source project and contributions are very welcome :)

I personally believe that the timestamp aligned semantic is correct and we should not sacrifice it. As mentioned above, I am happy to complement the design space and offer all 4 KTable variants. The non-timestamp aligned KTable should not be too hard to implement. The broadcast plus timestamp alignment thing is the most difficult one. The plugable strategy might also not be too hard to implement. But all of those would require a KIP to get a sound design.","15/Jan/19 17:58;edmondo1984; I think we might be experiencing this with KTable-KTable , we have some join failures at startup of the app when we reprocess a long topic without having the state backed by local volume (since we are running on ephemeral storage) .

 

Could our problem have the same cause ? I never see KTable-Ktable mentioned here, but only KTable-KStream","16/Jan/19 17:27;mjsax;KTable-KTable join should not be effected because both sides are stateful (in contrast to KStream-KTable join for which only KTable side is stateful). Can you describe in more detail what ""join failures"" you observe, and what you exactly mean by this (missing expected join results?).

Also, what do you mean by ""we reprocess a long topic"" – what does long mean? Is sounds like, you topic is configures with retention but not compaction?","17/Jan/19 08:19;edmondo1984;Sure, most precisely, we are running a Ktable-KTable Kstreams app which performs a join of two compacted topics: they have the same number of partition and the same key, and when the up is ""running"" and we get new items on the topics, everything works fine.

 

Since we are still having our brokers in 0.11, sometimes the app crashes with OutOfOrderException and as it restarts, since we have no local storage, it will consume all the changelog. When this happens, we see some join failures at startup, i.e. data that we know and we checked exist with the correct timestamps on both topics which doesn't trigger an output join. 

 

We performed the following checks
 # The data is in the topics at the right time for both left and right side, with the right timestamp
 # Missed join can be re-triggered by making either the left side or the right side tick again
 # In the end, since one of the two Ktable is a join of kstream-kstream separate app consuming from a topic produced by Kafka connect, we end up updating the timestamp columns in the database to solve the problem
 # Note that at point 1 we have verified that the data is always available in the Ktables, so the join mentioned at the 3 item of this list works correctly and is executed in a separate app. The one failing is the ktable-ktable 

 

The impression is that when OutOfOrderException occurs and the app restarts, one of the two topics is consumed quicker than the other one (one of the two topics is much larger in terms of data size) and therefore the lookup of the Inner Join failst. ","18/Jan/19 19:09;mjsax;I guess what you describe related to timestamp synchronization. This is fixed in 2.1 via https://issues.apache.org/jira/browse/KAFKA-3514 – can you try out 2.1 to see if this fixed the issue? The problem is, that timestamp synchronization is best effort before 2.1 and KTable-KTable joins are ""eventually consistent"" for the case you describe but there is no guarantee that you get all intermediate result on replay. In 2.1, this should work fully consistent on replay.","18/Jan/19 20:14;edmondo1984;Obviously stupid question: 2.1 broker side or kafka-streams jar side?","18/Jan/19 21:02;mjsax;Updating Kafka Streams is sufficient – the fix is client side.","18/Jan/19 21:24;edmondo1984;Just checked, our app is already using 2.1 as a library. ","20/Jan/19 18:14;mjsax;Can it be related to caching? Did you disable caching? If not, try disabling it.","21/Jan/19 09:23;edmondo1984;How do you disable caching?","22/Jan/19 00:18;mjsax;Globally: [https://kafka.apache.org/21/documentation/streams/developer-guide/memory-mgmt.html]
For one operator via `Materialized` parameter: [https://kafka.apache.org/documentation/streams/developer-guide/dsl-api.html#ktable-ktable-join]","25/Sep/19 17:37;rbrooks982007;I have been reading through these comments to try to understand the KTable Bootstrap process. Please correct me if my take away is incorrect, it is as follows:

When the stream starts up KTables are created by taking a snapshot of the source topic based on the largest record time in the joining Stream source topic. The KTable's state store should be fully populated with the snapshot records before the streaming process any joins. Furthermore, KTables processing is prioritized over KStream processing.

Is this the correct understanding of the KTable bootstrap process?","25/Sep/19 17:58;mjsax;Not sure If I understand. Let's do an example (only showing timestamps, consuming from left (small offset) to right).

Table-topic-ts: 10, 12, 14, 16, 17, 19, 20

Stream-Topic:ts: 18, 19, 20, 21, 23

Assuming auto.offset.reset=earliest, we would compare the timestamps for both topic (on a per partition basis, ie, stream-p0 compare to table-p0 etc). In our example, the table records have the smallest timestamps and would be processed first, hence, the table would be ""bootstraped"" from 10 to 17. Afterwards, stream record with timestamp 18 would be processed because the next table update has larger timestamp 19.

If the records from both inputs have the same timestamp, there is currently no guarantee which record is processed first – it's a know gap that we need to address at some point.

Does this answer your question?","25/Sep/19 19:59;rbrooks982007;Okay, so what happens if the stream and broker are set up using the process-time semantics, default.time.extractor = WallClockTime? It doesn't sound like the table is initialized from a snapshot before any join processing is done, or given any prioritization. So I guess this entire thread is about the fact that you can have failed joins given similar scenarios, using default event-time:

premise: each topic has 1 partition, each record consists of a String key and Integer representing time produced.

application-stream( 100,000 records): [key1, 5], [key5, 3], [key2, 2], [key3, 4], [key4, 3]....

employee-stream(10,000 records):       [key1, 1], [key2, 2], [key3, 4], [key4, 3], [key5, 5]...

Time of processing Left Join

application-table:        null   null    maybe    maybe   null

employee-stream:      key1  key2  key3       key4      key5

results from Join:        fail    fail     maybe    maybe   fail

 

Is this the correct understanding?","25/Sep/19 21:10;mjsax;If you use the WallClockTime extractor, timestamps are assigned _after_ they are pulled from Kafka with the wall-clock time from you application (ie, client side) – btw: each time you re-read existing data, a different timestamp will be uses because wall-clock time will always be different. Hence, it's non-deterministic in which order records are processed because there is no contract what `poll()` returns.

So your understanding seems correct.

What you could do it, to write a custom timestamp extractor, and return `0` for each table side record and wall-clock time for each stream side record. In `extract()` to get a `ConsumerRecord` and can inspect the topic name to distinguish between both. Because `0` is smaller than wall-clock time, you can ""bootstrap"" the table to the end of the topic before any stream-side record gets processed.","25/Sep/19 22:27;rbrooks982007;[~mjsax] Thanks for your quick responses. This was very helpful.  For the proposed solution, we would also need to change this config correct - max.task.idle.ms?","25/Sep/19 23:12;mjsax;Yes, increasing max.task.idle.ms help to get stricter guarantees.","22/Oct/19 19:54;mjsax;Closing this ticket as ""invalid"" because KTables are populated based on the record timestamps and hence the idea of a ""blind bootstrap"" does not apply. Instead, an indirect bootstrap can be done by ensuring that the table record timestamps are smaller than the first stream record timestamp will allow to ""bootstrap"" a KTable.

For the case that somebody wants to completely decouple the KTable from the KStream (ie, disable time synchronization, and to get similar table semantics as for a global-KTable) a custom timestamp extractor that always returns zero can be used.

The current timestamp mechanism (cf. KAFKA-3514) is not perfect yet, but the gaps are tracked via KAFKA-6542 and KAFKA-7458 already and we don't need to double track.
 ",,,,,,,,,,,,,,,,,,,,
Streams should expose standby replication information & allow stale reads of state store,KAFKA-8994,13260981,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,vinoth,vinoth,vinoth,07/Oct/19 23:21,18/Oct/19 22:23,12/Jan/21 10:06,18/Oct/19 22:23,,,,,,,,,,,streams,,,,,,0,needs-kip,,,,"Currently Streams interactive queries (IQ) fail during the time period where there is a rebalance in progress. 

Consider the following scenario in a three node Streams cluster with node A, node S and node R, executing a stateful sub-topology/topic group with 1 partition and `_num.standby.replicas=1_`  
 * *t0*: A is the active instance owning the partition, B is the standby that keeps replicating the A's state into its local disk, R just routes streams IQs to active instance using StreamsMetadata
 * *t1*: IQs pick node R as router, R forwards query to A, A responds back to R which reverse forwards back the results.
 * *t2:* Active A instance is killed and rebalance begins. IQs start failing to A
 * *t3*: Rebalance assignment happens and standby B is now promoted as active instance. IQs continue to fail
 * *t4*: B fully catches up to changelog tail and rewinds offsets to A's last commit position, IQs continue to fail
 * *t5*: IQs to R, get routed to B, which is now ready to serve results. IQs start succeeding again

 

Depending on Kafka consumer group session/heartbeat timeouts, step t2,t3 can take few seconds (~10 seconds based on defaults values). Depending on how laggy the standby B was prior to A being killed, t4 can take few seconds-minutes. 

While this behavior favors consistency over availability at all times, the long unavailability window might be undesirable for certain classes of applications (e.g simple caches or dashboards). 

This issue aims to also expose information about standby B to R, during each rebalance such that the queries can be routed by an application to a standby to serve stale reads, choosing availability over consistency. 

 

 

 

 

 

 

 

 

 

 

 

 ",,ableegoldman,mjsax,vinoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-10-18 21:17:35.801,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 18 22:22:42 UTC 2019,,,,,,,"0|z07da0:",9223372036854775807,,,,,,,,,,,,,,,,"18/Oct/19 16:36;vinoth;Closing this, and merging this scope into KAFKA-6144","18/Oct/19 21:17;ableegoldman;[~vinoth] This should be closed as a duplicate (not fixed) right?","18/Oct/19 22:22;vinoth;yes tbf, I actually tried to change this field after I closed it. Could not locate. let me try reopening and reclosing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect Client Config Override policy,KAFKA-8265,13229152,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mageshn,mageshn,mageshn,19/Apr/19 21:33,17/Oct/19 02:27,12/Jan/21 10:06,05/Jun/19 17:51,,,,,,,,2.3.0,,,KafkaConnect,,,,,,0,,,,,"Right now, each source connector and sink connector inherit their client configurations from the worker properties. Within the worker properties, all configurations that have a prefix of ""producer."" or ""consumer."" are applied to all source connectors and sink connectors respectively.

We should allow the  ""producer."" or ""consumer."" to be overridden in accordance to an override policy determined by the administrator.",,githubbot,mageshn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6890,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-05-17 08:37:34.988,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 21 16:28:31 UTC 2019,,,,,,,"0|z01z3k:",9223372036854775807,,,,,,,,,,,,,,,,"17/May/19 08:37;githubbot;ijuma commented on pull request #6624: KAFKA-8265: Initial implementation for ConnectorClientConfigPolicy to enable overrides (KIP-458)
URL: https://github.com/apache/kafka/pull/6624
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","17/May/19 17:12;githubbot;mageshn commented on pull request #6755: KAFKA-8265 : Fix config name to match KIP-458, Return a copy of the ConfigDef in Client Configs.
URL: https://github.com/apache/kafka/pull/6755
 
 
   In the initial implementation for KIP-458 https://github.com/apache/kafka/pull/6624, the config name was incorrect and not consistent with what was specified in the KIP. This PR fixes the inconsistency.
   
   There was also a concern raised about the mutability of `ConfigDef` in https://github.com/apache/kafka/pull/6624#pullrequestreview-238877899. I have made an attempt to fix it by returning a copy every time.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","20/May/19 19:02;githubbot;rhauch commented on pull request #6755: KAFKA-8265 : Fix config name to match KIP-458, Return a copy of the ConfigDef in Client Configs.
URL: https://github.com/apache/kafka/pull/6755
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","21/May/19 16:28;githubbot;rhauch commented on pull request #6776: KAFKA-8265 : Match client config override prefix to match with the KIP
URL: https://github.com/apache/kafka/pull/6776
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transactional EoS for source connectors,KAFKA-6080,13110431,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,ryannedolan,astubbs,astubbs,18/Oct/17 18:24,16/Oct/19 15:51,12/Jan/21 10:06,,,,,,,,,,,,KafkaConnect,,,,,,7,needs-kip,,,,Exactly once (eos) message production for source connectors.,,astubbs,barten,Boris Kaliadin,dthg,ghilainm@gmail.com,jpechane,LeonardoZV,orlandojin,ravishnkr.m,rhauch,ryannedolan,schofielaj,sliebau,VinodKsheersagar,vpernin,yangguo1220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-01-26 16:48:09.348,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 16 15:51:20 UTC 2019,,,,,,,"0|i3lf9j:",9223372036854775807,,,,,,,,,,,,,,,,"26/Jan/18 16:48;rhauch;We'll need a KIP to define how this is done.","26/Jan/18 17:04;rhauch;Here are some initial thoughts.

First, we want source connector's tasks to be able to optionally define the transaction boundaries. Since tasks are not able to coordinate or collaborate directly (they may be in different processes), that implies that tasks cannot collaborate on a single transaction.

Second, a source task can't call a separate API to demarcate the transaction boundaries, since the source task is being asked to returning messages via the poll method. So, the transaction demarcation needs to be expressable within the sequence of {{SourceRecord}} objects that are returned. Doing this will also allow a single call to {{poll()}} return messages in multiple transactions. However, we may want to _recommend_ that when possible (or require?) tasks always begin and commit/rollback a transaction within the same set of records returned by a single call to {{poll()}}. Obviously this won't be a requirement, but Connect does not currently place any guarantees on the timing of subsequent calls to {{poll()}}.

We could add to the Kafka Connect API three new subtypes of {{SourceRecord}} so that source connectors can explicitly control the boundary of these EOS transactions:
 * {{BeginTransaction}} represents the beginning of a transaction in the source system
 * {{CommitTransaction}} represents the successful completion of a transaction in the source system
 * {{RollbackTransaction}} represents the cancellation of a transaction in the source system

These record types would include the source partition and source offset, but would have a null value for the key, key schema, value, value schema, topic, and partition number fields of {{SourceRecord}}.

Source connectors include these records with normal {{SourceRecord}} objects and return them in the {{List<SourceRecord>}} results. Each transaction is specified with a {{BeginTransaction}}, followed by one or more normal records, and ending with either {{CommitTransaction}} or {{RollbackTransaction}}. A transaction sequence with no source records between a {{BeginTransaction}} and {{CommitTransaction}} would simply update the source offsets for the source partition, allowing the connector to record it has made progress without producing new records (see KAFKA-3821). And any transaction sequence that ends with a {{RollbackTransaction}} would correspond to an aborted EOS transaction.

Any {{SourceRecord}} objects outside of an explicit transaction sequence will be also written using Kafka's EOS feature, although the framework would be free to determine the EOS transaction boundaries.

Some things we still need to consider:

* Currently each source task is given its own {{Producer}} instance, so that should eliminate cross-talk between the transaction boundaries from different tasks writing to the same topics.
* Should Connect always use EOS transactions, even when the source connector/task does not use them?
* Should offsets be committed within the same EOS transactions? If so, what happens when transactions are rolled back? Any offset committing during that transaction would also be rolled back, but is that actually what we want?
* Should connectors or tasks be required to declare that they may use transactions? If so, how?","20/Jun/18 18:28;sliebau;Would it make sense to add a fourth subtype of SourceRecord which contains any offsets that the source tasks wants committed to the state topic, so that these can be added to the transaction as well? This would enable ""proper"" EoS as the source task could read records, add these records and the latest offset to the transaction and the sink task would either write everything or write nothing, so regardless of what fails where, we could restart from the correct point.","15/Oct/19 18:42;ryannedolan;I'm thinking about this differently than you guys [~rhauch] [~sliebau], but I'd like to take this on, if there are no objections. I'll put a KIP together in the near future. Given this has been idle for a while, I'll go ahead and assign this to myself.","16/Oct/19 14:22;schofielaj;I think this is a very interesting idea too. I'll be sure to contribute to the KIP discussion. I think about it in a different way than having SourceRecord subclasses to demarcate source system transaction boundaries.","16/Oct/19 15:19;rhauch;My previous comment suggests combining a few different feature requests: allowing tasks to optionally demarcate source-level transaction boundaries, allowing tasks to optionally update source offsets without writing new source records, and enabling Kafka EOS for the producer (primarily to eliminate the risk of writing duplicate records). I think all are important and all are related to each other, so let's make sure that if we only tackle a subset of these that we don't prevent the addition of the remaining features.

The first two are pretty straightforward, and I thought EOS might be straightforward -- so much so that I actually had a prototype at one point to enable Kafka EOS in Connect, but ran into a snag: Connect doesn't have affinity when it comes to task assignments for each worker, yet Kafka's EOS requires that each producer using AK transactions needs a unique ID and, upon a failure, *the same producer with the same ID needs to abort the previously incomplete transaction*. That means that when a task is restarted, it's likely it won't be able to abort the previous transaction and we'd have to wait for the transaction to time out. IIUC we're trying to minimize or eliminate that constraint. As I learned with my prototype, changing the code to use Kafka EOS is actually easy -- it's the recovery semantics correct that's the hard part.","16/Oct/19 15:51;ryannedolan;My goal is to eliminate duplicates downstream of a SourceConnector. The records returned by poll() should be stored in Kafka exactly as they come -- in the same order, exactly once, no additional dupes introduced by the worker. This is mostly straightforward.

Here's my working hypothesis:
- Workers get producer IDs from their Herder, based on the task ID.
- Workers send, flush, commit transactionally.
- When a Task fails, the Worker aborts the transaction.
- When a Worker fails, any outstanding transactions time out

No API changes are required for this, afaict. However, I may end up completely reimplementing WorkerSourceTask to get this right.

I'm happy to collaborate or yield if you guys want to pick this up: otherwise I'll put a KIP together very soon.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add possibility to reset Kafka streams application state to specific point in time,KAFKA-9031,13261881,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,lkokhreidze,lkokhreidze,11/Oct/19 20:55,16/Oct/19 09:33,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,needs-kip,,,,"Apache Flink has a feature called [savepoints|https://ci.apache.org/projects/flink/flink-docs-stable/ops/state/savepoints.html] which gives possibility to reset stream processing topology state to specific point in time. Similar feature can be useful for Kafka Streams applications as well.",,ableegoldman,jfilipiak,lkokhreidze,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-10-12 16:35:36.959,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 16 09:30:08 UTC 2019,,,,,,,"0|z07its:",9223372036854775807,,,,,,,,,,,,,,,,"11/Oct/19 20:59;lkokhreidze;Is a bit related to KAFKA-8236

Main difference is that in this ticket we're talking about consistent, periodic save-points no matter if new version of Kafka Streams application is deployed or not.","12/Oct/19 16:35;dongjin;Hi [~lkokhreidze], it seems like this issue is also related to KAFKA-8825 (and in turn, KAFKA-8766). How do you think?

For KAFKA-8825, I almost completed the draft implementation and working on the KIP. Could you have a look when I complete it?","13/Oct/19 02:18;mjsax;[~dongjin] – picking a ""starting point"" for a new application seems only to be semi-related, because it only applies for new deployments and the application would start with an empty state (especially for consumer that at inherently stateless).

This ticket is much more involved, as it tries to allow to reset an application including its state to a point in the past. It's far from trivial to reset application state in a consistent way.

Flink's savepoints are a way to do this, but the ""problem"" is, that those savepoints (that are basically copies of the local state stores at a specific point in time) must be store somewhere. Afaik, Flink stores savepoints in a configurable backend (like HDFS or S3). This implies that a dependency to an external system is introduced.

Furthermore, a savepoint dictates to what point in time you can reset, ie, it's not relative to when you start the application, but the savepoint itself dictates the start point. Last, you would need to take savepoint in advance to be able to use them – what is quite different to picking a ""start point"" because you can always pick an arbitrary startpoint – however, if you did not take a savepoint at a particular point in time in the past, you cannot reset to that point after the fact.","16/Oct/19 09:30;jfilipiak;Recommendation from my side to leave this feature out.

The natural kappa architecture is to re-bootstrap all state from kafka if necessary from the beginning. Thats were most of the simplicity of this architecture comes from. Implementing such a side kick will just increase complexity by a whole lot because its yet another fight against the architecture.

So i hope we can withstand the itchyness of a ticket being there and let  the project benefit from leaving this out.

Also: Isn't that why kafka supports microservices so well, people could just run flink as a separate consumer",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Give user options to turn off linger.ms/batch.size constraint,KAFKA-8973,13260453,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,bchen225242,bchen225242,bchen225242,03/Oct/19 21:52,09/Oct/19 19:55,12/Jan/21 10:06,09/Oct/19 19:55,,,,,,,,,,,,,,,,,0,,,,,"Currently there is no explicit way to understand whether we hit linger.ms or batch.size when producer sends out batches. From user's perspective, they should be allowed to opt-in either one or both, which improves the throughput/latency control in general.",,ableegoldman,bchen225242,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-10-04 19:26:37.805,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 07 21:07:33 UTC 2019,,,,,,,"0|z07a0o:",9223372036854775807,,,,,,,,,,,,,,,,"04/Oct/19 19:26;ableegoldman;If users want to opt-out of either, can't they just set it to infinity (or MAX_VALUE or whatever)?","04/Oct/19 19:35;ijuma;Yes, that's how people opt out today.","07/Oct/19 21:07;bchen225242;I guess it's not an action item that makes sense, closing it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Multiple Consumer Group Management (Describe, Reset, Delete)",KAFKA-7471,13188866,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,alex.dunayevsky,alex.dunayevsky,alex.dunayevsky,02/Oct/18 14:04,07/Oct/19 09:23,12/Jan/21 10:06,07/Oct/19 09:23,1.0.0,2.0.0,,,,,,2.4.0,,,tools,,,,,05/Oct/18 00:00,0,,,,,"Functionality needed:
 * Describe/Delete/Reset offsets on multiple consumer groups at a time (including each group by repeating `--group` parameter)
 * Describe/Delete/Reset offsets on ALL consumer groups at a time (add new --groups-all option similar to --topics-all)
 * Generate CSV for multiple consumer groups

What are the benifits? 
 * No need to start a new JVM to perform each query on every single consumer group
 * Abiltity to query groups by their status (for instance, `-v grepping` by `Stable` to spot problematic/dead/empty groups)
 * Ability to export offsets to reset for multiple consumer groups to a CSV file (needs CSV generation export/import format rework)

 ",,alex.dunayevsky,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-02-01 22:07:13.541,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 15 23:53:36 UTC 2019,,,,,,,"0|i3yqh3:",9223372036854775807,,,,,,,,,,,,,,,,"02/Oct/18 16:50;alex.dunayevsky;Pull request: https://github.com/apache/kafka/pull/5726","01/Feb/19 22:07;mjsax;Feature freeze deadline for 2.2 was yesterday. Moving this 2.3 release.","15/Apr/19 23:53;githubbot;vahidhashemian commented on pull request #5726: KAFKA-7471: Multiple Consumer Group Management Feature
URL: https://github.com/apache/kafka/pull/5726
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Return topic configs in CreateTopics response ,KAFKA-8907,13256616,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,13/Sep/19 21:00,07/Oct/19 08:41,12/Jan/21 10:06,07/Oct/19 08:39,,,,,,,,2.4.0,,,clients,,,,,,0,,,,,See [https://cwiki.apache.org/confluence/display/KAFKA/KIP-525+-+Return+topic+metadata+and+configs+in+CreateTopics+response] for details,,githubbot,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-09-24 11:04:41.331,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 27 12:28:05 UTC 2019,,,,,,,"0|z06mrc:",9223372036854775807,,omkreddy,,,,,,,,,,,,,,"24/Sep/19 11:04;githubbot;rajinisivaram commented on pull request #7380: KAFKA-8907; Return topic configs in CreateTopics response (KIP-525)
URL: https://github.com/apache/kafka/pull/7380
 
 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","27/Sep/19 12:28;githubbot;rajinisivaram commented on pull request #7380: KAFKA-8907; Return topic configs in CreateTopics response (KIP-525)
URL: https://github.com/apache/kafka/pull/7380
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Kafka Protocol should Support Optional Tagged Fields,KAFKA-8885,13255384,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cmccabe,cmccabe,cmccabe,06/Sep/19 22:55,07/Oct/19 04:13,12/Jan/21 10:06,07/Oct/19 04:13,,,,,,,,2.4.0,,,,,,,,,0,,,,,"Implement KIP-482: The Kafka Protocol should Support Optional Tagged Fields

See [KIP-482|https://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields]",,cmccabe,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-09-11 21:40:10.914,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 07 04:13:28 UTC 2019,,,,,,,"0|z06f5k:",9223372036854775807,,,,,,,,,,,,,,,,"11/Sep/19 21:40;githubbot;cmccabe commented on pull request #7325: KAFKA-8885: The Kafka Protocol should Support Optional Tagged Fields
URL: https://github.com/apache/kafka/pull/7325
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","07/Oct/19 04:13;githubbot;hachikuji commented on pull request #7325: KAFKA-8885: The Kafka Protocol should Support Optional Tagged Fields
URL: https://github.com/apache/kafka/pull/7325
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New Metric to Measure Number of Tasks on a Connector,KAFKA-8447,13236410,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,cvafadari,cvafadari,30/May/19 05:55,03/Oct/19 00:42,12/Jan/21 10:06,03/Oct/19 00:42,2.4.0,,,,,,,2.4.0,,,KafkaConnect,,,,,,0,needs-kip,,,,"KIP-475

Worker-level metrics expose number of tasks on a worker, but for many applications it is useful to have metrics on how many tasks each connector has.",,cvafadari,githubbot,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-10-02 22:59:18.131,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 03 00:42:26 UTC 2019,,,,,,,"0|z037u8:",9223372036854775807,,rhauch,,,,,,,,,,,,,,"02/Oct/19 22:59;rhauch;[KIP-475|https://cwiki.apache.org/confluence/display/KAFKA/KIP-475%3A+New+Metrics+to+Measure+Number+of+Tasks+on+a+Connector] has been adopted, and https://github.com/apache/kafka/pull/6843 implements the feature.

I've approved the PR, but I'm waiting for a green build before merging.","03/Oct/19 00:42;githubbot;rhauch commented on pull request #6843: KAFKA-8447: New Metric to Measure Number of Tasks on a Connector
URL: https://github.com/apache/kafka/pull/6843
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connection to Node2 was disconnected ,KAFKA-8327,13231914,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,suseemani@gmail.com,suseemani@gmail.com,07/May/19 03:25,23/Sep/19 07:36,12/Jan/21 10:06,,1.1.0,,,,,,,,,,consumer,,,,,,0,,,,,"Hi Team,

 

we are seeing the below errors in the kafka logs. We are using the kafka version 1.1.0,   COuld you please let us know if there are any fixes that can be applied to this ?

 

[2019-05-06 20:40:23,212] INFO [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Error sending fetch request (sessionId=896515694, epoch=738904) to node 2: java.io.IOException: Connection to 2 was disconnected before the response wa

s read. (org.apache.kafka.clients.FetchSessionHandler)

[2019-05-06 20:40:23,422] WARN [ReplicaFetcher replicaId=3, leaderId=2, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=3, maxWait=500, minBytes=1, maxBytes=10485760, fetchData=\{xxx-12=(offset=38826573, logStartOffset=38549032, maxBytes=1048576), xxxxxx-14=(offset=49033, logStartOffset=0, maxBytes=1048576), rWithSubscription-Cas-3=(offset=40752457, logStartOffset=40198369, maxBytes=1048576), xxxxxx-8=(offset=39543295, logStartOffset=39032103, maxBytes=1048576)}, isolationLevel=READ_UNCOMMITTED, toForget=, metadata=(s

essionId=896515694, epoch=738904)) (kafka.server.ReplicaFetcherThread)

java.io.IOException: Connection to 2 was disconnected before the response was read

 

We are also seeing the below errors..

 

[2019-05-07 01:39:57,310] INFO [ReplicaFetcher replicaId=0, leaderId=4, fetcherId=0] Retrying leaderEpoch request for partition __consumer_offsets-31 as the leader reported an error: NOT_LEADER_FOR_PARTITION (kafka.server.ReplicaFetcherThread)

 

Please let us know the reason for the same.

 

Thanks,

Mani

 ",,imohit,suseemani@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-09-21 17:25:24.178,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 23 07:36:05 UTC 2019,,,,,,,"0|z02g4o:",9223372036854775807,,,,,,,,,,,,,,,,"21/Sep/19 17:25;imohit;I am also facing this issue. Any workaround?","23/Sep/19 07:36;imohit;This has been solved by increasing the value of {{replica.socket.receive.buffer.bytes}} in all destination brokers.

After changing the above parameter and restarting broker. I was able to see the data in above-mentioned partitions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-145 - Expose Record Headers in Kafka Connect,KAFKA-5142,13067918,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,michael.andre.pearce,michael.andre.pearce,michael.andre.pearce,29/Apr/17 07:02,03/Sep/19 14:12,12/Jan/21 10:06,31/Jan/18 18:40,,,,,,,,1.1.0,,,clients,,,,,,17,,,,,"https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect

As KIP-82 introduced Headers into the core Kafka Product, it would be advantageous to expose them in the Kafka Connect Framework.
Connectors that replicate data between Kafka cluster or between other messaging products and Kafka would want to replicate the headers.
",,Akshar,dpeier,ewencp,githubbot,jeqo,michael.andre.pearce,rhauch,schizhov,sergioabplanalp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8863,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-04-29 07:11:52.351,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 31 18:40:40 UTC 2018,,,,,,,"0|i3ea2f:",9223372036854775807,,,,,,,,,,,,,,,,"29/Apr/17 07:11;githubbot;GitHub user michaelandrepearce opened a pull request:

    https://github.com/apache/kafka/pull/2942

    KAFKA-5142: Expose Record Headers in Kafka Connect

    as per KIP-145
    
    Add constructors to Connect/Source/SinkRecord to take Iterable<Header>
    add accessor method to ConnectRecord ""Headers headers()""
    Update WorkerSource/Sink to pass headers to/from Producer/ConsumerRecords

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/IG-Group/kafka KIP-145

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2942.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2942
    
----
commit 535ea20b63be67463d822530af95e535b020dd45
Author: Michael Andre Pearce <michael.andre.pearce@me.com>
Date:   2017-04-29T07:05:05Z

    KAFKA-5142: Expose Record Headers in Kafka Connect
    
    as per KIP-145
    
    Add constructors to Connect/Source/SinkRecord to take Iterable<Header>
    add accessor method to ConnectRecord ""Headers headers()""
    Update WorkerSource/Sink to pass headers to/from Producer/ConsumerRecords

----
","16/Oct/17 23:15;githubbot;GitHub user rhauch opened a pull request:

    https://github.com/apache/kafka/pull/4077

    KAFKA-5142: Added support for record headers, reusing Kafka client's interfaces

    *This is still a work in progress and should not be merged.*
    
    This is a proposed PR that implements most of [KIP-145](https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect) but with some changes. The Kafka client library's `Headers` and `Header` interfaces are used directly so as to minimize the overhead of converting instances to a Connect-specific object. However, a new `ConnectHeaders` class is proposed to provide a fluent builder for easily constructing headers either in source connectors or SMTs that need to add/remove/modify headers, and a reader utility component for reading header values and converting to primitives.
    
    Note that KIP-145 is still undergoing discussions, so this is provided merely as one possible approach.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rhauch/kafka kafka-5142

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/4077.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4077
    
----
commit 5f31191af94e2a324a694bc99e4639d968389ff2
Author: Randall Hauch <rhauch@gmail.com>
Date:   2017-10-16T22:59:50Z

    KAFKA-5142: Added support for record headers, reusing Kafka client's interfaces

----
","13/Dec/17 01:31;githubbot;GitHub user rhauch opened a pull request:

    https://github.com/apache/kafka/pull/4319

    [WIP] KAFKA-5142: Add Connect support for message headers (KIP-145)

    *NEW PROPOSAL FOR KIP-145... DO NOT MERGE*
    
    Changed the Connect API and runtime to support message headers as described in KIP-145.
    
    The new `Header` interface defines an immutable representation of a Kafka header (key-value pair) with support for the Connect value types and schemas. This interface provides methods for easily converting between many of the built-in primitive, structured, and logical data types.
    
    The new `Headers` interface defines an ordered collection of headers and is used to track all headers associated with a `ConnectRecord` (and thus `SourceRecord` and `SinkRecord`). This does allow multiple headers with the same key. The `Headers` contains methods for adding, removing, finding, and modifying headers. Convenience methods allow connectors and transforms to easily use and modify the headers for a record.
    
    A new `HeaderConverter` interface is also defined to enable the Connect runtime framework to be able to serialize and deserialize headers between the in-memory representation and Kafka’s byte[] representation. A new `SimpleHeaderConverter` implementation has been added, and this serializes to strings and deserializes by inferring the schemas (`Struct` header values are serialized without the schemas, so they can only be deserialized as `Map` instances without a schema.) The `StringConverter`, `JsonConverter`, and `ByteArrayConverter` have all been extended to also be `HeaderConverter` implementations. Each connector can be configured with a different header converter, although by default the `SimpleHeaderConverter` is used to serialize header values as strings without schemas.
    
    Unit and integration tests are added for `ConnectHeader` and `ConnectHeaders`, the two implementation classes for headers. Additional test methods are added for the methods added to the `Converter` implementations. Finally, the `ConnectRecord` object is already used heavily, so only limited tests need to be added while quite a few of the existing tests already cover the changes.
    
    ### Committer Checklist (excluded from commit message)
    - [ ] Verify design and implementation matches KIP-145
    - [ ] Verify test coverage and CI build status
    - [ ] Verify documentation (including upgrade notes)


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rhauch/kafka kafka-5142-b

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/4319.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4319
    
----
commit 1c35692da19f3c8c92ce60946a69f576878b958a
Author: Randall Hauch <rhauch@gmail.com>
Date:   2017-12-05T17:05:00Z

    KAFKA-5142: Add message headers to Connect API (KIP-145)
    
    Changed the Connect API to add message headers as described in KIP-145.
    
    The new `Header` interface defines an immutable representation of a Kafka header (name-value pair) with support for the Connect value types and schemas. Kafka headers have a string name and a binary value, which doesn’t align well with Connect’s existing data and schema mechanisms. Thus, Connect’s `Header` interface provides methods for easily converting between many of the built-in primitive, structured, and logical data types. And, as discussed below, a new `HeaderConverter` interface is added to define how the Kafka header binary values are converted to Connect data objects.
    
    The new `Headers` interface defines an ordered collection of headers and is used to track all headers associated with a `ConnectRecord`. Like the Kafka headers API, the Connect `Headers` interface allows storing multiple headers with the same key in an ordered list. The Connect `Headers` interface is mutable and has a number of methods that make it easy for connectors and transformations to add, modify, and remove headers from the record, and the interface is designed to allow chaining multiple mutating methods.
    
    The existing constructors and methods in `ConnectRecord`, `SinkRecord`, and `SourceRecord` are unchanged to maintain backward compatibility, and in these situations the records will contain an empty `Headers` object that connectors and transforms can modify. There is also an additional constructor that allows an existing `Headers` to be passed in. A new overloaded form of `newRecord` method was created to allow connectors and transforms to create a new record with an entirely new `Headers` object.
    
    A new `HeaderConverter` interface is also defined to enable the Connect runtime framework to be able to serialize and deserialize headers between the in-memory representation and Kafka’s byte[] representation.
    
    Unit and integration tests are added for `ConnectHeader` and `ConnectHeaders`, the two implementation classes for headers. The `ConnectRecord` object is already used heavily, so only limited tests need to be added while quite a few of the existing tests already cover the changes. However, new unit tests were added for `SinkRecord` and `SourceRecord to verify the header behavior, including when the `newRecord` methods are called.

commit f398eba326d6c0cc8732770cb3bfc962f0453995
Author: Randall Hauch <rhauch@gmail.com>
Date:   2017-12-13T01:27:26Z

    KAFKA-5142: Add message header converters to Connect API (KIP-145)
    
    This is the second commit for the public Connect API changes for KIP-145, and deals primarily with `HeaderConverter` implementations.
    
    Connect has three `Converter` implementations, `StringConverter`, `JsonConverter` and `ByteArrayConverter`. These were modified to also implement `HeaderConverter`, without changing any of the existing functionality.
    
    Like many of our pluggable components in Connect, the `HeaderConverter` interface extends `Configurable` that allows implementations to expose a `ConfigDef` that describes the supported configuration properties, and a `config` method that can be used to initialize the component with provided configuration properties. The `StringConverter`, `JsonConverter` and `ByteArrayConverter` were changed to support these methods in a backward compatible manner. There are now `StringConverterConfig` and `JsonConverterConfig` classes that define the `ConfigDef` for the implementations; the `ByteArrayConverter` has no configuration properties and doesn't need a config class.
    
    Note that the existing `Converter` interface has a special `config` signature with a parameter that sas whether the converter is being used for keys or values. This is different than the `Configurable.config` signature, so this commit adds new `ConverterConfig` abstract class that defines a `converter.type` property that can be used to set whether the converter is being used for keys, values, or headers. The existing `Converter` methods internally set this property based upon the supplied boolean parameter, so the default for `converter.type` can be `header`.

commit 14cf25a957ce1a7f0207f3fbdc9da5a30d5f3488
Author: Randall Hauch <rhauch@gmail.com>
Date:   2017-12-13T01:28:44Z

    KAFKA-5142: Add message headers to Connect runtime (KIP-145)
    
    This is the third commit for KIP-145 and changes the Connect runtime to support headers. Each Connect worker now configures a `HeaderConverter` for each connector task, in the same way it creates key and value `Converter` instances. This is entirely backward compatible, so that existing worker and connector configurations will work without changes. By default, the worker will use the `SimpleHeaderConverter` to serialize header values as strings and to deserialize them by inferring the schemas.

----
","24/Jan/18 07:31;rhauch;KIP-145 has passed.","31/Jan/18 18:40;ewencp;Issue resolved by pull request 4319
[https://github.com/apache/kafka/pull/4319]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide factories for creating Window instances,KAFKA-8829,13252686,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,marcospassos,marcospassos,23/Aug/19 14:18,25/Aug/19 18:55,12/Jan/21 10:06,,2.3.0,,,,,,,,,,streams,,,,,,0,,,,,"The API provides no ways to create {{Window}} instances without relying on internal classes.

 The issue becomes more evident when using session stores as both {{put}} and {{remove}} methods expects a windowed key, but the API does not expose any way to create a proper session-windowed key in the userland, leaving the developer with two choices: a) implement a window that duplicates the logic of {{SessionWindow}} or b) relies on the {{SessionWindow}}, which is an internal class.",,marcospassos,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-08-23 14:18:01.0,,,,,,,"0|z05zdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer benchmark test for paused partitions,KAFKA-8814,13251488,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,seglo,seglo,seglo,19/Aug/19 01:23,19/Aug/19 01:23,12/Jan/21 10:06,,,,,,,,,,,,consumer,system tests,tools,,,,0,,,,,"A new performance benchmark and corresponding {{ConsumerPerformance}} tools addition to support the paused partition performance improvement implemented in KAFKA-7548.  Before the fix, when the user would poll for completed fetched records for partitions that were paused, the consumer would throw away the data because it no longer fetchable.  If the partition is resumed then the data would have to be fetched over again.  The fix will cache completed fetched records for paused partitions indefinitely so they can be potentially be returned once the partition is resumed.

In the Jira issue KAFKA-7548 there are several informal test results shown based on a number of different paused partition scenarios, but it was suggested that a test in the benchmarks testsuite would be ideal to demonstrate the performance improvement.  In order to the implement this benchmark we must implement a new feature in {{ConsumerPerformance}} used by the benchmark testsuite and the {{kafka-consumer-perf-test.sh}} bin script that will pause partitions.  I added the following parameter:

{code:scala}
    val pausedPartitionsOpt = parser.accepts(""paused-partitions-percent"", ""The percentage [0-1] of subscribed "" +
      ""partitions to pause each poll."")
        .withOptionalArg()
        .describedAs(""percent"")
        .withValuesConvertedBy(regex(""^0(\\.\\d+)?|1\\.0$"")) // matches [0-1] with decimals
        .ofType(classOf[Float])
        .defaultsTo(0F)
{code}

This allows the user to specify a percentage (represented a floating point value from {{0..1}}) of partitions to pause each poll interval.  When the value is greater than {{0}} then we will take the next _n_ partitions to pause.  I ran the test on `trunk` and rebased onto the `2.3.0` tag for the following test summaries of {{kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput}}.  The test will rotate through pausing {{80%}} of assigned partitions (5/6) each poll interval.  I ran this on my laptop.

{{trunk}} ({{aa4ba8eee8e6f52a9d80a98fb2530b5bcc1b9a11}})

{code}
================================================================================
SESSION REPORT (ALL TESTS)
ducktape version: 0.7.5
session_id:       2019-08-18--010
run time:         2 minutes 29.104 seconds
tests run:        1
passed:           1
failed:           0
ignored:          0
================================================================================
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.paused_partitions_percent=0.8
status:     PASS
run time:   2 minutes 29.048 seconds
{""records_per_sec"": 450207.0953, ""mb_per_sec"": 42.9351}
--------------------------------------------------------------------------------
{code}

{{2.3.0}}

{code}
================================================================================
SESSION REPORT (ALL TESTS)
ducktape version: 0.7.5
session_id:       2019-08-18--011
run time:         2 minutes 41.228 seconds
tests run:        1
passed:           1
failed:           0
ignored:          0
================================================================================
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.paused_partitions_percent=0.8
status:     PASS
run time:   2 minutes 41.168 seconds
{""records_per_sec"": 246574.6024, ""mb_per_sec"": 23.5152}
--------------------------------------------------------------------------------
{code}

The increase in record and data throughput is significant.  Based on other consumer fetch metrics there are also improvements to fetch rate.  Depending on how often partitions are paused and resumed it's possible to save a lot of data transfer between the consumer and broker as well.

Please see the pull request for the associated changes.  I was unsure if I needed to create a KIP because while technically I added a new public api to the {{ConsumerPerformance}} tool, it was only to enable this benchmark to run.  If you feel that a KIP is necessary I'll create one.",,seglo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-08-19 01:23:06.0,,,,,,,"0|z05s40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create new re-elect controller admin function,KAFKA-1778,12755652,New Feature,In Progress,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,anigam,joestein,joestein,16/Nov/14 06:28,14/Aug/19 09:35,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,1,,,,,kafka --controller --elect,,alexismidon,anigam,becket_qin,doubleWei,guozhang,gwenshap,Hugh O'Brien,jeffwidman,jjkoshy,joestein,junrao,mgharat,sslavic,umesh9794@gmail.com,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-03-19 23:45:52.526,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 14 09:35:57 UTC 2019,,,,,,,"0|i22fjb:",9223372036854775807,,,,,,,,,,,,,,,,"19/Mar/15 23:45;anigam;I have a design for pinning the controller to a broker:
e we want to pin the controller to broker id x.

Handling the admin request in the controller:
a) We send the admin request to the controller.
b) It will create a persistent zookeeper node /admin/next_controller with data x.
c) It will then pull the information about broker id x to see if it is up and running through the alive broker list.
d) If the broker is up and running it will start 3-way handshake with x.
e) It will start a watch on /admin/ready_to_serve_as_controller zookeeper node.
f) It will send a message to the broker to tell it that it should become ready to serve as next_controller.
g) Broker x on receiving this message will create ephemeral node /admin/ready_to_server_as_controller.
h) Controller observes this change.
h) At this point the current controller will resign.

Changes in the election code:
a) All the brokers will pull from /admin/ready_to_server_as_controller with a watch.
b) If the brokers find that if this znode exists and their broker.id does not match the id specified in this ephemeral node they will simply not participate in the leader election.
c) Broker x will rightfully takes its place as the next controller.

c) The watches will be used in case broker x comes back to life.
d) In that case if I am the controller then I will resign.

Changes in the controller startup code:
a) Always pull from the /admin/next_controller for data changes as well as new data.
b) If there is any change try to setup the next broker similar to what has been specified in ""handling the admin request in the controller"".","20/Mar/15 00:15;mgharat;I did not get :

c) Broker x will rightfully takes its place as the next controller.

c) The watches will be used in case broker x comes back to life.
d) In that case if I am the controller then I will resign.

Can you explain this?","18/May/15 10:31;joestein;I was thinking that the broker when starting up would have another property. can.be.controller=false || can.be.controller=true

If a broker has this value to true, then it can be the controller and the thread starts up for the KafkaController, else it doesn't. Should be a few lines change in KafkaServer and config mod","18/May/15 17:24;becket_qin;Hey [~joestein], I'm a little bit confused here, just wondering in which case we would like to exclude some brokers as controller?","18/May/15 20:11;junrao;[~anigam], could you explain a bit the use case of pinning the controller to a broker?","19/May/15 08:16;jjkoshy;Jun - I think this is more for convenience/debugging and such. Right now there is no easy way to force a broker to become the controller.

Joe, since you filed this, you may be able to give some use case that you had in mind.

Abhishek, what would happen if the current controller A resigns and another broker B is pinned and about to become the new controller, but crashes before it can become the controller. Other brokers would not participate in controller reelection. So they should probably also watch the broker registrations so they know if the pinned controller goes down then they should proceed to participate in controller reelection anyway (to avoid a situation where you have no controller).","19/May/15 17:08;anigam;Jun,
The way I see it pinning the controller gives us multiple benefits:
a) If SREs are doing rolling upgrades they can set aside the broker on which the controller is pinned as the broker which they touch last.
This way there are only a limited number of controller moves and we can get more availability of the controller as a result as opposed to un-predictable number of controller moves.

b) I think more importantly if we do manual partition assignment we can set aside a broker to have very few partitions and this would reduce the impact on the controller from serving too many produce and consume events. To summarize it enables us to isolate the controller from the broker functionality potentially enabling us to push the brokers harder. 

Joel,
You are spot on. Since now all the brokers will be watching for the preferred controller node we can have the following situations:
a) All of them know about the preferred controller (zookeeper metadata has flowed to everyone). In this case the preferred controller would become the leader right away.

b) If some of them know about the preferred controller they will participate in the election and it is possible that somebody other than the preferred controller becomes the leader. What will happen in this case is that eventually this new controller will figure out that the preferred controller is available (thru zookeeper watch) to serve traffic it will resign and trigger another round of elections.
c) If none of them know about the preferred controller the behavior will be similar as above.

  ","21/May/15 00:39;jjkoshy;I may be missing some detail, but (a), (b), (c) don't quite fit the scenario I was asking about:

Even if all brokers know that a specific broker is supposed to become the preferred controller. What happens if that preferred controller is about to become the controller but crashes before it can update the /controller path in ZooKeeper. No further zookeeper watches will be triggered.

Also, can we avoid the message from current controller to the preferred controller by having all brokers just watch the admin/next_controller znode?

Under changes in election code - (a) did you mean that brokers should watch admin/next_controller znode (and not ready-to-serve-as-controller znode)?","21/May/15 15:25;junrao;[~anigam], for the two use cases that you mentioned, it seems that a simpler approach is what Joe said. Just configure a couple of brokers to be eligible for becoming the controller. Then, only those brokers will try to become the controller. I am not sure what the use case is to have an admin command to force the controller to move.","27/May/15 22:06;anigam;Joel,
What I was proposing was that all the brokers will watch the ready-to-serve-as-controller ephemeral node. In the scenario outlined where the preferred controller dies after the election is over but before it can write to the /controller node all the brokers will get this notification. Then there will be another round of elections in that case.

The controller is the one which pulls from /admin/next_controller persistent zookeeper node and also keeps a watch on it. If it detects this has been changed and the chosen broker id is different from it it will start the preferred controller move process.

""Also, can we avoid the message from current controller to the preferred controller by having all brokers just watch the admin/next_controller znode?"" This is definitely a better approach where zookeeper node can be used to achieve this messaging.

Jun,
In my opinion static assignment suffers from some issues where if the pre-determined controller goes down what happens or runs into any issues what happens.





","28/May/15 00:16;junrao;[~anigam], yes, you will need to configure multiple (e.g., 2 or 3) brokers eligible to be the controller in this approach. In this approach, the controller is guaranteed to be those eligible brokers. This seems a bit better for use case (b) since it allows one to completely isolate the controller from the other brokers that take produce/consumer load. In your proposal, the controller may still be on a non-preferred broker.  ","30/May/15 05:10;anigam;I believe what you are suggesting is that we can have a group of brokers flagged as potential brokers and all controller elections will be limited to that subset of brokers. Do I need to provide any failsafe in case all the flagged brokers are not able to participate in the required election and we are controller-less?

-Abhishek","30/May/15 09:40;joestein;Hey, sorry for late reply. I have seen now on a few dozen clusters situations where the broker gets into a state where the controller is hung and the only recourse is to either delete the znode from Zookeeper (/controller) to force a re-election or shutdown the broker. In the former case I have seen in one situation where the entire cluster went down. I am fairly certain this was because of the version of Zookeeper they were running (3.4.5) however I haven't ever tried to reproduce it. The latter case many folks don't want to shutdown the broker because they are in high traffic situations and doing so we could be a lot worse than the controller not working... sometimes that changes and they shut the broker down so the controller can fail over and their partition reassignment can continue to the new brokers they just launched (as an example).

So, originally we were thinking of fixing this be having an admin call that could trigger safely another leader election. We have been finding though that just having the broker start without it ever being able to be the controller (can.be.controller = false) is preferable in *a lot* of cases. This way there are brokers that will never be the controller and then some that could and with the brokers that could one of them would.

~ Joestein","11/Aug/15 18:33;gwenshap;Apparently I can't assign Reviewer if there is no patch, so [~guozhang], this is for you :)","11/Aug/15 21:30;guozhang;Chiming in late here, I think we are actually discussing two different, though somewhat overlapped issues:

1. When a controller is in bad state but not resigning, or if we just want to move controllers programmatically (i.e. not through deleting znode or bouncing broker), we want to trigger a re-election, and potentially enforce a certain broker to be the new controller during the re-election so that the whole cluster can still move on without losing one broker.

2. For isolating load scenarios, we want to start a broker while indicating it to be the controller candidate or not. Controller elections will only be triggered among the candidates.

Per the JIRA title suggests, I think we are targeting on the first issue, for which the motivation is mainly operation convenience; hence the solution for the second issue may not really be preferred since it still does not allow SREs to trigger a new election ([~charmalloc] corrects me if I am wrong). ","11/Aug/15 22:19;anigam;Hi Guozhang,
I agree 100% with you. Can you tell me what is the best way to move forward
on this on the open source side.

-Abhishek

On Tue, Aug 11, 2015 at 2:30 PM, Guozhang Wang (JIRA) <jira@apache.org>

","11/Aug/15 22:27;guozhang;Could you summarize your proposal on your 27/May/15 comment, and people can then discuss about safetyness in corner cases and efficiency? [~junrao] [~jjkoshy] [~charmalloc]","12/Aug/15 00:14;anigam;Thanks Guozhang,
I will write it up in a nice proposal.

-Abhishek

On Tue, Aug 11, 2015 at 3:28 PM, Guozhang Wang (JIRA) <jira@apache.org>

","03/Dec/15 21:43;granthenke;I broke this into its own task since it's independently tracked by [KIP-39: Pinning controller to broker |https://cwiki.apache.org/confluence/display/KAFKA/KIP-39+Pinning+controller+to+broker].","14/Aug/19 09:35;doubleWei;Is there any progress on this issue? In the preceding solution, a brokerX is specified as the next controler. If the brokerX is faulty and cannot be elected as a controller, the service is risky. A solution is provided. Currently, the controller is available in the broker range. In this way, the controller can be isolated and the controller can be highly reliable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"allow to attach callbacks in kafka streams, to be triggered when a window is expired ",KAFKA-6556,13138139,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,vvcephei,mazor.igal,mazor.igal,13/Feb/18 11:42,14/Aug/19 05:59,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,5,needs-kip,,,,"Allowing to attach callbacks in kafka streams, to be triggered when a window is expired,
 would help in use cases when the final state of the window is required.
 It would be also useful if together with that functionally the user would be able to control whether the callback would be triggered in addition to emitting the normal change log down the stream, or only triggering the callback when the window is expired. (for example in cases when only the final window state is required, and any updates to the window state during the window time interval are not important)  

An example for use case could be left join with proper sql semantics:
 A left join B currently would emit (a1,null) and (a1,b1) if b1 arrives within the defined join time window.
 what I would like is to have ONLY ONE result:
 (a1,null) if no b1 arrived during the defined join time window, OR ONLY (a1,b1) if b1 did arrived in the specified join time window.
 One possible solution could be to use the current kafka streams left join with downstream processor which would put the results in a windowed Ktable.
 The window size would be same as for the left join operation, however, only the final state of that window would be emitted down the stream once the time window is expired.
 So if the left join produces (a1, null) and after X minutes no (a1, b1) was produced, eventually only (a1, null) would be emitted, on the other hand, if the left join produces (a1, null) and after X-t minutes (a1, b1) was produced by the left join operation => only (a1, b1) would be emitted eventually down the stream after X minutes.

 

Another use case is when the window state is written to another kafka topic which is then used to persist the window states into a db, However, many times only the final window state
 is required, and functionality to get only the last window state would help in reducing load from the db, since only the final window state would be persisted to the db, instead of multiple db writes for each window state update. 

 

 ",,abhishek.agarwal,ableegoldman,astubbs,ddhirajkumar,edgurgel,guiboui,hustclf,jonathanpdx,mazor.igal,mjsax,nanalital,psby,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7368,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-03-12 11:32:37.002,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 14 05:59:56 UTC 2019,,,,,,,"0|i3q4af:",9223372036854775807,,,,,,,,,,,,,,,,"12/Mar/18 11:32;abhishek.agarwal;I am working on something similar (currently for tumbling windows). A construct to receive only the final updates for a window is quite useful and powerful. ","10/Sep/18 02:42;mjsax;[~vvcephei] I don't think this is covered by KIP-328 and think we should move it to 2.2.0 as fixed version? WDYT?","10/Sep/18 15:52;vvcephei;Yes, you're right. I was thinking superficially before, but this ticket specifically mentions joins.

It should be possible to do something similar, but it wouldn't be automatically fixed by KIP-328 final-result mode, since joins don't result in `KTable<Windowed<..>,..`.","01/Feb/19 22:10;mjsax;Feature freeze deadline for 2.2 was yesterday. Moving this 2.3 release.","13/Aug/19 19:00;nanalital;[~mjsax] i don't see the issue in the plan for release 2.3.1 , is it really in the plan  ?

Our use-case description: 
We want in addition for (and after) sending event *per key* when window ends (using suppress) we also want to send an event that *marks the end of the window*, so consumers will know that the window currently ended and that they can start using the new version of data.  ","14/Aug/19 05:59;mjsax;This feature is not schedule atm and I don't think that [~vvcephei] is working on it.

Not sure if I fully understand what you want/need for your use case though.

In any case, it's new feature and it would not be included in a bug fix release but either a minor or major release. However, the whole scope of the ticket and how it could be addressed in unclear atm, as there is KIP yet.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-460 Admin Leader Election RPC,KAFKA-8286,13229989,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jagsancio,jagsancio,jagsancio,24/Apr/19 22:34,12/Aug/19 08:33,12/Jan/21 10:06,29/May/19 17:45,,,,,,,,2.4.0,,,admin,clients,core,,,,0,admin,client,controller,,"Tracking issue for implementing KIP-460. Tasks:
 # [Done] Design KIP
 # [Done] Review KIP
 # [Done] Approve KIP
 # [Done] Update RPC to support KIP
 # [Done] Update controller to support KIP
 # [Done] Create CLI command (kafka-leader-election) that implement KIP
 # [Done] Search and replace any usage of “preferred” in the code
 # [Done] Add test for controller functionality
 # [Done] Revisit all of the documentation - generate and audit the new javadocs
 # [Done] Deprecated since... needs to be update
 # [Done] Review PR
 # [Done] Merge PR
 # [Done] Update the KIP based on the latest implementation
 # [Done] Add test for command",,githubbot,jagsancio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-05-29 17:44:54.714,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 01 00:18:53 UTC 2019,,,,,,,"0|z02494:",9223372036854775807,,,,,,,,,,,,,,,,"06/May/19 23:45;jagsancio;PR: https://github.com/apache/kafka/pull/6686/","24/May/19 20:34;jagsancio;https://github.com/apache/kafka/pull/6686","29/May/19 17:44;githubbot;hachikuji commented on pull request #6686: KAFKA-8286; Leader Election Admin RPC (KIP-460)
URL: https://github.com/apache/kafka/pull/6686
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","01/Jun/19 00:18;githubbot;jsancio commented on pull request #6857: KAFKA-8286: Integration tests for unclean electLeaders
URL: https://github.com/apache/kafka/pull/6857
 
 
   
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set SCRAM passwords via the Admin interface,KAFKA-8780,13249916,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,tombentley,tombentley,tombentley,09/Aug/19 15:06,09/Aug/19 15:06,12/Jan/21 10:06,,,,,,,,,,,,admin,,,,,,0,,,,,It should be possible to set user's SCRAM passwords via the Admin interface.,,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-08-09 15:06:02.0,,,,,,,"0|z05if4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka to support using ETCD beside Zookeeper,KAFKA-6598,13141265,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,stoader,stoader,27/Feb/18 15:55,05/Aug/19 18:43,12/Jan/21 10:06,,,,,,,,,,,,clients,core,,,,,15,,,,,"The current Kafka implementation is bound to {{Zookeeper}} to store its metadata for forming a cluster of nodes (producer/consumer/broker). 
As Kafka is becoming popular for streaming in various environments where {{Zookeeper}} is either not easy to deploy/manage or there are better alternatives to it there is a need 
to run Kafka with other metastore implementation than {{Zookeeper}}.

{{etcd}} can provide the same semantics as {{Zookeeper}} for Kafka and since {{etcd}} is the favorable choice in certain environments (e.g. Kubernetes) Kafka should be able to run with {{etcd}}.
From the user's point of view should be straightforward to configure to use {{etcd}} by just simply specifying a connection string that point to {{etcd}} cluster.

To avoid introducing instability the original interfaces should be kept and only the low level {{Zookeeper}} API calls should be replaced with \{{etcd}} API calls in case Kafka is configured 
to use {{etcd}}.

On the long run (which is out of scope of this jira) there should be an abstract layer in Kafka which then various metastore implementations would implement.",,_V_,adupriez,baluchicken,ben_b,cmccabe,darvar,davzucky,dhruvilshah,dlorych,hanm,ijuma,isimeonidis,lkokhreidze,mode,mseifert,otis,ouyi,philbour,ppatierno,pwegrzyn,reitzmichnicht,rgomes,schnie,scholzj,sderosiaux,shaharmor,smurakozi,stoader,tessus,tombar,tombentley,viktorsomogyi,vultron81,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-02-28 17:55:42.189,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 05 18:43:18 UTC 2019,,,,,,,"0|i3qnjb:",9223372036854775807,,,,,,,,,,,,,,,,"27/Feb/18 16:00;stoader;There is a first implementation of supporting the use of {{etcd}} as metastore here: https://github.com/banzaicloud/apache-kafka-on-k8s as part of the effort of running Kafka on Kubernetes.

[~vikgamov] can you advise if this requires a KIP ?","28/Feb/18 17:55;cmccabe;Thanks for looking at this.  We're planning on removing the ZooKeeper dependency (the KIP hasn't been posted yet, but will soon).  There are a number of advantages in removing the dependency on external systems.  Therefore, it shouldn't be necessary to add pluggability at this layer of the code.","08/Mar/18 14:34;tombentley;[~cmccabe] any more info about that (such as when might the KIP be published?)","28/Mar/18 16:56;ijuma;This was discussed before when ""KIP-30: Allow for brokers to have plug-able consensus and meta data storage sub systems"" was proposed. It's worth reading that discussion for the concerns with a pluggable approach.","28/Mar/18 20:59;ijuma;Here's a link: https://lists.apache.org/thread.html/2bc187040051008452b40b313db06b476c248ef7a5ed7529afe7b118@1448997154@%3Cdev.kafka.apache.org%3E","04/Apr/19 09:20;viktorsomogyi;[~ijuma], [~cmccabe] do you have any info if this work is still pending or dropped?","18/Apr/19 16:05;tessus;I'd also like to know the status of this. I really would like to use etcd instead of Zookeeper.","05/Jun/19 14:54;baluchicken;It's been more than a year since we are waiting for the KIP mentioned by [~cmccabe] I would like to know the status of that. 
Is that happening soon, or we can proceed somehow with this?","01/Jul/19 13:10;shaharmor;Would really appreciate removing zookeeper as it makes maintenance a whole lot harder.

When can we expect the KIP?","05/Aug/19 18:43;sderosiaux;The KIP is up: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a JSON Array Serde for serializing a List to a JSON Array and vice versa,KAFKA-8593,13241293,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,oweiler,oweiler,24/Jun/19 15:55,24/Jun/19 19:21,12/Jan/21 10:06,,,,,,,,,,,,core,,,,,,0,,,,,"When aggregating streams you often group several values by a key.

Because the JSON Serializer/Deserializer pair only supports Java objects and arrays, this leads to ugly code when working with lists, because you have to convert from / to an array or wrap the list in an object.

 

I propose a JSON array serializer/deserializer for that use case.

A basic Implementation can be found here: https://github.com/helpermethod/spring-kafka/commit/ac02464daf57dd6461bd30b6440e125dc2509ae7

 ",,ableegoldman,oweiler,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-06-24 19:21:51.895,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 24 19:21:51 UTC 2019,,,,,,,"0|z041u0:",9223372036854775807,,,,,,,,,,,,,,,,"24/Jun/19 15:56;oweiler;If this feature would be considered, I would gladly provide the PR.","24/Jun/19 19:21;vvcephei;Hey [~oweiler],

Just to be sure, it looks like this is a proposal for spring-kafka, is that right?

If so, that interface is maintained under Spring, not Apache Kafka. I think the right way to propose your change is listed here: https://github.com/spring-projects/spring-kafka#contributing-to-spring-kafka .

Apologies in advance if I misunderstood.

-John",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Admin tool to setup Kafka Stream topology (internal) topics,KAFKA-8342,13232363,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,clearpal7,bchen225242,bchen225242,09/May/19 02:08,21/Jun/19 09:51,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,newbie,,,,"We have seen customers who need to deploy their application to production environment but don't have access to create changelog and repartition topics. They need to ask admin team to manually create those topics before proceeding to start the actual stream job. We could add an admin tool to help them go through the process quicker by providing a command that could
 # Read through current stream topology
 # Create corresponding topics as needed, even including output topics.",,ableegoldman,bchen225242,carlos.duclos,clearpal7,guozhang,mjsax,pkleindl,richcase,Yohan123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-05-09 09:27:29.186,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 21 09:51:49 UTC 2019,,,,,,,"0|z02ivs:",9223372036854775807,,,,,,,,,,,,,,,,"09/May/19 09:27;pkleindl;This sounds very interesting, I hope it is ok if I add our use case and some thoughts:

Our current goal was to minimize deployment order dependencies so we don't have to mind deploying an application (mostly streams, but normal producers/consumers too) and have all relevant topics created before startup.

The internal topics are handled by Kafka Streams itself, but as we are moving to a more restricted approach we will be in the situation described above.

Our solution was to add the definition of topics for each application as a configuration artifact and share the dependencies via maven.

On startup (or triggered via a command-line call) this configuration is parsed and the topics are set up as needed.

We do this for all ""named"" topics including input and output topics and could add the internal topics if needed.

If I understand the approach above correctly this would mean that a user would have to export the topology (either the toString() version or something more beautiful) and pass it to the tool for processing.

One thing I would like to add to the use case is the interaction with the schema registry, as we are using Avro the creation of the schemas for the internal topics and a necessary pain too as I expect this to be restricted in a similar way as topic creation.","09/May/19 23:05;guozhang;[~pkleindl] thanks for sharing your use case, I think we indeed need to think more about what's a pleasant procedure for users to use any admin tools to pre-create topics --- better not to manually export the topology by themselves. As for schemas, they should be properly registered at runtime by Streams' producer clients I think.","24/May/19 02:04;bchen225242;[~clearpal7] Let me know if this sounds interesting to you.","24/May/19 04:57;clearpal7;[~bchen225242] I`m interested in this ticket to help them to go through the process quicker by providing a command tool.

could I take this ticket?

 ","24/May/19 06:56;bchen225242;[~clearpal7] Yea, feel free to take it. I will probably be away for a couple of days, you could take a initial look and see how that tool should be built. Thank you for the help!","20/Jun/19 12:22;carlos.duclos;Any progress on this? Do you need help?","21/Jun/19 09:20;clearpal7;yep. It is in progress.

If I need you help, I would like to be helped you.

Do you have any reference to work this??

Thanks","21/Jun/19 09:45;carlos.duclos;I have read the information in the ticket and the comments, that is all the information I have. Is there a design document that you could share with me?","21/Jun/19 09:51;clearpal7;Not yet. I`m so sorry.

I`m going to share some document what I`m doing so far.

It is probably will be sharing in a week.

Once again I`m so sorry about late.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Kafka Pluggable with customized Keystore/TrustStore,KAFKA-8561,13240328,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,thomas930410,thomas930410,thomas930410,19/Jun/19 01:11,19/Jun/19 03:41,12/Jan/21 10:06,,,,,,,,,,,,clients,security,,,,,0,,,,,"A lot of company needs to enable TLS for Kafka for security perspective and Kafka provides file-based configuration to load keystore and truststore from directories. 

However, it is hard to plug-in the customized in-memory Keystore and Truststore in current kafka version.

We want to make Keystore and Truststore pluggable which means Kafka Broker and Kafka Client could load the Keystore and Truststore from other service at the start time to enable Kafka using customized Keystore and Truststore. ",,ivanyu,thomas930410,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-06-19 01:11:52.0,,,,,,,"0|z03vw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generate Topic/Key from Json Transform,KAFKA-8554,13240211,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,gokhansari,gokhansari,18/Jun/19 14:01,18/Jun/19 14:19,12/Jan/21 10:06,,,,,,,,,,,,KafkaConnect,,,,,,0,,,,,"In a configurable pattern, topic and key generation is needed. This pattern could include static values and dynamic parameters which are exist in json tree.

Eg:
 * property.format = ""*signals_\{appId}_\{date}"" >> ""signals_app01_18-06-2019""*
 ** static '*signals*'
 ** parameter '*appId*' from json tree
 ** parameter or record '*date*'

 * property.date.field = ""*details.signalCreationDate""*
 ** parameter '*details.signalCreationDate*' path from json tree

 * property.date.format = ""*dd-MM-yyyy""*
 ** date format for date parameters or record dates

 

Extracting topic or key (properties) with these way will led developers to use them according to their business logic.

 

Especially this will be useful for Elasticsearch Kafka Connector in case of dynamic index name and dynamic document id generation needs.",,githubbot,gokhansari,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-06-18 14:06:00.957,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 18 14:19:03 UTC 2019,,,,,,,"0|z03v68:",9223372036854775807,,,,,,,,,,,,,,,,"18/Jun/19 14:06;githubbot;gokhansari commented on pull request #6960: KAFKA-8554 Generate Topic/Key from Json Transform
URL: https://github.com/apache/kafka/pull/6960
 
 
   In a configurable pattern, topic and key generation is needed. This pattern could include static values and dynamic parameters which are exist in json tree.
   
   Eg:
   
   - property.format = ""signals_{appId}_{date}"" >> ""signals_app01_18-06-2019""
   static 'signals'
   parameter 'appId' from json tree
   parameter or record 'date'
   - property.date.field = ""details.signalCreationDate""
   parameter 'details.signalCreationDate' path from json tree
   - property.date.format = ""dd-MM-yyyy""
   date format for date parameters or record dates
   
    
   Extracting topic or key (properties) with these way will led developers to use them according to their business logic.
   
   
   Especially this will be useful for Elasticsearch Kafka Connector in case of dynamic index name and dynamic document id generation needs.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","18/Jun/19 14:19;githubbot;gokhansari commented on pull request #6960: KAFKA-8554 Generate Topic/Key from Json Transform
URL: https://github.com/apache/kafka/pull/6960
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit the maximum number of connections per ip client ID,KAFKA-8505,13238199,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,soarez,soarez,07/Jun/19 13:07,07/Jun/19 13:07,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,"As highlighted by KAFKA-1512 back in 2014, it is important to be able to limit the number of client connections to brokers to maintain service availability. With multiple use-cases on the same cluster, it's important to prevent one misconfigured use-case from affecting other use-cases.

Cloud infrastructure technology has come a long way since then. Nowadays days in a private network using container orchestration technology, IPs come cheap. Limiting connections solely on origin IP is no longer acceptable.

Kafka needs to support connection limits based on client identity.

A new configuration property - {{max.connections.per.clientid}} - should work similarly to {{max.connections.per.ip}} using ConnectionQuotas, managed straight after parsing the request header in SocketServer.",,soarez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-06-07 13:07:28.0,,,,,,,"0|z03isw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add API to allow user to define end behavior of consumer failure,KAFKA-8438,13235891,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,Yohan123,Yohan123,28/May/19 02:14,05/Jun/19 20:08,12/Jan/21 10:06,,,,,,,,,,,,consumer,,,,,,0,needs-dicussion,needs-kip,,,"Recently, in a concerted effort to make Kafka's rebalances less painful, various approaches has been used to reduce the number of and impact of rebalances. Often, the trigger of a rebalance is a failure of some sort or a thrown exception during processing, in which case, the workload will be redistributed among surviving threads. Working to reduce rebalances due to random consumer crashes, a recent change to Kafka internals had been made (which introduces the concept of static membership) that prevents a rebalance from occurring within {{session.timeout.ms}} in the hope that the consumer thread which crashed would recover in that time interval and rejoin the group.

However, in some cases, some consumer threads would permanently go down or remain dead for long periods of time. In these scenarios, users of Kafka would possibly not be aware of such a crash until hours later after it happened which forces Kafka users to manually start a new KafkaConsumer process a considerable period of time after the failure had occurred. That is where the addition of a callback such as {{onConsumerFailure}} would help. There are multiple use cases for this callback (which is defined by the user). {{onConsumerFailure}} is called when a particular consumer thread goes under for some specified time interval (i.e. a config called {{acceptable.consumer.failure.timeout.ms}}). When called, this method could be used to log a consumer failure or should the user wish it, create a new thread which would then rejoin the consumer group (which could also include the required {{group.instance.id}} so that a rebalance wouldn't be re-triggered –- we would need to think about that). 

Should the old thread recover and attempt to rejoin the consumer group (with the substitute thread being part of the group), the old thread will be denied access and an exception would be thrown (to indicate that another process has already taken its place).

 

 

 ",,bchen225242,Yohan123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-05-28 04:24:44.253,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 05 20:08:47 UTC 2019,,,,,,,"0|z034mw:",9223372036854775807,,,,,,,,,,,,,,,,"28/May/19 02:16;Yohan123;cc [~bchen225242] WDYT?","28/May/19 04:23;Yohan123;Oh, by the way, will probably work on this issue once I'm done with the other PRs I have in progress.","28/May/19 04:24;bchen225242;Thanks for the proposal [~Yohan123]! it definitely does no harm for user to register a callback in case of a consumer failure, if they want to add alerting calls to customized service. However my assumption is that usually a thread consumer would die due to some uncaught exception during data processing or sth similar. If the exception is fatal, could we guarantee the triggering of this callback if the failure is not caused by consumer itself?

Also in terms of bootstrapping a new thread at this moment, who would be the caller? Does that mean we need to initialize a daemon thread on start-up to monitor consumer thread health?

The current plan is to use external monitoring service to detect consumer failures like this. So when something goes wrong, we could just choose to restart the service so that consumer will use the same `group.instance.id` to rejoin. Still, I feel this new API should be useful, but may be an overkill to solve the rebalance problem.

Maybe [~guozhang] and [~hachikuji] could also share some opinions here.","28/May/19 23:35;Yohan123;Hi [~bchen225242] Thanks for sharing your thoughts. :)  I didn't realize that processing exceptions will probably be the more likely cause for consumer crash (will change the issue description).  

I have done some thinking about this, and I think it could be done in a straightforward manner. Heartbeat thread does exist for a reason, and periodically, by design, Kafka server/broker will send a heartbeat request to check that the consumer is alive (otherwise, how would broker know when to kick a consumer out of the group and cause a rebalance?). We shouldn't need an extra daemon thread. Instead, I think it would be possible for Kafka broker to trigger this callback if it has detected that a consumer has fallen out of the group. 

Also, you mentioned something about the external monitoring service. This issue could help save the user some programming time of implementing this service because Kafka is in charge of detecting the failure instead of the client. (I mean it makes sense: we have more metrics and resources available for reference than the user since they couldn't query anything in internals). In summary, the user does not have to do the monitoring part, we do. I think that would significantly simplify the program they need to write to account for consumer failure.","31/May/19 04:19;bchen225242;Hey Richard, thanks for the update. I'm still not quite clear the purpose of adding this callback at the moment. Quoted from your original statement:

{{When called, this method could be used to log a consumer failure or should the user wish it, create a new thread which would then rejoin the consumer group (which could also include the required }}{{group.instance.id}}{{ so that a rebalance wouldn't be re-triggered –- we would need to think about that). }}


 I think logging a consumer failure is not that important, because either way user has to be aware of the failure, then this logging needs to talk to external service, generally a path from application -> local metrics agent -> remote time series DB -> pager system. It's not a very much saving of effort with this new callback.

Also for rebooting a new consumer, this consumer needs initialization configs and so on. Where are we planning to store these configuration data? If current consumer is already dead, who will be responsible for bringing up an almost-the-same consumer?

Could you brainstorm more use cases other than the error logging and rebooting here?","05/Jun/19 20:08;Yohan123;Sure [~bchen225242] will look into it and see what else I could think of.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Idempotent/transactional Producer (KIP-98),KAFKA-4815,13047090,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hachikuji,hachikuji,hachikuji,28/Feb/17 20:54,30/May/19 08:16,12/Jan/21 10:06,27/Jun/17 14:21,,,,,,,,0.11.0.0,,,clients,core,producer ,,,,4,kip,,,,This issue tracks implementation progress for KIP-98: https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging.,,2pl,baluchicken,githubbot,hachikuji,ijuma,jay.zhuang,mherstine,mjsax,tdas,ThomasL,tom.dewolf,umesh9794@gmail.com,wenxuanguan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-24 21:26:44.454,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 30 08:16:51 UTC 2019,,,,,,,"0|i3ar3b:",9223372036854775807,,,,,,,,,,,,,,,,"24/Mar/17 21:26;githubbot;GitHub user apurvam opened a pull request:

    https://github.com/apache/kafka/pull/2735

    KAFKA-4815 : Add idempotent producer semantics

    This is from the KIP-98 proposal. 
    
    The main points of discussion surround the correctness logic, particularly the Log class where incoming entries are validated and duplicates are dropped, and also the producer error handling to ensure that the semantics are sound from the users point of view.
    
    There is some subtlety in the idempotent producer semantics. This patch only guarantees idempotent production upto the point where an error has to be returned to the user. Once we hit a such a non-recoverable error, we can no longer guarantee message ordering nor idempotence without additional logic at the application level.
    
    In particular, if an application wants guaranteed message order without duplicates, then it needs to do the following on the error callback:
    
    # Close the producer so that no queued batches are sent. This is important for guaranteeing ordering.
    # Read the tail of the log to inspect the last message committed. This is important for avoiding duplicates.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/confluentinc/kafka exactly-once-idempotent-producer

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2735.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2735
    
----
commit 83f5b2937d368efa768be6d7df1f57d99425a252
Author: fpj <fpj@apache.org>
Date:   2016-10-10T16:19:52Z

    Add broker producer id mapping support

commit 37eac83364a67eebf0e1d96552d9fe864baa3d76
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-02-14T19:12:58Z

    KEOS: idempotent producer pid generation (#126)
    
    Add the transaction coordinator for pid generation and management.

commit 6d918d0b44302aa81627f3f2748756356614f228
Author: hachikuji <jason@confluent.io>
Date:   2017-02-23T18:59:46Z

    Minor fixes for test failures and consistency (#133)

commit d1868602be8b3e81d98644d673a73b9f730ad586
Author: Jason Gustafson <jason@confluent.io>
Date:   2017-03-01T22:40:30Z

    Fix headers, new checkstyle rules, and some breakage in KafkaApis

commit f2c01a71508cd5c1eaf8c03a372f7dc308af3cf2
Author: hachikuji <jason@confluent.io>
Date:   2017-03-07T00:23:11Z

    Avoid removal of non-expired PIDs when log cleaning (#138)

commit 31ff08eaf704b9e4fe6ee492277d981b47e8016c
Author: Apurva Mehta <apurva.1618@gmail.com>
Date:   2017-03-09T01:50:01Z

    Client side implementation of the idempotent producer. (#129)

commit 439f284fd4efa56449d43f2fb5e14614689c7b80
Author: Apurva Mehta <apurva@confluent.io>
Date:   2017-03-09T21:23:55Z

    Fix build errors due to rebase

commit 0a81a40258da29ed62caf745e7c1895ca7eead4c
Author: hachikuji <jason@confluent.io>
Date:   2017-03-10T17:50:01Z

    PIDs should be expired according to the transactional id expiration setting (#139)

commit 2643e5b6bb6ad6b3d1cd6b15073a92396be5693b
Author: Apurva Mehta <apurva@confluent.io>
Date:   2017-03-15T18:45:07Z

    Fix build errors due to rebase

commit 848137a28b02cd085605e92726d0457b0153e1c4
Author: Apurva Mehta <apurva@confluent.io>
Date:   2017-03-16T18:43:29Z

    Remove depependence of MemoryRecordsBuilder on TransactionState

commit 15d23b65166cff6dbd7be0e925ff9ec811d7baac
Author: hachikuji <jason@confluent.io>
Date:   2017-03-23T22:30:39Z

    A few minor improvements (#150)

commit fafffa8f2445e2a559f24e5286c948c616287f9a
Author: Apurva Mehta <apurva.1618@gmail.com>
Date:   2017-03-24T20:02:48Z

    Reset transaction state on all irrecoverable exceptions (#153)

commit 637f864887e341c51f4fd7192016d2afc45bcf1e
Author: Apurva Mehta <apurva@confluent.io>
Date:   2017-03-24T21:08:29Z

    Fix build errors due to rebase

----
","08/Jun/17 02:10;ijuma;I moved this JIRA to 0.11.1.0 as it includes a number of subtasks that won't be merged in 0.11.0.0. Ideally, we'd move the tasks that are not for 0.11.0.0 into another umbrella JIRA, but it's quite a bit of work and not clear if it's worth it.","30/May/19 08:16;wenxuanguan;how transaction support multiple producer instances.
when multiple producer share the same txn id, throw the following exception:
org.apache.kafka.common.KafkaException: Cannot execute transactional method because we are in an error state
	at org.apache.kafka.clients.producer.internals.TransactionManager.maybeFailWithError(TransactionManager.java:784)
	at org.apache.kafka.clients.producer.internals.TransactionManager.beginTransaction(TransactionManager.java:215)
	at org.apache.kafka.clients.producer.KafkaProducer.beginTransaction(KafkaProducer.java:606)
	at com.matt.test.kafka.producer.ProducerTransactionExample.main(ProducerTransactionExample.java:68)
Caused by: org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit total number of active connections in the broker,KAFKA-7730,13204215,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,13/Dec/18 12:01,25/May/19 22:32,12/Jan/21 10:06,14/Mar/19 23:21,2.1.0,,,,,,,2.3.0,,,network,,,,,,0,,,,,Add a new listener config `max.connections` to limit the maximum number of active connections on each listener. See https://cwiki.apache.org/confluence/display/KAFKA/KIP-402%3A+Improve+fairness+in+SocketServer+processors for details.,,githubbot,mjsax,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-12-14 16:31:59.347,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 14 23:21:04 UTC 2019,,,,,,,"0|s01hdc:",9223372036854775807,,gwenshap,,,,,,,,,,,,,,"14/Dec/18 16:31;githubbot;rajinisivaram opened a new pull request #6034: KAFKA-7730: Limit number of active connections per listener in brokers (KIP-402)
URL: https://github.com/apache/kafka/pull/6034
 
 
   Adds a new listener config `max.connections` to limit the number of active connections on each listener. The config may be prefixed with listener prefix. This limit may be dynamically reconfigured without restarting the broker.
   
   This is one of the PRs for KIP-402 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-402%3A+Improve+fairness+in+SocketServer+processors). Note that this is currently built on top of PR #6022 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","17/Feb/19 18:45;mjsax;Moving all major/minor/trivial tickets that are not merged yet out of 2.2 release.","14/Mar/19 23:21;githubbot;gwenshap commented on pull request #6034: KAFKA-7730: Limit number of active connections per listener in brokers (KIP-402)
URL: https://github.com/apache/kafka/pull/6034
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Protocol and consumer support for follower fetching,KAFKA-8365,13233306,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mumrah,mumrah,mumrah,14/May/19 19:11,21/May/19 22:50,12/Jan/21 10:06,18/May/19 05:51,,,,,,,,2.3.0,,,consumer,,,,,,0,,,,,"Add the consumer client changes and implement the protocol support for [KIP-392|https://cwiki.apache.org/confluence/display/KAFKA/KIP-392%3A+Allow+consumers+to+fetch+from+closest+replica]",,githubbot,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-05-14 19:39:54.774,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 21 22:50:28 UTC 2019,,,,,,,"0|z02oo8:",9223372036854775807,,,,,,,,,,,,,,,,"14/May/19 19:39;githubbot;mumrah commented on pull request #6731: KAFKA-8365 Consumer support for follower fetch
URL: https://github.com/apache/kafka/pull/6731
 
 
   Support for preferred read replica in consumer client
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","18/May/19 05:45;githubbot;hachikuji commented on pull request #6731: KAFKA-8365 Consumer support for follower fetch
URL: https://github.com/apache/kafka/pull/6731
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","20/May/19 16:42;githubbot;mumrah commented on pull request #6775: Minor: follow up for KAFKA-8365
URL: https://github.com/apache/kafka/pull/6775
 
 
   A few minor things left over from #6731 
   
   * Callers can specify replicaId in OffsetsForLeaderEpoch request
   * Improved FetchRequest constructor
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","21/May/19 22:50;githubbot;hachikuji commented on pull request #6775: Minor: follow up for KAFKA-8365
URL: https://github.com/apache/kafka/pull/6775
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
To add ACL with SSL authentication,KAFKA-8160,13224173,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Information Provided,,suseemani@gmail.com,suseemani@gmail.com,27/Mar/19 00:46,22/Apr/19 15:57,12/Jan/21 10:06,27/Mar/19 10:20,1.1.0,,,,,,,,,,consumer,producer ,,,,,0,,,,,We want to setup the SSL based authentication along with ACL in place.  Is that doable and can it be added as a feature ? ,,callmkarthik,sliebau,suseemani@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-03-27 10:20:00.692,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 22 15:57:20 UTC 2019,,,,,,,"0|z014ug:",9223372036854775807,,,,,,,,,,,,,,,,"27/Mar/19 00:48;suseemani@gmail.com;We are using SCRAM based authentication, which is not supported by Spark library, because of that we are seeing issues. So want to know if SSL authentication can be set in place, while retaining the acls ","27/Mar/19 10:20;sliebau;Hi [~suseemani@gmail.com]

 

you can absolutely use SSL based authentication with ACLs, please refer to the docs [here|https://kafka.apache.org/documentation/#security_ssl] and [here|https://kafka.apache.org/documentation/#security_authz] for more information.

For your specific question, you will have to use a custom PrincipalBuilder to ensure that principals that are extracted from certificates conform to what you set as username for your SCRAM users. 

 

As this is more of a support request, not a new feature I'll close this ticket, if you have any further questions, please don't hesitate to reach out on the users mailing list!","22/Apr/19 15:57;callmkarthik;Yup Its possible to use SSL with ACLs. I have done it with my Custom Principal Builder to use CN name from the Cert as Principal.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
End-to-End tool to measure performance and correctness test,KAFKA-1579,12732870,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,raulcf,raulcf,raulcf,08/Aug/14 17:02,15/Apr/19 11:44,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,"Producer and Consumer tool that can:
- measure performance (throughput and RTT)
- test for atomic writes (AW) correctness
- test for read isolation/partition (RI/partition) correctness
",,raulcf,sliebau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-04-15 11:44:03.845,,,false,,,,,,,,,,,,,,,,,,410898,,,Mon Apr 15 11:44:03 UTC 2019,,,,,,,"0|i1yoz3:",410891,,,,,,,,,,,,,,,,"15/Apr/19 11:44;sliebau;Hi [~raulcf]

this has been open for a few years now without comments, are you still interested in picking up this piece of work?

Otherwise I'd say we can close this down?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Namespaces to Kafka,KAFKA-2630,12903935,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,singhashish,singhashish,singhashish,10/Oct/15 01:24,01/Apr/19 23:10,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,9,,,,,"Apache Kafka is rapidly finding its place in data heavy organizations as a fault-tolerant message bus. One of the goals of Kafka is data integration, which makes it important to support many users in one Kafka system. With increasing adoption and user community, support for multi-tenancy is becoming a popular demand. There have been a few discussions on Apache Kafka’s mailing lists regarding the same, indicating importance of the feature. Namespaces will allow/ enable many functionalities that require logical grouping of topics. If you think topic as a SQL table, then namespace is a SQL database that lets you group tables together.

[KIP-37|https://cwiki.apache.org/confluence/display/KAFKA/KIP-37+-+Add+Namespaces+to+Kafka] covers the details.",,aarapov,ahmed.ragab,eidi,eronwright,flisky,j3graham,janapati.siva,javenzhang,jimhoagland,KellySchoenhofen,mimaison,mrsrinivas,singhashish,sumek,vahid,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-07-27 16:28:14.921,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 01 23:10:24 UTC 2019,,,,,,,"0|i2mugf:",9223372036854775807,,,,,,,,,,,,,,,,"27/Jul/16 16:28;vahid;[~singhashish] I am wondering what the status of this JIRA and the corresponding KIP is. Thanks.","02/Aug/16 21:15;vahid;Any update [~singhashish]?","08/Aug/16 22:01;singhashish;[~vahid] apologies for the delayed response. This is still on our active roadmap and we intend to work on it. I will provide more information on timing in a week or so, hope that works.","08/Aug/16 22:11;vahid;[~singhashish] Thanks for the update. I look forward to more detailed info when it's ready. Also, I'd be glad to help if needed. ","01/Apr/19 23:10;janapati.siva;[~singhashish] we are looking for name space feature for Kafka. Do you have update on this feature?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Overloaded StreamsBuilder Build Method to Accept java.util.Properties,KAFKA-7027,13165038,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bbejeck,bbejeck,bbejeck,08/Jun/18 22:09,15/Mar/19 16:08,12/Jan/21 10:06,10/Sep/18 23:10,,,,,,,,2.1.0,,,streams,,,,,,0,kip,,,,"Add overloaded method to {{StreamsBuilder}} accepting a {{java.utils.Properties}} instance.

 

KIP can be found here https://cwiki.apache.org/confluence/display/KAFKA/KIP-312%3A+Add+Overloaded+StreamsBuilder+Build+Method+to+Accept+java.util.Properties",,bbejeck,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-07-31 15:17:57.392,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 15 15:53:38 UTC 2019,,,,,,,"0|i3uohb:",9223372036854775807,,,,,,,,,,,,,,,,"31/Jul/18 15:17;githubbot;bbejeck opened a new pull request #5437: KAFKA-7027: Add overloaded build method to StreamsBuilder
URL: https://github.com/apache/kafka/pull/5437
 
 
   Implementation of [KIP-312](https://cwiki.apache.org/confluence/display/KAFKA/KIP-312%3A+Add+Overloaded+StreamsBuilder+Build+Method+to+Accept+java.util.Properties) required for enabling the use of the optimization framework.  
   
   This PR is required to allow for a 4th PR following on from https://github.com/apache/kafka/pull/5201 to enable optimizations.
   
   The existing streams tests were used for testing.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","31/Jul/18 16:24;githubbot;guozhangwang closed pull request #5437: KAFKA-7027: Add overloaded build method to StreamsBuilder
URL: https://github.com/apache/kafka/pull/5437
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsBuilder.java b/streams/src/main/java/org/apache/kafka/streams/StreamsBuilder.java
index ae6d44c449e..ba74f61973b 100644
--- a/streams/src/main/java/org/apache/kafka/streams/StreamsBuilder.java
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsBuilder.java
@@ -41,6 +41,7 @@
 import java.util.Collection;
 import java.util.Collections;
 import java.util.Objects;
+import java.util.Properties;
 import java.util.regex.Pattern;
 
 /**
@@ -513,10 +514,23 @@ public synchronized StreamsBuilder addGlobalStore(final StoreBuilder storeBuilde
 
     /**
      * Returns the {@link Topology} that represents the specified processing logic.
+     * Note that using this method means no optimizations are performed.
      *
      * @return the {@link Topology} that represents the specified processing logic
      */
     public synchronized Topology build() {
+        return build(null);
+    }
+
+    /**
+     * Returns the {@link Topology} that represents the specified processing logic and accepts
+     * a {@link Properties} instance used to indicate whether to optimize topology or not.
+     *
+     * @param props the {@link Properties} used for building possibly optimized topology
+     * @return the {@link Topology} that represents the specified processing logic
+     */
+    public synchronized Topology build(final Properties props) {
+        // the props instance will be used once optimization framework merged
         return topology;
     }
 }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","15/Mar/19 14:56;githubbot;bbejeck commented on pull request #6373: KAFKA-7027: Add an overload build method in scala
URL: https://github.com/apache/kafka/pull/6373
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","15/Mar/19 15:53;bbejeck;cherry-picked [https://github.com/apache/kafka/pull/6373] to 2.2 and 2.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Returned authorized operations in describe responses (KIP-430),KAFKA-7922,13215301,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,omkreddy,rsivaram,rsivaram,12/Feb/19 18:42,10/Mar/19 15:38,12/Jan/21 10:06,10/Mar/19 15:38,,,,,,,,2.3.0,,,core,,,,,,0,,,,,"Add an option to request authorized operations on resources when describing resources (topics, onsumer groups and cluster).

See https://cwiki.apache.org/confluence/display/KAFKA/KIP-430+-+Return+Authorized+Operations+in+Describe+Responses for details.",,githubbot,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-02-25 15:31:59.09,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 01 15:56:02 UTC 2019,,,,,,,"0|yi0woo:",9223372036854775807,,,,,,,,,,,,,,,,"25/Feb/19 15:31;githubbot;omkreddy commented on pull request #6322: KAFKA-7922: Return authorized operations in describe consumer group responses (KIP-430) [WIP]
URL: https://github.com/apache/kafka/pull/6322
 
 
   -  Use automatic RPC generation in DescribeGroups Request/Response classes
   -  https://cwiki.apache.org/confluence/display/KAFKA/KIP-430+-+Return+Authorized+Operations+in+Describe+Responses
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","01/Mar/19 11:38;githubbot;omkreddy commented on pull request #6352: KAFKA-7922: Return authorized operations in metedata request response (KIP-430 Part-2)
URL: https://github.com/apache/kafka/pull/6352
 
 
   -  Use automatic RPC generation in Meta Request/Response classes
   -  https://cwiki.apache.org/confluence/display/KAFKA/KIP-430+-+Return+Authorized+Operations+in+Describe+Responses
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","01/Mar/19 15:56;githubbot;omkreddy commented on pull request #6322: KAFKA-7922: Return authorized operations in describe consumer group responses (KIP-430 Part-1)
URL: https://github.com/apache/kafka/pull/6322
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specify default partitions and replication factor for regex based topics in kafka,KAFKA-8071,13220456,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,xabhi,xabhi,08/Mar/19 12:51,08/Mar/19 12:51,12/Jan/21 10:06,,2.1.1,,,,,,,,,,controller,,,,,,0,,,,,"Is it possible to specify different default partitions and replication factor for topics of type foo.*? If not what is the best way to achieve this from producer point of view?

I know of KafkaAdmin utils but the topic creation will happen on producer and I don't want to give admin permissions on metadata stored in zookeeper to the user running the producer for security reasons.",,xabhi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-03-08 12:51:04.0,,,,,,,"0|z00hyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streams does not have an in-memory windowed store,KAFKA-4730,13040094,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ableegoldman,enothereska,enothereska,03/Feb/17 10:19,02/Mar/19 01:25,12/Jan/21 10:06,21/Feb/19 03:10,0.10.2.0,,,,,,,2.3.0,,,streams,,,,,,2,kip,,,,"Streams has windowed persistent stores (e.g., see PersistentKeyValueFactory interface with ""windowed"" method), however it does not allow for windowed in-memory stores (e.g., see InMemoryKeyValueFactory interface).

In addition to the interface not allowing it, streams does not actually have an implementation of an in-memory windowed store.

The implications are that operations that require windowing cannot use in-memory stores.

KIP-428: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-428%3A+Add+in-memory+window+store]

 ",,astubbs,chogue,damianguy,enothereska,githubbot,guozhang,joscas,mjsax,mnaweb,nthean,roufique07,scosenza,soniclavier,Teabeault,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6910,KAFKA-4729,KAFKA-7952,,,,,,,,,,,,,,,KAFKA-8029,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-04-05 00:12:59.755,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 21 03:09:55 UTC 2019,,,,,,,"0|i39knb:",9223372036854775807,,,,,,,,,,,,,,,,"05/Apr/17 00:12;nthean;Hi [~enothereska], I'm interested in this ticket. What kind of data structure were you thinking of / would you suggest for the implementation of in-memory windowed stores? Composite keys for the Map the general implementation uses, separate key-value stores for each time segment, or something else?","06/Apr/17 17:17;enothereska;[~nthean] we'd love your help with this. It might be worth re-cycling the existing in-memory store, which probably means going with separate key-value stores for each time segment. A bit like how the persistent window store recycles the RocksDb basic store. 

This might require a KIP where we discuss the design first (https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals). We can guide you on that if you are interested. How does this sound? 

I've tentatively assigned this JIRA to you :)","14/Nov/17 02:53;mjsax;Is this covered by KAFKA-5651 ? \cc [~guozhang] [~damianguy]","14/Nov/17 15:49;damianguy;[~mjsax] no we still don't have an in-memory window store","14/Nov/17 16:54;guozhang;I do not think so. Plus we do not have an in-memory session store as well.","07/Aug/18 18:30;mnaweb;Hi Guys! Is there any precision for conclusion? Tks :)","07/Aug/18 20:04;mjsax;You mean on the used data structure? Not really, but we are open to suggestions. I think a segmented implementation does make sense. \cc [~vvcephei]","08/Feb/19 01:31;githubbot;ableegoldman commented on pull request #6239: KAFKA-4730: Streams does not have an in-memory windowed store
URL: https://github.com/apache/kafka/pull/6239
 
 
   Implemented an in-memory window store allowing for range queries. A finite retention period defines how long records will be kept, ie the window of time for fetching, and the grace period defines the window within which late-arriving data may still be written to the store.
   
   Unit tests were written to test the functionality of the window store, including its insert/update/delete and fetch operations. Single-record, all records, and range fetch were tested, for both time ranges and key ranges. The logging and metrics for late-arriving (dropped)records were tested as well as the ability to restore from a changelog. 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","21/Feb/19 03:09;githubbot;guozhangwang commented on pull request #6239: KAFKA-4730: Streams does not have an in-memory windowed store
URL: https://github.com/apache/kafka/pull/6239
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-182: Reduce Streams DSL overloads and allow easier use of custom storage engines,KAFKA-5651,13090229,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,damianguy,damianguy,damianguy,26/Jul/17 15:20,01/Mar/19 22:41,12/Jan/21 10:06,04/Oct/17 04:37,,,,,,,,1.0.0,,,streams,,,,,,0,,,,,,,andrey.dyachkov@gmail.com,ckamal,damianguy,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-4281,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-18 07:39:45.221,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 06 22:40:05 UTC 2017,,,,,,,"0|i3i1hb:",9223372036854775807,,,,,,,,,,,,,,,,"18/Aug/17 07:39;andrey.dyachkov@gmail.com;[~damianguy] could I help here with some of the tickets?","18/Aug/17 09:04;ckamal;[~damianguy] I'm too interested to work on some of the tickets.","07/Sep/17 12:30;damianguy;Sorry [~adyachkov] and [~ckamal] - i didn't see these comments earlier. Anyway, i have a plan for this and it involves multiple steps that are much easier done by myself. ","01/Oct/17 21:32;mjsax;[~damianguy] Can this be closed?","06/Oct/17 22:40;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/4009
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaStreams (and possibly others) should inherit Closeable,KAFKA-4721,13039577,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,,stevenschlansker,stevenschlansker,01/Feb/17 20:16,01/Mar/19 22:37,12/Jan/21 10:06,01/Mar/19 22:37,0.10.1.1,,,,,,,,,,streams,,,,,,0,needs-kip,,,,"KafkaStreams should inherit AutoCloseable or Closeable so that you can use try-with-resources:

{code}
try (KafkaStreams reader = storage.createStreams(builder)) {
    reader.start();
    stopCondition.join();
}
{code}
",,mjsax,stevenschlansker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-02-01 20:16:28.0,,,,,,,"0|i39hgf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mmap indexes lazily and skip sanity check for segments below recovery point,KAFKA-7283,13178687,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,hzxa21,hzxa21,hzxa21,13/Aug/18 19:56,21/Feb/19 05:52,12/Jan/21 10:06,21/Feb/19 05:52,,,,,,,,2.3.0,,,,,,,,,0,,,,,"This is a follow-up ticket for KIP-263.

Currently broker will mmap the index files, read the length as well as the last entry of the file, and sanity check index files of all log segments in the log directory after the broker is started. These operations can be slow because broker needs to open index file and read data into page cache. In this case, the time to restart a broker will increase proportional to the number of segments in the log directory.

Per the KIP discussion, we think we can skip sanity check for segments below the recovery point since Kafka does not provide guarantee for segments already flushed to disk and sanity checking only index file benefits little when the segment is also corrupted because of disk failure. Therefore, we can make the following changes to improve broker startup time:
 # Mmap the index file and populate fields of the index file on-demand rather than performing costly disk operations when creating the index object on broker startup.
 # Skip sanity checks on indexes of segments below the recovery point.

With these changes, the broker startup time will increase only proportional to the number of partitions in the log directly after cleaned shutdown because only active segments are mmaped and sanity checked.

 

 ",,doubleWei,githubbot,hzxa21,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-08-13 20:10:41.723,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 21 05:52:18 UTC 2019,,,,,,,"0|i3x03b:",9223372036854775807,,,,,,,,,,,,,,,,"13/Aug/18 20:10;githubbot;hzxa21 opened a new pull request #5498: KAFKA-7283: Enable lazy mmap on index files and skip sanity check for segments below recovery point
URL: https://github.com/apache/kafka/pull/5498
 
 
   Per the KIP-263 discussion, we think we can improve broker restart time by avoiding performing costly disk operations when sanity checking index files for segments below recovery point on broker startup.
   
   This PR includes the following changes:
   1. Mmap the index file and populate fields of the index file on-demand rather than performing costly disk operations when creating the index object on broker startup.
   2. Skip sanity checks on indexes of segments below the recovery point.
   
   We did experiments on a cluster with 15 brokers, each of which has ~3k segments (and there are 31.8k partitions with RF=3 which are evenly distributed across brokers; total bytes-in-rate is around 400 MBps). The results show that rolling bounce time reduces from 135 minutes to 55 minutes.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","21/Feb/19 05:50;githubbot;junrao commented on pull request #5498: KAFKA-7283: Enable lazy mmap on index files and skip sanity check for segments below recovery point
URL: https://github.com/apache/kafka/pull/5498
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","21/Feb/19 05:52;junrao;Merged the PR to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Idempotent/transactional Producer part 2 (KIP-98),KAFKA-5527,13082834,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,ijuma,ijuma,27/Jun/17 14:17,17/Feb/19 19:02,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,"KAFKA-4815 tracks the items that were included in 0.11.0.0. This JIRA is for tracking the ones that did not make it. Setting ""Fix version"" to 0.11.1.0, but that is subject to change.",,ijuma,lambdaliu,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-02-17 19:02:21.912,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 17 19:02:21 UTC 2019,,,,,,,"0|i3gsdj:",9223372036854775807,,,,,,,,,,,,,,,,"17/Feb/19 19:02;mjsax;Moving all major/minor/trivial tickets that are not merged yet out of 2.2 release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make the idempotent producer the default producer setting,KAFKA-5795,13097746,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,apurva,apurva,apurva,26/Aug/17 03:01,17/Feb/19 18:57,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,We would like to turn on idempotence by default. The KIP is here: https://cwiki.apache.org/confluence/display/KAFKA/KIP-185%3A+Make+exactly+once+in+order+delivery+per+partition+the+default+producer+setting,,apurva,astubbs,michal.klempa,mjsax,samuel.tatipamula,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-02-17 18:57:53.531,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 17 18:57:53 UTC 2019,,,,,,,"0|i3jbbb:",9223372036854775807,,,,,,,,,,,,,,,,"17/Feb/19 18:57;mjsax;Moving all major/minor/trivial tickets that are not merged yet out of 2.2 release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Command line tool to invalidate group metadata for clean assignment,KAFKA-7899,13214038,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Invalid,bchen225242,bchen225242,bchen225242,05/Feb/19 23:29,06/Feb/19 02:21,12/Jan/21 10:06,06/Feb/19 02:21,1.1.0,,,,,,,,,,consumer,streams,,,,,0,,,,,"Right now the group metadata will affect consumers under sticky assignment, since it persists previous topic partition assignment which affects the judgement of consumer leader. Specifically for KStream applications (under 1.1), if we are scaling up the cluster, it is hard to balance the traffic since most tasks would still go to ""previous round active"" assignments, even though we hope them to move towards other hosts.

It would be preferable to have a tool to invalidate the group metadata stored on broker, so that for sticky assignment we could have a clean start.",,bchen225242,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 06 02:21:00 UTC 2019,,,,,,,"0|yi0ox4:",9223372036854775807,,,,,,,,,,,,,,,,"06/Feb/19 02:21;bchen225242;It appears that the metadata should be generated by the client instead of broker. So basically this is invalid issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support request pipelining in the network server,KAFKA-659,12619158,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jkreps,jkreps,06/Dec/12 17:10,04/Feb/19 10:39,12/Jan/21 10:06,04/Feb/19 10:39,,,,,,,,,,,,,,,,,0,,,,,"Currently the network layer in kafka will only process a single request at a time from a given connection. The protocol is designed to allow pipelining of requests which would improve latency.

There are two changes that would have to made for this to work, in my understanding:
1. Currently once a completed request is read from a socket the server does not register for ""read interest"" again until a response is sent. The server would have to register for read interest immediately to allow reading more requests.
2. Currently the socket server adds all requests to a single ""request channel"" that serves as a work queue for all the background i/o threads. One requirement for Kafka is to do in order processing of requests from a given socket. This is currently achieved by not reading any new requests from a socket until the currently outstanding request is processed. To maintain this guarantee we would have to guarantee that all requests from a particular socket went to the same I/O thread. A simple way to do this would be to have work queue per I/O thread. One downside of this is that pinning requests to I/O threads will add latency variance--if that thread stalls due to a slow I/O no other thread can pick up the slack. So perhaps there is a better way that isn't overly complex?

Would be good to nail down the design for this as a first step.",,jkreps,kawamuray,sliebau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-01-28 09:37:35.552,,,false,,,,,,,,,,,,,,,,,,296353,,,Mon Feb 04 10:39:36 UTC 2019,,,,,,,"0|i148qn:",232879,,,,,,,,,,,,,,,,"28/Jan/19 09:37;sliebau;I think this issue has been resolved by now, I cannot find a specific commit or Jira to quote, but my understanding of the broker internals is, that we now have network threads that constantly feed requests into a request queue which is processed by io threads - which as far as I understand this ticket should satisfy what was intended here?","04/Feb/19 10:39;sliebau;I'll close this for now since no-one objected here or on the mailing list, so I'll assume my understanding of this being fixed by now is correct.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow multiple concurrent transactions on a single producer,KAFKA-6278,13121171,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,gfxmonk,gfxmonk,28/Nov/17 03:44,30/Jan/19 21:10,12/Jan/21 10:06,,,,,,,,,,,,producer ,,,,,,2,,,,,"It's recommended to share a producer between threads, because it's likely faster / cheaper.

However with the transactional API there's a big caveat. If you're using transactions, every message sent to a given producer instance will be considered part of the ""active transaction"" regardless of what thread it came from. Furthermore, if two threads want to use transactions on a shared producer instance, it (probably) won't work.

Possible fix: add an API which exposes the transaction ID to the user, instead of making it internal state of the producer. e.g.:

{noformat}
Transaction tx = producer.beginTransaction()
producer.send(tx, message)
producer.commitTransaction(tx)
{noformat}

That way, it's explicit which transaction a message will be part of, rather than the current state which is ""the open transaction, which may have been opened by an unrelated thread"".

See also initial discussion on slack: https://confluentcommunity.slack.com/archives/C488525JT/p1511739734000012",,asksol,gfxmonk,hachikuji,tdpero0516,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-11-28 22:31:22.172,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 30 21:10:45 UTC 2019,,,,,,,"0|i3n9dj:",9223372036854775807,,,,,,,,,,,,,,,,"28/Nov/17 22:31;hachikuji;Thanks for the report. This was an unfortunate tradeoff in the design of the transactional producer, but it simplified the API and the implementation considerably. One of the main benefits to sharing the producer instances is potentially better batching. However, the message batch format does not support messages from separate transactions, so we apparently lose some of that benefit even if the API supported it. There is still some potential benefit in sharing memory and network sockets. We have toyed with the idea of having a {{ProducerPool}} which could be used to get {{KafkaProducer}} instances which share the same underlying memory pool and network resources. That way, we wouldn't need to make any changes to the existing API.","30/Jan/19 21:10;asksol;@Jason can you use multiple transactional ids over the same network socket?  I tried sending multiple InitProducerIDRequests on the same socket and it doesn't complain, but would this be safe to do?

 

I'm trying to implement exactly-once in our kafka streams port, and reading the Kafka Streams source code it seems they are actually using one producer per TaskId+partition?! That would mean you could end up having hundreds of producers, not just one per thread, so now investigating using multiple transactional ids in the same producer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
API Method on Kafka Streams for processing chunks/batches of data,KAFKA-7432,13186795,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,sams,sams,22/Sep/18 10:35,22/Jan/19 21:33,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,14,,,,,"For many situations in Big Data it is preferable to work with a small buffer of records at a go, rather than one record at a time.

The natural example is calling some external API that supports batching for efficiency.

How can we do this in Kafka Streams? I cannot find anything in the API that looks like what I want.

So far I have:

{{builder.stream[String, String](""my-input-topic"") .mapValues(externalApiCall).to(""my-output-topic"")}}

What I want is:

{{builder.stream[String, String](""my-input-topic"") .batched(chunkSize = 2000).map(externalBatchedApiCall).to(""my-output-topic"")}}

In Scala and Akka Streams the function is called {{grouped}} or {{batch}}. In Spark Structured Streaming we can do {{mapPartitions.map(_.grouped(2000).map(externalBatchedApiCall))}}.

 

 

https://stackoverflow.com/questions/52366623/how-to-process-data-in-chunks-batches-with-kafka-streams",,ableegoldman,aklochkov,astubbs,baodepei1988,birdayz,cadonna,ckillmar,dimakrivolap,dmt,itzg,jrray,mathieude,mccraigmccraig,mjsax,raphaelauv,rrai,sams,savulchik,sayuan,serpinar,vvcephei,Yohan123,yujhe.li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-09-23 21:26:56.442,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 22 21:33:33 UTC 2019,,,,,,,"0|i3ydqv:",9223372036854775807,,,,,,,,,,,,,,,,"23/Sep/18 21:26;mjsax;This sounds similar to KAFKA-6989 – can you elaborate what the difference is between both ticket? If there is none, we could close this one as duplicate and track the request via KAFKA-6989.","24/Sep/18 07:08;sams;[~mjsax] I see the similarity in use case, but KAFKA-6988 seems to be requesting something like this:
{code:java}
builder.stream[String, String](""in"").async(numThreads = 10).map(someCallThatIsSlowButNotCPUIntensive).to(""out""){code}
but what we have is an external call that expects a batch of records, e.g. a signature like this:
{code:java}
def externalBatchedApiCall(it: List[A]): List[B]{code}","24/Sep/18 14:48;vvcephei;Hi [~sams],

This sounds like a valuable optimization. 

I agree that it's not the same request, but it seems like we'll get the best final result by considering this and KAFKA-6989 together in one design. Would it be ok with you to move this request into KAFKA-6989 for that reason ? Then, we could be sure that any KIP coming out would consider both batching and async.

 

In case you're looking for an immediate solution, I'd recommend using the Processor API to define a custom Processor that uses a state store to save up records until you get the desired number, and then `context.forward` them as a list. After the map, you could have a reciprocal processor to turn the list back into individual records. I think the implementation of your feature would look something like this in practice, so you could also contribute valuable experience to the KIP discussion.

Thanks,

-John","25/Sep/18 02:33;mjsax;I tend to agree with John, that batching async calls (if possible), would be an implementation details (maybe exposes with some configs, if necessary). At least, we should consider this in the design discussion – if it does not fit we can still work on two independent features.","25/Sep/18 10:36;sams;[~mjsax] [~vvcephei] OK let's merge them if you think that's a good idea.  I guess they both come under ""performance improvements"".

As long as it's clear that the intent here is to be able to batch data since an *external API accepts batches of data*, and that would be more efficient than calling the API with a list of single record.  Whether or not this happens async or not, was not my intention.  But yes, if this could happen async also, that would be cool","25/Sep/18 20:44;mjsax;As you point out, that you don't care about sync vs async, it might be valid to keep both tickets separated but link them only as related. Also note, that you can build sync batch calls manually: you would use a `transform()` with a state store: for each input record, you put it into the state store. If the store has enough records accumulated, you get all records from the store, and do a sync call to the external system will all records. Afterwards, you forward whatever data you want to forward, and finally, delete all records from the store.","17/Dec/18 01:56;Yohan123;Hi, just want to point out something here.

What Kafka currently supports is continuous processing, which Spark Streaming most recently implemented. In contrast, what this ticket is suggesting to implement is microbatch processing in which data is sent in batches.  In some data streaming circles, continuous processing is considered the best option for sending data. Microbatching was an older technique. 

I don't know if we need to implement this particular option, especially since latency overall for microbatching is higher than continuous processing.

Spark is moving from microbatch processing to continuous largely because of latency improvements. So with what Kafka has right now, this ticket probably wouldn't be necessary.","17/Dec/18 10:31;sams;[~Yohan123] This ticket is not really for ""microbatching"", perhaps you could call it ""nanobatching"" since unlike spark these batches are meant to be very small.

Kafka itself is technically always nanobatching anyway, since you do not want to ack messages one by one - as that is very inefficient.  Typically when you use the lower level KafkaConsumer API, you will process very small batches of data.  I would hazard a guess (without reading the code) that this is also how Kafka Streams is implemented.","22/Jan/19 21:33;baodepei1988;A real-world scenario to back this use case: 

An event logging topic has 60 partitions. One of many downstream consumers is a data materialization service. say, it consumes the event message and make RPC calls to several external data sources to fetch signals related to this event in parallel, and then write to a downstream sink (data store). The processing time for this message is then about 4ms, i.e., 250 message per second. That's a total of 15k msg per second. The producing rate is way higher than this rate, which causes huge delays.

One solution is to repartition the upstream topic. But in a corporate scenario, there are too many dependencies, not so easy. Micro-batch could be better. we process a batch of events and make RPC call once for these events.

We can definitely handle the ""micro-batch"" at the application level. But better to have this API in the Kafka streams API.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ability to print all internal topic names,KAFKA-6964,13162596,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,jadireddi,bbejeck,bbejeck,29/May/18 14:02,21/Jan/19 00:21,12/Jan/21 10:06,20/Jan/19 08:13,,,,,,,,,,,streams,,,,,,0,needs-kip,,,,"For security access reasons some streams users need to build all internal topics before deploying their streams application.  While it's possible to get all internal topic names from the {{Topology#describe()}} method, it would be nice to have a separate method that prints out only the internal topic names to ease the process.

I think this change will require a KIP, so I've added the appropriate label.",,bbejeck,guozhang,jadireddi,mjsax,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-06-11 11:50:27.373,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 21 00:21:58 UTC 2019,,,,,,,"0|i3u9fj:",9223372036854775807,,,,,,,,,,,,,,,,"11/Jun/18 11:50;jadireddi;Hi [~bbejeck],
Just trying to make myself clear. If we want to print internal topic names, can we use  `InternalTopologyBuilder#getSourceTopicNames()`  to get all topic names?","25/Jun/18 00:38;mjsax;[~jadireddi], `getSourceTopicNames()` as the name suggest return ""source topic"" – however, internal topics are repartition topic and changelog topic. Thus, repartition topics will be contained in `getSourceTopicNames()` but not contain changelog topic. Also, user topics must be filtered out.","26/Jun/18 14:34;jadireddi;[~mjsax],  After revisiting code, got 2 points in mind. Can you please help me in understanding, which one is valid.

1) As described in the ticket , we need to print only ""internal topic"". Does this mean we need to expose InternalTopologyBuilder#internalTopicNames Set,  that got added through `InternalTopologyBuilder#addInternalTopic` 
 2) As mentioned in your comment, repartition topic and changelog topic constitutes internal topics. Can we call `InternalTopologyBuilder#topicGroups` and read  `InternalTopicConfig#name` field from both repartitionTopics, stateChangelogTopics and print them.","26/Jun/18 15:20;bbejeck;[~jadireddi]  I was thinking along the lines of printing the contents of {{InternalTopologyBuilder#internalTopicNames}} but **having it exposed on the {{KafkaStreams}} object something like {{KafkaStreams#printInternalTopicNames()}}. 

But before you start working on this Jira, with KIP-290 (ACL prefixes) will be in the 2.0 release meaning users can now grant ACLs allowing to create topics prefixed with the application-id (my-application-id*) of a Kafka Streams application, reducing the need for this feature.

 

HTH,

Bill","26/Jun/18 17:52;jadireddi;Got it [~bbejeck]... Should we skip this feature for now and close?.","20/Jan/19 08:13;guozhang;I think we can close this ticket for now.","20/Jan/19 08:19;wushujames;[~guozhang], is the reason you are closing it as WONTFIX because of [~bbejeck]'s comment 26/Jun/18? Meaning, the primary reason to know the internal topic names was to be able to know what to add to an ACL, and the ACLs now support prefix matching, and the application writer knows the prefix, and therefore there is no need to expose the entire topic name?","21/Jan/19 00:21;guozhang;This is part of the reason. The more larger scoped context is:

1. We are allowing users to indicate processor names / state store names for stateful operations via controlled object and KIP-307. Hence for users who would like to have customized internal names for smooth upgrade etc, they can do that already. Much of the related discussions happened on that ticket.

2. As for security, ACLs will be able to be set via kafka-acls command lines which will translate into internal topics etc.

With these two in mind, I think we can close this JIRA now to not expose internal topics, but tackle it in the reversed ordering: allow users to customize their internal topics via public APIs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add findSessions functionality to ReadOnlySessionStore,KAFKA-7622,13198102,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,satish.duggana,xmar,xmar,13/Nov/18 17:02,20/Jan/19 08:17,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,needs-kip,user-experience,,,"When creating a session store from the DSL, and you get a {{ReadOnlySessionStore}}, you can fetch by key, but not by key and time as in a {{SessionStore}}, even if the key type is a {{Windowed<K>}}. So you would have to iterate through it to find the time-related entries, which should be less efficient than querying by time.

So the purpose of this ticket is to be able to query the store with (key, time).

Proposal is to add {{SessionStore's findSessions}}-like methods (i.e. time-bound access) to {{ReadOnlySessionStore.}}
  ",,bbejeck,guozhang,mjsax,xmar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-11-13 17:31:18.66,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 20 08:17:18 UTC 2019,,,,,,,"0|s00fyg:",9223372036854775807,,,,,,,,,,,,,,,,"13/Nov/18 17:31;bbejeck;Hi [~xmar], this seems like a useful feature. 

Please note that since the proposal involves changing a public interface a KIP ([https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals)] is required first. 

If you'd like to do the KIP yourself and would like some help with it, don't hesitate to ask on the dev mailing list, or you can ask here initially as well.

Thanks,

Bill","20/Jan/19 08:17;guozhang;I think the currently discussed KIP-420 is related to this request. cc [~xmar]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve wall-clock time punctuations,KAFKA-7699,13202063,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,mjsax,mjsax,03/Dec/18 18:13,16/Jan/19 19:43,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,needs-kip,,,,"Currently, wall-clock time punctuation allow to schedule periodic call backs based on wall-clock time progress. The punctuation time starts, when the punctuation is scheduled, thus, it's non-deterministic what is desired for many use cases (I want a call-back in 5 minutes from ""now"").

It would be a nice improvement, to allow users to ""anchor"" wall-clock punctation, too, similar to a cron job: Thus, a punctuation would be triggered at ""fixed"" times like the beginning of the next hour, independent when the punctuation was registered.",,ableegoldman,mjsax,shung,ueisele,vicentini,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-12-04 15:35:38.304,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 16 19:43:04 UTC 2019,,,,,,,"0|s01494:",9223372036854775807,,,,,,,,,,,,,,,,"04/Dec/18 15:35;vvcephei;Interesting thought! For anchored schedules, would you want to epoch-align it like the window boundaries, or actually pull in a cron-like scheduling library?

I've used Quartz before for scheduling ([http://www.quartz-scheduler.org/)], which is Apache 2.0 - licensed, so we could consider shading it into Kafka Streams.","04/Dec/18 19:28;mjsax;{quote}would you want to epoch-align it like the window boundaries, or actually pull in a cron-like scheduling library?
{quote}
I guess this would be part of the KIP discussion. Not a clear idea atm. Just created the ticket for tracking :)","07/Jan/19 21:16;shung;Hi, just curious (I am not an expert in this), why won't _ScheduledExecutorService_ works in this case? thanks!","08/Jan/19 17:58;mjsax;We want to provide internal scheduling that allows you to emit data to downstream operators and to access the local state of an operator. This is not possible with Java's `ScheduledExecutorService` because it's ""external"" to an operator.","16/Jan/19 19:43;shung;[~mjsax] Oh I see now I have more context. Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add a delay for further CG rebalances, beyond KIP-134 group.initial.rebalance.delay.ms",KAFKA-7725,13203959,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,astubbs,astubbs,12/Dec/18 10:44,03/Jan/19 21:52,12/Jan/21 10:06,,2.1.0,,,,,,,,,,clients,consumer,core,,,,0,,,,,"KIP-134 group.initial.rebalance.delay.ms was a good start, but there are much bigger problems where after a system is up and running, consumers can leave and join in large amounts, causing rebalance storms. One example is Mesosphere deploying new versions of an app - say there are 10 instances, then 10 more instances are deployed with the new version, then the old 10 are scaled down. Ideally this would be 1 or 2 rebalances, instead of 20.

The trade off is that if the delay is 5 seconds, every consumer joining within that window would extend it by another 5 seconds, potentially causing partitions to never be processed. To mitigate this, either a max rebalance delay could also be added, or multiple consumers joining won't extend the rebalance delay, so that it's always a max of 5 seconds.

Related: [KIP-345: Introduce static membership protocol to reduce consumer rebalances|https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances]
KAFKA-7018: persist memberId for consumer restart",,astubbs,bchen225242,guozhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-12-29 19:27:13.6,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 03 21:52:57 UTC 2019,,,,,,,"0|s01fsg:",9223372036854775807,,,,,,,,,,,,,,,,"29/Dec/18 19:27;bchen225242;Thanks [~astubbs] for sharing this issue. Will add this to the KIP-345 Jira list.","03/Jan/19 21:52;guozhang;[~astubbs] There are some after-thoughts on KIP-134 and the main critic is that this should not be a broker-side global config but rather a per-client thing. I think KIP-345 should be sufficiently fix the cons of KIP-134 and by that time we can safely deprecate this config (also indicated in KIP-345 as well).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for multiple certificates in a single keystore,KAFKA-5519,13082605,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,alla@confluent.io,alla@confluent.io,26/Jun/17 18:47,19/Dec/18 13:16,12/Jan/21 10:06,,0.10.2.1,,,,,,,,,,security,,,,,,3,upstream-issue,,,,"Background
Currently, we need to have a keystore exclusive to the component with exactly one key in it. Looking at the JSSE Reference guide, it seems like we would need to introduce our own KeyManager into the SSLContext which selects a configurable key alias name.
https://docs.oracle.com/javase/7/docs/api/javax/net/ssl/X509KeyManager.html 
has methods for dealing with aliases.
The goal here to use a specific certificate (with proper ACLs set for this client), and not just the first one that matches.
Looks like it requires a code change to the SSLChannelBuilder",,alla@confluent.io,clement_pellerin@ibi.com,LeonardoZV,sebadiaz,tombentley,waewoo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-07-05 08:41:26.915,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 19 13:16:46 UTC 2018,,,,,,,"0|i3gqyv:",9223372036854775807,,,,,,,,,,,,,,,,"05/Jul/17 08:41;tombentley;Can you explain why having a keystore per component is particularly problematic, but using multiple certificates in a keystore and using aliases would make things significantly better?","05/Jul/17 18:05;alla@confluent.io;I wouldn't call it problematic: I just imagine there are situations where multiple J2EE applications may want to use a single keystore and import their client certificates into a single keystore - in order to decrease management overhead by not having to maintain multiple keystores (like managing keystore passwords, for example).","24/May/18 09:35;sebadiaz;I m working for a future production with a centralized monitoring tool.

Mixing different encryption technologes for JMX/RMI/... on a weblogic server. the usage of a single keystore on the same server is not optionnal and by server design.

Please add a config setCertAlias for clients/producer/consumer.

 

 ","19/Dec/18 13:16;clement_pellerin@ibi.com;KIP-383 proposes a solution for KAFKA-6654 which would be a work-around for this Jira.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"JMX metrics from the broker for client connections: how many, what version, what language, source ip etc...",KAFKA-7731,13204248,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,astubbs,astubbs,13/Dec/18 14:19,13/Dec/18 14:21,12/Jan/21 10:06,,2.1.0,,,,,,,,,,core,,,,,,0,,,,,"Extremely useful for diagnosing large installations with many clients, auditing client usage, behaviour etc..",,astubbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-12-13 14:19:44.0,,,,,,,"0|s01hko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client login with already existing JVM subject,KAFKA-7677,13200824,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,gsomogyi,gsomogyi,27/Nov/18 10:46,27/Nov/18 10:52,12/Jan/21 10:06,,2.2.0,,,,,,,,,,,,,,,,0,,,,,"If JVM is already logged in to KDC and has a Subject + TGT in it's security context it can be used by clients and not logging in again. Example code:

{code:java}
org.apache.hadoop.security.UserGroupInformation.getCurrentUser().doAs(
  new java.security.PrivilegedExceptionAction[Unit] { 
    override def run(): Unit = {
    val subject = javax.security.auth.Subject.getSubject(java.security.AccessController.getContext())
    val adminClient = AdminClient.create...
  }
)
{code}
",,gsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-11-27 10:46:01.0,,,,,,,"0|s00wmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaConsumer / KafkaProducer should allow Reconfiguration of SSL Configuration,KAFKA-7205,13174545,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,mjungsbluth,mjungsbluth,25/Jul/18 20:27,22/Nov/18 15:45,12/Jan/21 10:06,,1.1.1,,,,,,,,,,clients,,,,,,0,needs-kip,,,,"Since Kafka 1.1 it is possible to reconfigure KeyStores on the broker side of things. 

When being serious about short lived keys, the client side should also support reconfiguring consumers and producers.

What I would propose is to implement {{Reconfigurable}}  on {{KafkaConsumer}} and {{KafkaProducer}}. The implementation has to pass the calls to NetworkClient which passes them on to Selector until they finally reach {{SslFactory}} which already implements {{Reconfigurable}}.

This seems pretty straightforward unless I am missing something important.  ",,mjungsbluth,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-11-22 15:45:11.093,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 22 15:45:11 UTC 2018,,,,,,,"0|i3wau7:",9223372036854775807,,,,,,,,,,,,,,,,"22/Nov/18 13:20;mjungsbluth;Meanwhile we did the necessary changes (which are pretty small) and verified that it actually works without recreating the Consumer. 

We would like to create a Pull Request to get this into mainline, what should this PR be base on, trunk?","22/Nov/18 15:45;omkreddy;Yes, you can raise PR against trunk.  Since this is a public API change, we need to go through KIP Process. https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-391: Allow Producing with Offsets for Cluster Replication,KAFKA-7666,13199886,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,ecomar,ecomar,ecomar,21/Nov/18 17:08,21/Nov/18 17:20,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,1,,,,,"Implementing KIP-391
https://cwiki.apache.org/confluence/display/KAFKA/KIP-391%3A+Allow+Producing+with+Offsets+for+Cluster+Replication",,ankon,donovanmuller1984,ecomar,githubbot,jnadler,nabilgasri,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-11-21 17:20:26.542,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 21 17:20:26 UTC 2018,,,,,,,"0|s00qvk:",9223372036854775807,,,,,,,,,,,,,,,,"21/Nov/18 17:20;githubbot;edoardocomar opened a new pull request #5937: KAFKA-7666: KIP-391 Producing with Offsets for Cluster Replication
URL: https://github.com/apache/kafka/pull/5937
 
 
   initial commit
   
   Co-authored-by: Edoardo Comar <ecomar@uk.ibm.com>
   Co-authored-by: Mickael Maison <mickael.maison@gmail.com>
   
   first implementation of https://cwiki.apache.org/confluence/display/KAFKA/KIP-391%3A+Allow+Producing+with+Offsets+for+Cluster+Replication
   
   current limitation - a Producer can only be used to send Records with or without offsets, it cannot mix
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Auto generate request/response classes,KAFKA-5255,13072442,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,tombentley,ijuma,ijuma,16/May/17 13:05,09/Nov/18 14:54,12/Jan/21 10:06,09/Nov/18 14:54,,,,,,,,,,,,,,,,,0,,,,,"We should automatically generate the request/response classes from the protocol definition. This is a major source of boilerplate, development effort and inconsistency at the moment. If we auto-generate the classes, we may also be able to avoid the intermediate `Struct` representation.",,ijuma,lindong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-10-02 14:41:18.463,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 09 14:54:10 UTC 2018,,,,,,,"0|i3f1yf:",9223372036854775807,,,,,,,,,,,,,,,,"02/Oct/18 14:41;lindong;Moving this to 2.2.0 since PR is not ready yet.","09/Nov/18 14:54;ijuma;There is a more recent ticket with a PR (KAFKA-7609) so closing as duplicate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a tool to Reset Consumer Group Offsets,KAFKA-4743,13041247,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jeqo,jeqo,jeqo,08/Feb/17 02:32,08/Nov/18 05:00,12/Jan/21 10:06,17/May/17 21:27,,,,,,,,0.11.0.0,,,consumer,core,tools,,,,1,kip,,,,"Add an external tool to reset Consumer Group offsets, and achieve rewind over the topics, without changing client-side code.

https://cwiki.apache.org/confluence/display/KAFKA/KIP-122%3A+Add+Reset+Consumer+Group+Offsets+tooling",,githubbot,gquintana,guozhang,hachikuji,jeffwidman,jeqo,mjsax,stephane.maarek@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-3059,,,,,,,,KAFKA-5181,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-01 19:16:05.248,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 12 17:57:25 UTC 2017,,,,,,,"0|i39rov:",9223372036854775807,,,,,,,,,,,,,,,,"01/Mar/17 19:16;githubbot;GitHub user jeqo opened a pull request:

    https://github.com/apache/kafka/pull/2624

    KAFKA-4743: Add Reset Consumer Group Offsets tooling [KIP-122]

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/jeqo/kafka feature/rewind-consumer-group-offset

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2624.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2624
    
----
commit ef14ccfc99360a389bf6597a7eb92cf8a1076c69
Author: Jorge Quilcate <quilcate.jorge@gmail.com>
Date:   2017-02-17T15:20:59Z

    Add reset-offset option

commit 25b3e758f32375431b56e0d810d7e66a154f8d78
Author: Jorge Quilcate <quilcate.jorge@gmail.com>
Date:   2017-02-17T15:58:18Z

    Add reset-offset print assignments

commit 398bd76580d4c937430a2c5fbee0e27659acc5da
Author: Jorge Quilcate <quilcate.jorge@gmail.com>
Date:   2017-02-20T02:28:12Z

    Draft Reset to specific commit

commit 3aa04620600b71c1d4fc6c6bf37f8cfd95bc6974
Author: Jorge Quilcate <quilcate.jorge@gmail.com>
Date:   2017-02-20T11:48:29Z

    Add plus, minus, earliest, latest cases

commit 01a65569046bba6f2425cb4429484afbcd548f2d
Author: Jorge Quilcate <quilcate.jorge@gmail.com>
Date:   2017-02-20T16:17:22Z

    change period by duration

commit 43fef5a88956fca084b8c43912be145215468daa
Author: Jorge Quilcate <quilcate.jorge@gmail.com>
Date:   2017-02-20T22:07:17Z

    add export/import reset plan

commit 308b2b7623169427c926cf61d77ec3dba1f2626a
Author: Jorge Quilcate <quilcate.jorge@gmail.com>
Date:   2017-02-20T22:10:50Z

    fix print

commit 34376f18078092584028b9c704a07b28c3d41143
Author: Jorge Quilcate <quilcate.jorge@gmail.com>
Date:   2017-02-21T10:45:59Z

    rename options, change json to csv format

commit bafc4e0ebd7e0673e638f39e39a7666735687420
Author: Jorge Quilcate <quilcate.jorge@gmail.com>
Date:   2017-02-21T11:32:28Z

    fix csv generation

commit d1b4588ac4f497a4e3605ad429dc17dd41535468
Author: Jorge Quilcate <quilcate.jorge@gmail.com>
Date:   2017-03-01T17:50:06Z

    add test cases

commit 93badea267b91d33c0ddbba64847dd53ca580f47
Author: Jorge Quilcate <quilcate.jorge@gmail.com>
Date:   2017-03-01T18:23:04Z

    add test cases for duration and datetime

commit 0af432fec9e800f2c207b81b71927b7ec73a7edf
Author: Jorge Quilcate <quilcate.jorge@gmail.com>
Date:   2017-03-01T18:55:44Z

    add test case for export import

commit 8178a4a8612cd86d5516cde9f0f61d5df9971dfb
Author: Jorge Quilcate <quilcate.jorge@gmail.com>
Date:   2017-03-01T19:05:15Z

    fix messages

----
","05/May/17 19:37;guozhang;Created 5181 without being aware of this ticket. I think if we can add a new request protocol to the admin client then this tool could be largely simplified: create an admin client and call its apis.","09/May/17 19:43;jeqo;Hi [~guozhang], I'm using the current operations from Consumer API to seek offsets by position and time, and commit new offsets. 
Do you have an idea on how this new request protocol could look like and how it will simplify the current implementation?","17/May/17 21:27;hachikuji;Issue resolved by pull request 2624
[https://github.com/apache/kafka/pull/2624]","17/May/17 21:28;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2624
","09/Jun/17 16:19;mjsax;[~jeqo] As this is done, are you interested to extend `bin/kafka-streams-application-reset.sh` with a similar feature? Would require a KIP, too (there is also no JIRA yet). The ""problem"" right now is, that user need to use two tools to reset an Streams application and set an arbitrary start offset. Would be nice if a single tool could do this :)","11/Jun/17 19:26;jeqo;I was about to ask you about the same :) I'd be keen to introduce this for Streams applications. 
Let me first take a look into `kafka-streams-application-reset` implementation and get back to you to work on the KIP. ","12/Jun/17 17:57;mjsax;Thanks a lot. Sounds great :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thread safe accumulator across all instances,KAFKA-7470,13188766,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,sams,sams,02/Oct/18 07:28,02/Oct/18 07:32,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,,,,,"Spark has a useful API for accumulating data in a thread safe way [https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.util.AccumulatorV2] and comes with some out-of-box useful accumulators e.g. for Longs [https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.util.LongAccumulator]

I usually use accumulators for wiring in debugging, profiling, monitoring and diagnostics into Spark jobs. I usually fire off a Future before running a Spark job to periodically print the stats (e.g. TPS, histograms, counts, timings, etc)

So far I cannot find anything that is similar for Kafka Streams. Does anything exist? I imagine this is possible at least for each instance of a Kafka app, but to make this work across several instances would require creating an intermediate topic.

 

Of course we want to be able to call this accumulator in a similar way to the Spark Accumulator while preserving guarantees.  Example usage:
{code:java}
val countAccumulator: Accumulator[Long] = ...

Future {
  every(1 minute) {
    logger.info(""Processed "" + countAccumulator.value + "" records"")
  }
}

stream.map(x => {
  countAccumulator.add(1)

  x
}){code}
 

 

 ",,mjsax,sams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-10-02 07:28:18.0,,,,,,,"0|i3ypuv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Streams - API for specifying internal stream name on join,KAFKA-5836,13099789,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,,lpandzic,lpandzic,05/Sep/17 09:49,01/Oct/18 06:39,12/Jan/21 10:06,01/Oct/18 06:39,0.11.0.0,,,,,,,,,,streams,,,,,,0,api,needs-kip,,,"Automatic topic name can be problematic in case of streams operation change/migration.
I'd like to be able to specify name of an internal topic so I can avoid creation of new stream and data ""loss"" when changing the Stream building.",,andy.chambers@fundingcircle.com,astubbs,guozhang,lpandzic,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-10-04 12:01:49.238,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 18 20:21:13 UTC 2017,,,,,,,"0|i3jnc7:",9223372036854775807,,,,,,,,,,,,,,,,"04/Oct/17 12:01;andy.chambers@fundingcircle.com;This problem is compounded by the use of the schema registry since these mutable names are propagated into the immutable schema registry.","04/Oct/17 19:16;guozhang;Which schema registry are you using Andy? Normally if a topic name has changed the producer would be auto-registering the schema again under the new topic, no?","08/Oct/17 17:21;andy.chambers@fundingcircle.com;We're using the confluent schema registry. It is my understanding that if you have schema ""a"" at position ""n"" in the topology, and schema b at position ""n+1"", and then you add a node into the topology before the one using schema ""a"", then KS will try to register a as a new version of subject b.","08/Oct/17 23:48;guozhang;Andy, not sure I understand your situation, regarding ""KS will try to register a as a new version of subject b"", could you elaborate a bit more for my understanding? Thanks in advance!","16/Oct/17 18:26;andy.chambers@fundingcircle.com;Ah sorry. Not KS of course but the confluent AvroSerializer which registers the schema as ""[topic-name]-value"". So to be completely concrete, the example below represents some pseudo-code for building a couple of small topologies. It demonstrates the scenario of wanting to evolve from v1 to v2 of some app. It is assumed that all topics use the confluent schema registry's builtin KafkaAvroSerializer for serialization. And that backwards-compatibility checking is enabled on the schema registry. Topic ""a"" expects messages from schema ""a"", topic ""b"" expects messages from schema ""b"" etc...

In app-v1:
  a = stream-of(a)
  b = stream-of(b)
  result = window-join(a, b) # implicitly creates topics for each of the input topics named
                                           # ""${applicationId}-storeName-changelog"" and because we're
                                           # using avro+schema-registry, we get corresponding subjects
                                           # of the same name in the schema registry. Lets say that in this
                                           # case, we get implicit topics where
                                           #    storeName=window-join-0001-changelog (holds topic a)
                                           #    storeName=window-join-0002-changelog (holds topic b)

In app-v2:
  a = stream-of(a)
  b = stream-of(b)

  # add some unrelated stuff to the topology but since it's a window join, in the same position
  # as the previous window the internal topics are the same but the topics involved in this join
  # are different. So now we have
  #    storeName=window-join-0001-changelog (holds topic c)
  #    storeName=window-join-0002-changelog (holds topic d)
  # When the KafkaAvroSerializer tries to serialize messages destined for these internal topics
  # it will fail because the schema registry is expecting messages adhering to schema a/b but
  # will actually get messages matching schema c/d. The serializer will attempt to register the
  # the ""new"" schemas and fail because they are not ""backward compatible"" with a/b.
  window-join(stream-of(c), stream-of(d))
     .foreach(spam-logs)

  result = window-join(a, b) ","17/Oct/17 01:00;guozhang;Thanks for the detailed scenarios Andy! That helps a lot for my understanding. I understand now that when you changed your application's input topic, the inner topic names remain the same since the topology itself is unchanged.

For this scenario, you'd probably consider resetting the previous application before upgrading or simply starting v2 of your application as a new app (i.e. with a different appId in your configs) since your input topics has changed and hence the previous logged states are not valid and hence cannot be reused any more.

I think the issue that [~lpandzic] brought up here is the opposite: that we changed part of the topology causing its inner topics / processor node names to have changed, whereas we still want to reuse the state from the previous run since users understand that the state is still valid.","17/Oct/17 22:39;andy.chambers@fundingcircle.com;Agree that bumping the application id would resolve the issue by creating completely new internal topics. But v2 isn't really ""changing"" the input topics. It is reading two new topics (c and d) and processing those independently of the original a and b. The required processing on a and b remains the same and the state that has been built up remains valid and might take quite a while to build from scratch if you're using kafka as an event log.","18/Oct/17 20:21;guozhang;[~andy.chambers@fundingcircle.com] Ah I understands now. I think I agree with you that this scenario can falls into this JIRA.

To unblock your use case, could you specify the join operator of topic c and d after the join operator of a and b in your code constructing the topology with builder?. I think by doing this the index of the storeName will be assigned with a larger value to get you around this issue (in the near term).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
implement a global configuration feature for brokers,KAFKA-1786,12756472,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,abiletskyi,joestein,joestein,19/Nov/14 18:01,04/Sep/18 15:34,12/Jan/21 10:06,04/Sep/18 15:34,,,,,,,,,,,,,,,,,0,,,,,"Global level configurations (much like topic level) for brokers are managed by humans and automation systems through server.properties.  

Some configuration make sense to use default (like it is now) or override from central location (zookeeper for now). We can modify this through the new CLI tool so that every broker can have exact same setting.  Some configurations we should allow to be overriden from server.properties (like port) but others we should use the global store as source of truth (e.g. auto topic enable, fetch replica message size, etc). Since most configuration I believe are going to fall into this category we should have the list of server.properties that can override the global config in the code in a list which we can manage... everything else the global takes precedence. ",,abhinavddr@gmail.com,abiletskyi,jholoman,joestein,nehanarkhede,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/15 09:00;abiletskyi;KAFKA_1786.patch;https://issues.apache.org/jira/secure/attachment/12694119/KAFKA_1786.patch",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2014-12-31 15:22:31.637,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 04 15:34:51 UTC 2018,,,,,,,"0|i22khj:",9223372036854775807,,nehanarkhede,,,,,,,,,,,,,,"31/Dec/14 15:22;abiletskyi;Created reviewboard https://reviews.apache.org/r/29513/diff/
 against branch origin/trunk

UPD: discraded for now. Uploaded a patch KAFAK_1786.patch based on KAFKA-1845 (since it's a separate big patch)","09/Jan/15 19:41;nehanarkhede;[~abiletskyi], [~charmalloc]. For large changes like this, we should circulate a proposal through the mailing list to get feedback from the other committers as well as the larger community. I'd prefer to have a detailed design proposal on our wiki and discuss that on the mailing list.","09/Jan/15 21:06;joestein;Hey [~nehanarkhede] I had sent this out on the mailing list a while back as part of the CLI tool changes (parent ticket) http://search-hadoop.com/m/4TaT4CJpj11 also with confluence page too https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Command+Line+and+Related+Improvements","23/Jan/15 09:00;abiletskyi;Patch KAFKA_1786.patch is based on KAFKA-1845 patch (KafkaConfig should use ConfigDef)","23/Feb/16 17:35;granthenke;Moved out of KIP-4 scope","12/Apr/18 13:57;omkreddy;looks like this requirement is covered in Dynamic Broker config feature (KIP-226).","04/Sep/18 15:34;omkreddy;Closing as duplicate of KIP-226. Please reopen if you think otherwise.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KOYA - Kafka on YARN,KAFKA-1754,12752908,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,thw,thw,05/Nov/14 00:29,04/Sep/18 15:15,12/Jan/21 10:06,,,,,,,,,,,,packaging,,,,,,6,,,,,"YARN (Hadoop 2.x) has enabled clusters to be used for a variety of workloads, emerging as distributed operating system for big data applications. Initiatives are on the way to bring long running services under the YARN umbrella, leveraging it for centralized resource management and operations ([YARN-896] and examples such as HBase, Accumulo or Memcached through Slider). This JIRA is to propose KOYA (Kafka On Yarn), a YARN application master to launch and manage Kafka clusters running on YARN. Brokers will use resources allocated through YARN with support for recovery, monitoring etc. Please see attached for more details.",,acmurthy,airbots,anew,ankurmitujjain,anthonyr,aperepel,ashwinchandrap,baltz16,capricornius,caritaou,chenshangan521@163.com,czyzu01,daisuke.kobayashi,diederik,dkaiser,guozhang,gwenshap,huaishk,indoos,jbarotin,Jobo,krisden,leftnoteasy,maheeg,maropu,minjikim,naggarwal,neelesh77,nehanarkhede,nielsbasjes,nikunj,otis,sb58,sdaingade,sharadag,shreyass123,sseth,sslavic,ssubbiah,stevel@apache.org,suda,tbenton,tgraves,thw,tzolov,varun_saxena,vybs,ywskycn,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-949,,,,,,,,,,,,,,,"05/Nov/14 00:31;thw;DT-KOYA-Proposal- JIRA.pdf;https://issues.apache.org/jira/secure/attachment/12679389/DT-KOYA-Proposal-+JIRA.pdf",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2014-11-05 02:57:50.275,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 07 18:48:50 UTC 2016,,,,,,,"0|i21z4n:",9223372036854775807,,,,,,,,,,,,,,,,"05/Nov/14 02:57;guozhang;Thanks for sharing, this is an interesting proposal. One question I have though is that, since YARN is designed for short task (e.g., MR job) resource allocation, is it a good fit for long-running services such as Kafka?","05/Nov/14 03:53;thw;YARN-896 references many of the challenges for long running applications and services and is also indicative that we are on the way to overcome those. For most part, there are ways to deal with those today until direct support becomes available in YARN itself. We at DataTorrent have been working on a YARN native platform for long running applications for ~2.5 years, in fact we aim to support applications that never go down! For long running services, initiatives such as Llama and Slider show how long running applications and services can be supported on YARN as available today. We have a working prototype for KOYA and looking forward to continue the effort in open source. We are looking forward to collaborate with everyone interested to run Kafka on YARN!  ","06/Nov/14 02:01;gwenshap;Looking forward to see the patch.

Streaming applications such as Samza, SparkStreaming and DataTorrents will benefit from running their workers on the same nodes as the partitions they are consuming data from. This is now possible in YARN.

My main concern is whether its possible to prevent YARN from spawning new container when a broker goes down.
Because starting the broker on a new node has huge overhead of replicating all the data over. We may prefer that this will not happen automatically, but only after someone verified that the original broker is truly gone.","06/Nov/14 06:58;acmurthy;Sounds like a great idea, thanks for taking a lead on this [~thw]!

Inspired by this jira, I've opened YARN-2817... I'd like to throw out the idea that Kafka could start by reserving entire drives on nodes, exclusively when running on YARN. This would ensure that Kafka would not get interference from other applications like HDFS, MR etc.

Thoughts?","06/Nov/14 07:01;acmurthy;bq. My main concern is whether its possible to prevent YARN from spawning new container when a broker goes down.

[~gwenshap] - YARN doesn't spawn a new container on its own... the Kafka-AM would get a notification that a particular broker (YARN container) has failed. The Kafka-AM can then decide when/where to request & launch a new broker.","06/Nov/14 07:03;acmurthy;[~thw] - please feel free to reach out to either yarn-dev@ or me personally if you need any help. We'd love to see this supported well. Thanks!","06/Nov/14 11:41;stevel@apache.org;This is great! YARN is getting better and better at long-lived services; Kafka should fit in nicely —and allocation of disks would be a very interesting feature to add here.

# As arun says, your AM gets to choose whether to spawn a new container -and where. You could try to ask for one back on the old machine, though if all data was on in the container's workspace, you've lost it all.
# YARN will restart the AM, and if you ask it at launch time, it will keep the containers up. Your AM has the task of rebuilding all its state. Hadoop 2.6+ adds sliding windows on restarts, so long lived apps aren't considered failures if their AM fails once a fortnight.
# Security is an issue with long-lived apps. Have you tried this on secure clusters yet?","06/Nov/14 16:30;thw;[~stevel@apache.org] Yes, we are running DataTorrent on secure clusters. You are probably thinking about the token max life time issue as addressed in [SLIDER-474]?   ","06/Nov/14 17:00;thw;[~acmurthy] Thanks for the support. Disk reservation will benefit Kafka greatly. Will reach out with a few other questions. 

{quote}
Inspired by this jira, I've opened YARN-2817... I'd like to throw out the idea that Kafka could start by reserving entire drives on nodes, exclusively when running on YARN. This would ensure that Kafka would not get interference from other applications like HDFS, MR etc.
{quote}","07/Nov/14 09:17;stevel@apache.org;Thomas, yes, keytabs &c as addressed in SLIDER-474. There were also changes made to the NM so that they could relaunch an AM even after the launch context tokens expired","10/Nov/14 02:59;nehanarkhede;bq. Streaming applications such as Samza, SparkStreaming and DataTorrents will benefit from running their workers on the same nodes as the partitions they are consuming data from. This is now possible in YARN.

[~gwenshap] We tried to deploy Samza and try to co-locate Kafka partitions on the same box. The main problem was of I/O and memory resource isolation. Page cache issues between stateful jobs (that need to write to the local k/v store) and the Kafka brokers. Plus, Kafka's partitioning style doesn't lend itself to locality (writes go to arbitrary partitions (boxes) based on key, and reads are spread across partitions on many boxes). This issue of resource isolation is not just a problem with something like Samza but will be an issue with running Kafka with any other I/O heavy application on YARN.","10/Nov/14 03:15;gwenshap;[~nehanarkhede] I agree that these are two serious concerns and something we'll have to give more thought to.

1. Resource isolation: [~acmurthy] generously offered to improve YARN's IO isolation, so Kafka's IO can be protected. Page cache is a much more challenging issue. I think (but am not sure) that SparkStreaming does not rely heavily on the page cache, so it may lend itself more readily to co-location. In any case, it remains to be seen whether YARN can help in that regard.

2. Can data locality be achieved with Kafka? 
If we can't get any benefits out of co-location, there's no point in trying to resolve the challenges :) 
Intuitively, I'd think that if we use partition keys, we can know what data is stored in each partition and can make use of that knowledge to optimize Stream processing. Perhaps skip some of the shuffle steps, and do more processing in the Kafka receiver. However, I don't have a specific design in mind for that at the moment. Need to think about it a bit more.

","05/Mar/15 16:52;raviprak;Thanks for the talk yesterday Thomas discussing Kafka on YARN via Slider! Are there still plans to create a new Kafka AM or is Slider the way forward?","05/Mar/15 19:09;thw;Slider is the way forward. More info on this: http://hortonworks.com/blog/koya-apache-slider/","07/Jul/16 18:48;thw;KOYA is now part of Slider:

https://github.com/apache/incubator-slider/tree/develop/app-packages/kafka
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate kafka into YARN,KAFKA-949,12653949,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Resolved,,kkasravi,kkasravi,20/Jun/13 15:04,04/Sep/18 15:15,12/Jan/21 10:06,04/Sep/18 15:15,0.8.0,,,,,,,,,,contrib,,,,,,4,,,,,kafka is being added to bigtop (BIGTOP-989). Having kafka services available under YARN will enable a number of cluster operations for kafka that YARN handles.,hadoop 2-0.X,aperepel,avik_dey@yahoo.com,barkbay,brugidou,capricornius,jimhoagland,kaolian,kkasravi,mauzhang,mkwhitacre,omkreddy,skadambi,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-01-27 17:07:11.974,,,false,,,,,,,,,,,,,,,,,,334226,,,Tue Sep 04 15:15:44 UTC 2018,,,,,,,"0|i1lnlb:",334552,,,,,,,,,,,,,,,,"23/Aug/13 00:44;kkasravi;This work is available under https://github.com/kkasravi/kafka-yarn, I can provide a patch which adds the project to https://github.com/apache/kafka/tree/0.8/contrib/yarn. Kafka-yarn has a dependency on BIGTOP-989 which installs kafka 0.8 beta1 as a service on linux (deb, rpm). Please advise.
Kam","27/Jan/14 17:07;skadambi;Any timelines on when this will be available? I'm very interested in having the brokers managed by YARN.","04/Sep/18 15:15;omkreddy;Resolving as duplicate of KAFKA-1754",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ability for producer to push to topic on remote server via ELB name,KAFKA-7306,13179716,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,toopt4,toopt4,18/Aug/18 01:19,22/Aug/18 18:41,12/Jan/21 10:06,,,,,,,,,,,,config,core,producer ,,,,0,,,,,"One ec2 (hostA) with producer wants to push to a topic on a different ec2 (hostB). hostB has got an ELB (hostC) in front of it. From my experimentation and reading docs online Kafka does not support hostA producer connecting to ELB name (and letting the ELB redirect to the hostB ec2)'s topic.

Other tool servers (Atlas, Hive.etc) allow hostA to reference remote ELB and have the ELB direct the connection to hostB where the tool servers run.",,omkreddy,toopt4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-08-22 18:41:55.325,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 22 18:41:55 UTC 2018,,,,,,,"0|i3x6fb:",9223372036854775807,,,,,,,,,,,,,,,,"22/Aug/18 18:41;omkreddy;check this: https://stackoverflow.com/questions/38666795/does-kafka-support-elb-in-front-of-broker-cluster",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support multiple auto-generated docs formats,KAFKA-7202,13174202,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,joel@confluent.io,joel@confluent.io,24/Jul/18 20:45,20/Aug/18 22:18,12/Jan/21 10:06,,,,,,,,,,,,documentation,,,,,,0,,,,,Currently the configuration parameters for Confluent/Kafka are autogenerated as HTML (and hosted at [https://kafka.apache.org/documentation/#configuration]). This request is to expand this to support other formats (e.g. RST) so that they can be easily leveraged by other authorign language formats.,,joel@confluent.io,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-07-24 20:45:53.0,,,,,,,"0|i3w8pz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add csv reporter in ProducerPerformance for the new producer,KAFKA-1939,12773709,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,junrao,xinyisu,xinyisu,10/Feb/15 06:56,14/Aug/18 12:15,12/Jan/21 10:06,,0.8.2.0,,,,,,,,,,producer ,,,,,,0,,,,,"Currently, new producer only supports a jmx reporter for the metrics. It does not output csv report as old producer does.

We propose to add csv reporter in ProducerPerformance for the new producer by using the new metrics api.",,xinyisu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7289,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-02-10 06:56:13.0,,,,,,,"0|i25eov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Please add OWASP Dependency Check to the build,KAFKA-7246,13176936,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,ABakerIII,ABakerIII,05/Aug/18 04:05,05/Aug/18 04:05,12/Jan/21 10:06,,0.10.2.3,0.11.0.4,0.8.1.2,0.9.0.2,1.1.2,2.1.0,3.0.0,,,,build,,,,,,1,build,easy-fix,security,," Please add OWASP Dependency Check to the build.  OWASP DC makes an outbound REST call to MITRE Common Vulnerabilities & Exposures (CVE) to perform a lookup for each dependant .jar to list any/all known vulnerabilities for each jar.  This step is needed because a manual MITRE CVE lookup/check on the main component does not include checking for vulnerabilities in components or in dependant libraries.

OWASP Dependency check : https://www.owasp.org/index.php/OWASP_Dependency_Check has plug-ins for most Java build/make types (ant, maven, ivy, gradle).   

Also, add the appropriate command to the nightly build to generate a report of all known vulnerabilities in any/all third party libraries/dependencies that get pulled in. example : mvn -Powasp -Dtest=false -DfailIfNoTests=false clean aggregate

Generating this report nightly/weekly will help inform the project's development team if any dependant libraries have a reported known vulnerailities.  Project teams that keep up with removing vulnerabilities on a weekly basis will help protect businesses that rely on these open source componets.","All development, build, test, environments.",ABakerIII,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-05 04:05:42.0,,,,,,,"0|i3wpaf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hierarchical Topics,KAFKA-1175,12683749,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,pradeepg26,pradeepg26,10/Dec/13 02:16,03/Aug/18 22:28,12/Jan/21 10:06,,,,,,,,,,,,core,,,,,,2,,,,,Allow for creation of hierarchical topics so that related topics can be grouped together.,,chamilad,helena_e,jpeeters,junrao,pradeepg26,skuehn,zandegran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-12-10 05:18:00.011,,,false,,,,,,,,,,,,,,,,,,362821,,,Mon Oct 31 13:49:36 UTC 2016,,,,,,,"0|i1qjiv:",363127,,,,,,,,,,,,,,,,"10/Dec/13 02:17;pradeepg26;Linking the Confluence page containing the initial proposal","10/Dec/13 02:30;pradeepg26;I'm very interested in this feature. I created this JIRA  to track the progress and, to start a dialog so we can discuss this further. I would love to work on this if no one is working on it. I would love to discuss how this would relate/affect securing Kafka [KAFKA-1176]","10/Dec/13 05:18;junrao;Could you share some of the use cases that you have in mind?","10/Dec/13 06:29;pradeepg26;In the proposal, [~jkreps] talks about a couple of use cases at LinkedIn.

# Group a set of topics that are related by the application area (ads, search, etc.)
# Group a set of topics based on usage paradigm (tracking, metrics, etc.)

At Lithium, we have an extension of those use cases. We have multiple products that are deployed for different customers. So we need to partition by customer_id and product_id. So, we may have hierarchical topics of the following nature:
# /p1/google/tracking/search/click_events
# /p1/google/tracking/community/message_create_events
# /p1/linkedin/tracking/search/click_events
# /p1/linkedin/tracking/community/message_create_events
# /p2/apache/tracking/core/user_ident_event","24/Jul/15 15:38;jpeeters;I agree that this would be a very useful enhancement. We would use it for similar reasons as Praheep, i.e. to group topics that are related as per application area etc. Currently trying to achieve this by a flat namespace structure, e.g. p1.google.tracking.search.click_events.

As a caveat/disclaimer - we are only just starting to explore Kafka and like it a lot. It just struck us as odd that there was no hierarchy for the topics, given that we'd like to use this as the main message bus for all our stuff, across various domains within the company.

Anyway - thanks for the great work!","31/Oct/16 13:49;helena_e;Hoping this hasn't totally fizzled out and the ZK portion isn't a complete barrier. If further justification is needed, in terms of distributed systems design patterns this enables many I use. For instance if you want some consumers on topics behaviors.anomalies.x, others on behaviors.anomalies.y and also want to watch behaviors.anomalies.*

This is a strategy leveraged in other messaging systems which is helpful, and lends to securing particular topics as well, for instance consumers of behaviors.anomalies.{x or y} should not have permissions to any namespace above x or y and should not be able to subscribe to behaviors.anomalies.*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add kafka logbak appender,KAFKA-2717,12909397,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,vesense,vesense,31/Oct/15 10:39,03/Aug/18 17:47,12/Jan/21 10:06,,,,,,,,,,,,logging,,,,,,0,,,,,"Since many applications use logback as their log framework.
So, KafkaLogbakAppender would make it easier for integrating with kafka, just like KafkaLog4jAppender.",,githubbot,vesense,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-10-31 10:54:53.977,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 26 22:38:07 UTC 2016,,,,,,,"0|i2nrxb:",9223372036854775807,,,,,,,,,,,,,,,,"31/Oct/15 10:54;githubbot;GitHub user vesense opened a pull request:

    https://github.com/apache/kafka/pull/398

    KAFKA-2717: Add kafka logbak appender

    https://issues.apache.org/jira/browse/KAFKA-2717

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vesense/kafka kafka-logback

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/398.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #398
    
----
commit 8403829dd194ad99f507acc94a01274a6d2b5f39
Author: vesense <best.wangxin@163.com>
Date:   2015-10-30T08:34:21Z

    ignore subproject .gitignore file

commit eb7d7d512f2087f2914c6e538b242c2c6d98d5d3
Author: vesense <best.wangxin@163.com>
Date:   2015-10-30T13:14:06Z

    add logback appender

commit 9de9c4ed45c2b3fc4263c2b0d1dd14b2f40fbb12
Author: vesense <best.wangxin@163.com>
Date:   2015-10-31T10:44:59Z

    Add kafka logbak appender

----
","26/Dec/16 22:38;githubbot;Github user pono closed the pull request at:

    https://github.com/apache/kafka/pull/398
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka should write its metrics to a Kafka topic,KAFKA-2569,12895573,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,wushujames,wushujames,22/Sep/15 20:20,03/Aug/18 00:27,12/Jan/21 10:06,,,,,,,,,,,,metrics,,,,,,0,,,,,"Kafka is often used to hold and transport monitoring data.

In order to monitor Kafka itself, Kafka currently exposes many metrics via JMX, which require using a tool to pull the JMX metrics, and then write them to the monitoring system.

It would be convenient if Kafka could simply send its metrics to a Kafka topic. This would make most sense if the Kafka topic was in a different Kafka cluster, but could still be useful even if it was sent to a topic in the same Kafka cluster.

Of course, if sent to the same cluster, it would not be accessible if the cluster itself was down.

This would allow monitoring of Kafka itself without requiring people to set up their own JMX-to-monitoring-system pipelines.",,delbaeth,otis,utkarshcmu,wushujames,zandegran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-23 03:08:25.164,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 24 10:45:00 UTC 2015,,,,,,,"0|i2lf9j:",9223372036854775807,,,,,,,,,,,,,,,,"23/Sep/15 03:08;otis;bq. This would allow monitoring of Kafka itself without requiring people to set up their own JMX-to-monitoring-system pipelines.

But wouldn't that then require people to set up their own ""Kafka topic to monitoring system"" pipelines, which relatively few people and monitoring tools have?
","23/Sep/15 03:51;wushujames;Yes, although I didn't realize that was a rare thing. I'm pretty new to the operations-side of Kafka.

How do people typically monitor things with JMX metrics (like Kafka and Zookeeper)? ","24/Sep/15 03:16;otis;bq. How do people typically monitor things with JMX metrics (like Kafka and Zookeeper)?

They use tools like SPM for Kafka / ZK / etc., or home-grown stuff.  See http://sematext.com/spm/integrations/kafka-monitoring.html for example.
","24/Sep/15 10:45;utkarshcmu;[~otis] & [~wushujames] - This is the use case which I faced a couple of months ago. So, I implemented JMXTrans KafkaWriter to send JMX Metrics to Kafka. Here is the link:

https://github.com/jmxtrans/jmxtrans/tree/master/jmxtrans-output/jmxtrans-output-kafka

So, just install JMXTrans on Kafka instances and send Kafka metrics to any Kafka(itself or another one). Hope this helps. Please let me know if there are questions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose topic size for monitoring,KAFKA-6040,13108217,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,michael.andre.pearce,michael.andre.pearce,10/Oct/17 08:23,03/Aug/18 00:15,12/Jan/21 10:06,,,,,,,,,,,,metrics,,,,,,0,,,,,"Currently it seems there is no monitoring endpoint of topic size, even though the cluster must calculate such estimation to maintain retention.size

This is useful operationally to get understanding of actual topic usages, based on what the broker believes.

As interim it seems some scripts exist to get this, but it would be ideal to get the value the broker believes/calculates.

https://gist.github.com/kapkaev/ef633348dbecfedc0166",,ijuma,michael.andre.pearce,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-10-10 09:10:34.594,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 10 09:10:34 UTC 2017,,,,,,,"0|i3l2jb:",9223372036854775807,,,,,,,,,,,,,,,,"10/Oct/17 09:10;ijuma;Retention is done on a partition, not topic, so the broker doesn't need to maintain a topic size at the moment. The partition size is exposed via a Log size metric",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka should be able to generate Hadoop delegation tokens,KAFKA-1696,12747117,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,parth.brahmbhatt,jkreps,jkreps,09/Oct/14 20:35,02/Aug/18 19:59,12/Jan/21 10:06,,,,,,,,,,,,security,,,,,,6,,,,,For access from MapReduce/etc jobs run on behalf of a user.,,a.modgil1981,amarouni,anandriyer,benoyantony,bosco,chris.cotter,chtyim,clayb,eronwright,feestend,guozhang,gwenshap,hkropp,jerryshao,jghoman,jkreps,jugadj,krisden,lingesh,manishnema,merlin,mherstine,mikhels,mridley,noslowerdna,omalley,omkreddy,parth.brahmbhatt,qwertymaniac,richardatcloudera,singhashish,smurakozi,sriharsha,sttts,tbenton,tgraves,umesh9794@gmail.com,vijikarthi,weichiu,yxzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1686,KAFKA-1695,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-10-10 01:38:22.163,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 02 19:58:49 UTC 2018,,,,,,,"0|i2104f:",9223372036854775807,,,,,,,,,,,,,,,,"10/Oct/14 01:38;gwenshap;The way Hadoop currently works, MapReduce tasks (and similarly anything in YARN containers) does not have access to Kerberos tickets. Instead, it uses DIGEST-MD5 authentication scheme. This is explained in great detail here: http://carfield.com.hk/document/distributed/hadoop-security-design.pdf

The gist is that when the job is started, the job client obtains a signed ""delegation token"" and secret ""token authenticator"" from the service (typically NN or JT). These are distributed to the mappers (or containers) through a credentials cache. The server keeps part of the secret (typically in NN/JT memory) and uses that to authenticate the clients.

In order for Kafka to support this authentication scheme, we need to:
1) Be able to generate the token
2) Be able to store the Broker half of the secret in a way that is secured but accessible to all brokers (probably ZK)
3) Be able to authenticate clients using the tokens

Other goals I see are:
1) Reuse Hadoop code to avoid re-inventing the wheel, but otherwise minimize dependencies
2) If at all possible, without patching Hadoop, we want this to be transparent to existing MR jobs (especially Camus). If not possible, we should provide an easy-to-use authentication API to minimize the required changes.

Open question: Do we need to support token renewal? 

I'm working on a design doc for this part, but it will probably only happen after Strata.
","10/Oct/14 04:08;gwenshap;Since the delegation tokens depend on SASL/MD5-DIGEST, it will be easier to implement after the SASL/GSS (Kerberos) is in. We'll want to re-use a lot of the same code in both cases.

Also, there's a good chance we'll use ZK to store the broker secrets, so being able to authenticate with secure ZK is a dependency.","22/Oct/14 16:36;sriharsha;[~gwenshap] Incase if you not started working on it I am interested in taking it up. Thanks.","22/Oct/14 16:55;gwenshap;I started on the design doc, but I'll admit that I was not in a hurry since this is blocked on the Kerberos support anyway.

If you have your own thoughts and want to collaborate on the design, I'll be happy to work together. We can figure out who is doing the coding when it becomes more relevant.","22/Oct/14 16:59;sriharsha;[~gwenshap] sounds good to me. Since it was unassigned I assigned it to myself. Feel free to reassign. I'll work on putting together my thoughts on this.","19/Jan/16 00:24;parth.brahmbhatt;[~gwenshap] Are you still working on this? We have some customers that needs this feature and if you have not started the design work I would like to take this over. ","22/Jan/16 22:52;singhashish;[~harsha_ch] [~parth.brahmbhatt] [~gwenshap] seems like we have similar needs and would like to know if someone is working on this. If required I can pitch in.","22/Jan/16 23:04;gwenshap;I am not working on it, so you guys figure out ownership :)","22/Jan/16 23:05;parth.brahmbhatt;I will assign it to my self and file a KIP ","22/Jan/16 23:06;singhashish;Thanks [~gwenshap], [~parth.brahmbhatt]! Looking forward to the KIP proposal.","30/Jan/16 18:25;singhashish;[~parth.brahmbhatt] just curious if you have the KIP draft up for review. Willing to help, if you need any. Thanks!","31/Jan/16 03:47;sriharsha;[~singhashish] We are working on it.Will post the KIP to wiki.","02/Feb/16 04:17;gwenshap;[~harsha_ch], you are following [~rsivaram]'s KIP on new SASL mechanisms, right? 

Hadoop implements Delegation Tokens as a SASL mechanism and I was wondering if you were planning on doing the same? If so, you may have an opinion on how the mechanism is determined. If you are not planning on hooking into SASL, then never mind :)



","17/Feb/16 01:28;parth.brahmbhatt;[~gwenshap] [~harsha_ch] [~singhashish] I posted an initial KIP Draft https://cwiki.apache.org/confluence/display/KAFKA/KIP-48+Delegation+token+support+for+Kafka. I haven't yet opened a Discuss thread as I need to verify some assumptions I have made. ","25/Feb/16 19:18;singhashish;[~parth.brahmbhatt] thanks for working out the design. Great write up. I think it is in a decent shape to initiate a discuss thread on the KIP. I have following questions/ concerns.

1. We probably need new error-codes to indicate token renewal failure, etc. I will probably be a good idea to list them as well.
2. Probably use coordinator to communicate token changes from one broker to other brokers. So, a broker generating token will add to ZK, coordinator watches for new or changed tokens and communicates to other brokers in cluster.
3. May be provide an outline on the cli tool for creating/renewing/deleting delegation tokens.","25/Feb/16 19:25;parth.brahmbhatt;[~singhashish] Thanks for taking the time to review. I will add the errorcode/exception to the KIP and sample CLI.

For point 2, can you elaborate why it might be a better idea to communicate via coordinator? When I was thinking about distributing the tokens I felt the coordinator will just add extra complexity and wont buy us much. We already have multiple things ACLs/Configs for which we rely on zookeeper watchers so it seemed  like a natural choice. ","25/Feb/16 19:43;singhashish;[~parth.brahmbhatt] I am in favor of keeping this token information as much contained within brokers, not on ZK. Infact, how does the idea of limiting the delegation token generation to coordinator sounds to you? So, delegationToken req goes to co-ordinator and it is responsible of passing the token to client after it has been distributed to other brokers. This will avoid a race condition of brokers not updated with delegation token before clients try using it, which is possible in current design suggestion.","25/Feb/16 20:22;parth.brahmbhatt;So here is how that request path would work in my mind:

* Client sends request for token acquisition to any broker.
* Broker forwards the request to the controller.
* Controller generates the token and pushes the tokens to all brokers. (Will need a new API)
* Controller responds back to original broker with the token.
* Broker responds back to client with the token.

Renewal is pretty much the same.

The race condition you are describing can still happen in the above case during renewal because controller may have pushed the renewal information to a subset of broker and die. The clients depending on which broker it connects to may get an exception or success. I do agree though that given controller would not have responded back with success the original renew request should be retried and most likely the scenario can be avoided.

If the above steps seems right , here are the advantages of this approach:

Advantage:
* Token generation/renewal will not involve zookeeper. I am not too worried about the load on zookeeper added due to this but it definitely seems more secure and follows the Hadoop model more closely. However zookeeper needs to be secure for lot of other things in kafka so not sure if this should really be a concern.
* Clients will get better consistency.

Disadvantage:
* We will have to add new APIs to support controller pushing tokens to brokers on top of the minimal APIs that are currently proposed. I like the publicly available APIs to be minimal and I like them to be something that we expect clients to use + this adds more development complexity. Overall this seems like a more philosophical thing so depending on who you ask they may see this as disadvantage or not. 
* We will also have to add APIs to support the bootstrapping case. What I mean is , when a new broker comes up it will have to get all delegation tokens from the controller so we will again need to add new APIs like getAllTokens. Again some of us may see that as disadvantage and some may not.
* In catastrophic failures where all brokers go down, the tokens will be lost even if servers are restarted as tokens are not persisted anywhere. Granted if something like this happens customer has bigger things to worry about but if they don't have to regenerate/redistribute tokens that is one less thing.

I don't see strong reasons to go one way or another so I would still like to go with zookeeper but don't really feel strongly about it. If you think I have mischaracterized what you were proposing feel free to add more details or list and other advantages/disadvantages.
","25/Feb/16 21:34;singhashish;[~parth.brahmbhatt] thanks for the detailed analysis, as you have mentioned the disadvantages are not really big ones here. Moreover moving away from ZK, tends to be the general direction. KIP-4 is WIP and suggestions have been made to make Kafka metadata store pluggable. I am more in favor of going via controller. I do agree it will be more work and this work is kind of in high demand. Probably having it in 0.10.0.0 would be ideal. I am willing to help out, if needed.","25/Feb/16 21:37;gwenshap;Just to clarify - the move away from ZK is for clients, not for brokers.
The suggestions to make Kafka metadata store pluggable were never accepted and are far from being in consensus.

I would not base any broker-side design on the assumption that ZK is going anywhere (just on the assumption that clients shouldn't have to depend on it).","25/Feb/16 21:44;parth.brahmbhatt;[~gwenshap] [~sriharsha] Please share your preference here so I can update the KIP accordingly and start a discussion thread. I wanted to try SASL over MD5 to ensure that this design will work as is but I don't see myself writing that prototype soon so might as well start the KIP discussion.","25/Feb/16 21:52;gwenshap;I don't have specific opinions on the KIP yet, I have not read it...

I was responding to Ashish who said: ""moving away from ZK, tends to be the general direction"" 
This is incorrect, and I want to correct this impression.","25/Feb/16 21:53;gwenshap;One thing I can't find in the KIP:
* Will we support SASL/Kerberos and SASL/Digest on same port? 

","25/Feb/16 21:55;singhashish;I think posting a discuss thread makes sense. Discuss thread will help get
more audience and suggestions. If we choose to go the coordinator route, we
can always adopt KIP while discuss is going on. However, it would be nice
to mention that this discussion has mentioned on the JIRA. Or maybe just
copy paste the content from here to avoid duplicating points.

On Thursday, February 25, 2016, Parth Brahmbhatt (JIRA) <jira@apache.org>



-- 
Ashish 🎤h
","25/Feb/16 22:38;parth.brahmbhatt;[~gwenshap] I think that discussion is happening as part of another KIP and no matter what we chose I don't think it affects the delegation token design. ","25/Feb/16 22:44;sriharsha;[~gwenshap] Port is not relevant to this JIRA. That discussion is part of KIP-43. And we decided to single port for SASL. Each mechanism inside SASL shouldn't have its own port.","25/Feb/16 23:46;gwenshap;OK, sounds good. Thanks. 
It does make this issue depend on KIP-43 getting in.","04/Mar/16 00:00;eronwright;I'd like clarification on whether renewal is possible using the delegation token for authentication, and whether an infinite expiration will be possible (with the appropriate configuration).   

I'm thinking of the scenario of a production-level Flink streaming job, consuming a topic in perpetuity.    The client that submits the job should obtain a delegation token using their Kerberos credential, then hand the delegation token to the running job.   The job should periodically renew the token(s).   Ideally the delegation token may be used to authenticate the renewal request.    It doesn't seem easy to have Flink use a Kerberos credential to renew it, but may be possible with a service principal of some kind.      

The notion that the token eventually expires seems incompatible with long-running jobs.   A key purpose of delegation tokens is to avoid distributing keytabs, but how does that reconcile with expiration?
","19/Apr/16 15:26;singhashish;[~parth.brahmbhatt] are you still working on this?","19/Apr/16 16:12;sriharsha;[~singhashish] Yes we are actively working on it.","19/Apr/16 17:45;singhashish;Good to know [~harsha_ch]! Thanks for the quick response.","14/Oct/16 00:48;chtyim;May I ask for any update about this feature? Any idea in which Kafka version will have this?","29/Nov/16 18:14;singhashish;It has been a while, since any progress has been made on this. It is becoming increasingly important for a lot of users. I am assuming that it is OK, if I assign this JIRA to myself and start working on it. Multiple people have asked for updated here and on associated discuss list. I will wait for a day before assigning the JIRA to myself.","29/Nov/16 18:58;sriharsha;[~singhashish] [~omkreddy]  is working on it. I think its best to break this down into multiple JIRAs and distribute the work.","29/Nov/16 19:02;singhashish;[~omkreddy] would you be able to provide an update on what progress have you made. As [~harsha_ch] suggested, we can collaborate to help things move faster. ","12/Dec/16 17:11;omkreddy;.[~singhashish] Sorry for the late reply. I reinitiated the KIP discussion. Pl review the KIP.  I will the divide the work into multiple JIRAs.","14/Dec/16 01:37;singhashish;[~omkreddy] as I indicated on discuss list, I have made some progress along this to put together a POC. I would love if my work can help us speed up here. Do you think it is OK, if I propose some sub-jiras and add PRs over there. There is a lot still left to do, but I can quickly throw some initial pieces that will help us with start. Makes sense?","14/Dec/16 06:07;omkreddy;[~singhashish] Yes, Pl go ahead and create sub-jiras and PRs , So that we avoid any duplicate efforts.","14/Dec/16 22:54;singhashish;[~omkreddy] I had to convert this JIRA from sub-task to a new feature JIRA, to be able to create sub-tasks. Also created some sub-tasks. I have left most of them open, except the first one, for which I plan to submit a PR shortly. Also, I have partially looked into authentication via tokens, basically add a TokenAuthenticator, so if you are Ok with me taking up that task, I will be happy to.","15/Dec/16 17:24;omkreddy;Thanks for creating sub-tasks. About authentication, we are still discussing about credentials of
SCRAM mechanism. As mentioned in the mailing list, I am planning to pass tokenID, hmac as username and
password. Now we need to store SCRAM credentials in ZK along with token details. Pl check if this
fits into your design. This works depends on KAFKA-3751.","12/Jul/17 05:55;guozhang;[~omkreddy] [~singhashish] [~parth.brahmbhatt] Are you still working on this JIRA (KIP-48) right now?","12/Jul/17 13:28;omkreddy;[~guozhang] This got delayed due to some internal works.  Will raise the first version PR by this month end.","02/Aug/18 19:58;merlin;ping, does this GA or not? [~omkreddy]",,,,,,,,,,,,,,,,,,,,
MirrorMaker topic renaming,KAFKA-3841,12979188,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,,yangguo1220,yangguo1220,14/Jun/16 22:54,26/Jul/18 19:29,12/Jan/21 10:06,18/Dec/16 09:37,0.10.0.0,,,,,,,,,,tools,,,,,,0,,,,,"Our organization (walmart.com) has been a Kafka user since some years back and MirrorMaker has been a convenient tool to bring our Kafka data from one Kafka cluster to another cluster.

In many our use cases, the mirrored topic from the source cluster may not want to have the same name in the target cluster. This could be a valid scenario when the same topic name already exists on the target cluster, or we want to append the name of the data center to the topic name in the target cluster, such as ""grocery_items_mirror_sunnyvale"", to explicitly identify the source (e.g. sunnyvale) and nature (e.g. mirroring) of the topic.

We have implemented the MirrorMaker topic renaming feature internally which has been used for production over a couple of years. While keeping our internal Kafka fork with the above ""renaming"" branch across version upgrade does not cost us too much labor, we think it may be meaningful to contribute back to the community so that potentially many people may have the similar expectation and could benefit from this feature.
",,daluu,erikdw,krisden,ryannedolan,wushujames,yangguo1220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-14 23:03:48.784,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 26 19:20:55 UTC 2018,,,,,,,"0|i2zgtr:",9223372036854775807,,,,,,,,,,,,,,,,"14/Jun/16 23:03;wushujames;We have a similar use case. Mirrormaker has a config option called message.handler that lets you modify the message as mirrormaker produces it to the new cluster. See the docs here:

https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/tools/MirrorMaker.scala#L142-L152

We are using that functionality to change the topic name. See https://github.com/gwenshap/kafka-examples/blob/master/MirrorMakerHandler/src/main/java/com/shapira/examples/TopicSwitchingHandler.java for an example.



","14/Jun/16 23:04;wushujames;More comments here: https://github.com/gwenshap/kafka-examples/tree/master/MirrorMakerHandler
","14/Jun/16 23:38;yangguo1220;Thanks. This is good to know. As MessageHandler was recently added since 0.9 (where our production has not been upgraded to there yet), we has been using our own solution for topic renaming. 

Looks like we may keep using our stuff or adopt the message handler when upgrading to 0.9 or above.

Intend to mark this jira as ""Resolve"".","17/Dec/16 00:30;wushujames;[~yangguo1220], should we mark this JIRA as ""resolved""?","26/Jul/18 19:20;daluu;Just wanted to mention related info about the message handler option: [https://www.opencore.com/blog/2017/1/170131-mirrormaker-change-topic/]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support dynamic updates of frequently updated broker configs,KAFKA-6240,13119643,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,20/Nov/17 13:15,23/Jul/18 10:33,12/Jan/21 10:06,15/Feb/18 13:31,,,,,,,,1.1.0,,,core,,,,,,0,,,,,"See [KIP-226|https://cwiki.apache.org/confluence/display/KAFKA/KIP-226+-+Dynamic+Broker+Configuration] for details.

Implementation will be done under sub-tasks.",,brettrann,memelet,Narendra Kumar,rsivaram,smush618,suhaschikkanna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1229,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-03-19 23:44:09.182,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 23 10:33:59 UTC 2018,,,,,,,"0|i3mzz3:",9223372036854775807,,hachikuji,,,,,,,,,,,,,,"19/Mar/18 23:44;brettrann;If you've come here from release notes looking for more info, here's the dynamic config documentation: http://kafka.apache.org/11/documentation.html#dynamicbrokerconfigs","23/Jul/18 10:33;smush618;bin/kafka-configs.sh --bootstrap-server node10.hcbss:7667,node11.hcbss:7667,node12.hcbss:7667 --alter --add-config 'sasl.mechanism.inter.broker.protocol=PLAIN,sasl.enabled.mechanisms=PLAIN,sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=""admin"" password=""admin"" user_admin=""admin"" user_cbss=""cbss123"" --entity-name 12 --entity-type brokers --command-config config/consumer.properties

i want to know why i can not update sasl.jaas.config to kafka dynamicly?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prometheus /metrics http endpoint for monitoring integration,KAFKA-7172,13172650,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,harisekhon,harisekhon,17/Jul/18 13:33,20/Jul/18 09:03,12/Jan/21 10:06,,1.1.2,,,,,,,,,,metrics,,,,,,0,,,,,"Feature Request to add Prometheus /metrics http endpoint for monitoring integration:

[https://prometheus.io/docs/prometheus/latest/configuration/configuration/#%3Cscrape_config%3E]

Prometheus metrics format for that endpoint:

[https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md]

 

See also ticket for /jmx http endpoint similar to what Hadoop and HBase have done for years in

https://issues.apache.org/jira/browse/KAFKA-3377

 ",,harisekhon,ibaklanov,jon@cybus.co.uk,serge.travin,sslavic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-07-19 11:49:27.412,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 20 09:03:44 UTC 2018,,,,,,,"0|i3vz5r:",9223372036854775807,,,,,,,,,,,,,,,,"19/Jul/18 11:49;sslavic;[~harisekhon] have you considered [https://github.com/prometheus/jmx_exporter] ?","20/Jul/18 09:03;harisekhon;That's a fair suggestion as a workaround for now although I believe in the long run most services will want to implement Prometheus monitoring support natively.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose the log dir for a partition as a metric,KAFKA-4503,13026353,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,ijuma,ijuma,07/Dec/16 13:13,19/Jul/18 21:48,12/Jan/21 10:06,,,,,,,,,,,,metrics,,,,,,0,,,,,It would be useful to be able to map a partition to a log directory if multiple log directories are used.,,githubbot,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-12-07 13:17:13.732,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 07 16:21:40 UTC 2016,,,,,,,"0|i37akn:",9223372036854775807,,,,,,,,,,,,,,,,"07/Dec/16 13:17;githubbot;GitHub user ijuma opened a pull request:

    https://github.com/apache/kafka/pull/2224

    KAFKA-4503: Expose the log dir for a partition as a metric

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ijuma/kafka kafka-4503-log-dir-for-partition-as-metric

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2224.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2224
    
----
commit 796983fcbd6e268f05f2e33851304cfbbc92aee7
Author: Ismael Juma <ismael@juma.me.uk>
Date:   2016-12-07T13:16:39Z

    Introduce `LogDir` gauge in `Log`

----
","07/Dec/16 16:19;githubbot;Github user ijuma closed the pull request at:

    https://github.com/apache/kafka/pull/2224
","07/Dec/16 16:21;ijuma;Also see KAFKA-1614, which is a bit broader.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
General wildcard support for ACL's in kafka,KAFKA-6369,13125065,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,astubbs,astubbs,15/Dec/17 00:23,19/Jul/18 18:20,12/Jan/21 10:06,,,,,,,,,,,,security,,,,,,1,,,,,"Especially for streams apps where all intermediate topics are prefixed with the application id.

For example, add read and write access to mystreamsapp.* so any new topics created by the app don't need to have specific permissions applied to them.",,astubbs,Kjelle,lpicanco,PhilGrayson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-5713,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-01-08 17:36:50.515,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 08 17:36:50 UTC 2018,,,,,,,"0|i3nx9r:",9223372036854775807,,,,,,,,,,,,,,,,"08/Jan/18 17:36;lpicanco;Seens related to KAFKA-5713",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support migration of old consumers to new consumers without downtime,KAFKA-4513,13026713,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,onurkaraman,ijuma,ijuma,08/Dec/16 15:54,26/Jun/18 12:00,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,1,,,,,"Some ideas were discussed in the following thread:

http://markmail.org/message/ovngfw3ibixlquxh",,ijuma,noslowerdna,omkreddy,roufique07,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-01-04 14:35:19.651,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 26 12:00:57 UTC 2018,,,,,,,"0|i37csn:",9223372036854775807,,,,,,,,,,,,,,,,"04/Jan/18 14:35;noslowerdna;Is this Jira ([KIP link|https://cwiki.apache.org/confluence/display/KAFKA/KIP-125%3A+ZookeeperConsumerConnector+to+KafkaConsumer+Migration+and+Rollback]) still actively being considered? As time passes the need to migrate old to new consumers continues to decrease.","26/Jun/18 12:00;omkreddy;[~ijuma] Can we close this JIRA?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Garbage collect old consumer metadata entries,KAFKA-559,12611031,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Auto Closed,ewencp,jkreps,jkreps,09/Oct/12 18:09,15/Jun/18 17:16,12/Jan/21 10:06,15/Jun/18 17:16,,,,,,,,,,,,,,,,,2,newbie,project,,,"Many use cases involve tranient consumers. These consumers create entries under their consumer group in zk and maintain offsets there as well. There is currently no way to delete these entries. It would be good to have a tool that did something like
  bin/delete-obsolete-consumer-groups.sh [--topic t1] --since [date] --zookeeper [zk_connect]
This would scan through consumer group entries and delete any that had no offset update since the given date.",,ewencp,guozhang,jghoman,jjkoshy,jkreps,mike744,nehanarkhede,omkreddy,swapnilghike,tejasp,tmancill,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/14 16:55;ewencp;KAFKA-559.patch;https://issues.apache.org/jira/secure/attachment/12677332/KAFKA-559.patch","04/Jul/13 01:59;tejasp;KAFKA-559.v1.patch;https://issues.apache.org/jira/secure/attachment/12590765/KAFKA-559.v1.patch","08/Jul/13 19:04;tejasp;KAFKA-559.v2.patch;https://issues.apache.org/jira/secure/attachment/12591257/KAFKA-559.v2.patch",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2012-10-09 22:28:53.14,,,false,,,,,,,,,,,,,,,,,,246208,,,Fri Jun 15 17:16:19 UTC 2018,,,,,,,"0|i07i6n:",41713,,jjkoshy,,,,,,,,,,,,,,"09/Oct/12 22:28;nehanarkhede;This is a good tool to have. In addition to being able to remove the consumer groups with no offset updates since the zk modified timestamp, it will be good to add a defensive check to ensure that there are no consumers currently registered under that group. This will prevent us from accidentally deleting the offset information for alive consumer groups.","02/Jul/13 21:57;jjkoshy;Assigning to Tejas, since he has done some work on this recently.

","02/Jul/13 21:58;jjkoshy;One additional recommendation: support a --dry-run option.

","04/Jul/13 01:59;tejasp;Attached KAFKA-559.v1.patch above. 
- It has been developed to work do cleanup with ""group-id"" as input from user instead of ""topic"". 
- An additional ""dry-run"" feature is provided so that people can see what all znodes would get deleted w/o actually deleting them.","04/Jul/13 07:27;swapnilghike;Some feedback:

1. Passing a groupId for cleanup will make the cleanup job tedious since we tend to have hundreds of console-consumer group ids in ZK that are stale. Running the tool for a particular topic or all topics probably makes more sense. 

2. I would suggest accepting a date param ""mm-dd-yyyy hh:mm:ss,SSS"" as a String instead of accepting a timestamp value, and deleting the group only if it has had no updates to its offsets since that date, as described above.

3. It's dangerous to delete the entire group if the date/""since"" is not provided. It's very easy for user to specify only two arguments (topic and zkconnect) and not specify the date. Let's also make sure that the user always specifies a date.

4. ""dry-run"" does not need to accept any value. You can simply use parser.accepts(""dry-run"", ""...."") and then use if (options.has(dryRunOpt)) { yeay } else { nay }.

5. We can inline exitIfNoPathExists, the implementation is small and clear enough.

6. We should have an info statement when the group ids are deleted in the non dry-run mode.

7. info(""Removal has successfully completed."") can probably be refactored to something more specific to this tool.

8. Instead of writing a different info statement for dry-run mode, I think you should be able to set logIdent of Logging to ""[dry-run]"" or """" depending on which mode the tool is working in. This will let you have a single info statement for both modes. 

Minor stuff:

1. I think we tend to use camelCase in variable names instead of underscores. 
2. Whitespaces can be made more consistent.","08/Jul/13 19:04;tejasp;_1. Passing a groupId for cleanup will make the cleanup job tedious since we tend to have hundreds of console-consumer group ids in ZK that are stale. Running the tool for a particular topic or all topics probably makes more sense._
I had received different requirement spec which was based on ""group"" and not ""topic"". In the new patch, I have added support for topic based deletion too.

_2. I would suggest accepting a date param ""mm-dd-yyyy hh:mm:ss,SSS"" as a String instead of accepting a timestamp value, and deleting the group only if it has had no updates to its offsets since that date, as described above._
I had a discussion with Joel about this one and he had suggested me to use the EPOCH time thing instead of ""mm-dd-yyyy"". I am open for modification but if there is a consensus about it.

_3. It's dangerous to delete the entire group if the date/""since"" is not provided. It's very easy for user to specify only two arguments (topic and zkconnect) and not specify the date. Let's also make sure that the user always specifies a date._
Ok. Change implemented

_4. ""dry-run"" does not need to accept any value. You can simply use parser.accepts(""dry-run"", ""...."") and then use if (options.has(dryRunOpt)) ....._
Agreed. Change implemented

_5. We can inline exitIfNoPathExists, the implementation is small and clear enough._
While adding support for topic based deletion, that method had no use so got rid of it. 

_6. We should have an info statement when the group ids are deleted in the non dry-run mode._
Good catch :) Change implemented

_7. info(""Removal has successfully completed."") can probably be refactored to something more specific to this tool._
Below is what I changed it to:
logger.info(""Kafka obsolete Zk entires cleanup tool shutdown successfully."")

_8. Instead of writing a different info statement for dry-run mode, I think you should be able to set logIdent of Logging to ""[dry-run]"" or """" depending on which mode the tool is working in._
Nice one :) Change implemented

Minor stuff:
_1. I think we tend to use camelCase in variable names instead of underscores._
_2. Whitespaces can be made more consistent._

I just read http://kafka.apache.org/coding-guide.html. Will take care of that from now on. 

One related question: is there a code formatter for Kafka ?
Creating a fat wiki page with a bunch of project specific formatting is helpful but people won't have that in mind everytime they code. 
It gets worse when people work on multiple projects which follow different conventions. Here is what some other projects came up with to account for that:
- https://issues.apache.org/jira/browse/HBASE-3678
- http://svn.apache.org/viewvc/nutch/branches/2.x/eclipse-codeformat.xml?view=markup
- https://github.com/cloudera/blog-eclipse/blob/master/hadoop-format.xml

Attached KAFKA-559.v2.patch : Implemented the changes suggested by [~swapnilghike] and did some sundry refactoring","09/Jul/13 07:19;jjkoshy;Thanks for the patch. Overall, looks good.  Couple of comments, mostly minor
in no particular order:

* I think dry-run does not need any further qualifier such as withOptionalArg, describedAs, ofType - it's just a flag.
* For --since, I prefer the seconds since epoch over some fixed input format which then brings in ambiguity such as timezone 24h vs 12h, etc. A better alternative would be to accept date strings and use  the DateFormat class with lenient parsing turned on or something like that. --before may be more intuitive than --since.
* Can use CommandLineUtils.checkRequiredArgs
* deleteBy matching - prefer to use case match and thereby avoid the explicit check on valid values. Also the message on invalid value of deleteBy should inform what the valid values are.
* Right now you support the following modes: delete stale topics across all groups, delete stale topics in a specific group. I think it would be useful to make deleteBy optional - if unspecified, it scans all groups and gets rid of stale groups.
* line 75: warn (""msg"", e)
* line 101: should provide a reason for aborting
* line 110: doesn't gropudirs have an offset path? if not maybe we should add it
* Logging should include last mtime as that may be useful information reported by the dry-run
* No need to add a wrapper shell script for the tool.
* make all of the methods except main private.
* The return statements can be dropped - i.e., just write the return value.
* Several vars can be vals instead.
* removeBrokerPartitionpairs: I don't think you would want to do a partial delete under a topic directory. You can check that all the partition offset paths are <= since and if so, just delete the topic path. With that the method would be better named something like deleteUnconsumedTopicsFromGroup?
* Finally, you are probably aware that there are a bunch of race conditions - e.g., checkIfLiveConsumers is a helpful check to have but not guaranteed to be correct as some consumers may creep in while the tool is running. However, I think it is reasonable for a tool like this to ignore that since a ""since"" value way back would mean the probability of that occuring is very low. Similar note for deleteGroupIfNoTopicExists.
","09/Jul/13 17:24;guozhang;One minor comment: add one debug() at the entry of each function such as removeObsoleteConsumerGroups and removeObsoleteConsumerTopics so that, for example, if we get an warn that the function aborts, we know it aborts from which point.","27/Oct/14 16:55;ewencp;Created reviewboard https://reviews.apache.org/r/27232/diff/
 against branch origin/trunk","27/Oct/14 16:57;ewencp;This is an updated version of the patch by [~tejas.patil]. I'm pretty sure I've addressed all the issues [~jjkoshy] brought up.","10/Nov/14 04:08;nehanarkhede;Reassigning to [~jjkoshy] since he reviewed previous versions and has many review comments.","15/Nov/16 16:18;tmancill;Hi [~ewencp].  The attachment [KAFKA-559.patch|https://issues.apache.org/jira/secure/attachment/12677332/KAFKA-559.patch] deletes bin/kafka-cleanup-obsolete-zk-entries.sh in the 3rd patch - I don't think this is intentional, is it?

{code}
From 1351639d8e7dc2c4b5e869b05960951a82cc629a Mon Sep 17 00:00:00 2001
From: Ewen Cheslack-Postava <me@ewencp.org>
Date: Thu, 23 Oct 2014 15:03:10 -0700
Subject: [PATCH 3/4] Fix naming: entires -> entries.

---
 bin/kafka-cleanup-obsolete-zk-entires.sh           |  19 --
 .../kafka/tools/CleanupObsoleteZkEntires.scala     | 331 ---------------------
 .../kafka/tools/CleanupObsoleteZkEntries.scala     | 331 +++++++++++++++++++++
 3 files changed, 331 insertions(+), 350 deletions(-)
 delete mode 100755 bin/kafka-cleanup-obsolete-zk-entires.sh
 delete mode 100644 core/src/main/scala/kafka/tools/CleanupObsoleteZkEntires.scala
 create mode 100644 core/src/main/scala/kafka/tools/CleanupObsoleteZkEntries.scala

diff --git a/bin/kafka-cleanup-obsolete-zk-entires.sh b/bin/kafka-cleanup-obsolete-zk-entires.sh
deleted file mode 100755
index f2c0cb8..0000000
--- a/bin/kafka-cleanup-obsolete-zk-entires.sh
+++ /dev/null
{code}

Instead, the shell script should be updated for the correct classname - something like:

{code}
diff --git a/bin/kafka-cleanup-obsolete-zk-entires.sh b/bin/kafka-cleanup-obsolete-zk-entires.sh
index f2c0cb8..0625183 100755
--- a/bin/kafka-cleanup-obsolete-zk-entires.sh
+++ b/bin/kafka-cleanup-obsolete-zk-entires.sh
@@ -16,4 +16,4 @@

 base_dir=$(dirname $0)
 export KAFKA_OPTS=""-Xmx512M -server -Dcom.sun.management.jmxremote -Dlog4j.configuration=file:$base_dir/kafka-console-consumer-log4j.properties""
-$base_dir/kafka-run-class.sh kafka.tools.CleanupObsoleteZkEntires $@
+$base_dir/kafka-run-class.sh kafka.tools.CleanupObsoleteZkEntries $@
{code}","15/Jun/18 17:16;omkreddy;Closing inactive issue. The old consumer is no longer supported.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for Prefixed ACLs,KAFKA-6841,13156348,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,BigAndy,piyushvijay,piyushvijay,01/May/18 15:28,15/Jun/18 05:30,12/Jan/21 10:06,15/Jun/18 05:30,,,,,,,,2.0.0,,,admin,security,,,,,0,,,,,"Kafka supports authorize access to resources like topics, consumer groups etc. by way of ACLs. The current supported semantic of resource name and principal name in ACL definition is either full resource/principal name or special wildcard '**'*, which matches everything.

Kafka should support a way of defining bulk ACLs instead of specifying individual ACLs.

The details for the feature are available here - [https://cwiki.apache.org/confluence/display/KAFKA/KIP-290%3A+Support+for+wildcard+suffixed+ACLs]

 ",,danhanley,githubbot,jdowning,maxxxx,piyushvijay,randy_b,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-5713,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-15 19:06:43.899,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 05 01:14:25 UTC 2018,,,,,,,"0|i3t7sf:",9223372036854775807,,junrao,,,,,,,,,,,,,,"15/May/18 19:06;rsivaram;[~piyushvijay] Since we don't normally backport KIPs to bugfix releases, the fix version for this Jira should be 2.0.0?","25/May/18 09:43;githubbot;piyushvijay opened a new pull request #5079: [WIP] KAFKA-6841: Add support for wildcard suffixed ACLs
URL: https://github.com/apache/kafka/pull/5079
 
 
   This is a WIP and require refactoring for storing Acls at dual location on ZK.
   Because I'm forward-porting it from 1.0.x and code has changed significantly since then.
   I'll post a new version tomorrow but this is for early preview.
   
   Details are in KIP-290:
   https://cwiki.apache.org/confluence/display/KAFKA/KIP-290%3A+Support+for+wildcard+suffixed+ACLs
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","05/Jun/18 01:14;githubbot;piyushvijay closed pull request #5079: KAFKA-6841: Add support for Prefixed ACLs
URL: https://github.com/apache/kafka/pull/5079
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/checkstyle/import-control.xml b/checkstyle/import-control.xml
index e4f9a4e8f59..87d9ee50c1e 100644
--- a/checkstyle/import-control.xml
+++ b/checkstyle/import-control.xml
@@ -146,7 +146,7 @@
     </subpackage>
 
     <subpackage name=""utils"">
-      <allow pkg=""org.apache.kafka.common.metrics"" />
+      <allow pkg=""org.apache.kafka.common"" />
     </subpackage>
   </subpackage>
 
diff --git a/checkstyle/suppressions.xml b/checkstyle/suppressions.xml
index ba48c38cb28..0e1861dbd9b 100644
--- a/checkstyle/suppressions.xml
+++ b/checkstyle/suppressions.xml
@@ -51,7 +51,7 @@
               files=""(Utils|Topic|KafkaLZ4BlockOutputStream|AclData).java""/>
 
     <suppress checks=""CyclomaticComplexity""
-              files=""(ConsumerCoordinator|Fetcher|Sender|KafkaProducer|BufferPool|ConfigDef|RecordAccumulator|KerberosLogin|AbstractRequest|AbstractResponse|Selector|SslFactory|SslTransportLayer|SaslClientAuthenticator|SaslClientCallbackHandler).java""/>
+              files=""(ConsumerCoordinator|Fetcher|Sender|KafkaProducer|BufferPool|ConfigDef|RecordAccumulator|KerberosLogin|AbstractRequest|AbstractResponse|Selector|SslFactory|SslTransportLayer|SaslClientAuthenticator|SaslClientCallbackHandler|ResourceUtils).java""/>
 
     <suppress checks=""JavaNCSS""
               files=""AbstractRequest.java|KerberosLogin.java|WorkerSinkTaskTest.java|TransactionManagerTest.java""/>
diff --git a/clients/src/main/java/org/apache/kafka/common/acl/AclBindingFilter.java b/clients/src/main/java/org/apache/kafka/common/acl/AclBindingFilter.java
index 64f16cd7460..5841b5aeb9a 100644
--- a/clients/src/main/java/org/apache/kafka/common/acl/AclBindingFilter.java
+++ b/clients/src/main/java/org/apache/kafka/common/acl/AclBindingFilter.java
@@ -19,7 +19,6 @@
 
 import org.apache.kafka.common.annotation.InterfaceStability;
 import org.apache.kafka.common.resource.ResourceFilter;
-import org.apache.kafka.common.resource.ResourceType;
 
 import java.util.Objects;
 
@@ -36,9 +35,7 @@
     /**
      * A filter which matches any ACL binding.
      */
-    public static final AclBindingFilter ANY = new AclBindingFilter(
-        new ResourceFilter(ResourceType.ANY, null),
-        new AccessControlEntryFilter(null, null, AclOperation.ANY, AclPermissionType.ANY));
+    public static final AclBindingFilter ANY = new AclBindingFilter(ResourceFilter.ANY, AccessControlEntryFilter.ANY);
 
     /**
      * Create an instance of this filter with the provided parameters.
diff --git a/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java b/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java
index 7f43caf8696..8bc546efcec 100644
--- a/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java
+++ b/clients/src/main/java/org/apache/kafka/common/protocol/CommonFields.java
@@ -17,6 +17,7 @@
 package org.apache.kafka.common.protocol;
 
 import org.apache.kafka.common.protocol.types.Field;
+import org.apache.kafka.common.requests.ResourceNameType;
 
 public class CommonFields {
     public static final Field.Int32 THROTTLE_TIME_MS = new Field.Int32(""throttle_time_ms"",
@@ -45,6 +46,7 @@
     public static final Field.Int8 RESOURCE_TYPE = new Field.Int8(""resource_type"", ""The resource type"");
     public static final Field.Str RESOURCE_NAME = new Field.Str(""resource_name"", ""The resource name"");
     public static final Field.NullableStr RESOURCE_NAME_FILTER = new Field.NullableStr(""resource_name"", ""The resource name filter"");
+    public static final Field.Int8 RESOURCE_NAME_TYPE = new Field.Int8(""resource_name_type"", ""The resource name type"", ResourceNameType.LITERAL.code());
     public static final Field.Str PRINCIPAL = new Field.Str(""principal"", ""The ACL principal"");
     public static final Field.NullableStr PRINCIPAL_FILTER = new Field.NullableStr(""principal"", ""The ACL principal filter"");
     public static final Field.Str HOST = new Field.Str(""host"", ""The ACL host"");
diff --git a/clients/src/main/java/org/apache/kafka/common/protocol/types/Field.java b/clients/src/main/java/org/apache/kafka/common/protocol/types/Field.java
index ec217f5bd0b..5c170017454 100644
--- a/clients/src/main/java/org/apache/kafka/common/protocol/types/Field.java
+++ b/clients/src/main/java/org/apache/kafka/common/protocol/types/Field.java
@@ -50,6 +50,9 @@ public Field(String name, Type type) {
         public Int8(String name, String docString) {
             super(name, Type.INT8, docString, false, null);
         }
+        public Int8(String name, String docString, byte defaultValue) {
+            super(name, Type.INT8, docString, true, defaultValue);
+        }
     }
 
     public static class Int32 extends Field {
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/CreateAclsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/CreateAclsRequest.java
index d281b3ba4af..8d6e9054353 100644
--- a/clients/src/main/java/org/apache/kafka/common/requests/CreateAclsRequest.java
+++ b/clients/src/main/java/org/apache/kafka/common/requests/CreateAclsRequest.java
@@ -36,6 +36,7 @@
 import static org.apache.kafka.common.protocol.CommonFields.PERMISSION_TYPE;
 import static org.apache.kafka.common.protocol.CommonFields.PRINCIPAL;
 import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_NAME;
+import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_NAME_TYPE;
 import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_TYPE;
 
 public class CreateAclsRequest extends AbstractRequest {
@@ -50,8 +51,18 @@
                     OPERATION,
                     PERMISSION_TYPE))));
 
+    private static final Schema CREATE_ACLS_REQUEST_V1 = new Schema(
+            new Field(CREATIONS_KEY_NAME, new ArrayOf(new Schema(
+                    RESOURCE_TYPE,
+                    RESOURCE_NAME,
+                    RESOURCE_NAME_TYPE,
+                    PRINCIPAL,
+                    HOST,
+                    OPERATION,
+                    PERMISSION_TYPE))));
+
     public static Schema[] schemaVersions() {
-        return new Schema[]{CREATE_ACLS_REQUEST_V0};
+        return new Schema[]{CREATE_ACLS_REQUEST_V0, CREATE_ACLS_REQUEST_V1};
     }
 
     public static class AclCreation {
@@ -139,6 +150,7 @@ public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable throwable
         short versionId = version();
         switch (versionId) {
             case 0:
+            case 1:
                 List<CreateAclsResponse.AclCreationResponse> responses = new ArrayList<>();
                 for (int i = 0; i < aclCreations.size(); i++)
                     responses.add(new CreateAclsResponse.AclCreationResponse(ApiError.fromThrowable(throwable)));
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/CreateAclsResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/CreateAclsResponse.java
index 6c3579893ce..ac981f8e67e 100644
--- a/clients/src/main/java/org/apache/kafka/common/requests/CreateAclsResponse.java
+++ b/clients/src/main/java/org/apache/kafka/common/requests/CreateAclsResponse.java
@@ -42,8 +42,11 @@
                     ERROR_CODE,
                     ERROR_MESSAGE))));
 
+    // v1 is same as v0, request has additional resource_name_type
+    private static final Schema CREATE_ACLS_RESPONSE_V1 = CREATE_ACLS_RESPONSE_V0;
+
     public static Schema[] schemaVersions() {
-        return new Schema[]{CREATE_ACLS_RESPONSE_V0};
+        return new Schema[]{CREATE_ACLS_RESPONSE_V0, CREATE_ACLS_RESPONSE_V1};
     }
 
     public static class AclCreationResponse {
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsRequest.java
index 2d50ea65cda..1125c6acf5e 100644
--- a/clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsRequest.java
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsRequest.java
@@ -37,6 +37,7 @@
 import static org.apache.kafka.common.protocol.CommonFields.PERMISSION_TYPE;
 import static org.apache.kafka.common.protocol.CommonFields.PRINCIPAL_FILTER;
 import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_NAME_FILTER;
+import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_NAME_TYPE;
 import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_TYPE;
 
 public class DeleteAclsRequest extends AbstractRequest {
@@ -51,8 +52,18 @@
                     OPERATION,
                     PERMISSION_TYPE))));
 
+    private static final Schema DELETE_ACLS_REQUEST_V1 = new Schema(
+            new Field(FILTERS, new ArrayOf(new Schema(
+                    RESOURCE_TYPE,
+                    RESOURCE_NAME_FILTER,
+                    RESOURCE_NAME_TYPE,
+                    PRINCIPAL_FILTER,
+                    HOST_FILTER,
+                    OPERATION,
+                    PERMISSION_TYPE))));
+
     public static Schema[] schemaVersions() {
-        return new Schema[]{DELETE_ACLS_REQUEST_V0};
+        return new Schema[]{DELETE_ACLS_REQUEST_V0, DELETE_ACLS_REQUEST_V1};
     }
 
     public static class Builder extends AbstractRequest.Builder<DeleteAclsRequest> {
@@ -115,10 +126,11 @@ public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable throwable
         short versionId = version();
         switch (versionId) {
             case 0:
+            case 1:
                 List<DeleteAclsResponse.AclFilterResponse> responses = new ArrayList<>();
                 for (int i = 0; i < filters.size(); i++) {
                     responses.add(new DeleteAclsResponse.AclFilterResponse(
-                        ApiError.fromThrowable(throwable), Collections.<DeleteAclsResponse.AclDeletionResult>emptySet()));
+                            ApiError.fromThrowable(throwable), Collections.<DeleteAclsResponse.AclDeletionResult>emptySet()));
                 }
                 return new DeleteAclsResponse(throttleTimeMs, responses);
             default:
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsResponse.java
index 7ae25da2c62..ae16abaf0c5 100644
--- a/clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsResponse.java
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DeleteAclsResponse.java
@@ -43,6 +43,7 @@
 import static org.apache.kafka.common.protocol.CommonFields.PERMISSION_TYPE;
 import static org.apache.kafka.common.protocol.CommonFields.PRINCIPAL;
 import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_NAME;
+import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_NAME_TYPE;
 import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_TYPE;
 import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;
 
@@ -51,7 +52,7 @@
     private final static String FILTER_RESPONSES_KEY_NAME = ""filter_responses"";
     private final static String MATCHING_ACLS_KEY_NAME = ""matching_acls"";
 
-    private static final Schema MATCHING_ACL = new Schema(
+    private static final Schema MATCHING_ACL_V0 = new Schema(
             ERROR_CODE,
             ERROR_MESSAGE,
             RESOURCE_TYPE,
@@ -61,16 +62,35 @@
             OPERATION,
             PERMISSION_TYPE);
 
+    private static final Schema MATCHING_ACL_V1 = new Schema(
+            ERROR_CODE,
+            ERROR_MESSAGE,
+            RESOURCE_TYPE,
+            RESOURCE_NAME,
+            RESOURCE_NAME_TYPE,
+            PRINCIPAL,
+            HOST,
+            OPERATION,
+            PERMISSION_TYPE);
+
     private static final Schema DELETE_ACLS_RESPONSE_V0 = new Schema(
             THROTTLE_TIME_MS,
             new Field(FILTER_RESPONSES_KEY_NAME,
                     new ArrayOf(new Schema(
                             ERROR_CODE,
                             ERROR_MESSAGE,
-                            new Field(MATCHING_ACLS_KEY_NAME, new ArrayOf(MATCHING_ACL), ""The matching ACLs"")))));
+                            new Field(MATCHING_ACLS_KEY_NAME, new ArrayOf(MATCHING_ACL_V0), ""The matching ACLs"")))));
+
+    private static final Schema DELETE_ACLS_RESPONSE_V1 = new Schema(
+            THROTTLE_TIME_MS,
+            new Field(FILTER_RESPONSES_KEY_NAME,
+                    new ArrayOf(new Schema(
+                            ERROR_CODE,
+                            ERROR_MESSAGE,
+                            new Field(MATCHING_ACLS_KEY_NAME, new ArrayOf(MATCHING_ACL_V1), ""The matching ACLs"")))));
 
     public static Schema[] schemaVersions() {
-        return new Schema[]{DELETE_ACLS_RESPONSE_V0};
+        return new Schema[]{DELETE_ACLS_RESPONSE_V0, DELETE_ACLS_RESPONSE_V1};
     }
 
     public static class AclDeletionResult {
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsRequest.java
index 1bacac7c7f5..855153b7ca1 100644
--- a/clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsRequest.java
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsRequest.java
@@ -32,6 +32,7 @@
 import static org.apache.kafka.common.protocol.CommonFields.PERMISSION_TYPE;
 import static org.apache.kafka.common.protocol.CommonFields.PRINCIPAL_FILTER;
 import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_NAME_FILTER;
+import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_NAME_TYPE;
 import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_TYPE;
 
 public class DescribeAclsRequest extends AbstractRequest {
@@ -43,8 +44,17 @@
             OPERATION,
             PERMISSION_TYPE);
 
+    private static final Schema DESCRIBE_ACLS_REQUEST_V1 = new Schema(
+            RESOURCE_TYPE,
+            RESOURCE_NAME_FILTER,
+            RESOURCE_NAME_TYPE,
+            PRINCIPAL_FILTER,
+            HOST_FILTER,
+            OPERATION,
+            PERMISSION_TYPE);
+
     public static Schema[] schemaVersions() {
-        return new Schema[]{DESCRIBE_ACLS_REQUEST_V0};
+        return new Schema[]{DESCRIBE_ACLS_REQUEST_V0, DESCRIBE_ACLS_REQUEST_V1};
     }
 
     public static class Builder extends AbstractRequest.Builder<DescribeAclsRequest> {
@@ -93,6 +103,7 @@ public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable throwable
         short versionId = version();
         switch (versionId) {
             case 0:
+            case 1:
                 return new DescribeAclsResponse(throttleTimeMs, ApiError.fromThrowable(throwable),
                         Collections.<AclBinding>emptySet());
             default:
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsResponse.java
index a21230bfc39..c520f90c13d 100644
--- a/clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsResponse.java
+++ b/clients/src/main/java/org/apache/kafka/common/requests/DescribeAclsResponse.java
@@ -41,6 +41,7 @@
 import static org.apache.kafka.common.protocol.CommonFields.PERMISSION_TYPE;
 import static org.apache.kafka.common.protocol.CommonFields.PRINCIPAL;
 import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_NAME;
+import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_NAME_TYPE;
 import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_TYPE;
 import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;
 
@@ -48,7 +49,7 @@
     private final static String RESOURCES_KEY_NAME = ""resources"";
     private final static String ACLS_KEY_NAME = ""acls"";
 
-    private static final Schema DESCRIBE_ACLS_RESOURCE = new Schema(
+    private static final Schema DESCRIBE_ACLS_RESOURCE_V0 = new Schema(
             RESOURCE_TYPE,
             RESOURCE_NAME,
             new Field(ACLS_KEY_NAME, new ArrayOf(new Schema(
@@ -57,14 +58,30 @@
                     OPERATION,
                     PERMISSION_TYPE))));
 
+    private static final Schema DESCRIBE_ACLS_RESOURCE_V1 = new Schema(
+            RESOURCE_TYPE,
+            RESOURCE_NAME,
+            RESOURCE_NAME_TYPE,
+            new Field(ACLS_KEY_NAME, new ArrayOf(new Schema(
+                    PRINCIPAL,
+                    HOST,
+                    OPERATION,
+                    PERMISSION_TYPE))));
+
     private static final Schema DESCRIBE_ACLS_RESPONSE_V0 = new Schema(
             THROTTLE_TIME_MS,
             ERROR_CODE,
             ERROR_MESSAGE,
-            new Field(RESOURCES_KEY_NAME, new ArrayOf(DESCRIBE_ACLS_RESOURCE), ""The resources and their associated ACLs.""));
+            new Field(RESOURCES_KEY_NAME, new ArrayOf(DESCRIBE_ACLS_RESOURCE_V0), ""The resources and their associated ACLs.""));
+
+    private static final Schema DESCRIBE_ACLS_RESPONSE_V1 = new Schema(
+            THROTTLE_TIME_MS,
+            ERROR_CODE,
+            ERROR_MESSAGE,
+            new Field(RESOURCES_KEY_NAME, new ArrayOf(DESCRIBE_ACLS_RESOURCE_V1), ""The resources and their associated ACLs.""));
 
     public static Schema[] schemaVersions() {
-        return new Schema[]{DESCRIBE_ACLS_RESPONSE_V0};
+        return new Schema[]{DESCRIBE_ACLS_RESPONSE_V0, DESCRIBE_ACLS_RESPONSE_V1};
     }
 
     private final int throttleTimeMs;
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/RequestUtils.java b/clients/src/main/java/org/apache/kafka/common/requests/RequestUtils.java
index a1c27b725b8..a1e4453cdb7 100644
--- a/clients/src/main/java/org/apache/kafka/common/requests/RequestUtils.java
+++ b/clients/src/main/java/org/apache/kafka/common/requests/RequestUtils.java
@@ -24,6 +24,7 @@
 import org.apache.kafka.common.resource.Resource;
 import org.apache.kafka.common.resource.ResourceFilter;
 import org.apache.kafka.common.resource.ResourceType;
+import org.apache.kafka.common.resource.ResourceNameType;
 
 import static org.apache.kafka.common.protocol.CommonFields.HOST;
 import static org.apache.kafka.common.protocol.CommonFields.HOST_FILTER;
@@ -33,6 +34,7 @@
 import static org.apache.kafka.common.protocol.CommonFields.PRINCIPAL_FILTER;
 import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_NAME;
 import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_NAME_FILTER;
+import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_NAME_TYPE;
 import static org.apache.kafka.common.protocol.CommonFields.RESOURCE_TYPE;
 
 final class RequestUtils {
@@ -42,23 +44,27 @@ private RequestUtils() {}
     static Resource resourceFromStructFields(Struct struct) {
         byte resourceType = struct.get(RESOURCE_TYPE);
         String name = struct.get(RESOURCE_NAME);
-        return new Resource(ResourceType.fromCode(resourceType), name);
+        byte resourceNameType = struct.get(RESOURCE_NAME_TYPE);
+        return new Resource(ResourceType.fromCode(resourceType), name, ResourceNameType.fromCode(resourceNameType));
     }
 
     static void resourceSetStructFields(Resource resource, Struct struct) {
         struct.set(RESOURCE_TYPE, resource.resourceType().code());
         struct.set(RESOURCE_NAME, resource.name());
+        struct.set(RESOURCE_NAME_TYPE, resource.resourceNameType().code());
     }
 
     static ResourceFilter resourceFilterFromStructFields(Struct struct) {
         byte resourceType = struct.get(RESOURCE_TYPE);
         String name = struct.get(RESOURCE_NAME_FILTER);
-        return new ResourceFilter(ResourceType.fromCode(resourceType), name);
+        byte resourceNameType = struct.get(RESOURCE_NAME_TYPE);
+        return new ResourceFilter(ResourceType.fromCode(resourceType), name, ResourceNameType.fromCode(resourceNameType));
     }
 
     static void resourceFilterSetStructFields(ResourceFilter resourceFilter, Struct struct) {
         struct.set(RESOURCE_TYPE, resourceFilter.resourceType().code());
         struct.set(RESOURCE_NAME_FILTER, resourceFilter.name());
+        struct.set(RESOURCE_NAME_TYPE, resourceFilter.resourceNameType().code());
     }
 
     static AccessControlEntry aceFromStructFields(Struct struct) {
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/Resource.java b/clients/src/main/java/org/apache/kafka/common/requests/Resource.java
index bd814661ae3..b7cbae65a90 100644
--- a/clients/src/main/java/org/apache/kafka/common/requests/Resource.java
+++ b/clients/src/main/java/org/apache/kafka/common/requests/Resource.java
@@ -20,10 +20,26 @@
 public final class Resource {
     private final ResourceType type;
     private final String name;
+    private final ResourceNameType resourceNameType;
 
-    public Resource(ResourceType type, String name) {
+    /**
+     * @param type resource type
+     * @param name resouce name
+     * @param resourceNameType resource name type
+     */
+    public Resource(ResourceType type, String name, ResourceNameType resourceNameType) {
         this.type = type;
         this.name = name;
+        this.resourceNameType = resourceNameType;
+    }
+
+    /**
+     * Resource name type would default to ResourceNameType.LITERAL.
+     * @param type resource type
+     * @param name resource name type
+     */
+    public Resource(ResourceType type, String name) {
+        this(type, name, ResourceNameType.LITERAL);
     }
 
     public ResourceType type() {
@@ -34,6 +50,10 @@ public String name() {
         return name;
     }
 
+    public ResourceNameType resourceNameType() {
+        return resourceNameType;
+    }
+
     @Override
     public boolean equals(Object o) {
         if (this == o)
@@ -43,18 +63,19 @@ public boolean equals(Object o) {
 
         Resource resource = (Resource) o;
 
-        return type == resource.type && name.equals(resource.name);
+        return type == resource.type && name.equals(resource.name) && resourceNameType.equals(resource.resourceNameType);
     }
 
     @Override
     public int hashCode() {
         int result = type.hashCode();
         result = 31 * result + name.hashCode();
+        result = 31 * result + resourceNameType.hashCode();
         return result;
     }
 
     @Override
     public String toString() {
-        return ""Resource(type="" + type + "", name='"" + name + ""')"";
+        return ""Resource(type="" + type + "", name='"" + name + ""', resourceNameType="" + resourceNameType + "")"";
     }
 }
diff --git a/clients/src/main/java/org/apache/kafka/common/requests/ResourceNameType.java b/clients/src/main/java/org/apache/kafka/common/requests/ResourceNameType.java
new file mode 100644
index 00000000000..142a3f31b22
--- /dev/null
+++ b/clients/src/main/java/org/apache/kafka/common/requests/ResourceNameType.java
@@ -0,0 +1,83 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.kafka.common.requests;
+
+import org.apache.kafka.common.annotation.InterfaceStability;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+@InterfaceStability.Evolving
+public enum ResourceNameType {
+    /**
+     * Represents any ResourceNameType which this client cannot understand,
+     * perhaps because this client is too old.
+     */
+    UNKNOWN((byte) 0),
+
+    /**
+     * In a filter, matches any ResourceNameType.
+     */
+    ANY((byte) 1),
+
+    /**
+     * A Kafka topic.
+     */
+    LITERAL((byte) 2),
+
+    /**
+     * A consumer group.
+     */
+    WILDCARD_SUFFIXED((byte) 3);
+
+    private final static Map<Byte, ResourceNameType> CODE_TO_VALUE;
+
+    static {
+        final Map<Byte, ResourceNameType> codeToValues = new HashMap<>();
+        for (ResourceNameType resourceType : ResourceNameType.values()) {
+            codeToValues.put(resourceType.code, resourceType);
+        }
+        CODE_TO_VALUE = Collections.unmodifiableMap(codeToValues);
+    }
+
+    private final byte code;
+
+    ResourceNameType(byte code) {
+        this.code = code;
+    }
+
+    /**
+     * @return Return code.
+     */
+    public byte code() {
+        return code;
+    }
+
+    /**
+     * Return the ResourceNameType with the provided code or `ResourceNameType.UNKNOWN` if one cannot be found.
+     */
+    public static ResourceNameType fromCode(byte code) {
+        ResourceNameType resourceNameType = CODE_TO_VALUE.get(code);
+        if (resourceNameType == null) {
+            return UNKNOWN;
+        }
+        return resourceNameType;
+    }
+
+}
diff --git a/clients/src/main/java/org/apache/kafka/common/resource/Resource.java b/clients/src/main/java/org/apache/kafka/common/resource/Resource.java
index f41f41a04b6..ebebd9408ca 100644
--- a/clients/src/main/java/org/apache/kafka/common/resource/Resource.java
+++ b/clients/src/main/java/org/apache/kafka/common/resource/Resource.java
@@ -30,6 +30,7 @@
 public class Resource {
     private final ResourceType resourceType;
     private final String name;
+    private final ResourceNameType resourceNameType;
 
     /**
      * The name of the CLUSTER resource.
@@ -46,12 +47,26 @@
      *
      * @param resourceType non-null resource type
      * @param name non-null resource name
+     * @param resourceNameType non-null resource name type
      */
-    public Resource(ResourceType resourceType, String name) {
+    public Resource(ResourceType resourceType, String name, ResourceNameType resourceNameType) {
         Objects.requireNonNull(resourceType);
         this.resourceType = resourceType;
         Objects.requireNonNull(name);
         this.name = name;
+        Objects.requireNonNull(resourceNameType);
+        this.resourceNameType = resourceNameType;
+    }
+
+    /**
+     * Create an instance of this class with the provided parameters.
+     * Resource name type would default to ResourceNameType.LITERAL.
+     *
+     * @param resourceType non-null resource type
+     * @param name non-null resource name
+     */
+    public Resource(ResourceType resourceType, String name) {
+        this(resourceType, name, ResourceNameType.LITERAL);
     }
 
     /**
@@ -61,6 +76,13 @@ public ResourceType resourceType() {
         return resourceType;
     }
 
+    /**
+     * Return the resource name type.
+     */
+    public ResourceNameType resourceNameType() {
+        return resourceNameType;
+    }
+
     /**
      * Return the resource name.
      */
@@ -72,19 +94,19 @@ public String name() {
      * Create a filter which matches only this Resource.
      */
     public ResourceFilter toFilter() {
-        return new ResourceFilter(resourceType, name);
+        return new ResourceFilter(resourceType, name, resourceNameType);
     }
 
     @Override
     public String toString() {
-        return ""(resourceType="" + resourceType + "", name="" + ((name == null) ? ""<any>"" : name) + "")"";
+        return ""(resourceType="" + resourceType + "", name="" + ((name == null) ? ""<any>"" : name) + "", resourceNameType="" + resourceNameType + "")"";
     }
 
     /**
      * Return true if this Resource has any UNKNOWN components.
      */
     public boolean isUnknown() {
-        return resourceType.isUnknown();
+        return resourceType.isUnknown() || resourceNameType.isUnknown();
     }
 
     @Override
@@ -92,11 +114,13 @@ public boolean equals(Object o) {
         if (!(o instanceof Resource))
             return false;
         Resource other = (Resource) o;
-        return resourceType.equals(other.resourceType) && Objects.equals(name, other.name);
+        return resourceType.equals(other.resourceType)
+                && Objects.equals(name, other.name)
+                && resourceNameType.equals(other.resourceNameType);
     }
 
     @Override
     public int hashCode() {
-        return Objects.hash(resourceType, name);
+        return Objects.hash(resourceType, name, resourceNameType);
     }
 }
diff --git a/clients/src/main/java/org/apache/kafka/common/resource/ResourceFilter.java b/clients/src/main/java/org/apache/kafka/common/resource/ResourceFilter.java
index 0a4611f9874..c7e29fa65dd 100644
--- a/clients/src/main/java/org/apache/kafka/common/resource/ResourceFilter.java
+++ b/clients/src/main/java/org/apache/kafka/common/resource/ResourceFilter.java
@@ -21,6 +21,8 @@
 
 import java.util.Objects;
 
+import static org.apache.kafka.common.resource.ResourceUtils.matchResource;
+
 /**
  * A filter which matches Resource objects.
  *
@@ -30,22 +32,37 @@
 public class ResourceFilter {
     private final ResourceType resourceType;
     private final String name;
+    private final ResourceNameType resourceNameType;
 
     /**
      * Matches any resource.
      */
-    public static final ResourceFilter ANY = new ResourceFilter(ResourceType.ANY, null);
+    public static final ResourceFilter ANY = new ResourceFilter(ResourceType.ANY, null, ResourceNameType.ANY);
 
     /**
      * Create an instance of this class with the provided parameters.
+     * Resource name type defaults to ResourceNameType.LITERAL
      *
      * @param resourceType non-null resource type
      * @param name resource name or null
      */
     public ResourceFilter(ResourceType resourceType, String name) {
+        this(resourceType, name, ResourceNameType.LITERAL);
+    }
+
+    /**
+     * Create an instance of this class with the provided parameters.
+     *
+     * @param resourceType non-null resource type
+     * @param name resource name or null
+     * @param resourceNameType non-null resource name type
+     */
+    public ResourceFilter(ResourceType resourceType, String name, ResourceNameType resourceNameType) {
         Objects.requireNonNull(resourceType);
         this.resourceType = resourceType;
         this.name = name;
+        Objects.requireNonNull(resourceNameType);
+        this.resourceNameType = resourceNameType;
     }
 
     /**
@@ -62,16 +79,23 @@ public String name() {
         return name;
     }
 
+    /**
+     * Return the resource name type.
+     */
+    public ResourceNameType resourceNameType() {
+        return resourceNameType;
+    }
+
     @Override
     public String toString() {
-        return ""(resourceType="" + resourceType + "", name="" + ((name == null) ? ""<any>"" : name) + "")"";
+        return ""(resourceType="" + resourceType + "", name="" + ((name == null) ? ""<any>"" : name) + "", resourceNameType="" + resourceNameType + "")"";
     }
 
     /**
      * Return true if this ResourceFilter has any UNKNOWN components.
      */
     public boolean isUnknown() {
-        return resourceType.isUnknown();
+        return resourceType.isUnknown() || resourceNameType.isUnknown();
     }
 
     @Override
@@ -79,23 +103,21 @@ public boolean equals(Object o) {
         if (!(o instanceof ResourceFilter))
             return false;
         ResourceFilter other = (ResourceFilter) o;
-        return resourceType.equals(other.resourceType) && Objects.equals(name, other.name);
+        return resourceType.equals(other.resourceType)
+                && Objects.equals(name, other.name)
+                && Objects.equals(resourceNameType, other.resourceNameType);
     }
 
     @Override
     public int hashCode() {
-        return Objects.hash(resourceType, name);
+        return Objects.hash(resourceType, name, resourceNameType);
     }
 
     /**
      * Return true if this filter matches the given Resource.
      */
     public boolean matches(Resource other) {
-        if ((name != null) && (!name.equals(other.name())))
-            return false;
-        if ((resourceType != ResourceType.ANY) && (!resourceType.equals(other.resourceType())))
-            return false;
-        return true;
+        return matchResource(other, this);
     }
 
     /**
@@ -115,6 +137,10 @@ public String findIndefiniteField() {
             return ""Resource type is UNKNOWN."";
         if (name == null)
             return ""Resource name is NULL."";
+        if (resourceNameType == ResourceNameType.ANY)
+            return ""Resource name type is ANY."";
+        if (resourceNameType == ResourceNameType.UNKNOWN)
+            return ""Resource name type is UNKNOWN."";
         return null;
     }
 }
diff --git a/clients/src/main/java/org/apache/kafka/common/resource/ResourceNameType.java b/clients/src/main/java/org/apache/kafka/common/resource/ResourceNameType.java
new file mode 100644
index 00000000000..623fcf77368
--- /dev/null
+++ b/clients/src/main/java/org/apache/kafka/common/resource/ResourceNameType.java
@@ -0,0 +1,89 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.kafka.common.resource;
+
+import org.apache.kafka.common.annotation.InterfaceStability;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ * Resource name type.
+ */
+@InterfaceStability.Evolving
+public enum ResourceNameType {
+    /**
+     * Represents any ResourceType which this client cannot understand,
+     * perhaps because this client is too old.
+     */
+    UNKNOWN((byte) 0),
+
+    /**
+     * In a filter, matches any ResourceType.
+     */
+    ANY((byte) 1),
+
+    /**
+     * A Kafka topic.
+     */
+    LITERAL((byte) 2),
+
+    /**
+     * A consumer group.
+     */
+    WILDCARD_SUFFIXED((byte) 3);
+
+    private final static Map<Byte, ResourceNameType> CODE_TO_VALUE;
+
+    static {
+        final Map<Byte, ResourceNameType> codeToValues = new HashMap<>();
+        for (ResourceNameType resourceType : ResourceNameType.values()) {
+            codeToValues.put(resourceType.code, resourceType);
+        }
+        CODE_TO_VALUE = Collections.unmodifiableMap(codeToValues);
+    }
+
+    private final byte code;
+
+    ResourceNameType(byte code) {
+        this.code = code;
+    }
+
+    /**
+     * Return the code of this resource.
+     */
+    public byte code() {
+        return code;
+    }
+
+    /**
+     * Return the ResourceNameType with the provided code or `ResourceNameType.UNKNOWN` if one cannot be found.
+     */
+    public static ResourceNameType fromCode(byte code) {
+        return CODE_TO_VALUE.getOrDefault(code, UNKNOWN);
+    }
+
+    /**
+     * Return whether this resource name type is UNKNOWN.
+     */
+    public boolean isUnknown() {
+        return this == UNKNOWN;
+    }
+
+}
diff --git a/clients/src/main/java/org/apache/kafka/common/resource/ResourceUtils.java b/clients/src/main/java/org/apache/kafka/common/resource/ResourceUtils.java
new file mode 100644
index 00000000000..c665a021325
--- /dev/null
+++ b/clients/src/main/java/org/apache/kafka/common/resource/ResourceUtils.java
@@ -0,0 +1,123 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.kafka.common.resource;
+
+public final class ResourceUtils {
+
+    private ResourceUtils() {}
+
+    public static final String WILDCARD_MARKER = ""*"";
+
+    public static boolean matchResource(Resource stored, Resource input) {
+        return matchResource(stored, input.toFilter());
+    }
+
+    public static boolean matchResource(Resource stored, ResourceFilter input) { // TODO matching criteria should be different for delete call?
+        if (!input.resourceType().equals(ResourceType.ANY) && !input.resourceType().equals(stored.resourceType())) {
+            return false;
+        }
+        switch (stored.resourceNameType()) {
+            case LITERAL:
+                switch (input.resourceNameType()) {
+                    case ANY:
+                        return input.name() == null
+                                || input.name().equals(stored.name())
+                                || stored.name().equals(WILDCARD_MARKER);
+                    case LITERAL:
+                        return input.name() == null
+                                || stored.name().equals(input.name())
+                                || stored.name().equals(WILDCARD_MARKER);
+                    case WILDCARD_SUFFIXED:
+                        return false;
+                    default:
+                        return false;
+                }
+            case WILDCARD_SUFFIXED:
+                switch (input.resourceNameType()) {
+                    case ANY:
+                        return input.name() == null
+                                || matchWildcardSuffixedString(stored.name() + WILDCARD_MARKER, input.name())
+                                || stored.name().equals(input.name());
+                    case LITERAL:
+                        return input.name() == null
+                                || matchWildcardSuffixedString(stored.name() + WILDCARD_MARKER, input.name());
+                    case WILDCARD_SUFFIXED:
+                        return stored.name().equals(input.name());
+                    default:
+                        return false;
+                }
+            default:
+                return false;
+        }
+    }
+
+    /**
+     * Returns true if two strings match, both of which might end with a WILDCARD_MARKER (which matches everything).
+     * Examples:
+     *   matchWildcardSuffixedString(""rob"", ""rob"") => true
+     *   matchWildcardSuffixedString(""*"", ""rob"") => true
+     *   matchWildcardSuffixedString(""ro*"", ""rob"") => true
+     *   matchWildcardSuffixedString(""rob"", ""bob"") => false
+     *   matchWildcardSuffixedString(""ro*"", ""bob"") => false
+     *
+     *   matchWildcardSuffixedString(""rob"", ""*"") => true
+     *   matchWildcardSuffixedString(""rob"", ""ro*"") => true
+     *   matchWildcardSuffixedString(""bob"", ""ro*"") => false
+     *
+     *   matchWildcardSuffixedString(""ro*"", ""ro*"") => true
+     *   matchWildcardSuffixedString(""rob*"", ""ro*"") => false
+     *   matchWildcardSuffixedString(""ro*"", ""rob*"") => true
+     *
+     * @param wildcardSuffixedPattern Value stored in ZK in either resource name or Acl.
+     * @param resourceName Value present in the request.
+     * @return true if there is a match (including wildcard-suffix matching).
+     */
+    static boolean matchWildcardSuffixedString(String wildcardSuffixedPattern, String resourceName) { // TODO review this method again after design changes
+
+        if (wildcardSuffixedPattern.equals(resourceName) || wildcardSuffixedPattern.equals(WILDCARD_MARKER) || resourceName.equals(WILDCARD_MARKER)) {
+            // if strings are equal or either of acl or resourceName is a wildcard
+            return true;
+        }
+
+        if (wildcardSuffixedPattern.endsWith(WILDCARD_MARKER)) {
+
+            String aclPrefix = wildcardSuffixedPattern.substring(0, wildcardSuffixedPattern.length() - WILDCARD_MARKER.length());
+
+            if (resourceName.endsWith(WILDCARD_MARKER)) {
+                // when both acl and resourceName ends with wildcard, non-wildcard prefix of resourceName should start with non-wildcard prefix of acl
+                String inputPrefix = resourceName.substring(0, resourceName.length() - WILDCARD_MARKER.length());
+                return inputPrefix.startsWith(aclPrefix);
+            }
+
+            // when acl ends with wildcard but resourceName doesn't, then resourceName should start with non-wildcard prefix of acl
+            return resourceName.startsWith(aclPrefix);
+
+        } else {
+
+            if (resourceName.endsWith(WILDCARD_MARKER)) {
+                // when resourceName ends with wildcard but acl doesn't, then acl should start with non-wildcard prefix of resourceName
+                String inputPrefix = resourceName.substring(0, resourceName.length() - WILDCARD_MARKER.length());
+                return wildcardSuffixedPattern.startsWith(inputPrefix);
+            }
+
+            // when neither acl nor resourceName ends with wildcard, they have to match exactly.
+            return wildcardSuffixedPattern.equals(resourceName);
+
+        }
+    }
+}
diff --git a/clients/src/test/java/org/apache/kafka/common/acl/AclBindingTest.java b/clients/src/test/java/org/apache/kafka/common/acl/AclBindingTest.java
index 0ebcdfedb4f..fac18e87057 100644
--- a/clients/src/test/java/org/apache/kafka/common/acl/AclBindingTest.java
+++ b/clients/src/test/java/org/apache/kafka/common/acl/AclBindingTest.java
@@ -21,8 +21,8 @@
 import org.apache.kafka.common.resource.ResourceType;
 import org.junit.Test;
 
-import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertTrue;
 
 public class AclBindingTest {
@@ -43,11 +43,11 @@
         new AccessControlEntry(""User:ANONYMOUS"", ""127.0.0.1"", AclOperation.UNKNOWN, AclPermissionType.DENY));
 
     private static final AclBindingFilter ANY_ANONYMOUS = new AclBindingFilter(
-        new ResourceFilter(ResourceType.ANY, null),
+        ResourceFilter.ANY,
         new AccessControlEntryFilter(""User:ANONYMOUS"", null, AclOperation.ANY, AclPermissionType.ANY));
 
     private static final AclBindingFilter ANY_DENY = new AclBindingFilter(
-        new ResourceFilter(ResourceType.ANY, null),
+        ResourceFilter.ANY,
         new AccessControlEntryFilter(null, null, AclOperation.ANY, AclPermissionType.DENY));
 
     private static final AclBindingFilter ANY_MYTOPIC = new AclBindingFilter(
@@ -103,9 +103,9 @@ public void testUnknowns() throws Exception {
 
     @Test
     public void testMatchesAtMostOne() throws Exception {
-        assertEquals(null, ACL1.toFilter().findIndefiniteField());
-        assertEquals(null, ACL2.toFilter().findIndefiniteField());
-        assertEquals(null, ACL3.toFilter().findIndefiniteField());
+        assertNull(ACL1.toFilter().findIndefiniteField());
+        assertNull(ACL2.toFilter().findIndefiniteField());
+        assertNull(ACL3.toFilter().findIndefiniteField());
         assertFalse(ANY_ANONYMOUS.matchesAtMostOne());
         assertFalse(ANY_DENY.matchesAtMostOne());
         assertFalse(ANY_MYTOPIC.matchesAtMostOne());
diff --git a/clients/src/test/java/org/apache/kafka/common/resource/ResourceUtilsTest.java b/clients/src/test/java/org/apache/kafka/common/resource/ResourceUtilsTest.java
new file mode 100644
index 00000000000..14429a39ce9
--- /dev/null
+++ b/clients/src/test/java/org/apache/kafka/common/resource/ResourceUtilsTest.java
@@ -0,0 +1,111 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.kafka.common.resource;
+
+import org.junit.Test;
+
+import static org.apache.kafka.common.resource.ResourceUtils.WILDCARD_MARKER;
+import static org.apache.kafka.common.resource.ResourceUtils.matchResource;
+import static org.apache.kafka.common.resource.ResourceUtils.matchWildcardSuffixedString;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+public class ResourceUtilsTest {
+
+    @Test
+    public void testmatchWildcardSuffixedString() {
+        // everything should match wildcard string
+        assertTrue(matchWildcardSuffixedString(WILDCARD_MARKER, WILDCARD_MARKER));
+        assertTrue(matchWildcardSuffixedString(WILDCARD_MARKER, ""f""));
+        assertTrue(matchWildcardSuffixedString(WILDCARD_MARKER, ""foo""));
+        assertTrue(matchWildcardSuffixedString(WILDCARD_MARKER, ""fo"" + WILDCARD_MARKER));
+        assertTrue(matchWildcardSuffixedString(WILDCARD_MARKER, ""f"" + WILDCARD_MARKER));
+
+        assertTrue(matchWildcardSuffixedString(""f"", WILDCARD_MARKER));
+        assertTrue(matchWildcardSuffixedString(""f"", ""f""));
+        assertTrue(matchWildcardSuffixedString(""f"", ""f"" + WILDCARD_MARKER));
+        assertFalse(matchWildcardSuffixedString(""f"", ""foo""));
+        assertFalse(matchWildcardSuffixedString(""f"", ""fo"" + WILDCARD_MARKER));
+
+        assertTrue(matchWildcardSuffixedString(""foo"", WILDCARD_MARKER));
+        assertTrue(matchWildcardSuffixedString(""foo"", ""foo""));
+        assertTrue(matchWildcardSuffixedString(""foo"", ""fo"" + WILDCARD_MARKER));
+        assertTrue(matchWildcardSuffixedString(""foo"", ""f"" + WILDCARD_MARKER));
+        assertTrue(matchWildcardSuffixedString(""foo"", ""foo"" + WILDCARD_MARKER));
+        assertFalse(matchWildcardSuffixedString(""foo"", ""f""));
+        assertFalse(matchWildcardSuffixedString(""foo"", ""foot"" + WILDCARD_MARKER));
+
+        assertTrue(matchWildcardSuffixedString(""fo"" + WILDCARD_MARKER, WILDCARD_MARKER));
+        assertTrue(matchWildcardSuffixedString(""fo"" + WILDCARD_MARKER, ""fo""));
+        assertTrue(matchWildcardSuffixedString(""fo"" + WILDCARD_MARKER, ""foo""));
+        assertTrue(matchWildcardSuffixedString(""fo"" + WILDCARD_MARKER, ""foot""));
+        assertTrue(matchWildcardSuffixedString(""fo"" + WILDCARD_MARKER, ""fo"" + WILDCARD_MARKER));
+
+        assertTrue(matchWildcardSuffixedString(""fo"" + WILDCARD_MARKER, ""foo"" + WILDCARD_MARKER));
+        assertTrue(matchWildcardSuffixedString(""fo"" + WILDCARD_MARKER, ""foot"" + WILDCARD_MARKER));
+        assertFalse(matchWildcardSuffixedString(""fo"" + WILDCARD_MARKER, ""f""));
+        assertFalse(matchWildcardSuffixedString(""fo"" + WILDCARD_MARKER, ""f"" + WILDCARD_MARKER));
+    }
+
+    @Test
+    public void testMatchResource() {
+        // same resource should match
+        assertTrue(
+                matchResource(
+                        new Resource(ResourceType.TOPIC, ""ResourceType.TOPICA""),
+                        new Resource(ResourceType.TOPIC, ""ResourceType.TOPICA"")
+                )
+        );
+
+        // different resource shouldn't match
+        assertFalse(
+                matchResource(
+                        new Resource(ResourceType.TOPIC, ""ResourceType.TOPICA""),
+                        new Resource(ResourceType.TOPIC, ""ResourceType.TOPICB"")
+                )
+        );
+        assertFalse(
+                matchResource(
+                        new Resource(ResourceType.TOPIC, ""ResourceType.TOPICA""),
+                        new Resource(ResourceType.GROUP, ""ResourceType.TOPICA"")
+                )
+        );
+
+        // wildcard resource should match
+        assertTrue(
+                matchResource(
+                        new Resource(ResourceType.TOPIC, WILDCARD_MARKER),
+                        new Resource(ResourceType.TOPIC, ""ResourceType.TOPICA"")
+                )
+        );
+
+        // wildcard-suffix resource should match
+        assertTrue(
+                matchResource(
+                        new Resource(ResourceType.TOPIC, ""ResourceType.TOPIC"", ResourceNameType.WILDCARD_SUFFIXED),
+                        new Resource(ResourceType.TOPIC, ""ResourceType.TOPICA"")
+                )
+        );
+        assertFalse(
+                matchResource(
+                        new Resource(ResourceType.TOPIC, ""ResourceType.TOPIC"", ResourceNameType.WILDCARD_SUFFIXED),
+                        new Resource(ResourceType.TOPIC, ""topiA"")
+                )
+        );
+    }
+}
diff --git a/clients/src/test/java/org/apache/kafka/common/utils/SecurityUtilsTest.java b/clients/src/test/java/org/apache/kafka/common/utils/SecurityUtilsTest.java
index 273c13abbfe..b40fc11660e 100644
--- a/clients/src/test/java/org/apache/kafka/common/utils/SecurityUtilsTest.java
+++ b/clients/src/test/java/org/apache/kafka/common/utils/SecurityUtilsTest.java
@@ -34,7 +34,7 @@ public void testPrincipalNameCanContainSeparator() {
     @Test
     public void testParseKafkaPrincipalWithNonUserPrincipalType() {
         String name = ""foo"";
-        String principalType = ""Group"";
+        String principalType = ""GROUP"";
         KafkaPrincipal principal = SecurityUtils.parseKafkaPrincipal(principalType + "":"" + name);
         assertEquals(principalType, principal.getPrincipalType());
         assertEquals(name, principal.getName());
diff --git a/core/src/main/scala/kafka/security/SecurityUtils.scala b/core/src/main/scala/kafka/security/SecurityUtils.scala
index 573a16b8c58..24af79cb5dd 100644
--- a/core/src/main/scala/kafka/security/SecurityUtils.scala
+++ b/core/src/main/scala/kafka/security/SecurityUtils.scala
@@ -17,7 +17,7 @@
 
 package kafka.security
 
-import kafka.security.auth.{Acl, Operation, PermissionType, Resource, ResourceType}
+import kafka.security.auth.{Acl, Operation, PermissionType, Resource, ResourceNameType, ResourceType}
 import org.apache.kafka.common.acl.{AccessControlEntry, AclBinding, AclBindingFilter}
 import org.apache.kafka.common.protocol.Errors
 import org.apache.kafka.common.requests.ApiError
@@ -32,10 +32,11 @@ object SecurityUtils {
   def convertToResourceAndAcl(filter: AclBindingFilter): Either[ApiError, (Resource, Acl)] = {
     (for {
       resourceType <- Try(ResourceType.fromJava(filter.resourceFilter.resourceType))
+      resourceNameType <- Try(ResourceNameType.fromJava(filter.resourceFilter.resourceNameType))
       principal <- Try(KafkaPrincipal.fromString(filter.entryFilter.principal))
       operation <- Try(Operation.fromJava(filter.entryFilter.operation))
       permissionType <- Try(PermissionType.fromJava(filter.entryFilter.permissionType))
-      resource = Resource(resourceType, filter.resourceFilter.name)
+      resource = Resource(resourceType, filter.resourceFilter.name, resourceNameType)
       acl = Acl(principal, permissionType, filter.entryFilter.host, operation)
     } yield (resource, acl)) match {
       case Failure(throwable) => Left(new ApiError(Errors.INVALID_REQUEST, throwable.getMessage))
@@ -44,7 +45,7 @@ object SecurityUtils {
   }
 
   def convertToAclBinding(resource: Resource, acl: Acl): AclBinding = {
-    val adminResource = new AdminResource(resource.resourceType.toJava, resource.name)
+    val adminResource = new AdminResource(resource.resourceType.toJava, resource.name, resource.resourceNameType.toJava)
     val entry = new AccessControlEntry(acl.principal.toString, acl.host.toString,
       acl.operation.toJava, acl.permissionType.toJava)
     new AclBinding(adminResource, entry)
diff --git a/core/src/main/scala/kafka/security/auth/Acl.scala b/core/src/main/scala/kafka/security/auth/Acl.scala
index 67f3d9592f2..29376f7821f 100644
--- a/core/src/main/scala/kafka/security/auth/Acl.scala
+++ b/core/src/main/scala/kafka/security/auth/Acl.scala
@@ -25,6 +25,7 @@ import scala.collection.JavaConverters._
 object Acl {
   val WildCardPrincipal: KafkaPrincipal = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, ""*"")
   val WildCardHost: String = ""*""
+  val WildCardString: String = ""*""
   val AllowAllAcl = new Acl(WildCardPrincipal, Allow, WildCardHost, All)
   val PrincipalKey = ""principal""
   val PermissionTypeKey = ""permissionType""
diff --git a/core/src/main/scala/kafka/security/auth/Authorizer.scala b/core/src/main/scala/kafka/security/auth/Authorizer.scala
index 6f4ca0eb522..d0672cdf01b 100644
--- a/core/src/main/scala/kafka/security/auth/Authorizer.scala
+++ b/core/src/main/scala/kafka/security/auth/Authorizer.scala
@@ -37,7 +37,7 @@ trait Authorizer extends Configurable {
   /**
    * @param session The session being authenticated.
    * @param operation Type of operation client is trying to perform on resource.
-   * @param resource Resource the client is trying to access.
+   * @param resource Resource the client is trying to access. Resource name type is always literal in input resource.
    * @return true if the operation should be permitted, false otherwise
    */
   def authorize(session: Session, operation: Operation, resource: Resource): Boolean
@@ -60,21 +60,21 @@ trait Authorizer extends Configurable {
 
   /**
    * remove a resource along with all of its acls from acl store.
-   * @param resource
+   * @param resource the resource from which these acls should be removed.
    * @return
    */
   def removeAcls(resource: Resource): Boolean
 
   /**
    * get set of acls for this resource
-   * @param resource
+   * @param resource the resource to which the acls belong.
    * @return empty set if no acls are found, otherwise the acls for the resource.
    */
   def getAcls(resource: Resource): Set[Acl]
 
   /**
    * get the acls for this principal.
-   * @param principal
+   * @param principal principal name.
    * @return empty Map if no acls exist for this principal, otherwise a map of resource -> acls for the principal.
    */
   def getAcls(principal: KafkaPrincipal): Map[Resource, Set[Acl]]
@@ -90,4 +90,3 @@ trait Authorizer extends Configurable {
   def close(): Unit
 
 }
-
diff --git a/core/src/main/scala/kafka/security/auth/Resource.scala b/core/src/main/scala/kafka/security/auth/Resource.scala
index 311f5b5083a..e7ac9c0b549 100644
--- a/core/src/main/scala/kafka/security/auth/Resource.scala
+++ b/core/src/main/scala/kafka/security/auth/Resource.scala
@@ -29,6 +29,9 @@ object Resource {
       case _ => throw new IllegalArgumentException(""expected a string in format ResourceType:ResourceName but got "" + str)
     }
   }
+
+    def apply(resourceType: ResourceType, name: String) = new Resource(resourceType, name, Literal)
+
 }
 
 /**
@@ -36,10 +39,16 @@ object Resource {
  * @param resourceType type of resource.
  * @param name name of the resource, for topic this will be topic name , for group it will be group name. For cluster type
  *             it will be a constant string kafka-cluster.
+ * @param resourceNameType type of resource name: literal, wildcard-suffixed, etc.
  */
-case class Resource(resourceType: ResourceType, name: String) {
+case class Resource(resourceType: ResourceType, name: String, resourceNameType: ResourceNameType) {
+
+  def this(resourceType: ResourceType, name: String) {
+    this(resourceType, name, Literal)
+  }
 
   override def toString: String = {
+    // can't be changed because it will break backward compatibility with acl change notification
     resourceType.name + Resource.Separator + name
   }
 }
diff --git a/core/src/main/scala/kafka/security/auth/ResourceNameType.scala b/core/src/main/scala/kafka/security/auth/ResourceNameType.scala
new file mode 100644
index 00000000000..3765d8deeff
--- /dev/null
+++ b/core/src/main/scala/kafka/security/auth/ResourceNameType.scala
@@ -0,0 +1,47 @@
+/**
+  * Licensed to the Apache Software Foundation (ASF) under one or more
+  * contributor license agreements.  See the NOTICE file distributed with
+  * this work for additional information regarding copyright ownership.
+  * The ASF licenses this file to You under the Apache License, Version 2.0
+  * (the ""License""); you may not use this file except in compliance with
+  * the License.  You may obtain a copy of the License at
+  * <p/>
+  * http://www.apache.org/licenses/LICENSE-2.0
+  * <p/>
+  * Unless required by applicable law or agreed to in writing, software
+  * distributed under the License is distributed on an ""AS IS"" BASIS,
+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  * See the License for the specific language governing permissions and
+  * limitations under the License.
+  */
+
+package kafka.security.auth
+
+import kafka.common.{BaseEnum, KafkaException}
+import org.apache.kafka.common.resource.{ResourceNameType => JResourceNameType}
+
+sealed trait ResourceNameType extends BaseEnum {
+  def toJava: JResourceNameType
+}
+
+case object Literal extends ResourceNameType {
+  val name = ""Literal""
+  val toJava = JResourceNameType.LITERAL
+}
+
+case object WildcardSuffixed extends ResourceNameType {
+  val name = ""WildcardSuffixed""
+  val toJava = JResourceNameType.WILDCARD_SUFFIXED
+}
+
+object ResourceNameType {
+
+  def fromString(resourceNameType: String): ResourceNameType = {
+    val rType = values.find(rType => rType.name.equalsIgnoreCase(resourceNameType))
+    rType.getOrElse(throw new KafkaException(resourceNameType + "" not a valid resourceNameType name. The valid names are "" + values.mkString("","")))
+  }
+
+  def values: Seq[ResourceNameType] = List(Literal, WildcardSuffixed)
+
+  def fromJava(operation: JResourceNameType): ResourceNameType = fromString(operation.toString.replaceAll(""_"", """"))
+}
diff --git a/core/src/main/scala/kafka/security/auth/SimpleAclAuthorizer.scala b/core/src/main/scala/kafka/security/auth/SimpleAclAuthorizer.scala
index c439f5e15af..4426da32fbf 100644
--- a/core/src/main/scala/kafka/security/auth/SimpleAclAuthorizer.scala
+++ b/core/src/main/scala/kafka/security/auth/SimpleAclAuthorizer.scala
@@ -24,10 +24,12 @@ import com.typesafe.scalalogging.Logger
 import kafka.common.{NotificationHandler, ZkNodeChangeNotificationListener}
 import kafka.network.RequestChannel.Session
 import kafka.security.auth.SimpleAclAuthorizer.VersionedAcls
+import kafka.security.auth.storage.AclStore
 import kafka.server.KafkaConfig
 import kafka.utils.CoreUtils.{inReadLock, inWriteLock}
 import kafka.utils._
-import kafka.zk.{AclChangeNotificationSequenceZNode, AclChangeNotificationZNode, KafkaZkClient}
+import kafka.zk.KafkaZkClient
+import org.apache.kafka.common.resource.ResourceUtils
 import org.apache.kafka.common.security.auth.KafkaPrincipal
 import org.apache.kafka.common.utils.{SecurityUtils, Time}
 
@@ -54,8 +56,9 @@ class SimpleAclAuthorizer extends Authorizer with Logging {
   private val authorizerLogger = Logger(""kafka.authorizer.logger"")
   private var superUsers = Set.empty[KafkaPrincipal]
   private var shouldAllowEveryoneIfNoAclIsFound = false
-  private var zkClient: KafkaZkClient = null
-  private var aclChangeListener: ZkNodeChangeNotificationListener = null
+  private var zkClient: KafkaZkClient = _
+  private var aclChangeListener: ZkNodeChangeNotificationListener = _
+  private var wildcardSuffixedAclChangeListener: ZkNodeChangeNotificationListener = _
 
   private val aclCache = new scala.collection.mutable.HashMap[Resource, VersionedAcls]
   private val lock = new ReentrantReadWriteLock()
@@ -97,14 +100,16 @@ class SimpleAclAuthorizer extends Authorizer with Logging {
 
     loadCache()
 
-    aclChangeListener = new ZkNodeChangeNotificationListener(zkClient, AclChangeNotificationZNode.path, AclChangeNotificationSequenceZNode.SequenceNumberPrefix, AclChangedNotificationHandler)
+    aclChangeListener = new ZkNodeChangeNotificationListener(zkClient, AclStore.literalAclStore.aclChangesZNode.path, AclStore.literalAclStore.aclChangeNotificationSequenceZNode.SequenceNumberPrefix, AclChangedNotificationHandler)
     aclChangeListener.init()
+    wildcardSuffixedAclChangeListener = new ZkNodeChangeNotificationListener(zkClient, AclStore.wildcardSuffixedAclStore.aclChangesZNode.path, AclStore.wildcardSuffixedAclStore.aclChangeNotificationSequenceZNode.SequenceNumberPrefix, AclChangedNotificationHandler)
+    wildcardSuffixedAclChangeListener.init()
   }
 
   override def authorize(session: Session, operation: Operation, resource: Resource): Boolean = {
     val principal = session.principal
     val host = session.clientAddress.getHostAddress
-    val acls = getAcls(resource) ++ getAcls(new Resource(resource.resourceType, Resource.WildCardResource))
+    val acls = getMatchingAcls(resource)
 
     // Check if there is any Deny acl match that would disallow this operation.
     val denyMatch = aclMatch(operation, resource, principal, host, Deny, acls)
@@ -143,18 +148,28 @@ class SimpleAclAuthorizer extends Authorizer with Logging {
     } else false
   }
 
-  private def aclMatch(operations: Operation, resource: Resource, principal: KafkaPrincipal, host: String, permissionType: PermissionType, acls: Set[Acl]): Boolean = {
+  private def aclMatch(operation: Operation, resource: Resource, principal: KafkaPrincipal, host: String, permissionType: PermissionType, acls: Set[Acl]): Boolean = {
     acls.find { acl =>
       acl.permissionType == permissionType &&
-        (acl.principal == principal || acl.principal == Acl.WildCardPrincipal) &&
-        (operations == acl.operation || acl.operation == All) &&
+        matchPrincipal(acl.principal, principal) &&
+        (operation == acl.operation || acl.operation == All) &&
         (acl.host == host || acl.host == Acl.WildCardHost)
     }.exists { acl =>
-      authorizerLogger.debug(s""operation = $operations on resource = $resource from host = $host is $permissionType based on acl = $acl"")
+      authorizerLogger.debug(s""operation = $operation on resource = $resource from host = $host is $permissionType based on acl = $acl"")
       true
     }
   }
 
+  /**
+    * @param valueInAcl KafkaPrincipal value present in Acl.
+    * @param input KafkaPrincipal present in the request.
+    * @return true if there is a match (including wildcard-suffix matching).
+    */
+  def matchPrincipal(valueInAcl: KafkaPrincipal, input: KafkaPrincipal): Boolean = {
+    (valueInAcl.getPrincipalType == input.getPrincipalType || valueInAcl.getPrincipalType == Acl.WildCardString) &&
+      (valueInAcl.getName.equals(input.getName) || valueInAcl.getName.equals(Acl.WildCardString))
+  }
+
   override def addAcls(acls: Set[Acl], resource: Resource) {
     if (acls != null && acls.nonEmpty) {
       inWriteLock(lock) {
@@ -191,13 +206,22 @@ class SimpleAclAuthorizer extends Authorizer with Logging {
   override def getAcls(principal: KafkaPrincipal): Map[Resource, Set[Acl]] = {
     inReadLock(lock) {
       aclCache.mapValues { versionedAcls =>
-        versionedAcls.acls.filter(_.principal == principal)
+        versionedAcls.acls.filter(acl => matchPrincipal(acl.principal, principal))
       }.filter { case (_, acls) =>
         acls.nonEmpty
       }.toMap
     }
   }
 
+  private def getMatchingAcls(resource: Resource): Set[Acl] = {
+    inReadLock(lock) {
+      aclCache.filterKeys(stored => ResourceUtils.matchResource(
+        new org.apache.kafka.common.resource.Resource(stored.resourceType.toJava, stored.name, stored.resourceNameType.toJava),
+        new org.apache.kafka.common.resource.Resource(resource.resourceType.toJava, resource.name, resource.resourceNameType.toJava)
+      )).flatMap(_._2.acls).toSet
+    }
+  }
+
   override def getAcls(): Map[Resource, Set[Acl]] = {
     inReadLock(lock) {
       aclCache.mapValues(_.acls).toMap
@@ -206,20 +230,23 @@ class SimpleAclAuthorizer extends Authorizer with Logging {
 
   def close() {
     if (aclChangeListener != null) aclChangeListener.close()
+    if (wildcardSuffixedAclChangeListener != null) wildcardSuffixedAclChangeListener.close()
     if (zkClient != null) zkClient.close()
   }
 
-  private def loadCache()  {
+  private def loadCache() {
     inWriteLock(lock) {
-      val resourceTypes = zkClient.getResourceTypes()
-      for (rType <- resourceTypes) {
-        val resourceType = ResourceType.fromString(rType)
-        val resourceNames = zkClient.getResourceNames(resourceType.name)
-        for (resourceName <- resourceNames) {
-          val versionedAcls = getAclsFromZk(Resource(resourceType, resourceName))
-          updateCache(new Resource(resourceType, resourceName), versionedAcls)
+      AclStore.AclStores.foreach(aclStore => {
+        val resourceTypes = zkClient.getResourceTypes(aclStore)
+        for (rType <- resourceTypes) {
+          val resourceType = ResourceType.fromString(rType)
+          val resourceNames = zkClient.getResourceNames(aclStore, resourceType)
+          for (resourceName <- resourceNames) {
+            val versionedAcls = getAclsFromZk(new Resource(resourceType, resourceName, aclStore.resourceNameType))
+            updateCache(new Resource(resourceType, resourceName, aclStore.resourceNameType), versionedAcls)
+          }
         }
-      }
+      })
     }
   }
 
@@ -305,7 +332,7 @@ class SimpleAclAuthorizer extends Authorizer with Logging {
   }
 
   private def updateAclChangedFlag(resource: Resource) {
-    zkClient.createAclChangeNotification(resource.toString)
+    zkClient.createAclChangeNotification(AclStore.fromResource(resource), resource.toString)
   }
 
   private def backoffTime = {
diff --git a/core/src/main/scala/kafka/security/auth/storage/AclStore.scala b/core/src/main/scala/kafka/security/auth/storage/AclStore.scala
new file mode 100644
index 00000000000..c77b04b72e5
--- /dev/null
+++ b/core/src/main/scala/kafka/security/auth/storage/AclStore.scala
@@ -0,0 +1,104 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package kafka.security.auth.storage
+
+import java.nio.charset.StandardCharsets.UTF_8
+
+import kafka.security.auth.SimpleAclAuthorizer.VersionedAcls
+import kafka.security.auth._
+import kafka.utils.{Json, ZkUtils}
+import org.apache.zookeeper.data.Stat
+
+import scala.collection.JavaConverters._
+import scala.collection.Seq
+
+/**
+  * Acl Store.
+  */
+class AclStore(aclZNod: AclZNode, aclChangesZNod: AclChangesZNode, resourceNameTyp: ResourceNameType) {
+  def aclZNode: AclZNode = aclZNod
+  def aclChangesZNode: AclChangesZNode = aclChangesZNod
+  def resourceNameType: ResourceNameType = resourceNameTyp
+  def resourceTypeZNode = ResourceTypeZNode(aclZNode)
+  def resourceZNode = ResourceZNode(aclZNode)
+  def aclChangeNotificationSequenceZNode = AclChangeNotificationSequenceZNode(aclChangesZNode)
+}
+
+case class AclZNode(nodePath: String) {
+  def path: String = nodePath
+}
+
+case class AclChangesZNode(nodePath: String) {
+  def path: String = nodePath
+}
+
+case class ResourceTypeZNode(aclZNode: AclZNode) {
+  def path(resourceType: ResourceType) = s""${aclZNode.path}/${resourceType.name}""
+}
+
+case class ResourceZNode(aclZNode: AclZNode) {
+  def path(resource: Resource) = s""${aclZNode.path}/${resource.resourceType.name}/${resource.name}""
+  def encode(acls: Set[Acl]): Array[Byte] = Json.encodeAsBytes(Acl.toJsonCompatibleMap(acls).asJava)
+  def decode(bytes: Array[Byte], stat: Stat): VersionedAcls = VersionedAcls(Acl.fromBytes(bytes), stat.getVersion)
+}
+
+case class AclChangeNotificationSequenceZNode(aclChangesZNode: AclChangesZNode) {
+  def SequenceNumberPrefix = ""acl_changes_""
+  def createPath = s""${aclChangesZNode.path}/$SequenceNumberPrefix""
+  def deletePath(sequenceNode: String) = s""${aclChangesZNode.path}/$sequenceNode""
+  def encode(resourceName: String): Array[Byte] = resourceName.getBytes(UTF_8)
+  def decode(bytes: Array[Byte]): String = new String(bytes, UTF_8)
+}
+
+object AclStore {
+  val literalAclStore: AclStore = new AclStore(
+    AclZNode(
+      /**
+        * The root acl storage node. Under this node there will be one child node per resource type (Topic, Cluster, Group).
+        * under each resourceType there will be a unique child for each resource instance and the data for that child will contain
+        * list of its acls as a json object. Following gives an example:
+        *
+        * <pre>
+        * /kafka-acl/Topic/topic-1 => {""version"": 1, ""acls"": [ { ""host"":""host1"", ""permissionType"": ""Allow"",""operation"": ""Read"",""principal"": ""User:alice""}]}
+        * /kafka-acl/Cluster/kafka-cluster => {""version"": 1, ""acls"": [ { ""host"":""host1"", ""permissionType"": ""Allow"",""operation"": ""Read"",""principal"": ""User:alice""}]}
+        * /kafka-acl/Group/group-1 => {""version"": 1, ""acls"": [ { ""host"":""host1"", ""permissionType"": ""Allow"",""operation"": ""Read"",""principal"": ""User:alice""}]}
+        * </pre>
+        */
+      ZkUtils.KafkaAclPath),
+    AclChangesZNode(
+      /**
+        * Notification node which gets updated with the resource name when acl on a resource is changed.
+        */
+      ZkUtils.KafkaAclChangesPath),
+    Literal
+  )
+
+  val wildcardSuffixedAclStore: AclStore = new AclStore(
+    AclZNode(ZkUtils.KafkaWildcardSuffixedAclPath),
+    AclChangesZNode(ZkUtils.KafkaWildcardSuffixedAclChangesPath),
+    WildcardSuffixed
+  )
+
+  val AclStores = Seq(literalAclStore, wildcardSuffixedAclStore)
+
+  def fromResource(resource: Resource): AclStore = {
+    AclStores.find(_.resourceNameType.equals(resource.resourceNameType)).getOrElse(
+      throw new IllegalArgumentException(""Unsupported resource name type: "" + resource.resourceNameType)
+    )
+  }
+}
diff --git a/core/src/main/scala/kafka/server/KafkaApis.scala b/core/src/main/scala/kafka/server/KafkaApis.scala
index f86026d9f95..971b62f7510 100644
--- a/core/src/main/scala/kafka/server/KafkaApis.scala
+++ b/core/src/main/scala/kafka/server/KafkaApis.scala
@@ -1867,7 +1867,7 @@ class KafkaApis(val requestChannel: RequestChannel,
         val filter = describeAclsRequest.filter()
         val returnedAcls = auth.getAcls.toSeq.flatMap { case (resource, acls) =>
           acls.flatMap { acl =>
-            val fixture = new AclBinding(new AdminResource(resource.resourceType.toJava, resource.name),
+            val fixture = new AclBinding(new AdminResource(resource.resourceType.toJava, resource.name, resource.resourceNameType.toJava),
                 new AccessControlEntry(acl.principal.toString, acl.host.toString, acl.operation.toJava, acl.permissionType.toJava))
             if (filter.matches(fixture)) Some(fixture)
             else None
@@ -1941,7 +1941,7 @@ class KafkaApis(val requestChannel: RequestChannel,
           val filtersWithIndex = filters.zipWithIndex
           for ((resource, acls) <- aclMap; acl <- acls) {
             val binding = new AclBinding(
-              new AdminResource(resource.resourceType.toJava, resource.name),
+              new AdminResource(resource.resourceType.toJava, resource.name, resource.resourceNameType.toJava),
               new AccessControlEntry(acl.principal.toString, acl.host.toString, acl.operation.toJava,
                 acl.permissionType.toJava))
 
diff --git a/core/src/main/scala/kafka/utils/ZkUtils.scala b/core/src/main/scala/kafka/utils/ZkUtils.scala
index 0c162434541..b8e83dc8524 100644
--- a/core/src/main/scala/kafka/utils/ZkUtils.scala
+++ b/core/src/main/scala/kafka/utils/ZkUtils.scala
@@ -55,6 +55,8 @@ object ZkUtils {
   val LogDirEventNotificationPath = ""/log_dir_event_notification""
   val KafkaAclPath = ""/kafka-acl""
   val KafkaAclChangesPath = ""/kafka-acl-changes""
+  val KafkaWildcardSuffixedAclPath = ""/kafka-wildcard-acl""
+  val KafkaWildcardSuffixedAclChangesPath = ""/kafka-wildcard-acl-changes""
 
   val ConsumersPath = ""/consumers""
   val ClusterIdPath = s""$ClusterPath/id""
diff --git a/core/src/main/scala/kafka/zk/KafkaZkClient.scala b/core/src/main/scala/kafka/zk/KafkaZkClient.scala
index 0cf158ecdad..d1d7805a255 100644
--- a/core/src/main/scala/kafka/zk/KafkaZkClient.scala
+++ b/core/src/main/scala/kafka/zk/KafkaZkClient.scala
@@ -26,6 +26,7 @@ import kafka.controller.LeaderIsrAndControllerEpoch
 import kafka.log.LogConfig
 import kafka.metrics.KafkaMetricsGroup
 import kafka.security.auth.SimpleAclAuthorizer.VersionedAcls
+import kafka.security.auth.storage.AclStore
 import kafka.security.auth.{Acl, Resource, ResourceType}
 import kafka.server.ConfigType
 import kafka.utils.Logging
@@ -943,9 +944,11 @@ class KafkaZkClient private (zooKeeperClient: ZooKeeperClient, isSecure: Boolean
    * Creates the required zk nodes for Acl storage
    */
   def createAclPaths(): Unit = {
-    createRecursive(AclZNode.path, throwIfPathExists = false)
-    createRecursive(AclChangeNotificationZNode.path, throwIfPathExists = false)
-    ResourceType.values.foreach(resource => createRecursive(ResourceTypeZNode.path(resource.name), throwIfPathExists = false))
+    AclStore.AclStores.foreach(aclStore => {
+      createRecursive(aclStore.aclZNode.path, throwIfPathExists = false)
+      createRecursive(aclStore.aclChangesZNode.path, throwIfPathExists = false)
+      ResourceType.values.foreach(resourceType => createRecursive(aclStore.resourceTypeZNode.path(resourceType), throwIfPathExists = false))
+    })
   }
 
   /**
@@ -954,10 +957,11 @@ class KafkaZkClient private (zooKeeperClient: ZooKeeperClient, isSecure: Boolean
    * @return  VersionedAcls
    */
   def getVersionedAclsForResource(resource: Resource): VersionedAcls = {
-    val getDataRequest = GetDataRequest(ResourceZNode.path(resource))
+    val resourceZNode = AclStore.fromResource(resource).resourceZNode
+    val getDataRequest = GetDataRequest(resourceZNode.path(resource))
     val getDataResponse = retryRequestUntilConnected(getDataRequest)
     getDataResponse.resultCode match {
-      case Code.OK => ResourceZNode.decode(getDataResponse.data, getDataResponse.stat)
+      case Code.OK => resourceZNode.decode(getDataResponse.data, getDataResponse.stat)
       case Code.NONODE => VersionedAcls(Set(), -1)
       case _ => throw getDataResponse.resultException.get
     }
@@ -972,18 +976,19 @@ class KafkaZkClient private (zooKeeperClient: ZooKeeperClient, isSecure: Boolean
    * @return true if the update was successful and the new version
    */
   def conditionalSetOrCreateAclsForResource(resource: Resource, aclsSet: Set[Acl], expectedVersion: Int): (Boolean, Int) = {
+    val resourceZNode = AclStore.fromResource(resource).resourceZNode
     def set(aclData: Array[Byte],  expectedVersion: Int): SetDataResponse = {
-      val setDataRequest = SetDataRequest(ResourceZNode.path(resource), aclData, expectedVersion)
+      val setDataRequest = SetDataRequest(resourceZNode.path(resource), aclData, expectedVersion)
       retryRequestUntilConnected(setDataRequest)
     }
 
     def create(aclData: Array[Byte]): CreateResponse = {
-      val path = ResourceZNode.path(resource)
+      val path = resourceZNode.path(resource)
       val createRequest = CreateRequest(path, aclData, acls(path), CreateMode.PERSISTENT)
       retryRequestUntilConnected(createRequest)
     }
 
-    val aclData = ResourceZNode.encode(aclsSet)
+    val aclData = resourceZNode.encode(aclsSet)
 
     val setDataResponse = set(aclData, expectedVersion)
     setDataResponse.resultCode match {
@@ -1003,11 +1008,13 @@ class KafkaZkClient private (zooKeeperClient: ZooKeeperClient, isSecure: Boolean
 
   /**
    * Creates Acl change notification message
+   * @param aclStore acl store
    * @param resourceName resource name
    */
-  def createAclChangeNotification(resourceName: String): Unit = {
-    val path = AclChangeNotificationSequenceZNode.createPath
-    val createRequest = CreateRequest(path, AclChangeNotificationSequenceZNode.encode(resourceName), acls(path), CreateMode.PERSISTENT_SEQUENTIAL)
+  def createAclChangeNotification(aclStore: AclStore, resourceName: String): Unit = {
+    val aclChangeNotificationSequenceZNode = aclStore.aclChangeNotificationSequenceZNode
+    val path = aclChangeNotificationSequenceZNode.createPath
+    val createRequest = CreateRequest(path, aclChangeNotificationSequenceZNode.encode(resourceName), acls(path), CreateMode.PERSISTENT_SEQUENTIAL)
     val createResponse = retryRequestUntilConnected(createRequest)
     createResponse.maybeThrow
   }
@@ -1030,21 +1037,23 @@ class KafkaZkClient private (zooKeeperClient: ZooKeeperClient, isSecure: Boolean
    * @throws KeeperException if there is an error while deleting Acl change notifications
    */
   def deleteAclChangeNotifications(): Unit = {
-    val getChildrenResponse = retryRequestUntilConnected(GetChildrenRequest(AclChangeNotificationZNode.path))
-    if (getChildrenResponse.resultCode == Code.OK) {
-      deleteAclChangeNotifications(getChildrenResponse.children)
-    } else if (getChildrenResponse.resultCode != Code.NONODE) {
-      getChildrenResponse.maybeThrow
-    }
+    AclStore.AclStores.foreach(aclStore => {
+      val getChildrenResponse = retryRequestUntilConnected(GetChildrenRequest(aclStore.aclChangesZNode.path))
+      if (getChildrenResponse.resultCode == Code.OK) {
+        deleteAclChangeNotifications(getChildrenResponse.children, aclStore)
+      } else if (getChildrenResponse.resultCode != Code.NONODE) {
+        getChildrenResponse.maybeThrow
+      }
+    })
   }
 
   /**
    * Deletes the Acl change notifications associated with the given sequence nodes
    * @param sequenceNodes
    */
-  private def deleteAclChangeNotifications(sequenceNodes: Seq[String]): Unit = {
+  private def deleteAclChangeNotifications(sequenceNodes: Seq[String], aclStore: AclStore): Unit = {
     val deleteRequests = sequenceNodes.map { sequenceNode =>
-      DeleteRequest(AclChangeNotificationSequenceZNode.deletePath(sequenceNode), ZkVersion.NoVersion)
+      DeleteRequest(aclStore.aclChangeNotificationSequenceZNode.deletePath(sequenceNode), ZkVersion.NoVersion)
     }
 
     val deleteResponses = retryRequestsUntilConnected(deleteRequests)
@@ -1059,17 +1068,18 @@ class KafkaZkClient private (zooKeeperClient: ZooKeeperClient, isSecure: Boolean
    * Gets the resource types
    * @return list of resource type names
    */
-  def getResourceTypes(): Seq[String] = {
-    getChildren(AclZNode.path)
+  def getResourceTypes(aclStore: AclStore): Seq[String] = {
+    getChildren(aclStore.aclZNode.path)
   }
 
   /**
    * Gets the resource names for a give resource type
-   * @param resourceType
+   * @param aclStore Acl store.
+   * @param resourceType Resource type.
    * @return list of resource names
    */
-  def getResourceNames(resourceType: String): Seq[String] = {
-    getChildren(ResourceTypeZNode.path(resourceType))
+  def getResourceNames(aclStore: AclStore, resourceType: ResourceType): Seq[String] = {
+    getChildren(aclStore.resourceTypeZNode.path(resourceType))
   }
 
   /**
@@ -1078,7 +1088,7 @@ class KafkaZkClient private (zooKeeperClient: ZooKeeperClient, isSecure: Boolean
    * @return delete status
    */
   def deleteResource(resource: Resource): Boolean = {
-    deleteRecursive(ResourceZNode.path(resource))
+    deleteRecursive(AclStore.fromResource(resource).resourceZNode.path(resource))
   }
 
   /**
@@ -1087,7 +1097,7 @@ class KafkaZkClient private (zooKeeperClient: ZooKeeperClient, isSecure: Boolean
    * @return existence status
    */
   def resourceExists(resource: Resource): Boolean = {
-    pathExists(ResourceZNode.path(resource))
+    pathExists(AclStore.fromResource(resource).resourceZNode.path(resource))
   }
 
   /**
@@ -1097,7 +1107,7 @@ class KafkaZkClient private (zooKeeperClient: ZooKeeperClient, isSecure: Boolean
    * @return return true if it succeeds, false otherwise (the current version is not the expected version)
    */
   def conditionalDelete(resource: Resource, expectedVersion: Int): Boolean = {
-    val deleteRequest = DeleteRequest(ResourceZNode.path(resource), expectedVersion)
+    val deleteRequest = DeleteRequest(AclStore.fromResource(resource).resourceZNode.path(resource), expectedVersion)
     val deleteResponse = retryRequestUntilConnected(deleteRequest)
     deleteResponse.resultCode match {
       case Code.OK | Code.NONODE => true
diff --git a/core/src/main/scala/kafka/zk/ZkData.scala b/core/src/main/scala/kafka/zk/ZkData.scala
index 64aed564fd4..2502aff43a8 100644
--- a/core/src/main/scala/kafka/zk/ZkData.scala
+++ b/core/src/main/scala/kafka/zk/ZkData.scala
@@ -25,8 +25,6 @@ import kafka.api.{ApiVersion, KAFKA_0_10_0_IV1, LeaderAndIsr}
 import kafka.cluster.{Broker, EndPoint}
 import kafka.common.KafkaException
 import kafka.controller.{IsrChangeNotificationHandler, LeaderIsrAndControllerEpoch}
-import kafka.security.auth.SimpleAclAuthorizer.VersionedAcls
-import kafka.security.auth.{Acl, Resource}
 import kafka.server.{ConfigType, DelegationTokenManager}
 import kafka.utils.Json
 import org.apache.kafka.common.TopicPartition
@@ -444,45 +442,6 @@ object StateChangeHandlers {
   def zkNodeChangeListenerHandler(seqNodeRoot: String) = s""change-notification-$seqNodeRoot""
 }
 
-/**
- * The root acl storage node. Under this node there will be one child node per resource type (Topic, Cluster, Group).
- * under each resourceType there will be a unique child for each resource instance and the data for that child will contain
- * list of its acls as a json object. Following gives an example:
- *
- * <pre>
- * /kafka-acl/Topic/topic-1 => {""version"": 1, ""acls"": [ { ""host"":""host1"", ""permissionType"": ""Allow"",""operation"": ""Read"",""principal"": ""User:alice""}]}
- * /kafka-acl/Cluster/kafka-cluster => {""version"": 1, ""acls"": [ { ""host"":""host1"", ""permissionType"": ""Allow"",""operation"": ""Read"",""principal"": ""User:alice""}]}
- * /kafka-acl/Group/group-1 => {""version"": 1, ""acls"": [ { ""host"":""host1"", ""permissionType"": ""Allow"",""operation"": ""Read"",""principal"": ""User:alice""}]}
- * </pre>
- */
-object AclZNode {
-  def path = ""/kafka-acl""
-}
-
-object ResourceTypeZNode {
-  def path(resourceType: String) = s""${AclZNode.path}/$resourceType""
-}
-
-object ResourceZNode {
-  def path(resource: Resource) = s""${AclZNode.path}/${resource.resourceType}/${resource.name}""
-  def encode(acls: Set[Acl]): Array[Byte] = {
-    Json.encodeAsBytes(Acl.toJsonCompatibleMap(acls).asJava)
-  }
-  def decode(bytes: Array[Byte], stat: Stat): VersionedAcls = VersionedAcls(Acl.fromBytes(bytes), stat.getVersion)
-}
-
-object AclChangeNotificationZNode {
-  def path = ""/kafka-acl-changes""
-}
-
-object AclChangeNotificationSequenceZNode {
-  val SequenceNumberPrefix = ""acl_changes_""
-  def createPath = s""${AclChangeNotificationZNode.path}/$SequenceNumberPrefix""
-  def deletePath(sequenceNode: String) = s""${AclChangeNotificationZNode.path}/${sequenceNode}""
-  def encode(resourceName : String): Array[Byte] = resourceName.getBytes(UTF_8)
-  def decode(bytes: Array[Byte]): String = new String(bytes, UTF_8)
-}
-
 object ClusterZNode {
   def path = ""/cluster""
 }
@@ -535,6 +494,14 @@ object DelegationTokenInfoZNode {
   def decode(bytes: Array[Byte]): Option[TokenInformation] = DelegationTokenManager.fromBytes(bytes)
 }
 
+// need to be duplicated
+object AclZNodesInfo {
+  val KafkaAclPath = ""/kafka-acl""
+  val KafkaAclChangesPath = ""/kafka-acl-changes""
+  val KafkaWildcardSuffixedAclPath = ""/kafka-wildcard-acl""
+  val KafkaWildcardSuffixedAclChangesPath = ""/kafka-wildcard-acl-changes""
+}
+
 object ZkData {
 
   // Important: it is necessary to add any new top level Zookeeper path to the Seq
@@ -545,8 +512,10 @@ object ZkData {
     ControllerZNode.path,
     ControllerEpochZNode.path,
     IsrChangeNotificationZNode.path,
-    AclZNode.path,
-    AclChangeNotificationZNode.path,
+    AclZNodesInfo.KafkaAclPath,
+    AclZNodesInfo.KafkaAclChangesPath,
+    AclZNodesInfo.KafkaWildcardSuffixedAclPath,
+    AclZNodesInfo.KafkaWildcardSuffixedAclChangesPath,
     ProducerIdBlockZNode.path,
     LogDirEventNotificationZNode.path,
     DelegationTokenAuthZNode.path)
diff --git a/core/src/test/scala/integration/kafka/api/SaslSslAdminClientIntegrationTest.scala b/core/src/test/scala/integration/kafka/api/SaslSslAdminClientIntegrationTest.scala
index 099af5247d3..23085007784 100644
--- a/core/src/test/scala/integration/kafka/api/SaslSslAdminClientIntegrationTest.scala
+++ b/core/src/test/scala/integration/kafka/api/SaslSslAdminClientIntegrationTest.scala
@@ -98,6 +98,8 @@ class SaslSslAdminClientIntegrationTest extends AdminClientIntegrationTest with
     new AccessControlEntry(""User:ANONYMOUS"", ""*"", AclOperation.WRITE, AclPermissionType.ALLOW))
   val groupAcl = new AclBinding(new Resource(ResourceType.GROUP, ""*""),
     new AccessControlEntry(""User:*"", ""*"", AclOperation.ALL, AclPermissionType.ALLOW))
+  val userAcl = new AclBinding(new Resource(ResourceType.TOPIC, ""*""),
+    new AccessControlEntry(""User:*"", ""*"", AclOperation.ALL, AclPermissionType.ALLOW))
 
   @Test
   override def testAclOperations(): Unit = {
@@ -145,7 +147,7 @@ class SaslSslAdminClientIntegrationTest extends AdminClientIntegrationTest with
     assertEquals(Set(filterA, filterB, filterC), results2.values.keySet.asScala)
     assertEquals(Set(groupAcl), results2.values.get(filterA).get.values.asScala.map(_.binding).toSet)
     assertEquals(Set(transactionalIdAcl), results2.values.get(filterC).get.values.asScala.map(_.binding).toSet)
-    assertEquals(Set(acl2), results2.values.get(filterB).get.values.asScala.map(_.binding).toSet)
+    assertEquals(Set(acl2, userAcl), results2.values.get(filterB).get.values.asScala.map(_.binding).toSet)
 
     waitForDescribeAcls(client, filterB, Set())
     waitForDescribeAcls(client, filterC, Set())
@@ -224,8 +226,6 @@ class SaslSslAdminClientIntegrationTest extends AdminClientIntegrationTest with
 
   private def testAclGet(expectAuth: Boolean): Unit = {
     TestUtils.waitUntilTrue(() => {
-      val userAcl = new AclBinding(new Resource(ResourceType.TOPIC, ""*""),
-        new AccessControlEntry(""User:*"", ""*"", AclOperation.ALL, AclPermissionType.ALLOW))
       val results = client.describeAcls(userAcl.toFilter)
       if (expectAuth) {
         Try(results.values.get) match {
diff --git a/core/src/test/scala/unit/kafka/common/ZkNodeChangeNotificationListenerTest.scala b/core/src/test/scala/unit/kafka/common/ZkNodeChangeNotificationListenerTest.scala
index e46bd9b726f..e351f3f7b76 100644
--- a/core/src/test/scala/unit/kafka/common/ZkNodeChangeNotificationListenerTest.scala
+++ b/core/src/test/scala/unit/kafka/common/ZkNodeChangeNotificationListenerTest.scala
@@ -18,8 +18,9 @@ package kafka.common
 
 import java.nio.charset.StandardCharsets
 
+import kafka.security.auth.storage.AclStore
 import kafka.utils.TestUtils
-import kafka.zk.{AclChangeNotificationSequenceZNode, AclChangeNotificationZNode, ZooKeeperTestHarness}
+import kafka.zk.ZooKeeperTestHarness
 import org.junit.Test
 
 class ZkNodeChangeNotificationListenerTest extends ZooKeeperTestHarness {
@@ -40,11 +41,11 @@ class ZkNodeChangeNotificationListenerTest extends ZooKeeperTestHarness {
     val notificationMessage2 = ""message2""
     val changeExpirationMs = 1000
 
-    val notificationListener = new ZkNodeChangeNotificationListener(zkClient,  AclChangeNotificationZNode.path,
-      AclChangeNotificationSequenceZNode.SequenceNumberPrefix, notificationHandler, changeExpirationMs)
+    val notificationListener = new ZkNodeChangeNotificationListener(zkClient,  AclStore.literalAclStore.aclChangesZNode.path,
+      AclStore.literalAclStore.aclChangeNotificationSequenceZNode.SequenceNumberPrefix, notificationHandler, changeExpirationMs)
     notificationListener.init()
 
-    zkClient.createAclChangeNotification(notificationMessage1)
+    zkClient.createAclChangeNotification(AclStore.literalAclStore, notificationMessage1)
     TestUtils.waitUntilTrue(() => invocationCount == 1 && notification == notificationMessage1,
       ""Failed to send/process notification message in the timeout period."")
 
@@ -56,11 +57,11 @@ class ZkNodeChangeNotificationListenerTest extends ZooKeeperTestHarness {
      * can fail as the second node can be deleted depending on how threads get scheduled.
      */
 
-    zkClient.createAclChangeNotification(notificationMessage2)
+    zkClient.createAclChangeNotification(AclStore.literalAclStore, notificationMessage2)
     TestUtils.waitUntilTrue(() => invocationCount == 2 && notification == notificationMessage2,
       ""Failed to send/process notification message in the timeout period."")
 
-    (3 to 10).foreach(i => zkClient.createAclChangeNotification(""message"" + i))
+    (3 to 10).foreach(i => zkClient.createAclChangeNotification(AclStore.literalAclStore, ""message"" + i))
 
     TestUtils.waitUntilTrue(() => invocationCount == 10 ,
       s""Expected 10 invocations of processNotifications, but there were $invocationCount"")
diff --git a/core/src/test/scala/unit/kafka/security/auth/SimpleAclAuthorizerTest.scala b/core/src/test/scala/unit/kafka/security/auth/SimpleAclAuthorizerTest.scala
index 1e18f1d7bce..7679fe2447b 100644
--- a/core/src/test/scala/unit/kafka/security/auth/SimpleAclAuthorizerTest.scala
+++ b/core/src/test/scala/unit/kafka/security/auth/SimpleAclAuthorizerTest.scala
@@ -38,6 +38,7 @@ class SimpleAclAuthorizerTest extends ZooKeeperTestHarness {
   var resource: Resource = null
   val superUsers = ""User:superuser1; User:superuser2""
   val username = ""alice""
+  val principal = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, username)
   var config: KafkaConfig = null
 
   @Before
@@ -419,6 +420,122 @@ class SimpleAclAuthorizerTest extends ZooKeeperTestHarness {
     TestUtils.waitAndVerifyAcls(Set.empty[Acl], simpleAclAuthorizer2, resource)
   }
 
+  @Test
+  def testAuthorizeTrueOnWildCardAcl(): Unit = {
+    val session = Session(principal, InetAddress.getByName(""192.168.3.1""))
+
+    // verify authorize fails with no acls present
+    assertFalse(simpleAclAuthorizer.authorize(session, Read, resource))
+
+    // add wildcard acl and verify authorize succeeds
+    val acl = new Acl(principal, Allow, WildCardHost, Read)
+    simpleAclAuthorizer.addAcls(Set[Acl](acl), new Resource(Topic, Acl.WildCardString))
+    assertTrue(simpleAclAuthorizer.authorize(session, Read, resource))
+
+    // remove wildcard acl and verify authorize fails again
+    simpleAclAuthorizer.removeAcls(Set[Acl](acl), new Resource(Topic, Acl.WildCardString))
+    assertFalse(simpleAclAuthorizer.authorize(session, Read, resource))
+
+    // add wildcard-suffixed acl and verify authorize succeeds
+    simpleAclAuthorizer.addAcls(Set[Acl](acl), new Resource(Topic, resource.name.charAt(0).toString, WildcardSuffixed))
+    assertTrue(simpleAclAuthorizer.authorize(session, Read, resource))
+  }
+
+  @Test
+  def testMatchPrincipal(): Unit = {
+    // same username should match
+    assertTrue(
+      simpleAclAuthorizer.matchPrincipal(
+        new KafkaPrincipal(KafkaPrincipal.USER_TYPE, ""rob""),
+        new KafkaPrincipal(KafkaPrincipal.USER_TYPE, ""rob"")
+      )
+    )
+
+    // different username shouldn't match
+    assertFalse(
+      simpleAclAuthorizer.matchPrincipal(
+        new KafkaPrincipal(KafkaPrincipal.USER_TYPE, ""rob""),
+        new KafkaPrincipal(KafkaPrincipal.USER_TYPE, ""bob"")
+      )
+    )
+
+    // any username should match wildcard principal
+    assertTrue(
+      simpleAclAuthorizer.matchPrincipal(
+        Acl.WildCardPrincipal,
+        new KafkaPrincipal(KafkaPrincipal.USER_TYPE, ""rob"")
+      )
+    )
+
+    // wildcard principal type should match
+    assertTrue(
+      simpleAclAuthorizer.matchPrincipal(
+        new KafkaPrincipal(Acl.WildCardString, ""rob""),
+        new KafkaPrincipal(KafkaPrincipal.USER_TYPE, ""rob"")
+      )
+    )
+
+    // different principal type shouldn't match
+    assertFalse(
+      simpleAclAuthorizer.matchPrincipal(
+        new KafkaPrincipal(""userType1"", ""rob""),
+        new KafkaPrincipal(""userType2"", ""rob"")
+      )
+    )
+  }
+
+  @Test
+  def testGetAcls(): Unit = {
+    assertEquals(0, simpleAclAuthorizer.getAcls(resource).size)
+    assertEquals(0, simpleAclAuthorizer.getAcls(new Resource(Topic, Acl.WildCardString)).size)
+    assertEquals(0, simpleAclAuthorizer.getAcls(new Resource(Topic, resource.name.charAt(0) + Acl.WildCardString)).size)
+    assertEquals(0, simpleAclAuthorizer.getAcls(new Resource(Topic, resource.name + ""t"")).size)
+
+    val acl1 = new Acl(principal, Allow, WildCardHost, Read)
+    simpleAclAuthorizer.addAcls(Set[Acl](acl1), resource)
+    assertEquals(1, simpleAclAuthorizer.getAcls(resource).size)
+    assertEquals(0, simpleAclAuthorizer.getAcls(new Resource(Topic, Acl.WildCardString)).size)
+    assertEquals(0, simpleAclAuthorizer.getAcls(new Resource(Topic, resource.name.charAt(0).toString, WildcardSuffixed)).size)
+    assertEquals(0, simpleAclAuthorizer.getAcls(new Resource(Topic, resource.name + ""t"")).size)
+
+    // add same acl on wildcard resource
+    simpleAclAuthorizer.addAcls(Set[Acl](acl1), new Resource(Topic, Acl.WildCardString))
+    assertEquals(1, simpleAclAuthorizer.getAcls(resource).size)
+    assertEquals(1, simpleAclAuthorizer.getAcls(new Resource(Topic, Acl.WildCardString)).size)
+    assertEquals(0, simpleAclAuthorizer.getAcls(new Resource(Topic, resource.name.charAt(0).toString, WildcardSuffixed)).size)
+    assertEquals(0, simpleAclAuthorizer.getAcls(new Resource(Topic, resource.name + ""t"")).size)
+
+    // add different acl on resource
+    val acl2 = new Acl(principal, Allow, WildCardHost, Write)
+    simpleAclAuthorizer.addAcls(Set[Acl](acl2), resource)
+    assertEquals(2, simpleAclAuthorizer.getAcls(resource).size)
+    assertEquals(1, simpleAclAuthorizer.getAcls(new Resource(Topic, Acl.WildCardString)).size)
+    assertEquals(0, simpleAclAuthorizer.getAcls(new Resource(Topic, resource.name.charAt(0) + Acl.WildCardString)).size)
+    assertEquals(0, simpleAclAuthorizer.getAcls(new Resource(Topic, resource.name + ""t"")).size)
+
+  }
+
+  @Test
+  def testGetAclsPrincipal(): Unit = {
+    assertEquals(0, simpleAclAuthorizer.getAcls(principal).size)
+
+    val acl1 = new Acl(principal, Allow, WildCardHost, Write)
+    simpleAclAuthorizer.addAcls(Set[Acl](acl1), resource)
+    assertEquals(1, simpleAclAuthorizer.getAcls(principal).size)
+
+    simpleAclAuthorizer.addAcls(Set[Acl](acl1), new Resource(Topic, Acl.WildCardString))
+    assertEquals(2, simpleAclAuthorizer.getAcls(principal).size)
+
+    val acl2 = new Acl(Acl.WildCardPrincipal, Allow, WildCardHost, Write)
+    simpleAclAuthorizer.addAcls(Set[Acl](acl1), new Resource(Group, ""groupA""))
+    assertEquals(3, simpleAclAuthorizer.getAcls(principal).size)
+
+    // add wildcard-suffixed principal acl on wildcard group name
+    val acl3 = new Acl(new KafkaPrincipal(KafkaPrincipal.USER_TYPE, principal.getName.charAt(0) + Acl.WildCardString), Allow, WildCardHost, Write)
+    simpleAclAuthorizer.addAcls(Set[Acl](acl1), new Resource(Group, Acl.WildCardString))
+    assertEquals(4, simpleAclAuthorizer.getAcls(principal).size)
+  }
+
   private def changeAclAndVerify(originalAcls: Set[Acl], addedAcls: Set[Acl], removedAcls: Set[Acl], resource: Resource = resource): Set[Acl] = {
     var acls = originalAcls
 
diff --git a/core/src/test/scala/unit/kafka/security/auth/storage/AclStoreTest.scala b/core/src/test/scala/unit/kafka/security/auth/storage/AclStoreTest.scala
new file mode 100644
index 00000000000..f22aed68df5
--- /dev/null
+++ b/core/src/test/scala/unit/kafka/security/auth/storage/AclStoreTest.scala
@@ -0,0 +1,44 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ * <p/>
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * <p/>
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package unit.kafka.security.auth.storage
+
+import kafka.security.auth.storage.AclStore
+import kafka.security.auth.{Literal, Resource, Topic, WildcardSuffixed}
+import kafka.utils.ZkUtils
+import org.junit.Assert.assertEquals
+import org.junit.Test
+
+class AclStoreTest {
+
+  @Test
+  def testAclStore(): Unit = {
+
+    assertEquals(ZkUtils.KafkaAclPath, AclStore.literalAclStore.aclZNode.path)
+    assertEquals(WildcardSuffixed, AclStore.wildcardSuffixedAclStore.resourceNameType)
+    assertEquals(""/kafka-wildcard-acl/Topic/topicName"", AclStore.wildcardSuffixedAclStore.resourceZNode.path(new Resource(Topic, ""topicName"")))
+
+  }
+
+  @Test
+  def testFromResource(): Unit = {
+    val literalResource = new Resource(Topic, ""name"", Literal)
+    assertEquals(AclStore.literalAclStore, AclStore.fromResource(literalResource))
+    val wildcardResource = new Resource(Topic, ""name"", WildcardSuffixed)
+    assertEquals(AclStore.wildcardSuffixedAclStore, AclStore.fromResource(wildcardResource))
+  }
+}
diff --git a/core/src/test/scala/unit/kafka/utils/TestUtils.scala b/core/src/test/scala/unit/kafka/utils/TestUtils.scala
index ec6c756d453..1e4b80218f8 100755
--- a/core/src/test/scala/unit/kafka/utils/TestUtils.scala
+++ b/core/src/test/scala/unit/kafka/utils/TestUtils.scala
@@ -24,8 +24,8 @@ import java.nio.charset.{Charset, StandardCharsets}
 import java.security.cert.X509Certificate
 import java.util.{Collections, Properties}
 import java.util.concurrent.{Callable, Executors, TimeUnit}
-import javax.net.ssl.X509TrustManager
 
+import javax.net.ssl.X509TrustManager
 import kafka.api._
 import kafka.cluster.{Broker, EndPoint}
 import kafka.consumer.{ConsumerConfig, ConsumerTimeoutException, KafkaStream}
diff --git a/core/src/test/scala/unit/kafka/zk/KafkaZkClientTest.scala b/core/src/test/scala/unit/kafka/zk/KafkaZkClientTest.scala
index 1aeca2203b4..de74112e42b 100644
--- a/core/src/test/scala/unit/kafka/zk/KafkaZkClientTest.scala
+++ b/core/src/test/scala/unit/kafka/zk/KafkaZkClientTest.scala
@@ -16,35 +16,36 @@
 */
 package kafka.zk
 
-import java.util.{Collections, Properties, UUID}
 import java.nio.charset.StandardCharsets.UTF_8
 import java.util.concurrent.{CountDownLatch, TimeUnit}
+import java.util.{Collections, Properties, UUID}
 
 import kafka.api.{ApiVersion, LeaderAndIsr}
 import kafka.cluster.{Broker, EndPoint}
+import kafka.controller.LeaderIsrAndControllerEpoch
 import kafka.log.LogConfig
 import kafka.security.auth._
+import kafka.security.auth.storage.AclStore
 import kafka.server.ConfigType
 import kafka.utils.CoreUtils
+import kafka.zk.KafkaZkClient.UpdateLeaderAndIsrResult
+import kafka.zookeeper._
 import org.apache.kafka.common.TopicPartition
 import org.apache.kafka.common.network.ListenerName
+import org.apache.kafka.common.security.JaasUtils
 import org.apache.kafka.common.security.auth.{KafkaPrincipal, SecurityProtocol}
 import org.apache.kafka.common.security.token.delegation.TokenInformation
 import org.apache.kafka.common.utils.{SecurityUtils, Time}
 import org.apache.zookeeper.KeeperException.{Code, NoNodeException, NodeExistsException}
+import org.apache.zookeeper.data.Stat
 import org.junit.Assert._
 import org.junit.{After, Before, Test}
+
 import scala.collection.JavaConverters._
 import scala.collection.mutable.ArrayBuffer
 import scala.collection.{Seq, mutable}
 import scala.util.Random
 
-import kafka.controller.LeaderIsrAndControllerEpoch
-import kafka.zk.KafkaZkClient.UpdateLeaderAndIsrResult
-import kafka.zookeeper._
-import org.apache.kafka.common.security.JaasUtils
-import org.apache.zookeeper.data.Stat
-
 class KafkaZkClientTest extends ZooKeeperTestHarness {
 
   private val group = ""my-group""
@@ -427,16 +428,16 @@ class KafkaZkClientTest extends ZooKeeperTestHarness {
   @Test
   def testAclManagementMethods() {
 
-    assertFalse(zkClient.pathExists(AclZNode.path))
-    assertFalse(zkClient.pathExists(AclChangeNotificationZNode.path))
-    ResourceType.values.foreach(resource => assertFalse(zkClient.pathExists(ResourceTypeZNode.path(resource.name))))
+    assertFalse(zkClient.pathExists(AclStore.literalAclStore.aclZNode.path))
+    assertFalse(zkClient.pathExists(AclStore.literalAclStore.aclChangesZNode.path))
+    ResourceType.values.foreach(resource => assertFalse(zkClient.pathExists(AclStore.literalAclStore.resourceTypeZNode.path(resource))))
 
     // create acl paths
     zkClient.createAclPaths
 
-    assertTrue(zkClient.pathExists(AclZNode.path))
-    assertTrue(zkClient.pathExists(AclChangeNotificationZNode.path))
-    ResourceType.values.foreach(resource => assertTrue(zkClient.pathExists(ResourceTypeZNode.path(resource.name))))
+    assertTrue(zkClient.pathExists(AclStore.literalAclStore.aclZNode.path))
+    assertTrue(zkClient.pathExists(AclStore.literalAclStore.aclChangesZNode.path))
+    ResourceType.values.foreach(resource => assertTrue(zkClient.pathExists(AclStore.literalAclStore.resourceTypeZNode.path(resource))))
 
     val resource1 = new Resource(Topic, UUID.randomUUID().toString)
     val resource2 = new Resource(Topic, UUID.randomUUID().toString)
@@ -469,10 +470,10 @@ class KafkaZkClientTest extends ZooKeeperTestHarness {
     assertEquals(1, versionedAcls.zkVersion)
 
     //get resource Types
-    assertTrue(ResourceType.values.map( rt => rt.name).toSet == zkClient.getResourceTypes().toSet)
+    assertTrue(ResourceType.values.map( rt => rt.name).toSet == zkClient.getResourceTypes(AclStore.literalAclStore).toSet)
 
     //get resource name
-    val resourceNames = zkClient.getResourceNames(Topic.name)
+    val resourceNames = zkClient.getResourceNames(AclStore.literalAclStore, Topic)
     assertEquals(2, resourceNames.size)
     assertTrue(Set(resource1.name,resource2.name) == resourceNames.toSet)
 
@@ -486,13 +487,13 @@ class KafkaZkClientTest extends ZooKeeperTestHarness {
     assertTrue(zkClient.conditionalDelete(resource2, 0))
 
 
-    zkClient.createAclChangeNotification(""resource1"")
-    zkClient.createAclChangeNotification(""resource2"")
+    zkClient.createAclChangeNotification(AclStore.literalAclStore, ""resource1"")
+    zkClient.createAclChangeNotification(AclStore.literalAclStore, ""resource2"")
 
-    assertEquals(2, zkClient.getChildren(AclChangeNotificationZNode.path).size)
+    assertEquals(2, zkClient.getChildren(AclStore.literalAclStore.aclChangesZNode.path).size)
 
     zkClient.deleteAclChangeNotifications()
-    assertTrue(zkClient.getChildren(AclChangeNotificationZNode.path).isEmpty)
+    assertTrue(zkClient.getChildren(AclStore.literalAclStore.aclChangesZNode.path).isEmpty)
   }
 
   @Test


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support new Admin API for single topic,KAFKA-7046,13165590,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,darion,darion,12/Jun/18 14:56,12/Jun/18 14:56,12/Jan/21 10:06,,1.1.0,,,,,,,,,,admin,,,,,,0,,,,,"When I create topic delete and describe topic with AdminClient often use just one topic .

Currently I must warp it into a collection .

 ",,darion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-06-12 14:56:29.0,,,,,,,"0|i3urvr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Streams] Schedulable KTable as Graph source (for minimizing aggregation pressure),KAFKA-6953,13162229,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,flaviostutz,flaviostutz,26/May/18 13:49,10/Jun/18 16:53,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,1,,,,,"=== PROBLEM ===
We have faced the following scenario/problem in a lot of situations with KStreams:
   - Huge incoming data being processed by numerous application instances
   - Need to aggregate, or count the overall data as a single value (something like ""count the total number of messages that has been processed among all distributed instances"")
   - The challenge here is to manage this kind of situation without any bottlenecks. We don't need the overall aggregation of all instances states at each processed message, so it is possible to store the partial aggregations on local stores and, at time to time, query those states and aggregate them, avoiding bottlenecks.

Some ways we KNOW it wouldn't work because of bottlenecks:
    - Sink all instances local counter/aggregation result to a Topic with a single partition so that we could have another Graph with a single instance that could aggregate all results
         - In this case, if I had 500 instances processing 1000/s each (with no bottlenecks), I would have a single partition topic with 500k messages/s for my single aggregating instance to process that much messages (IMPOSSIBLE bottleneck)

=== TRIALS ===
These are some ways we managed to do this:
   - Expose a REST endpoint so that Prometheus could extract local metrics of each application instance's state stores and them calculate the total count on Prometheus using queries
         - we don't like this much because we believe KStreams was meant to INPUT and OUTPUT data using Kafka Topics for simplicity and power
   - Create a scheduled Punctuate at the end of the Graph so that we can query (using getAllMetadata) all other instances's state store counters, sum them all and them publish to another Kafka Topic from time to time.
          - For this to work we created a way so that only one application instance's Punctuate algorithm would perform the calculations (something like a master election through instance ids and metadata)

=== PROPOSAL ===
Create a new DSL Source with the following characteristics:
   - Source parameters: ""scheduled time"" (using cron's like config), ""state store name"", bool ""from all application instances""
   - Behavior: At the desired time, query all K,V tuples from the state store and source those messages to the Graph
          - If ""from all application instances"" is true, query the tuples from all application instances state stores and source them all, concatenated

   - This is a way to create a ""timed aggregation barrier"" to avoid bottlenecks. With this we could enhance the ability of KStreams to better handle the CAP Theorem characteristics, so that one could choose to have Consistency over Availability.",,adupriez,astubbs,brunocn80@gmail.com,darion,flaviostutz,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-26 18:49:43.504,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 10 16:53:09 UTC 2018,,,,,,,"0|i3u75z:",9223372036854775807,,,,,,,,,,,,,,,,"26/May/18 18:49;mjsax;Couple of comments:
 - With sufficiently large caches the first approach about piping all data throw a singled-partitioned topic should actually be feasible. (cf. [https://kafka.apache.org/11/documentation/streams/developer-guide/memory-mgmt.html)]
 - The idea about REST proxy is also feasible; for this exact pattern, ""Interactive Queries"" was built (cf. [https://kafka.apache.org/documentation/streams/developer-guide/interactive-queries.html);] to bad if you ""don't like it""

Additionally, Kafka Streams exposes JXM metrics and you can register your own metrics there: [https://docs.confluent.io/current/streams/monitoring.html] (this might actually be the recommended way to get stats on how many records got processed).

Overall, I am not convinced atm that adding a feature as proposed is necessary. (But I am happy to get convinced :))","27/May/18 04:51;flaviostutz;I used the ""message count"" example for simplicity. Thanks for the tips on using metrics for this job (I didn't know it), but now I am facing more complex aggregations. In my case I am using a SessionWindow for aggregating certain data and summing up a certain field of the events among various partitions.

_- With sufficiently large caches the first approach about piping all data throw a singled-partitioned topic should actually be feasible._
    - For a scenario where we have ~500k messages/second, even with a large cache the single threaded task wouldn't leverage enough CPU for the final aggregation. Please advise me if I am wrong.

_- The idea about REST proxy is also feasible; for this exact pattern, ""Interactive Queries"" was built; to bad if you ""don't like it""_
    - The REST proxy works well, but I need to maintain an external database for the final aggregation. It would be great if I could use KStream capabilities for this job.

_- Additionally, Kafka Streams exposes JXM metrics and you can register your own metrics there:_
    - Thanks for the tip.

_- Overall, I am not convinced atm that adding a feature as proposed is necessary. (But I am happy to get convinced :))_
    - We are building some basic web analytics with KStreams and we are already facing the need for a ""timed global aggregation"" at our early stages. This is very common because leveraging distributed processing for generating different views on data can't be solved by key partitioning because a single keyed/partitioned entry has a lot of fields, so those field aggregations gets distributed among application instances. This seemed to me like a simple, but very useful feature.

I can help by submiting PRs for this feature request so that you could guide me.

-Flávio Stutz
github.com/flaviostutz","27/May/18 06:39;mjsax;Thanks for sharing more details. I guess I miss understood the actual feature request. It's about a general ability to do global aggregations? I agree that this is a useful feature and could be supported better. Atm, it requires quite some custom code, but even as of today, you can do a global aggregation IMHO.
{quote}For a scenario where we have ~500k messages/second, even with a large cache the single threaded task wouldn't leverage enough CPU for the final aggregation. Please advise me if I am wrong.
{quote}
This really depends on the use case. I think there are two scenarios:
 # You cannot pre-aggregate (for example computing median): for this case, the number of input message/sec will determine your performance. The requested feature is not applicable for this case.
 # You can pre-aggregate (for example, sum, average, max): for this case, the number of input message/sec is actually not too important because you can scale out the pre-aggregation step accordingly. Thus, if you pre-aggregate per key, the downstream load depends on the number of unique keys. It should be feasible to handle the downstream load for this case (with large enough caches, the downstream load would be number-of-unique-keys/commit-interval – on commit, all caches are flushed). If the number of unique keys is really too large and caches are not sufficient, it should be possible to break the global aggregation itself into a two step process: you assign artificial keys and do an intermediate (pre) aggregation that puts multiple keys together. This step can be scaled out to whatever extend you need by using enough surrogate keys. Only afterwards you do the a actual single-keyed global aggregation.

Does this make sense?
{quote}The REST proxy works well, but I need to maintain an external database for the final aggregation. It would be great if I could use KStream capabilities for this job.
{quote}
Not sure what you mean by this.

 

We are always happy to get contributions :) However, we should first agree on a design. I also think, that adding a feature like this requires a KIP (cf [https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals).] We are happy to guide you through the KIP process etc. Atm I am still not sure though, if I understand the feature request correctly.","27/May/18 15:00;flaviostutz;""... but even as of today, you can do a global aggregation IMHO.""
*We can surely do global aggregation using Interactive Queries or single partitioning. This is what you mean, right?*


""This really depends on the use case. I think there are two scenarios:
 # You cannot pre-aggregate (for example computing median)...
 # You can pre-aggregate (for example, sum, average, max)...

Does this make sense?""

*Yes. The proposed feature would be useful on scenarios where you* *can* *pre-aggregate.* 
*It could be useful in scenarios where you accept skipping some data in order to avoid unnecessary pressure too. For example, in signal processing I already faced situations where I had a lot of data coming in, but needed to process just  'temporal snapshots' of it, creating a temporal normalization barrier. In our web analytics case, we can use this to avoid processing all clicks of a user if he is rapidly clicking on various items and I am only interested on his last click of a series of 5 clicks on the last 5 seconds. For this I could keep the last clicked item in a local storage and at each 5s the scheduled KTable would trigger a more complex Graph for processing the last clicked item. In this case, the scheduled KTable source could trigger tuples only from the local storage, not from all local storages.*


""The REST proxy works well, but I need to maintain an external database for the final aggregation. It would be great if I could use KStream capabilities for this job.

Not sure what you mean by this.""

*I was describing a scenario where you achieve global aggregation by exposing a REST proxy in each application instance so that you can query each instance using interactive queries to get each local storage pre-aggregation results and send this data to an external database, so that you can perform the final global aggregation. Using an external element requires more code if you want to put back the global aggregated value back to Kafka Streams for further processing, as happens in our case because we use the global aggregated value in other Graphs.*


""We are always happy to get contributions :) However, we should first agree on a design....""
*Sure, that's why I am trying to make sense here :)*

 ","27/May/18 16:52;mjsax;Thanks!

I agree that we could make a (two-step) global aggregation a built-in feature. Not sure about ""Schedulable KTables"" though – this sounds like a very specific but not a generic feature and thus I am not sure if a general purpose library as Kafka Streams should add this. It sounds more like a custom operator that you can build for your use case manually. You would read a topic as `builder.stream().transform()` and attach a store to the `Transformer`. The `Transfomer` only put incoming data into the store, but does not send data downstream during regular processing. The schedule you need to send data downstream to ""source the Graph"" can be implemented via punctuations.

If you want  to design a global aggregation as part of the DSL, feel free to work on a design a propose a KIP. I am sure, that the community will accept it.

(Of course, it's only my personal opinion and I cannot speak for the community...)","27/May/18 17:43;flaviostutz;Great to hear. I will propose a KIP... thanks.","27/May/18 18:54;mjsax;Would be good to hear [~guozhang] opinion on this first.","30/May/18 04:24;guozhang;Flavio, thanks for reporting your use case. On top of my head I agree with Matthias that this ""single-value aggregate"" feature could be worked out on the interactive query side, given that 1) your aggregation can be pre-aggregated (i.e. it is commutative and associative), and also since you mentioned 2) you only need to get this global aggregate value periodically, and not very frequently. I was thinking that you do not even need the ""external database"" for that value: whenever you want to get that value, you just query all the local stores and do the final aggregate on-the-fly. Does that make sense?","30/May/18 16:18;flaviostutz;[~guozhang], you are right on your observations, but doing it by hand is very laborious. We have to create custom REST endpoints, coordinate schedulers and make deployments aside Kafka Streams Applications and, preferably, publish the results back to a Kafka topic in order to achieve the desired processing result (being the global aggregation here).
The idea here is KStreams to have a native way to handle this type of requirement, because today users are getting confused and creating various patterns to manage this sort of problem. 
On the GlobalKTable discussions, I saw some users looking for this kind of global aggregation feature, but GlobalKTable wasn't made for handling huge distributed writes as it is ""global"" and has possible bottlenecks (am I wrong?).","08/Jun/18 03:17;flaviostutz;I sent an email requesting permission to create the KIP (I already wrote the text) but had no response for two weeks. Do you possibly know what is going on?","08/Jun/18 17:39;mjsax;Just looked it up. You did not share your wiki ID and did not follow up on my reply:
{quote}What is your wiki ID?

-Matthias{quote}
Did you not see my reply?","09/Jun/18 13:41;flaviostutz;Sorry but I didn’t see your reply. Was it by email? Is Wiki ID the same as my login here in Apache Jira? If so, it is “flaviostutz”","10/Jun/18 06:39;mjsax;It was by email to dev-list. It's a different account – if you don't have a wiki account yet, please create one and let us know you ID. Thx.","10/Jun/18 12:13;flaviostutz;I've just created my cwiki account ([https://cwiki.apache.org/confluence/display/KAFKA/Index).] My ID is ""flaviostutz"". Is it right?

Thanks again for your support.","10/Jun/18 16:53;mjsax;Added you. You should now have permission to edit and create new pages. Thanks for contributing!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-295 Add Streams Config for Optional Optimization,KAFKA-6935,13161454,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,bbejeck,bbejeck,bbejeck,23/May/18 14:53,07/Jun/18 01:11,12/Jan/21 10:06,07/Jun/18 01:11,2.0.0,,,,,,,2.0.0,,,streams,,,,,,0,kip,,,,KIP-295: https://cwiki.apache.org/confluence/display/KAFKA/KIP-295%3A+Add+Streams+Configuration+Allowing+for+Optional+Topology+Optimization,,bbejeck,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-23 14:58:32.237,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 07 01:11:06 UTC 2018,,,,,,,"0|i3u2ev:",9223372036854775807,,,,,,,,,,,,,,,,"23/May/18 14:58;githubbot;bbejeck opened a new pull request #5071: KAFKA-6935: Add config for allowing optional optimization
URL: https://github.com/apache/kafka/pull/5071
 
 
   Adding configuration to `StreamsConfig` allowing for making topology optimization optional.
   
   Added unit tests are verifying default values, setting correct value and failure on invalid values.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","07/Jun/18 01:11;githubbot;guozhangwang closed pull request #5071: KAFKA-6935: Add config for allowing optional optimization
URL: https://github.com/apache/kafka/pull/5071
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java
index 18dc891682d..05c064e8f27 100644
--- a/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java
+++ b/streams/src/main/java/org/apache/kafka/streams/StreamsConfig.java
@@ -16,8 +16,8 @@
  */
 package org.apache.kafka.streams;
 
-import org.apache.kafka.clients.admin.AdminClient;
 import org.apache.kafka.clients.CommonClientConfigs;
+import org.apache.kafka.clients.admin.AdminClient;
 import org.apache.kafka.clients.admin.AdminClientConfig;
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.clients.consumer.KafkaConsumer;
@@ -32,10 +32,10 @@
 import org.apache.kafka.common.metrics.Sensor;
 import org.apache.kafka.common.serialization.Serde;
 import org.apache.kafka.common.serialization.Serdes;
+import org.apache.kafka.streams.errors.DefaultProductionExceptionHandler;
 import org.apache.kafka.streams.errors.DeserializationExceptionHandler;
 import org.apache.kafka.streams.errors.LogAndFailExceptionHandler;
 import org.apache.kafka.streams.errors.ProductionExceptionHandler;
-import org.apache.kafka.streams.errors.DefaultProductionExceptionHandler;
 import org.apache.kafka.streams.errors.StreamsException;
 import org.apache.kafka.streams.processor.DefaultPartitionGrouper;
 import org.apache.kafka.streams.processor.FailOnInvalidTimestamp;
@@ -195,6 +195,16 @@
      */
     public static final String ADMIN_CLIENT_PREFIX = ""admin."";
 
+    /**
+     * Config value for parameter (@link #TOPOLOGY_OPTIMIZATION ""topology.optimization"" for disabling topology optimization
+     */
+    public static final String NO_OPTIMIZATION = ""none"";
+
+    /**
+     * Config value for parameter (@link #TOPOLOGY_OPTIMIZATION ""topology.optimization"" for enabling topology optimization
+     */
+    public static final String OPTIMIZE = ""all"";
+
     /**
      * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 0.10.0.x}.
      */
@@ -382,6 +392,10 @@
     public static final String STATE_DIR_CONFIG = ""state.dir"";
     private static final String STATE_DIR_DOC = ""Directory location for state store."";
 
+    /** {@code topology.optimization} */
+    public static final String TOPOLOGY_OPTIMIZATION = ""topology.optimization"";
+    private static final String TOPOLOGY_OPTIMIZATION_DOC = ""A configuration telling Kafka Streams if it should optimize the topology, disabled by default"";
+
     /** {@code upgrade.from} */
     public static final String UPGRADE_FROM_CONFIG = ""upgrade.from"";
     public static final String UPGRADE_FROM_DOC = ""Allows upgrading from versions 0.10.0/0.10.1/0.10.2/0.11.0/1.0/1.1 to version 1.2 (or newer) in a backward compatible way. "" +
@@ -480,6 +494,12 @@
                     CommonClientConfigs.DEFAULT_SECURITY_PROTOCOL,
                     Importance.MEDIUM,
                     CommonClientConfigs.SECURITY_PROTOCOL_DOC)
+            .define(TOPOLOGY_OPTIMIZATION,
+                    Type.STRING,
+                    NO_OPTIMIZATION,
+                    in(NO_OPTIMIZATION, OPTIMIZE),
+                    Importance.MEDIUM,
+                    TOPOLOGY_OPTIMIZATION_DOC)
 
             // LOW
 
diff --git a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java
index e991b6ff89e..cdd4d097ba3 100644
--- a/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java
+++ b/streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java
@@ -45,6 +45,7 @@
 import static org.apache.kafka.common.requests.IsolationLevel.READ_COMMITTED;
 import static org.apache.kafka.common.requests.IsolationLevel.READ_UNCOMMITTED;
 import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;
+import static org.apache.kafka.streams.StreamsConfig.TOPOLOGY_OPTIMIZATION;
 import static org.apache.kafka.streams.StreamsConfig.adminClientPrefix;
 import static org.apache.kafka.streams.StreamsConfig.consumerPrefix;
 import static org.apache.kafka.streams.StreamsConfig.producerPrefix;
@@ -583,6 +584,27 @@ public void shouldThrowExceptionIfMaxInflightRequestsGreatherThanFiveIfEosEnable
         }
     }
 
+    @Test
+    public void shouldSpecifyNoOptimizationWhenNotExplicitlyAddedToConfigs() {
+        final String expectedOptimizeConfig = ""none"";
+        final String actualOptimizedConifig = streamsConfig.getString(TOPOLOGY_OPTIMIZATION);
+        assertEquals(""Optimization should be \""none\"""", expectedOptimizeConfig, actualOptimizedConifig);
+    }
+
+    @Test
+    public void shouldSpecifyOptimizationWhenNotExplicitlyAddedToConfigs() {
+        final String expectedOptimizeConfig = ""all"";
+        props.put(TOPOLOGY_OPTIMIZATION, ""all"");
+        final StreamsConfig config = new StreamsConfig(props);
+        final String actualOptimizedConifig = config.getString(TOPOLOGY_OPTIMIZATION);
+        assertEquals(""Optimization should be \""all\"""", expectedOptimizeConfig, actualOptimizedConifig);
+    }
+
+    @Test(expected = ConfigException.class)
+    public void shouldThrowConfigExceptionWhenOptimizationConfigNotValueInRange() {
+        props.put(TOPOLOGY_OPTIMIZATION, ""maybe"");
+        new StreamsConfig(props);
+    }
 
     static class MisconfiguredSerde implements Serde {
         @Override


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Making state store queryable on replicas,KAFKA-6924,13160627,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,,NaviBrar,NaviBrar,21/May/18 05:41,22/May/18 15:16,12/Jan/21 10:06,22/May/18 15:16,,,,,,,,,,,streams,,,,,,0,,,,,"State store in Kafka streams are currently only queryable when StreamTask is in RUNNING state. The idea is to make it queryable even for StandbyTasks to decrease the downtime if client is not able to fetch data from Active machine.

Suppose the coordinator is not able to connect to machine which had active partition due to some reason. So, rather than failing that request we could serve request from replica which could be on some other machine. Although state on replica might be little behind the active but it could still be beneficial in some cases to serve request from replica than failing the request.

It's very important improvement as it could simply improve the availability of microservices developed using kafka streams.

I am working on a patch for this change. Any feedback or comments are welcome.

Also, I have gone thorugh https://issues.apache.org/jira/browse/KAFKA-6031, has this been implemented?",,guozhang,mjsax,NaviBrar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-21 22:33:36.782,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 22 07:52:55 UTC 2018,,,,,,,"0|i3txon:",9223372036854775807,,,,,,,,,,,,,,,,"21/May/18 22:33;guozhang;Hello Navinder, thanks for reporting this JIRA.

Is it the same request as of https://issues.apache.org/jira/browse/KAFKA-6144?","22/May/18 07:52;NaviBrar;Hi [~guozhang] thanks for pointing it out. I believe it is same. We can close it and track there.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optionally support OpenSSL for SSL/TLS ,KAFKA-2561,12895129,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,ijuma,ijuma,21/Sep/15 14:10,21/May/18 20:42,12/Jan/21 10:06,,0.9.0.0,,,,,,,,,,security,,,,,,2,,,,,"JDK's `SSLEngine` is unfortunately a bit slow (KAFKA-2431 covers this in more detail). We should consider supporting OpenSSL for SSL/TLS. Initial experiments on my laptop show that it performs a lot better:

{code}
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, config
2015-09-21 14:41:58:245, 2015-09-21 14:47:02:583, 28610.2295, 94.0081, 30000000, 98574.6111, Java 8u60/server auth JDK SSLEngine/TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA
2015-09-21 14:38:24:526, 2015-09-21 14:40:19:941, 28610.2295, 247.8900, 30000000, 259931.5514, Java 8u60/server auth OpenSslEngine/TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
2015-09-21 14:49:03:062, 2015-09-21 14:50:27:764, 28610.2295, 337.7751, 30000000, 354182.9000, Java 8u60/plaintext
{code}

Extracting the throughput figures:

* JDK SSLEngine: 94 MB/s
* OpenSSL SSLEngine: 247 MB/s
* Plaintext: 337 MB/s (code from trunk, so no zero-copy due to KAFKA-2517)

In order to get these figures, I used Netty's `OpenSslEngine` by hacking `SSLFactory` to use Netty's `SslContextBuilder` and made a few changes to `SSLTransportLayer` in order to workaround differences in behaviour between `OpenSslEngine` and JDK's SSLEngine (filed https://github.com/netty/netty/issues/4235 and https://github.com/netty/netty/issues/4238 upstream).",,aneale,baluchicken,becket_qin,cmolter,dqminh,ecomar,felixgv,gabbi,gquintana,ijuma,jaikiran,jeffwidman,krisden,prasincs,roczei,saeedsh,salyh,sriharsha,TaoFeng,thesquelched,waleedfateem,xkrt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-24 16:51:22.632,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 21 20:42:26 UTC 2018,,,,,,,"0|i2lci7:",9223372036854775807,,,,,,,,,,,,,,,,"22/Sep/15 16:21;ijuma;In order to implement this properly (as opposed to a simple test), the following steps are needed:

1. Add an optional build dependency on netty-tcnative. This library contains a fork of tomcat native that is available in Maven and is available for major platforms (Linux, OS X, Windows). It also handles extracting the platform-specific JNI code at runtime (similar to snappy-java). apr and openssl need to be installed separately.
2. Provide an implementation of `SSLEngine` based on OpenSSL. The easy option would be to add an optional dependency on `netty-handler`, which includes this. If this is not acceptable, there are some alternatives like extracting the code into a separate library or copying it into Kafka.
3. Add a way to configure the `SSLEngine` implementation (OpenSSL or JDK).
4. Change `SSLFactory` to build the appropriate `SSLEngine` based on the configuration added in `3`.
5. Potentially introduce a runtime mechanism to select `OpenSslEngine` by default if the required libraries are present (since it's much faster)
6. Potentially update `SSLTransportLayer` to handle differences in behaviour between the different `SSLEngine` implementations (the need for this depends on whether we the issues reported to Netty are fixed or not). The main one is that `OpenSslEngine.unwrap` consumes multiple SSL records (instead of just one) and it may produce a different number of SSL records (if they don't all fit into the application buffer).
7. Use `allocateDirect` to allocate the buffers in `SSLTransportLayer` when using `OpenSslEngine` to avoid copies on each `wrap` and `unwrap` call.
8. Design and implement the story around the formats for keys, certificates, key chains and certificate chains supported. OpenSSL doesn't understand the JKS format since it's Java-specific. Netty uses the `PKCS#8` format for keys and PEM format for chains when the OpenSSL engine is used.
9. Update tests to test all `SSLEngine` implementations.

Testing of this is more complicated than usual due to the native code aspect and we would have to test it in all of our supported platforms.

Given the work that I've already done, it would probably take a couple of weeks to agree on the details and implement the code (including unit tests). Maybe another week for testing on the various platforms.
","24/Sep/15 16:51;becket_qin;[~ijuma] Just curious, have we found out why the performance differs so much? Is it possible that we can tweak some settings of JDK SslEngine to improve the performance?","24/Sep/15 17:14;ijuma;[~becket_qin], there are two problems:

* The JDK SSLEngine implementation for AES-GCM doesn't use the relevant CPU instructions yet and a highly optimised implementation of AES-GCM is faster than a highly optimised implementation of AES-CBC. OpenSSL is twice as fast when using AES-GCM instead of AES-CBC in a SSLEngine micro-benchmark. In the same benchmark, JDK 8u60 with AES-CBC is four times faster than with AES-GCM. This issue should improve in Java 9 (http://openjdk.java.net/jeps/8046943).

* The JDK SSLEngine implementation is not particularly well implemented and generates a lot of garbage based on what Netty's Norman Maurer said in a presentation.

I don't think we can do much about it apart from waiting for Java 9 and potentially contributing improvements to the implementation. We are not the first to run into this and that is why Tomcat, Finagle and Netty have the OpenSSL implementation too.

Also note that the results I posted in the description are for a test with a single broker and consumer running in the same machine. If you have many cores and they are lightly loaded, you _may_ still be able to saturate the network in the meantime.","24/Sep/15 19:58;becket_qin;[~ijuma] Thanks for the explanation. It looks Java 9 is still one year away from release. I am not sure what should we do now. If might take several weeks as you suggested to implement the patch and some time longer to make it stable. A few months later, if Java 9 has better performance than OpenSSL, are we going to switch back to JDK? What do you think?","24/Sep/15 20:08;ijuma;The current plan is to go with the JDK implementation for 0.9.0.0. As you say, there isn't enough time to do anything else. If and when we add an implementation based on OpenSSL, it will be optional. The default engine can be discussed when we get to that point.

I doubt that the Java 9 implementation will be faster than OpenSSL, but it will hopefully narrow the gap. In any case, that's quite far away as you said. And people take their time to upgrade as well.

I filed this issue to record and share our findings, but we are not planning to do additional work on this until after 0.9.0.0 now.","18/Nov/15 09:50;ijuma;Encryption speed improvements in JDK 9 (note the following doesn't include the `SSLEngine` overhead, which is probably still significant):

{quote}
• JDK 9: up to a 62x performance gain over the JDK 8 GA implementation
• up to 5.45x over 8u60 implementation
• 8u60 performance improved due to https://bugs.openjdk.java.net/browse/JDK-8069072
{quote}

https://blogs.oracle.com/mullan/entry/slides_for_javaone_2015_session","26/Apr/16 21:24;thesquelched;Now that Java 9 has been delayed until 2017, can this get a bump in priority?","21/Jul/16 21:10;salyh;Here is a working draft: https://github.com/salyh/kafka/commit/9337c56df9b8387bf42f756faf5be08118259139

First sketch to make SslFactory ready for native OpenSSl support leveraging netty and netty tcnative.
Requires netty 4.0.30 (common, handler, buffer, codec) and tcnative fork-1.1.33.19 for the respective OS and of course OpenSSL installed (recent 1.0.1 or better 1.0.2). Could not get the gradle dependency stuff to work so maybe one can add the required dependencies. 

","30/Oct/17 11:53;jaikiran;I just came across this JIRA, so I thought I will update it with my own recent experiments with OpenSSL (Java 8) and Kafka. For those interested, I got some performance numbers OpenSSL (backed by WildFly OpenSSL Java bindings) and have detailed them in my blog[1]. Later this week, I plan to rerun the same thing with Java 9 and see how it performs.

[1] https://jaitechwriteups.blogspot.com/2017/10/kafka-with-openssl.html
","11/Nov/17 07:41;jaikiran;> Later this week, I plan to rerun the same thing with Java 9 and see how it performs.

The same blog[1] has now been updated to include the performance numbers when Java 9 was used. To summarize, there's a drastic improvement in the SSLEngine shipped in JRE 9 as compared to JRE 8. OpenSSL (backed by WildFly OpenSSL), however, still out-performs the default shipped SSLEngine even in Java 9.

[1] https://jaitechwriteups.blogspot.com/2017/10/kafka-with-openssl.html","13/Jan/18 21:00;prasincs;[~ijuma] is this still being planned anytime soon? I'd like to check if Java 9 or using Open/Boring/LibreSSL have any meaningful performance improvments for SSL.","21/May/18 20:42;ijuma;[~prasincs], given the improvement in Java 9, this work seems less appealing. The blog post from [~jaikiran] has some numbers. Even though OpenSSL is a bit faster in some cases still, the gap has narrowed considerably.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Devops Insights,KAFKA-6865,13157252,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Incomplete,,baviskar,baviskar,04/May/18 16:58,04/May/18 19:10,12/Jan/21 10:06,04/May/18 19:10,,,,,,,,,,,,,,,,,0,,,,,Devops Insights,,baviskar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,2018-05-04 16:58:15.0,,,,,,,"0|i3tcyv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configurable Quota Management (KIP-257),KAFKA-6576,13139977,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,21/Feb/18 19:02,25/Apr/18 09:42,12/Jan/21 10:06,06/Apr/18 21:55,,,,,,,,2.0.0,,,core,,,,,,0,,,,,See [https://cwiki.apache.org/confluence/display/KAFKA/KIP-257+-+Configurable+Quota+Management] for details.,,githubbot,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-03-13 09:40:31.379,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 06 21:49:37 UTC 2018,,,,,,,"0|i3qfm7:",9223372036854775807,,junrao,,,,,,,,,,,,,,"13/Mar/18 09:40;githubbot;rajinisivaram opened a new pull request #4699: KAFKA-6576: Configurable Quota Management (KIP-257)
URL: https://github.com/apache/kafka/pull/4699
 
 
   Enable quota calculation to be customized using a configurable callback. See KIP-257 for details.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","06/Apr/18 21:49;githubbot;rajinisivaram closed pull request #4699: KAFKA-6576: Configurable Quota Management (KIP-257)
URL: https://github.com/apache/kafka/pull/4699
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):

diff --git a/clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaCallback.java b/clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaCallback.java
new file mode 100644
index 00000000000..210e9f45840
--- /dev/null
+++ b/clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaCallback.java
@@ -0,0 +1,106 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.server.quota;
+
+import org.apache.kafka.common.Cluster;
+import org.apache.kafka.common.Configurable;
+import org.apache.kafka.common.security.auth.KafkaPrincipal;
+
+import java.util.Map;
+
+/**
+ * Quota callback interface for brokers that enables customization of client quota computation.
+ */
+public interface ClientQuotaCallback extends Configurable {
+
+    /**
+     * Quota callback invoked to determine the quota metric tags to be applied for a request.
+     * Quota limits are associated with quota metrics and all clients which use the same
+     * metric tags share the quota limit.
+     *
+     * @param quotaType Type of quota requested
+     * @param principal The user principal of the connection for which quota is requested
+     * @param clientId  The client id associated with the request
+     * @return quota metric tags that indicate which other clients share this quota
+     */
+    Map<String, String> quotaMetricTags(ClientQuotaType quotaType, KafkaPrincipal principal, String clientId);
+
+    /**
+     * Returns the quota limit associated with the provided metric tags. These tags were returned from
+     * a previous call to {@link #quotaMetricTags(ClientQuotaType, KafkaPrincipal, String)}. This method is
+     * invoked by quota managers to obtain the current quota limit applied to a metric when the first request
+     * using these tags is processed. It is also invoked after a quota update or cluster metadata change.
+     * If the tags are no longer in use after the update, (e.g. this is a {user, client-id} quota metric
+     * and the quota now in use is a {user} quota), null is returned.
+     *
+     * @param quotaType  Type of quota requested
+     * @param metricTags Metric tags for a quota metric of type `quotaType`
+     * @return the quota limit for the provided metric tags or null if the metric tags are no longer in use
+     */
+    Double quotaLimit(ClientQuotaType quotaType, Map<String, String> metricTags);
+
+    /**
+     * Quota configuration update callback that is invoked when quota configuration for an entity is
+     * updated in ZooKeeper. This is useful to track configured quotas if built-in quota configuration
+     * tools are used for quota management.
+     *
+     * @param quotaType   Type of quota being updated
+     * @param quotaEntity The quota entity for which quota is being updated
+     * @param newValue    The new quota value
+     */
+    void updateQuota(ClientQuotaType quotaType, ClientQuotaEntity quotaEntity, double newValue);
+
+    /**
+     * Quota configuration removal callback that is invoked when quota configuration for an entity is
+     * removed in ZooKeeper. This is useful to track configured quotas if built-in quota configuration
+     * tools are used for quota management.
+     *
+     * @param quotaType   Type of quota being updated
+     * @param quotaEntity The quota entity for which quota is being updated
+     */
+    void removeQuota(ClientQuotaType quotaType, ClientQuotaEntity quotaEntity);
+
+    /**
+     * Returns true if any of the existing quota configs may have been updated since the last call
+     * to this method for the provided quota type. Quota updates as a result of calls to
+     * {@link #updateClusterMetadata(Cluster)}, {@link #updateQuota(ClientQuotaType, ClientQuotaEntity, double)}
+     * and {@link #removeQuota(ClientQuotaType, ClientQuotaEntity)} are automatically processed.
+     * So callbacks that rely only on built-in quota configuration tools always return false. Quota callbacks
+     * with external quota configuration or custom reconfigurable quota configs that affect quota limits must
+     * return true if existing metric configs may need to be updated. This method is invoked on every request
+     * and hence is expected to be handled by callbacks as a simple flag that is updated when quotas change.
+     *
+     * @param quotaType Type of quota
+     */
+    boolean quotaResetRequired(ClientQuotaType quotaType);
+
+    /**
+     * Metadata update callback that is invoked whenever UpdateMetadata request is received from
+     * the controller. This is useful if quota computation takes partitions into account.
+     * Topics that are being deleted will not be included in `cluster`.
+     *
+     * @param cluster Cluster metadata including partitions and their leaders if known
+     * @return true if quotas have changed and metric configs may need to be updated
+     */
+    boolean updateClusterMetadata(Cluster cluster);
+
+    /**
+     * Closes this instance.
+     */
+    void close();
+}
+
diff --git a/clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaEntity.java b/clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaEntity.java
new file mode 100644
index 00000000000..a5ff082dfef
--- /dev/null
+++ b/clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaEntity.java
@@ -0,0 +1,62 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.server.quota;
+
+import java.util.List;
+
+/**
+ * The metadata for an entity for which quota is configured. Quotas may be defined at
+ * different levels and `configEntities` gives the list of config entities that define
+ * the level of this quota entity.
+ */
+public interface ClientQuotaEntity {
+
+    /**
+     * Entity type of a {@link ConfigEntity}
+     */
+    public enum ConfigEntityType {
+        USER,
+        CLIENT_ID,
+        DEFAULT_USER,
+        DEFAULT_CLIENT_ID
+    }
+
+    /**
+     * Interface representing a quota configuration entity. Quota may be
+     * configured at levels that include one or more configuration entities.
+     * For example, {user, client-id} quota is represented using two
+     * instances of ConfigEntity with entity types USER and CLIENT_ID.
+     */
+    public interface ConfigEntity {
+        /**
+         * Returns the name of this entity. For default quotas, an empty string is returned.
+         */
+        String name();
+
+        /**
+         * Returns the type of this entity.
+         */
+        ConfigEntityType entityType();
+    }
+
+    /**
+     * Returns the list of configuration entities that this quota entity is comprised of.
+     * For {user} or {clientId} quota, this is a single entity and for {user, clientId}
+     * quota, this is a list of two entities.
+     */
+    List<ConfigEntity> configEntities();
+}
diff --git a/clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaType.java b/clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaType.java
new file mode 100644
index 00000000000..4dd67d3125d
--- /dev/null
+++ b/clients/src/main/java/org/apache/kafka/server/quota/ClientQuotaType.java
@@ -0,0 +1,26 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.server.quota;
+
+/**
+ * Types of quotas that may be configured on brokers for client requests.
+ */
+public enum ClientQuotaType {
+    PRODUCE,
+    FETCH,
+    REQUEST
+}
diff --git a/core/src/main/scala/kafka/server/ClientQuotaManager.scala b/core/src/main/scala/kafka/server/ClientQuotaManager.scala
index 8ec27a3a4c0..0f8690fd8b9 100644
--- a/core/src/main/scala/kafka/server/ClientQuotaManager.scala
+++ b/core/src/main/scala/kafka/server/ClientQuotaManager.scala
@@ -16,24 +16,29 @@
  */
 package kafka.server
 
+import java.{lang, util}
 import java.util.concurrent.{ConcurrentHashMap, DelayQueue, TimeUnit}
 import java.util.concurrent.locks.ReentrantReadWriteLock
 
+import kafka.network.RequestChannel.Session
+import kafka.server.ClientQuotaManager._
 import kafka.utils.{Logging, ShutdownableThread}
-import org.apache.kafka.common.MetricName
+import org.apache.kafka.common.{Cluster, MetricName}
 import org.apache.kafka.common.metrics._
 import org.apache.kafka.common.metrics.stats.{Avg, Rate, Total}
+import org.apache.kafka.common.security.auth.KafkaPrincipal
 import org.apache.kafka.common.utils.{Sanitizer, Time}
+import org.apache.kafka.server.quota.{ClientQuotaCallback, ClientQuotaEntity, ClientQuotaType}
 
 import scala.collection.JavaConverters._
 
 /**
  * Represents the sensors aggregated per client
- * @param quotaEntity Quota entity representing <client-id>, <user> or <user, client-id>
+ * @param metricTags Quota metric tags for the client
  * @param quotaSensor @Sensor that tracks the quota
  * @param throttleTimeSensor @Sensor that tracks the throttle time
  */
-case class ClientSensors(quotaEntity: QuotaEntity, quotaSensor: Sensor, throttleTimeSensor: Sensor)
+case class ClientSensors(metricTags: Map[String, String], quotaSensor: Sensor, throttleTimeSensor: Sensor)
 
 /**
  * Configuration settings for quota management
@@ -61,9 +66,6 @@ object ClientQuotaManagerConfig {
   val NanosToPercentagePerSecond = 100.0 / TimeUnit.SECONDS.toNanos(1)
 
   val UnlimitedQuota = Quota.upperBound(Long.MaxValue)
-  val DefaultClientIdQuotaId = QuotaId(None, Some(ConfigEntityName.Default), Some(ConfigEntityName.Default))
-  val DefaultUserQuotaId = QuotaId(Some(ConfigEntityName.Default), None, None)
-  val DefaultUserClientIdQuotaId = QuotaId(Some(ConfigEntityName.Default), Some(ConfigEntityName.Default), Some(ConfigEntityName.Default))
 }
 
 object QuotaTypes {
@@ -71,11 +73,60 @@ object QuotaTypes {
   val ClientIdQuotaEnabled = 1
   val UserQuotaEnabled = 2
   val UserClientIdQuotaEnabled = 4
+  val CustomQuotas = 8 // No metric update optimizations are used with custom quotas
 }
 
-case class QuotaId(sanitizedUser: Option[String], clientId: Option[String], sanitizedClientId: Option[String])
+object ClientQuotaManager {
+  val DefaultClientIdQuotaEntity = KafkaQuotaEntity(None, Some(DefaultClientIdEntity))
+  val DefaultUserQuotaEntity = KafkaQuotaEntity(Some(DefaultUserEntity), None)
+  val DefaultUserClientIdQuotaEntity = KafkaQuotaEntity(Some(DefaultUserEntity), Some(DefaultClientIdEntity))
 
-case class QuotaEntity(quotaId: QuotaId, sanitizedUser: String, clientId: String, sanitizedClientId: String, quota: Quota)
+  case class UserEntity(sanitizedUser: String) extends ClientQuotaEntity.ConfigEntity {
+    override def entityType: ClientQuotaEntity.ConfigEntityType = ClientQuotaEntity.ConfigEntityType.USER
+    override def name: String = Sanitizer.desanitize(sanitizedUser)
+    override def toString: String = s""user $sanitizedUser""
+  }
+
+  case class ClientIdEntity(clientId: String) extends ClientQuotaEntity.ConfigEntity {
+    override def entityType: ClientQuotaEntity.ConfigEntityType = ClientQuotaEntity.ConfigEntityType.CLIENT_ID
+    override def name: String = clientId
+    override def toString: String = s""client-id $clientId""
+  }
+
+  case object DefaultUserEntity extends ClientQuotaEntity.ConfigEntity {
+    override def entityType: ClientQuotaEntity.ConfigEntityType = ClientQuotaEntity.ConfigEntityType.DEFAULT_USER
+    override def name: String = ConfigEntityName.Default
+    override def toString: String = ""default user""
+  }
+
+  case object DefaultClientIdEntity extends ClientQuotaEntity.ConfigEntity {
+    override def entityType: ClientQuotaEntity.ConfigEntityType = ClientQuotaEntity.ConfigEntityType.DEFAULT_CLIENT_ID
+    override def name: String = ConfigEntityName.Default
+    override def toString: String = ""default client-id""
+  }
+
+  case class KafkaQuotaEntity(userEntity: Option[ClientQuotaEntity.ConfigEntity],
+                              clientIdEntity: Option[ClientQuotaEntity.ConfigEntity]) extends ClientQuotaEntity {
+    override def configEntities: util.List[ClientQuotaEntity.ConfigEntity] =
+      (userEntity.toList ++ clientIdEntity.toList).asJava
+    def sanitizedUser: String = userEntity.map {
+      case entity: UserEntity => entity.sanitizedUser
+      case DefaultUserEntity => ConfigEntityName.Default
+    }.getOrElse("""")
+    def clientId: String = clientIdEntity.map(_.name).getOrElse("""")
+
+    override def toString: String = {
+      val user = userEntity.map(_.toString).getOrElse("""")
+      val clientId = clientIdEntity.map(_.toString).getOrElse("""")
+      s""$user $clientId"".trim
+    }
+  }
+
+  object DefaultTags {
+    val User = ""user""
+    val ClientId = ""client-id""
+  }
+}
 
 /**
  * Helper class that records per-client metrics. It is also responsible for maintaining Quota usage statistics
@@ -107,21 +158,26 @@ class ClientQuotaManager(private val config: ClientQuotaManagerConfig,
                          private val metrics: Metrics,
                          private val quotaType: QuotaType,
                          private val time: Time,
-                         threadNamePrefix: String) extends Logging {
-  private val overriddenQuota = new ConcurrentHashMap[QuotaId, Quota]()
+                         threadNamePrefix: String,
+                         clientQuotaCallback: Option[ClientQuotaCallback] = None) extends Logging {
   private val staticConfigClientIdQuota = Quota.upperBound(config.quotaBytesPerSecondDefault)
-  @volatile private var quotaTypesEnabled =
-    if (config.quotaBytesPerSecondDefault == Long.MaxValue) QuotaTypes.NoQuotas
-    else QuotaTypes.ClientIdQuotaEnabled
+  private val clientQuotaType = quotaTypeToClientQuotaType(quotaType)
+  @volatile private var quotaTypesEnabled = clientQuotaCallback match {
+    case Some(_) => QuotaTypes.CustomQuotas
+    case None =>
+      if (config.quotaBytesPerSecondDefault == Long.MaxValue) QuotaTypes.NoQuotas
+      else QuotaTypes.ClientIdQuotaEnabled
+  }
   private val lock = new ReentrantReadWriteLock()
   private val delayQueue = new DelayQueue[ThrottledResponse]()
   private val sensorAccessor = new SensorAccess(lock, metrics)
   private[server] val throttledRequestReaper = new ThrottledRequestReaper(delayQueue, threadNamePrefix)
+  private val quotaCallback = clientQuotaCallback.getOrElse(new DefaultQuotaCallback)
 
   private val delayQueueSensor = metrics.sensor(quotaType + ""-delayQueue"")
   delayQueueSensor.add(metrics.metricName(""queue-size"",
-                                      quotaType.toString,
-                                      ""Tracks the size of the delay queue""), new Total())
+    quotaType.toString,
+    ""Tracks the size of the delay queue""), new Total())
   start() // Use start method to keep findbugs happy
   private def start() {
     throttledRequestReaper.start()
@@ -132,7 +188,7 @@ class ClientQuotaManager(private val config: ClientQuotaManagerConfig,
    * @param delayQueue DelayQueue to dequeue from
    */
   class ThrottledRequestReaper(delayQueue: DelayQueue[ThrottledResponse], prefix: String) extends ShutdownableThread(
-    s""${prefix}ThrottledRequestReaper-${quotaType}"", false) {
+    s""${prefix}ThrottledRequestReaper-$quotaType"", false) {
 
     override def doWork(): Unit = {
       val response: ThrottledResponse = delayQueue.poll(1, TimeUnit.SECONDS)
@@ -158,17 +214,18 @@ class ClientQuotaManager(private val config: ClientQuotaManagerConfig,
    * Records that a user/clientId changed some metric being throttled (produced/consumed bytes, request processing time etc.)
    * If quota has been violated, callback is invoked after a delay, otherwise the callback is invoked immediately.
    * Throttle time calculation may be overridden by sub-classes.
-   * @param sanitizedUser user principal of client
+   *
+   * @param session  the session associated with this request
    * @param clientId clientId that produced/fetched the data
-   * @param value amount of data in bytes or request processing time as a percentage
+   * @param value    amount of data in bytes or request processing time as a percentage
    * @param callback Callback function. This will be triggered immediately if quota is not violated.
    *                 If there is a quota violation, this callback will be triggered after a delay
    * @return Number of milliseconds to delay the response in case of Quota violation.
    *         Zero otherwise
    */
-  def maybeRecordAndThrottle(sanitizedUser: String, clientId: String, value: Double, callback: Int => Unit): Int = {
+  def maybeRecordAndThrottle(session: Session, clientId: String, value: Double, callback: Int => Unit): Int = {
     if (quotasEnabled) {
-      val clientSensors = getOrCreateQuotaSensors(sanitizedUser, clientId)
+      val clientSensors = getOrCreateQuotaSensors(session, clientId)
       recordAndThrottleOnQuotaViolation(clientSensors, value, callback)
     } else {
       // Don't record any metrics if quotas are not enabled at any level
@@ -187,9 +244,8 @@ class ClientQuotaManager(private val config: ClientQuotaManagerConfig,
     } catch {
       case _: QuotaViolationException =>
         // Compute the delay
-        val clientQuotaEntity = clientSensors.quotaEntity
-        val clientMetric = metrics.metrics().get(clientRateMetricName(clientQuotaEntity.sanitizedUser, clientQuotaEntity.clientId))
-        throttleTimeMs = throttleTime(clientMetric, getQuotaMetricConfig(clientQuotaEntity.quota)).toInt
+        val clientMetric = metrics.metrics().get(clientRateMetricName(clientSensors.metricTags))
+        throttleTimeMs = throttleTime(clientMetric).toInt
         clientSensors.throttleTimeSensor.record(throttleTimeMs)
         // If delayed, add the element to the delayQueue
         delayQueue.add(new ThrottledResponse(time, throttleTimeMs, callback))
@@ -209,126 +265,27 @@ class ClientQuotaManager(private val config: ClientQuotaManagerConfig,
   }
 
   /**
-   * Determines the quota-id for the client with the specified user principal
-   * and client-id and returns the quota entity that encapsulates the quota-id
-   * and the associated quota override or default quota.
+   * Returns the quota for the client with the specified (non-encoded) user principal and client-id.
    *
+   * Note: this method is expensive, it is meant to be used by tests only
    */
-  private def quotaEntity(sanitizedUser: String, clientId: String, sanitizedClientId: String) : QuotaEntity = {
-    quotaTypesEnabled match {
-      case QuotaTypes.NoQuotas | QuotaTypes.ClientIdQuotaEnabled =>
-        val quotaId = QuotaId(None, Some(clientId), Some(sanitizedClientId))
-        var quota = overriddenQuota.get(quotaId)
-        if (quota == null) {
-          quota = overriddenQuota.get(ClientQuotaManagerConfig.DefaultClientIdQuotaId)
-          if (quota == null)
-            quota = staticConfigClientIdQuota
-        }
-        QuotaEntity(quotaId, """", clientId, sanitizedClientId, quota)
-      case QuotaTypes.UserQuotaEnabled =>
-        val quotaId = QuotaId(Some(sanitizedUser), None, None)
-        var quota = overriddenQuota.get(quotaId)
-        if (quota == null) {
-          quota = overriddenQuota.get(ClientQuotaManagerConfig.DefaultUserQuotaId)
-          if (quota == null)
-            quota = ClientQuotaManagerConfig.UnlimitedQuota
-        }
-        QuotaEntity(quotaId, sanitizedUser, """", """", quota)
-      case QuotaTypes.UserClientIdQuotaEnabled =>
-        val quotaId = QuotaId(Some(sanitizedUser), Some(clientId), Some(sanitizedClientId))
-        var quota = overriddenQuota.get(quotaId)
-        if (quota == null) {
-          quota = overriddenQuota.get(QuotaId(Some(sanitizedUser), Some(ConfigEntityName.Default), Some(ConfigEntityName.Default)))
-          if (quota == null) {
-            quota = overriddenQuota.get(QuotaId(Some(ConfigEntityName.Default), Some(clientId), Some(sanitizedClientId)))
-            if (quota == null) {
-              quota = overriddenQuota.get(ClientQuotaManagerConfig.DefaultUserClientIdQuotaId)
-              if (quota == null)
-                quota = ClientQuotaManagerConfig.UnlimitedQuota
-            }
-          }
-        }
-        QuotaEntity(quotaId, sanitizedUser, clientId, sanitizedClientId, quota)
-      case _ =>
-        quotaEntityWithMultipleQuotaLevels(sanitizedUser, clientId, sanitizedClientId)
-    }
-  }
-
-  private def quotaEntityWithMultipleQuotaLevels(sanitizedUser: String, clientId: String, sanitizerClientId: String) : QuotaEntity = {
-    val userClientQuotaId = QuotaId(Some(sanitizedUser), Some(clientId), Some(sanitizerClientId))
-
-    val userQuotaId = QuotaId(Some(sanitizedUser), None, None)
-    val clientQuotaId = QuotaId(None, Some(clientId), Some(sanitizerClientId))
-    var quotaId = userClientQuotaId
-    var quotaConfigId = userClientQuotaId
-    // 1) /config/users/<user>/clients/<client-id>
-    var quota = overriddenQuota.get(quotaConfigId)
-    if (quota == null) {
-      // 2) /config/users/<user>/clients/<default>
-      quotaId = userClientQuotaId
-      quotaConfigId = QuotaId(Some(sanitizedUser), Some(ConfigEntityName.Default), Some(ConfigEntityName.Default))
-      quota = overriddenQuota.get(quotaConfigId)
-
-      if (quota == null) {
-        // 3) /config/users/<user>
-        quotaId = userQuotaId
-        quotaConfigId = quotaId
-        quota = overriddenQuota.get(quotaConfigId)
-
-        if (quota == null) {
-          // 4) /config/users/<default>/clients/<client-id>
-          quotaId = userClientQuotaId
-          quotaConfigId = QuotaId(Some(ConfigEntityName.Default), Some(clientId), Some(sanitizerClientId))
-          quota = overriddenQuota.get(quotaConfigId)
-
-          if (quota == null) {
-            // 5) /config/users/<default>/clients/<default>
-            quotaId = userClientQuotaId
-            quotaConfigId = QuotaId(Some(ConfigEntityName.Default), Some(ConfigEntityName.Default), Some(ConfigEntityName.Default))
-            quota = overriddenQuota.get(quotaConfigId)
-
-            if (quota == null) {
-              // 6) /config/users/<default>
-              quotaId = userQuotaId
-              quotaConfigId = QuotaId(Some(ConfigEntityName.Default), None, None)
-              quota = overriddenQuota.get(quotaConfigId)
-
-              if (quota == null) {
-                // 7) /config/clients/<client-id>
-                quotaId = clientQuotaId
-                quotaConfigId = QuotaId(None, Some(clientId), Some(sanitizerClientId))
-                quota = overriddenQuota.get(quotaConfigId)
-
-                if (quota == null) {
-                  // 8) /config/clients/<default>
-                  quotaId = clientQuotaId
-                  quotaConfigId = QuotaId(None, Some(ConfigEntityName.Default), Some(ConfigEntityName.Default))
-                  quota = overriddenQuota.get(quotaConfigId)
-
-                  if (quota == null) {
-                    quotaId = clientQuotaId
-                    quotaConfigId = null
-                    quota = staticConfigClientIdQuota
-                  }
-                }
-              }
-            }
-          }
-        }
-      }
-    }
-    val quotaUser = if (quotaId == clientQuotaId) """" else sanitizedUser
-    val quotaClientId = if (quotaId == userQuotaId) """" else clientId
-    QuotaEntity(quotaId, quotaUser, quotaClientId, sanitizerClientId, quota)
+  def quota(user: String, clientId: String): Quota = {
+    val userPrincipal = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, user)
+    quota(userPrincipal, clientId)
   }
 
   /**
-   * Returns the quota for the client with the specified (non-encoded) user principal and client-id.
-   * 
+   * Returns the quota for the client with the specified user principal and client-id.
+   *
    * Note: this method is expensive, it is meant to be used by tests only
    */
-  def quota(user: String, clientId: String) = {
-    quotaEntity(Sanitizer.sanitize(user), clientId, Sanitizer.sanitize(clientId)).quota
+  def quota(userPrincipal: KafkaPrincipal, clientId: String): Quota = {
+    val metricTags = quotaCallback.quotaMetricTags(clientQuotaType, userPrincipal, clientId)
+    Quota.upperBound(quotaLimit(metricTags))
+  }
+
+  private def quotaLimit(metricTags: util.Map[String, String]): Double = {
+    Option(quotaCallback.quotaLimit(clientQuotaType, metricTags)).map(_.toDouble)getOrElse(Long.MaxValue)
   }
 
   /*
@@ -339,10 +296,11 @@ class ClientQuotaManager(private val config: ClientQuotaManagerConfig,
    * we need to add a delay of X to W such that O * W / (W + X) = T.
    * Solving for X, we get X = (O - T)/T * W.
    */
-  protected def throttleTime(clientMetric: KafkaMetric, config: MetricConfig): Long = {
+  protected def throttleTime(clientMetric: KafkaMetric): Long = {
+    val config = clientMetric.config
     val rateMetric: Rate = measurableAsRate(clientMetric.metricName(), clientMetric.measurable())
     val quota = config.quota()
-    val difference = clientMetric.value() - quota.bound
+    val difference = clientMetric.metricValue.asInstanceOf[Double] - quota.bound
     // Use the precise window used by the rate calculation
     val throttleTimeMs = difference / quota.bound * rateMetric.windowSize(config, time.milliseconds())
     throttleTimeMs.round
@@ -360,56 +318,72 @@ class ClientQuotaManager(private val config: ClientQuotaManagerConfig,
    * This function either returns the sensors for a given client id or creates them if they don't exist
    * First sensor of the tuple is the quota enforcement sensor. Second one is the throttle time sensor
    */
-  def getOrCreateQuotaSensors(sanitizedUser: String, clientId: String): ClientSensors = {
-    val sanitizedClientId = Sanitizer.sanitize(clientId)
-    val clientQuotaEntity = quotaEntity(sanitizedUser, clientId, sanitizedClientId)
+  def getOrCreateQuotaSensors(session: Session, clientId: String): ClientSensors = {
+    // Use cached sanitized principal if using default callback
+    val metricTags = quotaCallback match {
+      case callback: DefaultQuotaCallback => callback.quotaMetricTags(session.sanitizedUser, clientId)
+      case _ => quotaCallback.quotaMetricTags(clientQuotaType, session.principal, clientId).asScala.toMap
+    }
     // Names of the sensors to access
-    ClientSensors(
-      clientQuotaEntity,
+    val sensors = ClientSensors(
+      metricTags,
       sensorAccessor.getOrCreate(
-        getQuotaSensorName(clientQuotaEntity.quotaId),
+        getQuotaSensorName(metricTags),
         ClientQuotaManagerConfig.InactiveSensorExpirationTimeSeconds,
-        clientRateMetricName(clientQuotaEntity.sanitizedUser, clientQuotaEntity.clientId),
-        Some(getQuotaMetricConfig(clientQuotaEntity.quota)),
+        clientRateMetricName(metricTags),
+        Some(getQuotaMetricConfig(metricTags)),
         new Rate
       ),
-      sensorAccessor.getOrCreate(getThrottleTimeSensorName(clientQuotaEntity.quotaId),
+      sensorAccessor.getOrCreate(getThrottleTimeSensorName(metricTags),
         ClientQuotaManagerConfig.InactiveSensorExpirationTimeSeconds,
-        throttleMetricName(clientQuotaEntity),
+        throttleMetricName(metricTags),
         None,
         new Avg
       )
     )
+    if (quotaCallback.quotaResetRequired(clientQuotaType))
+      updateQuotaMetricConfigs()
+    sensors
   }
 
-  private def getThrottleTimeSensorName(quotaId: QuotaId): String = quotaType + ""ThrottleTime-"" + quotaId.sanitizedUser.getOrElse("""") + ':' + quotaId.clientId.getOrElse("""")
+  private def metricTagsToSensorSuffix(metricTags: Map[String, String]): String =
+    metricTags.values.mkString("":"")
 
-  private def getQuotaSensorName(quotaId: QuotaId): String = quotaType + ""-"" + quotaId.sanitizedUser.getOrElse("""") + ':' + quotaId.clientId.getOrElse("""")
+  private def getThrottleTimeSensorName(metricTags: Map[String, String]): String =
+    s""${quotaType}ThrottleTime-${metricTagsToSensorSuffix(metricTags)}""
 
-  protected def getQuotaMetricConfig(quota: Quota): MetricConfig = {
+  private def getQuotaSensorName(metricTags: Map[String, String]): String =
+    s""$quotaType-${metricTagsToSensorSuffix(metricTags)}""
+
+  private def getQuotaMetricConfig(metricTags: Map[String, String]): MetricConfig = {
+    getQuotaMetricConfig(quotaLimit(metricTags.asJava))
+  }
+
+  private def getQuotaMetricConfig(quotaLimit: Double): MetricConfig = {
     new MetricConfig()
-            .timeWindow(config.quotaWindowSizeSeconds, TimeUnit.SECONDS)
-            .samples(config.numQuotaSamples)
-            .quota(quota)
+      .timeWindow(config.quotaWindowSizeSeconds, TimeUnit.SECONDS)
+      .samples(config.numQuotaSamples)
+      .quota(new Quota(quotaLimit, true))
   }
 
   protected def getOrCreateSensor(sensorName: String, metricName: MetricName): Sensor = {
     sensorAccessor.getOrCreate(
-        sensorName,
-        ClientQuotaManagerConfig.InactiveSensorExpirationTimeSeconds,
-        metricName,
-        None,
-        new Rate
-      )
+      sensorName,
+      ClientQuotaManagerConfig.InactiveSensorExpirationTimeSeconds,
+      metricName,
+      None,
+      new Rate
+    )
   }
 
   /**
    * Overrides quotas for <user>, <client-id> or <user, client-id> or the dynamic defaults
    * for any of these levels.
-   * @param sanitizedUser user to override if quota applies to <user> or <user, client-id>
-   * @param clientId client to override if quota applies to <client-id> or <user, client-id>
+   *
+   * @param sanitizedUser     user to override if quota applies to <user> or <user, client-id>
+   * @param clientId          client to override if quota applies to <client-id> or <user, client-id>
    * @param sanitizedClientId sanitized client ID to override if quota applies to <client-id> or <user, client-id>
-   * @param quota custom quota to apply or None if quota override is being removed
+   * @param quota             custom quota to apply or None if quota override is being removed
    */
   def updateQuota(sanitizedUser: Option[String], clientId: Option[String], sanitizedClientId: Option[String], quota: Option[Quota]) {
     /*
@@ -421,86 +395,233 @@ class ClientQuotaManager(private val config: ClientQuotaManagerConfig,
      */
     lock.writeLock().lock()
     try {
-      val quotaId = QuotaId(sanitizedUser, clientId, sanitizedClientId)
-      val userInfo = sanitizedUser match {
-        case Some(ConfigEntityName.Default) => ""default user ""
-        case Some(user) => ""user "" + user + "" ""
-        case None => """"
+      val userEntity = sanitizedUser.map {
+        case ConfigEntityName.Default => DefaultUserEntity
+        case user => UserEntity(user)
       }
-      val clientIdInfo = clientId match {
-        case Some(ConfigEntityName.Default) => ""default client-id""
-        case Some(id) => ""client-id "" + id
-        case None => """"
+      val clientIdEntity = sanitizedClientId.map {
+        case ConfigEntityName.Default => DefaultClientIdEntity
+        case _ => ClientIdEntity(clientId.getOrElse(throw new IllegalStateException(""Client-id not provided"")))
       }
+      val quotaEntity = KafkaQuotaEntity(userEntity, clientIdEntity)
+
+      if (userEntity.nonEmpty) {
+        if (quotaEntity.clientIdEntity.nonEmpty)
+          quotaTypesEnabled |= QuotaTypes.UserClientIdQuotaEnabled
+        else
+          quotaTypesEnabled |= QuotaTypes.UserQuotaEnabled
+      } else if (clientIdEntity.nonEmpty)
+        quotaTypesEnabled |= QuotaTypes.ClientIdQuotaEnabled
+
       quota match {
-        case Some(newQuota) =>
-          info(s""Changing ${quotaType} quota for ${userInfo}${clientIdInfo} to $newQuota.bound}"")
-          overriddenQuota.put(quotaId, newQuota)
-          (sanitizedUser, clientId) match {
-            case (Some(_), Some(_)) => quotaTypesEnabled |= QuotaTypes.UserClientIdQuotaEnabled
-            case (Some(_), None) => quotaTypesEnabled |= QuotaTypes.UserQuotaEnabled
-            case (None, Some(_)) => quotaTypesEnabled |= QuotaTypes.ClientIdQuotaEnabled
-            case (None, None) =>
-          }
-        case None =>
-          info(s""Removing ${quotaType} quota for ${userInfo}${clientIdInfo}"")
-          overriddenQuota.remove(quotaId)
+        case Some(newQuota) => quotaCallback.updateQuota(clientQuotaType, quotaEntity, newQuota.bound)
+        case None => quotaCallback.removeQuota(clientQuotaType, quotaEntity)
       }
+      val updatedEntity = if (userEntity.contains(DefaultUserEntity) || clientIdEntity.contains(DefaultClientIdEntity))
+        None // more than one entity may need updating, so `updateQuotaMetricConfigs` will go through all metrics
+      else
+        Some(quotaEntity)
+      updateQuotaMetricConfigs(updatedEntity)
 
-      val quotaMetricName = clientRateMetricName(sanitizedUser.getOrElse(""""), clientId.getOrElse(""""))
-      val allMetrics = metrics.metrics()
+    } finally {
+      lock.writeLock().unlock()
+    }
+  }
 
-      // If multiple-levels of quotas are defined or if this is a default quota update, traverse metrics
-      // to find all affected values. Otherwise, update just the single matching one.
-      val singleUpdate = quotaTypesEnabled match {
-        case QuotaTypes.NoQuotas | QuotaTypes.ClientIdQuotaEnabled | QuotaTypes.UserQuotaEnabled | QuotaTypes.UserClientIdQuotaEnabled =>
-          !sanitizedUser.filter(_ == ConfigEntityName.Default).isDefined && !clientId.filter(_ == ConfigEntityName.Default).isDefined
-        case _ => false
+  /**
+   * Updates metrics configs. This is invoked when quota configs are updated in ZooKeeper
+   * or when partitions leaders change and custom callbacks that implement partition-based quotas
+   * have updated quotas.
+   * @param updatedQuotaEntity If set to one entity and quotas have only been enabled at one
+   *    level, then an optimized update is performed with a single metric update. If None is provided,
+   *    or if custom callbacks are used or if multi-level quotas have been enabled, all metric configs
+   *    are checked and updated if required.
+   */
+  def updateQuotaMetricConfigs(updatedQuotaEntity: Option[KafkaQuotaEntity] = None): Unit = {
+    val allMetrics = metrics.metrics()
+
+    // If using custom quota callbacks or if multiple-levels of quotas are defined or
+    // if this is a default quota update, traverse metrics to find all affected values.
+    // Otherwise, update just the single matching one.
+    val singleUpdate = quotaTypesEnabled match {
+      case QuotaTypes.NoQuotas | QuotaTypes.ClientIdQuotaEnabled | QuotaTypes.UserQuotaEnabled | QuotaTypes.UserClientIdQuotaEnabled =>
+        updatedQuotaEntity.nonEmpty
+      case _ => false
+    }
+    if (singleUpdate) {
+      val quotaEntity = updatedQuotaEntity.getOrElse(throw new IllegalStateException(""Quota entity not specified""))
+      val user = quotaEntity.sanitizedUser
+      val clientId = quotaEntity.clientId
+      val metricTags = Map(DefaultTags.User -> user, DefaultTags.ClientId -> clientId)
+
+      val quotaMetricName = clientRateMetricName(metricTags)
+      // Change the underlying metric config if the sensor has been created
+      val metric = allMetrics.get(quotaMetricName)
+      if (metric != null) {
+        Option(quotaCallback.quotaLimit(clientQuotaType, metricTags.asJava)).foreach { newQuota =>
+          info(s""Sensor for $quotaEntity already exists. Changing quota to $newQuota in MetricConfig"")
+          metric.config(getQuotaMetricConfig(newQuota))
+        }
       }
-      if (singleUpdate) {
-          // Change the underlying metric config if the sensor has been created
-          val metric = allMetrics.get(quotaMetricName)
-          if (metric != null) {
-            val metricConfigEntity = quotaEntity(sanitizedUser.getOrElse(""""), clientId.getOrElse(""""), sanitizedClientId.getOrElse(""""))
-            val newQuota = metricConfigEntity.quota
-            info(s""Sensor for ${userInfo}${clientIdInfo} already exists. Changing quota to ${newQuota.bound()} in MetricConfig"")
-            metric.config(getQuotaMetricConfig(newQuota))
-          }
-      } else {
-          allMetrics.asScala.filterKeys(n => n.name == quotaMetricName.name && n.group == quotaMetricName.group).foreach {
-            case (metricName, metric) =>
-              val userTag = if (metricName.tags.containsKey(""user"")) metricName.tags.get(""user"") else """"
-              val clientIdTag = if (metricName.tags.containsKey(""client-id"")) metricName.tags.get(""client-id"") else """"
-              val metricConfigEntity = quotaEntity(userTag, clientIdTag, Sanitizer.sanitize(clientIdTag))
-              if (metricConfigEntity.quota != metric.config.quota) {
-                val newQuota = metricConfigEntity.quota
-                info(s""Sensor for quota-id ${metricConfigEntity.quotaId} already exists. Setting quota to ${newQuota.bound} in MetricConfig"")
-                metric.config(getQuotaMetricConfig(newQuota))
-              }
+    } else {
+      val quotaMetricName = clientRateMetricName(Map.empty)
+      allMetrics.asScala.filterKeys(n => n.name == quotaMetricName.name && n.group == quotaMetricName.group).foreach {
+        case (metricName, metric) =>
+          val metricTags = metricName.tags
+          Option(quotaCallback.quotaLimit(clientQuotaType, metricTags)).foreach { quota =>
+            val newQuota = quota.asInstanceOf[Double]
+            if (newQuota != metric.config.quota.bound) {
+              info(s""Sensor for quota-id $metricTags already exists. Setting quota to $newQuota in MetricConfig"")
+              metric.config(getQuotaMetricConfig(newQuota))
+            }
           }
       }
-
-    } finally {
-      lock.writeLock().unlock()
     }
   }
 
-  protected def clientRateMetricName(sanitizedUser: String, clientId: String): MetricName = {
+  protected def clientRateMetricName(quotaMetricTags: Map[String, String]): MetricName = {
     metrics.metricName(""byte-rate"", quotaType.toString,
-                   ""Tracking byte-rate per user/client-id"",
-                   ""user"", sanitizedUser,
-                   ""client-id"", clientId)
+      ""Tracking byte-rate per user/client-id"",
+      quotaMetricTags.asJava)
   }
 
-  private def throttleMetricName(quotaEntity: QuotaEntity): MetricName = {
+  private def throttleMetricName(quotaMetricTags: Map[String, String]): MetricName = {
     metrics.metricName(""throttle-time"",
-                       quotaType.toString,
-                       ""Tracking average throttle-time per user/client-id"",
-                       ""user"", quotaEntity.sanitizedUser,
-                       ""client-id"", quotaEntity.clientId)
+      quotaType.toString,
+      ""Tracking average throttle-time per user/client-id"",
+      quotaMetricTags.asJava)
   }
 
-  def shutdown() = {
+  private def quotaTypeToClientQuotaType(quotaType: QuotaType): ClientQuotaType = {
+    quotaType match {
+      case QuotaType.Fetch => ClientQuotaType.FETCH
+      case QuotaType.Produce => ClientQuotaType.PRODUCE
+      case QuotaType.Request => ClientQuotaType.REQUEST
+      case _ => throw new IllegalArgumentException(s""Not a client quota type: $quotaType"")
+    }
+  }
+
+  def shutdown(): Unit = {
     throttledRequestReaper.shutdown()
   }
+
+  class DefaultQuotaCallback extends ClientQuotaCallback {
+    private val overriddenQuotas = new ConcurrentHashMap[ClientQuotaEntity, Quota]()
+
+    override def configure(configs: util.Map[String, _]): Unit = {}
+
+    override def quotaMetricTags(quotaType: ClientQuotaType, principal: KafkaPrincipal, clientId: String): util.Map[String, String] = {
+      quotaMetricTags(Sanitizer.sanitize(principal.getName), clientId).asJava
+    }
+
+    override def quotaLimit(quotaType: ClientQuotaType, metricTags: util.Map[String, String]): lang.Double = {
+      val sanitizedUser = metricTags.get(DefaultTags.User)
+      val clientId = metricTags.get(DefaultTags.ClientId)
+      var quota: Quota = null
+
+      if (sanitizedUser != null && clientId != null) {
+        val userEntity = Some(UserEntity(sanitizedUser))
+        val clientIdEntity = Some(ClientIdEntity(clientId))
+        if (!sanitizedUser.isEmpty && !clientId.isEmpty) {
+          // /config/users/<user>/clients/<client-id>
+          quota = overriddenQuotas.get(KafkaQuotaEntity(userEntity, clientIdEntity))
+          if (quota == null) {
+            // /config/users/<user>/clients/<default>
+            quota = overriddenQuotas.get(KafkaQuotaEntity(userEntity, Some(DefaultClientIdEntity)))
+          }
+          if (quota == null) {
+            // /config/users/<default>/clients/<client-id>
+            quota = overriddenQuotas.get(KafkaQuotaEntity(Some(DefaultUserEntity), clientIdEntity))
+          }
+          if (quota == null) {
+            // /config/users/<default>/clients/<default>
+            quota = overriddenQuotas.get(DefaultUserClientIdQuotaEntity)
+          }
+        } else if (!sanitizedUser.isEmpty) {
+          // /config/users/<user>
+          quota = overriddenQuotas.get(KafkaQuotaEntity(userEntity, None))
+          if (quota == null) {
+            // /config/users/<default>
+            quota = overriddenQuotas.get(DefaultUserQuotaEntity)
+          }
+        } else if (!clientId.isEmpty) {
+          // /config/clients/<client-id>
+          quota = overriddenQuotas.get(KafkaQuotaEntity(None, clientIdEntity))
+          if (quota == null) {
+            // /config/clients/<default>
+            quota = overriddenQuotas.get(DefaultClientIdQuotaEntity)
+          }
+          if (quota == null)
+            quota = staticConfigClientIdQuota
+        }
+      }
+      if (quota == null) null else quota.bound
+    }
+
+    override def updateClusterMetadata(cluster: Cluster): Boolean = {
+      // Default quota callback does not use any cluster metadata
+      false
+    }
+
+    override def updateQuota(quotaType: ClientQuotaType, entity: ClientQuotaEntity, newValue: Double): Unit = {
+      val quotaEntity = entity.asInstanceOf[KafkaQuotaEntity]
+      info(s""Changing $quotaType quota for $quotaEntity to $newValue"")
+      overriddenQuotas.put(quotaEntity, new Quota(newValue, true))
+    }
+
+    override def removeQuota(quotaType: ClientQuotaType, entity: ClientQuotaEntity): Unit = {
+      val quotaEntity = entity.asInstanceOf[KafkaQuotaEntity]
+      info(s""Removing $quotaType quota for $quotaEntity"")
+      overriddenQuotas.remove(quotaEntity)
+    }
+
+    override def quotaResetRequired(quotaType: ClientQuotaType): Boolean = false
+
+    def quotaMetricTags(sanitizedUser: String, clientId: String) : Map[String, String] = {
+      val (userTag, clientIdTag) = quotaTypesEnabled match {
+        case QuotaTypes.NoQuotas | QuotaTypes.ClientIdQuotaEnabled =>
+          ("""", clientId)
+        case QuotaTypes.UserQuotaEnabled =>
+          (sanitizedUser, """")
+        case QuotaTypes.UserClientIdQuotaEnabled =>
+          (sanitizedUser, clientId)
+        case _ =>
+          val userEntity = Some(UserEntity(sanitizedUser))
+          val clientIdEntity = Some(ClientIdEntity(clientId))
+
+          var metricTags = (sanitizedUser, clientId)
+          // 1) /config/users/<user>/clients/<client-id>
+          if (!overriddenQuotas.containsKey(KafkaQuotaEntity(userEntity, clientIdEntity))) {
+            // 2) /config/users/<user>/clients/<default>
+            metricTags = (sanitizedUser, clientId)
+            if (!overriddenQuotas.containsKey(KafkaQuotaEntity(userEntity, Some(DefaultClientIdEntity)))) {
+              // 3) /config/users/<user>
+              metricTags = (sanitizedUser, """")
+              if (!overriddenQuotas.containsKey(KafkaQuotaEntity(userEntity, None))) {
+                // 4) /config/users/<default>/clients/<client-id>
+                metricTags = (sanitizedUser, clientId)
+                if (!overriddenQuotas.containsKey(KafkaQuotaEntity(Some(DefaultUserEntity), clientIdEntity))) {
+                  // 5) /config/users/<default>/clients/<default>
+                  metricTags = (sanitizedUser, clientId)
+                  if (!overriddenQuotas.containsKey(DefaultUserClientIdQuotaEntity)) {
+                    // 6) /config/users/<default>
+                    metricTags = (sanitizedUser, """")
+                    if (!overriddenQuotas.containsKey(DefaultUserQuotaEntity)) {
+                      // 7) /config/clients/<client-id>
+                      // 8) /config/clients/<default>
+                      // 9) static client-id quota
+                      metricTags = ("""", clientId)
+                    }
+                  }
+                }
+              }
+            }
+          }
+          metricTags
+      }
+      Map(DefaultTags.User -> userTag, DefaultTags.ClientId -> clientIdTag)
+    }
+
+    override def close(): Unit = {}
+  }
 }
diff --git a/core/src/main/scala/kafka/server/ClientRequestQuotaManager.scala b/core/src/main/scala/kafka/server/ClientRequestQuotaManager.scala
index 59fa4218acb..3078a62175e 100644
--- a/core/src/main/scala/kafka/server/ClientRequestQuotaManager.scala
+++ b/core/src/main/scala/kafka/server/ClientRequestQuotaManager.scala
@@ -22,13 +22,17 @@ import kafka.network.RequestChannel
 import org.apache.kafka.common.MetricName
 import org.apache.kafka.common.metrics._
 import org.apache.kafka.common.utils.Time
+import org.apache.kafka.server.quota.ClientQuotaCallback
+
+import scala.collection.JavaConverters._
 
 
 class ClientRequestQuotaManager(private val config: ClientQuotaManagerConfig,
                                 private val metrics: Metrics,
                                 private val time: Time,
-                                threadNamePrefix: String)
-                                extends ClientQuotaManager(config, metrics, QuotaType.Request, time, threadNamePrefix) {
+                                threadNamePrefix: String,
+                                quotaCallback: Option[ClientQuotaCallback])
+                                extends ClientQuotaManager(config, metrics, QuotaType.Request, time, threadNamePrefix, quotaCallback) {
   val maxThrottleTimeMs = TimeUnit.SECONDS.toMillis(this.config.quotaWindowSizeSeconds)
   def exemptSensor = getOrCreateSensor(exemptSensorName, exemptMetricName)
 
@@ -43,7 +47,7 @@ class ClientRequestQuotaManager(private val config: ClientQuotaManagerConfig,
     }
 
     if (quotasEnabled) {
-      val quotaSensors = getOrCreateQuotaSensors(request.session.sanitizedUser, request.header.clientId)
+      val quotaSensors = getOrCreateQuotaSensors(request.session, request.header.clientId)
       request.recordNetworkThreadTimeCallback = Some(timeNanos => recordNoThrottle(quotaSensors, nanosToPercentage(timeNanos)))
 
       recordAndThrottleOnQuotaViolation(
@@ -62,15 +66,14 @@ class ClientRequestQuotaManager(private val config: ClientQuotaManagerConfig,
     }
   }
 
-  override protected def throttleTime(clientMetric: KafkaMetric, config: MetricConfig): Long = {
-    math.min(super.throttleTime(clientMetric, config), maxThrottleTimeMs)
+  override protected def throttleTime(clientMetric: KafkaMetric): Long = {
+    math.min(super.throttleTime(clientMetric), maxThrottleTimeMs)
   }
 
-  override protected def clientRateMetricName(sanitizedUser: String, clientId: String): MetricName = {
+  override protected def clientRateMetricName(quotaMetricTags: Map[String, String]): MetricName = {
     metrics.metricName(""request-time"", QuotaType.Request.toString,
-                   ""Tracking request-time per user/client-id"",
-                   ""user"", sanitizedUser,
-                   ""client-id"", clientId)
+      ""Tracking request-time per user/client-id"",
+      quotaMetricTags.asJava)
   }
 
   private def exemptMetricName: MetricName = {
diff --git a/core/src/main/scala/kafka/server/DynamicBrokerConfig.scala b/core/src/main/scala/kafka/server/DynamicBrokerConfig.scala
index 766907a7de2..1839768ecd4 100755
--- a/core/src/main/scala/kafka/server/DynamicBrokerConfig.scala
+++ b/core/src/main/scala/kafka/server/DynamicBrokerConfig.scala
@@ -75,13 +75,12 @@ object DynamicBrokerConfig {
 
   private[server] val DynamicSecurityConfigs = SslConfigs.RECONFIGURABLE_CONFIGS.asScala
 
-  val AllDynamicConfigs = mutable.Set[String]()
-  AllDynamicConfigs ++= DynamicSecurityConfigs
-  AllDynamicConfigs ++= LogCleaner.ReconfigurableConfigs
-  AllDynamicConfigs ++= DynamicLogConfig.ReconfigurableConfigs
-  AllDynamicConfigs ++= DynamicThreadPool.ReconfigurableConfigs
-  AllDynamicConfigs ++= Set(KafkaConfig.MetricReporterClassesProp)
-  AllDynamicConfigs ++= DynamicListenerConfig.ReconfigurableConfigs
+  val AllDynamicConfigs = DynamicSecurityConfigs ++
+    LogCleaner.ReconfigurableConfigs ++
+    DynamicLogConfig.ReconfigurableConfigs ++
+    DynamicThreadPool.ReconfigurableConfigs ++
+    Set(KafkaConfig.MetricReporterClassesProp) ++
+    DynamicListenerConfig.ReconfigurableConfigs
 
   private val PerBrokerConfigs = DynamicSecurityConfigs  ++
     DynamicListenerConfig.ReconfigurableConfigs
@@ -159,16 +158,17 @@ class DynamicBrokerConfig(private val kafkaConfig: KafkaConfig) extends Logging
       addBrokerReconfigurable(kafkaServer.logManager.cleaner)
     addReconfigurable(new DynamicLogConfig(kafkaServer.logManager))
     addReconfigurable(new DynamicMetricsReporters(kafkaConfig.brokerId, kafkaServer))
+    addReconfigurable(new DynamicClientQuotaCallback(kafkaConfig.brokerId, kafkaConfig))
     addBrokerReconfigurable(new DynamicListenerConfig(kafkaServer))
   }
 
   def addReconfigurable(reconfigurable: Reconfigurable): Unit = CoreUtils.inWriteLock(lock) {
-    require(reconfigurable.reconfigurableConfigs.asScala.forall(AllDynamicConfigs.contains))
+    verifyReconfigurableConfigs(reconfigurable.reconfigurableConfigs.asScala)
     reconfigurables += reconfigurable
   }
 
   def addBrokerReconfigurable(reconfigurable: BrokerReconfigurable): Unit = CoreUtils.inWriteLock(lock) {
-    require(reconfigurable.reconfigurableConfigs.forall(AllDynamicConfigs.contains))
+    verifyReconfigurableConfigs(reconfigurable.reconfigurableConfigs)
     brokerReconfigurables += reconfigurable
   }
 
@@ -176,6 +176,11 @@ class DynamicBrokerConfig(private val kafkaConfig: KafkaConfig) extends Logging
     reconfigurables -= reconfigurable
   }
 
+  private def verifyReconfigurableConfigs(configNames: Set[String]): Unit = CoreUtils.inWriteLock(lock) {
+    val nonDynamic = configNames.filter(DynamicConfig.Broker.nonDynamicProps.contains)
+    require(nonDynamic.isEmpty, s""Reconfigurable contains non-dynamic configs $nonDynamic"")
+  }
+
   // Visibility for testing
   private[server] def currentKafkaConfig: KafkaConfig = CoreUtils.inReadLock(lock) {
     currentConfig
@@ -705,6 +710,36 @@ object DynamicListenerConfig {
   )
 }
 
+class DynamicClientQuotaCallback(brokerId: Int, config: KafkaConfig) extends Reconfigurable {
+
+  override def configure(configs: util.Map[String, _]): Unit = {}
+
+  override def reconfigurableConfigs(): util.Set[String] = {
+    val configs = new util.HashSet[String]()
+    config.quotaCallback.foreach {
+      case callback: Reconfigurable => configs.addAll(callback.reconfigurableConfigs)
+      case _ =>
+    }
+    configs
+  }
+
+  override def validateReconfiguration(configs: util.Map[String, _]): Unit = {
+    config.quotaCallback.foreach {
+      case callback: Reconfigurable => callback.validateReconfiguration(configs)
+      case _ =>
+    }
+  }
+
+  override def reconfigure(configs: util.Map[String, _]): Unit = {
+    config.quotaCallback.foreach {
+      case callback: Reconfigurable =>
+        config.dynamicConfig.maybeReconfigure(callback, config.dynamicConfig.currentKafkaConfig, configs)
+        true
+      case _ => false
+    }
+  }
+}
+
 class DynamicListenerConfig(server: KafkaServer) extends BrokerReconfigurable with Logging {
 
   override def reconfigurableConfigs: Set[String] = {
diff --git a/core/src/main/scala/kafka/server/KafkaApis.scala b/core/src/main/scala/kafka/server/KafkaApis.scala
index 9e79afa2a5b..f43f8a5b4b6 100644
--- a/core/src/main/scala/kafka/server/KafkaApis.scala
+++ b/core/src/main/scala/kafka/server/KafkaApis.scala
@@ -231,6 +231,13 @@ class KafkaApis(val requestChannel: RequestChannel,
           adminManager.tryCompleteDelayedTopicOperations(topic)
         }
       }
+      config.quotaCallback.foreach { callback =>
+        if (callback.updateClusterMetadata(metadataCache.getClusterMetadata(clusterId, request.context.listenerName))) {
+          quotas.fetch.updateQuotaMetricConfigs()
+          quotas.produce.updateQuotaMetricConfigs()
+          quotas.request.updateQuotaMetricConfigs()
+        }
+      }
       sendResponseExemptThrottle(request, new UpdateMetadataResponse(Errors.NONE))
     } else {
       sendResponseMaybeThrottle(request, _ => new UpdateMetadataResponse(Errors.CLUSTER_AUTHORIZATION_FAILED))
@@ -445,7 +452,7 @@ class KafkaApis(val requestChannel: RequestChannel,
       request.apiRemoteCompleteTimeNanos = time.nanoseconds
 
       quotas.produce.maybeRecordAndThrottle(
-        request.session.sanitizedUser,
+        request.session,
         request.header.clientId,
         numBytesAppended,
         produceResponseCallback)
@@ -610,7 +617,7 @@ class KafkaApis(val requestChannel: RequestChannel,
         // This may be slightly different from the actual response size. But since down conversions
         // result in data being loaded into memory, it is better to do this after throttling to avoid OOM.
         val responseStruct = unconvertedFetchResponse.toStruct(versionId)
-        quotas.fetch.maybeRecordAndThrottle(request.session.sanitizedUser, clientId, responseStruct.sizeOf,
+        quotas.fetch.maybeRecordAndThrottle(request.session, clientId, responseStruct.sizeOf,
           fetchResponseCallback)
       }
     }
diff --git a/core/src/main/scala/kafka/server/KafkaConfig.scala b/core/src/main/scala/kafka/server/KafkaConfig.scala
index 5a1dca395bb..096b918d5d9 100755
--- a/core/src/main/scala/kafka/server/KafkaConfig.scala
+++ b/core/src/main/scala/kafka/server/KafkaConfig.scala
@@ -37,6 +37,7 @@ import org.apache.kafka.common.metrics.Sensor
 import org.apache.kafka.common.network.ListenerName
 import org.apache.kafka.common.record.TimestampType
 import org.apache.kafka.common.security.auth.SecurityProtocol
+import org.apache.kafka.server.quota.ClientQuotaCallback
 
 import scala.collection.JavaConverters._
 import scala.collection.Map
@@ -390,6 +391,7 @@ object KafkaConfig {
   val QuotaWindowSizeSecondsProp = ""quota.window.size.seconds""
   val ReplicationQuotaWindowSizeSecondsProp = ""replication.quota.window.size.seconds""
   val AlterLogDirsReplicationQuotaWindowSizeSecondsProp = ""alter.log.dirs.replication.quota.window.size.seconds""
+  val ClientQuotaCallbackClassProp = ""client.quota.callback.class""
 
   val DeleteTopicEnableProp = ""delete.topic.enable""
   val CompressionTypeProp = ""compression.type""
@@ -672,6 +674,10 @@ object KafkaConfig {
   val QuotaWindowSizeSecondsDoc = ""The time span of each sample for client quotas""
   val ReplicationQuotaWindowSizeSecondsDoc = ""The time span of each sample for replication quotas""
   val AlterLogDirsReplicationQuotaWindowSizeSecondsDoc = ""The time span of each sample for alter log dirs replication quotas""
+  val ClientQuotaCallbackClassDoc = ""The fully qualified name of a class that implements the ClientQuotaCallback interface, "" +
+    ""which is used to determine quota limits applied to client requests. By default, <user, client-id>, <user> or <client-id> "" +
+    ""quotas stored in ZooKeeper are applied. For any given request, the most specific quota that matches the user principal "" +
+    ""of the session and the client-id of the request is applied.""
   /** ********* Transaction Configuration ***********/
   val TransactionIdExpirationMsDoc = ""The maximum time of inactivity before a transactional id is expired by the "" +
     ""transaction coordinator. Note that this also influences producer id expiration: Producer ids are guaranteed to expire "" +
@@ -913,6 +919,7 @@ object KafkaConfig {
       .define(QuotaWindowSizeSecondsProp, INT, Defaults.QuotaWindowSizeSeconds, atLeast(1), LOW, QuotaWindowSizeSecondsDoc)
       .define(ReplicationQuotaWindowSizeSecondsProp, INT, Defaults.ReplicationQuotaWindowSizeSeconds, atLeast(1), LOW, ReplicationQuotaWindowSizeSecondsDoc)
       .define(AlterLogDirsReplicationQuotaWindowSizeSecondsProp, INT, Defaults.AlterLogDirsReplicationQuotaWindowSizeSeconds, atLeast(1), LOW, AlterLogDirsReplicationQuotaWindowSizeSecondsDoc)
+      .define(ClientQuotaCallbackClassProp, CLASS, null, LOW, ClientQuotaCallbackClassDoc)
 
       /** ********* SSL Configuration ****************/
       .define(PrincipalBuilderClassProp, CLASS, null, MEDIUM, PrincipalBuilderClassDoc)
@@ -1204,6 +1211,7 @@ class KafkaConfig(val props: java.util.Map[_, _], doLog: Boolean, dynamicConfigO
   val replicationQuotaWindowSizeSeconds = getInt(KafkaConfig.ReplicationQuotaWindowSizeSecondsProp)
   val numAlterLogDirsReplicationQuotaSamples = getInt(KafkaConfig.NumAlterLogDirsReplicationQuotaSamplesProp)
   val alterLogDirsReplicationQuotaWindowSizeSeconds = getInt(KafkaConfig.AlterLogDirsReplicationQuotaWindowSizeSecondsProp)
+  val quotaCallback = Option(getConfiguredInstance(KafkaConfig.ClientQuotaCallbackClassProp, classOf[ClientQuotaCallback]))
 
   /** ********* Transaction Configuration **************/
   val transactionIdExpirationMs = getInt(KafkaConfig.TransactionalIdExpirationMsProp)
diff --git a/core/src/main/scala/kafka/server/KafkaServer.scala b/core/src/main/scala/kafka/server/KafkaServer.scala
index d7ca65658f6..437912569ff 100755
--- a/core/src/main/scala/kafka/server/KafkaServer.scala
+++ b/core/src/main/scala/kafka/server/KafkaServer.scala
@@ -595,6 +595,8 @@ class KafkaServer(val config: KafkaConfig, time: Time = Time.SYSTEM, threadNameP
 
         if (quotaManagers != null)
           CoreUtils.swallow(quotaManagers.shutdown(), this)
+        config.quotaCallback.foreach(_.close())
+
         // Even though socket server is stopped much earlier, controller can generate
         // response for controlled shutdown request. Shutdown server at the end to
         // avoid any failures (e.g. when metrics are recorded)
diff --git a/core/src/main/scala/kafka/server/MetadataCache.scala b/core/src/main/scala/kafka/server/MetadataCache.scala
index 7cdb8f1a1a3..43fe35287d3 100755
--- a/core/src/main/scala/kafka/server/MetadataCache.scala
+++ b/core/src/main/scala/kafka/server/MetadataCache.scala
@@ -17,6 +17,7 @@
 
 package kafka.server
 
+import java.util.Collections
 import java.util.concurrent.locks.ReentrantReadWriteLock
 
 import scala.collection.{Seq, Set, mutable}
@@ -28,7 +29,7 @@ import kafka.controller.StateChangeLogger
 import kafka.utils.CoreUtils._
 import kafka.utils.Logging
 import org.apache.kafka.common.internals.Topic
-import org.apache.kafka.common.{Node, TopicPartition}
+import org.apache.kafka.common.{Cluster, Node, PartitionInfo, TopicPartition}
 import org.apache.kafka.common.network.ListenerName
 import org.apache.kafka.common.protocol.Errors
 import org.apache.kafka.common.requests.{MetadataResponse, UpdateMetadataRequest}
@@ -127,6 +128,14 @@ class MetadataCache(brokerId: Int) extends Logging {
     }
   }
 
+  def getAllPartitions(): Map[TopicPartition, UpdateMetadataRequest.PartitionState] = {
+    inReadLock(partitionMetadataLock) {
+      cache.flatMap { case (topic, partitionStates) =>
+        partitionStates.map { case (partition, state ) => (new TopicPartition(topic, partition), state) }
+      }.toMap
+    }
+  }
+
   def getNonExistingTopics(topics: Set[String]): Set[String] = {
     inReadLock(partitionMetadataLock) {
       topics -- cache.keySet
@@ -180,6 +189,27 @@ class MetadataCache(brokerId: Int) extends Logging {
 
   def getControllerId: Option[Int] = controllerId
 
+  def getClusterMetadata(clusterId: String, listenerName: ListenerName): Cluster = {
+    inReadLock(partitionMetadataLock) {
+      val nodes = aliveNodes.map { case (id, nodes) => (id, nodes.get(listenerName).orNull) }
+      def node(id: Integer): Node = nodes.get(id).orNull
+      val partitions = getAllPartitions()
+        .filter { case (_, state) => state.basePartitionState.leader != LeaderAndIsr.LeaderDuringDelete }
+        .map { case (tp, state) =>
+          new PartitionInfo(tp.topic, tp.partition, node(state.basePartitionState.leader),
+            state.basePartitionState.replicas.asScala.map(node).toArray,
+            state.basePartitionState.isr.asScala.map(node).toArray,
+            state.offlineReplicas.asScala.map(node).toArray)
+        }
+      val unauthorizedTopics = Collections.emptySet[String]
+      val internalTopics = getAllTopics().filter(Topic.isInternal).asJava
+      new Cluster(clusterId, nodes.values.filter(_ != null).toList.asJava,
+        partitions.toList.asJava,
+        unauthorizedTopics, internalTopics,
+        getControllerId.map(id => node(id)).orNull)
+    }
+  }
+
   // This method returns the deleted TopicPartitions received from UpdateMetadataRequest
   def updateCache(correlationId: Int, updateMetadataRequest: UpdateMetadataRequest): Seq[TopicPartition] = {
     inWriteLock(partitionMetadataLock) {
diff --git a/core/src/main/scala/kafka/server/QuotaFactory.scala b/core/src/main/scala/kafka/server/QuotaFactory.scala
index 01441b57323..c758b5a345d 100644
--- a/core/src/main/scala/kafka/server/QuotaFactory.scala
+++ b/core/src/main/scala/kafka/server/QuotaFactory.scala
@@ -54,9 +54,9 @@ object QuotaFactory extends Logging {
 
   def instantiate(cfg: KafkaConfig, metrics: Metrics, time: Time, threadNamePrefix: String): QuotaManagers = {
     QuotaManagers(
-      new ClientQuotaManager(clientFetchConfig(cfg), metrics, Fetch, time, threadNamePrefix),
-      new ClientQuotaManager(clientProduceConfig(cfg), metrics, Produce, time, threadNamePrefix),
-      new ClientRequestQuotaManager(clientRequestConfig(cfg), metrics, time, threadNamePrefix),
+      new ClientQuotaManager(clientFetchConfig(cfg), metrics, Fetch, time, threadNamePrefix, cfg.quotaCallback),
+      new ClientQuotaManager(clientProduceConfig(cfg), metrics, Produce, time, threadNamePrefix, cfg.quotaCallback),
+      new ClientRequestQuotaManager(clientRequestConfig(cfg), metrics, time, threadNamePrefix, cfg.quotaCallback),
       new ReplicationQuotaManager(replicationConfig(cfg), metrics, LeaderReplication, time),
       new ReplicationQuotaManager(replicationConfig(cfg), metrics, FollowerReplication, time),
       new ReplicationQuotaManager(alterLogDirsReplicationConfig(cfg), metrics, AlterLogDirsReplication, time)
diff --git a/core/src/test/scala/integration/kafka/api/BaseQuotaTest.scala b/core/src/test/scala/integration/kafka/api/BaseQuotaTest.scala
index 9b1c2aad2f9..b265182af23 100644
--- a/core/src/test/scala/integration/kafka/api/BaseQuotaTest.scala
+++ b/core/src/test/scala/integration/kafka/api/BaseQuotaTest.scala
@@ -16,31 +16,29 @@ package kafka.api
 
 import java.util.{Collections, HashMap, Properties}
 
-import kafka.server.{ClientQuotaManagerConfig, DynamicConfig, KafkaConfig, KafkaServer, QuotaId, QuotaType}
+import kafka.api.QuotaTestClients._
+import kafka.server.{ClientQuotaManager, ClientQuotaManagerConfig, DynamicConfig, KafkaConfig, KafkaServer, QuotaType}
 import kafka.utils.TestUtils
 import org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}
 import org.apache.kafka.clients.producer._
 import org.apache.kafka.clients.producer.internals.ErrorLoggingCallback
-import org.apache.kafka.common.{MetricName, TopicPartition}
+import org.apache.kafka.common.{Metric, MetricName, TopicPartition}
 import org.apache.kafka.common.metrics.{KafkaMetric, Quota}
+import org.apache.kafka.common.security.auth.KafkaPrincipal
 import org.junit.Assert._
 import org.junit.{Before, Test}
 
-abstract class BaseQuotaTest extends IntegrationTestHarness {
+import scala.collection.JavaConverters._
 
-  def userPrincipal : String
-  def producerQuotaId : QuotaId
-  def consumerQuotaId : QuotaId
-  def overrideQuotas(producerQuota: Long, consumerQuota: Long, requestQuota: Double)
-  def removeQuotaOverrides()
+abstract class BaseQuotaTest extends IntegrationTestHarness {
 
   override val serverCount = 2
   val producerCount = 1
   val consumerCount = 1
 
-  private val producerBufferSize = 300000
   protected def producerClientId = ""QuotasTestProducer-1""
   protected def consumerClientId = ""QuotasTestConsumer-1""
+  protected def createQuotaTestClients(topic: String, leaderNode: KafkaServer): QuotaTestClients
 
   this.serverConfig.setProperty(KafkaConfig.ControlledShutdownEnableProp, ""false"")
   this.serverConfig.setProperty(KafkaConfig.OffsetsTopicReplicationFactorProp, ""2"")
@@ -49,7 +47,7 @@ abstract class BaseQuotaTest extends IntegrationTestHarness {
   this.serverConfig.setProperty(KafkaConfig.GroupMaxSessionTimeoutMsProp, ""30000"")
   this.serverConfig.setProperty(KafkaConfig.GroupInitialRebalanceDelayMsProp, ""0"")
   this.producerConfig.setProperty(ProducerConfig.ACKS_CONFIG, ""0"")
-  this.producerConfig.setProperty(ProducerConfig.BUFFER_MEMORY_CONFIG, producerBufferSize.toString)
+  this.producerConfig.setProperty(ProducerConfig.BUFFER_MEMORY_CONFIG, ""300000"")
   this.producerConfig.setProperty(ProducerConfig.CLIENT_ID_CONFIG, producerClientId)
   this.consumerConfig.setProperty(ConsumerConfig.GROUP_ID_CONFIG, ""QuotasTest"")
   this.consumerConfig.setProperty(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, 4096.toString)
@@ -63,9 +61,10 @@ abstract class BaseQuotaTest extends IntegrationTestHarness {
   val defaultConsumerQuota = 2500
   val defaultRequestQuota = Int.MaxValue
 
-  var leaderNode: KafkaServer = null
-  var followerNode: KafkaServer = null
-  private val topic1 = ""topic-1""
+  val topic1 = ""topic-1""
+  var leaderNode: KafkaServer = _
+  var followerNode: KafkaServer = _
+  var quotaTestClients: QuotaTestClients = _
 
   @Before
   override def setUp() {
@@ -75,22 +74,19 @@ abstract class BaseQuotaTest extends IntegrationTestHarness {
     val leaders = createTopic(topic1, numPartitions, serverCount)
     leaderNode = if (leaders(0) == servers.head.config.brokerId) servers.head else servers(1)
     followerNode = if (leaders(0) != servers.head.config.brokerId) servers.head else servers(1)
+    quotaTestClients = createQuotaTestClients(topic1, leaderNode)
   }
 
   @Test
   def testThrottledProducerConsumer() {
 
     val numRecords = 1000
-    val producer = producers.head
-    val produced = produceUntilThrottled(producer, numRecords)
-    assertTrue(""Should have been throttled"", producerThrottleMetric.value > 0)
-    verifyProducerThrottleTimeMetric(producer)
+    val produced = quotaTestClients.produceUntilThrottled(numRecords)
+    quotaTestClients.verifyProduceThrottle(expectThrottle = true)
 
     // Consumer should read in a bursty manner and get throttled immediately
-    val consumer = consumers.head
-    consumeUntilThrottled(consumer, produced)
-    assertTrue(""Should have been throttled"", consumerThrottleMetric.value > 0)
-    verifyConsumerThrottleTimeMetric(consumer)
+    quotaTestClients.consumeUntilThrottled(produced)
+    quotaTestClients.verifyConsumeThrottle(expectThrottle = true)
   }
 
   @Test
@@ -100,154 +96,187 @@ abstract class BaseQuotaTest extends IntegrationTestHarness {
     props.put(DynamicConfig.Client.ProducerByteRateOverrideProp, Long.MaxValue.toString)
     props.put(DynamicConfig.Client.ConsumerByteRateOverrideProp, Long.MaxValue.toString)
 
-    overrideQuotas(Long.MaxValue, Long.MaxValue, Int.MaxValue)
-    waitForQuotaUpdate(Long.MaxValue, Long.MaxValue, Int.MaxValue)
+    quotaTestClients.overrideQuotas(Long.MaxValue, Long.MaxValue, Int.MaxValue)
+    quotaTestClients.waitForQuotaUpdate(Long.MaxValue, Long.MaxValue, Int.MaxValue)
 
     val numRecords = 1000
-    assertEquals(numRecords, produceUntilThrottled(producers.head, numRecords))
-    assertEquals(""Should not have been throttled"", 0.0, producerThrottleMetric.value, 0.0)
+    assertEquals(numRecords, quotaTestClients.produceUntilThrottled(numRecords))
+    quotaTestClients.verifyProduceThrottle(expectThrottle = false)
 
     // The ""client"" consumer does not get throttled.
-    assertEquals(numRecords, consumeUntilThrottled(consumers.head, numRecords))
-    assertEquals(""Should not have been throttled"", 0.0, consumerThrottleMetric.value, 0.0)
+    assertEquals(numRecords, quotaTestClients.consumeUntilThrottled(numRecords))
+    quotaTestClients.verifyConsumeThrottle(expectThrottle = false)
   }
 
   @Test
   def testQuotaOverrideDelete() {
     // Override producer and consumer quotas to unlimited
-    overrideQuotas(Long.MaxValue, Long.MaxValue, Int.MaxValue)
-    waitForQuotaUpdate(Long.MaxValue, Long.MaxValue, Int.MaxValue)
+    quotaTestClients.overrideQuotas(Long.MaxValue, Long.MaxValue, Int.MaxValue)
+    quotaTestClients.waitForQuotaUpdate(Long.MaxValue, Long.MaxValue, Int.MaxValue)
 
     val numRecords = 1000
-    assertEquals(numRecords, produceUntilThrottled(producers.head, numRecords))
-    assertEquals(""Should not have been throttled"", 0.0, producerThrottleMetric.value, 0.0)
-    assertEquals(numRecords, consumeUntilThrottled(consumers.head, numRecords))
-    assertEquals(""Should not have been throttled"", 0.0, consumerThrottleMetric.value, 0.0)
+    assertEquals(numRecords, quotaTestClients.produceUntilThrottled(numRecords))
+    quotaTestClients.verifyProduceThrottle(expectThrottle = false)
+    assertEquals(numRecords, quotaTestClients.consumeUntilThrottled(numRecords))
+    quotaTestClients.verifyConsumeThrottle(expectThrottle = false)
 
     // Delete producer and consumer quota overrides. Consumer and producer should now be
     // throttled since broker defaults are very small
-    removeQuotaOverrides()
-    val produced = produceUntilThrottled(producers.head, numRecords)
-    assertTrue(""Should have been throttled"", producerThrottleMetric.value > 0)
+    quotaTestClients.removeQuotaOverrides()
+    val produced = quotaTestClients.produceUntilThrottled(numRecords)
+    quotaTestClients.verifyProduceThrottle(expectThrottle = true)
 
     // Since producer may have been throttled after producing a couple of records,
     // consume from beginning till throttled
     consumers.head.seekToBeginning(Collections.singleton(new TopicPartition(topic1, 0)))
-    consumeUntilThrottled(consumers.head, numRecords + produced)
-    assertTrue(""Should have been throttled"", consumerThrottleMetric.value > 0)
+    quotaTestClients.consumeUntilThrottled(numRecords + produced)
+    quotaTestClients.verifyConsumeThrottle(expectThrottle = true)
   }
 
   @Test
   def testThrottledRequest() {
 
-    overrideQuotas(Long.MaxValue, Long.MaxValue, 0.1)
-    waitForQuotaUpdate(Long.MaxValue, Long.MaxValue, 0.1)
+    quotaTestClients.overrideQuotas(Long.MaxValue, Long.MaxValue, 0.1)
+    quotaTestClients.waitForQuotaUpdate(Long.MaxValue, Long.MaxValue, 0.1)
 
     val consumer = consumers.head
     consumer.subscribe(Collections.singleton(topic1))
     val endTimeMs = System.currentTimeMillis + 10000
     var throttled = false
-    while ((!throttled || exemptRequestMetric == null) && System.currentTimeMillis < endTimeMs) {
+    while ((!throttled || quotaTestClients.exemptRequestMetric == null) && System.currentTimeMillis < endTimeMs) {
       consumer.poll(100)
-      val throttleMetric = consumerRequestThrottleMetric
-      throttled = throttleMetric != null && throttleMetric.value > 0
+      val throttleMetric = quotaTestClients.throttleMetric(QuotaType.Request, consumerClientId)
+      throttled = throttleMetric != null && metricValue(throttleMetric) > 0
     }
 
     assertTrue(""Should have been throttled"", throttled)
-    verifyConsumerThrottleTimeMetric(consumer, Some(ClientQuotaManagerConfig.DefaultQuotaWindowSizeSeconds * 1000.0))
+    quotaTestClients.verifyConsumerClientThrottleTimeMetric(expectThrottle = true,
+      Some(ClientQuotaManagerConfig.DefaultQuotaWindowSizeSeconds * 1000.0))
+
+    val exemptMetric = quotaTestClients.exemptRequestMetric
+    assertNotNull(""Exempt requests not recorded"", exemptMetric)
+    assertTrue(""Exempt requests not recorded"", metricValue(exemptMetric) > 0)
+  }
+}
+
+object QuotaTestClients {
+  def metricValue(metric: Metric): Double = metric.metricValue().asInstanceOf[Double]
+}
+
+abstract class QuotaTestClients(topic: String,
+                                leaderNode: KafkaServer,
+                                producerClientId: String,
+                                consumerClientId: String,
+                                producer: KafkaProducer[Array[Byte], Array[Byte]],
+                                consumer: KafkaConsumer[Array[Byte], Array[Byte]]) {
+
+  def userPrincipal : KafkaPrincipal
+  def overrideQuotas(producerQuota: Long, consumerQuota: Long, requestQuota: Double)
+  def removeQuotaOverrides()
+
+  def quotaMetricTags(clientId: String): Map[String, String]
 
-    assertNotNull(""Exempt requests not recorded"", exemptRequestMetric)
-    assertTrue(""Exempt requests not recorded"", exemptRequestMetric.value > 0)
+  def quota(quotaManager: ClientQuotaManager, userPrincipal: KafkaPrincipal, clientId: String): Quota = {
+    quotaManager.quota(userPrincipal, clientId)
   }
 
-  def produceUntilThrottled(p: KafkaProducer[Array[Byte], Array[Byte]], maxRecords: Int): Int = {
+  def produceUntilThrottled(maxRecords: Int, waitForRequestCompletion: Boolean = true): Int = {
     var numProduced = 0
     var throttled = false
     do {
       val payload = numProduced.toString.getBytes
-      p.send(new ProducerRecord[Array[Byte], Array[Byte]](topic1, null, null, payload),
-             new ErrorLoggingCallback(topic1, null, null, true)).get()
+      val future = producer.send(new ProducerRecord[Array[Byte], Array[Byte]](topic, null, null, payload),
+        new ErrorLoggingCallback(topic, null, null, true))
       numProduced += 1
-      val throttleMetric = producerThrottleMetric
-      throttled = throttleMetric != null && throttleMetric.value > 0
+      do {
+        val metric = throttleMetric(QuotaType.Produce, producerClientId)
+        throttled = metric != null && metricValue(metric) > 0
+      } while (!future.isDone && (!throttled || waitForRequestCompletion))
     } while (numProduced < maxRecords && !throttled)
     numProduced
   }
 
-  def consumeUntilThrottled(consumer: KafkaConsumer[Array[Byte], Array[Byte]], maxRecords: Int): Int = {
-    consumer.subscribe(Collections.singleton(topic1))
+  def consumeUntilThrottled(maxRecords: Int, waitForRequestCompletion: Boolean = true): Int = {
+    consumer.subscribe(Collections.singleton(topic))
     var numConsumed = 0
     var throttled = false
     do {
       numConsumed += consumer.poll(100).count
-      val throttleMetric = consumerThrottleMetric
-      throttled = throttleMetric != null && throttleMetric.value > 0
+      val metric = throttleMetric(QuotaType.Fetch, consumerClientId)
+      throttled = metric != null && metricValue(metric) > 0
     }  while (numConsumed < maxRecords && !throttled)
 
     // If throttled, wait for the records from the last fetch to be received
-    if (throttled && numConsumed < maxRecords) {
+    if (throttled && numConsumed < maxRecords && waitForRequestCompletion) {
       val minRecords = numConsumed + 1
       while (numConsumed < minRecords)
-          numConsumed += consumer.poll(100).count
+        numConsumed += consumer.poll(100).count
     }
     numConsumed
   }
 
-  def waitForQuotaUpdate(producerQuota: Long, consumerQuota: Long, requestQuota: Double) {
-    TestUtils.retry(10000) {
-      val quotaManagers = leaderNode.apis.quotas
-      val overrideProducerQuota = quotaManagers.produce.quota(userPrincipal, producerClientId)
-      val overrideConsumerQuota = quotaManagers.fetch.quota(userPrincipal, consumerClientId)
-      val overrideProducerRequestQuota = quotaManagers.request.quota(userPrincipal, producerClientId)
-      val overrideConsumerRequestQuota = quotaManagers.request.quota(userPrincipal, consumerClientId)
+  def verifyProduceThrottle(expectThrottle: Boolean, verifyClientMetric: Boolean = true): Unit = {
+    verifyThrottleTimeMetric(QuotaType.Produce, producerClientId, expectThrottle)
+    if (verifyClientMetric)
+      verifyProducerClientThrottleTimeMetric(expectThrottle)
+  }
 
-      assertEquals(s""ClientId $producerClientId of user $userPrincipal must have producer quota"", Quota.upperBound(producerQuota), overrideProducerQuota)
-      assertEquals(s""ClientId $consumerClientId of user $userPrincipal must have consumer quota"", Quota.upperBound(consumerQuota), overrideConsumerQuota)
-      assertEquals(s""ClientId $producerClientId of user $userPrincipal must have request quota"", Quota.upperBound(requestQuota), overrideProducerRequestQuota)
-      assertEquals(s""ClientId $consumerClientId of user $userPrincipal must have request quota"", Quota.upperBound(requestQuota), overrideConsumerRequestQuota)
+  def verifyConsumeThrottle(expectThrottle: Boolean, verifyClientMetric: Boolean = true): Unit = {
+    verifyThrottleTimeMetric(QuotaType.Fetch, consumerClientId, expectThrottle)
+    if (verifyClientMetric)
+      verifyConsumerClientThrottleTimeMetric(expectThrottle)
+  }
+
+  def verifyThrottleTimeMetric(quotaType: QuotaType, clientId: String, expectThrottle: Boolean): Unit = {
+    val throttleMetricValue = metricValue(throttleMetric(quotaType, clientId))
+    if (expectThrottle) {
+      assertTrue(""Should have been throttled"", throttleMetricValue > 0)
+    } else {
+      assertEquals(""Should not have been throttled"", 0.0, throttleMetricValue, 0.0)
     }
   }
 
-  private def verifyProducerThrottleTimeMetric(producer: KafkaProducer[_, _]) {
+  def throttleMetricName(quotaType: QuotaType, clientId: String): MetricName = {
+    leaderNode.metrics.metricName(""throttle-time"",
+      quotaType.toString,
+      quotaMetricTags(clientId).asJava)
+  }
+
+  def throttleMetric(quotaType: QuotaType, clientId: String): KafkaMetric = {
+    leaderNode.metrics.metrics.get(throttleMetricName(quotaType, clientId))
+  }
+
+  def exemptRequestMetric: KafkaMetric = {
+    val metricName = leaderNode.metrics.metricName(""exempt-request-time"", QuotaType.Request.toString, """")
+    leaderNode.metrics.metrics.get(metricName)
+  }
+
+  def verifyProducerClientThrottleTimeMetric(expectThrottle: Boolean) {
     val tags = new HashMap[String, String]
     tags.put(""client-id"", producerClientId)
     val avgMetric = producer.metrics.get(new MetricName(""produce-throttle-time-avg"", ""producer-metrics"", """", tags))
     val maxMetric = producer.metrics.get(new MetricName(""produce-throttle-time-max"", ""producer-metrics"", """", tags))
 
-    TestUtils.waitUntilTrue(() => avgMetric.value > 0.0 && maxMetric.value > 0.0,
-        s""Producer throttle metric not updated: avg=${avgMetric.value} max=${maxMetric.value}"")
+    if (expectThrottle) {
+      TestUtils.waitUntilTrue(() => metricValue(avgMetric) > 0.0 && metricValue(maxMetric) > 0.0,
+        s""Producer throttle metric not updated: avg=${metricValue(avgMetric)} max=${metricValue(maxMetric)}"")
+    } else
+      assertEquals(""Should not have been throttled"", 0.0, metricValue(maxMetric), 0.0)
   }
 
-  private def verifyConsumerThrottleTimeMetric(consumer: KafkaConsumer[_, _], maxThrottleTime: Option[Double] = None) {
+  def verifyConsumerClientThrottleTimeMetric(expectThrottle: Boolean, maxThrottleTime: Option[Double] = None) {
     val tags = new HashMap[String, String]
     tags.put(""client-id"", consumerClientId)
     val avgMetric = consumer.metrics.get(new MetricName(""fetch-throttle-time-avg"", ""consumer-fetch-manager-metrics"", """", tags))
     val maxMetric = consumer.metrics.get(new MetricName(""fetch-throttle-time-max"", ""consumer-fetch-manager-metrics"", """", tags))
 
-    TestUtils.waitUntilTrue(() => avgMetric.value > 0.0 && maxMetric.value > 0.0,
-        s""Consumer throttle metric not updated: avg=${avgMetric.value} max=${maxMetric.value}"")
-    maxThrottleTime.foreach(max => assertTrue(s""Maximum consumer throttle too high: ${maxMetric.value}"", maxMetric.value <= max))
-  }
-
-  private def throttleMetricName(quotaType: QuotaType, quotaId: QuotaId): MetricName = {
-    leaderNode.metrics.metricName(""throttle-time"",
-                                  quotaType.toString,
-                                  ""Tracking throttle-time per user/client-id"",
-                                  ""user"", quotaId.sanitizedUser.getOrElse(""""),
-                                  ""client-id"", quotaId.clientId.getOrElse(""""))
-  }
-
-  def throttleMetric(quotaType: QuotaType, quotaId: QuotaId): KafkaMetric = {
-    leaderNode.metrics.metrics.get(throttleMetricName(quotaType, quotaId))
-  }
-
-  private def producerThrottleMetric = throttleMetric(QuotaType.Produce, producerQuotaId)
-  private def consumerThrottleMetric = throttleMetric(QuotaType.Fetch, consumerQuotaId)
-  private def consumerRequestThrottleMetric = throttleMetric(QuotaType.Request, consumerQuotaId)
-
-  private def exemptRequestMetric: KafkaMetric = {
-    val metricName = leaderNode.metrics.metricName(""exempt-request-time"", QuotaType.Request.toString, """")
-    leaderNode.metrics.metrics.get(metricName)
+    if (expectThrottle) {
+      TestUtils.waitUntilTrue(() => metricValue(avgMetric) > 0.0 && metricValue(maxMetric) > 0.0,
+        s""Consumer throttle metric not updated: avg=${metricValue(avgMetric)} max=${metricValue(maxMetric)}"")
+      maxThrottleTime.foreach(max => assertTrue(s""Maximum consumer throttle too high: ${metricValue(maxMetric)}"",
+        metricValue(maxMetric) <= max))
+    } else
+      assertEquals(""Should not have been throttled"", 0.0, metricValue(maxMetric), 0.0)
   }
 
   def quotaProperties(producerQuota: Long, consumerQuota: Long, requestQuota: Double): Properties = {
@@ -257,4 +286,19 @@ abstract class BaseQuotaTest extends IntegrationTestHarness {
     props.put(DynamicConfig.Client.RequestPercentageOverrideProp, requestQuota.toString)
     props
   }
+
+  def waitForQuotaUpdate(producerQuota: Long, consumerQuota: Long, requestQuota: Double, server: KafkaServer = leaderNode) {
+    TestUtils.retry(10000) {
+      val quotaManagers = server.apis.quotas
+      val overrideProducerQuota = quota(quotaManagers.produce, userPrincipal, producerClientId)
+      val overrideConsumerQuota = quota(quotaManagers.fetch, userPrincipal, consumerClientId)
+      val overrideProducerRequestQuota = quota(quotaManagers.request, userPrincipal, producerClientId)
+      val overrideConsumerRequestQuota = quota(quotaManagers.request, userPrincipal, consumerClientId)
+
+      assertEquals(s""ClientId $producerClientId of user $userPrincipal must have producer quota"", Quota.upperBound(producerQuota), overrideProducerQuota)
+      assertEquals(s""ClientId $consumerClientId of user $userPrincipal must have consumer quota"", Quota.upperBound(consumerQuota), overrideConsumerQuota)
+      assertEquals(s""ClientId $producerClientId of user $userPrincipal must have request quota"", Quota.upperBound(requestQuota), overrideProducerRequestQuota)
+      assertEquals(s""ClientId $consumerClientId of user $userPrincipal must have request quota"", Quota.upperBound(requestQuota), overrideConsumerRequestQuota)
+    }
+  }
 }
diff --git a/core/src/test/scala/integration/kafka/api/ClientIdQuotaTest.scala b/core/src/test/scala/integration/kafka/api/ClientIdQuotaTest.scala
index 3e0832790b5..b084b3ca5ea 100644
--- a/core/src/test/scala/integration/kafka/api/ClientIdQuotaTest.scala
+++ b/core/src/test/scala/integration/kafka/api/ClientIdQuotaTest.scala
@@ -16,18 +16,15 @@ package kafka.api
 
 import java.util.Properties
 
-import kafka.server.{DynamicConfig, KafkaConfig, QuotaId}
+import kafka.server.{DynamicConfig, KafkaConfig, KafkaServer}
 import org.apache.kafka.common.security.auth.KafkaPrincipal
 import org.apache.kafka.common.utils.Sanitizer
 import org.junit.Before
 
 class ClientIdQuotaTest extends BaseQuotaTest {
 
-  override val userPrincipal = KafkaPrincipal.ANONYMOUS.getName
   override def producerClientId = ""QuotasTestProducer-!@#$%^&*()""
   override def consumerClientId = ""QuotasTestConsumer-!@#$%^&*()""
-  override val producerQuotaId = QuotaId(None, Some(producerClientId), Some(Sanitizer.sanitize(producerClientId)))
-  override val consumerQuotaId = QuotaId(None, Some(consumerClientId), Some(Sanitizer.sanitize(consumerClientId)))
 
   @Before
   override def setUp() {
@@ -35,24 +32,35 @@ class ClientIdQuotaTest extends BaseQuotaTest {
     this.serverConfig.setProperty(KafkaConfig.ConsumerQuotaBytesPerSecondDefaultProp, defaultConsumerQuota.toString)
     super.setUp()
   }
-  override def overrideQuotas(producerQuota: Long, consumerQuota: Long, requestQuota: Double) {
-    val producerProps = new Properties()
-    producerProps.put(DynamicConfig.Client.ProducerByteRateOverrideProp, producerQuota.toString)
-    producerProps.put(DynamicConfig.Client.RequestPercentageOverrideProp, requestQuota.toString)
-    updateQuotaOverride(producerClientId, producerProps)
-
-    val consumerProps = new Properties()
-    consumerProps.put(DynamicConfig.Client.ConsumerByteRateOverrideProp, consumerQuota.toString)
-    consumerProps.put(DynamicConfig.Client.RequestPercentageOverrideProp, requestQuota.toString)
-    updateQuotaOverride(consumerClientId, consumerProps)
-  }
-  override def removeQuotaOverrides() {
-    val emptyProps = new Properties
-    updateQuotaOverride(producerClientId, emptyProps)
-    updateQuotaOverride(consumerClientId, emptyProps)
-  }
 
-  private def updateQuotaOverride(clientId: String, properties: Properties) {
-    adminZkClient.changeClientIdConfig(Sanitizer.sanitize(clientId), properties)
+  override def createQuotaTestClients(topic: String, leaderNode: KafkaServer): QuotaTestClients = {
+    new QuotaTestClients(topic, leaderNode, producerClientId, consumerClientId, producers.head, consumers.head) {
+      override def userPrincipal: KafkaPrincipal = KafkaPrincipal.ANONYMOUS
+      override def quotaMetricTags(clientId: String): Map[String, String] = {
+        Map(""user"" -> """", ""client-id"" -> clientId)
+      }
+
+      override def overrideQuotas(producerQuota: Long, consumerQuota: Long, requestQuota: Double) {
+        val producerProps = new Properties()
+        producerProps.put(DynamicConfig.Client.ProducerByteRateOverrideProp, producerQuota.toString)
+        producerProps.put(DynamicConfig.Client.RequestPercentageOverrideProp, requestQuota.toString)
+        updateQuotaOverride(producerClientId, producerProps)
+
+        val consumerProps = new Properties()
+        consumerProps.put(DynamicConfig.Client.ConsumerByteRateOverrideProp, consumerQuota.toString)
+        consumerProps.put(DynamicConfig.Client.RequestPercentageOverrideProp, requestQuota.toString)
+        updateQuotaOverride(consumerClientId, consumerProps)
+      }
+
+      override def removeQuotaOverrides() {
+        val emptyProps = new Properties
+        updateQuotaOverride(producerClientId, emptyProps)
+        updateQuotaOverride(consumerClientId, emptyProps)
+      }
+
+      private def updateQuotaOverride(clientId: String, properties: Properties) {
+        adminZkClient.changeClientIdConfig(Sanitizer.sanitize(clientId), properties)
+      }
+    }
   }
 }
diff --git a/core/src/test/scala/integration/kafka/api/CustomQuotaCallbackTest.scala b/core/src/test/scala/integration/kafka/api/CustomQuotaCallbackTest.scala
new file mode 100644
index 00000000000..886d6962a6c
--- /dev/null
+++ b/core/src/test/scala/integration/kafka/api/CustomQuotaCallbackTest.scala
@@ -0,0 +1,453 @@
+/**
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ **/
+
+package kafka.api
+
+import java.io.File
+import java.{lang, util}
+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}
+import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger}
+import java.util.{Collections, Properties}
+
+import kafka.api.GroupedUserPrincipalBuilder._
+import kafka.api.GroupedUserQuotaCallback._
+import kafka.server._
+import kafka.utils.JaasTestUtils.ScramLoginModule
+import kafka.utils.{JaasTestUtils, Logging, TestUtils}
+import kafka.zk.ConfigEntityChangeNotificationZNode
+import org.apache.kafka.clients.admin.{AdminClient, AdminClientConfig}
+import org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}
+import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}
+import org.apache.kafka.common.{Cluster, Reconfigurable}
+import org.apache.kafka.common.config.SaslConfigs
+import org.apache.kafka.common.errors.SaslAuthenticationException
+import org.apache.kafka.common.network.ListenerName
+import org.apache.kafka.common.security.auth._
+import org.apache.kafka.common.security.scram.ScramCredential
+import org.apache.kafka.server.quota._
+import org.junit.Assert._
+import org.junit.{After, Before, Test}
+
+import scala.collection.mutable.ArrayBuffer
+import scala.collection.JavaConverters._
+
+class CustomQuotaCallbackTest extends IntegrationTestHarness with SaslSetup {
+
+  override protected def securityProtocol = SecurityProtocol.SASL_SSL
+  override protected def listenerName = new ListenerName(""CLIENT"")
+  override protected def interBrokerListenerName: ListenerName = new ListenerName(""BROKER"")
+
+  override protected lazy val trustStoreFile = Some(File.createTempFile(""truststore"", "".jks""))
+  override val consumerCount: Int = 0
+  override val producerCount: Int = 0
+  override val serverCount: Int = 2
+
+  private val kafkaServerSaslMechanisms = Seq(""SCRAM-SHA-256"")
+  private val kafkaClientSaslMechanism = ""SCRAM-SHA-256""
+  override protected val serverSaslProperties = Some(kafkaServerSaslProperties(kafkaServerSaslMechanisms, kafkaClientSaslMechanism))
+  override protected val clientSaslProperties = Some(kafkaClientSaslProperties(kafkaClientSaslMechanism))
+  private val adminClients = new ArrayBuffer[AdminClient]()
+  private var producerWithoutQuota: KafkaProducer[Array[Byte], Array[Byte]] = _
+
+  val defaultRequestQuota = 1000
+  val defaultProduceQuota = 2000 * 1000 * 1000
+  val defaultConsumeQuota = 1000 * 1000 * 1000
+
+  @Before
+  override def setUp() {
+    startSasl(jaasSections(kafkaServerSaslMechanisms, Some(""SCRAM-SHA-256""), KafkaSasl, JaasTestUtils.KafkaServerContextName))
+    this.serverConfig.setProperty(KafkaConfig.ProducerQuotaBytesPerSecondDefaultProp, Long.MaxValue.toString)
+    this.serverConfig.setProperty(KafkaConfig.ConsumerQuotaBytesPerSecondDefaultProp, Long.MaxValue.toString)
+    this.serverConfig.setProperty(KafkaConfig.ClientQuotaCallbackClassProp, classOf[GroupedUserQuotaCallback].getName)
+    this.serverConfig.setProperty(s""${listenerName.configPrefix}${KafkaConfig.PrincipalBuilderClassProp}"",
+      classOf[GroupedUserPrincipalBuilder].getName)
+    this.serverConfig.setProperty(KafkaConfig.DeleteTopicEnableProp, ""true"")
+    super.setUp()
+    brokerList = TestUtils.bootstrapServers(servers, listenerName)
+
+    producerConfig.put(SaslConfigs.SASL_JAAS_CONFIG,
+      ScramLoginModule(JaasTestUtils.KafkaScramAdmin, JaasTestUtils.KafkaScramAdminPassword).toString)
+    producerWithoutQuota = createNewProducer
+    producers += producerWithoutQuota
+  }
+
+  @After
+  override def tearDown(): Unit = {
+    // Close producers and consumers without waiting for requests to complete
+    // to avoid waiting for throttled responses
+    producers.foreach(_.close(0, TimeUnit.MILLISECONDS))
+    producers.clear()
+    consumers.foreach(_.close(0, TimeUnit.MILLISECONDS))
+    consumers.clear()
+    super.tearDown()
+  }
+
+  override def configureSecurityBeforeServersStart() {
+    super.configureSecurityBeforeServersStart()
+    zkClient.makeSurePersistentPathExists(ConfigEntityChangeNotificationZNode.path)
+    createScramCredentials(zkConnect, JaasTestUtils.KafkaScramAdmin, JaasTestUtils.KafkaScramAdminPassword)
+  }
+
+  @Test
+  def testCustomQuotaCallback() {
+    // Large quota override, should not throttle
+    var brokerId = 0
+    var user = createGroupWithOneUser(""group0_user1"", brokerId)
+    user.configureAndWaitForQuota(1000000, 2000000)
+    quotaLimitCalls.values.foreach(_.set(0))
+    user.produceConsume(expectProduceThrottle = false, expectConsumeThrottle = false)
+
+    // ClientQuotaCallback#quotaLimit is invoked by each quota manager once for each new client
+    assertEquals(1, quotaLimitCalls(ClientQuotaType.PRODUCE).get)
+    assertEquals(1, quotaLimitCalls(ClientQuotaType.FETCH).get)
+    assertTrue(s""Too many quotaLimit calls $quotaLimitCalls"", quotaLimitCalls(ClientQuotaType.REQUEST).get <= serverCount)
+    // Large quota updated to small quota, should throttle
+    user.configureAndWaitForQuota(9000, 3000)
+    user.produceConsume(expectProduceThrottle = true, expectConsumeThrottle = true)
+
+    // Quota override deletion - verify default quota applied (large quota, no throttling)
+    user = addUser(""group0_user2"", brokerId)
+    user.removeQuotaOverrides()
+    user.waitForQuotaUpdate(defaultProduceQuota, defaultConsumeQuota, defaultRequestQuota)
+    user.removeThrottleMetrics() // since group was throttled before
+    user.produceConsume(expectProduceThrottle = false, expectConsumeThrottle = false)
+
+    // Make default quota smaller, should throttle
+    user.configureAndWaitForQuota(8000, 2500, divisor = 1, group = None)
+    user.produceConsume(expectProduceThrottle = true, expectConsumeThrottle = true)
+
+    // Configure large quota override, should not throttle
+    user = addUser(""group0_user3"", brokerId)
+    user.configureAndWaitForQuota(2000000, 2000000)
+    user.removeThrottleMetrics() // since group was throttled before
+    user.produceConsume(expectProduceThrottle = false, expectConsumeThrottle = false)
+
+    // Quota large enough for one partition, should not throttle
+    brokerId = 1
+    user = createGroupWithOneUser(""group1_user1"", brokerId)
+    user.configureAndWaitForQuota(8000 * 100, 2500 * 100)
+    user.produceConsume(expectProduceThrottle = false, expectConsumeThrottle = false)
+
+    // Create large number of partitions on another broker, should result in throttling on first partition
+    val largeTopic = ""group1_largeTopic""
+    createTopic(largeTopic, numPartitions = 99, leader = 0)
+    user.waitForQuotaUpdate(8000, 2500, defaultRequestQuota)
+    user.produceConsume(expectProduceThrottle = true, expectConsumeThrottle = true)
+
+    // Remove quota override and test default quota applied with scaling based on partitions
+    user = addUser(""group1_user2"", brokerId)
+    user.waitForQuotaUpdate(defaultProduceQuota / 100, defaultConsumeQuota / 100, defaultRequestQuota)
+    user.removeThrottleMetrics() // since group was throttled before
+    user.produceConsume(expectProduceThrottle = false, expectConsumeThrottle = false)
+    user.configureAndWaitForQuota(8000 * 100, 2500 * 100, divisor=100, group = None)
+    user.produceConsume(expectProduceThrottle = true, expectConsumeThrottle = true)
+
+    // Remove the second topic with large number of partitions, verify no longer throttled
+    adminZkClient.deleteTopic(largeTopic)
+    user = addUser(""group1_user3"", brokerId)
+    user.waitForQuotaUpdate(8000 * 100, 2500 * 100, defaultRequestQuota)
+    user.removeThrottleMetrics() // since group was throttled before
+    user.produceConsume(expectProduceThrottle = false, expectConsumeThrottle = false)
+
+    // Alter configs of custom callback dynamically
+    val adminClient = createAdminClient()
+    val newProps = new Properties
+    newProps.put(GroupedUserQuotaCallback.DefaultProduceQuotaProp, ""8000"")
+    newProps.put(GroupedUserQuotaCallback.DefaultFetchQuotaProp, ""2500"")
+    TestUtils.alterConfigs(servers, adminClient, newProps, perBrokerConfig = false)
+    user.waitForQuotaUpdate(8000, 2500, defaultRequestQuota)
+    user.produceConsume(expectProduceThrottle = true, expectConsumeThrottle = true)
+  }
+
+  /**
+   * Creates a group with one user and one topic with one partition.
+   * @param firstUser First user to create in the group
+   * @param brokerId The broker id to use as leader of the partition
+   */
+  private def createGroupWithOneUser(firstUser: String, brokerId: Int): GroupedUser = {
+    val user = addUser(firstUser, brokerId)
+    createTopic(user.topic, numPartitions = 1, brokerId)
+    user.configureAndWaitForQuota(defaultProduceQuota, defaultConsumeQuota, divisor = 1, group = None)
+    user
+  }
+
+  private def createTopic(topic: String, numPartitions: Int, leader: Int): Unit = {
+    val assignment = (0 until numPartitions).map { i => i -> Seq(leader) }.toMap
+    TestUtils.createTopic(zkClient, topic, assignment, servers)
+  }
+
+  private def createAdminClient(): AdminClient = {
+    val config = new util.HashMap[String, Object]
+    config.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,
+      TestUtils.bootstrapServers(servers, new ListenerName(""BROKER"")))
+    clientSecurityProps(""admin-client"").asInstanceOf[util.Map[Object, Object]].asScala.foreach { case (key, value) =>
+      config.put(key.toString, value)
+    }
+    config.put(SaslConfigs.SASL_JAAS_CONFIG,
+      ScramLoginModule(JaasTestUtils.KafkaScramAdmin, JaasTestUtils.KafkaScramAdminPassword).toString)
+    val adminClient = AdminClient.create(config)
+    adminClients += adminClient
+    adminClient
+  }
+
+  private def produceWithoutThrottle(topic: String, numRecords: Int): Unit = {
+    (0 until numRecords).foreach { i =>
+      val payload = i.toString.getBytes
+      producerWithoutQuota.send(new ProducerRecord[Array[Byte], Array[Byte]](topic, null, null, payload))
+    }
+  }
+
+  private def addUser(user: String, leader: Int): GroupedUser = {
+
+    val password = s""$user:secret""
+    createScramCredentials(zkConnect, user, password)
+    servers.foreach { server =>
+      val cache = server.credentialProvider.credentialCache.cache(kafkaClientSaslMechanism, classOf[ScramCredential])
+      TestUtils.waitUntilTrue(() => cache.get(user) != null, ""SCRAM credentials not created"")
+    }
+
+    val userGroup = group(user)
+    val topic = s""${userGroup}_topic""
+    val producerClientId = s""$user:producer-client-id""
+    val consumerClientId = s""$user:producer-client-id""
+
+    producerConfig.put(ProducerConfig.CLIENT_ID_CONFIG, producerClientId)
+    producerConfig.put(SaslConfigs.SASL_JAAS_CONFIG, ScramLoginModule(user, password).toString)
+    val producer = createNewProducer
+    producers += producer
+
+    consumerConfig.put(ConsumerConfig.CLIENT_ID_CONFIG, consumerClientId)
+    consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, s""$user-group"")
+    consumerConfig.put(SaslConfigs.SASL_JAAS_CONFIG, ScramLoginModule(user, password).toString)
+    val consumer = createNewConsumer
+    consumers += consumer
+
+    GroupedUser(user, userGroup, topic, servers(leader), producerClientId, consumerClientId, producer, consumer)
+  }
+
+  case class GroupedUser(user: String, userGroup: String, topic: String, leaderNode: KafkaServer,
+                         producerClientId: String, consumerClientId: String,
+                         producer: KafkaProducer[Array[Byte], Array[Byte]],
+                         consumer: KafkaConsumer[Array[Byte], Array[Byte]]) extends
+    QuotaTestClients(topic, leaderNode, producerClientId, consumerClientId, producer, consumer) {
+
+    override def userPrincipal: KafkaPrincipal = GroupedUserPrincipal(user, userGroup)
+
+    override def quotaMetricTags(clientId: String): Map[String, String] = {
+      Map(GroupedUserQuotaCallback.QuotaGroupTag -> userGroup)
+    }
+
+    override def overrideQuotas(producerQuota: Long, consumerQuota: Long, requestQuota: Double): Unit = {
+      configureQuota(userGroup, producerQuota, consumerQuota, requestQuota)
+    }
+
+    override def removeQuotaOverrides(): Unit = {
+      adminZkClient.changeUserOrUserClientIdConfig(quotaEntityName(userGroup), new Properties)
+    }
+
+    def configureQuota(userGroup: String, producerQuota: Long, consumerQuota: Long, requestQuota: Double): Unit = {
+      val quotaProps = quotaProperties(producerQuota, consumerQuota, requestQuota)
+      adminZkClient.changeUserOrUserClientIdConfig(quotaEntityName(userGroup), quotaProps)
+    }
+
+    def configureAndWaitForQuota(produceQuota: Long, fetchQuota: Long, divisor: Int = 1,
+                                 group: Option[String] = Some(userGroup)): Unit = {
+      configureQuota(group.getOrElse(""""), produceQuota, fetchQuota, defaultRequestQuota)
+      waitForQuotaUpdate(produceQuota / divisor, fetchQuota / divisor, defaultRequestQuota)
+    }
+
+    def produceConsume(expectProduceThrottle: Boolean, expectConsumeThrottle: Boolean): Unit = {
+      val numRecords = 1000
+      val produced = produceUntilThrottled(numRecords, waitForRequestCompletion = false)
+      verifyProduceThrottle(expectProduceThrottle, verifyClientMetric = false)
+      // make sure there are enough records on the topic to test consumer throttling
+      produceWithoutThrottle(topic, numRecords - produced)
+      consumeUntilThrottled(numRecords, waitForRequestCompletion = false)
+      verifyConsumeThrottle(expectConsumeThrottle, verifyClientMetric = false)
+    }
+
+    def removeThrottleMetrics(): Unit = {
+      def removeSensors(quotaType: QuotaType, clientId: String): Unit = {
+        val sensorSuffix = quotaMetricTags(clientId).values.mkString("":"")
+        leaderNode.metrics.removeSensor(s""${quotaType}ThrottleTime-$sensorSuffix"")
+        leaderNode.metrics.removeSensor(s""$quotaType-$sensorSuffix"")
+      }
+      removeSensors(QuotaType.Produce, producerClientId)
+      removeSensors(QuotaType.Fetch, consumerClientId)
+      removeSensors(QuotaType.Request, producerClientId)
+      removeSensors(QuotaType.Request, consumerClientId)
+    }
+
+    private def quotaEntityName(userGroup: String): String = s""${userGroup}_""
+  }
+}
+
+object GroupedUserPrincipalBuilder {
+  def group(str: String): String = {
+    if (str.indexOf(""_"") <= 0)
+      """"
+    else
+      str.substring(0, str.indexOf(""_""))
+  }
+}
+
+class GroupedUserPrincipalBuilder extends KafkaPrincipalBuilder {
+  override def build(context: AuthenticationContext): KafkaPrincipal = {
+    val securityProtocol = context.securityProtocol
+    if (securityProtocol == SecurityProtocol.SASL_PLAINTEXT || securityProtocol == SecurityProtocol.SASL_SSL) {
+      val user = context.asInstanceOf[SaslAuthenticationContext].server().getAuthorizationID
+      val userGroup = group(user)
+      if (userGroup.isEmpty)
+        new KafkaPrincipal(KafkaPrincipal.USER_TYPE, user)
+      else
+        GroupedUserPrincipal(user, userGroup)
+    } else
+      throw new IllegalStateException(s""Unexpected security protocol $securityProtocol"")
+  }
+}
+
+case class GroupedUserPrincipal(user: String, userGroup: String) extends KafkaPrincipal(KafkaPrincipal.USER_TYPE, user)
+
+object GroupedUserQuotaCallback {
+  val QuotaGroupTag = ""group""
+  val DefaultProduceQuotaProp = ""default.produce.quota""
+  val DefaultFetchQuotaProp = ""default.fetch.quota""
+  val UnlimitedQuotaMetricTags = Collections.emptyMap[String, String]
+  val quotaLimitCalls = Map(
+    ClientQuotaType.PRODUCE -> new AtomicInteger,
+    ClientQuotaType.FETCH -> new AtomicInteger,
+    ClientQuotaType.REQUEST -> new AtomicInteger
+  )
+}
+
+/**
+ * Quota callback for a grouped user. Both user principals and topics of each group
+ * are prefixed with the group name followed by '_'. This callback defines quotas of different
+ * types at the group level. Group quotas are configured in ZooKeeper as user quotas with
+ * the entity name ""${group}_"". Default group quotas are configured in ZooKeeper as user quotas
+ * with the entity name ""_"".
+ *
+ * Default group quotas may also be configured using the configuration options
+ * ""default.produce.quota"" and ""default.fetch.quota"" which can be reconfigured dynamically
+ * without restarting the broker. This tests custom reconfigurable options for quota callbacks,
+ */
+class GroupedUserQuotaCallback extends ClientQuotaCallback with Reconfigurable with Logging {
+
+  var brokerId: Int = -1
+  val customQuotasUpdated = ClientQuotaType.values.toList
+    .map(quotaType =>(quotaType -> new AtomicBoolean)).toMap
+  val quotas = ClientQuotaType.values.toList
+    .map(quotaType => (quotaType -> new ConcurrentHashMap[String, Double])).toMap
+
+  val partitionRatio = new ConcurrentHashMap[String, Double]()
+
+  override def configure(configs: util.Map[String, _]): Unit = {
+    brokerId = configs.get(KafkaConfig.BrokerIdProp).toString.toInt
+  }
+
+  override def reconfigurableConfigs: util.Set[String] = {
+    Set(DefaultProduceQuotaProp, DefaultFetchQuotaProp).asJava
+  }
+
+  override def validateReconfiguration(configs: util.Map[String, _]): Unit = {
+    reconfigurableConfigs.asScala.foreach(configValue(configs, _))
+  }
+
+  override def reconfigure(configs: util.Map[String, _]): Unit = {
+    configValue(configs, DefaultProduceQuotaProp).foreach(value => quotas(ClientQuotaType.PRODUCE).put("""", value))
+    configValue(configs, DefaultFetchQuotaProp).foreach(value => quotas(ClientQuotaType.FETCH).put("""", value))
+    customQuotasUpdated.values.foreach(_.set(true))
+  }
+
+  private def configValue(configs: util.Map[String, _], key: String): Option[Long] = {
+    val value = configs.get(key)
+    if (value != null) Some(value.toString.toLong) else None
+  }
+
+  override def quotaMetricTags(quotaType: ClientQuotaType, principal: KafkaPrincipal, clientId: String): util.Map[String, String] = {
+    principal match {
+      case groupPrincipal: GroupedUserPrincipal =>
+        val userGroup = groupPrincipal.userGroup
+        val quotaLimit = quotaOrDefault(userGroup, quotaType)
+        if (quotaLimit != null)
+          Map(QuotaGroupTag -> userGroup).asJava
+        else
+          UnlimitedQuotaMetricTags
+      case _ =>
+        UnlimitedQuotaMetricTags
+    }
+  }
+
+  override def quotaLimit(quotaType: ClientQuotaType, metricTags: util.Map[String, String]): lang.Double = {
+    quotaLimitCalls(quotaType).incrementAndGet
+    val group = metricTags.get(QuotaGroupTag)
+    if (group != null) quotaOrDefault(group, quotaType) else null
+  }
+
+  override def updateClusterMetadata(cluster: Cluster): Boolean = {
+    val topicsByGroup = cluster.topics.asScala.groupBy(group)
+
+    !topicsByGroup.forall { case (group, groupTopics) =>
+      val groupPartitions = groupTopics.flatMap(topic => cluster.partitionsForTopic(topic).asScala)
+      val totalPartitions = groupPartitions.size
+      val partitionsOnThisBroker = groupPartitions.count { p => p.leader != null && p.leader.id == brokerId }
+      val multiplier = if (totalPartitions == 0)
+        1
+      else if (partitionsOnThisBroker == 0)
+        1.0 / totalPartitions
+      else
+        partitionsOnThisBroker.toDouble / totalPartitions
+      partitionRatio.put(group, multiplier) != multiplier
+    }
+  }
+
+  override def updateQuota(quotaType: ClientQuotaType, quotaEntity: ClientQuotaEntity, newValue: Double): Unit = {
+    quotas(quotaType).put(userGroup(quotaEntity), newValue)
+  }
+
+  override def removeQuota(quotaType: ClientQuotaType, quotaEntity: ClientQuotaEntity): Unit = {
+    quotas(quotaType).remove(userGroup(quotaEntity))
+  }
+
+  override def quotaResetRequired(quotaType: ClientQuotaType): Boolean = customQuotasUpdated(quotaType).getAndSet(false)
+
+  def close(): Unit = {}
+
+  private def userGroup(quotaEntity: ClientQuotaEntity): String = {
+    val configEntity = quotaEntity.configEntities.get(0)
+    if (configEntity.entityType == ClientQuotaEntity.ConfigEntityType.USER)
+      group(configEntity.name)
+    else
+      throw new IllegalArgumentException(s""Config entity type ${configEntity.entityType} is not supported"")
+  }
+
+  private def quotaOrDefault(group: String, quotaType: ClientQuotaType): lang.Double = {
+    val quotaMap = quotas(quotaType)
+    var quotaLimit: Any = quotaMap.get(group)
+    if (quotaLimit == null)
+      quotaLimit = quotaMap.get("""")
+    if (quotaLimit != null) scaledQuota(quotaType, group, quotaLimit.asInstanceOf[Double]) else null
+  }
+
+  private def scaledQuota(quotaType: ClientQuotaType, group: String, configuredQuota: Double): Double = {
+    if (quotaType == ClientQuotaType.REQUEST)
+      configuredQuota
+    else {
+      val multiplier = partitionRatio.get(group)
+      if (multiplier <= 0.0) configuredQuota else configuredQuota * multiplier
+    }
+  }
+}
+
+
diff --git a/core/src/test/scala/integration/kafka/api/UserClientIdQuotaTest.scala b/core/src/test/scala/integration/kafka/api/UserClientIdQuotaTest.scala
index 453ac910269..47c8f5fa1da 100644
--- a/core/src/test/scala/integration/kafka/api/UserClientIdQuotaTest.scala
+++ b/core/src/test/scala/integration/kafka/api/UserClientIdQuotaTest.scala
@@ -18,7 +18,7 @@ import java.io.File
 import java.util.Properties
 
 import kafka.server._
-import org.apache.kafka.common.security.auth.SecurityProtocol
+import org.apache.kafka.common.security.auth.{KafkaPrincipal, SecurityProtocol}
 import org.apache.kafka.common.utils.Sanitizer
 import org.junit.Before
 
@@ -27,11 +27,8 @@ class UserClientIdQuotaTest extends BaseQuotaTest {
   override protected def securityProtocol = SecurityProtocol.SSL
   override protected lazy val trustStoreFile = Some(File.createTempFile(""truststore"", "".jks""))
 
-  override val userPrincipal = ""O=A client,CN=localhost""
   override def producerClientId = ""QuotasTestProducer-!@#$%^&*()""
   override def consumerClientId = ""QuotasTestConsumer-!@#$%^&*()""
-  override def producerQuotaId = QuotaId(Some(Sanitizer.sanitize(userPrincipal)), Some(producerClientId), Some(Sanitizer.sanitize(producerClientId)))
-  override def consumerQuotaId = QuotaId(Some(Sanitizer.sanitize(userPrincipal)), Some(consumerClientId), Some(Sanitizer.sanitize(consumerClientId)))
 
   @Before
   override def setUp() {
@@ -39,30 +36,41 @@ class UserClientIdQuotaTest extends BaseQuotaTest {
     this.serverConfig.setProperty(KafkaConfig.ProducerQuotaBytesPerSecondDefaultProp, Long.MaxValue.toString)
     this.serverConfig.setProperty(KafkaConfig.ConsumerQuotaBytesPerSecondDefaultProp, Long.MaxValue.toString)
     super.setUp()
-    val defaultProps = quotaProperties(defaultProducerQuota, defaultConsumerQuota, defaultRequestQuota)
+    val defaultProps = quotaTestClients.quotaProperties(defaultProducerQuota, defaultConsumerQuota, defaultRequestQuota)
     adminZkClient.changeUserOrUserClientIdConfig(ConfigEntityName.Default + ""/clients/"" + ConfigEntityName.Default, defaultProps)
-    waitForQuotaUpdate(defaultProducerQuota, defaultConsumerQuota, defaultRequestQuota)
+    quotaTestClients.waitForQuotaUpdate(defaultProducerQuota, defaultConsumerQuota, defaultRequestQuota)
   }
 
-  override def overrideQuotas(producerQuota: Long, consumerQuota: Long, requestQuota: Double) {
-    val producerProps = new Properties()
-    producerProps.setProperty(DynamicConfig.Client.ProducerByteRateOverrideProp, producerQuota.toString)
-    producerProps.setProperty(DynamicConfig.Client.RequestPercentageOverrideProp, requestQuota.toString)
-    updateQuotaOverride(userPrincipal, producerClientId, producerProps)
+  override def createQuotaTestClients(topic: String, leaderNode: KafkaServer): QuotaTestClients = {
+    new QuotaTestClients(topic, leaderNode, producerClientId, consumerClientId, producers.head, consumers.head) {
+      override def userPrincipal: KafkaPrincipal = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, ""O=A client,CN=localhost"")
+      override def quotaMetricTags(clientId: String): Map[String, String] = {
+        Map(""user"" -> Sanitizer.sanitize(userPrincipal.getName), ""client-id"" -> clientId)
+      }
 
-    val consumerProps = new Properties()
-    consumerProps.setProperty(DynamicConfig.Client.ConsumerByteRateOverrideProp, consumerQuota.toString)
-    consumerProps.setProperty(DynamicConfig.Client.RequestPercentageOverrideProp, requestQuota.toString)
-    updateQuotaOverride(userPrincipal, consumerClientId, consumerProps)
-  }
+      override def overrideQuotas(producerQuota: Long, consumerQuota: Long, requestQuota: Double) {
+        val producerProps = new Properties()
+        producerProps.setProperty(DynamicConfig.Client.ProducerByteRateOverrideProp, producerQuota.toString)
+        producerProps.setProperty(DynamicConfig.Client.RequestPercentageOverrideProp, requestQuota.toString)
+        updateQuotaOverride(userPrincipal.getName, producerClientId, producerProps)
 
-  override def removeQuotaOverrides() {
-    val emptyProps = new Properties
-    adminZkClient.changeUserOrUserClientIdConfig(Sanitizer.sanitize(userPrincipal) + ""/clients/"" + Sanitizer.sanitize(producerClientId), emptyProps)
-    adminZkClient.changeUserOrUserClientIdConfig(Sanitizer.sanitize(userPrincipal) + ""/clients/"" + Sanitizer.sanitize(consumerClientId), emptyProps)
-  }
+        val consumerProps = new Properties()
+        consumerProps.setProperty(DynamicConfig.Client.ConsumerByteRateOverrideProp, consumerQuota.toString)
+        consumerProps.setProperty(DynamicConfig.Client.RequestPercentageOverrideProp, requestQuota.toString)
+        updateQuotaOverride(userPrincipal.getName, consumerClientId, consumerProps)
+      }
+
+      override def removeQuotaOverrides() {
+        val emptyProps = new Properties
+        adminZkClient.changeUserOrUserClientIdConfig(Sanitizer.sanitize(userPrincipal.getName) +
+          ""/clients/"" + Sanitizer.sanitize(producerClientId), emptyProps)
+        adminZkClient.changeUserOrUserClientIdConfig(Sanitizer.sanitize(userPrincipal.getName) +
+          ""/clients/"" + Sanitizer.sanitize(consumerClientId), emptyProps)
+      }
 
-  private def updateQuotaOverride(userPrincipal: String, clientId: String, properties: Properties) {
-    adminZkClient.changeUserOrUserClientIdConfig(Sanitizer.sanitize(userPrincipal) + ""/clients/"" + Sanitizer.sanitize(clientId), properties)
+      private def updateQuotaOverride(userPrincipal: String, clientId: String, properties: Properties) {
+        adminZkClient.changeUserOrUserClientIdConfig(Sanitizer.sanitize(userPrincipal) + ""/clients/"" + Sanitizer.sanitize(clientId), properties)
+      }
+    }
   }
 }
diff --git a/core/src/test/scala/integration/kafka/api/UserQuotaTest.scala b/core/src/test/scala/integration/kafka/api/UserQuotaTest.scala
index 91a92faf68f..3386c91ab81 100644
--- a/core/src/test/scala/integration/kafka/api/UserQuotaTest.scala
+++ b/core/src/test/scala/integration/kafka/api/UserQuotaTest.scala
@@ -17,9 +17,9 @@ package kafka.api
 import java.io.File
 import java.util.Properties
 
-import kafka.server.{ConfigEntityName, KafkaConfig, QuotaId}
+import kafka.server.{ConfigEntityName, KafkaConfig, KafkaServer}
 import kafka.utils.JaasTestUtils
-import org.apache.kafka.common.security.auth.SecurityProtocol
+import org.apache.kafka.common.security.auth.{KafkaPrincipal, SecurityProtocol}
 import org.apache.kafka.common.utils.Sanitizer
 import org.junit.{After, Before}
 
@@ -32,20 +32,15 @@ class UserQuotaTest extends BaseQuotaTest with SaslSetup {
   override protected val serverSaslProperties = Some(kafkaServerSaslProperties(kafkaServerSaslMechanisms, kafkaClientSaslMechanism))
   override protected val clientSaslProperties = Some(kafkaClientSaslProperties(kafkaClientSaslMechanism))
 
-  override val userPrincipal = JaasTestUtils.KafkaClientPrincipalUnqualifiedName2
-  override val producerQuotaId = QuotaId(Some(userPrincipal), None, None)
-  override val consumerQuotaId = QuotaId(Some(userPrincipal), None, None)
-
-
   @Before
   override def setUp() {
     startSasl(jaasSections(kafkaServerSaslMechanisms, Some(""GSSAPI""), KafkaSasl, JaasTestUtils.KafkaServerContextName))
     this.serverConfig.setProperty(KafkaConfig.ProducerQuotaBytesPerSecondDefaultProp, Long.MaxValue.toString)
     this.serverConfig.setProperty(KafkaConfig.ConsumerQuotaBytesPerSecondDefaultProp, Long.MaxValue.toString)
     super.setUp()
-    val defaultProps = quotaProperties(defaultProducerQuota, defaultConsumerQuota, defaultRequestQuota)
+    val defaultProps = quotaTestClients.quotaProperties(defaultProducerQuota, defaultConsumerQuota, defaultRequestQuota)
     adminZkClient.changeUserOrUserClientIdConfig(ConfigEntityName.Default, defaultProps)
-    waitForQuotaUpdate(defaultProducerQuota, defaultConsumerQuota, defaultRequestQuota)
+    quotaTestClients.waitForQuotaUpdate(defaultProducerQuota, defaultConsumerQuota, defaultRequestQuota)
   }
 
   @After
@@ -54,18 +49,27 @@ class UserQuotaTest extends BaseQuotaTest with SaslSetup {
     closeSasl()
   }
 
-  override def overrideQuotas(producerQuota: Long, consumerQuota: Long, requestQuota: Double) {
-    val props = quotaProperties(producerQuota, consumerQuota, requestQuota)
-    updateQuotaOverride(props)
-  }
+  override def createQuotaTestClients(topic: String, leaderNode: KafkaServer): QuotaTestClients = {
+    new QuotaTestClients(topic, leaderNode, producerClientId, consumerClientId, producers.head, consumers.head) {
+      override val userPrincipal = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, JaasTestUtils.KafkaClientPrincipalUnqualifiedName2)
+      override def quotaMetricTags(clientId: String): Map[String, String] = {
+        Map(""user"" -> userPrincipal.getName, ""client-id"" -> """")
+      }
 
-  override def removeQuotaOverrides() {
-    val emptyProps = new Properties
-    updateQuotaOverride(emptyProps)
-    updateQuotaOverride(emptyProps)
-  }
+      override def overrideQuotas(producerQuota: Long, consumerQuota: Long, requestQuota: Double) {
+        val props = quotaProperties(producerQuota, consumerQuota, requestQuota)
+        updateQuotaOverride(props)
+      }
+
+      override def removeQuotaOverrides() {
+        val emptyProps = new Properties
+        updateQuotaOverride(emptyProps)
+        updateQuotaOverride(emptyProps)
+      }
 
-  private def updateQuotaOverride(properties: Properties) {
-    adminZkClient.changeUserOrUserClientIdConfig(Sanitizer.sanitize(userPrincipal), properties)
+      private def updateQuotaOverride(properties: Properties) {
+        adminZkClient.changeUserOrUserClientIdConfig(Sanitizer.sanitize(userPrincipal.getName), properties)
+      }
+    }
   }
 }
diff --git a/core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala b/core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala
index e0ab55c2158..bb62fb7fc67 100644
--- a/core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala
+++ b/core/src/test/scala/integration/kafka/server/DynamicBrokerReconfigurationTest.scala
@@ -746,7 +746,7 @@ class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSet
     val unknownConfig = ""some.config""
     props.put(unknownConfig, ""some.config.value"")
 
-    alterConfigs(adminClients.head, props, perBrokerConfig = true).all.get
+    TestUtils.alterConfigs(servers, adminClients.head, props, perBrokerConfig = true).all.get
 
     TestUtils.waitUntilTrue(() => servers.forall(server => server.config.listeners.size == existingListenerCount + 1),
       ""Listener config not updated"")
@@ -799,7 +799,7 @@ class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSet
     listenerProps.foreach(props.remove)
     props.put(KafkaConfig.ListenersProp, listeners)
     props.put(KafkaConfig.ListenerSecurityProtocolMapProp, listenerMap)
-    alterConfigs(adminClients.head, props, perBrokerConfig = true).all.get
+    TestUtils.alterConfigs(servers, adminClients.head, props, perBrokerConfig = true).all.get
 
     TestUtils.waitUntilTrue(() => servers.forall(server => server.config.listeners.size == existingListenerCount - 1),
       ""Listeners not updated"")
@@ -1054,20 +1054,6 @@ class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSet
     assertTrue(s""Advertised listener update not propagated by controller: $endpoints"", altered)
   }
 
-  private def alterConfigs(adminClient: AdminClient, props: Properties, perBrokerConfig: Boolean): AlterConfigsResult = {
-    val configEntries = props.asScala.map { case (k, v) => new ConfigEntry(k, v) }.toList.asJava
-    val newConfig = new Config(configEntries)
-    val configs = if (perBrokerConfig) {
-      servers.map { server =>
-        val resource = new ConfigResource(ConfigResource.Type.BROKER, server.config.brokerId.toString)
-        (resource, newConfig)
-      }.toMap.asJava
-    } else {
-      Map(new ConfigResource(ConfigResource.Type.BROKER, """") -> newConfig).asJava
-    }
-    adminClient.alterConfigs(configs)
-  }
-
   private def alterConfigsOnServer(server: KafkaServer, props: Properties): Unit = {
     val configEntries = props.asScala.map { case (k, v) => new ConfigEntry(k, v) }.toList.asJava
     val newConfig = new Config(configEntries)
@@ -1077,7 +1063,7 @@ class DynamicBrokerReconfigurationTest extends ZooKeeperTestHarness with SaslSet
   }
 
   private def reconfigureServers(newProps: Properties, perBrokerConfig: Boolean, aPropToVerify: (String, String), expectFailure: Boolean = false): Unit = {
-    val alterResult = alterConfigs(adminClients.head, newProps, perBrokerConfig)
+    val alterResult = TestUtils.alterConfigs(servers, adminClients.head, newProps, perBrokerConfig)
     if (expectFailure) {
       val oldProps = servers.head.config.values.asScala.filterKeys(newProps.containsKey)
       val brokerResources = if (perBrokerConfig)
diff --git a/core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala b/core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala
index 1aabbb3b1d4..c0bad91d0d7 100644
--- a/core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala
+++ b/core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala
@@ -18,7 +18,10 @@ package kafka.server
 
 import java.util.Collections
 
+import kafka.network.RequestChannel.Session
+import kafka.server.QuotaType._
 import org.apache.kafka.common.metrics.{MetricConfig, Metrics, Quota}
+import org.apache.kafka.common.security.auth.KafkaPrincipal
 import org.apache.kafka.common.utils.{MockTime, Sanitizer}
 import org.junit.Assert.{assertEquals, assertTrue}
 import org.junit.{Before, Test}
@@ -38,43 +41,49 @@ class ClientQuotaManagerTest {
     numCallbacks = 0
   }
 
+  private def maybeRecordAndThrottle(quotaManager: ClientQuotaManager, user: String, clientId: String, value: Double): Int = {
+    val principal = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, user)
+    quotaManager.maybeRecordAndThrottle(Session(principal, null),clientId, value, this.callback)
+  }
+
   private def testQuotaParsing(config: ClientQuotaManagerConfig, client1: UserClient, client2: UserClient, randomClient: UserClient, defaultConfigClient: UserClient) {
-    val clientMetrics = new ClientQuotaManager(config, newMetrics, QuotaType.Produce, time, """")
+    val clientMetrics = new ClientQuotaManager(config, newMetrics, Produce, time, """")
 
     try {
       // Case 1: Update the quota. Assert that the new quota value is returned
       clientMetrics.updateQuota(client1.configUser, client1.configClientId, client1.sanitizedConfigClientId, Some(new Quota(2000, true)))
       clientMetrics.updateQuota(client2.configUser, client2.configClientId, client2.sanitizedConfigClientId, Some(new Quota(4000, true)))
 
-      assertEquals(""Default producer quota should be "" + config.quotaBytesPerSecondDefault, new Quota(config.quotaBytesPerSecondDefault, true), clientMetrics.quota(randomClient.user, randomClient.clientId))
-      assertEquals(""Should return the overridden value (2000)"", new Quota(2000, true), clientMetrics.quota(client1.user, client1.clientId))
-      assertEquals(""Should return the overridden value (4000)"", new Quota(4000, true), clientMetrics.quota(client2.user, client2.clientId))
+      assertEquals(""Default producer quota should be "" + config.quotaBytesPerSecondDefault,
+        config.quotaBytesPerSecondDefault, clientMetrics.quota(randomClient.user, randomClient.clientId).bound, 0.0)
+      assertEquals(""Should return the overridden value (2000)"", 2000, clientMetrics.quota(client1.user, client1.clientId).bound, 0.0)
+      assertEquals(""Should return the overridden value (4000)"", 4000, clientMetrics.quota(client2.user, client2.clientId).bound, 0.0)
 
       // p1 should be throttled using the overridden quota
-      var throttleTimeMs = clientMetrics.maybeRecordAndThrottle(client1.user, client1.clientId, 2500 * config.numQuotaSamples, this.callback)
+      var throttleTimeMs = maybeRecordAndThrottle(clientMetrics, client1.user, client1.clientId, 2500 * config.numQuotaSamples)
       assertTrue(s""throttleTimeMs should be > 0. was $throttleTimeMs"", throttleTimeMs > 0)
 
       // Case 2: Change quota again. The quota should be updated within KafkaMetrics as well since the sensor was created.
       // p1 should not longer be throttled after the quota change
       clientMetrics.updateQuota(client1.configUser, client1.configClientId, client1.sanitizedConfigClientId, Some(new Quota(3000, true)))
-      assertEquals(""Should return the newly overridden value (3000)"", new Quota(3000, true), clientMetrics.quota(client1.user, client1.clientId))
+      assertEquals(""Should return the newly overridden value (3000)"", 3000, clientMetrics.quota(client1.user, client1.clientId).bound, 0.0)
 
-      throttleTimeMs = clientMetrics.maybeRecordAndThrottle(client1.user, client1.clientId, 0, this.callback)
+      throttleTimeMs = maybeRecordAndThrottle(clientMetrics, client1.user, client1.clientId, 0)
       assertEquals(s""throttleTimeMs should be 0. was $throttleTimeMs"", 0, throttleTimeMs)
 
       // Case 3: Change quota back to default. Should be throttled again
       clientMetrics.updateQuota(client1.configUser, client1.configClientId, client1.sanitizedConfigClientId, Some(new Quota(500, true)))
-      assertEquals(""Should return the default value (500)"", new Quota(500, true), clientMetrics.quota(client1.user, client1.clientId))
+      assertEquals(""Should return the default value (500)"", 500, clientMetrics.quota(client1.user, client1.clientId).bound, 0.0)
 
-      throttleTimeMs = clientMetrics.maybeRecordAndThrottle(client1.user, client1.clientId, 0, this.callback)
+      throttleTimeMs = maybeRecordAndThrottle(clientMetrics, client1.user, client1.clientId, 0)
       assertTrue(s""throttleTimeMs should be > 0. was $throttleTimeMs"", throttleTimeMs > 0)
 
       // Case 4: Set high default quota, remove p1 quota. p1 should no longer be throttled
       clientMetrics.updateQuota(client1.configUser, client1.configClientId, client1.sanitizedConfigClientId, None)
       clientMetrics.updateQuota(defaultConfigClient.configUser, defaultConfigClient.configClientId, defaultConfigClient.sanitizedConfigClientId, Some(new Quota(4000, true)))
-      assertEquals(""Should return the newly overridden value (4000)"", new Quota(4000, true), clientMetrics.quota(client1.user, client1.clientId))
+      assertEquals(""Should return the newly overridden value (4000)"", 4000, clientMetrics.quota(client1.user, client1.clientId).bound, 0.0)
 
-      throttleTimeMs = clientMetrics.maybeRecordAndThrottle(client1.user, client1.clientId, 1000 * config.numQuotaSamples, this.callback)
+      throttleTimeMs = maybeRecordAndThrottle(clientMetrics, client1.user, client1.clientId, 1000 * config.numQuotaSamples)
       assertEquals(s""throttleTimeMs should be 0. was $throttleTimeMs"", 0, throttleTimeMs)
 
     } finally {
@@ -150,11 +159,11 @@ class ClientQuotaManagerTest {
   @Test
   def testQuotaConfigPrecedence() {
     val quotaManager = new ClientQuotaManager(ClientQuotaManagerConfig(quotaBytesPerSecondDefault=Long.MaxValue),
-        newMetrics, QuotaType.Produce, time, """")
+        newMetrics, Produce, time, """")
 
     def checkQuota(user: String, clientId: String, expectedBound: Int, value: Int, expectThrottle: Boolean) {
-      assertEquals(new Quota(expectedBound, true), quotaManager.quota(user, clientId))
-      val throttleTimeMs = quotaManager.maybeRecordAndThrottle(user, clientId, value * config.numQuotaSamples, this.callback)
+      assertEquals(expectedBound, quotaManager.quota(user, clientId).bound, 0.0)
+      val throttleTimeMs = maybeRecordAndThrottle(quotaManager, user, clientId, value * config.numQuotaSamples)
       if (expectThrottle)
         assertTrue(s""throttleTimeMs should be > 0. was $throttleTimeMs"", throttleTimeMs > 0)
       else
@@ -223,14 +232,14 @@ class ClientQuotaManagerTest {
   @Test
   def testQuotaViolation() {
     val metrics = newMetrics
-    val clientMetrics = new ClientQuotaManager(config, metrics, QuotaType.Produce, time, """")
+    val clientMetrics = new ClientQuotaManager(config, metrics, Produce, time, """")
     val queueSizeMetric = metrics.metrics().get(metrics.metricName(""queue-size"", ""Produce"", """"))
     try {
       /* We have 10 second windows. Make sure that there is no quota violation
        * if we produce under the quota
        */
       for (_ <- 0 until 10) {
-        clientMetrics.maybeRecordAndThrottle(""ANONYMOUS"", ""unknown"", 400, callback)
+        maybeRecordAndThrottle(clientMetrics, ""ANONYMOUS"", ""unknown"", 400)
         time.sleep(1000)
       }
       assertEquals(10, numCallbacks)
@@ -241,7 +250,7 @@ class ClientQuotaManagerTest {
       // (600 - quota)/quota*window-size = (600-500)/500*10.5 seconds = 2100
       // 10.5 seconds because the last window is half complete
       time.sleep(500)
-      val sleepTime = clientMetrics.maybeRecordAndThrottle(""ANONYMOUS"", ""unknown"", 2300, callback)
+      val sleepTime = maybeRecordAndThrottle(clientMetrics, ""ANONYMOUS"", ""unknown"", 2300)
 
       assertEquals(""Should be throttled"", 2100, sleepTime)
       assertEquals(1, queueSizeMetric.value().toInt)
@@ -257,12 +266,12 @@ class ClientQuotaManagerTest {
 
       // Could continue to see delays until the bursty sample disappears
       for (_ <- 0 until 10) {
-        clientMetrics.maybeRecordAndThrottle(""ANONYMOUS"", ""unknown"", 400, callback)
+        maybeRecordAndThrottle(clientMetrics, ""ANONYMOUS"", ""unknown"", 400)
         time.sleep(1000)
       }
 
       assertEquals(""Should be unthrottled since bursty sample has rolled over"",
-                   0, clientMetrics.maybeRecordAndThrottle(""ANONYMOUS"", ""unknown"", 0, callback))
+                   0, maybeRecordAndThrottle(clientMetrics, ""ANONYMOUS"", ""unknown"", 0))
     } finally {
       clientMetrics.shutdown()
     }
@@ -271,7 +280,7 @@ class ClientQuotaManagerTest {
   @Test
   def testRequestPercentageQuotaViolation() {
     val metrics = newMetrics
-    val quotaManager = new ClientRequestQuotaManager(config, metrics, time, """")
+    val quotaManager = new ClientRequestQuotaManager(config, metrics, time, """", None)
     quotaManager.updateQuota(Some(""ANONYMOUS""), Some(""test-client""), Some(""test-client""), Some(Quota.upperBound(1)))
     val queueSizeMetric = metrics.metrics().get(metrics.metricName(""queue-size"", ""Request"", """"))
     def millisToPercent(millis: Double) = millis * 1000 * 1000 * ClientQuotaManagerConfig.NanosToPercentagePerSecond
@@ -280,7 +289,7 @@ class ClientQuotaManagerTest {
        * if we are under the quota
        */
       for (_ <- 0 until 10) {
-        quotaManager.maybeRecordAndThrottle(""ANONYMOUS"", ""test-client"", millisToPercent(4), callback)
+        maybeRecordAndThrottle(quotaManager, ""ANONYMOUS"", ""test-client"", millisToPercent(4))
         time.sleep(1000)
       }
       assertEquals(10, numCallbacks)
@@ -292,7 +301,7 @@ class ClientQuotaManagerTest {
       // (10.2 - quota)/quota*window-size = (10.2-10)/10*10.5 seconds = 210ms
       // 10.5 seconds interval because the last window is half complete
       time.sleep(500)
-      val throttleTime = quotaManager.maybeRecordAndThrottle(""ANONYMOUS"", ""test-client"", millisToPercent(67.1), callback)
+      val throttleTime = maybeRecordAndThrottle(quotaManager, ""ANONYMOUS"", ""test-client"", millisToPercent(67.1))
 
       assertEquals(""Should be throttled"", 210, throttleTime)
       assertEquals(1, queueSizeMetric.value().toInt)
@@ -308,22 +317,22 @@ class ClientQuotaManagerTest {
 
       // Could continue to see delays until the bursty sample disappears
       for (_ <- 0 until 11) {
-        quotaManager.maybeRecordAndThrottle(""ANONYMOUS"", ""test-client"", millisToPercent(4), callback)
+        maybeRecordAndThrottle(quotaManager, ""ANONYMOUS"", ""test-client"", millisToPercent(4))
         time.sleep(1000)
       }
 
       assertEquals(""Should be unthrottled since bursty sample has rolled over"",
-                   0, quotaManager.maybeRecordAndThrottle(""ANONYMOUS"", ""test-client"", 0, callback))
+                   0, maybeRecordAndThrottle(quotaManager, ""ANONYMOUS"", ""test-client"", 0))
 
       // Create a very large spike which requires > one quota window to bring within quota
-      assertEquals(1000, quotaManager.maybeRecordAndThrottle(""ANONYMOUS"", ""test-client"", millisToPercent(500), callback))
+      assertEquals(1000, maybeRecordAndThrottle(quotaManager, ""ANONYMOUS"", ""test-client"", millisToPercent(500)))
       for (_ <- 0 until 10) {
         time.sleep(1000)
-        assertEquals(1000, quotaManager.maybeRecordAndThrottle(""ANONYMOUS"", ""test-client"", 0, callback))
+        assertEquals(1000, maybeRecordAndThrottle(quotaManager, ""ANONYMOUS"", ""test-client"", 0))
       }
       time.sleep(1000)
       assertEquals(""Should be unthrottled since bursty sample has rolled over"",
-                   0, quotaManager.maybeRecordAndThrottle(""ANONYMOUS"", ""test-client"", 0, callback))
+                   0, maybeRecordAndThrottle(quotaManager, ""ANONYMOUS"", ""test-client"", 0))
 
     } finally {
       quotaManager.shutdown()
@@ -333,13 +342,13 @@ class ClientQuotaManagerTest {
   @Test
   def testExpireThrottleTimeSensor() {
     val metrics = newMetrics
-    val clientMetrics = new ClientQuotaManager(config, metrics, QuotaType.Produce, time, """")
+    val clientMetrics = new ClientQuotaManager(config, metrics, Produce, time, """")
     try {
-      clientMetrics.maybeRecordAndThrottle(""ANONYMOUS"", ""client1"", 100, callback)
+      maybeRecordAndThrottle(clientMetrics, ""ANONYMOUS"", ""client1"", 100)
       // remove the throttle time sensor
       metrics.removeSensor(""ProduceThrottleTime-:client1"")
       // should not throw an exception even if the throttle time sensor does not exist.
-      val throttleTime = clientMetrics.maybeRecordAndThrottle(""ANONYMOUS"", ""client1"", 10000, callback)
+      val throttleTime = maybeRecordAndThrottle(clientMetrics, ""ANONYMOUS"", ""client1"", 10000)
       assertTrue(""Should be throttled"", throttleTime > 0)
       // the sensor should get recreated
       val throttleTimeSensor = metrics.getSensor(""ProduceThrottleTime-:client1"")
@@ -352,14 +361,14 @@ class ClientQuotaManagerTest {
   @Test
   def testExpireQuotaSensors() {
     val metrics = newMetrics
-    val clientMetrics = new ClientQuotaManager(config, metrics, QuotaType.Produce, time, """")
+    val clientMetrics = new ClientQuotaManager(config, metrics, Produce, time, """")
     try {
-      clientMetrics.maybeRecordAndThrottle(""ANONYMOUS"", ""client1"", 100, callback)
+      maybeRecordAndThrottle(clientMetrics, ""ANONYMOUS"", ""client1"", 100)
       // remove all the sensors
       metrics.removeSensor(""ProduceThrottleTime-:client1"")
       metrics.removeSensor(""Produce-ANONYMOUS:client1"")
       // should not throw an exception
-      val throttleTime = clientMetrics.maybeRecordAndThrottle(""ANONYMOUS"", ""client1"", 10000, callback)
+      val throttleTime = maybeRecordAndThrottle(clientMetrics, ""ANONYMOUS"", ""client1"", 10000)
       assertTrue(""Should be throttled"", throttleTime > 0)
 
       // all the sensors should get recreated
@@ -376,10 +385,10 @@ class ClientQuotaManagerTest {
   @Test
   def testClientIdNotSanitized() {
     val metrics = newMetrics
-    val clientMetrics = new ClientQuotaManager(config, metrics, QuotaType.Produce, time, """")
+    val clientMetrics = new ClientQuotaManager(config, metrics, Produce, time, """")
     val clientId = ""client@#$%""
     try {
-      clientMetrics.maybeRecordAndThrottle(""ANONYMOUS"", clientId, 100, callback)
+      maybeRecordAndThrottle(clientMetrics, ""ANONYMOUS"", clientId, 100)
 
       // The metrics should use the raw client ID, even if the reporters internally sanitize them
       val throttleTimeSensor = metrics.getSensor(""ProduceThrottleTime-:"" + clientId)
diff --git a/core/src/test/scala/unit/kafka/server/DynamicBrokerConfigTest.scala b/core/src/test/scala/unit/kafka/server/DynamicBrokerConfigTest.scala
index 5c88bf27f6d..9c8acb48024 100755
--- a/core/src/test/scala/unit/kafka/server/DynamicBrokerConfigTest.scala
+++ b/core/src/test/scala/unit/kafka/server/DynamicBrokerConfigTest.scala
@@ -27,11 +27,12 @@ import org.apache.kafka.common.config.{ConfigException, SslConfigs}
 import org.easymock.EasyMock
 import org.junit.Assert._
 import org.junit.Test
+import org.scalatest.junit.JUnitSuite
 
 import scala.collection.JavaConverters._
 import scala.collection.Set
 
-class DynamicBrokerConfigTest {
+class DynamicBrokerConfigTest extends JUnitSuite {
 
   @Test
   def testConfigUpdate(): Unit = {
@@ -126,6 +127,35 @@ class DynamicBrokerConfigTest {
     verifyConfigUpdateWithInvalidConfig(config, origProps, validProps, invalidProps)
   }
 
+  @Test
+  def testReconfigurableValidation(): Unit = {
+    val origProps = TestUtils.createBrokerConfig(0, TestUtils.MockZkConnect, port = 8181)
+    val config = KafkaConfig(origProps)
+    val invalidReconfigurableProps = Set(KafkaConfig.LogCleanerThreadsProp, KafkaConfig.BrokerIdProp, ""some.prop"")
+    val validReconfigurableProps = Set(KafkaConfig.LogCleanerThreadsProp, KafkaConfig.LogCleanerDedupeBufferSizeProp, ""some.prop"")
+
+    def createReconfigurable(configs: Set[String]) = new Reconfigurable {
+      override def configure(configs: util.Map[String, _]): Unit = {}
+      override def reconfigurableConfigs(): util.Set[String] = configs.asJava
+      override def validateReconfiguration(configs: util.Map[String, _]): Unit = {}
+      override def reconfigure(configs: util.Map[String, _]): Unit = {}
+    }
+    intercept[IllegalArgumentException] {
+      config.dynamicConfig.addReconfigurable(createReconfigurable(invalidReconfigurableProps))
+    }
+    config.dynamicConfig.addReconfigurable(createReconfigurable(validReconfigurableProps))
+
+    def createBrokerReconfigurable(configs: Set[String]) = new BrokerReconfigurable {
+      override def reconfigurableConfigs: collection.Set[String] = configs
+      override def validateReconfiguration(newConfig: KafkaConfig): Unit = {}
+      override def reconfigure(oldConfig: KafkaConfig, newConfig: KafkaConfig): Unit = {}
+    }
+    intercept[IllegalArgumentException] {
+      config.dynamicConfig.addBrokerReconfigurable(createBrokerReconfigurable(invalidReconfigurableProps))
+    }
+    config.dynamicConfig.addBrokerReconfigurable(createBrokerReconfigurable(validReconfigurableProps))
+  }
+
   @Test
   def testSecurityConfigs(): Unit = {
     def verifyUpdate(name: String, value: Object): Unit = {
diff --git a/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala b/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala
index bfbae2bde03..2a7d6d400d5 100644
--- a/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala
+++ b/core/src/test/scala/unit/kafka/server/RequestQuotaTest.scala
@@ -23,6 +23,7 @@ import kafka.log.LogConfig
 import kafka.network.RequestChannel.Session
 import kafka.security.auth._
 import kafka.utils.TestUtils
+
 import org.apache.kafka.clients.admin.NewPartitions
 import org.apache.kafka.common.acl.{AccessControlEntry, AccessControlEntryFilter, AclBinding, AclBindingFilter, AclOperation, AclPermissionType}
 import org.apache.kafka.common.resource.{ResourceFilter, Resource => AdminResource, ResourceType => AdminResourceType}
@@ -132,13 +133,16 @@ class RequestQuotaTest extends BaseRequestTest {
     waitAndCheckResults()
   }
 
+  def session(user: String): Session = Session(new KafkaPrincipal(KafkaPrincipal.USER_TYPE, user), null)
+
   private def throttleTimeMetricValue(clientId: String): Double = {
     val metricName = leaderNode.metrics.metricName(""throttle-time"",
                                   QuotaType.Request.toString,
                                   """",
                                   ""user"", """",
                                   ""client-id"", clientId)
-    val sensor = leaderNode.quotaManagers.request.getOrCreateQuotaSensors(""ANONYMOUS"", clientId).throttleTimeSensor
+    val sensor = leaderNode.quotaManagers.request.getOrCreateQuotaSensors(session(""ANONYMOUS""),
+      clientId).throttleTimeSensor
     metricValue(leaderNode.metrics.metrics.get(metricName), sensor)
   }
 
@@ -148,7 +152,8 @@ class RequestQuotaTest extends BaseRequestTest {
                                   """",
                                   ""user"", """",
                                   ""client-id"", clientId)
-    val sensor = leaderNode.quotaManagers.request.getOrCreateQuotaSensors(""ANONYMOUS"", clientId).quotaSensor
+    val sensor = leaderNode.quotaManagers.request.getOrCreateQuotaSensors(session(""ANONYMOUS""),
+      clientId).quotaSensor
     metricValue(leaderNode.metrics.metrics.get(metricName), sensor)
   }
 
diff --git a/core/src/test/scala/unit/kafka/utils/TestUtils.scala b/core/src/test/scala/unit/kafka/utils/TestUtils.scala
index 4b8740600ff..16b7e87487e 100755
--- a/core/src/test/scala/unit/kafka/utils/TestUtils.scala
+++ b/core/src/test/scala/unit/kafka/utils/TestUtils.scala
@@ -41,9 +41,11 @@ import Implicits._
 import kafka.controller.LeaderIsrAndControllerEpoch
 import kafka.zk.{AdminZkClient, BrokerIdsZNode, BrokerInfo, KafkaZkClient}
 import org.apache.kafka.clients.CommonClientConfigs
+import org.apache.kafka.clients.admin.{AdminClient, AlterConfigsResult, Config, ConfigEntry}
 import org.apache.kafka.clients.consumer.{ConsumerRecord, KafkaConsumer, OffsetAndMetadata, RangeAssignor}
 import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}
 import org.apache.kafka.common.TopicPartition
+import org.apache.kafka.common.config.ConfigResource
 import org.apache.kafka.common.header.Header
 import org.apache.kafka.common.internals.Topic
 import org.apache.kafka.common.network.{ListenerName, Mode}
@@ -1492,6 +1494,21 @@ object TestUtils extends Logging {
     }
   }
 
+  def alterConfigs(servers: Seq[KafkaServer], adminClient: AdminClient, props: Properties,
+                   perBrokerConfig: Boolean): AlterConfigsResult = {
+    val configEntries = props.asScala.map { case (k, v) => new ConfigEntry(k, v) }.toList.asJava
+    val newConfig = new Config(configEntries)
+    val configs = if (perBrokerConfig) {
+      servers.map { server =>
+        val resource = new ConfigResource(ConfigResource.Type.BROKER, server.config.brokerId.toString)
+        (resource, newConfig)
+      }.toMap.asJava
+    } else {
+      Map(new ConfigResource(ConfigResource.Type.BROKER, """") -> newConfig).asJava
+    }
+    adminClient.alterConfigs(configs)
+  }
+
   /**
    * Capture the console output during the execution of the provided function.
    */


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow Kafka to be used for horizontally-scalable real-time stream visualization,KAFKA-6754,13150484,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,billdestein,billdestein,06/Apr/18 00:41,06/Apr/18 18:05,12/Jan/21 10:06,,,,,,,,,,,,clients,core,,,,,0,,,,,"I've developed a patch that allows Kafka to be used as the back-end for horizontally-scalable real-time stream visualization systems.

I've created a five-minute demo video here.

https://goo.gl/cERVmb

I'd like to get thoughts from the Kafka leadership on whether my patch could be made part of Kafka going forward.  I'll create a wiki with implementation details if there is interest.

I intend to open source the time series portal as a separate project because it will consist of lots of React and Redux code that probably doesn't belong in the Kafka code base.

Thanks,  Bill DeStein",,billdestein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-04-06 00:41:48.0,,,,,,,"0|i3s7tr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Host Affinity to facilitate faster restarts of kafka streams applications,KAFKA-6645,13144707,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Information Provided,,giridhar1202,giridhar1202,13/Mar/18 12:37,02/Apr/18 10:38,12/Jan/21 10:06,02/Apr/18 10:38,,,,,,,,,,,streams,,,,,,2,,,,,"Since Kafka Streams applications have lot of state in the stores in general, it would be good to remember the assignment of partitions to machines. So that when whole application is restarted for some reason, there is a way to use past assignment of partitions to machines and there won't be need to build up whole state by reading off of changelog kafka topic. This would result in faster start-up.

Samza has support for Host Affinity ([https://samza.apache.org/learn/documentation/0.14/yarn/yarn-host-affinity.html])

KIP-54 ([https://cwiki.apache.org/confluence/display/KAFKA/KIP-54+-+Sticky+Partition+Assignment+Strategy)] , handles cases where some members of consumer group goes down / comes up, and KIP-54 ensures there is minimal diff between assignments before and after rebalance. 

But to handle whole restart use case, we need to remember past assignment somewhere, and use it after restart.

Please let us know if this is already solved problem / some cleaner way of achieving this objective",,asurana,giridhar1202,guozhang,mjsax,NaviBrar,prasanna2991,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-03-14 05:35:52.061,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 02 10:36:29 UTC 2018,,,,,,,"0|i3r8bj:",9223372036854775807,,,,,,,,,,,,,,,,"14/Mar/18 05:35;mjsax;This should be supported already. On startup, Kafka Streams inspects it's local state directory and adds available stores into rebalance metadata (ie, prev-assigned standby tasks). This allows to reassign partitions accordingly to avoid state migration.

Note, that upcoming 1.1 release contains some improvements to partition assignment. This Jira might still be valid in order to improve the existing strategy further. It would be great if you could try out 1.0 or better 1.1 release and check if Kafka Streams behaves as expected. If not, it would be great to learn what is missing in detail so we can close those gaps.

Thanks a lot!","14/Mar/18 06:16;guozhang;Hi Giridhar, could you share with us a bit more on your usage pattern for a complete restart? As Matthias mentioned, since Kafka Streams library always looks into the specified state directory for existing task ids from the previous runs and encode that information in the rebalance protocol, ""stickiness"" should be auto-applied already, and if you did not observe this is the case, we'd like to investigate further.","14/Mar/18 06:34;giridhar1202;Thank you for your reply [~mjsax]

Can you please provide us with code pointer for this.

Is there some duration of time for which leader of consumer group waits for all the consumers to join the group ?

If Kafka Streams application is running on say 10 machines before, and we stopped the application on all machines now.

Now, say we are in the process of bringing up the application on the machines. During this it should not be the case that kafka streams thinks that other machines are down and try to assign partitions within the machines that are currently up.","14/Mar/18 18:34;guozhang;Hi Giridhar,

I think I understand your issue now. What you may have observed is may not be completely resolved by the stickiness behavior: when you are (re-)starting your application that have multiple instances, the coordinator may not wait enough long time to have every instance to join the consumer group before issuing the rebalance. More specifically in your case, although you may restart the 10 machines at roughly the same time, there is still some window gap that some machines starts up earlier than others, so if only, say 5 machines are recognized by the coordinator in the first rebalance, it has to reassign some of the tasks of the other 5 machines to these existing 5 ones, causing some long restoration latency, and only after that a new rebalance will be triggered with all 10 machines up and running, and tasks will be reassigned back.

To remedy this issue, in https://cwiki.apache.org/confluence/display/KAFKA/KIP-134%3A+Delay+initial+consumer+group+rebalance we introduced a new config on the broker side to wait for longer time before triggering a rebalance for a new group. You can try increasing that config value (default to 3 seconds) and see if it helps to wait for longer time to get all instances to join the group and hence let sticky assignor to make the ideal assignment.","14/Mar/18 20:36;mjsax;About code pointer: [https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java#L255]

Also note, even if only 5 instances start and those try to restore state, when new instanced come up delayed, tasks will be reassigned – the restore will be interrupted and task should ""migrate"" back to the original hosts. (There are still some known gaps here though, but in general this is how it should work.)","02/Apr/18 10:36;giridhar1202;[~guozhang]  & [~mjsax]

Thank you for comments.

We noticed the same behavior as you explained.

Might be good idea to include a line or two about host affinity feature in kafka streams documentation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a GlobalKStream object type for stream event broadcast,KAFKA-6646,13144788,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,astubbs,astubbs,13/Mar/18 17:07,14/Mar/18 06:10,12/Jan/21 10:06,,1.1.0,,,,,,,,,,streams,,,,,,1,api,needs-kip,,,"There are some use cases where having a global KStream object is useful. For example, where a single event is sent, with very low frequency, to a cluster of Kafka stream nodes to trigger all nodes to do some processing of state stored on their instance.

Workaround currently is to either create a second kstream app instance, being careful to configure it with a different state dir, and give it a unique app name per instance, then create a kstream in each one. Or - you can use the normal consumer client inside your kstream app with unique consumer groups.",,ableegoldman,astubbs,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-03-13 17:07:33.0,,,,,,,"0|i3r8tj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Warm up new replicas from scratch when changelog topic has LIMITED retention time,KAFKA-6643,13144626,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,NaviBrar,NaviBrar,13/Mar/18 05:44,13/Mar/18 06:02,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,2,,,,,"In the current scenario, Kafka Streams has changelog Kafka topics(internal topics having all the data for the store) which are used to build the state of replicas. So, if we keep the number of standby replicas as 1, we still have more availability for persistent state stores as changelog Kafka topics are also replicated depending upon broker replication policy but that also means we are using at least 4 times the space(1 master store, 1 replica store, 1 changelog, 1 changelog replica). 

Now if we have an year's data in persistent stores(rocksdb), we don't want the changelog topics to have an year's data as it will put an unnecessary burden on brokers(in terms of space). If we have to scale our kafka streams application(having 200-300 TB's of data) we have to scale the kafka brokers as well. We want to reduce this dependency and find out ways to just use changelog topic as a queue, having just 2 or 3 days of data and warm up the replicas from scratch in some other way.

I have few proposals in that respect.
1. Use a new kafka topic related to each partition which we need to warm up on the fly(when node containing that partition crashes. Produce into this topic from another replica/active and built new replica through this topic.
2. Use peer to peer file transfer(such as SFTP) as rocksdb can create backups, which can be transferred from source node to destination node when a new replica has to be built from scratch.
3. Use HDFS in intermediate instead of kafka topic where we can keep scheduled backups for each partition and use those to build new replicas.",,ableegoldman,asurana,NaviBrar,prasanna2991,scosenza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-03-13 05:44:37.0,,,,,,,"0|i3r7tj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a dryrun option to release.py,KAFKA-5939,13103555,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,damianguy,damianguy,20/Sep/17 09:42,08/Mar/18 00:46,12/Jan/21 10:06,,,,,,,,,,,,tools,,,,,,0,,,,,"It would be great to add a `dryrun` feature to `release.py` so that it can be used to test changes to the scripts etc. At the moment you need to make sure all JIRAs are closed for the release, have no uncommited changes etc, which is a bit of a hassle when you just want to test a change you've made to the script. There may be other things that need to be skipped, too",,damianguy,ewencp,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-09-20 09:54:21.694,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 08 00:46:21 UTC 2018,,,,,,,"0|i3kafr:",9223372036854775807,,,,,,,,,,,,,,,,"20/Sep/17 09:54;ijuma;We could perhaps call it a `lenient` mode or something. `dryrun` probably implies something slightly different (if I understand the goal of the JIRA).","08/Mar/18 00:46;ewencp;I think dry run is fine if we're clear about what it means. The scariest part of developing the script originally was the final steps around pushing tags. Otherwise most of it is only affecting stuff that's easy to clean up. To me, the most useful dry run would:
 * Still prompt about the steps that would upload, but just say what they would do
 * Say what tagging it would do, but skip it
 * Still do the full build and provide artifacts, which allows you to do some test run and validation without really generating an RC

I'm not sure how the publication to the maven repo would work as a dry-run, maybe just publish to a local directory.

The other thing which could possibly be handled here or could be a different Jira is to have a local copy of all artifacts when you've completed. This would apply to the regular release artifacts (it was a pain to pull them back down from my Kafka home directory in order to promote the release) and the maven artifacts (which is useful for doing validation, which should also be automated).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Kafka to save credentials in Java Key Store on Zookeeper node,KAFKA-6602,13141870,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,airbots,airbots,airbots,01/Mar/18 18:44,01/Mar/18 18:44,12/Jan/21 10:06,,,,,,,,,,,,security,,,,,,0,,,,,"Kafka connect needs to talk to multifarious distributed systems. However, each system has its own authentication mechanism. How we manage these credentials become a common problem. 

Here are my thoughts:
 # We may need to save it in java key store;
 # We may need to put this key store in a distributed system (topic or zookeeper);
 # Key store password may be configured in Kafka configuration;

I have implement the feature that allows store java key store in zookeeper node. If Kafka community likes this idea, I am happy to contribute.",,airbots,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-03-01 18:44:03.0,,,,,,,"0|i3qr93:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect should allow pluggable encryption for records,KAFKA-6525,13135818,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,rhauch,rhauch,02/Feb/18 17:20,02/Feb/18 17:20,12/Jan/21 10:06,,,,,,,,,,,,KafkaConnect,,,,,,0,needs-kip,,,,"The Connect framework does not easily support pluggable encryption and decryption mechanisms. It is possible to use custom Converters to encrypt/decrypt individual keys and values when the encryption metadata (keys, algorithm, etc.) can be specified in the Converter. or when the key and/or value are _wrapped_ to include the metadata. 

However, if the encryption metadata is to be stored as headers, then as of AK 1.1 Connect does have support for using headers in connectors and SMTs, but not Converters. 

We should make it easier to plug encryption and decryption mechanisms into Connect. Since we're moving to Java 8, one approach might be to change the Converter interface to add a default methods that also supply the headers (and maybe the whole record). 

An alternative is to define a new plugin interface that can be used to filter/transform/map the entire source and sink records. Here's we'd actually call this for source connectors before the Converter, and for sink connectors after the Converter is called.",,astubbs,jchipmunk,rhauch,tjee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-02-02 17:20:06.0,,,,,,,"0|i3ppzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Idempotent production for source connectors,KAFKA-6079,13110430,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,,astubbs,astubbs,18/Oct/17 18:23,29/Jan/18 18:48,12/Jan/21 10:06,29/Jan/18 18:48,,,,,,,,,,,KafkaConnect,,,,,,0,,,,,"Idempotent production for source connection to reduce duplicates at least from retires.
",,astubbs,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6080,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-01-29 18:48:32.672,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 29 18:48:32 UTC 2018,,,,,,,"0|i3lf9b:",9223372036854775807,,,,,,,,,,,,,,,,"29/Jan/18 18:48;rhauch;Marking this as WONTFIX since it is *already possible since AK 0.11* to configure the Connect worker producers [to use idempotent delivery|https://kafka.apache.org/documentation/#upgrade_11_exactly_once_semantics]:

{quote}
Idempotent delivery ensures that messages are delivered exactly once to a particular topic partition during the lifetime of a single producer.
{quote}

That eliminates duplicate events due to retries, but a failure of a Connector worker might still mean records are written to Kafka but the worker fails before it can commit the latest offsets; when it restarts, it begins from the last committed offsets and this may certainly result in duplicate messages that were written before the failure.

See KAFKA-6080 for changing Connect to support exactly-once semantics for source connectors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timestamp-based log compaction,KAFKA-5533,13083185,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,enothereska,enothereska,28/Jun/17 15:34,27/Jan/18 04:26,12/Jan/21 10:06,,0.11.0.0,,,,,,,,,,core,,,,,,0,kip,,,,"Today log compaction is based on offsets, not timestamps. That means that a late arriving record with a small timestamp TS can over-write a record with a larger timestamp TL. This affects computations that depend on correct processing of timestamps. For example, in the streams library, this would mean that processing for KTables with late arriving data would not be correct.

This JIRA will require a KIP.",,astubbs,enothereska,mjsax,scosenza,umesh9794@gmail.com,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-06-28 15:34:03.0,,,,,,,"0|i3gujj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Exactly-Once Semantics to Streams,KAFKA-4923,13057664,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mjsax,mjsax,mjsax,20/Mar/17 20:18,18/Jan/18 18:35,12/Jan/21 10:06,17/May/17 00:23,,,,,,,,0.11.0.0,,,streams,,,,,,0,exactly-once,kip,,,https://cwiki.apache.org/confluence/display/KAFKA/KIP-129%3A+Streams+Exactly-Once+Semantics,,darion,githubbot,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-5361,KAFKA-5362,KAFKA-5096,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-04-30 09:17:00.553,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 18 18:35:37 UTC 2018,,,,,,,"0|i3cjfj:",9223372036854775807,,,,,,,,,,,,,,,,"30/Apr/17 09:17;githubbot;GitHub user mjsax opened a pull request:

    https://github.com/apache/kafka/pull/2945

    KAFKA-4923: Add Exactly-Once Semantics to Streams

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/mjsax/kafka kafka-4923-add-eos-to-streams

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2945.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2945
    
----
commit f2161c80df79573b1cd045e2947a245ed1e1e1aa
Author: Matthias J. Sax <matthias@confluent.io>
Date:   2017-04-05T01:07:14Z

    code cleanup

commit abdd6e3c43d727bb4c17f28f69e2677eb8b9be73
Author: Matthias J. Sax <matthias@confluent.io>
Date:   2017-03-21T19:12:25Z

    KAFKA-4923: Add Exactly-Once Semantics to Streams

----
","04/May/17 05:26;githubbot;GitHub user mjsax opened a pull request:

    https://github.com/apache/kafka/pull/2974

    KAFKA-4923: Add Exaclty-Once Semantics to Streams (testing)

     - add broker compatibility system tests

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/mjsax/kafka kafka-4923-add-eos-to-streams-add-broker-check-and-system-test

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2974.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2974
    
----
commit f92b7168a572967656290a6ad75d4799823d4392
Author: Matthias J. Sax <matthias@confluent.io>
Date:   2017-05-03T18:28:59Z

    KAFKA-4923: Add Exaclty-Once Semantics to Streams (testing)
     - add broker compatibility system tests

----
","17/May/17 00:23;guozhang;Issue resolved by pull request 2945
[https://github.com/apache/kafka/pull/2945]","17/May/17 00:24;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2945
","17/May/17 01:07;githubbot;GitHub user mjsax opened a pull request:

    https://github.com/apache/kafka/pull/3077

    KAFKA-4923: Add Streams EOS integration tests

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/mjsax/kafka kafka-4923-stream-eos-integration-tests

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3077.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3077
    
----
commit 0bb3bae10d4e85227554bcd677e3f4f7911ad79c
Author: Matthias J. Sax <matthias@confluent.io>
Date:   2017-05-17T00:37:58Z

    KAFKA-4923: Add Streams EOS integration tests

----
","22/May/17 05:17;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2974
","01/Jun/17 16:24;githubbot;Github user mjsax closed the pull request at:

    https://github.com/apache/kafka/pull/3077
","18/Jan/18 18:35;githubbot;brandonkirchner opened a new pull request #4438: KAFKA-4923 add UUID serializer / deserializer
URL: https://github.com/apache/kafka/pull/4438
 
 
   Added a UUID Serializer / Deserializer.
   
   Added the UUID type to the SerializationTest
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add the AdminClient in Streams' KafkaClientSupplier,KAFKA-6170,13116087,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,03/Nov/17 23:43,12/Jan/18 10:55,12/Jan/21 10:06,20/Nov/17 19:25,,,,,,,,1.1.0,,,streams,,,,,,0,,,,,"We will add Java AdminClient to Kafka Streams, in order to replace the internal StreamsKafkaClient. More details can be found in KIP-220 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-220%3A+Add+AdminClient+into+Kafka+Streams%27+ClientSupplier)",,githubbot,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-4857,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-11-13 01:56:51.55,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 28 17:39:41 UTC 2017,,,,,,,"0|i3me3b:",9223372036854775807,,,,,,,,,,,,,,,,"13/Nov/17 01:56;mjsax;Does this subsume KAFKA-4857 ?","13/Nov/17 20:43;guozhang;I have updated KAFKA-4857 to separate the two stories","14/Nov/17 21:46;githubbot;GitHub user guozhangwang opened a pull request:

    https://github.com/apache/kafka/pull/4211

    [WIP] KAFKA-6170: Add AdminClient to Streams

    1. Add The AdminClient into Kafka Streams, which is shared among all the threads.
    2. Refactored mutual dependency between StreamPartitionAssignor / StreamTread to TaskManager as discussed in https://github.com/apache/kafka/pull/3624#discussion_r132614639.
    
    
    ### Committer Checklist (excluded from commit message)
    - [ ] Verify design and implementation 
    - [ ] Verify test coverage and CI build status
    - [ ] Verify documentation (including upgrade notes)


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/guozhangwang/kafka K6170-admin-client

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/4211.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4211
    
----
commit 6c5b20ea34323a101118286e9282568f428b8e34
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-07T00:14:53Z

    add AdminClient

commit fc908e06d80816db1e28e0f1d05e1d10fa1d0379
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-13T22:13:37Z

    Merge branch 'trunk' of https://git-wip-us.apache.org/repos/asf/kafka into K6170-admin-client

commit d1be566efe65c71c068a6e948c59f7bd980d6bd8
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-14T21:41:20Z

    refactor thread / assignor dependency

commit d1a778fff0cbaeb8ea00421d89fcd50552b93eba
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-14T21:44:09Z

    revert TaskManager APIs

----
","16/Nov/17 00:16;githubbot;GitHub user guozhangwang opened a pull request:

    https://github.com/apache/kafka/pull/4224

    [WIP] KAFKA-6170; KIP-220 Part 2: Break dependency of Assignor on StreamThread

    This refactoring is discussed in https://github.com/apache/kafka/pull/3624#discussion_r132614639. More specifically:
    
    1. Moved the access of `StreamThread` in `StreamPartitionAssignor` to `TaskManager`, removed any fields stored in `StreamThread` such as `processId` and `clientId` that are only to be used in `StreamPartitionAssignor`, and pass them to `TaskManager` if necessary.
    2. Moved any in-memory states, `metadataWithInternalTopics`, `partitionsByHostState`, `standbyTasks`, `activeTasks` to `TaskManager` so that `StreamPartitionAssignor` becomes a stateless thin layer that access TaskManager directly.
    3. Remove the reference of `StreamPartitionAssignor` in `StreamThread`, instead consolidate all related functionalities such as `cachedTasksIds ` in `TaskManager` which could be retrieved by the `StreamThread` and the `StreamPartitionAssignor` directly.
    4. Finally, removed the two interfaces used for `StreamThread` and `StreamPartitionAssignor`.
    
    5. Some minor fixes on logPrefixes, etc.
    
    Future work: when replacing the StreamsKafkaClient, we would let `StreamPartitionAssignor` to retrieve it from `TaskManager` directly, and also its closing call do not need to be called (`KafkaStreams` will be responsible for closing it).
    
    ### Committer Checklist (excluded from commit message)
    - [ ] Verify design and implementation 
    - [ ] Verify test coverage and CI build status
    - [ ] Verify documentation (including upgrade notes)


You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/guozhangwang/kafka K6170-refactor-assignor

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/4224.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4224
    
----
commit 6c5b20ea34323a101118286e9282568f428b8e34
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-07T00:14:53Z

    add AdminClient

commit fc908e06d80816db1e28e0f1d05e1d10fa1d0379
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-13T22:13:37Z

    Merge branch 'trunk' of https://git-wip-us.apache.org/repos/asf/kafka into K6170-admin-client

commit d1be566efe65c71c068a6e948c59f7bd980d6bd8
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-14T21:41:20Z

    refactor thread / assignor dependency

commit d1a778fff0cbaeb8ea00421d89fcd50552b93eba
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-14T21:44:09Z

    revert TaskManager APIs

commit f9e5fbff4c18764bc64793dc9b5c376d956cd67c
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-15T02:04:55Z

    move logic of assignor to task manager

commit bfd08c45cab067035d4980d85d6e7ff9cd5a6e36
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-15T02:16:37Z

    minor fix

commit f95dc0bb9849356ab721c4f7e042a813fcb34330
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-15T02:22:34Z

    extract delegating restore listener

commit 10ceff07c23ea555bd25ea74baa4b995ea0f3a83
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-15T02:26:59Z

    add admin configs in streams config

commit 41dc2b0790866bb5f8325191102622bdbd5fbe23
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-15T19:39:29Z

    add AdminClient to stream thread

commit 3592206eb7c06313a7f553242329f6eb578b4cbd
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-15T22:49:39Z

    Merge branch 'trunk' of https://git-wip-us.apache.org/repos/asf/kafka into K6170-admin-client

commit 03e64d0bb4a6581d4105f9faa4d95cd6e20f45f3
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-15T23:49:16Z

    add admin prefix

commit 035b3a6a04025d397fec8abb535d9b148f722792
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-11-16T00:07:19Z

    merge from K6170-admin-client

----
","20/Nov/17 19:25;guozhang;Issue resolved by pull request 4211
[https://github.com/apache/kafka/pull/4211]","20/Nov/17 19:27;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/4211
","28/Nov/17 17:39;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/4224
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add new metrics to support health checks,KAFKA-5746,13095276,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,17/Aug/17 09:15,20/Dec/17 13:39,12/Jan/21 10:06,28/Sep/17 21:04,,,,,,,,1.0.0,,,metrics,,,,,,0,,,,,"It will be useful to have some additional metrics to support health checks.
Details are in [KIP-188|https://cwiki.apache.org/confluence/display/KAFKA/KIP-188+-+Add+new+metrics+to+support+health+checks]",,githubbot,guozhang,rsivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-818,KAFKA-1920,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-21 13:31:50.701,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 20 13:39:45 UTC 2017,,,,,,,"0|i3iw7z:",9223372036854775807,,,,,,,,,,,,,,,,"21/Aug/17 13:31;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/3705

    [WIP] KAFKA-5746: Add new metrics to support health checks

    This is currently built on top of PR #3673

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka KAFKA-5746-new-metrics

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3705.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3705
    
----

----
","23/Sep/17 04:48;guozhang;*Reminder to the contributor / reviewer of the PR*: please note that the code deadline for 1.0.0 is less than 2 weeks away (Oct. 4th). Please re-evaluate your JIRA and see if it still makes sense to be merged into 1.0.0 or it could be pushed out to 1.1.0, or be closed directly if the JIRA itself is not valid any more, or re-assign yourself as contributor / committer if you are no longer working on the JIRA.","28/Sep/17 21:01;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3705
","29/Sep/17 01:44;githubbot;GitHub user ijuma opened a pull request:

    https://github.com/apache/kafka/pull/3989

    KAFKA-5746; Fix conversion count computed in `downConvert`

    It should be the number of records instead of the
    number of batches.
    
    A few additional clean-ups: minor renames,
    removal of unused code, test fixes.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ijuma/kafka kafka-5746-health-metrics-follow-up

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3989.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3989
    
----
commit a85feb150a4aea8d879535c27ddb857278790e63
Author: Ismael Juma <ismael@juma.me.uk>
Date:   2017-09-29T01:40:29Z

    KAFKA-5746; Fix downConvert's conversion count
    
    It should be the number of records instead of the
    number of batches.
    
    A few additional clean-ups: minor renames,
    removal of unused code, test fixes.

----
","29/Sep/17 08:55;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3989
","29/Sep/17 20:18;githubbot;GitHub user ijuma opened a pull request:

    https://github.com/apache/kafka/pull/3996

    KAFKA-5746; Return 0.0 from Metric.value() instead of throwing exception

    This is less likely to break custom metric reporters and since the method
    is deprecated, people will be warned about this potential issue.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ijuma/kafka avoid-exception-in-measurable-value

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3996.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3996
    
----
commit e97844d036dfe1bc2e795ddb12b5c61201ab3c9c
Author: Ismael Juma <ismael@juma.me.uk>
Date:   2017-09-29T20:14:57Z

    KAFKA-5746; Return 0.0 from Metric.value() instead of throwing exception
    
    This is less likely to break custom metric reporters and since the method
    is deprecated, people will be warned about this potential issue.

----
","01/Oct/17 00:31;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3996
","05/Oct/17 19:19;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/4026

    KAFKA-5746: Document new broker metrics added for health checks

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka MINOR-KIP-188-metrics-docs

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/4026.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4026
    
----
commit 924e3d2f56a6c24a42018f396612c95f02cc5fe1
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2017-10-05T19:12:15Z

    KAFKA-5746: Document new broker metrics added for health checks

----
","20/Dec/17 13:39;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/4026
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incremental Batch Processing for Kafka Streams,KAFKA-4437,13023038,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,mjsax,mjsax,23/Nov/16 23:33,11/Dec/17 18:02,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,9,,,,,"We want to add an “auto stop” feature that terminate a stream application when it has processed all the data that was newly available at the time the application started (to at current end-of-log, i.e., current high watermark). This allows to chop the (infinite) log into finite chunks where each run for the application processes one chunk. This feature allows for incremental batch-like processing; think ""start-process-stop-restart-process-stop-...""

For details see KIP-95: https://cwiki.apache.org/confluence/display/KAFKA/KIP-95%3A+Incremental+Batch+Processing+for+Kafka+Streams",,andremissaglia,avi_parkassist,callritesh2005,enothereska,graphex,mjsax,orair,scosenza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-12-13 14:50:15.08,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 19 22:56:19 UTC 2017,,,,,,,"0|i36q3z:",9223372036854775807,,,,,,,,,,,,,,,,"13/Dec/16 14:50;avi_parkassist;I have a comment but there doesn’t seem to be a mailing list thread yet… do you plan to start one soon?","13/Dec/16 17:23;mjsax;There is a mailing list thread... Just forgot to update the KIP Wiki page... Here it goes: http://search-hadoop.com/m/Kafka/uyzND1YI7Uf2hpKcc?subj=+DISCUSS+KIP+95+Incremental+Batch+Processing+for+Kafka+Streams","13/Dec/16 19:14;avi_parkassist;Ah, great, thanks!","07/Mar/17 14:28;callritesh2005;Dear Team,

Could you please let us know regarding the status of this issue. We are highly interested in this feature and we were under the assumption that this will be released with version 0.10.2.0.

Thanks,
Ritesh Agarwal
+31 616245389","07/Mar/17 17:41;mjsax;Kafka 0.10.2 got released already not containing this feature. Right now, the KIP is still under discussion but nobody is actively working on it. It might not even make it into next release 0.11.0.0 -- it does not seem that anybody finds time picking it up at the moment :(","19/Jun/17 22:42;avi_parkassist;The KIP page says:

bq. Released: 0.10.2.0

Seems like it should be updated.","19/Jun/17 22:56;mjsax;Thanks [~avi_parkassist]. Updated :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Global threshold on data retention size,KAFKA-1489,12720542,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,sandris,sandris,11/Jun/14 09:50,28/Nov/17 14:52,12/Jan/21 10:06,,0.8.1.1,,,,,,,,,,log,,,,,,5,,,,,"Currently, Kafka has per topic settings to control the size of one single log (log.retention.bytes). With lots of topics of different volume and as they grow in number, it could become tedious to maintain topic level settings applying to a single log. 

Often, a chunk of disk space is dedicated to Kafka that hosts all logs stored, so it'd make sense to have a configurable threshold to control how much space *all* data in one Kafka log data directory can take up.

See also:
http://mail-archives.apache.org/mod_mbox/kafka-users/201406.mbox/browser
http://mail-archives.apache.org/mod_mbox/kafka-users/201311.mbox/%3C20131107015125.GC9718@jkoshy-ld.linkedin.biz%3E",,adupriez,githubbot,guozhang,gwenshap,jimhoagland,jkreps,joedj,joliver,jzampieron@zproject.net,llchen,otis,sandris,stevenz3wu,tanbamboo,tmonahan,zarzyk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-06-11 18:25:56.561,,,false,,,,,,,,,,,,,,,,,,398741,,,Tue Nov 28 14:52:30 UTC 2017,,,,,,,"0|i1wnbj:",398863,,,,,,,,,,,,,,,,"11/Jun/14 18:25;joliver;I'll work this one if there are no objections.","11/Jun/14 18:36;jkreps;Go for it!

One slight oddity to consider is this. Different nodes will have different partitions. So the amount of data retained for different replicas of the same partition may vary quite a lot. A replica on a node with lots of data will retain little, and one on a more empty broker will retain lots. The current per-partition retention strategies are only approximately the same across nodes as well, but this will potentially be much more extreme.

In fact, in steady state any partition movement will simultaneously cause data to get purged to free up space.

I don't think this is necessarily a problem but we will need to warn people.","01/Aug/14 05:07;jkreps;Thinking about this I think the right way for this to work would be to introduce a new topic-level config, something like topic.retention.bytes. To use this the broker would divide this count by the number of partitions to compute an effective per-partition retention number. This would avoid uneven retention as described above.","01/Aug/14 16:53;stevenz3wu;topic.rentetion.bytes is also less convenient to use, because one cluster can host multiple topics. now I need to calculate quota for each topic in the cluster.

I would use ""global retention size"" as a safety blanket. it will guarantee that we won't have disk full problem. It's better to drop old msgs than drop new msgs with disk full problem. We should still have disk util alert (e.g. at 80%) and plan to increase capacity or decrease retention period. ","01/Aug/14 17:00;joliver;+1 for Steven's strategy - I could see this being immediately beneficial to end users and not overly difficult to configure.

FYI, I won't get to this for some time.","22/Aug/14 00:45;gwenshap;I still don't see how ""global retention size"" will help avoid disk full, which happens on a single node level.

Assume I have 3 nodes, each with 200GB of space for Kafka. 600GB total. I set my ""global retention size"" to 500GB, to be safe. However, I'm doing a horrible job balancing my messages between partitions and one node ends up with all the data. I'll run out of space after 200GB, long before hitting my ""global retention size"".

Extreme example to make the point that global limits can't help me manage space on a node level.","22/Aug/14 01:14;stevenz3wu;maybe ""global"" is a misleading/inaccurate term. it is per node. In your example, you can set ""global retention size"" to ""180GB"" per node.","22/Aug/14 01:17;gwenshap;Thanks for clarifying! The issue description said "" configurable threshold to control how much space all data in Kafka can take up. ""

""All data"" was a bit misleading. ","22/Aug/14 04:01;jkreps;Yeah I get what you want here, I think you basically want to say ""keep as much data as you can, only throw away when you are low on space"". As I mentioned this is a little weird in that retention among replicas for a partition may be somewhat different from machine to machine depending on the distribution of data on each replica, but this may be okay.

So maybe what you are looking for is something like
max.total.disk.space.bytes=12345
disk.full.discard.policy={oldest, largest, none}
(we can improve the names).

Probably the total disk space config should be per-data directory (since you may have multiple volumes and you don't want any of them to get full).

Let me know if that sounds right.","22/Aug/14 04:22;stevenz3wu;retention among replicas may be somewhat different. I also think it should be ok, because this is a safety blank. we should normally try to plan the capacity to avoid the scenario.

yeah. disk full policy is what I am looking for. ""drop latest"" would sound like a weird option/policy though, because it can trigger offset gap/jump error on consumer side. and in general, it's rare for business use case to drop ""new"" data.

I didn't quite understand ""per-data"" dir. I thought each kafka server/process can only have one root/data dir specified by ""log.dir"" property. then it can't use multiple volumes. please correct me if I am wrong here.

","22/Aug/14 07:22;sandris;That's exactly right Jay, I've amended the description (also to clear up the earlier confusion around ""all data"").","22/Aug/14 15:45;jimhoagland;Steven, you can list more than one directory under log.dir; I've used as many as 10 (10 different volumes).  Each directory is essentially a different partition.

I think the setting be should per-directory and set at the broker level (in server.properties).

We do need to think through what actions to take when approaching the limit (disk.full.discard.policy).  Reasonable choices to me seem to be:
* don't do anything (user doesn't want a limit to be enforced)
* scale back the retention policy for each topic by as high a percent as needed to free up a noticeable amount of space (may require trying multiple percentages)
* discard least recently used topic (for cases where the topics change over time)
* discard least recently used topic, but only if the topic follows a user specified naming pattern
* start rejecting new messages (for cases where the limit is a hard limit but where it is not acceptable to discard data early without a human in the loop); this is a fallback case as well and what we should do when a volume being written to is nearly full

It would be more work to set up so may not be worth it, but perhaps this could be plug-able (user can select what class to use when hitting the limit).","22/Aug/14 16:47;stevenz3wu;[~jimhoagland] thanks for the clarification. per-direction/volume makes sense. 

But it shouldn't take much work to set up. user can just configure the disk-full-policy properties as Jay mentioned earlier. understand that it is probably fine for some use cases to do nothing and just drop the new messages. But drop-msgs-when-disk-full can be very useful to many users assuming consumers are usually fairly close to write head. then it is usually harmless to drop old msgs. ","23/Aug/14 04:21;jkreps;I agree I think generally it makes most sense to consider dropping from the end rather than rejecting new messages. If you buy that then you can think about this feature as being about how to choose which partition to drop the last segment from when you are over your space allocation. The two obvious ways would be (a) drop the oldest segment amongst all logs or (b) drop from the partition which is taking up the most space. However Jim points out the case that make these slightly confusing: you can have different retention settings by space and time for each topic. So if you have one topic which has retention 30 days and one topic with retention 1 day then this emergency discard would always discard from the 30 day topic. Jim's alternative actually makes some sense--assume all topics are in steady state (i.e. up against their maximum retention be it size or time). Then you can just discard (say) 10% across the board. So if that were the case I think the only config you need is something like
  max.total.disk.space.bytes=12345
and we can probably just hard code the 10% discard when you hit this limit.","23/Aug/14 04:45;stevenz3wu;Jay, agree that a simple cross-the-board drop sounds like a good strategy to me. maybe we just need to two config?
max.disk.util.percent=90   # applied per volume
discard.percent=10           # 10% drop cross all topics/partitions","23/Aug/14 05:11;jkreps;Max utilization percent would be ideal but Java doesn't make that easy. How would we get the free space by volume? Obviously any approach that involves shelling out is a no-go as Java's approach to fork can be super heavy-weight. The alternative is to make you give the size which obviously isn't ideal...","25/Aug/14 13:15;jimhoagland;Good discussion here...

In a steady state an emergency discard based on reducing the retention by 10% will free up 10% of disk.  That assumes the topics have been used for long enough to have started to be affected by retention settings and it assumes a steady rate of incoming messages.  If the topic is new relative to its retention period a 10% cut in the retention period may free up 0% of disk.  If there has been a recent increase in messages (either a temporary spike or a new normal), then we would get less than 10% of disk freed.

That said, it adds complication to handle those cases so we may want to pass on those in the first solution.  Things certainly get significantly more complicated and hard to test if the retention cut needs to be iterative and based on how much previous attempts helped.  However, if it is feasible to predict how much impact a reduced retention period will have and we can take that into account when we do the emergency discard then we can adjust the retention cut percentage accordingly.  In fact we may just want to make the discard percent based on how much disk will actually get freed (applied on a per-topic basis) and not really based on a percent reduction in retention period.","25/Aug/14 15:07;stevenz3wu;>  In fact we may just want to make the discard percent based on how much disk will actually get freed (applied on a per-topic basis) and not really based on a percent reduction in retention period.

+1","25/Aug/14 17:29;gwenshap;I actually like the idea of configuring max bytes rather than %.
People love configuring 10% free space, but then modern servers often have 36TB space and 10% is a lot of space to waste!  

I also like the idea of dropping the oldest segment for each log on the space-crunched machine. Its straight forward to explain and implement, and segment sizes are configurable per topic and globally, so admins have ample control.
","25/Aug/14 18:21;guozhang;One concern I have about a global threshold based on percentage is about multi-tenancy. Say we have two applications using the same Kafka server, one with huge traffic on topic A and another with small traffic on topic B, and hence because of the huge traffic of the first application causing disk to be full, we will drop some messages of another application's topic B without the application even being aware of that happening.. 

The per-topic config right now basically provides guarantees for each topic / tenant using Kafka, although it may be tedious, it sounds like the right approach to me for hosting multiple applications. I agree that it ignores the global disk capacity, which will be caused usually when 1) application traffic has naturally increased, in this case we probably should drop its data more aggressively but instead in order to still make the data retention guarantee (e.g. 7 days) we need to expand disk space or event the cluster size; 2) some testing / temporary data get accumulated but not cleaned up in time, in this case we would probably just delete the temporary topic and free up the space; to automate this case we probably can tag the topic when created it (e.g. setting log.retention.size to ""-1"" indicating we do not want to retain any of its data), and when the disk full is alerted, the system can firstly try to clean up those data.

Another issue is that, like Jay said, there is a risk that different nodes may have different sized logs retained for the same partition due to the retention policy, and hence when there is a leader change the consumer clients may get ""out-of-range"" exception. We also need to be careful handling that case.","26/Aug/14 03:29;jimhoagland;> The per-topic config right now basically provides guarantees for each topic / tenant using Kafka, although it may be tedious, it sounds
> like the right approach to me for hosting multiple applications.

I agree.  Is there anything we can do to make it easier to re-used most of the same settings between a set of topics (e.g. those set of topics belonging to a single tenant)?  (Of course there is much more that is needed in Kafka to really have secure multi-tenancy.)

>  setting log.retention.size to ""-1"" indicating we do not want to retain any of its data

If you want to go a bit further on that, then you ask the user to set a retention priority; those with the lowest priority will be cleaned up first.  However I'm not sure if we want to add the complexity or prioritized deletes in the first version.

> Another issue is that, like Jay said, there is a risk that different nodes may have different sized logs retained for the same partition due 
> to the retention policy, and hence when there is a leader change the consumer clients may get ""out-of-range"" exception. We also need 
> to be careful handling that case.

Good point.  If we do get to a condition like that we should say out-of-range to indicate that some messages may have been skipped.  If feasible we should try to coordinate the pruning of a topic across the different partitions (at least amongst those with brokers that are online).  Given this I think we should definitely try to have the invocation of this logic be proactive rather than reactive to allow a bit more relaxed timeframe.  Maybe the check could be part of the retention cleanup process though we would probably want this check to be in place though probably run more frequently than normal retention cleanup.  In any case we should make sure that the emergency drop and normal retention cleanup don't somehow confuse each other by running at the same time.","09/May/16 06:33;githubbot;GitHub user bendrees opened a pull request:

    https://github.com/apache/kafka/pull/1348

    KAFKA-1489: Global threshold on data retention size

    Implemented a ""log retention policy"" based on keeping a certain
    percentage of disk space free. In dynamic situations where topics
    are added in unpredictable ways, the other log retention
    parameters are not entirely sufficient to prevent out-of-disk
    conditions from occurring. The new log.retention.disk.usage.percent
    parameter provides this guarantee. It is applied after all the
    other retention parameters are applied, at the end of each log
    cleanup cycle. Oldest segments (across all topics) are pruned
    until usage falls below this percentage of each disk's capacity.
    The default value is 100, which effectively disables the feature.
    
    This is my original work and I license the work to the project under
    the project's open source license.
    
    @junrao, @jkreps, @gwenshap

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/bendrees/kafka KAFKA-1489

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1348.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1348
    
----
commit 26ef1c5e4a432421f9c1dbdac84d19de1d0ccf54
Author: Ben Drees <ben.drees@zenti.com>
Date:   2016-05-09T06:29:48Z

    Implemented a ""log retention policy"" based on keeping a certain
    percentage of disk space free. In dynamic situations where topics
    are added in unpredictable ways, the other log retention
    parameters are not entirely sufficient to prevent out-of-disk
    conditions from occurring. The new log.retention.disk.usage.percent
    parameter provides this guarantee. It is applied after all the
    other retention parameters are applied, at the end of each log
    cleanup cycle. Oldest segments (across all topics) are pruned
    until usage falls below this percentage of each disk's capacity.
    The default value is 100, which effectively disables the feature.

----
","28/Nov/17 14:52;jzampieron@zproject.net;I'd like to propose thinking about this from a bullet-proof operational perspective. 

When I'm running a cluster, about the only thing I care about is not filling up the disk and dropping new data... almost everyone wants to use the disk as a giant circular buffer. Rotating out the old when it gets full, no matter which topic, partition, whatever.

Regardless of if I have unbalanced topics, partitions, none of that matters here... it's strictly *per-broker* (containerized or not) that I want to retain X amount of bytes of data in my data folder in order to stay running.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka should have a last resort retention setting for max disk used,KAFKA-6235,13119518,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,astubbs,astubbs,19/Nov/17 19:44,25/Nov/17 19:14,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,"Kafka should have an emergency retention setting for max disk used to prevent the broker running out of disk and partitions going off line. When this max is reached, Kafka could perhaps delete segments from the largest topics. Would have to be used with care as current behaviour is to preserve data at the cost of availability. This would favour availability over data retention.

At the moment it's quite hard to reason about disk usage and Kafka as the max byte settings are all per partition, and the math can get complicated when you have lots of topics of different use cases and sizes..",,astubbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-11-19 19:44:52.0,,,,,,,"0|i3mz7b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update kafka-configs.sh to use the new AdminClient,KAFKA-6251,13119989,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,,rsivaram,rsivaram,21/Nov/17 16:14,22/Nov/17 12:25,12/Jan/21 10:06,22/Nov/17 12:25,,,,,,,,,,,tools,,,,,,0,,,,,"The tool {{kafka-configs.sh}} that is used to describe/update dynamic configuration options (topic/quota etc.) currently updates configs directly in ZooKeeper. We should switch this to using the new AdminClient so that updates can be validated and secured without access to ZK. 

This needs a KIP since command line options will need to change.",,rsivaram,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-11-21 16:55:49.017,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 22 12:23:56 UTC 2017,,,,,,,"0|i3n23b:",9223372036854775807,,,,,,,,,,,,,,,,"21/Nov/17 16:55;tombentley;This is a dupe of https://issues.apache.org/jira/browse/KAFKA-5561, I think","21/Nov/17 18:11;rsivaram;I think KAFKA-5561 is updating only {{TopicCommand}} i.e. {{kafka-topics.sh}}. This JIRA is to update {{ConfigCommand}} i.e. {{kafka-configs.sh}}.","22/Nov/17 09:14;tombentley;You're right, but then https://issues.apache.org/jira/browse/KAFKA-5722","22/Nov/17 12:23;rsivaram;[~tombentley] Yes, that is the same one. closing this. Thank you!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Phabricator for code review,KAFKA-679,12625152,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Do,sriramsub,nehanarkhede,nehanarkhede,23/Dec/12 20:55,17/Nov/17 14:33,12/Jan/21 10:06,17/Nov/17 14:33,,,,,,,,,,,,,,,,,0,,,,,"Sriram proposed adding phabricator support for code reviews. 

From http://phabricator.org/ : ""Phabricator is a open source collection of web applications which make it easier to write, review, and share source code. It is currently available as an early release. Phabricator was developed at Facebook.""

It's open source so pretty much anyone could host an instance of this software.

To begin with, there will be a public-facing instance located at http://reviews.facebook.net (sponsored by Facebook and hosted by the OSUOSL http://osuosl.org).

We can use this JIRA to deal with adding (and ensuring) Apache-friendly support that will allow us to do code reviews with Phabricator for Kafka.",,ijuma,jkreps,nehanarkhede,sliebau,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-12-23 22:36:37.337,,,false,,,,,,,,,,,,,,,,,,301692,,,Fri Nov 17 14:33:21 UTC 2017,,,,,,,"0|i16vpb:",248268,,,,,,,,,,,,,,,,"23/Dec/12 20:56;nehanarkhede;Sriram, would you like to take this on ?","23/Dec/12 22:36;jkreps;This seems kind of all-encompassing (code review, wiki, bug tracking, etc). Are you saying we would use this for everything or just code reviews. For code reviews, is it better than review board?","23/Dec/12 22:37;jkreps;Also how would we host it? We need something we can have someone else administer because it is a huge hassle.","23/Dec/12 22:47;nehanarkhede;For code reviews, it seems to be better than reviewboard. There are a bunch of nifty features like being able to upload diffs from command line, update diffs from command line.  There also seems to be integration with Apache JIRA, Hive and Hbase seem to be using it - https://issues.apache.org/jira/browse/HIVE-2486, https://issues.apache.org/jira/browse/HBASE-4611. 

Regarding hosting, here is what they say - ""To begin with, there will be a public-facing instance located at http://reviews.facebook.net (sponsored by Facebook and hosted by the OSUOSL http://osuosl.org).""
 
I'm not saying we just decide to use it, but it will be great to have someone look into how easy it is to use and integrate with Apache.","23/Dec/12 23:10;sriramsub;It looks promising. I will have a look at it and we can then further discuss on the pros and cons.","17/Nov/17 14:00;sliebau;Is there still any activity in this direction ongoing or pending? Seeing as the ticket has not been updated in five years and review/coding/etc. has moved to github I'd assume this can be closed?","17/Nov/17 14:33;ijuma;Yes, this can be safely closed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New consumer checklist,KAFKA-1326,12703569,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,25/Mar/14 19:54,06/Nov/17 17:47,12/Jan/21 10:06,06/Nov/17 17:47,0.8.2.1,,,,,,,0.9.0.1,,,consumer,,,,,,3,feature,,,,"We will use this JIRA to track the list of issues to resolve to get a working new consumer client. The consumer client can work in phases -

1. Add new consumer APIs and configs
2. Refactor Sender. We will need to use some common APIs from Sender.java (https://issues.apache.org/jira/browse/KAFKA-1316)
3. Add metadata fetch and refresh functionality to the consumer (This will require https://issues.apache.org/jira/browse/KAFKA-1316)
4. Add functionality to support subscribe(TopicPartition...partitions). This will add SimpleConsumer functionality to the new consumer. This does not include any group management related work.
5. Add ability to commit offsets to Kafka. This will include adding functionality to the commit()/commitAsync()/committed() APIs. This still does not include any group management related work.
6. Add functionality to the offsetsBeforeTime() API.
7. Add consumer co-ordinator election to the server. This will only add a new module for the consumer co-ordinator, but not necessarily all the logic to do group management. 

At this point, we will have a fully functional standalone consumer and a server side co-ordinator module. This will be a good time to start adding group management functionality to the server and consumer.

8. Add failure detection capability to the consumer when group management is used. This will not include any rebalancing logic, just the ability to detect failures using session.timeout.ms.
9. Add rebalancing logic to the server and consumer. This will be a tricky and potentially large change since it will involve implementing the group management protocol.
10. Add system tests for the new consumer
11. Add metrics 
12. Convert mirror maker to use the new consumer.
13. Convert perf test to use the new consumer
14. Performance testing and analysis.
15. Review and fine tune log4j logging",,eidi,guozhang,hachikuji,hongyu.bi,jarcec,jkreps,liqusha,nehanarkhede,nmarasoi,onurkaraman,philltomlinson,rmetzger,vybs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-387,KAFKA-389,KAFKA-390,KAFKA-364,KAFKA-360,KAFKA-361,KAFKA-1655,,,,,KAFKA-2387,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-07-11 19:16:18.22,,,false,,,,,,,,,,,,,,,,,,381903,,,Thu Oct 02 21:47:47 UTC 2014,,,,,,,"0|i1tspz:",382178,,,,,,,,,,,,,,,,"11/Jul/14 19:16;nmarasoi;Hey guys,
Where is some doc on the ""new consumer""? This is a 0.9.0 thing right?
What drives the need for a new consumer, I was aware only of the low level SimpleConsumer and of the high abstraction level consumer which transparently handled zookeeper offsets commiting and off-thread proactive downloading the upcoming content.
Is there this consumer implementation already started?
What can I take from the unassigned?:P","12/Jul/14 00:11;jkreps;Hey [~nmarasoi] there are some docs on the plans here:
https://cwiki.apache.org/confluence/display/KAFKA/Consumer+Client+Re-Design
and there was some discussion on the APIs on the mailing list.

This may be a slightly tricky one to jump in on, just because it is a fairly substantial chunk of work that doesn't divide up that well. If you are looking for a meaty project, though, there is a bunch of good stuff. I'd be happy to find some things that are cool, substantial, and approachable for someone getting started with the project.","02/Oct/14 21:47;guozhang;A couple of more points we have seen from the old consumer that needs to be carefully addressed in the new consumer:

1. Memory management / decompression: in the old consumer, de-compression can easily allocate a huge amount of memory within a very short time, we need to add similar memory management as new producer to bound the memory usage, and at the same time make sure that we do not allocate more memory than necessary while doing the de-compression.

2. Max fetch size: since the old consumer's max fetch size is fixed by a config, it as a result requires the similar config on the broker / producer. It should be easy to let the new consumer dynamically increase its max fetch size when receiving a single partial message so we can get rid of the new messages.

3. Problems with consumer iterator: today's consumer iterator has several inconsistencies with the Java iterator principles, for example KAFKA-520, and that exception thrown while de-serializing the message will cause it to be skipped since iter.next() has already been called, etc. Although we are not going with the stream-based API in the new consumer but instead use a non-blocking pooling model, we need to make sure these usage pattern issues are not carried in the new API.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cancel ""kafka-reassign-partitions"" Job",KAFKA-1506,12723285,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,,pslung,pslung,24/Jun/14 06:30,01/Nov/17 11:54,12/Jan/21 10:06,01/Nov/17 11:54,0.8.1,0.8.1.1,,,,,,,,,replication,tools,,,,,0,newbie++,,,,"I started a reassignment, and for some reason it just takes forever. However, it won¹t let me start another reassignment job while this one is running. So a tool to cancel a reassignment job is needed. ",,fullung,gmazza,guozhang,ijuma,leoxlin,mr_soundcloud,pslung,virendra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-06-24 15:26:03.455,,,false,,,,,,,,,,,,,,,,,,401472,,,Wed Nov 01 11:54:37 UTC 2017,,,,,,,"0|i1x3rz:",401545,,nehanarkhede,,,,,,,,,,,,,,"24/Jun/14 15:26;guozhang;Any exceptions/errors you saw in the controller log?","24/Jun/14 17:37;pslung;I’m not sure. Unfortunately I have already removed everything so I can’t
verify that now.

Paul Lung



","15/Oct/14 21:27;gmazza;As Guozhang noted, the ""taking forever"" may have been due to exceptions/errors that cancel functionality (which may be very difficult to implement correctly) won't necessarily fix.  If reassignment is something that is not normally expected to take an inordinate amount of time to complete, there probably is no need for cancel functionality but just fixing the problems that are causing the long time, i.e., checking the logs for any problems with Kafka or the user's configuration causing the delay.","01/Nov/17 11:18;mr_soundcloud;Related issue with more reasons to cancel: KAFKA-1676

There are lots of reasons one would want to cancel a reassignment, completely unrelated to bugs in Kafka and purely out of operational considerations (we decided we don't want to do this after all; the reassignment is hurting cluster performance too much; the reassignment is to a dead broker). It appears that the only way to do this is to mess with Zookeeper, or lying about broker IDs (bringing up a new broker with an old ID).

For me, just cancelling everything that is going on and leaving things where they are is the right thing to do – one of the reasons to cancel is because the act of moving things around hurts the cluster, and in that case moving everything back would be just as bad. I would not expect anything to be moved back. Just being able to tell Kafka to ""stop and hold"" would be a significant improvement, and let operators then re-reassign as needed.","01/Nov/17 11:54;ijuma;Marking as a duplicate of KAFKA-1676 as the latter contains more information.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ability to cancel replica reassignment in progress,KAFKA-1676,12746198,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,rberdeen,rberdeen,06/Oct/14 17:29,01/Nov/17 11:54,12/Jan/21 10:06,,0.8.1.1,,,,,,,,,,controller,,,,,,1,newbie,,,,"I've had several situations where I have started a replica reassignment that I've needed to cancel before it completed.

This has happened 
* when moving to a new broker that turns out to be running on an impaired server
* if the extra replication hurts cluster performance
* dealing with replication bugs in kafka, like KAFKA-1670
* when a single replica reassignment is taking a long time, and I want to start more replica assignments without waiting for the current one to finish.

For the first three cases, as a last resort I have deleted the {{/admin/reassign_partitions}} key from ZooKeeper and restarted the controller. I would like to be able to do this by signaling the controller to stop, and to leave the list of assignments as they exist at that moment.",,airbots,bobrik,ijuma,kpocius,locatelli,mr_soundcloud,nehanarkhede,rberdeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-10-10 01:07:16.85,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 01 11:54:02 UTC 2017,,,,,,,"0|i20ukf:",9223372036854775807,,,,,,,,,,,,,,,,"10/Oct/14 01:07;nehanarkhede;[~rberdeen], This is simpler to do if it is just a matter of canceling the pending assignments that have not yet been started. However, once a reassignment operation starts on a partition, it is somewhat complicated to reverse that and may not be worth it.","01/Nov/17 11:13;mr_soundcloud;Just cancelling everything that is going on and leaving things where they are is the right thing to do anyway – one of the reasons to cancel is because the act of moving things around hurts the cluster, and in that case moving everything back would be just as bad.

I think this issue is a dup of KAFKA-1506 though?","01/Nov/17 11:54;ijuma;One thing to consider is what to do with the extra replicas that have been created and haven't caught up yet. Not doing anything about those would mean that the extra traffic would continue. So, there is a case for stopping the extra replicas that have not completed yet. In any case, a KIP would be required.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support ExpanderSketch algorithm for space and time efficient stream processing.,KAFKA-6152,13113266,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,ebegoli,ebegoli,31/Oct/17 10:01,31/Oct/17 11:49,12/Jan/21 10:06,,,,,,,,,,,,core,,,,,,0,features,performance,,,"Support a new ExpanderSketch algorithm (Larsen et al., 2016) based on cluster-preserving clustering and considered the best contemporary streaming algorithm (Quanta, 2017). 

It achieves optimal O(ε^p^_log_n) space, O(_log_n) update time, and fast O(ε^p^poly(_log_n)) query time, and _whp_ correctness

Larsen, K. G., Nelson, J., Nguyên, H. L., & Thorup, M. (2016, October). Heavy hitters via cluster-preserving clustering. In Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on (pp. 61-70). IEEE.
https://arxiv.org/abs/1604.01357

Hartnett, K., (2017, October). Best-Ever Algorithm Found for Huge Streams of Data. Quanta Magazine, October 2017, online at: https://www.quantamagazine.org/best-ever-algorithm-found-for-huge-streams-of-data-20171024/",,ebegoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15724800,15724800,,0%,15724800,15724800,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/17 09:57;ebegoli;larsen2016.pdf;https://issues.apache.org/jira/secure/attachment/12894949/larsen2016.pdf",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-10-31 10:01:31.0,,,,,,,"0|i3lwov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
how to process when all isr crashed,KAFKA-6067,13109897,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,yuhaiyang,yuhaiyang,17/Oct/17 07:05,19/Oct/17 02:17,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,"  when a topic partition's all isr crashed, the partition is Unavailability, why don't like hdfs when a replication in isr crashed and auto move the rep to another alive one,
  what's the original intention of this design?",,astubbs,yuhaiyang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-10-18 14:59:01.779,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 19 02:17:41 UTC 2017,,,,,,,"0|i3lcdb:",9223372036854775807,,,,,,,,,,,,,,,,"18/Oct/17 14:59;astubbs;Are you including the leader? If all the followers and the leader have crashed, then another node cannot run the partition without data loss.
Do you mean if just the followers have crashed, but the leader is still alive, why not assign another broker to become new followers?","19/Oct/17 02:17;yuhaiyang;yes, when all rep in isr crashed , we lost the data on the partition ; why not when a rep in isr crashed, when replicate data on the leader to another alive broker",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add cumulative count attribute for all Kafka rate metrics,KAFKA-5738,13095002,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,16/Aug/17 12:20,04/Oct/17 14:55,12/Jan/21 10:06,14/Sep/17 23:10,,,,,,,,1.0.0,,,metrics,,,,,,0,,,,,"Add cumulative count attribute to all Kafka rate metrics to make downstream processing simpler, more accurate, and more flexible.
 
See [KIP-187|https://cwiki.apache.org/confluence/display/KAFKA/KIP-187+-+Add+cumulative+count+metric+for+all+Kafka+rate+metrics] for details.",,githubbot,ijuma,rsivaram,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-17 10:58:50.909,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 04 14:55:44 UTC 2017,,,,,,,"0|i3iujj:",9223372036854775807,,,,,,,,,,,,,,,,"17/Aug/17 10:58;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/3686

    [WIP] KAFKA-5738: Add cumulative count for rate metrics

    Implementation of https://cwiki.apache.org/confluence/display/KAFKA/KIP-187+-+Add+cumulative+count+metric+for+all+Kafka+rate+metrics

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka KAFKA-5738

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3686.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3686
    
----

----
","14/Sep/17 23:09;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3686
","17/Sep/17 07:56;wushujames;The PR for this forgot to add the new Sender metrics to getAllTemplates() method, which means we won't have autogenerated docs for those metrics. https://github.com/apache/kafka/blame/8a5e86660593eab49c64fdfb5ef090634ae5ae06/clients/src/main/java/org/apache/kafka/clients/producer/internals/SenderMetricsRegistry.java#L111

It's an easy mistake to make, since there is unfortunately no automatic way to keep that list in sync with the actual metrics.

Should I submit a PR to add those in? Or [~rsivaram], would you like to do it?","17/Sep/17 08:50;ijuma;Please submit a PR [~wushujames], if you have the time. To avoid such issues, we could write a test that compares the templates with the actual metrics. And/or we could always use the templates to create actual metrics.","17/Sep/17 16:22;rsivaram;[~wushujames] Thank you for reporting this. I will submit a PR since I broke it. Perhaps you could review the PR.
[~ijuma] It looks like the metrics are created using the templates, but the list used to autogenerate docs needs to be manually populated since it is not required when not generating docs. Adding a unit test to avoid this issue in future.","04/Oct/17 13:00;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/4014

    KAFKA-5738: Upgrade note for cumulative count metric (KIP-187)

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka MINOR-upgrade-KIP-187

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/4014.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #4014
    
----
commit 146a05fce547c559602799b81188793ed749d7a8
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2017-10-04T12:54:46Z

    KAFKA-5738: Upgrade note for cumulative count metric (KIP-187)

----
","04/Oct/17 14:55;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/4014
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ChangeReplicaDirRequest and DescribeReplicaDirRequest (KIP-113),KAFKA-5694,13091971,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Implemented,lindong,lindong,lindong,02/Aug/17 18:56,22/Sep/17 15:25,12/Jan/21 10:06,03/Sep/17 06:25,,,,,,,,1.0.0,,,,,,,,,0,,,,,,,astubbs,githubbot,Kjelle,lindong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-04 05:22:03.261,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 03 06:22:40 UTC 2017,,,,,,,"0|i3ic67:",9223372036854775807,,,,,,,,,,,,,,,,"04/Aug/17 05:22;githubbot;GitHub user lindong28 opened a pull request:

    https://github.com/apache/kafka/pull/3621

    KAFKA-5694; Add ChangeReplicaDirRequest and DescribeReplicaDirRequest (KIP-113)

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/lindong28/kafka KAFKA-5694

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3621.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3621
    
----
commit 0c3262f53de8a99da0d1e2b6b7e817a4c570353b
Author: Dong Lin <lindong28@gmail.com>
Date:   2017-08-02T19:10:07Z

    KAFKA-5694; Add ChangeReplicaDirRequest and DescribeReplicaDirRequest (KIP-113)

----
","03/Sep/17 06:22;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3621
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle SASL authentication failures as non-retriable exceptions in clients,KAFKA-5854,13100495,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vahid,rsivaram,rsivaram,07/Sep/17 15:02,20/Sep/17 21:54,12/Jan/21 10:06,20/Sep/17 21:52,,,,,,,,1.0.0,,,clients,,,,,,0,,,,,"Produce and consumer changes to avoid retries on authentication failures.

Details are in [KIP-152|https://cwiki.apache.org/confluence/display/KAFKA/KIP-152+-+Improve+diagnostics+for+SASL+authentication+failures]",,githubbot,rsivaram,vahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-09-12 00:08:56.614,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 20 21:54:03 UTC 2017,,,,,,,"0|i3jrov:",9223372036854775807,,rsivaram,,,,,,,,,,,,,,"12/Sep/17 00:08;githubbot;GitHub user vahidhashemian opened a pull request:

    https://github.com/apache/kafka/pull/3832

    KAFKA-5854: (WIP) Handle SASL authentication failures as non-retriable exceptions in clients

    This PR depends on the in progress [PR for KAFKA-4764](https://github.com/apache/kafka/pull/3708).

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vahidhashemian/kafka KAFKA-5854

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3832.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3832
    
----
commit a7eca1ecb0d56004559193c716ec83601c5e95f4
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2017-08-21T18:59:43Z

    KAFKA-4764: Wrap SASL tokens in Kafka headers to improve diagnostics

commit e8cc0e663379c75447d8ec894a41b150b68d1f29
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2017-09-11T23:15:22Z

    KAFKA-5854: (WIP) Handle SASL authentication failures as non-retriable exceptions in clients

----
","20/Sep/17 21:52;rsivaram;Issue resolved by pull request 3832
[https://github.com/apache/kafka/pull/3832]","20/Sep/17 21:54;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3832
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SMT to select Kafka topic based on field in message key or value,KAFKA-5869,13101444,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,yevabyzek,yevabyzek,11/Sep/17 21:01,11/Sep/17 21:50,12/Jan/21 10:06,,,,,,,,,,,,KafkaConnect,,,,,,1,needs-kip,,,,"For a given source connector, it would be useful to be able to dynamically determine the Kafka topic to write to based on a field in the message key or value. 

The topic name can be:

(a) derived from the field, e.g.
{noformat}
""topic-${field}""
{noformat}

or

(b) branch on the field, e.g.

{noformat}
""topic1"" if ${field}=x
""topic2"" if ${field}=y
{noformat}


Workaround: create a custom SMT, that generates a new record to the desired topic",,rhauch,yevabyzek,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-09-11 21:01:42.0,,,,,,,"0|i3jxkf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Preferred Replica should be global and dynamic,KAFKA-5808,13098468,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,Canes,Canes,30/Aug/17 05:57,04/Sep/17 12:46,12/Jan/21 10:06,,0.11.0.0,,,,,,,,,,core,,,,,,0,,,,,"When we create a topic in kafka, broker assigns replicas for partitions in this topic, and the First Replica will be the Preferred Replica which means that kafka cluster will migrate partition leader to Preferred Replica on the basis of ''imbalance rate''.

Consider that with the increasing of the brokers, the partitions Preferred Replicas are always the one assigned when created those topic. So the load balancing is not scalable with the change of the scale of the brokers.

So I would like to propose to modify the assignment of the Preferred Replica automatically when cluster expands with appropriate consideration of performance declining.

",,Canes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-08-30 05:57:25.0,,,,,,,"0|i3jfpz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-28: Kafka Streams Checklist,KAFKA-2590,12901068,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,28/Sep/15 17:00,23/Aug/17 00:41,12/Jan/21 10:06,23/Aug/17 00:40,,,,,,,,0.11.0.0,,,streams,,,,,,0,,,,,This is an umbrella story for the processor client and Kafka Streams feature implementation.,,enothereska,guozhang,mjsax,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-09-28 17:00:46.0,,,,,,,"0|i2md53:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add UnderMinIsrPartitionCount and per-partition UnderMinIsr metrics,KAFKA-5341,13075440,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,lindong,lindong,lindong,28/May/17 08:08,18/Aug/17 16:33,12/Jan/21 10:06,28/Jul/17 18:29,,,,,,,,1.0.0,,,,,,,,,0,,,,,"We currently have under replicated partitions, but we do not have a metric to track the number of partitions whose in-sync replicas count < minIsr. Partitions whose in-syn replicas count < minIsr will be unavailable to those producers who uses ack = all. It is important for Kafka operators to be notified of the existence of such partition because their existence reduces the availability of the Kafka service.

More specifically, we can define a per-broker metric UnderMinIsrPartitionCount as ""The number of partitions that this broker leads for which in-sync replicas count < minIsr."" So if the RF was 3, and min ISR is 2, then when there are 2 replicas in ISR this partition would be in the under replicated partitions count. When there is 1 replica in ISR, this partition would also be in the UnderMinIsrPartitionCount.

See https://cwiki.apache.org/confluence/display/KAFKA/KIP-164-+Add+UnderMinIsrPartitionCount+and+per-partition+UnderMinIsr+metrics for more detail.",,alamaison,ewencp,githubbot,jeffwidman,lindong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-07-20 14:05:43.706,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 28 18:30:18 UTC 2017,,,,,,,"0|i3fkfb:",9223372036854775807,,ewencp,,,,,,,,,,,,,,"20/Jul/17 14:05;alamaison;This would be a really useful metric to have.  If I had to pick one metric to show to non-experts to indicate whether the cluster is usable, this would be it.  UnderreplicatedPartitions is useful, especially for devops to see whether the cluster is in an abnormal state, but a cluster with underreplicated is usable as long as there are >= the minIrs replicas of every toppar.","26/Jul/17 17:33;githubbot;GitHub user lindong28 opened a pull request:

    https://github.com/apache/kafka/pull/3583

    KAFKA-5341; Add UnderMinIsrPartitionCount and per-partition UnderMinIsr metrics (KIP-164)

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/lindong28/kafka KAFKA-5341

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3583.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3583
    
----
commit 93f541c249def6bdc158cb79592278d18cdf3ff8
Author: Dong Lin <lindong28@gmail.com>
Date:   2017-05-28T08:10:28Z

    KAFKA-5341; Add UnderMinIsrPartitionCount and per-partition UnderMinIsr metrics (KIP-164)

----
","28/Jul/17 18:29;ewencp;Issue resolved by pull request 3583
[https://github.com/apache/kafka/pull/3583]","28/Jul/17 18:30;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3583
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-72 Allow putting a bound on memory consumed by Incoming requests,KAFKA-4602,13032426,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,radai,radai,radai,06/Jan/17 00:25,26/Jul/17 06:21,12/Jan/21 10:06,26/Jul/17 06:21,,,,,,,,1.0.0,,,core,,,,,,1,,,,,"this issue tracks the implementation of KIP-72, as outlined here - https://cwiki.apache.org/confluence/display/KAFKA/KIP-72%3A+Allow+putting+a+bound+on+memory+consumed+by+Incoming+requests",,davispw,ecomar,ewencp,githubbot,radai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/17 02:42;davispw;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12873208/screenshot-1.png",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2017-01-06 22:40:10.481,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 26 06:21:14 UTC 2017,,,,,,,"0|i38c0v:",9223372036854775807,,ewencp,,,,,,,,,,,,,,"06/Jan/17 22:40;githubbot;GitHub user radai-rosenblatt opened a pull request:

    https://github.com/apache/kafka/pull/2330

    KAFKA-4602 - KIP-72 - Allow putting a bound on memory consumed by Incoming requests

    this is the initial implementation.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/radai-rosenblatt/kafka broker-memory-pool-with-muting

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2330.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2330
    
----
commit 5697bd5a766b61bc9fe5e20c9a605249dd9b284d
Author: radai-rosenblatt <radai.rosenblatt@gmail.com>
Date:   2016-09-27T16:51:30Z

    KAFKA-4602 - introduce MemoryPool interface, use it to control total outstanding memory dedicated to broker requests
    
    Signed-off-by: radai-rosenblatt <radai.rosenblatt@gmail.com>

----
","16/Jan/17 04:24;ewencp;[~radai] I'm going to bump this out of 0.10.2.0 since the PR has comments that I don't think have been addressed yet. Hopefully we can get it merged for 0.10.3.0 (or whatever the next release ends up being). Technically I haven't cut the release branch quite yet, so there may still be a window if you're really motivated to get this in before the subsequent release.","16/Jun/17 02:56;davispw;Does this address increased memory used by decompression of compressed messages during down-conversion for an older consumer?

Here's a stack trace from 0.10.2.  As far as we can tell, a bad client using the old consumer made a FetchRequest with maxBytes=1000000000 (1 billion).  Two problems:
* With socket.request.max.bytes=100MB on the broker, shouldn't the fetch size be limited, regardless?
* The request was for a topic with large, highly compressed messages.  A heap dump shows *6GB* of log records for apparently held in memory for just this one request before it threw the OutOfMemoryError.  (100MB -> 5GB would be consistent without our compression ratio.)

{code}
[2017-06-09 15:16:59,937] ERROR [KafkaApi-105] Error when handling request {replica_id=-1,max_wait_time=0,min_bytes=0,topics=[{topic=transportation.unclassified.legacy-export-consol.export-consol-changed-v2,partitions=[{partition=0,fetch
_offset=19,max_bytes=1000000000}]}]} (kafka.server.KafkaApis)
java.lang.OutOfMemoryError: Java heap space
        at java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:186)
        at java.io.DataOutputStream.writeInt(DataOutputStream.java:197)
        at org.apache.kafka.common.record.LogEntry.writeHeader(LogEntry.java:143)
        at org.apache.kafka.common.record.MemoryRecordsBuilder.appendUnchecked(MemoryRecordsBuilder.java:302)
        at org.apache.kafka.common.record.MemoryRecordsBuilder.appendWithOffset(MemoryRecordsBuilder.java:324)
        at org.apache.kafka.common.record.MemoryRecords.builderWithEntries(MemoryRecords.java:411)
        at org.apache.kafka.common.record.MemoryRecords.builderWithEntries(MemoryRecords.java:393)
        at org.apache.kafka.common.record.MemoryRecords.withLogEntries(MemoryRecords.java:378)
        at org.apache.kafka.common.record.MemoryRecords.withLogEntries(MemoryRecords.java:337)
        at org.apache.kafka.common.record.AbstractRecords.toMessageFormat(AbstractRecords.java:79)
        at kafka.server.KafkaApis$$anonfun$28.apply(KafkaApis.scala:481)
        at kafka.server.KafkaApis$$anonfun$28.apply(KafkaApis.scala:468)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$sendResponseCallback$3(KafkaApis.scala:468)
        at kafka.server.KafkaApis$$anonfun$handleFetchRequest$1.apply(KafkaApis.scala:538)
        at kafka.server.KafkaApis$$anonfun$handleFetchRequest$1.apply(KafkaApis.scala:538)
        at kafka.server.ReplicaManager.fetchMessages(ReplicaManager.scala:478)
        at kafka.server.KafkaApis.handleFetchRequest(KafkaApis.scala:530)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:81)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:62)
        at java.lang.Thread.run(Thread.java:745)
{code}
!screenshot-1.png!","26/Jul/17 06:21;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2330
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker-side compression configuration,KAFKA-1499,12722655,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,omkreddy,jjkoshy,jjkoshy,20/Jun/14 00:23,12/Jul/17 16:14,12/Jan/21 10:06,15/Jan/15 02:48,,,,,,,,0.9.0.0,,,,,,,,,0,newbie++,,,,"A given topic can have messages in mixed compression codecs. i.e., it can
also have a mix of uncompressed/compressed messages.

It will be useful to support a broker-side configuration to recompress
messages to a specific compression codec. i.e., all messages (for all
topics) on the broker will be compressed to this codec. We could have
per-topic overrides as well.

",,guozhang,gwenshap,jjkoshy,jkreps,junrao,nehanarkhede,omkreddy,sludwig,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,KAFKA-1374,,,,,,,"21/Aug/14 15:50;omkreddy;KAFKA-1499.patch;https://issues.apache.org/jira/secure/attachment/12663415/KAFKA-1499.patch","14/Aug/14 17:02;omkreddy;KAFKA-1499.patch;https://issues.apache.org/jira/secure/attachment/12661744/KAFKA-1499.patch","15/Aug/14 08:53;omkreddy;KAFKA-1499_2014-08-15_14:20:27.patch;https://issues.apache.org/jira/secure/attachment/12662031/KAFKA-1499_2014-08-15_14%3A20%3A27.patch","21/Aug/14 16:17;omkreddy;KAFKA-1499_2014-08-21_21:44:27.patch;https://issues.apache.org/jira/secure/attachment/12663420/KAFKA-1499_2014-08-21_21%3A44%3A27.patch","21/Sep/14 10:30;omkreddy;KAFKA-1499_2014-09-21_15:57:23.patch;https://issues.apache.org/jira/secure/attachment/12670282/KAFKA-1499_2014-09-21_15%3A57%3A23.patch","23/Sep/14 09:17;omkreddy;KAFKA-1499_2014-09-23_14:45:38.patch;https://issues.apache.org/jira/secure/attachment/12670674/KAFKA-1499_2014-09-23_14%3A45%3A38.patch","24/Sep/14 08:52;omkreddy;KAFKA-1499_2014-09-24_14:20:33.patch;https://issues.apache.org/jira/secure/attachment/12670932/KAFKA-1499_2014-09-24_14%3A20%3A33.patch","24/Sep/14 08:57;omkreddy;KAFKA-1499_2014-09-24_14:24:54.patch;https://issues.apache.org/jira/secure/attachment/12670933/KAFKA-1499_2014-09-24_14%3A24%3A54.patch","25/Sep/14 05:38;omkreddy;KAFKA-1499_2014-09-25_11:05:57.patch;https://issues.apache.org/jira/secure/attachment/12671153/KAFKA-1499_2014-09-25_11%3A05%3A57.patch","26/Oct/14 07:44;omkreddy;KAFKA-1499_2014-10-27_13:13:55.patch;https://issues.apache.org/jira/secure/attachment/12677159/KAFKA-1499_2014-10-27_13%3A13%3A55.patch","16/Dec/14 17:11;omkreddy;KAFKA-1499_2014-12-16_22:39:10.patch;https://issues.apache.org/jira/secure/attachment/12687518/KAFKA-1499_2014-12-16_22%3A39%3A10.patch","26/Dec/14 16:10;omkreddy;KAFKA-1499_2014-12-26_21:37:51.patch;https://issues.apache.org/jira/secure/attachment/12689182/KAFKA-1499_2014-12-26_21%3A37%3A51.patch",,,,,12.0,,,,,,,,,,,,,,,,,,,,2014-08-04 15:36:25.722,,,false,,,,,,,,,,,,,,,,,,400846,,,Wed Jul 12 16:14:55 UTC 2017,,,,,,,"0|i1x007:",400932,,jjkoshy,,,,,,,,,,,,,,"04/Aug/14 15:36;omkreddy;I would like to propose new server property ""*log.compression.type*"" and topic override property ""*compression.type*""
If set, this property is used for compression type at server-side. All non-compressed messages on the broker will be compressed to this compression type.

log.compression.type=none|gzip|snappy (default = none)
compression.type=none|gzip|snappy","11/Aug/14 15:26;junrao;Yes, I think this makes sense. The same property can be used in KAFKA-1374 when copying data during compaction.","11/Aug/14 15:32;gwenshap;+1
Being able to set compression broker-side will be very useful. ","14/Aug/14 17:02;omkreddy;Created reviewboard https://reviews.apache.org/r/24704/diff/
 against branch origin/trunk","15/Aug/14 08:53;omkreddy;Updated reviewboard https://reviews.apache.org/r/24704/diff/
 against branch origin/trunk","21/Aug/14 15:50;omkreddy;Created reviewboard https://reviews.apache.org/r/24935/diff/
 against branch origin/trunk","21/Aug/14 16:17;omkreddy;Updated reviewboard https://reviews.apache.org/r/24704/diff/
 against branch origin/trunk","15/Sep/14 03:56;nehanarkhede;[~jjkoshy] Any chance you can take a quick look at the updated patch to see if your review comments are addressed? If not, please feel free to reassign for review.","15/Sep/14 17:45;jjkoshy;Yes I will take a look. However, [~omkreddy] can you comment on whether you were able to run a stress test with your patch? As I mentioned in the RB my original WIP patch did not work after multiple compaction cycles.","16/Sep/14 04:18;omkreddy;[~jjkoshy]  I think your are commenting about KAFKA-1374(compaction support for compressed topics). I will update stress test results in couple of days.

Recap of work done in this patch:
1. Introduced new server side config properties  *broker.compression.enable* , *broker.compression.type* and per topic override  property *compression.type*

2. If broker.compression.enabled=true, then broker will re-compress the received messages to configured compression type, irrespective of their original compression.
","16/Sep/14 18:48;jjkoshy;Sorry you are right - I mixed up the two. For some reason I thought this was reviewed and checked-in. My bad - will review this and get back to you.","21/Sep/14 10:30;omkreddy;Updated reviewboard https://reviews.apache.org/r/24704/diff/
 against branch origin/trunk","23/Sep/14 09:17;omkreddy;Updated reviewboard https://reviews.apache.org/r/24704/diff/
 against branch origin/trunk","24/Sep/14 08:52;omkreddy;Updated reviewboard https://reviews.apache.org/r/24704/diff/
 against branch origin/trunk","24/Sep/14 08:57;omkreddy;Updated reviewboard https://reviews.apache.org/r/24704/diff/
 against branch origin/trunk","25/Sep/14 00:51;jjkoshy;[~omkreddy] I thought I would try this locally but it does not seem to work.

Set broker.compression.enable=true; compression.type=gzip (in the broker config)
Use console producer to send to some topic
Use DumpLogSegments to check that the messages are compressed (they are not)
This also applies to topics with a compression.type config explicitly set to gzip/snappy

The issue could be related to how the broker compression flags are passed to LogConfig.

Can you debug this locally and provide an updated RB?
","25/Sep/14 05:38;omkreddy;Updated reviewboard https://reviews.apache.org/r/24704/diff/
 against branch origin/trunk","25/Sep/14 05:45;omkreddy;i missed a line in LogConfig. Uploaded new patch with changes.","26/Sep/14 01:36;jjkoshy;Thanks - it works now.

Sorry I had forgotten to run the unit tests. The DynamicConfigChangeTest fails - however, we probably should not simply add the broker.compression.enable property to the valid list since it is not really a valid topic-level config. We can follow-up tomorrow on this.
","26/Sep/14 04:18;omkreddy;Yes, broker.compression.enable property  does not belongs to LogConfig. 
This property is available in kafkaConfig. KafkaConfig is not used in Log.scala.

Can I modify the constructor of LogManager, Log.scala classes to pass ""broker.compression.enable"" property.","26/Sep/14 23:20;jjkoshy;That sounds reasonable. Another option would be to allow the config to be in LogConfig and ""split"" ConfigNames into LogConfigNames and TopicConfigNames. Although broker.compression.enable is not a valid topic-level config it is a reasonable log-level config. TopicConfigNames can be used for validating topic configs. What do you think? [~jkreps] do you have any preference?
","28/Sep/14 18:30;jkreps;Hey [~jjkoshy] you can only specify log configs at the topic level not the partition level so effectively LogConfig is TopicConfig so I don't think we should split that. (Not sure if I'm properly understanding...)

Also, I wonder if we should get rid of broker.compression.enable? The implication is that we would retain the feature of keeping whatever compression the producer used. In the case where we are intermingling different compression types this makes log compaction pretty difficult as you have to retain the weird mixture of compression types. It seems like we should just make the change and have the log have a single compression type set by the topic. Thoughts?","29/Sep/14 17:46;jjkoshy;Yes I understand that the LogConfig is at the topic level. The broker-side compression-type is also intuitive as a topic-level config. The remaining problem is that we need to somehow pass in a config to indicate whether broker-side compression is enabled at all or not. That makes sense as a broker config, and I think it makes sense to pass it in to LogManager (either via its default logconfig or explicit constructor parameter), and I think it also makes sense to allow passing that to the Log class via a constructor parameter (because the Log class contains implementation details of how messages are appended which is where compression if any happens). I'm not sure whether we should allow it to be passed into to Log via the LogConfig which is why if we allow it we will need to ""filter"" that out in LogConfig.validateNames

The earlier patch did not include broker.compression.enable but we felt it would lead to confusing results - this is described in the earlier review comments. (See follow-up comments to Neha's initial review).","29/Sep/14 21:47;jkreps;Hey Joel, that makes sense.

I chatted with Neha. I understand the motivation behind the enable/disable config. Basically the concern I have here is that we are effectively making this feature work two different ways without a clear rationale for supporting both. This will be much more confusing then just providing the more sensible way.

As long as there is an option to disable broker compression then compaction won't work properly (at best compaction will have to just decompress the topic which most people will think is a bug). Plus the feature will be a bit confusing to use. People will see the topic-level compression setting and set it for their topic (e.g. enable snappy compression) but nothing will happen because broker compression will be disabled at the server level. Most people will think this is just broken.

Since we have to decompress and recompress the messages anyway having a uniform codec per topic is actually not a disadvantage (i.e. retaining heterogenous compression codecs given by the client will not make things more efficient).

So I would advocate for removing the enable/disable flag. I think then the statement of how compression works will be this: 
""A compression codec is specified per topic with a broker-level default. This will compress data on disk, as well as saving network bandwidth when the data is sent to the consumer. The producer can also compress data to save network when sending data to the broker, however data will always be written to disk with the compression codec specified for that topic irrespective of the compression used by the producer"".

After chatting with Neha I think we were on the same page. At first we thought it would be nice to have the enable/disable flag temporarily so we could retain the current behavior, but thinking about it that just prolongs the time until we have to remove it at which point behavior will change for the user anyway. So we might as well just make the change now, rather than changing behavior twice and leaving things in an odd state in between. What do you think?","30/Sep/14 06:00;omkreddy;Hi Jay,
{quote}
As long as there is an option to disable broker compression then compaction won't work properly
{quote}
    Can you explain why compaction won't work properly?

What can be the default compression type?  With this new behavior it is must for users/ops to configure some compression type. 

As we are trying to change current behavior, we may have to push this to 0.9 release.","30/Sep/14 17:03;jkreps;Here is the problem with compaction. Currently the log may contain a mixture of records in different compression codecs interleaved. Compaction means going through, decompressing, and recopying active records to a new compressed segment. However maintaining the original compression becomes quite complex and inefficient because we have to find the arbitrary boundaries in the log where one compressed message set ends and another begins. Even if we deal with the complexity and try to maintain the compression, over time this will result in having each message compressed individually.

Since we currently haven't been able to implement this the combination of compression and compaction don't work. The proposed fix was to move to a model where compression is set at the topic level and applied on the broker (as in this ticket). This would let the compaction always just recompress using the default compression type for the topic (i.e. the global default or topic override for that topic).

I think the default compression type should be none (i.e. producer may compress requests but the data won't be stored compressed).

I agree that this is a change in behavior and that users using compression will have to set compression types when they upgrade. I also think the change may confuse some people as the compression they set on the producer will no longer be carried through to the log/consumer. However leaving the on/off switch doesn't resolve this confusion, I think, it just makes it worse because it adds a whole other mode where compression by the producer is retained.

Thoughts?","01/Oct/14 01:49;jjkoshy;If we provide a broker-compression-enabled config: I think the problem with compaction is less of an issue than forgetting to enable the config. i.e., I agree that if an admin forgets to enable it and a user's topic has a compression.type override it is confusing if there are messages with some other compression type on the broker. With log compaction though: I think if there are heterogeneous codecs in the log then in a sense all bets are off. i.e., we can pick and choose whatever codec we want (say, the last non-non-compression codec in a batch) and not bother with preserving the retained message's compression codec. Besides, there is no guarantee that a specific producer's message is the one that that will be retained.

If we do not provide a broker-compression-enabled config: The main concern I have with this is that the most likely default is going to be NoCompressionCodec. Most people will forget to set this when upgrading and end up with uncompressed data which could be an issue for users with a lot of data. Even if people have alerts on disk usage and such, there will most likely be a moderate margin (wrt typical alert thresholds) and it may not be an option to just turn on the config at that point without doing a difficult (manual) clean up first to free up space.

So I guess we are down to picking the lesser of two evils - I'm not sure which one is less evil though :)

Anyone have any strong preference/further critique on the pros/cons of one over the other?
","01/Oct/14 04:37;jkreps;Yeah I totally agree.

I agree that some heuristic that worked batch-by-batch might be okay, I hadn't thought of that. Actually though I think the main motivation for this feature was to fix the compaction issue, so if that is an okay fix just doing that would be an alternative.

I also agree that NoCompressionCodec should be the default and unless people know about the change they will surely be confused by this switch. However I claim this is a temporary confusion based on the fact that previously Kafka compression worked one way and now it will work a new way. Plus they will in any case have this confusion if they turn on the feature. For any new user the configuration docs will all be updated and in the process of learning how to turn on compression they will learn how it works. I think we could help this with good release notes (when doing an upgrade people always read that to ensure it is in-place compatible).

I guess in the end what I am arguing is that we should make a choice. Either a single compression codec per topic is better and it should work that way or else having the producer specify compression is better and it should work that way. Giving the user the choice seems nice but it actually just adds complexity since now we will always have to document and explain both and tell people about the configuration knob to choose and then advise them on how to best make the choice (and then debug when they get lost in all this). If we think the right choice is very situation specific (in situation x, chose broker.compression.enabled=true, in situation y chose false) then okay maybe we need a config, but then let's figure out what the situations you want one versus the other. If it isn't situation specific we should just choose one and implement and document that.","02/Oct/14 18:44;guozhang;One random thought is that by doing so we will always be running the de-/re-compression at message receipt right? We have been discussing about possibilities to reduce multiple copies or de-/re-compress in Kafka (KAFKA-527), and I think the de-/re-compression can actually be done at least for reassigning message offsets.","03/Oct/14 15:30;jjkoshy;[~jkreps] Regarding the two approaches, both are susceptible to people forgetting (or misunderstanding) the configs. Providing a broker.compression.enabled property which defaults to false helps avoid an already deployed scenario from being affected. Producers continue to set whatever compression.type they already use and that is unaffected at the broker. The issues with this as you point out are an additional config to deal with and forgetting to turn it on which would be confusing when people use per-topic overrides.

On the other hand, the other approach of not having the config and assuming that broker compression (or decompression) is always enabled is better when people use per-topic overrides but is slightly dangerous/inconvenient if an existing deployment needs compression enabled and upgrades and does not set it to a suitable compression type.

How about the following: right now all of our server configs are always present - either explicitly specified or default. In this instance it is better to make broker.compression.type an optional config. i.e., if it is is explicitly specified use it otherwise, assume that broker compression/decompression is disabled. So when appending messages to the log: if (broker-compression-type-config is specified) use that; else if (topic-compression-type-override is specified) use that; else use whatever compression type the producer sent the message with.","03/Oct/14 15:34;jjkoshy;[~guozhang] I do believe we can avoid the need to assign offsets for every message by providing more information in the message header (e.g., put the offset for the earliest message and also include num. messages in the message-set). However, I'm not sure that is related to this. This patch may force a recompression if the target compression type is different from the original compression type. So not sure I follow what you mean - can you clarify?","03/Oct/14 15:38;guozhang;Hey Joel I think we are on the same page, but I was not clear before: I was just saying that by using a different broker-side compression if the compression codec is different between the producer and the broker then we cannot avoid de-/re-compression.","03/Oct/14 15:48;jjkoshy;Yes that is correct. However, in practice I think it is still worth thinking through how to avoid de/re-compression. This is because most companies would set a default compression codec in the producer and use the same setting in the broker. At least we can document this as best-practice.","03/Oct/14 18:39;jkreps;Hey [~jjkoshy] I agree that you can kind of argue for broker-side compression and you can kind of argue for just making client-side compression work with compaction. But I really don't understand the case for having both, other than reducing friction in a single release.","07/Oct/14 19:10;jjkoshy;Talked to Jay offline and here is a summary of what we discussed: the main motivations for this feature (currently) are:
# Log compaction
# Ensuring messages on the broker are compressed if a bunch of producers happen to send messages uncompressed - say if all producers in an organization happened to pick up a bad config over time
# Ensuring messages on the broker are compressed with a specific compression type - perhaps if downstream consumers want only that compression type

For the first use-case, we can potentially get around it as described above by picking any compression type - i.e., if we are writing out a batch of messages that contains various compression types we can just pick one of those types. This is not as neat as having an explicit target compression type but it seems reasonable.

The second and third use-cases are likely only marginally useful.

So we have a couple of options:
* Do nothing - given that we have (what seems to be) a reasonable approach for dealing log compaction. i.e., we can table this and revisit if we have a very compelling use case for it.
* Add the compression.type config as a server config and topic-override config.
** In order to address the concern of forgetting to set this (or misconfiguration) there are two approaches:
*** Make it an optional configuration as mentioned further above
*** Have it default to compression.type ""producer"" - which means use whatever compression type the producer used.
","09/Oct/14 16:32;omkreddy;[~jjkoshy] As we don't have strong use case, we can park this issue.  Now we can concentrate on compression in log compaction (KAFKA-1374).  Whenever you get a chance, Pl review KAFKA-1374.","09/Oct/14 16:58;jkreps;Specifically the nuance I thought made this better was to change the configuration slightly from the proposal in this patch. The problem with broker.compression.enable/compression.type was that the way compression worked was dependent on whether the enable/disable flag was set to true, which is confusing. Joel had an idea that I thought fixed this, let's see if other people agree. We instead have compression.type which is a broker-level config as well as a topic override. However this config takes the following options: NONE, GZIP, SNAPPY, PRODUCER. GZIP and SNAPPY are self-explanatory. NONE means that regardless of what the producer sets, the broker writes the message decompressed. PRODUCER means the broker attempts to retain whatever is used by the producer (this will just be approximate in the case of a log compaction and a mixture of codecs). We will default the behavior to PRODUCER so things will work as they do today for most people.

Thoughts?

Personally I think  although there isn't an incredibly pressing use case this would be a good generalization of how things work today that isn't too confusing.

Let's get consensus from the rest of the people who chimed in on this ticket on how things should work and whether this is worth doing before we proceed, though, so we don't cause to much churn for [~omkreddy] who is actually doing the work here. [~nehanarkhede], [~junrao], [~guozhang], [~gwenshap] thoughts?","09/Oct/14 17:47;nehanarkhede;bq. We instead have compression.type which is a broker-level config as well as a topic override. However this config takes the following options:  NONE, GZIP, SNAPPY, PRODUCER

+1 on this option, though it might be easier to name the config option UNCOMPRESSED instead of NONE.","09/Oct/14 17:51;jkreps;Agreed: UNCOMPRESSED is better.","10/Oct/14 21:54;jjkoshy;[~omkreddy] will review your other patch, but it seems we have consensus on this one as well.","26/Oct/14 07:44;omkreddy;Updated reviewboard https://reviews.apache.org/r/24704/diff/
 against branch origin/trunk","16/Dec/14 17:11;omkreddy;Updated reviewboard https://reviews.apache.org/r/24704/diff/
 against branch origin/trunk","16/Dec/14 17:21;omkreddy;[~jjkoshy]  can you review this patch?","17/Dec/14 06:12;jjkoshy;Thank you for the ping, and sorry about the delayed review. I should be able to get to this within a day.","26/Dec/14 16:10;omkreddy;Updated reviewboard https://reviews.apache.org/r/24704/diff/
 against branch origin/trunk","05/Jan/15 12:09;omkreddy;[~jjkoshy] Uploaded a new patch with some modifications.. Pl let me know if any changes are required.","15/Jan/15 02:48;jjkoshy;Thanks for the patch. Committed to trunk - with minor edits to comments and whitespace.","15/Jan/15 06:08;omkreddy;Thanks for the review.","12/Jul/17 13:08;sludwig;I have a question regarding this contract.

Assume a broker in its lifetime always had compression.type snappy and contains a number of topics. At some point in time then, it is shutdown, configured to compression.type lz4 (no other changes than that) and started again, just as usual in the same Kafka cluster. What happens to the existing topics?

Following from the discussion above and what has been agreed, implemented and documented, nothing should change in existing topics at all, also not on compaction or replication. The existing topics should remain forever with a LogConfig compression.type snappy. Only new topics should be in lz4, and only on that particular broker.

Is this correct, or is there some room for messing up a topic (maybe through replication, or because other brokers in the cluster have a different default compression.type... eventually leading to a dreaded mixture of compressions)?","12/Jul/17 13:34;omkreddy;[~sludwig]  Yes, newly produced data will use latest compression.type. In compact mode, it is possible that old data may get compacted using new compression.type.","12/Jul/17 15:23;sludwig;Is there a way to use the override on topic level for compression.type to prevent that? If so, how exactly can this be achieved?","12/Jul/17 16:14;omkreddy;[~sludwig] yes, you can override on topic level for compression.type. You can use kafka-configs.sh script to set the topic level for compression.typec config.
{code} bin/kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name my-topic
    --alter --add-config compression.type=snappy
{code}",,,,,,,,,,,
Add Global Table support to Kafka Streams,KAFKA-4490,13025914,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,damianguy,damianguy,damianguy,06/Dec/16 10:12,06/Jul/17 17:02,12/Jan/21 10:06,13/Jan/17 23:55,0.10.2.0,,,,,,,0.10.2.0,,,streams,,,,,,0,,,,,"As per KIP-99 https://cwiki.apache.org/confluence/display/KAFKA/KIP-99%3A+Add+Global+Tables+to+Kafka+Streams

Add support for Global Tables",,astubbs,damianguy,frankbass33,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-12-12 11:43:51.892,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 17:02:50 UTC 2017,,,,,,,"0|i377v3:",9223372036854775807,,guozhang,,,,,,,,,,,,,,"12/Dec/16 11:43;githubbot;GitHub user dguy opened a pull request:

    https://github.com/apache/kafka/pull/2244

    KAFKA-4490: Add Global Table support to Kafka Streams

    Add Global Tables to KafkaStreams. Global Tables are fully replicated once-per instance of KafkaStreams. A single thread is used to update them. They can be used to join with KStreams, KTables, and other GlobalKTables. When participating in a join a GlobalKTable is only ever used to perform a lookup, i.e., it will never cause data to be forwarded to downstream processor nodes.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/dguy/kafka global-tables

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2244.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2244
    
----
commit 57ede179c29ebafd3e020257cbee10f6a57a25ff
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-10-18T14:50:35Z

    global tables

----
","06/Jul/17 15:12;frankbass33;in my project we need  of this: 

<K1, V1, R> KTable<K, R> join(final GlobalKTable<K1, V1> globalTable,
                              final KeyValueMapper<K, V, K1> keyMapper,
                              final ValueJoiner<V, V1, R> joiner);

when please you release it ? 
it is strictly necessary there is another way to have the same results ?","06/Jul/17 17:02;mjsax;See: https://issues.apache.org/jira/browse/KAFKA-4628",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Connect embedded API,KAFKA-2378,12849232,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,ewencp,ewencp,27/Jul/15 06:24,08/Jun/17 23:32,12/Jan/21 10:06,,,,,,,,,,,,KafkaConnect,,,,,,0,needs-kip,,,,"Much of the required Copycat API will exist from previous patches since any main() method will need to do very similar operations. However, integrating with any other Java code may require additional API support.

For example, one of the use cases when integrating with any stream processing application will require knowing which topics will be written to. We will need to add APIs to expose the topics a registered connector is writing to so they can be consumed by a stream processing task",,ewencp,michal.harish,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-07-27 06:24:52.0,,,,,,,"0|i2hybr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support automatic restart of failed tasks,KAFKA-5352,13076161,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,jpechane,jpechane,31/May/17 11:01,31/May/17 16:37,12/Jan/21 10:06,,0.10.2.1,,,,,,,,,,KafkaConnect,,,,,,2,,,,,Sometimes a task fails when a connection is temporary lost to sink/source. The task now need to be restarted manually. It would be very useful if tasks are automcatially restarted by Connect when configured to do it.,,darvar,jchipmunk,jpechane,rhauch,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-3819,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-05-31 16:37:16.496,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 31 16:37:16 UTC 2017,,,,,,,"0|i3fovj:",9223372036854775807,,,,,,,,,,,,,,,,"31/May/17 16:37;rhauch;Potentially related to KAFKA-3819, and it's possible a single implementation will satisfy both.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Transactional Coordinator,KAFKA-5059,13063492,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,damianguy,damianguy,damianguy,12/Apr/17 13:53,31/May/17 06:44,12/Jan/21 10:06,31/May/17 06:44,,,,,,,,0.11.0.0,,,core,,,,,,0,,,,,"This covers the implementation of the transaction coordinator to support transactions, as described in KIP-98: https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging",,damianguy,githubbot,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-04-12 18:14:13.307,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 31 06:44:57 UTC 2017,,,,,,,"0|i3djd3:",9223372036854775807,,,,,,,,,,,,,,,,"12/Apr/17 18:14;githubbot;Github user dguy closed the pull request at:

    https://github.com/apache/kafka/pull/2846
","13/Apr/17 07:26;githubbot;GitHub user dguy opened a pull request:

    https://github.com/apache/kafka/pull/2849

    KAFKA-5059: Implement Transactional Coordinator

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/confluentinc/kafka exactly-once-tc

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2849.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2849
    
----
commit 4d17b7c96293ca8f9735049070512be9707aba27
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-03-02T01:42:49Z

    Transaction log message format (#134)
    
    * add transaction log message format
    * add transaction timeout to initPid request
    * collapse to one message type

commit af926510d2fd455a0ea4e82da83e10cde65db4e9
Author: Apurva Mehta <apurva@confluent.io>
Date:   2017-03-15T20:47:25Z

    Fix build and test errors due to reabse onto idempotent-producer branch

commit fc3544bf6b55c48d487ef2b7877280d3ac90debb
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-03-17T05:40:49Z

    Transaction log partition Immigration and Emigration (#142)
    
    * sub-package transaction and group classes within coordinator
    * add loading and cleaning up logic
    * add transaction configs

commit fc5fe9226dd4374018f6b5fe3c182158530af193
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-03-21T04:38:35Z

    Add transactions broker configs (#146)
    
    * add all broker-side configs
    * check for transaction timeout value
    * added one more exception type

commit ef390df0eacc8d1f32f96b2db792326a053a5db1
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-03-31T22:20:05Z

    Handle addPartitions and addOffsets on TC (#147)
    
    * handling add offsets to txn
    * add a pending state with prepareTransition / completeTransaction / abortTransition of state
    * refactor handling logic for multiple in-flight requests

commit 2a6526a861546eb4102b900d1da703fd2914bd43
Author: Apurva Mehta <apurva@confluent.io>
Date:   2017-04-07T19:49:19Z

    Fix build errors after rebase onto trunk and dropping out the request stubs and client changes.

commit 4d18bb178cd48364bf610e615b176ad8f0d8385f
Author: Apurva Mehta <apurva@confluent.io>
Date:   2017-04-03T21:17:25Z

    Fix test errors after rebase:
    
     1. Notable conflicts are with the small API changes to
    DelayedOperation and the newly introduced purgeDataBefore PR.
    
     2. Jason's update to support streaming decompression required a bit of
    an overhaul to the way we handle aborted transactions on the consumer.

commit f639b962e8ba618baaef47611e21e2b85b5e5725
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-03-24T22:42:53Z

    fix unit tests

commit 853c5e8abffdb723c6f6b818fdeeab94da8667ed
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-03-24T22:52:37Z

    add sender thread

commit 879c01c3b5b305485cfd26cb8ceedf453b984067
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-03-28T01:04:53Z

    rename TC Send Thread to general inter-broker send thread

commit 239e7f733f8b814ca2d966a80359d8d0de5dee50
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-03-29T21:58:45Z

    add tc channel manager

commit b1561da6e2893fad7bcfacba76db4e4df6414577
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-03-29T21:59:26Z

    missing files

commit 62685c7269fc648a2401fc7a71f31b9536d7c08a
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-03-31T22:15:37Z

    add the txn marker channel manager

commit 298790154c9bfe46f8e4a6b2e0372297fb19896a
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-04-05T16:09:27Z

    fix compilation errors

commit 4f5c23d051453d27f3179a442fe3d822b77d4e12
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-04-10T10:58:43Z

    integrate EndTxnRequest

commit e5f25f31e85fd8104c3df8f8195ccb60694610bc
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-04-10T13:43:40Z

    add test fo InterBrokerSendThread. Refactor to use delegation rather than inheritance

commit 8bbd7a07be28585cd329a1fc769fcc340f866af2
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-04-10T16:24:24Z

    refactor TransactionMarkerChannelManager. Add some test

commit 195bccf8c3945696e6e15cc093072ba83e706eec
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-04-10T18:25:57Z

    more tests

commit c28eb5a0b339cce023e278d7eafcf3e8a98fa8e2
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-04-11T09:23:36Z

    remove some answered TODOs

commit 4346c4d36f242e2480e4a808bed0ef19df6a2335
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-04-11T15:46:37Z

    update to WriteTxnMarkersRequest/Response from Trunk

commit 46880d78eae7d2e7853c404bd1d9b19b8ec4e569
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-04-11T16:19:01Z

    add missing @Test annotation

commit cbcd55e0d046d8c6d88ddfa5bbdfbc230b171e13
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-04-12T19:59:19Z

    fixes after rebase
    Add tests for TransactionMarkerRequestCompletionHandler

commit b307e5d395afb4fafaa4546d1284b9e5bc73c146
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-04-13T07:25:35Z

    Merge pull request #161 from confluentinc/exactly-once-end-txn
    
    Exactly once end txn

----
","26/Apr/17 21:11;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2849
","28/Apr/17 09:46;githubbot;GitHub user dguy opened a pull request:

    https://github.com/apache/kafka/pull/2934

    KAFKA-5059: [Follow Up] remove broken locking. Fix handleAddPartitions

    remove broken locking. fix handleAddPartitions after complete commit/abort
    respond with CONCURRENT_TRANSACTIONS in initPid

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/dguy/kafka follow-up-tc-work

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2934.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2934
    
----
commit 4986eef8468094a809ca1334486629043ffa34f2
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-04-28T09:19:03Z

    remove broken locking. fix handleAddPartitions after complete commit/abort
    respond with CONCURRENT_TRANSACTIONS in initPid

----
","01/May/17 23:48;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2934
","31/May/17 06:44;ijuma;Marking this as resolved since all, but one task have been completed and the uncompleted task will be done in a subsequent release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Range Scan for Windowed State Stores,KAFKA-5192,13069890,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,xvrl,xvrl,xvrl,08/May/17 07:13,19/May/17 00:04,12/Jan/21 10:06,19/May/17 00:03,,,,,,,,0.11.0.0,,,streams,,,,,,1,,,,,"Windowed state stores currently do not support key range scans, even though it seems reasonable to be able to – at least in a given window – do the same operations you would do on a key-value store.",,githubbot,guozhang,tanbamboo,umesh9794@gmail.com,xvrl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-05-19 00:03:25.821,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 19 00:04:02 UTC 2017,,,,,,,"0|i3em8f:",9223372036854775807,,,,,,,,,,,,,,,,"19/May/17 00:03;guozhang;Issue resolved by pull request 3027
[https://github.com/apache/kafka/pull/3027]","19/May/17 00:04;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3027
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support adding replicas to existing topic partitions via kafka-topics tool without manually setting broker assignments,KAFKA-1313,12702444,New Feature,Reopened,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,sree2k,mrlabbe,mrlabbe,19/Mar/14 18:10,17/May/17 17:48,12/Jan/21 10:06,,0.8.0,,,,,,,,,,tools,,,,,,8,newbie++,,,,"There is currently no easy way to add replicas to an existing topic partitions.

For example, topic create-test has been created with ReplicationFactor=1: 
Topic:create-test  PartitionCount:3    ReplicationFactor:1 Configs:
    Topic: create-test Partition: 0    Leader: 1   Replicas: 1 Isr: 1
    Topic: create-test Partition: 1    Leader: 2   Replicas: 2 Isr: 2
    Topic: create-test Partition: 2    Leader: 3   Replicas: 3 Isr: 3

I would like to increase the ReplicationFactor=2 (or more) so it shows up like this instead.
Topic:create-test  PartitionCount:3    ReplicationFactor:2 Configs:
    Topic: create-test Partition: 0    Leader: 1   Replicas: 1,2 Isr: 1,2
    Topic: create-test Partition: 1    Leader: 2   Replicas: 2,3 Isr: 2,3
    Topic: create-test Partition: 2    Leader: 3   Replicas: 3,1 Isr: 3,1

Use cases for this:
- adding brokers and thus increase fault tolerance
- fixing human errors for topics created with wrong values



",,ab10anand,abdbaddude,Bingkun Guo,Denis Makarenko,donnchadh,eidi,ewencp,granders,guozhang,jeffwidman,jkreps,joestein,jozi-k,jsh1234,leoxlin,mrlabbe,mrsrinivas,nbrownus,nehanarkhede,otis,Python_Max,stevenschlansker,timvanlaer,vahid,Xaelias,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-09-04 21:59:26.708,,,false,,,,,,,,,,,,,,,,,,380783,,,Wed May 17 17:48:09 UTC 2017,,,,,,,"0|i1tlvb:",381062,,,,,,,,,,,,,,,,"04/Sep/14 21:59;guozhang;Moving to 0.9.","04/Sep/14 22:02;joestein;Don't we already have this feature with reassign partition tool? Or am I missing something? If so we can close this","04/Sep/14 22:37;guozhang;My bad. Closing now.","14/Sep/14 16:51;nehanarkhede;[~guozhang], [~charmalloc], Doing this through the partition reassignment tool is just a hack. It is more convenient to have a tool that increases the replication factor of a topic.","04/Mar/15 23:50;granders;Quoting the Kafka docs:
The first step is to hand craft the custom reassignment plan in a json file-

> cat increase-replication-factor.json
{""version"":1,
 ""partitions"":[{""topic"":""foo"",""partition"":0,""replicas"":[5,6,7]}]}

Then, use the json file with the --execute option to start the reassignment process-
> bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file increase-replication-factor.json --execute
-------

As I understand it, the pain point is ""hand crafting"" the reassignment plan. From there on out, updating replication factor is functionally identical to any other reassignment. So my proposal is to add another file type (--replication-factor-json-file) to kafka-reassign-partitions.sh --generate (and/or --rebalance per KIP 6) which allows users to specify desired replication_factor per topic/partition.

For example:
>cat update-replication-factor.json
{""version"":1,
 ""partitions"":[{""topic"":""foo"",""partition"":0,""replication_factor"":3}]}

bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 --replication-factor-json-file update-replication-factor.json --generate

The user would then feed generated output into kafka-reassign-partitions.sh --execute as before.

[~nehanarkhede], let me know if this seems reasonable/is in the ballpark of what you had in mind.","08/Mar/15 20:42;nehanarkhede;[~granders]
Actually, I thought of this more like a separate tool, e.g. kafka-increase-replication-factor.sh --topic --replication-factor 
But it may be good to get feedback from the mailing list. I'd suggest starting a KIP discussion. I see this as a short but important KIP discussion.","08/Mar/15 20:48;jkreps;This is actually in the alter topic command today, it is just that it isn't implemented so it gives an error (with a typo):
{code}
bin/kafka-topics.sh --zookeeper localhost:2181 --topic test --alter --replication-factor 2
Option ""[replication-factor]"" can't be used with option""[alter]""
{code}
So I think this ticket could just be about making that error go away and have the command do what you expect (change the replication factor). I don't personally think we need a KIP for that, but I guess given that four people had four different ideas of how to do it, maybe we do?","08/Mar/15 20:56;nehanarkhede;I missed the fact that we had this option in the topics tool. Yes, this would be the right way to expose this feature. I don't think we need a KIP for this, but maybe it could be a quick discuss thread on the mailing list?","21/Mar/16 15:51;Bingkun Guo;+1 on supporting ""replication-factor"" option in ""alter"".","20/Jun/16 10:13;abdbaddude;+1 on supporting ""replication-factor"" option in ""alter"".","16/Jan/17 03:57;ewencp;Moved out of targeted fix version since this has been bumped out of multiple major and minor releases. Also dropped priority since this is annoying, but not impossibly currently -- this would be a nice improvement, but not terribly difficult to do with current tools (and being bumped out of releases indicates it isn't actually critical).","02/May/17 18:02;stevenschlansker;I started using Kafka Streams recently.  I did not know to configure the {{replication.factor}} tuneable and so now all of my automatically generated topics have the wrong replication factor.  I tried to update via {{kafka-topics.sh}} and obviously ended up here.  I understand why this got deprioritized, but consider now that in addition to an administrator creating topics (where they have an opportunity to set replication factor right), Kafka Streams creates topics behind your back and you may not realize your replication factor is wrong until you have a lot of existing data.

I can obviously fix it up by hand as outlined above but this is a pretty big wart and seems that it should be well worth fixing.","17/May/17 17:48;Xaelias;I feel like an ""easy"" fix for this would at least be enable the {code}bin/kafka-reassign-partitions.sh --generate --topics-to-move-json-file{code} to take an optional parameter for each topic that would be rf.
That way, at least, we would have an easy way to generate assignments with higher rf.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add internal leave.group.on.close config to consumer ,KAFKA-4881,13049980,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,damianguy,damianguy,damianguy,10/Mar/17 09:19,12/May/17 08:44,12/Jan/21 10:06,27/Mar/17 17:30,,,,,,,,0.11.0.0,,,clients,,,,,,0,,,,,"In streams we need to reduce the number of rebalances as they cause expensive shuffling of state during {{onPartitionsAssigned}} and {{onPartitionsRevoked}}. To achieve this we can choose to not send leave the group when a streams consumer is closed. This means that during bounces (with appropriate session timeout settings) we will see at most one rebalance per instance bounce.

As this is an optimization that is only relevant to streams at the moment, initially we will do this by adding an internal config to the consumer {{leave.group.on.close}}, this will default to true. When it is set to false {{AbstractCoordinator}} won't send the {{LeaveGroupRequest}}",,damianguy,githubbot,guozhang,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-27 17:30:57.448,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 12 08:44:28 UTC 2017,,,,,,,"0|i3b8ev:",9223372036854775807,,,,,,,,,,,,,,,,"27/Mar/17 17:30;guozhang;Issue resolved by pull request 2650
[https://github.com/apache/kafka/pull/2650]","27/Mar/17 17:31;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2650
","11/May/17 12:08;githubbot;GitHub user dguy opened a pull request:

    https://github.com/apache/kafka/pull/3025

    KAFKA-4881: add internal.leave.group.config to consumer

    Backport from https://github.com/apache/kafka/pull/2650

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/dguy/kafka kafka-4881-bp

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3025.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3025
    
----
commit 1eb6b1af11100b406b067f8c1e1d0e99c6543836
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-03-27T17:30:38Z

    backport from trunk

----
","12/May/17 08:44;githubbot;Github user dguy closed the pull request at:

    https://github.com/apache/kafka/pull/3025
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow Kafka connect source tasks to commit offsets without messages being sent,KAFKA-5084,13064877,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,,criccomini,criccomini,18/Apr/17 20:50,18/Apr/17 21:29,12/Jan/21 10:06,18/Apr/17 21:08,0.10.2.0,,,,,,,,,,KafkaConnect,,,,,,1,,,,,"We are currently running [Debezium|http://debezium.io/] connectors in Kafka connect. These connectors consume from MySQL's binlog, and produce into Kafka.

One of the things we've observed is that some of our Debezium connectors are not honoring the {{offset.flush.interval.ms}} setting (which is set to 60 seconds). Some of our connectors seem to be committing only sporadically. For low-volume connectors, the commits seem to happen once every hour or two, and sometimes even longer.

It sounds like the issue is that Kafka connect will only commit source task offsets when the source task produces new source records. This is because Kafka connect gets the offset to commit from an incoming source record. The problem with this approach is that there are (in my opinion) valid reasons to want to commit consumed offsets WITHOUT sending any new messages. Taking Debezium as an example, there are cases where Debezium consumes messages, but filters out messages based on a regex, or filter rule (e.g. table black lists). In such a case, Debezium is consuming messages from MySQL's binlog, and dropping them before they get to the Kafka connect framework. As such, Kafka connect never sees these messages, and doesn't commit any progress. This results in several problems:

# In the event of a failure, the connector could fall WAY back, since the last committed offset might be from hours ago, even thought it *has* processed all recent messages--it just hasn't sent anything to Kafka.
# For connectors like Debezium that consume from a source that has a *limited* window to fetch messages (MySQL's binlog has time/size based retention), you can actually fall off the edge of the binlog because the last commit can actually happen farther back than the binlog goes, even though Debezium has fetched every single message in the binlog--it just hasn't produced anything due to filtering.

Again, I don't see this as a Debezium-specific issue. I could imagine a similar scenario with an [SST-based Cassandra source|https://github.com/datamountaineer/stream-reactor/issues/162].

It would be nice if Kafka connect allowed us a way to commit offsets for source tasks even when messages haven't been sent recently. This would allow source tasks to log their progress even if they're opting not to send messages to Kafka due to filtering or for some other reason.

(See https://issues.jboss.org/browse/DBZ-220 for more context.)",,criccomini,rhauch,zarzyk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-3821,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-04-18 21:01:49.389,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 18 21:06:14 UTC 2017,,,,,,,"0|i3drwf:",9223372036854775807,,,,,,,,,,,,,,,,"18/Apr/17 21:01;rhauch;A bit more background may be in order. The Debezium MySQL connector can be configured to ignore binlog events that don't meet a certain criteria, so the connector is successfully making progress in the binlog event but will only produce {{SourceRecord}} objects when the binlog events satisfy the criteria. In discussions with [~criccomini], it seems that some MySQL connectors are configured such that very few of the binlog events satisfy the criteria, and so that connector very rarely outputs a {{SourceRecord}} even though large numbers of binlog events have been successfully processed.

The challenge is that the only way for a connector to supply a source offset to Kafka Connect is by including it in a {{SourceRecord}}. However, if the connector has no need to produce a {{SourceRecord}}, the connector can't tell Kafka Connect that it has made progress and it has a new offset that should be committed (at the next appropriate time).","18/Apr/17 21:02;rhauch;Looking back, it seems that this may actually be a duplicate of (or rather an additional use case for) KAFKA-3821, entitled ""Allow Kafka Connect source tasks to produce offset without writing to topics"".","18/Apr/17 21:06;criccomini;[~rhauch], hah, this does indeed look like a dupe of KAFKA-3821. I'll close this then.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add purgeDataBefore() API in AdminClient,KAFKA-4586,13031794,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,lindong,lindong,lindong,03/Jan/17 22:27,03/Apr/17 12:08,12/Jan/21 10:06,03/Apr/17 12:08,,,,,,,,0.11.0.0,,,,,,,,,0,,,,,Please visit https://cwiki.apache.org/confluence/display/KAFKA/KIP-107%3A+Add+purgeDataBefore%28%29+API+in+AdminClient for motivation etc.,,githubbot,lindong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-01 05:11:39.566,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 28 17:00:48 UTC 2017,,,,,,,"0|i3884f:",9223372036854775807,,,,,,,,,,,,,,,,"01/Feb/17 05:11;githubbot;GitHub user lindong28 opened a pull request:

    https://github.com/apache/kafka/pull/2476

    KAFKA-4586; Add purgeDataBefore() API (KIP-107)

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/lindong28/kafka KAFKA-4586

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2476.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2476
    
----
commit 3546fcc17e9ed816dc50536e23af8d0df6369591
Author: Dong Lin <lindong28@gmail.com>
Date:   2017-01-27T17:51:06Z

    KAFKA-4586; Add purgeDataBefore() API (KIP-107)

----
","05/Mar/17 10:19;githubbot;GitHub user lindong28 opened a pull request:

    https://github.com/apache/kafka/pull/2641

    KAFKA-4586 followup; Fix testMaxPollIntervalMsDelayInRevocation test failure

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/lindong28/kafka KAFKA-4820-followup

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2641.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2641
    
----
commit c1e429098f6c1dc3cd99c4ef1c0ba2a9662d7503
Author: Dong Lin <lindong28@gmail.com>
Date:   2017-03-05T10:10:55Z

    KAFKA-4586 followup; Fix testMaxPollIntervalMsDelayInRevocation test failure

----
","28/Mar/17 17:00;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2476
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change cleanup.policy config to accept a list of valid policies,KAFKA-4015,12994524,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,damianguy,damianguy,damianguy,03/Aug/16 08:51,31/Mar/17 22:44,12/Jan/21 10:06,25/Aug/16 15:16,0.10.1.0,,,,,,,0.10.1.0,,,core,,,,,,0,,,,,"There are some use cases where it is desirable to have a topic that supports both compact and delete policies, i.e., any topic that wants to be compacted by key, but also wants keys that haven't been updated for some time to be automatically expired.

Add a new compact_and_delete option to cleanup.policy. When set, both compact and delete cleanup strategies should run. This change needs to guarantee thread-safety.",,aozeritsky,damianguy,githubbot,huxi_2b,junrao,pat4520,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-08-16 08:53:14.26,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 31 22:44:13 UTC 2017,,,,,,,"0|i31uev:",9223372036854775807,,,,,,,,,,,,,,,,"16/Aug/16 08:53;githubbot;GitHub user dguy opened a pull request:

    https://github.com/apache/kafka/pull/1742

    KAFKA-4015: Add new cleanup.policy, compact_and_delete

    Added compact_and_delete cleanup.policy to LogConfig.
    Updated LogCleaner.CleanerThread to also run deletion for any topics configured with compact_and_delete.
    Ensure Log.deleteSegments only runs when delete is enabled.
    Additional Integration and unit tests to cover new option

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/dguy/kafka kafka-4015

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1742.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1742
    
----
commit 889f5b8cc763cd488567c6034d8c25be28596ee1
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-05T10:27:12Z

    enable cleanup.policy=compact_delete

----
","25/Aug/16 15:16;junrao;Issue resolved by pull request 1742
[https://github.com/apache/kafka/pull/1742]","25/Aug/16 15:16;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1742
","16/Dec/16 08:18;huxi_2b;[~damianguy]    I did not find any occurrence in the doc or source code for 'compact_and_delete' policy? Is it really  added into the 0.10.1 codebase?","31/Mar/17 22:44;pat4520;FYI, 'compact,delete' works with 0.10.2.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
run integration tests separate from unit tests,KAFKA-4909,13056636,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,damianguy,damianguy,damianguy,16/Mar/17 12:11,16/Mar/17 13:00,12/Jan/21 10:06,16/Mar/17 13:00,,,,,,,,,,,,,,,,,0,,,,,,,damianguy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-03-16 12:11:06.0,,,,,,,"0|i3cd2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add KTable/GlobalKTable Join to Streams,KAFKA-4880,13049972,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,damianguy,damianguy,damianguy,10/Mar/17 08:49,10/Mar/17 11:55,12/Jan/21 10:06,10/Mar/17 11:55,,,,,,,,,,,,,,,,,0,,,,,Outstanding work left to be done for https://cwiki.apache.org/confluence/display/KAFKA/KIP-99%3A+Add+Global+Tables+to+Kafka+Streams,,damianguy,miguno,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-10 08:52:34.007,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 10 08:52:34 UTC 2017,,,,,,,"0|i3b8d3:",9223372036854775807,,,,,,,,,,,,,,,,"10/Mar/17 08:52;miguno;FWIW, I wonder how much interest there actually is for this functionality.  Users have been requesting stream-globalTable joins, but personally I have yet to run into a person that wants table-globalTable joins.  Just saying.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka REST API,KAFKA-3294,12944769,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,omkreddy,sriharsha,sriharsha,26/Feb/16 04:48,10/Mar/17 09:44,12/Jan/21 10:06,03/Jan/17 23:10,,,,,,,,,,,,,,,,,0,,,,,"This JIRA is to build Kafka REST API for producer, consumer and also any administrative tasks such as create topic, delete topic. We do have lot of kafka client api support in different languages but having REST API for producer and consumer will make it easier for users to read or write Kafka. Also having administrative API will help in managing a cluster or building administrative dashboards.",,chienle,crazyjvm,ewencp,krisden,lindong,mgharat,noslowerdna,parth.brahmbhatt,sriharsha,sureshms,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-639,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-26 16:51:28.799,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 03 23:10:20 UTC 2017,,,,,,,"0|i2tslj:",9223372036854775807,,,,,,,,,,,,,,,,"26/Apr/16 16:51;mgharat;Are you thinking of exposing a rest endpoint on the broker itself?
","26/Apr/16 17:26;sriharsha;[~mgharat] we are actually building an external component that fronts the kafka brokers and uses produces api to push data coming from REST calls.","26/Apr/16 20:24;chienle;Might want to look into http://docs.confluent.io/2.0.1/","26/Apr/16 20:48;mgharat;[~sriharsha.sm] so this will be integrated with kafka brokers right? So I am trying to understand is that this will be something like mirror maker?","26/Apr/16 23:57;sriharsha;[~chienle] I am familiar with it. I want this to be part of Apache Kafka not in a external github project.

[~mgharat] Its an external netty server with REST API endpoints exposed where clients can send messages , internally it will use producers to send that data to brokers. Yes similar to mirror maker. What are your thoughts on integrating within kafka brokers?","28/Apr/16 05:26;parth.brahmbhatt;Here is a version we are working on https://github.com/Parth-Brahmbhatt/kafka-rest. Its still work in progress but Its better to start the discussion around getting more clarity on requirements, how we want to package and distribute it and what does the community need from this feature.","28/Apr/16 17:37;lindong;[~harsha_ch] [~parth.brahmbhatt] This seems to be a big addition to Kafka. Should we first have a KIP that documents the design for discussion on the mailing list? Many companies, like LinkedIn and Confluent, have their own Rest server implementation, probably with some difference in the API. Developers/users from these companies probably need to discuss and agree on the API for this Rest server, right?","28/Apr/16 18:01;sriharsha;[~lindong] we just shared the initial work we have. We are writing a kip and will post it in mailing list once its ready.","27/Sep/16 15:26;sriharsha;[~lindong] [~mgharat]  Here is KIP draft [~omkreddy] put together. It will be great if you can go over it. ","03/Jan/17 23:10;ewencp;Closing this for now as the KIP discussion ended with more folks not in favor of doing this. We can always reopen if it gets more community traction in the future.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support direct ByteBuffer serializers/deserializers in clients,KAFKA-4802,13046268,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,mattsicker,mattsicker,25/Feb/17 07:16,04/Mar/17 20:13,12/Jan/21 10:06,,,,,,,,,,,,clients,,,,,,1,,,,,"RecordAccumulator and Fetcher are already written to take advantage of a pool of ByteBuffers, but Serializer and Deserializer require you to return a byte array. If I have a key or value format that is better handled directly via ByteBuffer, the added conversion to a byte array introduces unnecessary garbage.

An example use case would be in enhancing the KafkaAppender in Log4j 2 to support garbage free logging (or minimal garbage; I haven't really looked at the entire code path).",,mattsicker,mikeyg,original-brownbear,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-04 20:13:38.755,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 04 20:13:38 UTC 2017,,,,,,,"0|i3amf3:",9223372036854775807,,,,,,,,,,,,,,,,"03/Mar/17 16:26;mattsicker;We also found a related issue in the inability to reuse ProducerRecord objects.","04/Mar/17 20:13;original-brownbear;I think this is a duplicate (or sub-issue rather) of https://issues.apache.org/jira/browse/KAFKA-2045 . Making this possible will require a relatively large change as you can see if you look into the discussion there.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tool for performance and correctness of transactions end-to-end,KAFKA-1569,12731814,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Abandoned,raulcf,raulcf,raulcf,04/Aug/14 18:30,28/Feb/17 05:47,12/Jan/21 10:06,28/Feb/17 05:47,,,,,,,,,,,,,,,,,0,transactions,,,,"A producer tool that creates an input file, reads it and sends it to the brokers according to some transaction configuration. And a consumer tool that read data from brokers with transaction boundaries and writes it to a file.",,hachikuji,raulcf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/14 21:58;raulcf;KAFKA-1569.patch;https://issues.apache.org/jira/secure/attachment/12659741/KAFKA-1569.patch","04/Aug/14 19:56;raulcf;KAFKA-1569.patch;https://issues.apache.org/jira/secure/attachment/12659711/KAFKA-1569.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2017-02-28 05:47:53.736,,,false,,,,,,,,,,,,,,,,,,409843,,,Tue Feb 28 05:47:53 UTC 2017,,,,,,,"0|i1yikf:",409838,,,,,,,,,,,,,,,,"04/Aug/14 19:56;raulcf;Created reviewboard https://reviews.apache.org/r/24255/diff/
 against branch origin/transactional_messaging","04/Aug/14 21:59;raulcf;Created reviewboard https://reviews.apache.org/r/24268/diff/
 against branch origin/transactional_messaging","28/Feb/17 05:47;hachikuji;This work has been superseded by KIP-98: https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Add transactional request definitions to clients package,KAFKA-1541,12727711,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Abandoned,raulcf,jjkoshy,jjkoshy,16/Jul/14 17:02,28/Feb/17 05:46,12/Jan/21 10:06,28/Feb/17 05:46,,,,,,,,,,,,,,,,,0,transactions,,,,Separate jira for this since KAFKA-1522 only adds definitions to the core package.,,hachikuji,jjkoshy,raulcf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/14 19:56;raulcf;KAFKA-1541.patch;https://issues.apache.org/jira/secure/attachment/12660210/KAFKA-1541.patch","04/Aug/14 21:51;raulcf;KAFKA-1541.patch;https://issues.apache.org/jira/secure/attachment/12659736/KAFKA-1541.patch","04/Aug/14 19:43;raulcf;KAFKA-1541.patch;https://issues.apache.org/jira/secure/attachment/12659708/KAFKA-1541.patch","17/Jul/14 17:27;raulcf;KAFKA-1541.patch;https://issues.apache.org/jira/secure/attachment/12656297/KAFKA-1541.patch",,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2014-07-17 17:27:45.209,,,false,,,,,,,,,,,,,,,,,,405816,,,Tue Feb 28 05:46:59 UTC 2017,,,,,,,"0|i1xu3z:",405837,,,,,,,,,,,,,,,,"16/Jul/14 17:35;jjkoshy;Same here: https://cwiki.apache.org/confluence/display/KAFKA/Patch+submission+and+review#Patchsubmissionandreview-Kafkapatchreviewtool","17/Jul/14 17:27;raulcf;Created reviewboard https://reviews.apache.org/r/23646/diff/
 against branch origin/transactional_messaging","04/Aug/14 19:43;raulcf;Created reviewboard https://reviews.apache.org/r/24253/diff/
 against branch origin/transactional_messaging","04/Aug/14 21:51;raulcf;Created reviewboard https://reviews.apache.org/r/24267/diff/
 against branch origin/transactional_messaging","06/Aug/14 19:57;raulcf;Created reviewboard https://reviews.apache.org/r/24417/diff/
 against branch origin/transactional_messaging","28/Feb/17 05:46;hachikuji;This work has been superseded by KIP-98: https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleConsumer should be transaction-aware,KAFKA-1527,12725504,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Abandoned,raulcf,jjkoshy,jjkoshy,04/Jul/14 15:35,28/Feb/17 05:46,12/Jan/21 10:06,28/Feb/17 05:46,,,,,,,,,,,,,,,,,0,transactions,,,,"This will help in further integration testing of the transactional producer. This could be implemented in the consumer-iterator level or at a higher level.
",,hachikuji,jjkoshy,raulcf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/14 23:23;raulcf;KAFKA-1527.patch;https://issues.apache.org/jira/secure/attachment/12657714/KAFKA-1527.patch","19/Aug/14 17:39;raulcf;KAFKA-1527_2014-08-19_10:39:53.patch;https://issues.apache.org/jira/secure/attachment/12662790/KAFKA-1527_2014-08-19_10%3A39%3A53.patch","20/Aug/14 01:22;raulcf;KAFKA-1527_2014-08-19_18:22:26.patch;https://issues.apache.org/jira/secure/attachment/12662943/KAFKA-1527_2014-08-19_18%3A22%3A26.patch",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2014-07-24 23:23:34.429,,,false,,,,,,,,,,,,,,,,,,403663,,,Tue Feb 28 05:46:34 UTC 2017,,,,,,,"0|i1xh2n:",403706,,,,,,,,,,,,,,,,"24/Jul/14 23:23;raulcf;Created reviewboard https://reviews.apache.org/r/23906/diff/
 against branch origin/transactional_messaging","19/Aug/14 17:39;raulcf;Updated reviewboard https://reviews.apache.org/r/23906/diff/
 against branch origin/transactional_messaging","20/Aug/14 01:22;raulcf;Updated reviewboard https://reviews.apache.org/r/23906/diff/
 against branch origin/transactional_messaging","28/Feb/17 05:46;hachikuji;This work has been superseded by KIP-98: https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Producer performance tool should have an option to enable transactions,KAFKA-1526,12725503,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Abandoned,raulcf,jjkoshy,jjkoshy,04/Jul/14 15:33,28/Feb/17 05:46,12/Jan/21 10:06,28/Feb/17 05:46,,,,,,,,,,,,,,,,,0,transactions,,,,If this flag is enabled the producer could start/commit/abort transactions randomly - we could add more configs/parameters for more control on transaction boundaries.,,hachikuji,jjkoshy,raulcf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/14 17:35;raulcf;KAFKA-1526.patch;https://issues.apache.org/jira/secure/attachment/12656301/KAFKA-1526.patch","19/Aug/14 17:54;raulcf;KAFKA-1526_2014-08-19_10:54:51.patch;https://issues.apache.org/jira/secure/attachment/12662797/KAFKA-1526_2014-08-19_10%3A54%3A51.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2014-07-17 17:35:58.5,,,false,,,,,,,,,,,,,,,,,,403662,,,Tue Feb 28 05:46:05 UTC 2017,,,,,,,"0|i1xh2f:",403705,,,,,,,,,,,,,,,,"17/Jul/14 17:35;raulcf;Created reviewboard https://reviews.apache.org/r/23647/diff/
 against branch origin/transactional_messaging","19/Aug/14 17:54;raulcf;Updated reviewboard https://reviews.apache.org/r/23647/diff/
 against branch origin/transactional_messaging","28/Feb/17 05:46;hachikuji;This work has been superseded by KIP-98: https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DumpLogSegments should print transaction IDs,KAFKA-1525,12725502,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Abandoned,lindong,jjkoshy,jjkoshy,04/Jul/14 15:30,28/Feb/17 05:45,12/Jan/21 10:06,28/Feb/17 05:45,,,,,,,,,,,,,,,,,0,transactions,,,,"This will help in some very basic integration testing of the transactional producer and brokers (i.e., until we have a transactional simple consumer).

We only need to print the txid's. There is no need to do transactional buffering.",,hachikuji,jjkoshy,lindong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/14 21:30;lindong;KAFKA-1525.patch;https://issues.apache.org/jira/secure/attachment/12656141/KAFKA-1525.patch","22/Jul/14 23:48;lindong;KAFKA-1525_2014-07-22_16:48:45.patch;https://issues.apache.org/jira/secure/attachment/12657225/KAFKA-1525_2014-07-22_16%3A48%3A45.patch","15/Aug/14 18:49;lindong;KAFKA-1525_2014-08-15_11:49:25.patch;https://issues.apache.org/jira/secure/attachment/12662122/KAFKA-1525_2014-08-15_11%3A49%3A25.patch",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2014-07-16 21:30:29.231,,,false,,,,,,,,,,,,,,,,,,403661,,,Tue Feb 28 05:45:44 UTC 2017,,,,,,,"0|i1xh27:",403704,,,,,,,,,,,,,,,,"16/Jul/14 21:30;lindong;Created reviewboard https://reviews.apache.org/r/23569/diff/
 against branch origin/transactional_messaging","22/Jul/14 23:49;lindong;Updated reviewboard https://reviews.apache.org/r/23569/diff/
 against branch origin/transactional_messaging","15/Aug/14 18:49;lindong;Updated reviewboard https://reviews.apache.org/r/23569/diff/
 against branch origin/transactional_messaging","28/Feb/17 05:45;hachikuji;This work has been superseded by KIP-98: https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement transactional producer,KAFKA-1524,12725500,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Abandoned,raulcf,jjkoshy,jjkoshy,04/Jul/14 15:27,28/Feb/17 05:45,12/Jan/21 10:06,28/Feb/17 05:45,,,,,,,,,,,,,,,,,1,transactions,,,,"Implement the basic transactional producer functionality as outlined in https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka

The scope of this jira is basic functionality (i.e., to be able to begin and commit or abort a transaction) without the failure scenarios.",,aartigupta,cjolif@apache.org,hachikuji,jjkoshy,kzadorozhny,noslowerdna,raulcf,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/14 19:53;raulcf;KAFKA-1524.patch;https://issues.apache.org/jira/secure/attachment/12660209/KAFKA-1524.patch","04/Aug/14 21:42;raulcf;KAFKA-1524.patch;https://issues.apache.org/jira/secure/attachment/12659732/KAFKA-1524.patch","17/Jul/14 17:38;raulcf;KAFKA-1524.patch;https://issues.apache.org/jira/secure/attachment/12656303/KAFKA-1524.patch","18/Aug/14 16:39;raulcf;KAFKA-1524_2014-08-18_09:39:34.patch;https://issues.apache.org/jira/secure/attachment/12662511/KAFKA-1524_2014-08-18_09%3A39%3A34.patch","20/Aug/14 16:15;raulcf;KAFKA-1524_2014-08-20_09:14:59.patch;https://issues.apache.org/jira/secure/attachment/12663142/KAFKA-1524_2014-08-20_09%3A14%3A59.patch",,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2014-07-17 17:38:02.861,,,false,,,,,,,,,,,,,,,,,,403659,,,Tue Feb 28 05:45:10 UTC 2017,,,,,,,"0|i1xh1r:",403702,,,,,,,,,,,,,,,,"16/Jul/14 17:34;jjkoshy;Can you use the patch review tool to submit?
https://cwiki.apache.org/confluence/display/KAFKA/Patch+submission+and+review#Patchsubmissionandreview-Kafkapatchreviewtool","17/Jul/14 17:38;raulcf;Created reviewboard https://reviews.apache.org/r/23648/diff/
 against branch origin/transactional_messaging","04/Aug/14 18:41;raulcf;Created reviewboard https://reviews.apache.org/r/24245/diff/
 against branch origin/transactional_messaging","04/Aug/14 21:44;raulcf;Created reviewboard https://reviews.apache.org/r/24265/diff/
 against branch origin/transactional_messaging","06/Aug/14 19:53;raulcf;Created reviewboard https://reviews.apache.org/r/24411/diff/
 against branch origin/transactional_messaging","08/Aug/14 22:56;jjkoshy;[~raulcf] will take a look at the updated patch. In future, would you mind updating the original RB instead of creating a new one? That makes it easier to do incremental reviews.","08/Aug/14 22:57;jjkoshy;BTW, that is described here:

https://cwiki.apache.org/confluence/display/KAFKA/Patch+submission+and+review#Patchsubmissionandreview-4.Updatepatch","18/Aug/14 16:39;raulcf;Updated reviewboard https://reviews.apache.org/r/24411/diff/
 against branch origin/transactional_messaging","20/Aug/14 16:15;raulcf;Updated reviewboard https://reviews.apache.org/r/24411/diff/
 against branch origin/transactional_messaging","11/Jul/16 12:51;cjolif@apache.org;Looks like it was never move forward, out of curiosity is there any actual interest in this and this is just lack of time or this is really not something in scope at all?","11/Jul/16 22:26;sriramsub;We hope to provide an update on this soon.","19/Feb/17 18:14;noslowerdna;Can this Jira be closed as obsolete? It appears to have been superseded by the design in https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging

Same comment applies to several other open Jiras with the ""transactions"" label.","28/Feb/17 05:45;hachikuji;This work has been superseded by KIP-98: https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement transaction manager module,KAFKA-1523,12725492,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Abandoned,lindong,jjkoshy,jjkoshy,04/Jul/14 13:58,28/Feb/17 05:44,12/Jan/21 10:06,28/Feb/17 05:44,,,,,,,,,,,,,,,,,1,transactions,,,,"* Entry point for transaction requests
* Appends transaction control records to the transaction journal
* Sends transaction control records to data brokers
* Responsible for expiring transactions
* Supports fail-over: for which it needs to maintain a transaction HW which is the offset of the BEGIN control record of the earliest pending transaction. It should checkpoint the HW periodically either to ZK/separate topic/offset commit.


We merge KAFKA-1565 transaction manager failover handling into this JIRA. Transaction manager should guarantee that, once a pre-commit/pre-abort request is acknowledged, commit/abort request will be delivered to partitions involved in the transaction.

This patch handles the following failover scenarios:
1) Transaction manager or its followers fail before txRequest is duplicated on local log and followers.
Solution: Transaction manager responds to request with error status. The producer keeps trying to commit.
2) The txPartition’s leader is not available.
Solution: Put txRequest on unSentTxRequestQueue. When metadataCache is updated, check and re-send txRequest from unSentTxRequestQueue if possible.
3) The txPartition’s leader fails when txRequest is in channel manager.
Solution: Retrieve all txRequests queued for transmission to this broker and put them on unSentTxRequestQueue.
4) Transaction manage does not receive success response from txPartition’s leaders within timeout period.
Solution: Transaction manager expires the txRequest and re-send it.
5) Transaction manager fails.
Solution: The new transaction manager reads transactionHW from zookeeper, and sends txRequest starting from the transactionHW.


This patch does not provide the following feature. These will be provided in separate patches.
1) Producer offset commit.
2) Transaction expiration.



",,BrentDouglas,hachikuji,jjkoshy,lindong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/14 03:13;lindong;KAFKA-1523_2014-07-17_20:12:55.patch;https://issues.apache.org/jira/secure/attachment/12656435/KAFKA-1523_2014-07-17_20%3A12%3A55.patch","22/Jul/14 23:45;lindong;KAFKA-1523_2014-07-22_16:45:42.patch;https://issues.apache.org/jira/secure/attachment/12657224/KAFKA-1523_2014-07-22_16%3A45%3A42.patch","06/Aug/14 04:27;lindong;KAFKA-1523_2014-08-05_21:25:55.patch;https://issues.apache.org/jira/secure/attachment/12660048/KAFKA-1523_2014-08-05_21%3A25%3A55.patch","09/Aug/14 04:37;lindong;KAFKA-1523_2014-08-08_21:36:52.patch;https://issues.apache.org/jira/secure/attachment/12660799/KAFKA-1523_2014-08-08_21%3A36%3A52.patch",,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2014-07-16 21:29:53.517,,,false,,,,,,,,,,,,,,,,,,403651,,,Tue Feb 28 05:44:22 UTC 2017,,,,,,,"0|i1xgzz:",403694,,,,,,,,,,,,,,,,"16/Jul/14 21:29;lindong;Created reviewboard https://reviews.apache.org/r/23568/diff/
 against branch origin/transactional_messaging","18/Jul/14 02:26;lindong;Updated reviewboard https://reviews.apache.org/r/23568/diff/
 against branch origin/transactional_messaging","18/Jul/14 03:01;lindong;Updated reviewboard https://reviews.apache.org/r/23568/diff/
 against branch origin/transactional_messaging","18/Jul/14 03:13;lindong;Updated reviewboard https://reviews.apache.org/r/23568/diff/
 against branch origin/transactional_messaging","22/Jul/14 23:46;lindong;Updated reviewboard https://reviews.apache.org/r/23568/diff/
 against branch origin/transactional_messaging","06/Aug/14 04:27;lindong;Updated reviewboard https://reviews.apache.org/r/23568/diff/
 against branch origin/transactional_messaging","09/Aug/14 04:37;lindong;Updated reviewboard https://reviews.apache.org/r/23568/diff/
 against branch origin/transactional_messaging","28/Feb/17 05:44;hachikuji;This work has been superseded by KIP-98: https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add KStream.peek(ForeachAction<K,V>)",KAFKA-4720,13039556,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,stevenschlansker,stevenschlansker,stevenschlansker,01/Feb/17 19:00,28/Feb/17 03:03,12/Jan/21 10:06,16/Feb/17 05:19,0.10.1.1,,,,,,,0.11.0.0,,,streams,,,,,,0,needs-kip,,,,"Java's Stream provides a handy peek method that observes elements in the stream without transforming or filtering them.  While you can emulate this functionality with either a filter or map, peek provides potentially useful semantic information (doesn't modify the stream) and is much more concise.

Example usage: using Dropwizard Metrics to provide event counters

{code}
KStream<Integer, String> s = ...;
s.map(this::mungeData)
 .peek((i, s) -> metrics.noteMungedEvent(i, s))
 .filter(this::hadProcessingError)
 .print();
{code}
",,cpennello_opentable,githubbot,guozhang,mjsax,stevenschlansker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-4772,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-03 20:49:22.85,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 16 05:25:01 UTC 2017,,,,,,,"0|i39hbr:",9223372036854775807,,,,,,,,,,,,,,,,"03/Feb/17 20:49;githubbot;GitHub user stevenschlansker opened a pull request:

    https://github.com/apache/kafka/pull/2493

    KAFKA-4720: add a KStream#peek(ForeachAction<K, V>)

    https://issues.apache.org/jira/browse/KAFKA-4720
    
    Peek is a handy method to have to insert diagnostics that do not affect the stream itself, but some external state such as logging or metrics collection.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/stevenschlansker/kafka kafka-4720-peek

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2493.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2493
    
----
commit 3f06ee010028335674f1a9e21c0fa740e3d5a950
Author: Steven Schlansker <sschlansker@opentable.com>
Date:   2017-02-03T20:46:43Z

    KAFKA-4720: add a KStream#peek(ForeachAction<K, V>)

----
","16/Feb/17 05:19;guozhang;Issue resolved by pull request 2493
[https://github.com/apache/kafka/pull/2493]","16/Feb/17 05:20;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2493
","16/Feb/17 05:25;guozhang;[~stevenschlansker] I have added you to contributor list, you can assign JIRAs to yourself in the future.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pluggable JAAS LoginModule configuration for SSL (KIP-127),KAFKA-4784,13044931,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,cshannon,cshannon,cshannon,21/Feb/17 19:11,26/Feb/17 15:02,12/Jan/21 10:06,26/Feb/17 15:02,,,,,,,,,,,security,,,,,,0,,,,,"Allow a custom JAAS LoginModule to be configured when using the SSL channel.

See https://cwiki.apache.org/confluence/display/KAFKA/KIP-127%3A+Pluggable+JAAS+LoginModule+configuration+for+SSL for more details",,brianjohnson,cshannon,githubbot,zgl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-25 22:19:07.346,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 26 15:02:58 UTC 2017,,,,,,,"0|i3ae7b:",9223372036854775807,,,,,,,,,,,,,,,,"25/Feb/17 22:19;githubbot;GitHub user ewencp opened a pull request:

    https://github.com/apache/kafka/pull/2598

    KAFKA-4784: Add ByteArrayConverter (KIP-128)

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ewencp/kafka kafka-4784-byte-array-converter

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2598.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2598
    
----
commit 2bab809a27391bdc6154ab6757e5a89064ac3c6c
Author: Ewen Cheslack-Postava <me@ewencp.org>
Date:   2017-02-25T22:18:28Z

    KAFKA-4784: Add ByteArrayConverter (KIP-128)

----
","25/Feb/17 22:20;githubbot;Github user ewencp closed the pull request at:

    https://github.com/apache/kafka/pull/2598
","26/Feb/17 15:02;cshannon;Per feedback, a custom PrincipalBuilder can be implemented which allows access to the X509 certificates and will provide similar functionality but is simpler.  This can be revisited later if this proves to be insufficient.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support session windows,KAFKA-3452,12952936,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,damianguy,guozhang,guozhang,23/Mar/16 18:20,03/Feb/17 19:13,12/Jan/21 10:06,06/Jan/17 18:12,,,,,,,,0.10.2.0,,,streams,,,,,,0,api,kip,,,"The Streams DSL currently does not provide session window as in the DataFlow model. We have seen some common use cases for this feature and it's better adding this support asap.

https://cwiki.apache.org/confluence/display/KAFKA/KIP-94+Session+Windows",,githubbot,guozhang,joshng,mjsax,paulrbrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-11-24 16:48:26.314,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 03 19:13:43 UTC 2017,,,,,,,"0|i2v40v:",9223372036854775807,,,,,,,,,,,,,,,,"24/Nov/16 16:48;githubbot;GitHub user dguy opened a pull request:

    https://github.com/apache/kafka/pull/2166

    KAFKA-3452: Support session windows

    Add support for SessionWindows based on design detailed in https://cwiki.apache.org/confluence/display/KAFKA/KIP-94+Session+Windows.
    This includes refactoring of the RocksDBWindowStore such that functionality common with the RocksDBSessionStore isn't duplicated.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/dguy/kafka kafka-3452-session-merge

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2166.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2166
    
----
commit 1876df6154012a13a5e83aff707143a5ebe1ab5b
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-10-05T08:29:59Z

    session windows

----
","06/Jan/17 18:12;guozhang;Issue resolved by pull request 2166
[https://github.com/apache/kafka/pull/2166]","06/Jan/17 18:13;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2166
","09/Jan/17 00:50;githubbot;GitHub user guozhangwang opened a pull request:

    https://github.com/apache/kafka/pull/2333

    KAFKA-3452 Follow-up: Refactoring StateStore hierarchies

    This is a refactoring follow-up of https://github.com/apache/kafka/pull/2166.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/guozhangwang/kafka K3452-followup-state-store-refactor

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2333.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2333
    
----
commit 575226cbbf84ae52a1962e338ae5ab0108dce2eb
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-01-09T00:48:26Z

    a first pass of the refactoring

----
","11/Jan/17 01:10;githubbot;GitHub user mjsax opened a pull request:

    https://github.com/apache/kafka/pull/2342

    KAFKA-3452: follow-up -- introduce SesssionWindows

     - TimeWindows represent half-open time intervals while SessionWindows represent closed time intervals

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/mjsax/kafka kafka-3452-session-window-follow-up

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2342.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2342
    
----
commit ac894ef83a3e937a06bfc3cc33febcf0dbdb7aa9
Author: Matthias J. Sax <matthias@confluent.io>
Date:   2017-01-11T01:06:53Z

    KAFKA-3452: follow-up -- introduce SesssionWindows
     - TimeWindows represent half-open time intervals while SessionWindows represent closed time intervals

----
","12/Jan/17 04:34;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2342
","16/Jan/17 11:58;githubbot;Github user dguy closed the pull request at:

    https://github.com/apache/kafka/pull/2359
","17/Jan/17 22:14;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2360
","03/Feb/17 19:13;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2333
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ability to create a shadow consumer group,KAFKA-1088,12673944,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,nehanarkhede,noslowerdna,noslowerdna,15/Oct/13 18:02,24/Jan/17 05:18,12/Jan/21 10:06,,,,,,,,,,,,consumer,,,,,,0,,,,,"I have a consumer group API request that hopefully can be included in the client rewrite [1] project: the ability to create a new consumer group that is initialized to the latest topic/partition offsets of another existing group. Our use case is being able to inspect an active group's unprocessed messages in a non-invasive manner from an admin or troubleshooting perspective. This shadow group would be short-lived and given a randomly generated  name. It's nice to see that we'll be able to designate it as ephemeral as well so that it is automatically cleaned up.

To obtain this message browsing functionality today with 0.8, we programmatically copy the group ZK paths prior to activating the ""spy"" group so that it only sees the unprocessed messages of the target. Obviously not ideal, but it worked well as a quick hack since we understand how Kafka writes and reads the offset data in ZK.

[1] https://cwiki.apache.org/confluence/display/KAFKA/Client+Rewrite",,donnchadh,jeffwidman,noslowerdna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,353567,,,2013-10-15 18:02:34.0,,,,,,,"0|i1oyi7:",353859,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Let SourceConnector implementations access the offset reader,KAFKA-3813,12977338,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,rhauch,rhauch,09/Jun/16 18:47,20/Jan/17 19:12,12/Jan/21 10:06,,0.10.0.0,,,,,,,,,,KafkaConnect,,,,,,1,needs-kip,,,,"When a source connector is started, having access to the {{OffsetStorageReader}} would allow it to more intelligently configure its tasks based upon the stored offsets. (Currently only the {{SourceTask}} implementations can access the offset reader (via the {{SourceTaskContext}}), but the {{SourceConnector}} does not have access to the offset reader.)

Of course, accessing the stored offsets is not likely to be useful for sink connectors.",,kyle@kylekuypers.com,rhauch,shikhar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 09 18:53:18 UTC 2016,,,,,,,"0|i2z8pr:",9223372036854775807,,,,,,,,,,,,,,,,"09/Jun/16 18:53;rhauch;The current API defines {{ConnectorContext}} interface and uses it when initializing {{SourceConnector}} and {{SinkConnector}} instances, and there is no specialization of {{ConnectorContext}}.

There seem to be (at least) two approaches to implementing this improvement:

# Add {{SourceConnectorContext}} and {{SinkConnectorContext}} specializations of the {{ConnectorContext}}, add a {{offsetStorageReader()}} method to {{SourceConnectorContext}}, and change the rest of the Kafka Connect framework to use these. This would likely be a breaking change, but it would distinguish between the contexts for source and sink connectors, allowing the contexts to evolve differently for different kinds of connectors.
# Add {{offsetStorageReader()}} to {{ConnectorContext}}. This does add a method and would be byte-code compatible, and thus would be far less invasive and more straightforward.

Thoughts?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Added WaitForReplaction admin tool.,KAFKA-1300,12699516,New Feature,Patch Available,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,brenden,brenden,07/Mar/14 21:54,17/Jan/17 13:11,12/Jan/21 10:06,,0.8.0,,,,,,,,,,tools,,,,,,1,patch,,,,"I have created a tool similar to the broker shutdown tool for doing rolling restarts of Kafka clusters.

The tool watches the max replica lag of the specified broker, and waits until the lag drops to 0 before exiting.

To do a rolling restart, here's the process we use:

for (broker <- brokers) {
  run shutdown tool for broker
  terminate broker
  start new broker
  run wait for replication tool on new broker
}

Here's an example command line use:

./kafka-run-class.sh kafka.admin.WaitForReplication --zookeeper zk.host.com:2181 --num.retries 100 --retry.interval.ms 60000 --broker 0",Ubuntu 12.04,alexismidon,brenden,donnchadh,guozhang,ijuma,jjkoshy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/14 21:56;brenden;0001-Added-WaitForReplaction-admin-tool.patch;https://issues.apache.org/jira/secure/attachment/12633469/0001-Added-WaitForReplaction-admin-tool.patch",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2014-04-30 19:03:41.128,,,false,,,,,,,,,,,,,,,,,,377863,,,Tue Jan 17 13:11:32 UTC 2017,,,,,,,"0|i1t3zj:",378155,,,,,,,,,,,,,,,,"29/Apr/14 22:43;brenden;Bump!

Anyone interested in this?  Presumably this would be valuable to others.","30/Apr/14 19:03;jjkoshy;Is this needed given that controlled shutdown is inbuilt into the broker? The retry counts and retry intervals are also configurable.","30/Apr/14 19:12;brenden;This tool is orthogonal to the controlled shutdown tool.  This is to help ensure that, once a broker comes online, it is in a fully replicated state.","30/Apr/14 20:27;jjkoshy;Understood, but the primary use case would be to proceed to do a controlled
shutdown of the next broker in the shutdown plan. However, with retries and
a large enough retry interval that is not needed. (E.g., you can set a very
large number of retries.)

The documentation recommends closely monitoring under-replicated-partition
counts across the cluster (and alert if it is anything other than zero).
i.e., ensuring brokers are in a fully replicated state is a ""best-practice""
for operations and should be 24/7 (not just during bounces).

","11/Jul/14 23:45;alexismidon;Consiering that Kafka is designed to handle some replication lag, if you need to shutdown a broker it does not seem very useful to wait for the replica lag to be zero.
(If the broker is X messages behind, and my maintenance requires Y=F(message throughput) minutes, I can safely shutdown the broker is X+Y/throughput < replica.lag.max.messages.

So maybe that command will be more useful if it could take an argument that characterize X, i.e. how far behind can the broker be before a shutdown.","04/Sep/14 21:45;guozhang;Moving out of 0.8.2 as for now.","17/Jan/17 13:11;ijuma;No conclusion to the discussion, so removing the fix version for now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add http metrics reporter,KAFKA-3736,12971442,New Feature,Patch Available,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,amuraru,amuraru,20/May/16 07:49,16/Jan/17 19:07,12/Jan/21 10:06,,,,,,,,,,,,core,,,,,,0,,,,,The current builtin JMX metrics reporter is pretty heavy in terms of load and collection. A new http lightweight reporter is proposed to expose the metrics via a local http port.,,amuraru,benstopford,boniek,fekelund,githubbot,gwenshap,ijuma,jthakrar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-20 08:30:28.202,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 24 23:42:26 UTC 2016,,,,,,,"0|i2y9gn:",9223372036854775807,,,,,,,,,,,,,,,,"20/May/16 08:30;githubbot;GitHub user amuraru opened a pull request:

    https://github.com/apache/kafka/pull/1412

    KAFKA-3736: Add HTTP Metrics reporter

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/hstack/kafka KAFKA-3736

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1412.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1412
    
----
commit 23eccd28156b08e4843ce97f3e64354c804a83c1
Author: Adrian Muraru <amuraru@adobe.com>
Date:   2016-05-19T20:02:18Z

    KAFKA-3736: Add HTTP Metrics reporter

----
","20/May/16 12:07;benstopford;Nice little idea! I think it might require a KIP as technically it's a public interface. [~ijuma] can confirm.

https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals","20/May/16 12:17;ijuma;Yes, we need a KIP indeed. It also adds new dependencies to the core jar, which we may want to avoid (we could introduce a new module perhaps).","20/May/16 13:05;amuraru;Agree - let me see if I can extract this to a {{-metrics-reporter}} module.","20/May/16 13:06;amuraru;I can add a KIP no problem but I was wondering if this is really a new feature given the fact the support is already there and there are two already reporters availbale:
JMX and CSV.","20/May/16 13:09;benstopford;I think it's the ""Any change that impacts the public interfaces of the project"" part that's triggering inclusion here. ","20/May/16 13:35;amuraru;But is this change changing the public interfaces? ","23/May/16 07:55;amuraru;Ok, just noticed that ""Monitoring"" is considered public facing change so I'm going to create a KIP.
For some reason I cannot add a new page on wiki here:
https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals
Do I need special perms?
Thanks a lot.","23/May/16 08:12;ijuma;[~amuraru], what is your Apache Wiki id? I can give you the required permissions.","24/May/16 23:42;gwenshap;Any reason not to leave this as a github project (like connectors, non-java clients, tons of admin tools, etc) and put it inside Kafka?

I hope the KIP will answer this concern.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable JAAS configuration for Kafka clients without a config file,KAFKA-4259,13010115,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,rsivaram,rsivaram,06/Oct/16 09:34,02/Jan/17 21:15,12/Jan/21 10:06,24/Dec/16 09:22,0.10.1.0,,,,,,,0.10.2.0,,,security,,,,,,2,kip,,,,See KIP-85 for details: https://cwiki.apache.org/confluence/display/KAFKA/KIP-85%3A+Dynamic+JAAS+configuration+for+Kafka+clients,,aperepel,bbende,githubbot,ijuma,noslowerdna,rsivaram,sslavic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-10-06 10:55:51.815,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 24 09:23:01 UTC 2016,,,,,,,"0|i34iin:",9223372036854775807,,,,,,,,,,,,,,,,"06/Oct/16 10:55;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/1979

    KAFKA-4259: Dynamic JAAS configuration for Kafka clients

    Implementation of KIP-85: https://cwiki.apache.org/confluence/display/KAFKA/KIP-85%3A+Dynamic+JAAS+configuration+for+Kafka+clients

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka KAFKA-4259

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1979.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1979
    
----
commit fc9922b0d9a80053f6a88d59dc8937ea27d6c7c8
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2016-10-06T09:51:43Z

    KAFKA-4259: Dynamic JAAS configuration for Kafka clients

----
","24/Dec/16 09:22;ijuma;Issue resolved by pull request 1979
[https://github.com/apache/kafka/pull/1979]","24/Dec/16 09:23;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1979
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide snap package,KAFKA-4512,13026694,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,mhall119,mhall119,08/Dec/16 13:57,08/Dec/16 14:00,12/Jan/21 10:06,,,,,,,,,,,,packaging,,,,,,0,,,,,"The new Snap packaging format makes it easy to install and update services like Kafka. It provides a one-command install process, reliable dependencies, and security for the user. A snap package will also allow Kafka to be run on the recently released Ubuntu Core images for IoT gateways and lightweight cloud instances.",,cmccabe,mhall119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/16 14:00;mhall119;snap_run;https://issues.apache.org/jira/secure/attachment/12842348/snap_run","08/Dec/16 14:00;mhall119;snapcraft.yaml;https://issues.apache.org/jira/secure/attachment/12842347/snapcraft.yaml",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 08 14:00:54 UTC 2016,,,,,,,"0|i37cof:",9223372036854775807,,,,,,,,,,,,,,,,"08/Dec/16 14:00;mhall119;Attached is a snap build config file that will produce a working Kafka snap, with zookeeper and kafka-server being started by systemd, and the kafka-topics.sh command exposed for the command line.

It needs to have the other commands exposed, and could probably be configured better with respect to logging output.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
time based segment index,KAFKA-87,12518203,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,cburroughs,cburroughs,cburroughs,08/Aug/11 13:40,05/Dec/16 12:54,12/Jan/21 10:06,05/Dec/16 12:54,,,,,,,,,,,core,,,,,,2,,,,,"A time index that:
 - Has minimal performance impact (such as by being append only)
 - Is suitable for
 - Works with getOffsetsBefore
 - Can have it's granularity configured.  With numbers >= 1 minute being ""normal"".
 - Can be disabled

See mailing list discussion : http://mail-archives.apache.org/mod_mbox/incubator-kafka-dev/201107.mbox/%3C4E2F678E.6060500@gmail.com%3E

",,dave@datadoghq.com,ijuma,jdanbrown,sharadag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-08-08 17:25:21.398,,,false,,,,,,,,,,,,,,,,,,59675,,,Mon Dec 05 12:54:40 UTC 2016,,,,,,,"0|i02a1b:",11227,,,,,,,,,,,,,,,,"08/Aug/11 17:25;jkreps;As a related thing, to make this really useful we should integrate it into the consumer API. I.e. give an API along the lines of consumer.resetTo(long).","22/Sep/11 17:30;cburroughs;Note to self: This came up on the user list and there was also significant interest in also indexing by message number (""I would like to go back 10,000 messages, please give me that offset"").","05/Dec/16 12:54;ijuma;Duplicate of KAFKA-3163",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change the consumer side load balancing and distributed co-ordination to use a consumer co-ordinator,KAFKA-264,12541512,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,nehanarkhede,nehanarkhede,nehanarkhede,06/Feb/12 21:53,07/Oct/16 16:03,12/Jan/21 10:06,04/Apr/15 21:33,0.7,0.8.0,,,,,,,,,core,,,,,,1,,,,,A high level design for the zookeeper consumer is here - https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Detailed+Consumer+Coordinator+Design,,eidi,guozhang,hschumacher,jkreps,kmiku7,msample,nehanarkhede,nmarasoi,scott_carey,tcherry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2419200,2419200,,0%,3024000,3024000,,,KAFKA-364,,,,,,,,,,,,,,,KAFKA-167,,,,,,,,,"09/Aug/12 23:02;guozhang;KAFKA-264.v1.patch;https://issues.apache.org/jira/secure/attachment/12540148/KAFKA-264.v1.patch",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-02-06 22:03:54.875,,,false,,,,,,,,,,,,,,,,,,226799,,,Sat Apr 04 21:33:17 UTC 2015,,,,,,,"0|i09lz3:",53993,,,,,,,,,,,,,,,,"06/Feb/12 22:03;prashanth.menon;Interesting stuff here.  Just a question that logically follows from having a coordinator: what happens if the coordinating process goes down or becomes unavailable? ","06/Feb/12 22:12;nehanarkhede;The consumer co-ordinator itself is highly available, since it is elected from amongst the available consumer processes in a group. That can be done using the standard recipe for leader election using Zookeeper","06/Feb/12 22:18;prashanth.menon;Hmm, forgive me if my questions are trivial :) 

What I mean is, the election of the leader can be handled by ZK, but what happens if the thread/process/machine with the coordinator goes down?  Does every consumer group listen on the consumer-leader path in ZK and issue a new round of coordinator election, ala how it'll be done for replication leaders?","06/Feb/12 22:27;nehanarkhede;If a process goes down, the ephemeral node for /consumers/[group]/leader will get deleted from Zookeeper. Each consumer process listens on that path, and triggers a leader election. However, these are very low level details of the implementation, which I intended to chalk out after the high level design looked good. 

You can read more about Zookeeper based leader election here - http://zookeeper.apache.org/doc/trunk/recipes.html#sc_leaderElection","06/Feb/12 22:43;prashanth.menon;Yup, got it, makes sense.  I misinterpreted the wiki and thought election was only done once and at startup, hence my queston.  I'll probably take another look when I get some spare time :-) Thanks, Neha.","09/Aug/12 23:02;guozhang;Please refer to this wiki page for the detailed description of the implementation:

https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Detailed+Consumer+Coordinator+Design#KafkaDetailedConsumerCoordinatorDesign-11.Implementation","03/Apr/13 19:29;scott_carey;Is this still going into 0.8?  Is it already in there?","03/Apr/13 19:48;nehanarkhede;This is scheduled for 0.9","04/Apr/13 00:14;scott_carey;I found it browsing ""fix version=0.8""","12/Jul/14 14:06;nmarasoi;https://cwiki.apache.org/confluence/display/KAFKA/Consumer+co-ordinator link does not work..","04/Apr/15 21:33;jkreps;I think this is no longer active.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unify store and downstream caching in streams,KAFKA-3776,12975115,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,enothereska,enothereska,enothereska,02/Jun/16 10:38,02/Oct/16 21:15,12/Jan/21 10:06,16/Sep/16 16:59,0.10.1.0,,,,,,,0.10.1.0,,,streams,,,,,,1,,,,,This is an umbrella story for capturing changes to processor caching in Streams as first described in KIP-63. https://cwiki.apache.org/confluence/display/KAFKA/KIP-63%3A+Unify+store+and+downstream+caching+in+streams,,arae,enothereska,ggevay,githubbot,guozhang,mfenniak,olli.poyry@iki.fi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-08-17 10:22:55.549,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 16 16:59:22 UTC 2016,,,,,,,"0|i2yvqv:",9223372036854775807,,enothereska,,,,,,,,,,,,,,"17/Aug/16 10:22;githubbot;GitHub user enothereska opened a pull request:

    https://github.com/apache/kafka/pull/1752

    KAFKA-3776: Unify store and downstream caching in streams [WiP]

    Work-in-progress PoC, not to be merged.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/enothereska/kafka KAFKA-3776-poc

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1752.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1752
    
----
commit 9e6d1e0b7b50b8c9bb1848c9e72913701a93cbb0
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-16T09:45:00Z

    Initial commit with stub cache

commit 2a3e46770b2aaddefd843f7cc6cc6727dd1cbc9e
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-17T08:22:23Z

    Adjustments so tests compile

commit 86bb6dcb6beac3d136e01f45cd727e6a6a691b5b
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-17T10:21:11Z

    Remove old cache from RocksDbStore and add global cache to RocksDb

----
","07/Sep/16 06:29;githubbot;Github user enothereska closed the pull request at:

    https://github.com/apache/kafka/pull/1752
","07/Sep/16 06:29;githubbot;GitHub user enothereska reopened a pull request:

    https://github.com/apache/kafka/pull/1752

    KAFKA-3776: Unify store and downstream caching in streams

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/enothereska/kafka KAFKA-3776-poc

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1752.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1752
    
----
commit 9e6d1e0b7b50b8c9bb1848c9e72913701a93cbb0
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-16T09:45:00Z

    Initial commit with stub cache

commit 2a3e46770b2aaddefd843f7cc6cc6727dd1cbc9e
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-17T08:22:23Z

    Adjustments so tests compile

commit 86bb6dcb6beac3d136e01f45cd727e6a6a691b5b
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-17T10:21:11Z

    Remove old cache from RocksDbStore and add global cache to RocksDb

commit 989bce66ddbfc150081cbf74edc8d3d2b6ae70d3
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-17T11:27:08Z

    Create unique cache key per store

commit 959885d4807e7f90030bca946dc5c1bdc4098754
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-17T12:51:17Z

    Enable caching for RocksDBWindowStore

commit 6f582ceb98a5dbd37732e148a0e2f064a90d5d13
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-17T14:17:07Z

    Unit test for windowed stores

commit 7810ab08b02b44e03812e9838062f811c64be0e7
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-19T14:45:27Z

    Plugged in new byte-based cache

commit 333e7992b5f038d1d46d1a17d31a41b700552c77
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-22T15:07:00Z

    Add ProcessorRecordContext for tracking timestamp,offset etc of record. Update Procesor and Store APIs to use context. Dont forward values from Processors that are using a Store. RocksDBStores now have a flush listener and forward to processors whenever the cache is flushed. Left one failing test that highlights the need for the range queries to not forward values downstream

commit 124ffdf84a42e0fe14f3be6897baecdb8f8b7ef0
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-22T15:56:36Z

    merge

commit 34985fae49084b4ee37f0a1cc0c3fdd024a89056
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-23T06:58:49Z

    Initial pass at range queries. Cache based on TreeMap

commit 30bde7301e7541cf2937f56b0303a43900db1fae
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-23T07:15:19Z

    Remove deleted entry from cache

commit 9dbafc07701c5acd6d2c2edbb1f3770670180673
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-23T09:37:17Z

    revert api changes. Track RecordContext via ProcessorContext

commit 63aacc465510bfc8cdb0b675b2d4ebb041a5e99c
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-23T13:18:19Z

    Pass at the 'all' method using cache iterator

commit af6686fb298c2417c5eff9a46cbb8b898df894df
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-23T13:18:24Z

    Merge branch 'KAFKA-3776-poc' of https://github.com/enothereska/kafka into KAFKA-3776-poc

commit 235b8612a121283841f8b3b8795ff6ff5c425c07
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-24T09:24:02Z

    disable caching for joins. expose enableCaching method on PersistentKeyValueFactory

commit a6986a66b08d653b4aa019557f39f0713949fe83
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-24T10:17:30Z

    Cleanup memory cache

commit 5b71d68d84007a03d59a3095467785da461d3c20
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-24T10:25:44Z

    Merged

commit 7658e02a4b203d09d607c0a510d0ae3421eb71a9
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-24T11:21:35Z

    forward before changelog.

commit 91bab64db1bf3673169879c80a15af67f4670b70
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-24T11:21:40Z

    Merge changes from Eno

commit fef6cae098b44cf2e715db00c84d97650114802f
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-24T13:19:12Z

    Flush order should be top-down

commit e1bcd35730a10c06a6e49d3882d787611b106f95
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-24T15:50:56Z

    Merged with trunk

commit 591d4802383d6e8dc0a3e0e97cd619dd99d3db0e
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-24T19:35:28Z

    extract caching out of store

commit dbd383c9066de031f48e8539ad071834b294b7b0
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-25T06:30:27Z

    Merge pull request #1 from enothereska/dg-3776-poc
    
    extract caching out of store

commit 290a66b087847ceb67640574a7a6fc8c8e4d33db
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-25T07:12:55Z

    rename MergedSortedCacheRocksDBIterator -> MergedSortedCacheKeyValueStoreIterator

commit 8a9981b16e59f2dcf656e698545a09733600a5b3
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-25T10:34:07Z

    refactor MergedSortedIterator. Some tests for MemoryLRUCacheIterator. Add PeekingKeyValueIterator

commit c5f45775fb8ff275bd27bf6bef90290a53bb2199
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-25T14:37:21Z

    more tests. fix bugs

commit 9ebdbd49164fdb281609bd8968c0d68453c2236c
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-25T16:54:53Z

    move all namespace related code into cache. make sure overlapping names do not clash

commit 4b4228a27b5f0d5ab3f3153e7d2898423d230712
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-25T17:16:01Z

    add generic types to DelegatingPeekingKeyValueIterator and tests

commit df7e77ac7264aa88dc9091f75ac93f217c38407c
Author: Damian Guy <damian.guy@gmail.com>
Date:   2016-08-26T10:20:18Z

    tidy up and tests

commit f5cca5cedcbc11eb918c5f0c41474c228053e53d
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-08-26T10:25:02Z

    Isolate caches for each store

----
","16/Sep/16 16:59;guozhang;Issue resolved by pull request 1752
[https://github.com/apache/kafka/pull/1752]","16/Sep/16 16:59;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1752
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support quota based on authenticated user name,KAFKA-3492,12955419,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rsivaram,junrao,junrao,01/Apr/16 21:10,17/Sep/16 17:06,12/Jan/21 10:06,17/Sep/16 17:06,,,,,,,,0.10.1.0,,,core,,,,,,0,,,,,"Currently, quota is based on the client.id set in the client configuration, which can be changed easily. Ideally, quota should be set on the authenticated user name. We will need to have a KIP proposal/discussion on this first.

Details are in KIP-55: https://cwiki.apache.org/confluence/display/KAFKA/KIP-55%3A+Secure+Quotas+for+Authenticated+Users",,aauradkar,githubbot,junrao,rsivaram,thesquelched,vrana2016,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-01 21:18:22.595,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 17 17:06:49 UTC 2016,,,,,,,"0|i2vjcf:",9223372036854775807,,,,,,,,,,,,,,,,"01/Apr/16 21:18;aauradkar;Very cool. Jun - are you planning to drive this?","01/Apr/16 21:30;junrao;[~aauradkar], not at this moment. So, feel free to grab this.","08/Apr/16 15:23;rsivaram;[~aauradkar] Are you planning to work on this? If not, I will be happy to submit a KIP.","08/Apr/16 17:28;aauradkar;[~rsivaram] - Feel free to take this. I'll help with comments and reviews.","08/Apr/16 18:03;rsivaram;[~aauradkar] Thank you.","13/Apr/16 18:40;thesquelched;It would nice to be able to assign clients minimum percentages of a user's quota.  For example, if user A has clients X and Y, both of which are contending for A's quota, it would be nice to assign client X a minimum of 25% of A's quota so that Y can't starve it out.","13/Apr/16 18:45;thesquelched;Really, I think the important thing is to be able to control the distribution of quota at times of contention, or barring that, a relatively fair partition of the quota among active clients.","18/Apr/16 15:55;rsivaram;[~junrao] [~aauradkar] [~thesquelched] I have created KIP-55 with a proposal. I have used rates rather than percentage for client quotas so that the units are consistent everywhere. Comments are welcome. Thank you.","22/Apr/16 14:15;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/1256

    KAFKA-3492: Secure quotas for authenticated users

    Code associated with KIP-55 to enable secure quotas for authenticated users with sub-quotas for client-ids of a user.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka KAFKA-3492

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1256.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1256
    
----
commit 2580d20bfd6ed07b870ba9992628533cb6d7172f
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2016-04-20T20:39:12Z

    KAFKA-3492: Secure quotas for authenticated users

----
","17/Aug/16 11:51;githubbot;Github user rajinisivaram closed the pull request at:

    https://github.com/apache/kafka/pull/1256
","17/Aug/16 12:43;githubbot;GitHub user rajinisivaram opened a pull request:

    https://github.com/apache/kafka/pull/1753

    KAFKA-3492: Secure quotas for authenticated users

    Implementation and tests for secure quotas at <user> and <user, client-id> levels as described in KIP-55. Also adds dynamic default quotas for <client-id>, <user> and <user-client-id>. For each client connection, the most specific quota matching the connection is used, with user quota taking precedence over client-id quota.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rajinisivaram/kafka KAFKA-3492

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1753.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1753
    
----

----
","17/Sep/16 17:06;junrao;Issue resolved by pull request 1753
[https://github.com/apache/kafka/pull/1753]","17/Sep/16 17:06;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1753
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cluster id,KAFKA-4093,13000661,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sumitarrawatia,ijuma,ijuma,27/Aug/16 11:30,17/Sep/16 07:18,12/Jan/21 10:06,17/Sep/16 07:18,,,,,,,,0.10.1.0,,,,,,,,,0,,,,,"The details can be found in the Cluster Id KIP:

https://cwiki.apache.org/confluence/display/KAFKA/KIP-78%3A+Cluster+Id",,githubbot,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-17 07:13:42.114,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 17 07:13:42 UTC 2016,,,,,,,"0|i32w9z:",9223372036854775807,,ijuma,,,,,,,,,,,,,,"10/Sep/16 07:32;ijuma;Pull request: https://github.com/apache/kafka/pull/1830","17/Sep/16 07:13;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1830
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Queryable state for Kafka Streams,KAFKA-3909,12984293,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,damianguy,enothereska,enothereska,28/Jun/16 08:39,12/Sep/16 23:56,12/Jan/21 10:06,12/Aug/16 09:06,0.10.1.0,,,,,,,0.10.1.0,,,streams,,,,,,0,,,,,This is an umbrella story for capturing changes to Kafka Streams to enable Queryable state as described in KIP-67 https://cwiki.apache.org/confluence/display/KAFKA/KIP-67%3A+Queryable+state+for+Kafka+Streams.,,cmatta,enothereska,julianhyde,rmetzger,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-06-28 08:39:14.0,,,,,,,"0|i308sf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow console consumer to consume from particular partitions when new consumer is used.,KAFKA-3176,12935402,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vahid,becket_qin,becket_qin,31/Jan/16 01:02,22/Jun/16 08:22,12/Jan/21 10:06,21/Jun/16 05:28,0.9.0.0,,,,,,,0.10.1.0,,,tools,,,,,,0,,,,,"Previously we have simple consumer shell which can consume from a particular partition. Moving forward we will deprecate simple consumer, it would be useful to allow console consumer to consumer from a particular partition when new consumer is used.",,becket_qin,ewencp,githubbot,vahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-16 14:29:15.937,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 22 08:22:48 UTC 2016,,,,,,,"0|i2s73j:",9223372036854775807,,,,,,,,,,,,,,,,"16/Feb/16 14:29;githubbot;GitHub user vahidhashemian opened a pull request:

    https://github.com/apache/kafka/pull/922

    KAFKA-3176: Add partition/offset options to both old and new console consumers

    With this pull request both old and new console consumers can be provided with optional --partition and --offset arguments so messages from a particular partition and starting from a particular offset are only consumed.
    
    The following rules are also implemented to avoid invalid combinations of arguments:
    - If --partition is provided --topic has to be provided too.
    - If --partition is provided --bootstrap-server (and not --zookeeper) should be provided too.
    - If --offset is provided --partition has to be provided too.
    - --offset and --from-beginning cannot be used at the same time.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vahidhashemian/kafka KAFKA-3176

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/922.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #922
    
----
commit 5164a0d7ae344bb6cded0975849f6d36f9596982
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2016-02-16T13:24:38Z

    Add partition/offset options to both old and new console consumers
    
    With this change both old and new console consumers can be provided with optional --partition and --offset arguments so messages from a particular partition and starting from a particular offset are only consumed.
    
    The following rules are also implemented to avoid invalid combinations of arguments:
    - If --partition is provided --topic has to be provided too.
    - If --partition is provided --bootstrap-server (and not --zookeeper) should be provided too.
    - If --offset is provided --partition has to be provided too.
    - --offset and --from-beginning cannot be used at the same time.

----
","16/Feb/16 15:18;githubbot;Github user vahidhashemian closed the pull request at:

    https://github.com/apache/kafka/pull/922
","16/Feb/16 15:18;githubbot;GitHub user vahidhashemian reopened a pull request:

    https://github.com/apache/kafka/pull/922

    KAFKA-3176: Add partition/offset options to both old and new console consumers

    With this pull request both old and new console consumers can be provided with optional --partition and --offset arguments so messages from a particular partition and starting from a particular offset are only consumed.
    
    The following rules are also implemented to avoid invalid combinations of arguments:
    - If --partition is provided --topic has to be provided too.
    - If --partition is provided --bootstrap-server (and not --zookeeper) should be provided too.
    - If --offset is provided --partition has to be provided too.
    - --offset and --from-beginning cannot be used at the same time.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vahidhashemian/kafka KAFKA-3176

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/922.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #922
    
----
commit 5164a0d7ae344bb6cded0975849f6d36f9596982
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2016-02-16T13:24:38Z

    Add partition/offset options to both old and new console consumers
    
    With this change both old and new console consumers can be provided with optional --partition and --offset arguments so messages from a particular partition and starting from a particular offset are only consumed.
    
    The following rules are also implemented to avoid invalid combinations of arguments:
    - If --partition is provided --topic has to be provided too.
    - If --partition is provided --bootstrap-server (and not --zookeeper) should be provided too.
    - If --offset is provided --partition has to be provided too.
    - --offset and --from-beginning cannot be used at the same time.

----
","20/Jun/16 21:42;githubbot;Github user vahidhashemian closed the pull request at:

    https://github.com/apache/kafka/pull/922
","20/Jun/16 21:42;githubbot;GitHub user vahidhashemian reopened a pull request:

    https://github.com/apache/kafka/pull/922

    KAFKA-3176: Add partition/offset options to the new consumer

    With this pull request the new console consumer can be provided with optional --partition and --offset arguments so only messages from a particular partition and starting from a particular offset are consumed.
    
    The following rules are also implemented to avoid invalid combinations of arguments:
    - If --partition or --offset is provided --new-consumer has to be provided too.
    - If --partition is provided --topic has to be provided too.
    - If --offset is provided --partition has to be provided too.
    - --offset and --from-beginning cannot be used at the same time.
    
    This patch is co-authored with @rajinisivaram.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vahidhashemian/kafka KAFKA-3176

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/922.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #922
    
----
commit 5058e4f1c81ed8f47f021248490cf8d3ad1ecc3c
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2016-02-16T13:24:38Z

    KAFKA-3176: Add partition/offset options to the new console consumer
    
    With this feature the new console consumer can be provided with optional --partition and --offset arguments so messages from a particular partition and starting from a particular offset are consumed.
    
    The following rules are also implemented to avoid invalid combinations of arguments:
    - If --partition and/or --offset are provided --new-consumer has to be provided too.
    - If --partition is provided --topic has to be provided too.
    - If --offset is provided --partition has to be provided too.
    - --offset and --from-beginning cannot be used at the same time.

----
","20/Jun/16 23:06;githubbot;Github user vahidhashemian closed the pull request at:

    https://github.com/apache/kafka/pull/922
","20/Jun/16 23:06;githubbot;GitHub user vahidhashemian reopened a pull request:

    https://github.com/apache/kafka/pull/922

    KAFKA-3176: Add partition/offset options to the new consumer

    With this pull request the new console consumer can be provided with optional --partition and --offset arguments so only messages from a particular partition and starting from a particular offset are consumed.
    
    The following rules are also implemented to avoid invalid combinations of arguments:
    - If --partition or --offset is provided --new-consumer has to be provided too.
    - If --partition is provided --topic has to be provided too.
    - If --offset is provided --partition has to be provided too.
    - --offset and --from-beginning cannot be used at the same time.
    
    This patch is co-authored with @rajinisivaram.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vahidhashemian/kafka KAFKA-3176

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/922.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #922
    
----
commit 5058e4f1c81ed8f47f021248490cf8d3ad1ecc3c
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2016-02-16T13:24:38Z

    KAFKA-3176: Add partition/offset options to the new console consumer
    
    With this feature the new console consumer can be provided with optional --partition and --offset arguments so messages from a particular partition and starting from a particular offset are consumed.
    
    The following rules are also implemented to avoid invalid combinations of arguments:
    - If --partition and/or --offset are provided --new-consumer has to be provided too.
    - If --partition is provided --topic has to be provided too.
    - If --offset is provided --partition has to be provided too.
    - --offset and --from-beginning cannot be used at the same time.

----
","21/Jun/16 04:12;githubbot;Github user vahidhashemian closed the pull request at:

    https://github.com/apache/kafka/pull/922
","21/Jun/16 04:12;githubbot;GitHub user vahidhashemian reopened a pull request:

    https://github.com/apache/kafka/pull/922

    KAFKA-3176: Add partition/offset options to the new consumer

    With this pull request the new console consumer can be provided with optional --partition and --offset arguments so only messages from a particular partition and starting from a particular offset are consumed.
    
    The following rules are also implemented to avoid invalid combinations of arguments:
    - If --partition or --offset is provided --new-consumer has to be provided too.
    - If --partition is provided --topic has to be provided too.
    - If --offset is provided --partition has to be provided too.
    - --offset and --from-beginning cannot be used at the same time.
    
    This patch is co-authored with @rajinisivaram.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vahidhashemian/kafka KAFKA-3176

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/922.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #922
    
----
commit 5058e4f1c81ed8f47f021248490cf8d3ad1ecc3c
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2016-02-16T13:24:38Z

    KAFKA-3176: Add partition/offset options to the new console consumer
    
    With this feature the new console consumer can be provided with optional --partition and --offset arguments so messages from a particular partition and starting from a particular offset are consumed.
    
    The following rules are also implemented to avoid invalid combinations of arguments:
    - If --partition and/or --offset are provided --new-consumer has to be provided too.
    - If --partition is provided --topic has to be provided too.
    - If --offset is provided --partition has to be provided too.
    - --offset and --from-beginning cannot be used at the same time.

----
","21/Jun/16 05:17;githubbot;Github user vahidhashemian closed the pull request at:

    https://github.com/apache/kafka/pull/922
","21/Jun/16 05:17;githubbot;GitHub user vahidhashemian reopened a pull request:

    https://github.com/apache/kafka/pull/922

    KAFKA-3176: Add partition/offset options to the new consumer

    With this pull request the new console consumer can be provided with optional --partition and --offset arguments so only messages from a particular partition and starting from a particular offset are consumed.
    
    The following rules are also implemented to avoid invalid combinations of arguments:
    - If --partition or --offset is provided --new-consumer has to be provided too.
    - If --partition is provided --topic has to be provided too.
    - If --offset is provided --partition has to be provided too.
    - --offset and --from-beginning cannot be used at the same time.
    
    This patch is co-authored with @rajinisivaram.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vahidhashemian/kafka KAFKA-3176

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/922.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #922
    
----
commit 5058e4f1c81ed8f47f021248490cf8d3ad1ecc3c
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2016-02-16T13:24:38Z

    KAFKA-3176: Add partition/offset options to the new console consumer
    
    With this feature the new console consumer can be provided with optional --partition and --offset arguments so messages from a particular partition and starting from a particular offset are consumed.
    
    The following rules are also implemented to avoid invalid combinations of arguments:
    - If --partition and/or --offset are provided --new-consumer has to be provided too.
    - If --partition is provided --topic has to be provided too.
    - If --offset is provided --partition has to be provided too.
    - --offset and --from-beginning cannot be used at the same time.

----
","21/Jun/16 05:28;ewencp;Issue resolved by pull request 922
[https://github.com/apache/kafka/pull/922]","21/Jun/16 05:28;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/922
","21/Jun/16 17:53;githubbot;GitHub user vahidhashemian opened a pull request:

    https://github.com/apache/kafka/pull/1536

    MINOR: KAFKA-3176 Follow-up to fix minor issues

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vahidhashemian/kafka minor/KAFKA-3176-Followup

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1536.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1536
    
----
commit 843026de9c224235fe46f3fc0e882cfd4f4fbf15
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2016-06-21T17:36:53Z

    MINOR: KAFKA-3176 Follow-up to fix minor issues

----
","21/Jun/16 18:17;githubbot;Github user vahidhashemian closed the pull request at:

    https://github.com/apache/kafka/pull/1536
","21/Jun/16 18:17;githubbot;GitHub user vahidhashemian reopened a pull request:

    https://github.com/apache/kafka/pull/1536

    MINOR: KAFKA-3176 Follow-up to fix minor issues

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vahidhashemian/kafka minor/KAFKA-3176-Followup

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1536.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1536
    
----
commit 843026de9c224235fe46f3fc0e882cfd4f4fbf15
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2016-06-21T17:36:53Z

    MINOR: KAFKA-3176 Follow-up to fix minor issues

----
","22/Jun/16 08:22;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1536
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Copycat checklist,KAFKA-2365,12849216,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ewencp,ewencp,ewencp,27/Jul/15 05:40,15/Jun/16 16:53,12/Jan/21 10:06,15/Jun/16 16:53,0.9.0.0,,,,,,,0.10.0.0,,,KafkaConnect,,,,,,1,feature,,,,"This covers the development plan for [KIP-26|https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=58851767]. There are a number of features that can be developed in sequence to make incremental progress, and often in parallel:

* Initial patch - connector API and core implementation
* Runtime data API
* Standalone CLI
* REST API
* Distributed copycat - CLI
* Distributed copycat - coordinator
* Distributed copycat - config storage
* Distributed copycat - offset storage
* Log/file connector (sample source/sink connector)
* Elasticsearch sink connector (sample sink connector for full log -> Kafka -> Elasticsearch sample pipeline)
* Copycat metrics
* System tests (including connector tests)
* Mirrormaker connector
* Copycat documentation

This is an initial list, but it might need refinement to allow for more incremental progress and may be missing features we find we want before the initial release.",,dajac,devstr,ewencp,flisky,githubbot,gwenshap,hachikuji,kzadorozhny,liqusha,martinkl,nehanarkhede,pfxuan,wushujames,yarikc@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-07-27 06:09:01.988,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 21 17:47:44 UTC 2015,,,,,,,"0|i2hy87:",9223372036854775807,,ewencp,,,,,,,,,,,,,,"27/Jul/15 06:09;gwenshap;I see the release is 0.8.3 - is the intent to get the whole thing in before 0.8.3 is released? 
(Previously only security and the new consumer were in scope IIRC).

Also, is the intent to have the API as ""stable"" by 0.8.3? 
If not, perhaps it makes sense to clearly mark it as ""evolving"" so no one will make the wrong assumptions (I think the new consumer caused some confusion).","27/Jul/15 07:13;ewencp;I tagged it given my current goals for the timeline. It's aggressive, but this is what I'll be focusing on and I think it's better to put it on *some* release, even optimistically, rather than leaving it completely ambiguous. Probably requires further discussion to figure out how we tag things alpha/beta/stable since I don't think we've done so great with that (delete topic anyone?) and any really major new features, e.g. even the new consumer, need to come with a warning that they have not yet been broadly deployed and tested in production.

I also assigned myself for most of these issues, but I'm obviously more than willing to pass some work on to someone else :)

Finally, there's no component for this yet! Anyone know how to add that (I'm assuming any committer can do it)? I'd like to update all these JIRAs once that component is available.","27/Jul/15 16:53;gwenshap;I added a component, added you as component lead and made sure all new CopyCat jiras will automatically assign to you.

If you object to the auto-assignment, let me know - I figured that at this stage it will save you a click of assigning everything to you... ","27/Jul/15 16:54;gwenshap;BTW. Two connectors that appeared in the KIP discussion but are not in the JIRA are JDBC and HDFS. 
Is the idea to leave them out for now?","27/Jul/15 17:06;ewencp;Thanks, auto assignment works fine, I appreciate the click savings.

On JDBC and HDFS, I haven't converted them over to use this patch in Kafka yet (previously they were using the code in a separate repository under an io.confluent package), but here are two prototypes: https://github.com/confluentinc/copycat-jdbc and https://github.com/confluentinc/copycat-hdfs These are incomplete, but have enough basic functionality to work as POC, and may also be useful in evaluating the core connector APIs for KAFKA-2366.

I think there are two things we should address before pulling any of that code in. 1. We should probably discuss what the process is for pulling a connector into Kafka core, especially when it means additional dependencies. This includes how we manage releases. Should connectors go under contrib like the hadoop-producer and hadoop-consumer? Should votes be done for specific connectors since each one means taking on additional maintenance burden? 2. Structurally, I'd like to make sure we sort out how all these artifacts will show up in the builds. I mentioned this in KAFKA-2366, but before refactoring any more code to fit into the build, it'd be nice to make sure we have the code organization we want to stick with. (If you can't tell, I prefer to avoid mucking with the gradle build as little as possible...)","27/Jul/15 17:34;nehanarkhede;Worth discussing a process for including a connector in Kafka core, but I think we went through this in the KIP discussion and here is what I think. To keep packaging, review and code management easier, it is better to just include a couple lightweight connectors enough to show the usage of the copypcat APIs (file in/file out). Any connector that requires depending on an external system (HDFS, Elasticsearch) should really live elsewhere. We should also delete the ones under contrib, they never ended up getting supported by the community. 

Since there will always be connectors that need to live outside Kafka, I think we should instead focus on how to make tooling easier for discovering and using these federated connectors. ","21/Oct/15 17:47;githubbot;GitHub user ewencp opened a pull request:

    https://github.com/apache/kafka/pull/344

    KAFKA-2365: Handle null keys and value validation properly in OffsetStorageWriter.

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ewencp/kafka kafka-2365-offset-storage-writer-null-values

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/344.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #344
    
----
commit f7241b508190d64d7b5ac48560b01bb14d89ffa9
Author: Ewen Cheslack-Postava <me@ewencp.org>
Date:   2015-10-21T17:47:14Z

    KAFKA-2365: Handle null keys and value validation properly in OffsetStorageWriter.

----
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add performance suite for Kafka,KAFKA-174,12528991,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,nehanarkhede,nehanarkhede,26/Oct/11 23:15,24/May/16 20:53,12/Jan/21 10:06,24/May/16 20:53,0.8.0,,,,,,,,,,,,,,,,0,replication,,,,"This is a placeholder JIRA for adding a perf suite to Kafka. The high level proposal is here -
https://cwiki.apache.org/confluence/display/KAFKA/Performance+testing

There will be more JIRAs covering smaller tasks to fully implement this. They will be linked to this JIRA. ",,donnchadh,fullung,gwenshap,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-10-27 00:22:21.536,,,false,,,,,,,,,,,,,,,,,,214851,,,Tue May 24 20:53:16 UTC 2016,,,,,,,"0|i029lr:",11157,,,,,,,,,,,,,,,,"27/Oct/11 00:22;cburroughs; - If you are willing to ask OS tools. iostat has some relevant stuff (such as await)

 - Coda Hale's metrics have some nice tools for latency, I'd be happy to take that as a sub task.","14/Nov/11 18:53;jkreps;Chris, following up on this, I need to redo the JMX for the network server for KAFKA-202, let's move the discussion on internal metrics and JMX to KAFKA-203.","06/Apr/12 17:42;nehanarkhede;It will be good to think about performance tests relevant to replication","24/May/16 20:53;gwenshap;was done ages ago...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple version support for ducktape performance tests,KAFKA-3490,12955353,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ijuma,ijuma,ijuma,01/Apr/16 16:59,17/May/16 14:29,12/Jan/21 10:06,13/Apr/16 20:51,,,,,,,,0.10.0.0,,,,,,,,,0,,,,,"To verify the performance impact of changes, it is very handy to be able to run ducktape performance tests across multiple Kafka versions. Luckily [~geoffra] has done most of the work for this.",,ewencp,githubbot,ijuma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-01 17:00:33.778,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 13 20:51:44 UTC 2016,,,,,,,"0|i2vixr:",9223372036854775807,,ewencp,,,,,,,,,,,,,,"01/Apr/16 17:00;githubbot;GitHub user ijuma opened a pull request:

    https://github.com/apache/kafka/pull/1173

    KAFKA-3490; Multiple version support for ducktape performance tests

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/confluentinc/kafka kafka-3490-multiple-version-support-perf-tests

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1173.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1173
    
----
commit 897fd95ed5143f8cb4576e9fbea3c71834db8770
Author: Geoff Anderson <geoff@confluent.io>
Date:   2015-11-17T19:42:23Z

    Initial sketch of versioned performance services

commit 6044324ea31384c129ec3996b2551060ba45297c
Author: Geoff Anderson <geoff@confluent.io>
Date:   2015-11-18T18:29:08Z

    Added version-awareness to end to end latency

commit cde81f2f4f55be2d0871ea0950e1c8294c6d3ddf
Author: Geoff Anderson <geoff@confluent.io>
Date:   2015-11-19T03:39:27Z

    Added version support to consumer performance service

commit 299aef12ea97fe9ccce2dd8d5b507fa6888bc27c
Author: Geoff Anderson <geoff@confluent.io>
Date:   2015-11-20T00:44:16Z

    ConsumerPerformance: override new consumer flag if version < 0.9.0.0

commit 847cc9d47404a52e56ea7a0ec6c8a1ea88e620bc
Author: Geoff Anderson <geoff@confluent.io>
Date:   2015-11-20T10:05:30Z

    Added sanity check on consumer performance with new consumer

----
","13/Apr/16 20:51;ewencp;Issue resolved by pull request 1173
[https://github.com/apache/kafka/pull/1173]","13/Apr/16 20:51;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1173
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement partial mirroring functionality in Kafka Mirror Maker ,KAFKA-3609,12961603,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,Sukhdev Saini,Sukhdev Saini,22/Apr/16 10:56,22/Apr/16 11:15,12/Jan/21 10:06,,0.9.0.0,,,,,,,,,,core,,,,,,1,,,,,"It will be useful to implement partial or subset of partitions mirroring functionality in Kafka Mirror Maker. 
This will be useful in number of scenarios:
1. Where business hosts two separate AWS clusters and mirroring everything might be quite expensive.
2. Mirror cluster is maintaining some kind of local state associated with subset of partitions (like a local on-disk key-value store) and hence it should only mirror partitions it is maintaining on disk.",,omkreddy,Sukhdev Saini,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-22 11:15:59.43,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 22 11:15:59 UTC 2016,,,,,,,"0|i2wl0f:",9223372036854775807,,,,,,,,,,,,,,,,"22/Apr/16 11:15;omkreddy;Current Mirror Maker supports partial mirroring of topics by using --whitelist config property. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JSON (de)serialization for kafka-client,KAFKA-3573,12959455,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Invalid,,stepio.ua,stepio.ua,17/Apr/16 13:21,19/Apr/16 19:13,12/Jan/21 10:06,19/Apr/16 19:13,,,,,,,,,,,clients,,,,,,0,,,,,"I've implemented generic implementations for:
 - org.apache.kafka.common.serialization.Serializer
 - org.apache.kafka.common.serialization.Deserializer

They provide support for serializing & deserializing POJO-entities into JSON using:
 - com.fasterxml.jackson.core:jackson-core
 - com.fasterxml.jackson.core:jackson-databind
 - org.apache.kafka.common.serialization.StringSerializer
 - org.apache.kafka.common.serialization.StringDeserializer

The code is shared as a separate project on github:
https://github.com/stepio/kafka-json

But I may create a pull request for kafka-client to include this implementation to the main code. What do you think?",,guozhang,liquanpei,stepio.ua,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-04-17 18:10:34.263,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 19 18:35:08 UTC 2016,,,,,,,"0|i2w87r:",9223372036854775807,,,,,,,,,,,,,,,,"17/Apr/16 18:10;liquanpei;Thanks for working on this. Currently, there is a JSON Serializer/Deserializer in Kafka Connect. We are working on to move this to the common package so that it can be used by other clients.  ","17/Apr/16 19:06;stepio.ua;Good point, found your code here:
https://github.com/apache/kafka/tree/trunk/connect/json/src/main/java/org/apache/kafka/connect/json

Existing implementation works with JsonNode, mine - with POJOs. You may check the tests for details. Are you interested in such an approach?

P.S.: loved your idea of serializing directly to byte[] (avoiding the intermediate String), so implemented the same approach.","18/Apr/16 05:10;guozhang;We were working on moving the serdes from connect to common: https://issues.apache.org/jira/browse/KAFKA-3515. But as you realized it will bring additional dependencies to kafka-clients jar, and we were considering if breaking the jar to fine-grained jars for separate dependencies would make more sense.","19/Apr/16 12:20;stepio.ua;So the ticket is irrelevant for the project, right?

If I got your point, you already have a JSON implementation and you'll work with it in the future.","19/Apr/16 18:35;guozhang;The goal is to move the already existing JSON serde that was in the Connect package, so that users working with kafka-clients or depending on that jar can directly use it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add ByteBuffer Serializer&Deserializer,KAFKA-3046,12924617,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,vesense,vesense,29/Dec/15 06:10,19/Apr/16 04:49,12/Jan/21 10:06,23/Feb/16 23:04,,,,,,,,0.10.0.0,,,clients,,,,,,0,,,,,"ByteBuffer is widely used in many scenarios. (eg: storm-sql can specify kafka as the external data Source, we can use ByteBuffer for value serializer.) 
Adding ByteBuffer Serializer&Deserializer officially will be convenient for users to use.",,githubbot,guozhang,vesense,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-29 06:12:06.76,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 19 04:49:39 UTC 2016,,,,,,,"0|i2qd0n:",9223372036854775807,,,,,,,,,,,,,,,,"29/Dec/15 06:12;githubbot;GitHub user vesense opened a pull request:

    https://github.com/apache/kafka/pull/718

    KAFKA-3046: add ByteBuffer Serializer&Deserializer

    https://issues.apache.org/jira/browse/KAFKA-3046

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/vesense/kafka patch-3

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/718.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #718
    
----
commit 0f39db647e2a1a3189aba6d2fa96aa4c1156bf86
Author: Xin Wang <best.wangxin@163.com>
Date:   2015-12-29T05:57:13Z

    add bytebuffer ser

commit 711a8b594bd7a2a5eacb958b74ab85ba10d97158
Author: Xin Wang <best.wangxin@163.com>
Date:   2015-12-29T05:58:49Z

    add bytebuffer deser

----
","23/Feb/16 23:04;guozhang;Issue resolved by pull request 718
[https://github.com/apache/kafka/pull/718]","23/Feb/16 23:04;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/718
","19/Apr/16 04:49;guozhang;[~vesense] I'm currently working on KAFKA-3499 and I'm considering adding another Bytes class wrapping immutable byte arrays for serde (https://github.com/apache/kafka/pull/1229/files/9302bc18c8fa400b1fb7d4b36f29c8f0c812785c#r60079246).

Since your addition of ByteBufferSerde is not included in any releases yet, I'm wondering if I can just replace these classes with BytesSerializer and BytesDeserializer, since having both Bytes and ByteBuffer for serde sounds duplicates. What's your usage in spark-sql? Does that must require a mutable ByteBuffer, or does immutable Bytes work for you? Please let me know your thoughts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer re-design,KAFKA-364,12560271,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,11/Jun/12 23:49,18/Apr/16 09:14,12/Jan/21 10:06,18/Apr/16 09:14,,,,,,,,,,,,,,,,,0,,,,,"We've received quite a lot of feedback on the consumer side features over the past few months. Some of them are improvements to the current consumer design and some are simply new feature/API requests. I have attempted to write up the requirements that I've heard on this wiki -

https://cwiki.apache.org/confluence/display/KAFKA/Consumer+Client+Re-Design

This would involve some significant changes to the consumer APIs, so we would like to collect feedback on the proposal from our community. Since the list of changes is not small, we would like to understand if some features are preferred over others, and more importantly, if some features are not required at all. ",,fangy,hschumacher,nehanarkhede,ross.black,slon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-06-20 12:45:31.164,,,false,,,,,,,,,,,,,,,,,,241673,,,Wed Jun 20 13:18:16 UTC 2012,,,,,,,"0|i029sn:",11188,,,,,,,,,,,,,,,,"14/Jun/12 21:52;nehanarkhede;I would like to throw in a couple use cases:


  - Allow the new consumer to reset its offset to either the current
  largest or smallest.  This would be a great way to restart a process that
  has fallen behind.  The only way I know how to do this today, with the
  high-level consumer, is to delete the ZK nodes manually and restart the
  consumer.
  - Allow the consumer to reset its offset to some arbitrary value, and
  then write that offset into ZK.    Kind of like the first case, but would
  make rewinding/replays much easier.

Modularity (the ability to layer the ZK infrastructure on top of the simple
interface) would be great.

thanks,
Evan","14/Jun/12 21:52;nehanarkhede;Throwing a +1 on ""Allow the consumer to reset its offset to some arbitrary value, and then write that offset into ZK"".

We're currently running into a scenario where we would like to have 100% reliability, and we're losing a few messages when a connection is broken, but there were still a few messages in the OS TCP buffers. So, we're planning on shifting the ZK offset by a few seconds ""back in time"" if we detect a broker has gone down, to make sure all the messages will be actually delivered to the end consumer when that broker comes back up, even if there's a small amount of overlapping messages.

Thanks,

Marcos","20/Jun/12 12:45;ross.black;In the API redesign, it would be nice to somehow allow for flexible/pluggable control of the allocation of [broker:partition] from producers and to consumers when using zookeeper management.
(I was not certain whether ""Manual partition assignment"" covered this - it did not mention producer partition control)

I currently use SyncProducer and SimpleConsumer to directly control the set of [broker:partition] that a producer writes to and that a consumer reads from.

I need this for a scenario where the consumer holds some state (like a cache) on local disk.  It is expensive to discard the local state - the consumer must then instead perform a remote lookup with a very high latency. (> 5mins).  I need the partitioning performed by a producer to remain fixed until explicitly changed (the number of producers is relatively static, and each producer sends messages into a dedicated broker).  I need each consumer to fetch the same partitions unless a consumer has failed for more than some period of time (approx 5 mins) so that if it recovers quickly I have not wastefully discarded local state.

Currently if I use Producer with zookeeper, the Partitioner API allows me to partition messages, but then kafka code in the Producer controls allocation of the Partitioner result to a physical [broker:partition].  If I use Producer with fixed brokers, messages are allocated to random partitions.  If I use the high level consumer, kafka code in ZookeeperConsumerConnector controls the allocation of [broker:partition] to available consumers.

I understand if this is an over-specialised use-case to cater for.  At minimum I would like the equivalent functionality of SyncProducer and SimpleConsumer to be preserved in a public API.

Thanks,
Ross

","20/Jun/12 13:18;ross.black;I send batched messages with compression, and use the offsets retrieved by the consumer to get exactly-once semantics (by persisting consumer state with the offsets).  When using the message set iterator, for a e.g. batch of 5 messages the offset returned for messages 1-4 is the start of the *current* batch, and the offset for message 5 is the start of the *next* batch.  My code has to wait for the offset to change from the previous message before it persists (so that my consumer state is only persisted when a batch has been completed).  To me, this feels awkward in that it is not very explicit in the API (you have to know about internals to understand the processing required).  I think it could be useful to expose a flag that indicated batch-end, or to directly expose message batches (similar to the way shallowIterator does?).

Thanks,
Ross
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a centralized co-ordinator for consumer rebalancing,KAFKA-387,12596475,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,nehanarkhede,nehanarkhede,nehanarkhede,30/Jun/12 00:06,18/Apr/16 09:05,12/Jan/21 10:06,18/Apr/16 09:05,,,,,,,,,,,,,,,,,0,,,,,"Proposal for the consumer co-ordinator is here - https://cwiki.apache.org/confluence/display/KAFKA/Central+Consumer+Coordination

",,jimhoagland,lanzaa,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,241672,,,2012-06-30 00:06:42.0,,,,,,,"0|i029s7:",11186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secure Kafka,KAFKA-1176,12683753,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,pradeepg26,pradeepg26,10/Dec/13 02:28,18/Apr/16 07:11,12/Jan/21 10:06,18/Apr/16 07:11,,,,,,,,0.9.0.0,,,,,,,,,4,,,,,"Implement Authentication, Authorization, Encryption, ACL's for Kafka.",,anew,apsaltis,fwiffo,geetasg,ghelmling,jimhoagland,kaiyzen,omkreddy,pradeepg26,stefank,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-05-02 07:16:34.17,,,false,,,,,,,,,,,,,,,,,,362825,,,Mon Apr 18 07:11:39 UTC 2016,,,,,,,"0|i1qjjr:",363131,,,,,,,,,,,,,,,,"10/Dec/13 02:28;pradeepg26;Linking to the Security proposal wiki","02/May/14 07:16;jimhoagland;I think this issue will benefit from having a goal of secure mutli-tenancy for a Kafka cluster.  That is, there can be independent parties using the same Kafka cluster.  It might be useful to envision a hypothetical scenario where a company's IT is hosting Kafka cluster and has three tenants:
* the e-commerce team, using it for a record of online sales
* the security monitoring team, using it to receive a stream of security-related events for analysis
* a product dev team, receiving anonymized product telemetry from the field and using a third party to analyze it

What secure multi-tenancy would mean to me (other people may have different ideas) includes:
* tenants shouldn’t be able to see each others data: different tenants should not be able to read the contents of other tenants topics (by default at least)
* tenants shouldn’t be able to see what each other is doing:  they shouldn't be able to see each others topics or metadata about the topic such as size
* non-interference:  tenants should not be able to interfere with each other.  This suggests quotas along the lines of what is in KAFKA-656, including disk quotas and caps that would limit CPU and disk I/O resource usage.  This also suggests limits on the ability to add to a topic or to cause messages to be deleted.

In addition, I could image cases in which the team producing the messages wants to allow users (not necessarily on their team) to access certain topics.

Of course you need authentication to form the basis for identity.  If would be best to make it easy for organization to tie this to their existing authentication mechanisms.

I'm new to Kafka, but hopefully this makes sense.","18/Apr/16 07:11;omkreddy;Many of the security related features are available  from Kafka 0.9.0.0 release",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Connect Source connector for HBase ,KAFKA-2914,12917193,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,ewencp,nielsbasjes,nielsbasjes,01/Dec/15 09:24,06/Mar/16 21:24,12/Jan/21 10:06,06/Mar/16 21:24,,,,,,,,,,,KafkaConnect,,,,,,0,,,,,"In many cases I see HBase being used to persist data.
I would like to listen to the changes and process them in a streaming system (like Apache Flink).

Feature request: A Kafka Connect ""Source"" that listens to the changes in a specified HBase table. These changes are then stored in a 'standardized' form in Kafka so that it becomes possible to process the observed changes in near-realtime. I expect this 'standard' to be very HBase specific.

Implementation suggestion: Perhaps listening to the HBase WAL like the ""HBase Side Effects Processor"" does?
https://github.com/NGDATA/hbase-indexer/tree/master/hbase-sep
",,apurtell,ewencp,nielsbasjes,prateekrungta,rmetzger,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-02 08:00:33.527,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 06 21:24:13 UTC 2016,,,,,,,"0|i2p3u7:",9223372036854775807,,,,,,,,,,,,,,,,"02/Dec/15 08:00;ewencp;[~nielsbasjes] Agreed that an HBase source connector would be great, and thanks for the pointer on how other projects grab the WAL. I think something like this is probably the right way to hook into HBase since it gives you the complete picture and probably gives the most flexibility wrt how to translate the WAL into messages in Kafka.

The plan was to keep the connector development federated, which means connectors like this would generally be maintained outside Kafka's source tree. This is partly just a practical decision, since pulling in a large variety of connectors would drastically complicate Kafka, its packaging, and its release process. But it also has nice side effects like decoupling connector release schedules from Kafka's, such that connectors can iterate more quickly than Kafka itself.

We have one very simple set of connectors implemented in Kafka for demonstration purposes, and while we do have KAFKA-2375 filed for an elasticsearch connector, we really only used it as a possible example to include in Kafka itself since it would be a more realistic example that doesn't have any extra dependencies.

I think adding an HBase connector would be hugely valuable, but should probably be done outside Kafka. I'll circle back soon with a template repository that can be used to bootstrap new connectors. This would be a good starting point for an HBase connector.","02/Dec/15 19:11;wushujames;[~ewencp], do you want to fork/update https://github.com/wushujames/copycat-connector-skeleton? It hasn't yet been updated to support 0.9.0.","22/Dec/15 02:08;wushujames;https://github.com/wushujames/copycat-connector-skeleton has now been updated to support 0.9.0. And it has been renamed to https://github.com/wushujames/kafka-connector-skeleton","06/Mar/16 00:39;apurtell;See HBASE-15320","06/Mar/16 21:24;ewencp;[~apurtell] Thanks for the link. I'm going to mark this ""Won't Fix"" since the Kafka project is taking a federated approach to connector development and won't be including connectors within the project itself. Hopefully HBase connector development can be picked up at the link you've provided.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for Kerberos enabled Kafka,KAFKA-3109,12931403,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Invalid,,nielsbasjes,nielsbasjes,15/Jan/16 09:06,15/Jan/16 09:11,12/Jan/21 10:06,15/Jan/16 09:11,,,,,,,,,,,,,,,,,0,,,,,"In Kafka 0.9.0.0 support for Kerberos has been created ( KAFKA-1686 ).
Request: Allow Flink to forward/manage the Kerberos tickets for Kafka correctly so that we can use Kafka in a secured environment.

I expect the needed changes to be similar to FLINK-2977 which implements the same support for HBase.",,ijuma,nielsbasjes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-3058,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-01-15 09:07:34.137,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 15 09:11:26 UTC 2016,,,,,,,"0|i2rigv:",9223372036854775807,,,,,,,,,,,,,,,,"15/Jan/16 09:07;ijuma;Hi Niels. Did you mean to file this in the Flink project instead of the Kafka one?","15/Jan/16 09:08;nielsbasjes;Oeps, yes, sorry.
Can you move it (I cannot)
","15/Jan/16 09:11;nielsbasjes;Wrong project. Sorry.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Quotas for Kafka,KAFKA-2083,12787505,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,aauradkar,aauradkar,aauradkar,01/Apr/15 22:33,02/Dec/15 03:19,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,3,,,,,"Parent ticket to implement quotas in kafka. See discussion here:
https://cwiki.apache.org/confluence/display/KAFKA/KIP-13+-+Quotas

Subtasks will be filed for task breakdown.",,aauradkar,anil.wadhai@gmail.com,barakm,chaitanyap,fgiunchedi,ignacio83,pblaha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1209600,1209600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-04-01 22:33:02.0,,,,,,,"0|i27o67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Quotas to Kafka,KAFKA-656,12619152,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,,jkreps,jkreps,06/Dec/12 16:44,02/Dec/15 03:19,12/Jan/21 10:06,02/Dec/15 03:19,0.8.1,,,,,,,,,,core,,,,,,3,project,,,,"It would be nice to implement a quota system in Kafka to improve our support for highly multi-tenant usage. The goal of this system would be to prevent one naughty user from accidently overloading the whole cluster.

There are several quantities we would want to track:
1. Requests pers second
2. Bytes written per second
3. Bytes read per second

There are two reasonable groupings we would want to aggregate and enforce these thresholds at:
1. Topic level
2. Client level (e.g. by client id from the request)

When a request hits one of these limits we will simply reject it with a QUOTA_EXCEEDED exception.

To avoid suddenly breaking things without warning, we should ideally support two thresholds: a soft threshold at which we produce some kind of warning and a hard threshold at which we give the error. The soft threshold could just be defined as 80% (or whatever) of the hard threshold.

There are nuances to getting this right. If you measure second-by-second a single burst may exceed the threshold, so we need a sustained measurement over a period of time.

Likewise when do we stop giving this error? To make this work right we likely need to charge against the quota for request *attempts* not just successful requests. Otherwise a client that is overloading the server will just flap on and off--i.e. we would disable them for a period of time but when we re-enabled them they would likely still be abusing us.

It would be good to a wiki design on how this would all work as a starting point for discussion.",,ab10anand,ashwin95r,cagatayk,dajac,donnchadh,jcreasy,jkreps,liqusha,maxhen,mohandsedu,prashanth.menon,soumen.sarkar,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-2083,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-02-26 06:26:17.395,,,false,,,,,,,,,,,,,,,,,,296347,,,Tue Aug 26 21:06:33 UTC 2014,,,,,,,"0|i148pb:",232873,,,,,,,,,,,,,,,,"26/Feb/13 06:26;jcreasy;Kafka is using the Yammer/Coda Hale Metrics library now right? 

Would it be sufficient to track the three quantities by topic and client ID and take action if the 1/5/15-min load for that metric exceeded the thresholds defined? That is an EWMA so it would rise and taper off over time. 

Perhaps we could use an exponential back-off so that if you exceeded it once it would recover quickly and then after that take longer too cool-off before allowing the client again. ","26/Feb/13 19:25;jkreps;Yes, that is basic idea. The concern with ewma is just the opacity of it as an SLA. I kind of prefer picking a hard window of (say) 1-5 seconds and caping the requests/messages/etc in that window to a hard, configurable limit (e.g. 1000k) per client or topic. This is somewhat more clear about why you are in violation. EWMA also means a weighting over an infinite history of prior values which is not intuitive. For example a long history of zero messages/second does not justify a 5 second burst of exceptional load. The value of ewma is to smooth the estimate when data is sparse (otherwise you end up measuring ""sample variance"" and for low traffic things get a line that jumps around erratically). However this is okay for SLA violation because anyone approaching their SLA doesn't have a data sparsity problem--they are making tons of requests.","26/Feb/13 19:42;jcreasy;That makes sense. Thinking about this last night, we probably need to keep the quota stats in ZK in order to enforce them across the cluster, so that would make using Metrics harder also. ","26/Feb/13 19:57;jkreps;Yeah this a bit of a dilemma. Doing it cluster-wide with low latency is pretty hard. Arguably the thing you want to protect is really the per-server load. That is to say the limit is that we can't have one machine taking more than X messages/sec--though X might be fine if spread over enough servers. However since in a sense the number of servers is something of an implementation detail it makes it harder to express to the user what the speed limit is (after all if we add more servers from their pov the speed limit just went up if it is a per-server number). Maybe the sane way to do it is in terms of per-partition load rather than servers or topic overall. Thoughts?","26/Feb/13 20:35;jcreasy;Thinking about it that way, keeping the quotas and enforcements per-server is much easier to implement both in terms of code, and in terms of a user setting the proper values for their environment. 

Doing it per partition and per client I think would be better than per server or per topic. And only tracking it on a broker by broker basis will be sufficient because as a user, I would set a level for what my broker could handle, and it might be different for different brokers if I have non-homogeneous set of servers.","27/Feb/13 01:37;jcreasy;I could implement the checks as meters in the Metrics class, then evaluate the meters vs. the configured limits. I could define a Healthcheck as a means to report clients/topics that are violating their quota. 

http://metrics.codahale.com/manual/core/#man-core-healthchecks","27/Feb/13 04:46;jkreps;Yes, it would definitely make sense to use a metric for it since that will make it easier to monitor how close you are to the limit.

One related patch is the dynamic per-topic config patch. I have a feeling that these quotas would definitely be the kind of thing you would want to update dynamically. See KAFKA-554.

If you want to take a stab at it, that would be fantastic and I would be happy to help however I can. It would probably be good to start with a simple wiki of how it would work and get consensus on that.

Here is what I was thinking, we could add a class something like
  class Quotas {
    def record(client: String, topic: String, bytesToRead: Long, bytesToWrite: Long)
  }
The record() method would record the work done, and if we are over quota for that topic or client throw a QuotaExceededException. (We might need to split the record and the check, not sure).

This class can be integrated in KafkaApis to do the appropriate checks for each API. We should probably apply the quota to all client-facing apis, even things like metadata fetch which do no real read or write. These would just count against your total request counter and could have the bytes arguments both set to 0.","08/Mar/13 23:14;jcreasy;I'll get started on that Wiki, I see that 554 has been closed :)","20/Mar/13 00:57;jcreasy;Obviously a work in progress.

https://cwiki.apache.org/confluence/display/KAFKA/KAFKA-656+-+Quota+Design","25/Jun/13 00:35;prashanth.menon;Hey Jonathan, have you made any head way on this?  Let me know, I'd like to give this a go if you're tied up :)","25/Jun/13 00:37;jcreasy;Go for it, I knew someone would likely get to this before I got back to it, I didn't really do anything on it yet.","13/Jul/13 21:33;swapnilghike;Hey Prashanth/Jonathan, have you had the time to run with this? If you're busy, I would be interested in taking this up.","12/Aug/14 19:44;ab10anand;H [~jkreps] [~jcreasy],
 Guys don't see much activity on this thread. We are looking into solving this problem in our company and trying to figure out ways to do it best.  I looked into the suggestions proposed in this thread and I believe maintaining a state about observed rate of flow on each broker makes a very clear case. 
  Are we only looking into solving this from the broker side by throwing quota exceeded exception?? Because in this scenario, we have no control over the producers and they can keep on sending the data to the Kafka brokers and bombard the network even after getting the quota exceeded warning and exceptions. 
 We have few ideas on lease based service, where the producer requests for the lease and only sends data within the allotted quota. To send higher traffic, he would be required to renew his lease with the new bandwidth request ( we are trying to achieve this through a watcher node in zookeeper). Though this solution has issues, when a rogue producer sends messages to the system. We are in the design phase of our solution and it would be great to get inputs from you guys. It would be great if we can design something we can contribute back. 

Regards, 
Abhinav","12/Aug/14 20:02;jkreps;Hey [~ab10anand], I would rather not do explicit leases. The problem is that zookeeper is a very scarce resource so you have to then quota the lease updates themselves--basically it just seems complex. In any case, whether the mechanism is leases or error, the client needs some mechanism to throttle itself when it reaches the maximum of it's leased capacity or gets an error. I think the errors can do this naturally. Our new java producer actually already has a notion of a retry backoff, and I think we could generalize this to handle this case fairly easily. This will cause the client to produce backpressure when it hits its maximum.","18/Aug/14 14:21;ab10anand;Hi [~jkreps], 

 We also want to avoid using zookeeper as dependency. With the number of producers being pretty high, I am planning to follow the approach that has been proposed in this thread. I can create a design to get my approach reviewed. I am creating a QuotaManager utility to decide  the quota limits for each client at partition-topic level. and saving all the client topic/partition level information as metrics. Will throw QuotaExceeded exception and will catch it on the producer itself.  
Few questions ... Any known issues with high number of metrics ?  Any thing that I should be aware of to avoid affecting producer performance ? 
Let me know any suggestions that you have, I will try to incorporate in the design. 

Regards,
Abhinav","18/Aug/14 16:50;jkreps;Hey [~ab10anand] a couple of suggestions:
1. You could imagine a fairly complete quota system would involve all kinds of granularities at which you could enforce the quota (at the IP level, at the user level, etc). There are also all kinds of things we can quota: requests, bytes in, bytes out, etc. However for now let's just keep it simple. Let's just start with a per-topic bytes-written quota. I think this gives you 70% of what you want and will give us a chance to learn about it operationally before attempting something more complicated.
2. The quota should be specified at the topic level but enforced at the partition level. I.e. if you specify 10MB/sec on a topic with 10 partitions then what we will enforce would be 1MB/sec per partition.
3. We should make use of the topic-level configs to implement this. I.e. add a new configuration in LogConfig that defaults to an infinite quota.
4. One piece of work that was done in anticipation of quotas was to combine the metrics and quota systems. This metrics package is in use on the clients now, but not yet on the server (it is under clients/src/main/org/apache/kafka/common/metrics I think). At a high-level the idea is to be able to enforce quotas on exactly the same things we monitor with metrics to make the reporting side of things easier. This code may actually do most of what the QuotaManager would have done, i.e. it will maintain all the metrics and each metric can have an optional quota associated, if the metric exceeds the quota it will throw an exception. Check this out and see if it makes sense in the way you were thinking of using it.","19/Aug/14 17:58;ab10anand;Hi [~jkreps], 
 Thanks for the inputs. I am planning to start with the basic use case of per-topic bytes-written. Few questions that would help me make the code conform to the standards. 
1. As the number of partitions change, is the partition count maintained as a state variable on all broker or should we figure this out from zookeeper ?
2. The kafka server uses the Yammer metrics, though the clients have their own metrics library (with the SampledStat etc). Is there any specific reason to do so ? I would avoid changing the metric design on the broker side. 

Regards,
Abhinav","26/Aug/14 14:49;ab10anand;Hi [~jkreps], 

  I am trying to create a poc for this. Though I am not able to handle the scenario with producers where ack set to 0 (fire and forget producers). The approach that i see in the APIs is to close the connection to the producer, but this doesn't convey the reason for the connection close. I am not exactly sure if i am missing anything. I have tried scratching my head on this, though couldn't dig anything interesting in the codebase.  It would great if you can shed some light on this ? 

Regards,
Abhinav","26/Aug/14 15:06;jkreps;Hey [~ab10anand], that is right--in general for a fire and forget producer we can only close the connection as no acknowledgement is returned. I think that is the right behavior here, although as in other error cases it is somewhat confusing as to why that happened.

Starting with the new java producer we have implemented, though, there isn't much reason to use ack=0 vs ack=1 as the performance difference is extremely small since we improved the way we do I/O to eliminate any blocking.","26/Aug/14 15:49;ab10anand;Hi [~jkreps], 
 Thats great, so I have a basic poc setup for the new producer and with ack=1. With the performance being at par with fire and forget producers, this would handle most of our use cases. 

I had one more query with regard to the configuration management for the quotas. As the number of partition is updated, we either update the quota configuration in zookeeper or if the partition count is maintained as a state variable on all broker, we can use it to dynamically calculate the quota.  I like the idea of updating the quota configuration, every time the partitions are added. Let me know your inputs on this. 

Regards,
Abhinav","26/Aug/14 21:06;jkreps;Another alternative which might make thing simpler would be to make the quota configuration per partition. This would avoid having to adapt it if the partition count changed. This is not ideal since I think people naturally think about data size at the topic level but at the moment all our size-based retention is done at the partition level so you could argue that this is more consistent.

I think the other alternative which is a little bit more work is to store the per-topic quota in ZK and calculate quota/part-count to get the per partition limit. This per-partition limit would have to be updated when either the quota changed or the partition count changed. This might require a small bit of refactoring which I would be happy to walk you through if you end up going that route.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-30 Plug-able interface for consensus and storage sub-systems,KAFKA-2916,12917273,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,abiletskyi,abiletskyi,01/Dec/15 15:27,01/Dec/15 15:27,12/Jan/21 10:06,,,,,,,,,,,,core,,,,,,10,,,,,Checklist for KIP-30 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-30+-+Allow+for+brokers+to+have+plug-able+consensus+and+meta+data+storage+sub+systems),,abiletskyi,ben_b,bradfordcp,jeromatron,jjirsa,lordemsworth,sebastian.estevez@datastax.com,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-12-01 15:27:01.0,,,,,,,"0|i2p4bz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition balance tool between borkers,KAFKA-2106,12819211,New Feature,Patch Available,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,chenshangan521@163.com,chenshangan521@163.com,chenshangan521@163.com,08/Apr/15 07:08,10/Nov/15 18:59,12/Jan/21 10:06,,,,,,,,,,,,admin,,,,,,2,,,,,"The default partition assignment algorithm can work well in a static kafka cluster(number of brokers seldom change). Actually, in production env, number of brokers is always increasing according to the business data. When new brokers added to the cluster, it's better to provide a tool that can help to move existing data to new brokers. Currently, users need to choose topic or partitions manually and use the Reassign Partitions Tool (kafka-reassign-partitions.sh) to achieve the goal. It's a time-consuming task when there's a lot of topics in the cluster.",,allenxwang,amcelwee,chenshangan521@163.com,enothereska,guozhang,jthakrar,junrao,lizhitao,niklaus.xiao,sslavic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/15 17:10;chenshangan521@163.com;KAFKA-2106.3;https://issues.apache.org/jira/secure/attachment/12750856/KAFKA-2106.3","24/Apr/15 07:55;chenshangan521@163.com;KAFKA-2106.patch;https://issues.apache.org/jira/secure/attachment/12727842/KAFKA-2106.patch","27/Apr/15 06:00;chenshangan521@163.com;KAFKA-2106.patch.2;https://issues.apache.org/jira/secure/attachment/12728320/KAFKA-2106.patch.2",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2015-04-27 06:10:49.013,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 15 00:07:37 UTC 2015,,,,,,,"0|i2cye7:",9223372036854775807,,junrao,,,,,,,,,,,,,,"24/Apr/15 07:45;chenshangan521@163.com;When adding new brokers to the cluster, existing replica assignment should be reassigned to get better distribution.  The basic though is to reassign the topic in the new cluster while starting at the original broker starting index, the process is like rehash the existing data.","24/Apr/15 07:55;chenshangan521@163.com;Implement basic rehash assignment algorithm","27/Apr/15 06:00;chenshangan521@163.com;check reassignment status, Thread.sleep according to the status rather than always sleep","27/Apr/15 06:10;lizhitao;if I have this tools，wonderful","27/Apr/15 07:29;chenshangan521@163.com;if new assignment start from the original starting index, it will not get well distributed as the start index is generated from the old brokerList. If reassign from scratch, it can get the best distributing performance. Maybe I should try the later one.","17/Aug/15 17:10;chenshangan521@163.com;implement a global balance among the cluster, which will destroy the original partition assignment algo. But, the cluster can reach to a balanced state eventually.

It first divide all the topic into two groups: big topics and small topics. Number of parititions of big topics is bigger than number of brokers, while small topics are the opposite. It will guarantee that partitions of big topic is distributed evenly in the cluster. When it comes to some topics, do not constraint absolute even distribution as big topics.","25/Aug/15 00:55;allenxwang;I am curious in the following code why there is a restriction that partitions should have no more than 3 replicas and every partition has the same number or replicas. It is possible that different topic has different number of replicas given different requirement of availability.

{code}
  def filterValidTopicAssignment() = {
    val groupedByTopic = allTopicsAssignment.groupBy(tp => tp._1.topic)
    /**
     * check replicas:
     * replicas amount should be more than 0 and less than 3
     * all partitions should have the same amount of replicas
     *
     */
    var validTopicAssignment = groupedByTopic.filter(
      t => {
        t._2.head._2.size > 0 && t._2.head._2.size < 3 && t._2.values.map(seq => seq.length).toSet.size == 1
      }
    )

    if(includeTopicSet.size != 0) {
      validTopicAssignment = validTopicAssignment.filter(topicInfo => includeTopicSet.contains(topicInfo._1))
    }

    if(excludeTopicSet.size != 0) {
      validTopicAssignment = validTopicAssignment.filter(topicInfo => (! excludeTopicSet.contains(topicInfo._1)))
    }

    validTopicAssignment
  }
{code}","25/Aug/15 02:47;chenshangan521@163.com;There are two things here:
1) about 3 replica restriction
I think 3-replica is  a conventional way adopted. I've change the restriction to less or equal than 3 but forget to update the patch. If it's not necessary, I can remove this restriction. It does not affect the balancing algo.

2) every partition has the same number of replicas
I only make sure every partition of the same topic has the same number of replicas not all topics. I think it's reasonable. Maybe you misunderstand the logic ?
 ","14/Sep/15 21:56;junrao;[~chenshangan521@163.com], could you write up a high level description of what the new tool does? Also, for major changes, we require people to submit a KIP proposal (https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals).","15/Sep/15 07:32;sslavic;This one is nice to have for 0.9.0.0 release.

It's somewhat related to KAFKA-1792 and https://cwiki.apache.org/confluence/display/KAFKA/KIP-6+-+New+reassignment+partition+logic+for+rebalancing","15/Oct/15 00:07;guozhang;[~chenshangan521@163.com] I have added you to the contributors list, you can assign tickets to yourself moving forward.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Security for Kafka,KAFKA-1682,12746543,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,jkreps,jkreps,07/Oct/14 23:30,12/Oct/15 05:13,12/Jan/21 10:06,,0.10.1.0,,,,,,,,,,security,,,,,,27,,,,,"Parent ticket for security. Wiki and discussion is here:
https://cwiki.apache.org/confluence/display/KAFKA/Security",,akauppi,anderius,aperepel,apsaltis,bayroot22,becket_qin,benoyantony,blue20080,bosco,breton.g,chaitanyap,daisuke.kobayashi,dajac,dapengsun,davelatham,dbeech,devlinse,dlyle,douglasjose,dsimonto,eronwright,fengxiaoiie,forsberg,galak,geetasg,guozhang,gwenshap,hodgesz,hongyu.bi,iand675,idcmp,ijuma,jahubba,jcnnghm,jfarrell,jghoman,jkreps,jonbringhurst,junrao,kdkavanagh,krisden,liqusha,manishnema,marcin.kuthan,mdonsky,melanchenko,mgaffney,mhausenblas,mherstine,nachomezzadra,natalya,neelesh77,ransom,rmarshasatx,rnirmal,sandonjacobs,sathish.sgr,shrikantpatel,skuehn,smeder,speaktoraghav,spratte,sravya,sriharsha,tbenton,tgraves,vanyatka,vybs,wuti1609,ybarraud,zakkirkharim,,,,,,,,,ATLAS-82,,,,,,,,,,,,,,,,,KAFKA-1810,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-03-16 00:56:30.894,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 12 05:13:08 UTC 2015,,,,,,,"0|i20wmn:",9223372036854775807,,,,,,,,,,,,,,,,"16/Mar/15 00:56;junrao;Just so that everyone knows the sequencing and who is working on what. The following is a summary of the jiras that are being actively worked on.

1. KAFKA-1809 (multi-port), by [~gwenshap].
2. KAFKA-1928 (reuse network code in o.a.k.c.n in server), by [~gwenshap], depending on KAFKA-1809.
3. KAFKA-1684 (SSL), by [~sriharsha], depending on KAFKA-1928.
4. KAFKA-1686 (SASL), by [~sriharsha], depending on KAFKA-1928.","16/Mar/15 01:14;sriharsha;Also
5. KAFKA-1688(Authorization), by [~parth.brahmbhatt] , depending on KAFKA-1684 & KAFKA-1686","13/May/15 18:47;junrao;3. The SSL work is now being done in KAFKA-1690, on top of the network code in o.a.k.c.n.","04/Jun/15 19:20;guozhang;[~junrao] I am wondering if KAFKA-1927 needs to be done before KAFKA-1684 or can be done in parallel?","04/Jun/15 19:33;sriharsha;[~guozhang] I don't think KAFKA-1927 should block KAFKA-1684 or KAFKA-1690. 1927 is application protocol changes its not going to affect the work done in 1684 and 1690 which is at socketchannel and network level.","05/Jun/15 15:51;guozhang;[~sriharsha] That makes sense, thanks!","27/Jul/15 15:17;ijuma;One of the in-scope items in the wiki page is ""Auditing"". Is that information up to date and if so, is there a ticket for it? All the other items seem to be covered by the subtasks associated to this ticket.","27/Jul/15 15:26;sriharsha;[~ijuma] Here it is https://issues.apache.org/jira/browse/KAFKA-2162","27/Jul/15 15:28;ijuma;[~harsha_ch], thanks for the link and for making KAFKA-2162 a sub-task of this ticket.","12/Oct/15 05:07;zakkirkharim;i am  trying to find out whether TLS based authentication and the authorization outlined here is now part of the latest kaffa release. 
Does ""Resolved"" for one ""subtask"" means it has gone to a release ? Or all the sub tasks need to be ""REsolved"" only then we can expect all the code changes in the latest release? 
","12/Oct/15 05:13;gwenshap;Note that each subtask has ""fix version"" in its details. This indicates which version includes the fix.

In this case all subtasks are marked with ""0.9.0.0"", which is a future release. So, no existing release includes the security patches, those will be part of the next release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a configurable timeout for NetworkClient.send,KAFKA-2261,12836907,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,,glenn-sontheimer,glenn-sontheimer,10/Jun/15 18:14,11/Jun/15 00:53,12/Jan/21 10:06,11/Jun/15 00:53,0.8.2.0,,,,,,,,,,clients,,,,,,0,,,,,"Currently once a message has been submitted asynchronously (only option for 0.8.2) there is a possibility that the message could remain in the submission state and never initiate the callback.  There are have been several iterations of the code (in previous versions) to help address this issue.  However these changes handle specific scenarios known as of each point in time, e.g. successful sends and node disconnects.  Additional failure scenarios may exist and/or be introduced in future iterations of the code base.  A fail safe mechanism seems appropriate in this situation while work continues to cover known and discovered scenarios.  

Adding a configuration to allow the client application to specify a timeout for the message send provides the following advantages:
1.  The client application will be guaranteed that a callback will be performed for every message.
2.  The interaction with the Kafka Queue can be better tuned to the application's needs.  In some cases a shorter timeout will be necessary to ensure data does not become too stale.

 



",,becket_qin,glenn-sontheimer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-06-11 00:53:53.624,,,false,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,Thu Jun 11 00:53:53 UTC 2015,,,,,,,"0|i2fw4v:",9223372036854775807,,,,,,,,,,,,,,,,"11/Jun/15 00:53;becket_qin;This is duplicate to KAFKA-2120 and KIP-19 addressed it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support multiple data directories,KAFKA-188,12530462,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,05/Nov/11 23:39,30/May/15 00:45,12/Jan/21 10:06,02/Nov/12 19:02,,,,,,,,0.8.0,,,,,,,,,0,,,,,"Currently we allow only a single data directory. This means that a multi-disk configuration needs to be a RAID array or LVM volume or something like that to be mounted as a single directory.

For a high-throughput low-reliability configuration this would mean RAID0 striping. Common wisdom in Hadoop land has it that a JBOD setup that just mounts each disk as a separate directory and does application-level balancing over these results in about 30% write-improvement. For example see this claim here:
  http://old.nabble.com/Re%3A-RAID-vs.-JBOD-p21466110.html

It is not clear to me why this would be the case--it seems the RAID controller should be able to balance writes as well as the application so it may depend on the details of the setup.

Nonetheless this would be really easy to implement, all you need to do is add multiple data directories and balance partition creation over these disks.

One problem this might cause is if a particular topic is much larger than the others it might unbalance the load across the disks. The partition->disk assignment policy should probably attempt to evenly spread each topic to avoid this, rather than just trying keep the number of partitions balanced between disks.",,chenshangan521@163.com,darion,fullung,jcreasy,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-281,,,,,,,,,,,,,,,"25/Oct/12 17:53;jkreps;KAFKA-188-v2.patch;https://issues.apache.org/jira/secure/attachment/12550816/KAFKA-188-v2.patch","25/Oct/12 18:01;jkreps;KAFKA-188-v3.patch;https://issues.apache.org/jira/secure/attachment/12550820/KAFKA-188-v3.patch","25/Oct/12 23:31;jkreps;KAFKA-188-v4.patch;https://issues.apache.org/jira/secure/attachment/12550887/KAFKA-188-v4.patch","29/Oct/12 03:38;jkreps;KAFKA-188-v5.patch;https://issues.apache.org/jira/secure/attachment/12551150/KAFKA-188-v5.patch","29/Oct/12 22:11;jkreps;KAFKA-188-v6.patch;https://issues.apache.org/jira/secure/attachment/12551256/KAFKA-188-v6.patch","30/Oct/12 18:02;jkreps;KAFKA-188-v7.patch;https://issues.apache.org/jira/secure/attachment/12551389/KAFKA-188-v7.patch","31/Oct/12 18:31;jkreps;KAFKA-188-v8.patch;https://issues.apache.org/jira/secure/attachment/12551601/KAFKA-188-v8.patch","12/Oct/12 05:09;jkreps;KAFKA-188.patch;https://issues.apache.org/jira/secure/attachment/12548855/KAFKA-188.patch",,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2012-07-18 19:47:26.602,,,false,,,,,,,,,,,,,,,,,,216200,,,Sat May 30 00:45:14 UTC 2015,,,,,,,"0|i029hj:",11138,,,,,,,,,,,,,,,,"18/Jul/12 18:03;jkreps;Here are a few additional thoughts on this:
1. This is actually a lot more valuable after kafka 0.8 is out since we will already allow replication at a higher level so the raid is less desirable. Patch should definitely be on 0.8 branch, though it will likely be the same for 0.7.
2. It is worth deciding if we want to support unbalanced disk sizes and speeds. E.g. if you have a 7.2k RPM drive and a 10k rpm drive will we allow you to balance over these? I recommend we skip this for now, we can always do it later. So like with RAID, we will treat all drives equally.
3. I think the only change will be in LogManager. Instead of a single config.logDir parameter we will need logDirs, an array of directories. In createLog() we will need a policy that chooses the best disk on which to place the new log-partition.
4. There may be a few other places that assume a single log directory, may have to grep around and check for that. I don't think their is to much else, as everything else should interact through LogManager and once the Log instance is created it doesn't care where it's home directory is.

One approach to placement would be to always create new logs on the ""least loaded"" directory. The definition of ""least loaded"" is likely to be a heuristic. There are two things we want to balance (1) data size, (2) i/o throughput. If the retention policy is based on time, then size is a good proxy for throughput. However you could imagine having one log with very small retention size but very high throughput. Another problem is that the usage may change over time, and migration is not feasable. For example a new feature going through a ramped rollout might produce almost no data at first and then later produce gobs of data. Furthermore you might get odd results in the case where you manually pre-create many topics all at once as they would all end up on whichever directory had the least data.

I think a better strategy would be to not try to estimate the least-loaded partition and instead just do round-robin assignment (e.g. logDirs(counter.getAndIncrement() % logDirs.length)). The assumption would be that the number of partitions is large enough that  each topic has one partition on each disk.

Either of these strategies has a corner case if all data goes to a single topic (or one topic dominates the load distribution), and that topic has (say) 5 local partitions and 4 data directories. In this case one directory will get 2x the others. However this corner case could be worked around by carefully aligning the partition count and the total number of data directories, so I don't think we need to handle it here.","18/Jul/12 19:47;jcreasy;I started the implementation and my code looks much like you have described. I am now to the point of determining which data location to use. I am planning on doing a round-robin assignment for each partition. 

So, with 4 data dirs and the following topic/partition scheme:

topic1 - 2 partitions
topic2 - 4 partitions
topic3 - 1 partition
topic4 - 2 partitions

disk1 = topic1/1, topic2/3, topic4/2
disc2 = topic1/2, topic2/4
disc3 = topic2/1, topic3/1
disc4 = topic2/2, topic4/1

This is a good first step, we may want to later add re-balancing code based on metrics so that the ""produced/consumed messages per second"" are roughly balanced per disk. This may or may not be feasible and valuable and isn't really that important in this initial implementation.","31/Jul/12 22:24;jcreasy;Sorry, I meant to have this patch in sooner but got quite busy, I expect to have it soon.","29/Sep/12 23:30;jkreps;Hey Jonathan, I am working on some log stuff, mind if I take a shot at this too while I am in there?","02/Oct/12 02:31;jcreasy;Yes please, I have the code written, and I even tested it a little, but I've been able to spend no time on it in a few weeks. It's probably not done yet, but it's close.","02/Oct/12 15:53;jkreps;Do you want to just upload whatever you've got in whatever state its in and I can use that as a starting point?","12/Oct/12 05:09;jkreps;Okay, took a shot at this. Attached is a draft patch. It needs some tests specific to the log selection.

There is a hitch, which I discuss below.

The basic change is
1. Change KafkaConfig.logDir to logDirectories and take a CSV of paths (I still support the old setting as a fallback)
2. Move the whole clean shutdown marker file thing into the LogManager. I feel it should have always been inside LogManager since it is an implementation detail of log recovery. Now there is one such file per log directory.
3. Create new logs in the directories in a round-robin fashion. To do this I keep a counter in LogManager that controls the index of the dir in which we will next create a new log. Initialize this to a random dir index. Each time we create a new index use the dir this points at and then increment it.

The hitch is the high watermark file. Currently we keep it in log.dir. But what directory should we keep it in when there are multiple log directories? I hackily just use the first. However this creates a dependency on the order of the log dirs in the config, which is not ideal. If we sort them and use the dir that is alphabetically first then if we add new directories that will mess it up (whereas if we hadn't sorted we could have tacked them on the end).

Some options:
0. What I currently do, just use the first directory for the hwmark file.
1. Add a new configuration property, metadataDir and use this for the highwatermark file and the clean shutdown marker and any future persistent thing. Downside of this is that it requires a new config parameter the user has to set up.
2. Require a top level directory for all log directories. e.g. 
   all_logs/
      log_dir_1/
        my_topic-1/
        my_topic-2
      log_dir_2/
        ...
Obviously since the goal is to support multiple independently mounted disks and you might not control where they mount to you might have to use soft links. From a previous life I am remembering lots of pain related to java and softlinks.

I would like to get people's feedback on this.","14/Oct/12 00:25;junrao;Thanks for the patch. A couple of comments:

1. It's probably better to have a separate high watermark file per dir for partitions assigned to it. That way, if a disk is damaged, we only lose the high watermarks and the data for partitions on that disk.

2. About data balancing, in the normal case, assigning partitions in a round-robin way to log dirs is fine. However, if a disk is damaged and is replaced, initially there is no data on it. The round-robin approach would mean that the newly replaced disk will always have fewer partitions that other disks. The same issue can occur if a new dir is added in the configuration. An alternative approach is to assign a new partition to the dir with the fewest partitions, which would alleviate this issue.","14/Oct/12 22:02;jkreps;Yeah, I think I agree. It's a little more complex, be we can do a ""least loaded"" thing.","15/Oct/12 21:23;nehanarkhede;+1 on maintaining one highwatermark file per log directory. This file can contain highwatermarks for partitions that live in that log directory. One way of doing this is by maintaining the partition->highwatermark mapping per log directory inside the ReplicaManager and then just dumping that to the respective log directory's .highwatermark file by the checkpointing thread.","25/Oct/12 17:53;jkreps;Updated patch:
1. Split HWM file per data directory
2. Move to a ""least partitions"" partition assignment strategy
3. Add a unit test for the assignment strategy

I think I may have also fixed the transient failure in LogManager.testTimeBasedFlush, though it remains a time-bomb due to its reliance on the scheduler and wall-clock time.

One thing to think about is that the use of ""least loaded"" does have a few corner cases of its own. In general it won't differ much from round robin. The case where it will differ is the case where we add a new data directory to an existing server or lose a single data directory on a server. In this case ALL new partitions will be created in the empty data directory until it becomes full. The problem this could create  is that any new topics created during this time period will have all partitions assigned to the empty data dir. This may lead to imbalance of load. I think despite this, this strategy is better than (1) round robin, (2) RAID, or (3) something more complicated we might think of now.

This patch is ready for review.","25/Oct/12 18:01;jkreps;Wups, missed two minor changes in that last patch.","25/Oct/12 23:31;jkreps;Attached an updated patch with a few very minor changes:
1. If there is only a single log directory I skip the least loaded calculation. This calculation iterates over all logs so it could be a bit expensive in cases where we have very many logs (though it should be rare). This makes it so that the case where we have only one directory is no more expensive then it is now.
2. Fixed a deprecation warning
3. Fixed an outdated scaladoc comment","26/Oct/12 02:25;nehanarkhede;Took a quick look at patch v4. Here are few review comments -

1. KafkaConfig

We should probably raise an error if the same log directory name was specified more than once.

2. LogManager

2.1. I see you have a check in createLogIfNotExists to return if the log is already created. I guess this will happen if two threads execute the following at the same time and enter createLogIfNotExists one after the other.
    logs.get(topicAndPartition) match {
      case null => createLogIfNotExists(topicAndPartition)

I wonder if it is useful to move the lock to getOrCreateLog instead ? Also, shouldn't we use the same lock to protect other accesses to the ""logs"" data structure (getLog(), allLogs() and topics()) ?

2.2. Fix typo on nextLogDir ""chose the"" -> ""choose the""

3. ReplicaManager

3.1 Does it make sense to handle the absence of a matching Log for a topic partition correctly, instead of assuming the presence of one through the get API on an Option ?

3.2 Nit pick -> ""highwater marks"" -> ""high watermarks"" or ""highwatermarks"" ? :)

4. HighwatermarkCheckpoint

4.1 While you're in there, do you mind changing the following API to take in a map of TopicAndPartition->Long instead ? We've been bitten by scala bugs that don't handle equality on tuples very well.
  def write(highwaterMarksPerPartition: Map[(String, Int), Long])
","29/Oct/12 03:38;jkreps;New patch, rebased and addresses Neha's comments:

1. Good thought. Added a check in LogManager to detect duplicate data directories. This is not the only bad possibility though. It is possible to have another kafka process that has opened using the same data directory. I am not sure what would happen, but something bad. I added a per data-directory file lock to check for this. This adds a new file .lock to each data directory and uses it to do the equivalent of flock/funlock. This will lock access across processes or within a process.

2.1. I agree this is a bit roundabout. The reason is that the logs are in a Pool, which is really a ConcurrentHashMap. These are nice as they don't lock the whole hash table on each lookup. So I think although it is a little more verbose it is better how it is because in the common case (fetching a log) there is no global lock needed. This should also make the other accesses threadsafe.

2.2. Learned to spell Choose. :-)

3.1. I don't really understand this code that well, so I am not sure. If it is a programming error to for there not to be a log present then I would rather leave it (I think you would get the NoSuchElementException and it would be clear what happened). The reason is that adding a match/case statement in the middle of that groupby is going to make it awfully hard to understand.

3.2. Fixed, nice catch.

4.1. Done.

Also:
1. Re-arranged methods in LogManager to make a little more sense.","29/Oct/12 16:56;junrao;Thanks for patch v5. Some more comments:

50. LogManager.nextLogDir(): zeros should only include dirs not already used, right? Currently, it seems to include all log dirs.

51. ReplicaManager.checkpointHighWatermarks(): When handling a leaderAndIsr request, we first create a partition and then create a local replica (which creates the local log). So, there is a slight possibility that a partition in allPartitions may not have a local log. The simplest way is to ignore such partition when checkpointing HW.

52. VerifiableProperties:  The following constructor doesn't seem to be used.
def this() = this(new Properties)
","29/Oct/12 18:06;jkreps;Okay, you and Neha are giving conflicting advice on ReplicaManager. Can I omit the case where there is no replica? That is what is complicating this method, because if there is no replica then will there be a log to get the parent directory from?

 If so then I am left with:

  def checkpointHighWatermarks() {
    val replicas = allPartitions.values.map(_.getReplica(config.brokerId)).collect{case Some(replica) => replica}
    val replicasByDir = replicas.filter(_.log.isDefined).groupBy(_.log.get.dir.getParent)
    for((dir, reps) <- replicasByDir) {
      val hwms = reps.map(r => (TopicAndPartition(r.topic, r.partitionId) -> r.highWatermark)).toMap
      highWatermarkCheckpoints(dir).write(hwms)
    }
  }

which is much simpler. lmk.
","29/Oct/12 22:11;jkreps;Okay this patch addresses Jun's comments:

50. The zeros are actually correct, basically I am initializing to 0 and ten overwriting with the count if there is one. The goal is to ensure that there is an entry for each directory even if it has no logs (otherwise it would never get any logs assigned). It is possible to do this with some kind of case statement, but I think this is more readable.

51. Okay I used the logic above. The logic is now slightly different from what was there before. Now I filter any partition which has no replica from the file. I also filter any replica which has no log, though my understanding is that that shouldn't happen.

52. Left this. The idea is that previously for tests you could do new Properties so it makes sense to be able to do new VerifiableProperties. Not essential so happy either way.","30/Oct/12 14:53;junrao;Our system tests fail with the latest patch. 

python -B system_test_runner.py 2>&1 | tee test.out

Saw the following in broker log.

[2012-10-30 07:40:19,682] FATAL Fatal error during KafkaServerStable startup. Prepare to shutdown (kafka.server.KafkaServerStartable)
java.io.IOException: No such file or directory
        at java.io.UnixFileSystem.createFileExclusively(Native Method)
        at java.io.File.createNewFile(File.java:883)
        at kafka.utils.FileLock.<init>(FileLock.scala:12)
        at kafka.log.LogManager$$anonfun$10.apply(LogManager.scala:64)
        at kafka.log.LogManager$$anonfun$10.apply(LogManager.scala:64)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34)
        at kafka.log.LogManager.<init>(LogManager.scala:64)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:60)
        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:34)
        at kafka.Kafka$.main(Kafka.scala:46)
        at kafka.Kafka.main(Kafka.scala)
[2012-10-30 07:40:19,683] INFO [Kafka Server 1], shutting down (kafka.server.KafkaServer)
","30/Oct/12 18:02;jkreps;Looks like previously if you configured a data directory that didn't exist we created it for you. This behavior was broken since now we try to acquire a lock first.

This patch addresses that problem with the following changes:
0. Rebased
1. Cleaned up the LogManager initialization so that we create and validate directories before locking them. This fixes the issue.
2. It looks like the nio FileLock.tryLock has different behvaiors depending on whether the lock is held by your process or another process. This could lead to getting a bad error message if we failed to lock the data directory. Fixed this in our wrapper class.","30/Oct/12 22:23;junrao;Now, some unit tests fail with exceptions like the following. Most of them seem to be transient, but they show up more frequently now.

[0m[[31merror[0m] [0mTest Failed: testCleanShutdown(kafka.server.ServerShutdownTest)[0m
kafka.common.KafkaException: Failed to acquire lock on file .lock in /tmp/kafka-246675. A Kafka instance in another process or thread is using this directory.
","31/Oct/12 18:31;jkreps;This failing test was due to a real bug in LogManager.shutdown that lead to the locks not being released. I fixed this bug. In addition:
1. Rebased again
2. Cleaned up ServerShutdownTest and added comments since I spent a lot of time puzzling over this test to figure out what it was doing.
3. Add Utils.swallow to each shutdown call in KafkaServer.shutdown--otherwise any failure prevents the rest of the shutdown.","02/Nov/12 00:51;junrao;+1 on patch v8. Thanks,","02/Nov/12 19:02;jkreps;Checked in rebased version of v8. I did see one failure in the system tests, but chased it down and it was due to KAFKA-593.","07/Nov/14 09:15;chenshangan521@163.com;assign a new partition to the dir with the fewest partitions, it works fine if all of the partitions have most or less the same size. But if partition size varies quite a lot, it will cause disk usage imbalance. So it's better to take the disk usage into account.","07/Nov/14 18:00;jkreps;[~chenshangan521@163.com] see my comment earlier on this ticket on why assigning to the partitions with the smallest data size has a couple of really bad gotchas. That was why we went with fewest partitions.","29/May/15 11:42;chenshangan521@163.com;[~jkreps]  I think we could provide an alternative, user can choose either one: Partitions determined or segments determined.","29/May/15 18:28;jkreps;@chenshangan The issue with using data size was that it is very very common to create a bunch of topics as once. When you do this all new partitions will be put on the same least full partition. Then when data starts being written that partition will be totally overloaded.

We can make this configurable, but I think almost anyone who chooses that option will get bit by it.

I recommend we instead leave this as it is for initial placement and implement ""rebalancing"" option that actively migrates partitions to balance data between directories. This is harder to implement but I think it is what you actually want.","30/May/15 00:45;chenshangan521@163.com;[~jkreps] 
""I recommend we instead leave this as it is for initial placement and implement ""rebalancing"" option that actively migrates partitions to balance data between directories. This is harder to implement but I think it is what you actually want.""

Exactly, this is what I really want, but it's pretty hard to implement. And in our use case, we seldom create a bunch of topics at the same time, topics are increasing day by day.

Common use case:
1. a new kafka cluster setup, lots of topics from other kafka cluster or system dump data into this new cluster. segments determined policy works well as all topics are started from zero, so segments are consistent with partitions.

2. an existing  kafka cluster, topics are added day by day. This is the ideal case, segments policy will work well. 

3. an existing kafka cluster, topics are added in bunch. It might cause all new topics being put on the same least directory, of course it will cause bad consequence. But if the cluster is big enough and disk counts and capacity of a broker is big enough, and this is not a common use case, the consequence will not be so serious. Users use this option should consider how to avoid such situation.

Above all, it's worthy providing such an option. But If we can implement a ""rebalancing"" option, it would be perfect.   
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic Configuration via ZK,KAFKA-2204,12831208,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,aauradkar,aauradkar,aauradkar,19/May/15 23:11,19/May/15 23:11,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,Parent ticket to track all jiras for dynamic configuration via zookeeper.,,aauradkar,fpj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-05-19 23:11:03.0,,,,,,,"0|i2eyjz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Governor on concurrent replica reassignments,KAFKA-1677,12746206,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,granders,rberdeen,rberdeen,06/Oct/14 17:51,28/Feb/15 18:59,12/Jan/21 10:06,,,,,,,,,,,,controller,,,,,,0,newbie++,,,,"We have seen a cluster be killed via too many concurrent partition transfers. An ideal solution is a configuration setting to limit the number of concurrent transfers per host (dynamically tunable). (eg: transfer_limit defined in http://docs.basho.com/riak/latest/ops/advanced/configs/configuration-files/#Ring).

To work around this, we generate our assignments, then use a tool to feed the reassignments in small batches.

The size of the batch is based on either
* *the number partitions*, e.g., reassign all replicas for the first 2 partitions that have any moves
* *the number of individual replica moves*, e.g. when reassigning \[1,2,3,4] to \[5,6,7,8], first reassign to \[5,6,3,4] then reassign to \[5,6,7,8]",,bobrik,jkreps,rberdeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-10-06 23:13:39.427,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 06 23:13:39 UTC 2014,,,,,,,"0|i20ulr:",9223372036854775807,,,,,,,,,,,,,,,,"06/Oct/14 23:13;jkreps;Agreed, LinkedIn is doing a similar thing. This should definitely be done automatically.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a shutdownNow() call to new producer,KAFKA-1934,12773328,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,becket_qin,becket_qin,becket_qin,08/Feb/15 22:27,09/Feb/15 01:30,12/Jan/21 10:06,09/Feb/15 01:30,,,,,,,,,,,,,,,,,0,,,,,We have a use case where user want to stop send any more messages if an error occurred on a previous send. Otherwise the message order might be broken. The shutdownNow() call will stop the producer right away without draining the messages in accumulator.,,becket_qin,ewencp,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1660,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-02-09 01:09:45.711,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 09 01:19:38 UTC 2015,,,,,,,"0|i25cd3:",9223372036854775807,,,,,,,,,,,,,,,,"09/Feb/15 01:09;jkreps;This is a public interface change so we should do a quick KIP and discuss.

I don't think it makes sense to add a shutdownNow() method given the existing method we have is close(), if anything it would be closeNow(). I would propose instead adding close(long timeout, TimeUnit unit) where passing in 0 is the equivalent of immediate shutdown. This is more general and actually I think the better thing to use since generally you want to give some time for a graceful shutdown before dropping data.","09/Feb/15 01:19;ewencp;There was previous discussion of abort() and tryClose(timeout) in https://issues.apache.org/jira/browse/KAFKA-1659 and https://issues.apache.org/jira/browse/KAFKA-1660, including a patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow high performance SimpleConsumer use cases to still work with new Kafka 0.9 consumer APIs,KAFKA-1655,12744570,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Invalid,,valentin,valentin,28/Sep/14 22:47,08/Feb/15 00:23,12/Jan/21 10:06,08/Feb/15 00:23,0.10.1.0,,,,,,,,,,consumer,,,,,,0,,,,,"Hi guys,

currently Kafka allows consumers to either chose the low level or the high level API, depending on the specific requirements of the consumer implementation. However, I was told that the current low level API (SimpleConsumer) will be deprecated once the new Kafka 0.9 consumer APIs are available.

In this case it would be good, if we can ensure that the new API does offer some ways to get similar performance for use cases which perfectly fit the old SimpleConsumer API approach.

Example Use Case:
A high throughput HTTP API wrapper for consumer requests which gets HTTP REST calls to retrieve data for a specific set of topic partitions and offsets.
Here the SimpleConsumer is perfect because it allows connection pooling in the HTTP API web application with one pool per existing kafka broker and the web application can handle the required metadata managment to know which pool to fetch a connection for, for each used topic partition. This means connections to Kafka brokers can remain open/pooled and connection/reconnection and metadata overhead is minimized.

To achieve something similar with the new Kafka 0.9 consumer APIs, it would be good if it could:
- provide a lowlevel call to connect to a specific broker and to read data from a topic+partition+offset
OR
- ensure that subscribe/unsubscribe calls are very cheap and can run without requiring any network traffic. If I subscribe to a topic partition for which the same broker is the leader as the last topic partition which was in use for this consumer API connection, then the consumer API implementation should recognize this and should not do any disconnects/reconnects and just reuse the existing connection to that kafka broker.
Or put differently, it should be possible to do external metadata handling in the consumer API client and the client should be able to pool consumer API connections effectively by having one pool per Kafka broker.

Greetings
Valentin",,changsi0621,hodgesz,jkreps,valentin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-01-23 19:03:20.837,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 23 19:03:20 UTC 2015,,,,,,,"0|i20kjb:",9223372036854775807,,nehanarkhede,,,,,,,,,,,,,,"28/Sep/14 22:49;valentin;mailing list thread which led to the creation of this ticket:
{code}
On Sat, 27 Sep 2014 08:31:01 -0700, Jun Rao <junrao@gmail.com> wrote:
> Valentin,
> 
> That's a good point. We don't have this use case in mind when designing the
> new consumer api. A straightforward implementation could be removing the
> locally cached topic metadata for unsubscribed topics. It's probably
> possible to add a config value to avoid churns in caching the metadata.
> Could you file a jira so that we can track this?
> 
> Thanks,
> 
> Jun
> 
> On Thu, Sep 25, 2014 at 4:19 AM, Valentin <kafka-9999-vw@sblk.de> wrote:
> 
>>
>> Hi Jun, Hi Guozhang,
>>
>> hm, yeah, if the subscribe/unsubscribe is a smart and lightweight
>> operation this might work. But if it needs to do any additional calls to
>> fetch metadata during a subscribe/unsubscribe call, the overhead could
>> get
>> quite problematic. The main issue I still see here is that an additional
>> layer is added which does not really provide any benefit for a use case
>> like mine.
>> I.e. the leader discovery and connection handling you mention below don't
>> really offer value in this case, as for the connection pooling approach
>> suggested, I will have to discover and maintain leader metadata in my own
>> code anyway as well as handling connection pooling. So if I understand
>> the
>> current plans for the Kafka 0.9 consumer correctly, it just doesn't work
>> well for my use case. Sure, there are workarounds to make it work in my
>> scenario, but I doubt any of them would scale as well as my current
>> SimpleConsumer approach :|
>> Or am I missing something here?
>>
>> Greetings
>> Valentin
>>
>> On Wed, 24 Sep 2014 17:44:15 -0700, Jun Rao <junrao@gmail.com> wrote:
>> > Valentin,
>> >
>> > As Guozhang mentioned, to use the new consumer in the SimpleConsumer
>> way,
>> > you would subscribe to a set of topic partitions and the issue poll().
>> You
>> > can change subscriptions on every poll since it's cheap. The benefit
>> > you
>> > get is that it does things like leader discovery and maintaining
>> > connections to the leader automatically for you.
>> >
>> > In any case, we will leave the old consumer including the
>> > SimpleConsumer
>> > for sometime even after the new consumer is out.
>> >
>> > Thanks,
>> >
>> > Jun
>> >
>> > On Tue, Sep 23, 2014 at 12:23 PM, Valentin <kafka-9999-vw@sblk.de>
>> wrote:
>> >
>> >> Hi Jun,
>> >>
>> >> yes, that would theoretically be possible, but it does not scale at
>> all.
>> >>
>> >> I.e. in the current HTTP REST API use case, I have 5 connection pools
>> on
>> >> every tomcat server (as I have 5 brokers) and each connection pool
>> holds
>> >> upto 10 SimpleConsumer connections. So all in all I get a maximum of
>> >> 50
>> >> open connections per web application server. And with that I am able
>> >> to
>> >> handle most requests from HTTP consumers without having to open/close
>> >> any new connections to a broker host.
>> >>
>> >> If I would now do the same implementation with the new Kafka 0.9 high
>> >> level consumer, I would end up with >1000 connection pools (as I have
>> >> >1000 topic partitions) and each of these connection pools would
>> contain
>> >> a number of consumer connections. So all in all, I would end up with
>> >> thousands of connection objects per application server. Not really a
>> >> viable approach :|
>> >>
>> >> Currently I am wondering what the rationale is for deprecating the
>> >> SimpleConsumer API, if there are use cases which just work much better
>> >> using it.
>> >>
>> >> Greetings
>> >> Valentin
>> >>
>> >> On 23/09/14 18:16, Guozhang Wang wrote:
>> >> > Hello,
>> >> >
>> >> > For your use case, with the new consumer you can still create a new
>> >> > consumer instance for each  topic / partition, and remember the
>> mapping
>> >> of
>> >> > topic / partition => consumer. The upon receiving the http request
>> you
>> >> can
>> >> > then decide which consumer to use. Since the new consumer is single
>> >> > threaded, creating this many new consumers is roughly the same cost
>> >> > with
>> >> > the old simple consumer.
>> >> >
>> >> > Guozhang
>> >> >
>> >> > On Tue, Sep 23, 2014 at 2:32 AM, Valentin <kafka-9999-vw@sblk.de>
>> >> > wrote:
>> >> >
>> >> >>
>> >> >> Hi Jun,
>> >> >>
>> >> >> On Mon, 22 Sep 2014 21:15:55 -0700, Jun Rao <junrao@gmail.com>
>> wrote:
>> >> >>> The new consumer api will also allow you to do what you want in a
>> >> >>> SimpleConsumer (e.g., subscribe to a static set of partitions,
>> >> >>> control
>> >> >>> initial offsets, etc), only more conveniently.
>> >> >>
>> >> >> Yeah, I have reviewed the available javadocs for the new Kafka 0.9
>> >> >> consumer APIs.
>> >> >> However, while they still allow me to do roughly what I want, I
>> >> >> fear
>> >> that
>> >> >> they will result in an overall much worse performing implementation
>> on
>> >> my
>> >> >> side.
>> >> >> The main problem I have in my scenario is that consumer requests
>> >> >> are
>> >> >> coming in via stateless HTTP requests (each request is standalone
>> and
>> >> >> specifies topics+partitions+offsets to read data from) and I need
>> >> >> to
>> >> find a
>> >> >> good way to do connection pooling to the Kafka backend for good
>> >> >> performance. The SimpleConsumer would allow me to do that, an
>> approach
>> >> with
>> >> >> the new Kafka 0.9 consumer API seems to have a lot more overhead.
>> >> >>
>> >> >> Basically, what I am looking for is a way to pool connections per
>> >> >> Kafka
>> >> >> broker host, independent of the topics/partitions/clients/..., so
>> each
>> >> >> Tomcat app server would keep N disjunctive connection pools, if I
>> >> >> have N
>> >> >> Kafka broker hosts.
>> >> >> I would then keep some central metadata which tells me which hosts
>> are
>> >> the
>> >> >> leaders for which topic+partition and for an incoming HTTP client
>> >> request
>> >> >> I'd just take a Kafka connection from the pool for that particular
>> >> broker
>> >> >> host, request the data and return the connection to the pool. This
>> >> >> means
>> >> >> that a Kafka broker host will get requests from lots of different
>> end
>> >> >> consumers via the same TCP connection (sequentially of course).
>> >> >>
>> >> >> With the new Kafka consumer API I would have to
>> subscribe/unsubscribe
>> >> from
>> >> >> topics every time I take a connection from the pool and as the
>> request
>> >> may
>> >> >> need go to a different broker host than the last one, that wouldn't
>> >> >> even
>> >> >> prevent all the connection/reconnection overhead. I guess I could
>> >> >> create
>> >> >> one dedicated connection pool per topic-partition, that way
>> >> >> connection/reconnection overhead should be minimized, but that way
>> I'd
>> >> end
>> >> >> up with hundreds of connection pools per app server, also not a
>> >> >> good
>> >> >> approach.
>> >> >> All in all, the planned design of the new consumer API just doesn't
>> >> >> seem
>> >> >> to fit my use case well. Which is why I am a bit anxious about the
>> >> >> SimpleConsumer API being deprecated.
>> >> >>
>> >> >> Or am I missing something here? Thanks!
>> >> >>
>> >> >> Greetings
>> >> >> Valentin
{code}","23/Jan/15 19:03;jkreps;I believe this is handled in the new consumer API. Can you take a look at the APIs and see what you think. Essentially you would do something like

{code}
consumer.subscribe(topic, partition)
consumer.seek(topic, partition, offset)
records = consumer.poll(timeout)
consumer.unsubscribe(topic, partition)
{code}

The subscribe/unsubscribe and seek is just an in-memory modification.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add audit trail to kafka,KAFKA-260,12540652,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,jkreps,jkreps,jkreps,31/Jan/12 23:58,08/Feb/15 00:10,12/Jan/21 10:06,08/Feb/15 00:10,0.8.0,,,,,,,,,,,,,,,,15,,,,,"LinkedIn has a system that does monitoring on top of our data flow to ensure all data is delivered to all consumers of data. This works by having each logical ""tier"" through which data passes produce messages to a central ""audit-trail"" topic; these messages give a time period and the number of messages that passed through that tier in that time period. Example of tiers for data might be ""producer"", ""broker"", ""hadoop-etl"", etc. This makes it possible to compare the total events for a given time period to ensure that all events that are produced are consumed by all consumers.

This turns out to be extremely useful. We also have an application that ""balances the books"" and checks that all data is consumed in a timely fashion. This gives graphs for each topic and shows any data loss and the lag at which the data is consumed (if any).

This would be an optional feature that would allow you to to this kind of reconciliation automatically for all the topics kafka hosts against all the tiers of applications that interact with the data.

Some details, the proposed format of the data is JSON using the following format for messages:

{
  ""time"":1301727060032,  // the timestamp at which this audit message is sent
  ""topic"": ""my_topic_name"", // the topic this audit data is for
  ""tier"":""producer"", // a user-defined ""tier"" name
  ""bucket_start"": 1301726400000, // the beginning of the time bucket this data applies to
  ""bucket_end"": 1301727000000, // the end of the time bucket this data applies to
  ""host"":""my_host_name.datacenter.linkedin.com"", // the server that this was sent from
  ""datacenter"":""hlx32"", // the datacenter this occurred in
  ""application"":""newsfeed_service"", // a user-defined application name
  ""guid"":""51656274-a86a-4dff-b824-8e8e20a6348f"", // a unique identifier for this message
  ""count"":43634
}

DISCUSSION

Time is complex:
1. The audit data must be based on a timestamp in the events not the time on machine processing the event. Using this timestamp means that all downstream consumers will report audit data on the right time bucket. This means that there must be a timestamp in the event, which we don't currently require. Arguably we should just add a timestamp to the events, but I think it is sufficient for now just to allow the user to provide a function to extract the time from their events.
2. For counts to reconcile exactly we can only do analysis at a granularity based on the least common multiple of the bucket size used by all tiers. The simplest is just to configure them all to use the same bucket size. We currently use a bucket size of 10 mins, but anything from 1-60 mins is probably reasonable.

For analysis purposes one tier is designated as the source tier and we do reconciliation against this count (e.g. if another tier has less, that is treated as lost, if another tier has more that is duplication).

Note that this system makes false positives possible since you can lose an audit message. It also makes false negatives possible since if you lose both normal messages and the associated audit messages it will appear that everything adds up. The later problem is astronomically unlikely to happen exactly, though.

This would integrate into the client (producer and consumer both) in the following way:
1. The user provides a way to get timestamps from messages (required)
2. The user configures the tier name, host name, datacenter name, and application name as part of the consumer and producer config. We can provide reasonable defaults if not supplied (e.g. if it is a Producer then set tier to ""producer"" and get the hostname from the OS).

The application that processes this data is currently a Java Jetty app and talks to mysql. It feeds off the audit topic in kafka and runs both automatic monitoring checks and graphical displays of data against this. The data layer is not terribly scalable but because the audit data is sent only periodically this is enough to allow us to audit thousands of servers on very modest hardware, and having sql access makes diving into the data to trace problems to particular hosts easier.

LOGISTICS
I would recommend the following steps:
1. Add the audit application, the proposal would be to add a new top-level directory equivalent to core or perf called ""audit"" to house this application. At this point it would just be sitting there, not really being used.
2. Integrate these capabilities into the producer as part of the refactoring we are doing now
3. Integrate into consumer when possible

",,ashokg,boniek,bradhill99,cagatayk,criccomini,cscotta,davelatham,diederik,dtardon,eidi,felixgv,genx7up,guydou,jboyd963,jcnnghm,jcreasy,jfilipiak,jkreps,johanl,kamaradclimber,kzadorozhny,l1024,malonem,mcchang,nehanarkhede,noslowerdna,otis,sslavic,staslev,stevenz3wu,suryaprabhakar,theduderog,vjeran@tis.hr,yu.chenjie,zackdever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/12 23:14;jkreps;Picture 18.png;https://issues.apache.org/jira/secure/attachment/12513709/Picture+18.png","09/Mar/12 23:03;jkreps;kafka-audit-trail-draft.patch;https://issues.apache.org/jira/secure/attachment/12517800/kafka-audit-trail-draft.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-02-01 12:12:30.371,,,false,,,,,,,,,,,,,,,,,,226047,,,Mon Oct 27 14:59:39 UTC 2014,,,,,,,"0|i09m0f:",53999,,,,,,,,,,,,,,,,"01/Feb/12 12:12;charmalloc;is ""application"" synonymous with the new wired format ""client_id"" is so we should standardize on one or the other. +1 on the feature.","01/Feb/12 17:18;jkreps;Good point. No, in the sense that I am basically just describing two independent trains of thought, but you are absolutely right that there is no reason to have an application_id and a client_id. So let's pretend that that was the plan all along :-)","07/Feb/12 23:14;jkreps;Attached screenshot of audit app. This shows the aggregate data over a given time range, and graphs any dependencies as well as estimating the lag for loading the data (off the page on the screenshot).","09/Mar/12 23:03;jkreps;Draft patch for adding audit trail app to kafka.","11/Mar/12 14:26;guydou;Hi Jay,

I tried to apply this patch in my dev envoirnment, 

To which versiob should apply it?

I tried it on  kafka-0.7.0-incubating.

If it should work on that version, I couldn't see it on action (It did compile), so maybe I don't get the configuration, can you elaborate on how to configure the producer tier to send AuditData?

Thanks.


","01/Aug/12 02:53;jcreasy;I have the audit ui up and running in my dev environment as soon as I get a chance to patch our producers I should be able to submit a couple of tweaks to this patch for 0.7.1 and 0.8. 

If you have the producer code that generates the audit messages that would be pretty useful!","01/Aug/12 17:39;jkreps;We don't have a producer and consumer that emit the audit data that is open source, that logic currently resides in a linkedin-specific wrapper class. We would be interested in fully integrating this with the open source kafka.

One key question is how to get the timestamp used for auditing. Currently we rely on a special field in the message to get the timestamp. To kafka, of course, messages are just opaque byte[], so integrating this is a little challenging. For our usage we just made this a required field for our avro records. Three options for integration:
1. Support auditing only for messages which contain timestamps. Make the user provide a function for extracting the timestamp from the message if they want to use the audit app.
2. Add a special field to all messages that contains the timestamp that comes from message creation time. The downside of this is that it requires a change to the message format and this field might not be useful for everyone.
3. Add a generic key-value style header, and store the timestamp in that. The downside of having a generic header is that you have to store the key too.

I would probably vote for 2. I think timestamp and auditing is useful enough to argue for making it a top-level field.

We could also implement (1) as a first phase, and then later chose to add the timestamp to making auditing automatic. That might be a better approach to get stuff started easily.","02/Aug/12 18:45;nehanarkhede;I like option 2. Auditing is useful to have out-of-the-box.","27/Nov/12 09:39;l1024;Hi,

i like option 2. I wrote a generic consumer that stores the messages of a topic in small batches in Amazon S3. Everything was really simple (and generic) until i added partitioning based on a timestamp. From my experience in most cases messages/events have a creation time. I would imagine, that there are other 'high level' clients, that could make use of a (producer provided) timestamp and could be implemented much simpler, if they would not have to deal with some sort of timestamp extraction api.

lorenz ","18/Jan/13 04:32;felixgv;It would be possible to have optional timestamps by using the magic byte at the beginning of the Kafka Messages, no? If the Message contains the old (current) magic byte, then there's no timestamp, if it's the new magic byte, then there is a timestamp (without needing a key) somewhere in the header...","06/Jan/14 21:08;ashokg;Hi Jay, 

Is the final kafka-audit code available in kafka 0.8 or somewhere else? I don't see a top level directory with name ""audit"" in the kafka source.

I see the audit mentioned in the documentation at http://kafka.apache.org/documentation.html, however I am not able to find more details about how to configure and use it.","30/Jan/14 09:55;dtardon;Hi all,

Ashok, do you have any news about this kafka-audit topic? Did you resolve your doubts?

I am looking for this kind of feature.

Regards","25/Feb/14 19:13;theduderog;""It also makes false negatives possible since if you lose both normal messages and the associated audit messages it will appear that everything adds up. The later problem is astronomically unlikely to happen exactly, though.""

This may be true once messages have reached a broker.  However, if a producer process were to be killed (say by SIGKILL), both it's unack'ed normal messages and the audit data would be lost.  Would it make sense to persist the audit counts to the file system for producers so that they could potentially be recovered?","27/Oct/14 14:59;staslev;Hi guys,

We've adopted the data model above in Aletheia (https://github.com/outbrain/Aletheia), an open source data delivery framework we've been working on here at Outbrain. 
In Aletheia we call these audit trails ""Breadcrumbs"", and have them generated by the producer and consumer sides. We're working towards integrating the above mentioned patch in order to provide a client side dashboard.

Aletheia is by no means meant to replace Kafka, it is rather an abstraction layer on top of Kafka and other messaging systems, as we point out in the wiki.
Having audit capabilities built into Kafka would be really great, meanwhile, you're most welcome to check out Aletheia, perhaps you'll find it useful as it provides the Breadcrumb generation out of the box.

-Stas",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New producer checklist,KAFKA-1239,12693154,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,nehanarkhede,nehanarkhede,04/Feb/14 17:20,07/Feb/15 23:59,12/Jan/21 10:06,07/Feb/15 23:59,0.8.2.0,,,,,,,,,,producer ,,,,,,0,,,,,"Here is the list of todo items we have for the new producer (in no
particular order):
1. Rename to org.apache.* package
2. Discuss config approach
3. Finalize config approach
4. Add slf4j logging for debugging purposes
5. Discuss metrics approach
6. Add metrics
7. Convert perf test to optionally use new producer
8. Get system tests passing with new producer
9. Write integration tests that test the producer against the real server
10. Expand unit test coverage a bit
11. Performance testing and analysis.
12. Add compression support
13. Discuss and perhaps add retry support
14. Discuss the approach to protocol definition and perhaps refactor a bit
15. Deeper code review
16. Convert mirror maker

This doesn't count general bug fixing which I assume we will do as we find
them.

Let's file subtasks for each of the above, so there is a single place to track what's outstanding on the new producer. ",,nehanarkhede,rtyler,sybrandy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,371740,,,2014-02-04 17:20:00.0,,,,,,,"0|i1s2dz:",372040,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka web console,KAFKA-266,12541709,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,,velvia,velvia,07/Feb/12 22:49,07/Feb/15 23:57,12/Jan/21 10:06,07/Feb/15 23:57,,,,,,,,,,,contrib,,,,,,8,project,,,,"This issue is created to track a community-contributed Kafka Web UI.

Here is an initial list of goals:
- Be able to easily see which brokers are up
- Be able to see lists of topics, connected producers, consumer groups, connected consumers
- Be able to see, for each consumer/partition, its offset, and more importantly, # of bytes unconsumed (== largest offset for partition - current offset)
- (Wish list) have a graphical view of the offsets
- (Wish list) be able to clean up consumer state, such as stale claimed partitions

List of challenges/questions:
- Which framework?  Play! for Scala?
- Is all the data available from JMX and ZK?  Hopefully, watching the files on the filesystem can be avoided....
- How to handle large numbers of topics, partitions, consumers, etc. efficiently",,a.gazzarini,adenysenko,cagatayk,chikim79,claude.mamo,dtardon,eidi,ewhauser,guydou,iskandar,jcreasy,jkreps,junrao,kieren,lanzaa,malonem,millerjam,nehanarkhede,otis,samzer,sslavic,swapnilghike,timveil,velvia,vpernin,zayeem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2419200,2419200,,0%,2419200,2419200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-02-07 23:19:26.059,,,false,,,,,,,,,,,,,,,,,,226996,,,Thu Feb 27 02:43:19 UTC 2014,,,,,,,"0|i029t3:",11190,,,,,,,,,,,,,,,,"07/Feb/12 23:19;jkreps;I think this would be fantastic to have. We don't have such a thing at LinkedIn, we just have hooks into our monitoring system, but I think this would be better.

We do have an application that does a data audit. This basically proves that all data that is sent is received by all interested consumers. See KAFKA-260 for a detailed description and screenshot. We had planned to fold this functionality into the open source project. Since both of these are kind of monitoring/health functionality are pretty closely related, maybe it would make sense to combine them.

The existing app is a java app that implements runs a few jetty servlets to implement a JSON api. The UI just runs off this api and is implemented in hmtl/css/js. The charts are dygraph and it uses jquery for javascript helpers. I am not married to this setup (the original code was written by someone else and just refactored by me), so I would be okay moving it into another mvc framework if there is something much better out there as long as it isn't too complex.","29/Feb/12 18:21;velvia;Jay,

Would it be possible to look at the source of the java app you guys have, or open source it somehow?  It would be great to see how you guys are pulling out the stats, and might save lots of work in doing the web console.

thanks.","09/Mar/12 22:21;jkreps;yeah i will post a patch.","14/Mar/12 14:32;ewhauser;I would like to see a simple monitoring console as something that is embedded within the brokers themselves (ActiveMQ does something similar).  It should have minimal dependencies (jetty, servlets, and Javascript sound appropriate).  The simple HTTP interface should allow for quick checking to see if:

1) Brokers are up
2) Topic are being written to
3) Topics are being consumed

While most of this data is available via JMX already, HTTP is a preferable alternative.  An external application that provides comprehensive management, audit solution seems to make sense as well.","14/Mar/12 16:01;charmalloc;JMX, HTTP are both fine but also hooking into Graphite, Ganglia etc make for a more ubuitious friendly system (for the ops folk) I think KAFKA-203 really address this and a web console that is going to be for monitoring only I think should just pull the JSON from the http connector of the coda hale metrics implementation IMHO ","14/Mar/12 21:16;jkreps;My take is that, as Joe says, most serious folks already have some system they use for monitoring across their stack (ganglia or whatever). To really work operationally you can't try to replace this or add a new system. For this reason, I kind of prefer the web app to be an optional stand-alone thing since it may not be of use to everyone, though i think that complicates its design (potentially). I think the advantage of the web app is custom display of very kafka-specific things (the audit, cluster status, etc).","15/Mar/12 07:24;velvia;The web console would not primarily be for monitoring -- for us anyways -- and I agree it should not replace Ganglia etc.  (Although at some shops - most notably Google - having web routes for every single service and app is a policy).  I think the web console would be invaluable for debugging and status though.  For looking at a snapshot of the system easily. 

Also, standard JMX just doesnt work in EC2, you don't know what ports need to be opened up. 

I think the metric Im most interested in -- number of messages or MB outstanding -- is not available directly from Kafka server itself anyways.  

What is the timeframe for KAFKA-203?","15/Mar/12 07:24;velvia;The web console would not primarily be for monitoring -- for us anyways -- and I agree it should not replace Ganglia etc.  (Although at some shops - most notably Google - having web routes for every single service and app is a policy).  I think the web console would be invaluable for debugging and status though.  For looking at a snapshot of the system easily. 

Also, standard JMX just doesnt work in EC2, you don't know what ports need to be opened up. 

I think the metric Im most interested in -- number of messages or MB outstanding -- is not available directly from Kafka server itself anyways.  

What is the timeframe for KAFKA-203?","19/Jun/12 04:29;velvia;BTW, just some thoughts.  I'm thinking of not so much a web console for individual Kafka brokers, but a web console for an entire Kafka cluster, targetting mostly smaller clusters.   This would grab information from ZK and from JMX (or JMX-over-HTTP) for individual brokers to put together reports like:
- Status of different Kafka brokers in the cluster, including overall read and write rates, etc.
- Information on consumers (at least the high level consumers registered in ZK), outstanding messages or how far behind they are, etc.","31/Jan/13 20:06;guydou;Hi
I started building an admin web app, from which I let the users of the kafka cluster:
1. Watch how much their consumer group is behind a certain offset,
2. Update their topic to latest offset 

I developed it using play!,  it connects the ZK and the Brokers using the RPC, 

Will interest anyone?


 ","31/Jan/13 22:43;nehanarkhede;Absolutely, it seems like a good starting point. Would you mind uploading it as a patch ?","01/Feb/13 07:44;guydou;I am very happy to hear,

I will share you with my code, as soon as I get approval for doing that from my company legal department, I don't think it should be a problem .



 ","30/Apr/13 09:18;samzer;This would be absolutely great to have. Wanted to know, how it has progressed?","25/May/13 10:03;a.gazzarini;Any problem if it will be in Java? I have to explore better Kafka and I think It would be a nice opportunity....unfortunately I'm not so skilled in Scala","28/May/13 05:10;junrao;Andrea, thanks for your interest. The tool can be written in java, if that's more convenient.","13/Jul/13 21:35;swapnilghike;Hey Andrea, Guy, did you make any head way on this? ","18/Feb/14 03:22;millerjam;Hi Folks, 
I have a admin web app that I built for this purpose, it connects to Zk and displays information about brokers, topics, and consumers. 

It really useful, especially when just getting started with Kafka, I would like to contribute it to the this project.

It is a Play App, written in Scala and depends on Playframework, Kafka, and jQuery.  Any concerns about these dependencies?

 Also, what are the preferred next steps, should I attach source or put in public github, not sure how you would like to move forward?

","18/Feb/14 04:48;junrao;James,

Thanks for your interest. Perhaps you can start with a wiki in Kafka that describes your current design and how it looks? As for the code, we can put it in a separate dir and build a separate jar. So, the additional dependency won't affect other existing jars.","18/Feb/14 16:42;jkreps;Hey James,

That's awesome! Yeah I recommend github and adding it to our Ecosystem page. Maintaining outside tools with the main code base turned out to be a real drag for the people working on the tool. The apache process works better (we think) for a focused code base, github works well for a large diverse ecosystem as each person can maintain their piece without blocking code reviews or the need to release in sync with Kafka. So we aren't trying to suck in everything that integrates with Kafka into the main project.","24/Feb/14 21:30;claude.mamo;Adding another one to the list: https://github.com/claudemamo/kafka-web-console. 

The project is as well built on top of Play and uses AngularJS in addition to jQuery. It's using Twitter's Zookeeper client to make non-blocking queries to Zookeeper for brokers, consumers groups, partitions, etc... I forked the high level consumer and replaced its blocking queues with callbacks so that it would be possible to implement topic feeds without being too much of a drain on resources. I'm going to bump the version no. to 1.0 soon but in the meantime please feel free to take the console for a spin and let me know your thoughts. The current list of implemented features is:

- Topic feeds via Web Sockets
- Which brokers are up 
- Lists of topics & no. partitions, and connected consumer groups
- For each consumer/partition, its current offset

Finally the console provides a JSON interface.","25/Feb/14 03:45;jkreps;[~claude.mamo] Very cool. I added it to the Ecosystem page. It would be great if you could add a README and a couple screen shots to the project so people would know what they get with it.

[~jmiller] Let me know if you get this up on github and I'll add it to the Ecosystem page too.

It would also be good to just ping the users list and let people know that this exists. A LOT of people have been asking for something like this so I suspect there will be a lot of interest.","27/Feb/14 00:29;claude.mamo;Definitely [~jkreps], docs are in my TODO list before releasing the first version of the project. I noticed in the JIRA description that a goal for the console is to get a list of connected producers. I checked on ZK as well as on JMX and there is no info on producers, unless I missed it. Is this possible to achieve with the current version of Kafka?","27/Feb/14 02:43;jkreps;That is correct. If JMX would be a workable approach, adding JMX for producer connections should be reasonably straight-forward to do...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mirroring - enable non-random partitioning in embedded producer,KAFKA-333,12551521,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Invalid,,xwang,xwang,18/Apr/12 20:18,07/Feb/15 23:50,12/Jan/21 10:06,07/Feb/15 23:50,,,,,,,,,,,core,,,,,,0,,,,,"Currently the producer for mirroring uses the random partitioner. It will be very useful if we can specify partitioner there. One use case is when we aggregate ad server logs by mirroring the local kafka cluster on each ad server, we want to partition ad logs based on where they come from - the ad server id.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,236329,,,2012-04-18 20:18:26.0,,,,,,,"0|i029uv:",11198,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add an optional acknowledgement in the producer,KAFKA-57,12514694,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,07/Feb/15 23:49,12/Jan/21 10:06,07/Feb/15 23:49,,,,,,,,,,,,,,,,,0,,,,,Currently the producer will flush the socket buffer but does not wait for an answer. This is good for tracking-like use cases but bad for queue-like cases. We should add an optional acknowledgement to the protocol and have the producer await this.,,sharadag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,67309,,,2011-07-19 21:32:25.0,,,,,,,"0|i02a3b:",11236,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add looping and JSON output for ConsumerOffsetChecker,KAFKA-735,12629204,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Incomplete,,demaagd,demaagd,25/Jan/13 01:44,07/Feb/15 23:24,12/Jan/21 10:06,07/Feb/15 23:24,0.8.0,,,,,,,,,,tools,,,,,,0,patch,,,,"New options for kafka.tools.ConsumerOffsetChecker:
--asjson -  Json Output 
--loop N - Loop interval (in seconds, greater than 0)

Both are optional/independent:

--asjson w/o --loop => Output as JSON, once and terminates
--loop w/o --asjson => Output in default tabular format, repeats on loop defined interval
--loop w/ --asjson => Output as JSON, repeats on loop defined interval
neither --asjson nor --loop => Current behavior, output in tabular format once and terminates

Patch to be attached.",,demaagd,jkreps,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/13 02:16;demaagd;KAFKA-735-2.patch;https://issues.apache.org/jira/secure/attachment/12566439/KAFKA-735-2.patch","25/Jan/13 01:45;demaagd;KAFKA-735.patch;https://issues.apache.org/jira/secure/attachment/12566433/KAFKA-735.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2013-05-30 04:59:14.155,,,false,,,,,,,,,,,,,,,,,,309048,,,Sat Feb 07 23:24:28 UTC 2015,,,,,,,"0|i1dymv:",289708,,,,,,,,,,,,,,,,"25/Jan/13 02:16;demaagd;Correction (updates KAFKA-735.patch), small logic error in loop interval testing","30/May/13 04:59;junrao;Sorry for the late review. This probably should be fixed in trunk. The patch no longer applies. Could you rebase? Thanks,","07/Feb/15 23:24;jkreps;No follow-up.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit the maximum number of connections per ip address,KAFKA-1512,12724636,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jholoman,jkreps,jkreps,01/Jul/14 04:14,07/Jan/15 03:40,12/Jan/21 10:06,07/Jan/15 03:40,0.8.2.0,,,,,,,0.8.2.0,,,,,,,,,0,,,,,"To protect against client connection leaks add a new configuration
  max.connections.per.ip
that causes the SocketServer to enforce a limit on the maximum number of connections from each InetAddress instance. For backwards compatibility this will default to 2 billion.",,blue20080,donnchadh,guozhang,gwenshap,jholoman,jkreps,joestein,jongyoul,stephenchu810,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1812,KAFKA-1810,,,,,,"05/Jan/15 16:24;jholoman;KAFKA-1512-082.patch;https://issues.apache.org/jira/secure/attachment/12690098/KAFKA-1512-082.patch","01/Jul/14 19:42;jkreps;KAFKA-1512.patch;https://issues.apache.org/jira/secure/attachment/12653447/KAFKA-1512.patch","01/Jul/14 04:27;jkreps;KAFKA-1512.patch;https://issues.apache.org/jira/secure/attachment/12653315/KAFKA-1512.patch","03/Jul/14 22:18;jkreps;KAFKA-1512_2014-07-03_15:17:55.patch;https://issues.apache.org/jira/secure/attachment/12654004/KAFKA-1512_2014-07-03_15%3A17%3A55.patch","14/Jul/14 20:28;jkreps;KAFKA-1512_2014-07-14_13:28:15.patch;https://issues.apache.org/jira/secure/attachment/12655610/KAFKA-1512_2014-07-14_13%3A28%3A15.patch","24/Dec/14 02:47;jholoman;KAFKA-1512_2014-12-23_21:47:23.patch;https://issues.apache.org/jira/secure/attachment/12688973/KAFKA-1512_2014-12-23_21%3A47%3A23.patch",,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,2014-07-01 18:45:23.137,,,false,,,,,,,,,,,,,,,,,,402819,,,Wed Jan 07 03:40:52 UTC 2015,,,,,,,"0|i1xbz3:",402880,,jkreps,,,,,,,,,,,,,,"01/Jul/14 04:27;jkreps;Created reviewboard  against branch trunk","01/Jul/14 17:50;jkreps;A couple things to review:
0. The new config is max.connections.per.ip
1. I am using Socket.getInetAddress() as the key to limit on. I think an InetAddress is what we want...a socket address includes the port so is always unique, but there is sort of a weird hierarchy of things there. This also depends on this address being properly hashable (which it seems to be).
2. I made an unrelated change to how we set the recv buffer. We were weirdly setting this over and over again on the server socket every time we accepted a connection. I think this was a mistake, so I changed it to set it once. But if anyone knows a reason for this odd code that would make me more confident.
3. I don't know of a way to check the source address of a pending connection without actually accepting the connection. So as a result this patch accepts the connection, and then, if we are over quota, closes it.","01/Jul/14 18:45;guozhang;Jay, seems kafka-review-tool fails to create the RB, could you try again?","01/Jul/14 19:42;jkreps;Created reviewboard https://reviews.apache.org/r/23208/
 against branch trunk","03/Jul/14 20:46;jkreps;A proposal from the LI ops team is to also add an override for this so you can have custom limits for ips if you want:
{code}
  max.connections.per.ip.overrides=192.168.1.1:5, 192.168.1.2:, 192.168.1.3:45
{code}
If no objections I will implement this too.","03/Jul/14 21:05;gwenshap;It sounds like I can completely block specific IPs with:

max.connections.per.ip.overrides=192.168.1.1:0

Or selectively allow only specific IP to connect with:
max.connections.per.ip=0
max.connections.per.ip.overrides=192.168.1.1:20

Not an objection, just checking my understanding of this feature.","03/Jul/14 21:53;jkreps;Yes, I hadn't thought of that. Disabling connections could potentially be useful. The intended use was actually the other way around, basically default most things to something reasonable like 10 but have a way to whitelist some IPs to have unlimited connections.

The background here is that we were previously having clients bootstrap metadata through a VIP (which appears to the kafka nodes as a single ip). We just had an issue where a 200 node cluster that uses Kafka started creating and leaking connections through the vip which brought down a big shared cluster. So we thought we should have some limits. The hope was to change the VIP to DNS round-robin and gradually migrate the clients to that. In the meantime we thought it would be useful to be able to enforce the limit but whitelist the VIP with unlimited connections.

Thinking about this, maybe it is a little crazy hard coding ip/host names in config?","03/Jul/14 22:18;jkreps;Updated reviewboard https://reviews.apache.org/r/23208/
 against branch trunk","14/Jul/14 20:28;jkreps;Updated reviewboard https://reviews.apache.org/r/23208/
 against branch trunk","16/Jul/14 16:55;jkreps;Committed.","12/Dec/14 00:16;jholoman;[~jkreps], I noticed that the overrides are not fully implemented in this patch. Was the intent to leave that feature out? Based on your last comment I can see why but wanted to confirm. What are your thoughts on the override functionality now?

","12/Dec/14 18:30;jkreps;Ah, it looks like I didn't fully wire through the overrides from the config. That is a bug.","12/Dec/14 20:30;jholoman;[~jkreps] I can fix this if you're ok with that.","14/Dec/14 17:26;jholoman;Created reviewboard https://reviews.apache.org/r/29029/diff/
 against branch origin/trunk","15/Dec/14 02:17;jholoman;Updated reviewboard https://reviews.apache.org/r/29029/diff/
 against branch origin/trunk","15/Dec/14 02:21;jholoman;Updated reviewboard https://reviews.apache.org/r/29029/diff/
 against branch origin/trunk","15/Dec/14 02:30;jholoman;Updated reviewboard https://reviews.apache.org/r/29029/diff/
 against branch origin/trunk","24/Dec/14 02:24;jholoman;Updated reviewboard https://reviews.apache.org/r/29029/diff/
 against branch origin/trunk","24/Dec/14 02:47;jholoman;Updated reviewboard https://reviews.apache.org/r/29029/diff/
 against branch origin/trunk","24/Dec/14 02:54;jholoman;I modified the two tests related to max connections in the  SocketServerTest class a bit. Basically I'm just checking if the socket inpustream is still open, rather than sending a request through. I found in testing that isolating the tests to only test socketServer functionality was a bit hard and that sending real requests would be beneficial. I did not modify the other tests in SocketServerTest, though I think that utilizing the ClusterMetadataRequest in the tests rather than bogus and / or empty producer requests may be worth looking into. ","05/Jan/15 15:19;joestein;I was just about to commit this and realized this is introduced in 0.8.2 but not fully complete so we should have this as a patch for 0.8.2 and trunk. [~jholoman] can you upload a 0.8.2 patch also so we can double commit this to 0.8.2 and trunk please... if there are no objects to having this in 0.8.2 it seems reasonable since it was introduced in this release we shouldn't ship something not complete if we have fix available now.","05/Jan/15 16:24;jholoman;Patch against 0.8.2","07/Jan/15 03:40;joestein;committed to 0.8.2 branch and trunk, thanks for the patch Jeff",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support adding new partitions on brokers without reassignment of existing partitions,KAFKA-1829,12763727,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,,,allenxwang,allenxwang,24/Dec/14 23:01,24/Dec/14 23:01,12/Jan/21 10:06,,0.8.1.1,,,,,,,,,,,,,,,,1,,,,,"One solution to deal with increasing incoming traffic to Kafka brokers is to add new broker instances and new partitions at the same time. This is possible if disk size is not the bottleneck. In this case, ideally the new partitions would be assigned to new brokers without any changes to existing partitions. The reason behind it is that moving existing partitions would cause more traffic and hence more load.

The way to handle this now is a multi step process:

- add partitions
- generate replica assignment for the new partitions on the new brokers
- execute the assignment

This is complicated. Also, adding partitions and assigning replicas is not atomic -- when partitions are added, they are already assigned using a default assignment algorithm.

This feature would greatly help auto scaling Kafka to deal with increasing load.
",,allenxwang,astubbs,nfo,sekikn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-12-24 23:01:51.0,,,,,,,"0|i23rpr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transaction manager failover handling,KAFKA-1565,12731220,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,lindong,lindong,lindong,31/Jul/14 23:47,20/Aug/14 01:54,12/Jan/21 10:06,20/Aug/14 00:44,,,,,,,,,,,,,,,,,0,transactions,,,,"Transaction manager should guarantee that, once a pre-commit/pre-abort request is acknowledged, commit/abort request will be delivered to partitions involved in the transaction.

In particular, we handle the following failover scenarios:

1) Transaction manager or its followers fail before txRequest is duplicated on local log and followers.
Solution: Transaction manager responds to request with error status if it is alive. The producer keeps trying commit.

2) The txPartition’s leader is not available.
Solution: Put txRequest on unSentTxRequestQueue. When metadataCache is updated, check and re-send txRequest from unSentTxRequestQueue if possible.

3) The txPartition’s leader fails when txRequest is in channel manager.
Solution: Retrieve all txRequests queued for transmission to this broker and put them on unSentTxRequestQueue.

4) Transaction manage does not receive success response from txPartition’s leaders within timeout period.
Solution: Transaction manager expires the txRequest and re-send it.

5) Transaction manager fails.
Solution: The new transaction manager reads transactionHW from zookeeper, and sends txRequest starting from the transactionHW.

",,lindong,raulcf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/14 20:52;lindong;KAFKA-1565.patch;https://issues.apache.org/jira/secure/attachment/12659946/KAFKA-1565.patch",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2014-08-20 01:30:06.781,,,false,,,,,,,,,,,,,,,,,,409292,,,Wed Aug 20 01:54:16 UTC 2014,,,,,,,"0|i1yf7z:",409288,,,,,,,,,,,,,,,,"05/Aug/14 20:53;lindong;Created reviewboard https://reviews.apache.org/r/24340/diff/
 against branch origin/transactional_messaging","20/Aug/14 00:44;lindong;Merged into KAFKA-1523: Implement transaction manager module.","20/Aug/14 00:46;lindong;Merged into KAFKA-1523: Implement transaction manager module","20/Aug/14 01:30;raulcf;If this patch is not necessary/applicable anymore, can we just remove it?","20/Aug/14 01:54;lindong;Sure. The JIRA is closed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transactional messaging request/response definitions,KAFKA-1522,12725488,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,lindong,jjkoshy,jjkoshy,04/Jul/14 13:51,19/Aug/14 21:20,12/Jan/21 10:06,19/Aug/14 21:20,,,,,,,,,,,,,,,,,0,transactions,,,,"* Add the TransactionRequest 
* Add TransactionResponse
* Add transaction-id field to OffsetCommitRequest
* Add error code for errors such as non-initiated transaction (this will be used for the ProducerResponse and OffsetCommitResponse - say if a producer sends a request and its messages have a transaction ID that has not been initiated.
* TxCoordinatorMetadataRequest (to look up the transaction coordinator for a transaction group)
",,jjkoshy,lindong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/14 03:12;lindong;KAFKA-1522_2014-07-17_20:12:06.patch;https://issues.apache.org/jira/secure/attachment/12656434/KAFKA-1522_2014-07-17_20%3A12%3A06.patch","22/Jul/14 23:43;lindong;KAFKA-1522_2014-07-22_16:42:50.patch;https://issues.apache.org/jira/secure/attachment/12657221/KAFKA-1522_2014-07-22_16%3A42%3A50.patch","22/Jul/14 23:43;lindong;KAFKA-1522_2014-07-22_16:43:18.patch;https://issues.apache.org/jira/secure/attachment/12657222/KAFKA-1522_2014-07-22_16%3A43%3A18.patch","06/Aug/14 04:29;lindong;KAFKA-1522_2014-08-05_21:28:20.patch;https://issues.apache.org/jira/secure/attachment/12660050/KAFKA-1522_2014-08-05_21%3A28%3A20.patch","09/Aug/14 04:21;lindong;KAFKA-1522_2014-08-08_21:21:15.patch;https://issues.apache.org/jira/secure/attachment/12660798/KAFKA-1522_2014-08-08_21%3A21%3A15.patch","15/Aug/14 18:37;lindong;KAFKA-1522_2014-08-15_11:37:42.patch;https://issues.apache.org/jira/secure/attachment/12662119/KAFKA-1522_2014-08-15_11%3A37%3A42.patch",,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,2014-07-16 21:29:19.083,,,false,,,,,,,,,,,,,,,,,,403647,,,Fri Aug 15 18:37:59 UTC 2014,,,,,,,"0|i1xgz3:",403690,,,,,,,,,,,,,,,,"16/Jul/14 21:29;lindong;Created reviewboard https://reviews.apache.org/r/23567/diff/
 against branch origin/transactional_messaging","18/Jul/14 02:30;lindong;Updated reviewboard  against branch origin/transactional_messaging","18/Jul/14 02:30;lindong;Updated reviewboard https://reviews.apache.org/r/23567/diff/
 against branch origin/transactional_messaging","18/Jul/14 02:38;lindong;Updated reviewboard https://reviews.apache.org/r/23567/diff/
 against branch origin/transactional_messaging","18/Jul/14 03:12;lindong;Updated reviewboard https://reviews.apache.org/r/23567/diff/
 against branch origin/transactional_messaging","22/Jul/14 23:43;lindong;Updated reviewboard https://reviews.apache.org/r/23567/diff/
 against branch origin/transactional_messaging","06/Aug/14 04:29;lindong;Updated reviewboard https://reviews.apache.org/r/23567/diff/
 against branch origin/transactional_messaging","09/Aug/14 04:22;lindong;Updated reviewboard https://reviews.apache.org/r/23567/diff/
 against branch origin/transactional_messaging","15/Aug/14 18:37;lindong;Updated reviewboard https://reviews.apache.org/r/23567/diff/
 against branch origin/transactional_messaging",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add delete topic to topic commands and update DeleteTopicCommand,KAFKA-1443,12713231,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tnachen,tnachen,tnachen,09/May/14 01:17,22/Jul/14 14:30,12/Jan/21 10:06,06/Jun/14 16:47,,,,,,,,0.8.2.0,,,,,,,,,0,,,,,Add delete topic option to current topic commands,,junrao,nehanarkhede,stanleyxu2005,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/14 17:59;tnachen;KAFKA-1443.patch;https://issues.apache.org/jira/secure/attachment/12644140/KAFKA-1443.patch","09/May/14 01:18;tnachen;KAFKA-1443.patch;https://issues.apache.org/jira/secure/attachment/12644060/KAFKA-1443.patch","05/Jun/14 03:51;tnachen;KAFKA-1443_2014-06-04_20:51:44.patch;https://issues.apache.org/jira/secure/attachment/12648429/KAFKA-1443_2014-06-04_20%3A51%3A44.patch",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2014-06-06 16:47:22.123,,,false,,,,,,,,,,,,,,,,,,391547,,,Mon Jun 09 15:21:15 UTC 2014,,,,,,,"0|i1vfo7:",391759,,,,,,,,,,,,,,,,"09/May/14 01:18;tnachen;Created reviewboard  against branch origin/trunk","09/May/14 17:59;tnachen;Created reviewboard https://reviews.apache.org/r/21272/
 against branch origin/trunk","05/Jun/14 03:51;tnachen;Updated reviewboard https://reviews.apache.org/r/21272/diff/
 against branch origin/trunk","06/Jun/14 16:47;nehanarkhede;Thanks for the patch, pushed to trunk.","09/Jun/14 15:21;junrao;Thanks for the patch. Could you add deleteOpt to allTopicLevelOpts and make a pass in checkArgs() to see which option is invalid together with deleteOpt?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move partition assignment to the broker,KAFKA-167,12528528,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jkreps,jkreps,24/Oct/11 17:27,12/Jul/14 19:56,12/Jan/21 10:06,12/Jul/14 19:56,,,,,,,,,,,,,,,,,0,,,,,"Currently partitions are assigned to consumers for consumption via a co-ordination algorithm by the consumers. This means there is effectively no master broker node, which is nice in a way, but makes the consumer logic complex to implement. It would be good to move the co-ordination to the brokers. This would make implementation of a consumer client much much easier, since only minimal zk interaction would be required. There are a number of details that would  have to be worked out to finalize the protocol. Discussion is here:

http://mail-archives.apache.org/mod_mbox/incubator-kafka-dev/201109.mbox/%3cCAFbh0Q2ADjXcbCYqGh8TB4jXBAs3Hq5RLxk9o9w76we-bWanVw@mail.gmail.com%3e",,jkreps,nmarasoi,randgalt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-07-12 14:05:03.607,,,false,,,,,,,,,,,,,,,,,,214388,,,Sat Jul 12 19:56:16 UTC 2014,,,,,,,"0|i029tz:",11194,,,,,,,,,,,,,,,,"12/Jul/14 14:05;nmarasoi;Hi,

So I would suggest this: for each consumer group, one broker can be elected as partition/replica assignment HA coordinator using a standard election mechanism on zookeeper already available in Curator.

I think it is nicer to distribute as much as possible, and, while recognizing that the algorithm is more deterministic, stable and convergent when using a centralized coordinator, there is no reason to pick the same coordinator for different consumer groups.

Apart from the mail discussion thread, can you please indicate any discussion on design, implementation, so that I can try to pick a small part and code something for it?

Thanks,
Nicu Marasoiu","12/Jul/14 19:56;jkreps;This is being done (finally) in the new consumer implementation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrade to zkclient 0.1,KAFKA-245,12537923,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,pyritschard,pyritschard,pyritschard,10/Jan/12 17:28,19/Jun/14 05:16,12/Jan/21 10:06,10/Jan/12 23:17,0.7,,,,,,,,,,core,,,,,,0,newbie,,,,"the zkclient jar bundled with kafka should be synced with what is available on maven central. the artifact which has group com.github.sgroschupf, artifact id zkclient and version 0.1 is from a day after the one bundled with kafka and should thus be sufficient for kafka's needs.

I have tested it locally and find no regressions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/12 21:15;pyritschard;0001-KAFKA-245-zkclient-0.1-to-enable-maven-syncing.patch;https://issues.apache.org/jira/secure/attachment/12510104/0001-KAFKA-245-zkclient-0.1-to-enable-maven-syncing.patch","10/Jan/12 22:06;pyritschard;0003-follow-up-to-KAFKA-245-use-maven-to-fetch-zkclient.patch;https://issues.apache.org/jira/secure/attachment/12510113/0003-follow-up-to-KAFKA-245-use-maven-to-fetch-zkclient.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-01-10 23:16:43.467,,,false,,,,,,,,,,,,,,,,,,223429,,,Thu Jan 12 21:07:13 UTC 2012,,,,,,,"0|i0rscn:",160237,,,,,,,,,,,,,,,,"10/Jan/12 21:15;pyritschard;Solve kafka-245","10/Jan/12 22:06;pyritschard;With the following patch changing the version of the zkclient artifact, it is now possible to fetch it directly from maven central. This kills the zkclient specific code in KafkaProject.config","10/Jan/12 23:16;nehanarkhede;Thanks for the patch ! One thing missing in there is deleting the zkclient jar in core/lib, since now it will be pulled from Maven. However, I made that change and will check in this patch.","10/Jan/12 23:17;nehanarkhede;Committed this","12/Jan/12 19:37;junrao;Do we know the github revision/date corresponding to the zkclient 0.1 release?","12/Jan/12 20:56;nehanarkhede;From Maven, the date is 04-13-2011. ( http://repo1.maven.org/maven2/com/github/sgroschupf/zkclient/maven-metadata.xml )

The one we had was zkclient-20110412.jar which was a day older. Am I right ?","12/Jan/12 21:07;pyritschard;the jar contains files from april 13th, there's a ""0.1.0"" git tag but the last commit is from 2010",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow getTopicMetadata to get metadata for all topics,KAFKA-653,12618981,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jkreps,jkreps,06/Dec/12 03:56,19/Jun/14 05:11,12/Jan/21 10:06,23/Oct/13 16:39,,,,,,,,,,,,,,,,,0,newbie,,,,"Currently the topic metadata api requires a list of topics. This is good when you know what you want, but for tools that need to replicate all topics or those that match a pattern or something like that, they may not know their topic names a priori.

To support this it would be nice to have the behavior be that issuing the getTopicMetadata request with no topics yields metadata for all topics (as this is more useful than the current behavior of giving you metadata for no topics).",,jkreps,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-10-23 16:39:42.113,,,false,,,,,,,,,,,,,,,,,,296267,,,Wed Oct 23 16:39:42 UTC 2013,,,,,,,"0|i1483r:",232776,,,,,,,,,,,,,,,,"23/Oct/13 16:39;mumrah;This was fixed by KAFKA-690 and later improved in KAFKA-1091",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log Broker state ,KAFKA-1384,12707625,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tnachen,tnachen,tnachen,10/Apr/14 02:04,06/May/14 23:36,12/Jan/21 10:06,06/May/14 23:36,,,,,,,,0.8.2.0,,,,,,,,,0,,,,,"Currently we don't have visibility into what state the broker is currently in, ie: Startup -> Running -> Waiting Controlled shutdown -> Shutting down

So without knowing what state the broker it is it's hard to figure out what the current broker is performing.

This ticket is to add a new metric to expose the current broker state.",,darion,junrao,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/14 17:25;tnachen;KAFKA-1384.patch;https://issues.apache.org/jira/secure/attachment/12641956/KAFKA-1384.patch","26/Apr/14 07:10;tnachen;KAFKA-1384_2014-04-26_00:09:56.patch;https://issues.apache.org/jira/secure/attachment/12642069/KAFKA-1384_2014-04-26_00%3A09%3A56.patch","02/May/14 01:47;tnachen;KAFKA-1384_2014-05-01_18:47:03.patch;https://issues.apache.org/jira/secure/attachment/12642984/KAFKA-1384_2014-05-01_18%3A47%3A03.patch","02/May/14 19:13;tnachen;KAFKA-1384_2014-05-02_12:13:24.patch;https://issues.apache.org/jira/secure/attachment/12643096/KAFKA-1384_2014-05-02_12%3A13%3A24.patch","05/May/14 18:05;tnachen;KAFKA-1384_2014-05-05_11:05:15.patch;https://issues.apache.org/jira/secure/attachment/12643397/KAFKA-1384_2014-05-05_11%3A05%3A15.patch","05/May/14 21:25;tnachen;KAFKA-1384_2014-05-05_14:25:25.patch;https://issues.apache.org/jira/secure/attachment/12643431/KAFKA-1384_2014-05-05_14%3A25%3A25.patch","06/May/14 00:15;tnachen;KAFKA-1384_2014-05-05_17:14:57.patch;https://issues.apache.org/jira/secure/attachment/12643459/KAFKA-1384_2014-05-05_17%3A14%3A57.patch","06/May/14 20:21;tnachen;KAFKA-1384_2014-05-06_13:21:14.patch;https://issues.apache.org/jira/secure/attachment/12643624/KAFKA-1384_2014-05-06_13%3A21%3A14.patch",,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2014-04-10 07:27:23.01,,,false,,,,,,,,,,,,,,,,,,385948,,,Tue May 06 23:36:41 UTC 2014,,,,,,,"0|i1uhkf:",386212,,,,,,,,,,,,,,,,"10/Apr/14 07:27;darion;Could register  the broker state in zookeeper ?  And use commane-line tool to take it ","25/Apr/14 17:23;tnachen;I'm planning just to expose it via JMX, as we don't have a need to write it to zookeeper as other brokers doesn't have a need to query it.

Less contention to ZK as well, you have anything in mind why you thought putting it in ZK would be good?
","25/Apr/14 17:25;tnachen;Created reviewboard https://reviews.apache.org/r/20718/
 against branch origin/trunk","26/Apr/14 07:10;tnachen;Updated reviewboard https://reviews.apache.org/r/20718/
 against branch origin/trunk","02/May/14 01:47;tnachen;Updated reviewboard https://reviews.apache.org/r/20718/
 against branch origin/trunk","02/May/14 19:13;tnachen;Updated reviewboard https://reviews.apache.org/r/20718/
 against branch origin/trunk","05/May/14 18:05;tnachen;Updated reviewboard https://reviews.apache.org/r/20718/
 against branch origin/trunk","05/May/14 21:25;tnachen;Updated reviewboard https://reviews.apache.org/r/20718/
 against branch origin/trunk","06/May/14 00:15;tnachen;Updated reviewboard https://reviews.apache.org/r/20718/
 against branch origin/trunk","06/May/14 20:21;tnachen;Updated reviewboard https://reviews.apache.org/r/20718/
 against branch origin/trunk","06/May/14 23:36;junrao;Thanks for the patch. +1 and committed to trunk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code dump of new producer,KAFKA-1227,12690876,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,23/Jan/14 20:42,03/Apr/14 16:33,12/Jan/21 10:06,03/Apr/14 16:33,,,,,,,,0.8.2.0,,,,,,,,,1,,,,,"The plan is to take a dump of the producer code ""as is"" and then do a series of post-commit reviews to get it into shape. This bug tracks just the code dump.",,eribeiro,guozhang,jkreps,junrao,martinkl,omnomnom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/14 01:41;jkreps;KAFKA-1227.patch;https://issues.apache.org/jira/secure/attachment/12626805/KAFKA-1227.patch","03/Feb/14 05:58;jkreps;KAFKA-1227.patch;https://issues.apache.org/jira/secure/attachment/12626607/KAFKA-1227.patch","23/Jan/14 20:55;jkreps;KAFKA-1227.patch;https://issues.apache.org/jira/secure/attachment/12624907/KAFKA-1227.patch",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2014-01-23 22:50:18.175,,,false,,,,,,,,,,,,,,,,,,369616,,,Thu Apr 03 16:33:18 UTC 2014,,,,,,,"0|i1rpfb:",369921,,,,,,,,,,,,,,,,"23/Jan/14 20:55;jkreps;Created reviewboard https://reviews.apache.org/r/17263/
 against branch trunk","23/Jan/14 22:50;junrao;I made a pass of the producer client code. The following are my comments.

1. Selector: It seems that the selector never closes an existing socket on its own (other than when the selector itself is closed). For example, not existing sockets are closed after metadata refresh. This has the implication that it may increase the # of socket connections that a client has to maintain. For example, if every client uses all brokers as the metadata broker list, it means that every client will maintain a socket connection to every broker, which doesn't seem to be very scalable. Also, if a partition is moved to some new brokers, the client will still be maintaining the socket connections to the old brokers. In 0.8, we close all existing sockets everytime the metadata is refreshed.

2. Metadata: We need to think through the case when the clients use a VIP in the metadata broker list. In this patch, it seems that we only use the VIP once and then switch to actual broker list after first metadata update. This means that the producer can only issue metadata requests to brokers to which replicas are assigned. In 0.8, we always fetch metadata requests using the metadata broker list. Another thing that we do in 0.8 is to close the socket connection after each metadata request. When using a VIP, an idle socket connection can be killed by the load balancer. If the vip is not configured properly, it may take a long time (e.g., 8 minutes) to detect that the socket is already killed, which will slow down the fetching of metadata.

3. DefaultPartitioner:
3.1 This has the issue that every instance of producer always starts with partition 0, which could create imbalanced load if multiple producers are created at the same time.
3.2 Also, a better default partitioner when no partition key is provided, is probably to select a random ""available"" (i.e., leader node exists) partition, instead of just a random partition.

4.Partitioner.partition(): From cluster, we can get the partition list for a topic. Is the passed in numPartitions redundant?

5. Sender:
5.1 run(): It seems that it's possible to have a produce request and metadata request to be sent to the same node in one iteration. This will cause selector.poll() to fail since we can't send more than 1 request to the same node per poll.
5.2 produceRequest(): topicDatas is weird since data is the plural form of datum.

6. TopicPartition: How do we prevent that the computed hash code is exactly 0?

7. BufferExhaustedException: It's probably useful to include the requested size in the exception.

8. RecordAccumulator:
8.1 Should we call free bufferPool?
8.2 ready(): Should a partition be also considered ready if it has only 1 ReocrdBatch whose size is exactly of batchSize?

9. RecordBatch.done(): Should we unblock RecordSend after registered callbacks are called?

10. RecordSend: We should include at least the partition number and probably the topic itself.

11. Various mis-spellings:
11.1 ProducerRecord: chosing
11.2 KafkaProducer:
11.2.1 comments above send(): messaging waiting = > messages waiting
11.2.2 {@link kafka.clients.producer.RecordSend.await() await()}: 2 await()
11.3 RecordBatch: sufficent

12. Formatting: Should we use 4-space indentation vs 2-space? The latter is what we have been using in scala.

The following can be added to the TODO list:

13. BufferPool: When we add jmx, it's probably using to have one on size of the waiter list, and another on the available memory.

14. logging: It seems that there is no logging messages and we use e.printStackTrace() in a few places. Should we use log4j?

15. Configs: It would be useful to log every overridden value and unused property name.
","23/Jan/14 23:47;guozhang;Some more comments:

--- General

1. How to we decide where to put Exception definitions? Currently we have an errors folder in kafka.comm and some folders also have their only exceptions.

2. Shall we merge kafka.comm.protocol and kafka.comm.request folders since the requests definitions are highly dependent on the protocol class?

3. Shall we put Node, Cluster, Partition, TopicPartition in kafka.comm into one sub-folder, for example, called kafka.comm.metadata?

4. Shall we put the Serializer classes into the protocol folder?

5. Shall we move the kafka.clients.common.network sub-folder to kafka.common?


--- kafka.common.Cluster

1. Since the nextNode use global round robin, we need to make sure no more than one objects access a single Cluster’s nextNode.

--- kafka.common.StringSerialization

1. Shall we put config names such as ENCODING_CONFIG all in a single file?

--- kafka.common.AbstractIterator

1. makeNext is not supposed to left in other states other than DONE and READY?

--- kafka.common.protocl.Schema

1. Will Field order difference make to different schemas?

--- kafka.common.protocl.ProtoUtil

1. parseMetadataResponse: after reading the function I feel that the TopicInfo/PartitionInfo object for parsing might be preferable. We can put these objects in the Protocol.java file so any protocol change would only require one file edit.

--- kafka.common.record.LogEntry

1. Maybe we can rename to OffsetRecord?

--- kafka.common.record.Record

1. Do we expect MIN_HEADER_SIZE and RECORD_OVERHEAD to be different in the future? Currently their values are the same and the way they are computed are also identical.

--- kafka.common.request.RequestHeader

1. Is it better to define ""client_id"" strings as static field in the Protocol.java?

2. Does REQUEST/RESPONSE_HEADER also need to be versioned?

--- kafka.client.common.NetworkReceive

1. In the first constructor, why not also initializing the size buffer also to ByteBuffer.allocate(4)?

2. Why NetworkReceive not extending ByteBufferReceive?

--- kafka.client.common.Selector

1. “transmissions.send.remaining() <= 0”, under what condition can remaining() be < 0?

2. “if (trans != null) this.disconnected.add(trans.id); “, should it be trans == null?

--- kafka.client.producer.internals.BufferPool:

1. In the freeUp() function, should use this.free.pollLast().capacity() instead of limit()?

2. What is the rational of having just one poolable size?

--- kafka.clients.producer.internals.Metadata

1. After configs are added, we need to remove the hard-coded default values. So for all of these places we could leave a TODO mark for now.

--- kafka.clients.producer.internals.ProduceRequestResult

1. Its member fields are dependent on Protocol.java, so once we change the protocol we would probably also need to change this file.

--- kafka.clients.producer.internals.RecordAccumulator

1. Typo: “Get a list of topic-partitions which are ready to be send.”

--- kafka.clients.producer.internals.Sender

1. One corner case we may need to consider is the following: if a partition becomes not available, and producer keep sending data to this partition, then later on this partition could exhaust the memory, keeping other partitions to not able to take more messages but block waiting.

2. In handling dis-connection, the ProduceRequestResult will set the exception, and if await() is called this exception will be thrown and the callback not be executed. Since this exception is already stored in the RecordSend I think a better way is not throw exception on await() but let the callback function to handle it. That would make the application code more clean since otherwise the application need so try-catch the await() call.

3. In closing the producer, there is another corner case that the io thread can keep trying to send the rest of the data and failed. Probably we could add another option to drop whatever is in the buffer and let the callback functions of the application to handle them.","24/Jan/14 04:53;jkreps;Hey Jun, quick responses. I'll try to get a patch up with some of the minor things, though a few of the others I'd like to do post-commit.

1. WRT closing connections. Yes, this is true. I agree it is needed but decided to punt on it for the first pass. It is an important follow-up item. There are two cases to handle: metadata fetches and leadership handoffs. 
Obviously the Selector will not handle these special cases which are specific to this use case. Theoretically this could all be done in the Sender logic but it would be a bit complex. I think the best solution is just to have us time out idle connections after some configurable period of disuse (30 seconds, say). 

2. I think the VIP problem can be handled by just timing out idle connections. Special cases related to metadata won't help because non-metadata related connections can also be idle. Not retaining the bootstrap urls is intentional: future metadata requests should use the full broker set the producer is connecting to. You mention that this will cause the producer to prefer to fetch metadata from a broker to which it already has a connection for subsequent metadata fetches after the initial bootstrap, but this was the idea--no need to setup and then timeout another connection if we already have one.

3. WRT initializing the partitioner to 0: Yeah we can initialize to something random. This problem would be a bit pathological as you would have to start all your producers the same instant and send exactly the same number of messages through them for this to persist.

4. I included the numPartitions even though it is easily computable from cluster as all partitioners will need to mod by number of partitions, but the vast majority won't need to use the cluster. So it just seemed more intuitive rather than the user having to figure out that they can get it by calling into cluster and worrying about the underlying performance of that just to give it to them.

5.1 Yes, that is a bug.
5.2 It is a bit slangy :-)

6. I don't prevent this: a zero hash code will be recomputed each time, but this is an unlikely case and recomputing is what would happen in all cases if we didn't cache.

7. Good point, I'll improve the error message.

8.1 I'll try to think of a better name.
8.2 Yes, we can do that. I think that would be good for latency in the case where we had to allocate a non-standard size

9. I think you could argue either way in terms of the preferrable sequencing. However I wanted to reuse the RecordSend object as the argument to the callback rather than introduce another object. However this means I do need to complete the record send first otherwise the callback will block trying to access fields in the send.

10. Ah, very good point.

11. Thanks

12. I am not two picky. 2 spaces is the recommended style in scala and 4 spaces is the classic ""sun java style"". I would like to get our style formally specified in an IDE formatter. That is what I am using for eclipse and it is very nice, it does all formatting for you and ensures a very consistent style. I will start a thread on this one as likely everyone has an opinion.

13. I had the same thought. I'm not sure if it is better to give the current value or the average over the window. Thoughts? Since we mostly look at graphs polled every 30 seconds if we do the instantaneous measurement it amounts to just a single data point for the whole 30 seconds but that may be okay...

14. I figured we would need to discuss logging so I just punted for now. The standard insofar as there is one is really slf4j, which I consider kind of silly. There are really just a couple of places that need logging so maybe it would be fine to just use java.util.logging which comes with the jvm. I'll start a thread on this.

15. In a client I think this is something we should leave to the client. Printing lots of messages in their logs is a bit rude. I think it is better to give an API to get information about the configs.
","24/Jan/14 06:14;jkreps;Hey Guozhang, thanks for the detailed stylistic questions. I think these are important to discuss. Quick responses inline:

_1. How to we decide where to put Exception definitions? Currently we have an errors folder in kafka.comm and some folders also have their only exceptions._

That package was meant to be explicitly API errors. I.e. those errors which are defined in ErrorKeys.java with a registered error code and have a bidirectional mapping to this code. These represent communication between the client and server so I wanted to put them in a special place. In general exceptions should be kept with the package with which they most naturally fit (ConfigException goes with config, etc).

The most important code organization principle I had in mind was that each package should be either public or private. Only public packages will be javadoc'd. All classes in a public package are exposed to the user and are part of the public interface. The idea is that we would be very careful with these public packages. I considered actually differentiating these as something like kafka.clients.producer.pub or something but I thought that was a bit ugly--maybe there is another way to differentiate or annotate classes so that we are very explicit about public or private. Essentially any change to interfaces in these packages is breaking all our users so we have to think very carefully about API design and change. The rest of the classes (most of them actually) are really just an implementation detail and we can change them at will.

Currently the public packages are just:
  kafka.clients.producer
  kafka.common
  kafka.common.errors

One item I wanted to document and discuss as a separate thread was code organization as I think these kinds of conventions only work if they are documented and broadly understood.

2. Shall we merge kafka.comm.protocol and kafka.comm.request folders since the requests definitions are highly dependent on the protocol class?

I would rather not. The kafka.common.network package defines a low-level network framing based on size delimited messages. This is fully generic--the unit test tests an ""echo server"". It is not tied to any details of our protocol and it is really important that people not leak details of our protocol into it!!!! :-)

The protocol is just a bunch of message definitions and isn't tied to the network transport or framing at all. It is just a way of laying out bytes.

The request package combines the protocol definition and network framing.

I am hoping to keep these things orthogonal.

Once we get our build fixed (ahem), I'd really like us to get checkstyle integrated so we can enforce these kinds of package dependencies and keep some kind of logical coherence. It has been a struggle otherwise.

3. Shall we put Node, Cluster, Partition, TopicPartition in kafka.comm into one sub-folder, for example, called kafka.comm.metadata?

I'm open to that. I liked having the current flat package for simplicity for the user (fewer things to import). Basically I am trying to make the javadoc for the producer as simple and flat as possible.

4. Shall we put the Serializer classes into the protocol folder?

The Serializer is the PUBLIC interface for users to serialize their messages. It actually isn't related to the definition of our protocol definition.

5. Shall we move the kafka.clients.common.network sub-folder to kafka.common?

Yeah I think that is actually how it is. I previously had it separate and the rationale was that many of the classes...e.g. the Selector were really written with the clients in mind. Theoretically the same Selector class could be the basis for the socket server but I didn't really think those use cases through.

1. Since the nextNode use global round robin, we need to make sure no more than one objects access a single Cluster’s nextNode.

That may just be a bad name. The goal of that method was load balancing not iterating over the nodes. So actually the intention was to give a different node to each thread in the multithreaded case. 

1. Shall we put config names such as ENCODING_CONFIG all in a single file?

I planned to do a discussion on config. The way it works is that configs are defined by the ConfigDef. However we allow plug-in interfaces (Serializer, Partitioner, etc). These may need configs too, but these are (generally speaking) user classes. So we allow including user-defined configs. So StringSerializer is essentially a user plug-in that seemed useful enough to include in the main code. I think it makes more sense to document it's configs with the class rather than elsewhere.

— kafka.common.AbstractIterator
1. makeNext is not supposed to left in other states other than DONE and READY?

Yeah this is basically a transliteration of the same class in the main code base which is a transliteration of the iterator in Google Collections.

1. kafka.common.protocl.Schema: Will Field order difference make to different schemas?

Yes our protocol is based on position not name.

1. kafka.common.protocl.ProtoUtil: parseMetadataResponse: after reading the function I feel that the TopicInfo/PartitionInfo object for parsing might be preferable. We can put these objects in the Protocol.java file so any protocol change would only require one file edit.

I'd like to have a discussion about this. I tried this approach. The problem is that the mixing of protocol definition with business logic leads to leaking of logic into the protocol and makes the protocol hard to read. There are several other options. It would be good to discuss.

1. kafka.common.record.LogEntry: Maybe we can rename to OffsetRecord?
Hmm, I'm open to changing it but I'm not sure that's better. I try to avoid names like TopicPartition which are just the concatenation of all the fields as I feel the purpose of a name is to capture the concept the fields describe. I.e. if we add a new field we shouldn't need to lengthen the name!

1. kafka.common.record.Record: Do we expect MIN_HEADER_SIZE and RECORD_OVERHEAD to be different in the future? Currently their values are the same and the way they are computed are also identical.

Good point I'll look into this, these are just copied from the scala.

1. kafka.common.request.RequestHeader: Is it better to define ""client_id"" strings as static field in the Protocol.java?
Unlikely. Currently you can access a field by the string name or the field instance. The field instance is an array access and the name is a hash table lookup to get the field followed by an array access. So the two reasonable options are to have static variables for the Fields or to access with Strings. Let's procrastinate that to the discussion of handling request definitions.

2. kafka.client.common.NetworkReceive: Does REQUEST/RESPONSE_HEADER also need to be versioned?

If we want to change it. We didn't have a version number for these in the protocol so we can't add one now. Any header change today is non-backwards compatible.

1. In the first constructor, why not also initializing the size buffer also to ByteBuffer.allocate(4)?

The point of the size buffer is to read the size to allocate and read the message buffer, but that constructor takes an already allocated/read message buffer. I was using that constructor for unit testing to fake responses that weren't really being read.

2. Why NetworkReceive not extending ByteBufferReceive?

Yes this distressed me as well. Here is the issue. I want ByteBufferSend/NetworkSend to work on an array of bytebuffers to handle the case where you already have serialized chunks of data (i.e. message sets). But in the case of a receive we don't currently have a good way to concatenate buffers so reading into multiple buffers isn't useful. This is basically a limitation of the ByteBuffer api.

1. kafka.client.common.Selector: “transmissions.send.remaining() <= 0”, under what condition can remaining() be < 0?

None, I think.

2. “if (trans != null) this.disconnected.add(trans.id); “, should it be trans == null?

Well that would give a null pointer, no? What this is saying is ""if we have an id for this connection, record it as disconnected"".

1. kafka.client.producer.internals.BufferPoolIn the freeUp() function, should use this.free.pollLast().capacity() instead of limit()?

Yeah that would be better, technically anything on the free list must have capacity==limit but I think it is bad to depend on that.

2. What is the rational of having just one poolable size?

Basically just to avoid implementing malloc. I guess the choice is either to implement a generic BufferPool or one specific to the needs of the producer. Since most of the time the producer will be doing allocations of that poolable size it makes sense to keep those around. If you try to keep arbitrary sizes I think things quickly get complex but since we will almost always be allocating the same size it seemed simpler to just handle that case. I'm open to generalizing it if it isn't too complex.

— kafka.clients.producer.internals.Metadata
1. After configs are added, we need to remove the hard-coded default values. So for all of these places we could leave a TODO mark for now.
Yup.

— kafka.clients.producer.internals.ProduceRequestResult
1. Its member fields are dependent on Protocol.java, so once we change the protocol we would probably also need to change this file.

I don't believe this is dependent on the protocol, maybe you can elaborate?
 
1. kafka.clients.producer.internals.RecordAccumulatorTypo: “Get a list of topic-partitions which are ready to be send.”

Ack.

— kafka.clients.producer.internals.Sender
1. One corner case we may need to consider is the following: if a partition becomes not available, and producer keep sending data to this partition, then later on this partition could exhaust the memory, keeping other partitions to not able to take more messages but block waiting.

If I understand the case you are describing you are saying that the producer could use up the full buffer on a partition which is not available and the producer will then block. This is correct and that is the intention. This shouldn't block the sender, though, it will keep trying to send until the partition becomes available again. I think this is what you want: you can buffer for a while but eventually must either block or drop data if memory is bounded.

2. In handling dis-connection, the ProduceRequestResult will set the exception, and if await() is called this exception will be thrown and the callback not be executed. Since this exception is already stored in the RecordSend I think a better way is not throw exception on await() but let the callback function to handle it. That would make the application code more clean since otherwise the application need so try-catch the await() call.

I think the callback is always executed...if there is a case this doesn't happen it is a bug.

I agree that 
  if(result.hasError())
   // do something
is easier to read. The problem is that it is incumbant on you to check and if you don't it silently fails. This is the complaint people have about mongodb. The principle I am going on is that
   producer.send(message).await()
should be exactly interchangable with a blocking call. Anyhow I am sympathetic to your point, let's move it into the public api discussion.

3. In closing the producer, there is another corner case that the io thread can keep trying to send the rest of the data and failed. Probably we could add another option to drop whatever is in the buffer and let the callback functions of the application to handle them.

I think what you are saying is that close() blocks until all data is sent. That is the intention. Since send is async I think it is counterintuitive to fail/drop in-progress calls as the user may not know their calls aren't completed.","27/Jan/14 11:20;eribeiro;Hello folks,

I've just started to look into your new API design and would like to register a few observations, from an API design perspective, for now. I hope you enjoy my suggestions and, please, let me know what you think about them. Excuse me in advance for the long message. Well, let's start:

It is a good practice to replace the implementation specific (List) of the parameter by a more general (abstract or interface) type so

{code}
Cluster(java.util.List<Node> nodes, java.util.List<PartitionInfo> partitions) 
{code}

becomes

{code}
Cluster(java.util.Collection<Node> nodes, java.util.Collection<PartitionInfo> partitions) 
{code}

This makes it possible to pass a Set, a List, for example. The same goes to 

{code}
bootstrap(java.util.List<java.net.InetSocketAddress> addresses) 
{code}

that becomes

{code}
bootstrap(java.util.Collection<java.net.InetSocketAddress> addresses) 
{code}

This can seem futile, but I have been parts of ZooKeeper API that need to fix a thing, but are literally freezed because their API published a concrete class. :( 

Also, the methods who return a collection should also return a more generic collection so that the swap in the future (say change the List by a Set) doesn't become too difficult. Therefore,

{code}
java.util.List<PartitionInfo>	partitionsFor(java.lang.String topic) 
{code}

becomes

{code}
java.util.Collection<PartitionInfo>	partitionsFor(java.lang.String topic) 
{code}

I have also looked into empty() method. Hey, it returns a new object each time it's called! See

{code}
    /**
     * Create an empty cluster instance with no nodes and no topic-partitions.
     */
    public static Cluster empty() {
        return new Cluster(new ArrayList<Node>(0), new ArrayList<PartitionInfo>(0));
    }
{code}

There's no need to do this. You can create a EMPTY_CLUSTER field as below and then return it each time the method is called. See
{code}
   private static final Cluster EMPTY_CLUSTER = new Cluster(Collections.<Node>emptyList(), Collections.<Node>emptyList());

    ... 

    /**
     * Create an empty cluster instance with no nodes and no topic-partitions.
     */
    public static Cluster empty() {
        return EMPTY_CLUSTER;
    }
{code}

This option saves creation of unnecessary objects, and makes it easy to perform comparison as ""if (myNode == myCluster.empty())"" in the client side.;)

I also see the necessity of adding an 'isEmpty()' method so that users can check if the Cluster is empty. In the case of adding an isEmpty then the declaration of EMPTY_CLUSTER becomes

{code}
   private static final Cluster EMPTY_CLUSTER = new Cluster(Collections.<Node>emptyList(), Collections.<Node>emptyList()) {
							public boolean isEmpty() {
								return true;
							}
						}

{code}

As I said, it was just the first glance over the code. I can possibly have further suggestions, more algorithmic oriented or yet from an API design perspective, but that's all for now. I hope you like it.

Cheers,
Edward Ribeiro","27/Jan/14 22:56;jkreps;Hey Edward,

What I actually want for the Cluster constructor is to make it private to my jar but since that isn't possible I went with public so it is accessible to unit tests. In general that API wouldn't be used as the client tells you about the cluster not vice versa. But there is no harm in using the more general types, so I'll change those to Collection.

The partitionsFor call does need to return a list as one use case for that is being able to fetch by index, which is not part of Collection. This is useful when implementing a partitioner.","28/Jan/14 01:14;guozhang;About reasoning the offset in RecordSend, the value should be

RecordSend.relative_offset + RecordSend.ProduceRequestResult.base_offset.

not just

RecordSend.relative_offset

Right? If yes we'd better update the KafkaProducer's comments accordingly","03/Feb/14 05:58;jkreps;Created reviewboard https://reviews.apache.org/r/17653/
 against branch trunk","04/Feb/14 01:41;jkreps;Created reviewboard https://reviews.apache.org/r/17688/
 against branch trunk","04/Feb/14 11:51;eribeiro;Two review board entries were created for the same issue. Which one is valid? :)","04/Feb/14 16:41;jkreps;The later one. I think I haven't mastered our uber-patch tool.","26/Feb/14 01:11;junrao;Accumulated some more review comments.

20. Regarding VIP. We need to think through another corner case. When the brokers for all replicas of an existing topic die, the admin can start new brokers with existing broker ids to start the topic from scratch. Those new brokers can be added to the VIP. However, if the producer only uses the VIP once during startup, there is no way for the producer to identify the new brokers unless it's restarted.

21. AbstractConfig:
21.1 To be consistent, we probably should change the return value of the following api from Long to long.
   public Long getLong(String key)
21.1 Could we also add getDouble(String key)?

22. Metadata.fetch(): I am a bit puzzled by the following code. Not sure what the intention for the handling of InterruptedException is. This code will be called in the producer client thread, which will probably be interrupted during shutdown. When that happens, we probably should just throw the InterruptedException back to the caller. Otherwise, the producer may have to wait metadataFetchTimeoutMs before it can shut down. Also, it seems that we need to adjust maxWaitMs in each loop. Otherwise, we will be waiting longer than we should.
                try {
                    wait(maxWaitMs);
                } catch (InterruptedException e) { /* this is fine, just try again */
                }

23. Sender: We learned in kafka-1228 that it's not enough to just handle IOException in the following code. UnresolvedAddressException is an Error, not an IOException.
    private void initiateConnect(Node node, long now) {
        try {
            selector.connect(node.id(), new InetSocketAddress(node.host(), node.port()), this.socketSendBuffer, this.socketReceiveBuffer);
            this.nodeStates.connecting(node.id(), now);
        } catch (IOException e) {
            /* attempt failed, we'll try again after the backoff */
            nodeStates.disconnected(node.id());
            /* maybe the problem is our metadata, update it */
            metadata.forceUpdate();
        }
 
24. BufferPool.allocate(): Could we add some comments on when this call is blocked?

25. MemoryRecords: The following constructor doesn't seem to be used.
    public MemoryRecords(int size)

26. Spellings:
26.1 ConfigDef: seperated list, programmatically
26.2 Metadata: ellapsed
","18/Mar/14 17:57;junrao;Addressed commenets 20-26 in kafka-1307.","03/Apr/14 16:33;junrao;Resolving this one since all comments have been addressed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java API to shutdown the connector by using a consumergroup id,KAFKA-1345,12704184,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Not A Problem,,schandr,schandr,28/Mar/14 02:53,28/Mar/14 16:47,12/Jan/21 10:06,28/Mar/14 16:47,,,,,,,,,,,,,,,,,0,,,,,"Is there a way to close the consumerIterator(stream), clean up the associated threads by doing a look up on the consumerGroupId? This would help in use cases where the consumption and sending happens in different threads with different protocols. And if the sending thread is shutdown, the sending thread might want to close the consumption thread.",,nehanarkhede,schandr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-03-28 03:28:03.166,,,false,,,,,,,,,,,,,,,,,,382518,,,Fri Mar 28 16:44:28 UTC 2014,,,,,,,"0|i1twg7:",382786,,,,,,,,,,,,,,,,"28/Mar/14 03:28;nehanarkhede;I'm not so sure I understood that. What do you mean by sending thread? You can call close() on a consumer connector that is always associated with a single group.id. That will close the iterators which will then exit cleanly.","28/Mar/14 14:27;schandr;I have a use case where my application produces messages to the kafka topics. External applications need a way to get the messages from these topics. So we are providing a service to send those messages. When external applications initiates a connection, I am creating the consumerconnector(one consumergroup per external application). When the external application initiate the destroy connection, i should be able to look up the consumerconnector using the group id and initiate a shutdown, since the service will be serving multiple requests.
I can think of storing the consumerconnector instances in a map per request....but was wondering if there is a way i can just pass the consumergroupid and initiate the shutdown.
","28/Mar/14 15:53;nehanarkhede;Currently, one consumer instance is associated only with one group.id. So you will have to keep track of consumer instances and their group ids in order to invoke shutdown() on the right instance.","28/Mar/14 16:44;schandr;Thank you....that's what I thought so...I am keeping track of the consumer instance by the group id and  invoking the shutdown on the related instance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parallelize time-based flushes,KAFKA-190,12530468,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Won't Fix,,jkreps,jkreps,06/Nov/11 02:21,20/Mar/14 22:10,12/Jan/21 10:06,20/Mar/14 22:10,,,,,,,,,,,,,,,,,0,,,,,Currently all time based flushes are done sequentially. If the time-based flush is the only policy that is used this may limit throughput as a single thread may not saturate all the disks. It would be good to make the number of flush threads be configurable. We can probably default it to two threads (threads are cheap).,,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,216206,,,Thu Mar 20 22:10:59 UTC 2014,,,,,,,"0|i029zz:",11221,,,,,,,,,,,,,,,,"20/Mar/14 22:10;jkreps;I think this is less relevant with the new fsync management and replication code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add an API to commit offsets,KAFKA-657,12619154,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jkreps,jkreps,06/Dec/12 16:48,14/Mar/14 03:20,12/Jan/21 10:06,04/Jan/13 22:32,,,,,,,,0.8.1,,,,,,,,,0,project,,,,"Currently the consumer directly writes their offsets to zookeeper. Two problems with this: (1) This is a poor use of zookeeper, and we need to replace it with a more scalable offset store, and (2) it makes it hard to carry over to clients in other languages. A first step towards accomplishing that is to add a proper Kafka API for committing offsets. The initial version of this would just write to zookeeper as we do today, but in the future we would then have the option of changing this.

This api likely needs to take a sequence of consumer-group/topic/partition/offset entries and commit them all.

It would be good to do a wiki design on how this would work and consensus on that first.",,jjkoshy,jkreps,junrao,korebantic2,mumrah,nehanarkhede,sslavic,waldenchen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-993,,,,,,,"12/Dec/12 01:47;mumrah;KAFKA-657v1.patch;https://issues.apache.org/jira/secure/attachment/12560491/KAFKA-657v1.patch","14/Dec/12 01:17;mumrah;KAFKA-657v2.patch;https://issues.apache.org/jira/secure/attachment/12560895/KAFKA-657v2.patch","17/Dec/12 20:36;mumrah;KAFKA-657v3.patch;https://issues.apache.org/jira/secure/attachment/12561350/KAFKA-657v3.patch","20/Dec/12 19:02;mumrah;KAFKA-657v4.patch;https://issues.apache.org/jira/secure/attachment/12561958/KAFKA-657v4.patch","22/Dec/12 03:20;mumrah;KAFKA-657v5.patch;https://issues.apache.org/jira/secure/attachment/12562207/KAFKA-657v5.patch","27/Dec/12 15:46;mumrah;KAFKA-657v6.patch;https://issues.apache.org/jira/secure/attachment/12562478/KAFKA-657v6.patch","02/Jan/13 21:51;mumrah;KAFKA-657v7.patch;https://issues.apache.org/jira/secure/attachment/12562978/KAFKA-657v7.patch","04/Jan/13 15:04;mumrah;KAFKA-657v8.patch;https://issues.apache.org/jira/secure/attachment/12563294/KAFKA-657v8.patch",,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2012-12-07 19:33:05.006,,,false,,,,,,,,,,,,,,,,,,296349,,,Fri Mar 14 02:17:45 UTC 2014,,,,,,,"0|i148pr:",232875,,,,,,,,,,,,,,,,"07/Dec/12 19:33;mumrah;I have started a wiki page for this design discussion https://cwiki.apache.org/confluence/display/KAFKA/Commit+Offset+API+-+Proposal","12/Dec/12 01:47;mumrah;First pass at adding a commit offset request","12/Dec/12 02:56;jkreps;This looks great. To confirm, the final format for the commit response is 
 group [topic [partition offset]]

I think logically there are two phases of work around fixing offset management
1. Add the API and convert the consumer to use it
   a. CommitOffsetRequest/Response (to save your position)
   b. FetchOffsetRequest/Response (to read back a saved position)
   c. Integration into the consumer (using the new api in the scala client)
   d. Unit test coverage for these (say in kafka.integration.PrimitiveApiTest)
2. Move offsets out of zookeeper, since zookeeper doesn't scale well for writes

It would be nice to do (1) more or less together, and if we do it right (2) can be a follow-up item and need not be done by you unless you want to. We can definitely break (1) into successive patches if that is helpful to keep the individual changes small--I am happy to take what you have now if you are up for finishing the other items in (1). I would like to get people to brainstorm a little on (2) in parallel as it could potentially have some impact on (1). We have some time to fiddle with the API if we think of improvements before it would be released and we would have to start versioning changes, though, so we probably don't need to block on that.

So let me know if you are up for the rest of the items in (1)","12/Dec/12 03:04;jkreps;Oh yes, two other things:
1. We don't have a response in this api yet. We should at least have a way to indicate if the request failed (i.e. we got an error writing to zk, etc).
2. Would be good to replace the println with a proper log statement.","12/Dec/12 04:11;mumrah;I'm fine working on the rest of 1. 1a is simple enough, 1c I might need some direction on. For 1b, how exactly is it different from the existing Offsets API? I have never really been clear what the purpose of the old API is.","12/Dec/12 04:23;jkreps;The existing API describes the offset ranges contained in log segments on the server. It is poorly named and we should really rename it to something like LogMetadataRequest and we should really generalize it a bit to include things other than segment offset beginnings. The intended use case for the existing API is for new consumers when they consume for the first time in an existing stream. When they first start consuming they have no position in the log to read from (or to save out using your new api). They want to start consuming, but to start consuming they need a valid offset to start at. What offsets are valid depends on what is available on the server, so they need to be able to ask the server ""what offset ranges do you have"" and then they could chose to start consuming either at the beginning or end of that (or somewhere in the middle). Your api on the other hand answers the question ""what is the latest offset I have 'committed' (i.e. recorded as consumed)."" This would be used when a restart or rebalancing of the consumers occurs. Hope that makes sense? We could rename the existing API as part of this change to avoid the muddle.","12/Dec/12 18:15;mumrah;Thanks [~jkreps], that clears things up quite a bit. Another question I have is around the request envelope (clientId, correlationId, etc).

I understand correlationId is used to allow multiplexing requests/response, but what about replicaId, clientId, etc. I mostly copied these from other Request classes - a bit of cargo-cult programming I guess :)

{code}
    val versionId = buffer.getShort
    val correlationId = buffer.getInt
    val clientId = readShortString(buffer)
    val replicaId = buffer.getInt
{code}

Are all of these necessary for the OffsetCommitRequest/Response? Specifically, is replicaId necessary?","13/Dec/12 05:40;jkreps;I wonder if you could take a look at the updated docs and see if they seem clear. I tried to cover those, but, well, documentation is hard: https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol

Summary:
version id is the version of this api format. In the future if we decide we missed an important field (e.g. lastOffset) we will add it and bump the version number and handle both cases on the server side based on the version we see.
client id is a logical name for the client that could be used across many client servers. This is useful for logging and metrics (i.e. figuring out WHY you are suddenly getting 5x the qps, or whatever) if you have lots of clients.
replica id is just in the fetch request and shouldn't be in this request. A fetch request can be issued either by a normal consumer or by a replica and the broker has slightly different behavior in each case (e.g. whether uncommitted messages are visible...)

","14/Dec/12 01:17;mumrah;v2 of the patch. Includes OffsetFetch and OffsetCommit APIs, some unit tests for both.

This checks for the existence of a topic+partition in ZK before fetching or committing. Not sure if this is the behavior we want - simple enough to change.","17/Dec/12 17:53;jkreps;This looks great!

Three minor things:
1. Can you change the logging for the common case to debug? Our logging policy is that you should be able to run in INFO and have all messages be things you need to know.
2. Can you handle any exceptions from ZK and send back an UnknownException
3. Can you remove the checks on topic/partition validity?

(3) is maybe more controversial. Here is my rationale. First ZK is a huge bottleneck so adding two more zk round-trips will be a problem. Second we actually have two use cases for allowing the user to store offsets for non-existant topics or partitions. The first use case is that if you are doing mirroring between two clusters in different data centers (a common case) it probably makes sense to store the offsets in whatever data center the mirroring process runs in. However there is no requirement that the two clusters have the same partitioning. The second use case is probably specific only to our usage, we have several systems that produce ""offset""-like markers and being able to commit these all together to mark a single ""point in consumption time"" across all systems is nice. ","17/Dec/12 18:38;mumrah;Re 3: Maybe this is a case for the check-and-set functionality I originally proposed. The default case could update ZK with no checks (which would cover your two use cases), and a special case could do the check as well as check the last offset stored for a conditional update. Thoughts?","17/Dec/12 18:57;jkreps;I think that actually covers an orthogonal problem right?
1. Checking topic/partition covers bugs in the client impl that set the wrong values.
2. Check and set catches a bug that might lead to you clobbering your offset due to a concurrency issue where there are two processes both trying to update the same offset.

Originally my concern with (2) was that I wasn't sure if we could implement it in a post-zk world. Now that we wrote up that proposal in a lot more detail I think we can.

We wouldn't want to make the last offset mandatory because in the case that you are manually resetting your offset to 0 (or some low number) you might not know the previous value. But I think what you are proposing is that we could have a current_offset field in the request, and if it is set we would only update the offset if the current offset equals the given offset. We could make it optional by having the value -1 indicate ""don't care, clobber whatever is there"".

The question is, what is the use case for this? Our approach to the scala client has been to ensure mutual exclusion for the consumer processes, at which point this basically can't happen. I wonder how an alternative client implementation could make use of it? It would be good to work that out before including it.","17/Dec/12 19:12;mumrah;Yes, they are somewhat orthogonal. The only real use I can think of is concurrency control around offset updates, however if that stuff has been serialized in the Scala consumer then it is a non-issue. Perhaps it's best to punt on conditional commits for the time being.","17/Dec/12 19:51;jkreps;Makes sense. The API is versioned so we can always add that in as the use cases come up.","17/Dec/12 20:36;mumrah;v3 of the patch

1. info -> debug
2. catching general exceptions and returning UnknownCode
3. removed ZK path checks","20/Dec/12 15:05;mumrah;Jay, the one thing I'm still unclear on are the various failure scenarios. Could you double check that bit of the patch (in KafkaApis.scala)","20/Dec/12 18:26;junrao;I was trying to apply patch v3 in trunk (on a revision before the 0.8 merge patch), but got the following error. Anyone know what the issue is?

git apply -p0 ~/Downloads/KAFKA-657v3.patch
fatal: git apply: bad git-diff - inconsistent new filename on line 5
","20/Dec/12 19:02;mumrah;I regenerated the patch after rebasing from trunk. Jun, see if it works now: git apply -p1 KAFKA-657v4.patch","20/Dec/12 20:04;junrao;Thanks for patch v4. Some comments:

40. Could you add the Apache license header to all new files?

41. SimpleConsumer is a public API. So we need to add the new requests to the javaapi version of SimpleConsumer. We likely need a java version of the new requests/responses.

42. KafkaApis.handle(): Currently, for each type of requests, we catch all unexpected exceptions and send a corresponding response with an error code to the client. We need to do this for the 2 new types of requests too.

43. Do we plan to use the new API to commit offsets in the high level consumer?","20/Dec/12 20:45;jkreps;It probably makes sense to do this in two phases. Let's get the patch in that adds the api, then make the changes to the consumers to make use of it as phase 2.","20/Dec/12 21:24;jkreps;Also I had one more generalization I would like to add--metadata field. Sent proposal to the dev list. If no objections I will update the wiki.","22/Dec/12 03:20;mumrah;Attaching patch v5. Added ASL header and javaapi. Not really sure how to test the javaapi wrapper - tried writing a Java junit test but SBT was not too happy about that.

Jun, regarding point 42, I'm not sure what you mean, I have the ""case e => ..."" in both handle functions. What more do I need to add for exception handling here?

Jay, this does not include the metadata field. I will work on that next.","24/Dec/12 00:52;nehanarkhede;+1 on v5. Just a minor comment which I think Jun raised as well - 

KafkaApis
Today, we have a pretty awkward way of handling error codes for the various APIs in the handle() method. We should probably fix that, but until then, it will be good to maintain consistency. The new APIs are missing that error handling.","27/Dec/12 15:46;mumrah;Finally figured out what you guys meant by adding error handling to the handle method (the method named ""handle"", of course). Attaching v6 of the patch.","02/Jan/13 19:02;mumrah;Post-holiday bump. Anyone had a chance to look at the v6 patch?","02/Jan/13 20:43;jjkoshy;Hey David, the patch (and the wiki) looks great.

- For error handling. I think what Jun was referring to is the giant catch clause in handle - i.e., the new keys
  should be added as a case. That junk block of code really needs to be cleaned up :)
- KafkaApis: if(offsetStr == null) : I don't think this can happen.
- Default client id should probably be """" in all the request/responses i.e., to follow convention.
- It would be better to use 1.toShort or val CurrentVersion: Short = 1 (instead of 1.shortValue); although it's more
  or less a non-issue as it's in the object.
","02/Jan/13 21:51;mumrah;Attaching patch v7 - addresses Joel's comments.","02/Jan/13 23:22;junrao;Thanks for patch v7. Some more comments:

70. OffsetFetchResponse: requestInfo can be defined as Map[TopicAndPartition, (Long, Short)] without referencing Tuple2 directly.

71. KafkaApis.handle():
              case (topicAndPartition) => (topicAndPartition, Tuple2(-1L, ErrorMapping.codeFor(e.getClass.asInstanceOf[Class[Throwable]]))) can be
              case (topicAndPartition) => (topicAndPartition, (-1L, ErrorMapping.codeFor(e.getClass.asInstanceOf[Class[Throwable]])))
Also, instead of using -1, can we define a constant like InvalidOffset? Use that constant in handleOffsetFetchRequest() too.

72. javaapi.OffsetCommitResponse and javaapi.OffsetFetchResponse:  Remove unused import ByteBuffer.

73. javaapi.OffsetFetchResponse: Tuple2 is a scala thing. It would be weird to return that directly to the java caller. One way is to change the type of requestInfo in the scala version of OffsetFetchResponse to Map[TopicAndPartition, OffsetAndError]. This also follows our convention of limiting the usage of Tuple.

74. java version of OffsetCommit{Request/Response} and OffsetFetch{Request/Response}: We don't really need the methods sizeInBytes() and writeTo(). They are only used for serialization and we don't serialize the java version of the request/response. This issue seems to already exist in the java version of OffsetRequest and FetchRequest. Could you remove these two methods there too?

75. OffsetCommitTest:
75.1 Remove unused imports.
75.2 Wrong spelling non-existant.
75.3 When handling OffsetCommitRequests, we do exactly the same thing whether a topic/partition exists in KafkaApis. So, it doesn't seem that we need to test unknown topic/partition separately.



","02/Jan/13 23:48;jkreps;Looks good to me. A few things in addition to the other comments:
76. We baselined the other request versions to 0, but this one is starting at 1. Can we change it to 0 for consistency?
77. It would be good to add the metadata string field.","03/Jan/13 19:47;mumrah;We had talked about limiting the size of the metadata to 1k or something and making this configurable. My question is: where does this config belong, and how to get it into KafkaApis? Right now, I've added this config to KafkaConfig and have passed in the ""config"" variable from KafkaServer to KafkaApis.

Also, for the offset payloads, I'm opting to store them as a JSON object in ZooKeeper: {""offset"": 42, ""metadata"": ""foo""}","03/Jan/13 21:00;jkreps;Yeah that is the right place for a new config. It is worth discussing the name as part of the review since this ends up being kind of part of our ""api"" to the operator.

I would just skip storing the metadata for now (i.e. just throw it away). If we make the change in zk we need a script to grandfather from the old format to the new format. Since we will need a conversion script when we move off zk anyway it makes sense to avoid two conversions for users (one to add the metadata, and another to move off zk).","04/Jan/13 00:22;mumrah;I have ""offset.metadata.max.size"" for the time being with a default of 1024
","04/Jan/13 00:30;jkreps;Works for me.","04/Jan/13 15:04;mumrah;Attaching v8

70. I have consolidated (offset, metadata, error) into a case class OffsetMetadataAndError
71. Added InvalidOffset=-1 and NoMetadata="""" to OffsetMetadataAndError as constants
72. Done
73. Fixed by using OffsetMetadataAndError
74. Done
75. Removed: redundant test cases, unused imports, misspellings
76. Done
77. Added metadata to OffsetCommitRequest and OffsetFetchResponse. Currently not doing anything with the metadata

Other changes:

* Added ""offset.metadata.max.size"" to KafkaConfig, default 1024
* Pass KafkaConfig from KafkaServer to KafkaApis (through the constructor). Not sure if there is a more elegant way to do this, but this works
* Added OffsetMetadataTooLargeException
* Better error handling (maybe?) in the handle* methods. Instead of returning UnknownCode always, I handle specific cases and the catch-all uses ErrorMapping.codeFor instead of just UnknownCode


Also, I found a bug in ApiUtils.writeShortString while testing this code. I opened up KAFKA-680 with a fix attached. Until that fix is incorporated, OffsetCommitTest.testLargeMetadataPayload will fail.","04/Jan/13 17:37;jkreps;This looks good to me. If no further objections I am taking this on trunk.","04/Jan/13 21:15;nehanarkhede;+1 on v8","04/Jan/13 22:32;jkreps;Committed.","04/Jan/13 22:45;junrao;v8 looks good. Just some minor comments.

80. OffsetCommitTest.testCommitAndFetchOffsets(): Could you remove the commented out code? Also, remove unused imports.

81. We are trying to standardize the config names in kafka-648. Should we rename offset.metadata.max.size to offset.metadata.max.bytes?","04/Jan/13 23:42;mumrah;Jun, 

80. Those commented out tests will be valid once the metadata is actually stored. I left it to save someone the effort later on

81. +1

Since this patch has been committed maybe you or Jay can just make these changes on trunk.","05/Jan/13 00:03;jkreps;I updated the property name. Normally I am against commenting out code since it is in version control anyway, but in this case those tests are actually useful and not in version control so probably makes sense to leave them.","14/Mar/14 02:17;korebantic2;All,

I'm looking to update a client to support this new addition to 0.8.1. I just wanted to check-in, does the guide here reflect the latest for the protocol? 

https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-OffsetCommit/FetchAPI

It hasn't been updated since December so just wanted to make sure.


",,,,,,,,,,,,,,,,,,,,,,
Avoid fsync on log segment roll,KAFKA-615,12616311,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,15/Nov/12 21:17,03/Mar/14 17:39,12/Jan/21 10:06,06/Aug/13 04:41,,,,,,,,0.8.1,,,,,,,,,0,,,,,"It still isn't feasible to run without an application level fsync policy. This is a problem as fsync locks the file and tuning such a policy so that the flushes aren't so frequent that seeks reduce throughput, yet not so infrequent that the fsync is writing so much data that there is a noticable jump in latency is very challenging.

The remaining problem is the way that log recovery works. Our current policy is that if a clean shutdown occurs we do no recovery. If an unclean shutdown occurs we recovery the last segment of all logs. To make this correct we need to ensure that each segment is fsync'd before we create a new segment. Hence the fsync during roll.

Obviously if the fsync during roll is the only time fsync occurs then it will potentially write out the entire segment which for a 1GB segment at 50mb/sec might take many seconds. The goal of this JIRA is to eliminate this and make it possible to run with no application-level fsyncs at all, depending entirely on replication and background writeback for durability.",,guozhang,jkreps,junrao,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/13 20:41;jkreps;KAFKA-615-v1.patch;https://issues.apache.org/jira/secure/attachment/12591090/KAFKA-615-v1.patch","08/Jul/13 18:49;jkreps;KAFKA-615-v2.patch;https://issues.apache.org/jira/secure/attachment/12591253/KAFKA-615-v2.patch","11/Jul/13 15:35;jkreps;KAFKA-615-v3.patch;https://issues.apache.org/jira/secure/attachment/12591860/KAFKA-615-v3.patch","17/Jul/13 21:50;jkreps;KAFKA-615-v4.patch;https://issues.apache.org/jira/secure/attachment/12592869/KAFKA-615-v4.patch","03/Aug/13 05:10;jkreps;KAFKA-615-v5.patch;https://issues.apache.org/jira/secure/attachment/12595733/KAFKA-615-v5.patch","04/Aug/13 21:07;jkreps;KAFKA-615-v6.patch;https://issues.apache.org/jira/secure/attachment/12595829/KAFKA-615-v6.patch","05/Aug/13 19:55;jkreps;KAFKA-615-v7.patch;https://issues.apache.org/jira/secure/attachment/12596203/KAFKA-615-v7.patch","05/Aug/13 21:33;jkreps;KAFKA-615-v8.patch;https://issues.apache.org/jira/secure/attachment/12596224/KAFKA-615-v8.patch",,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,2013-08-04 17:26:07.914,,,false,,,,,,,,,,,,,,,,,,258072,,,Tue Aug 06 04:41:42 UTC 2013,,,,,,,"0|i0kmcf:",118437,,,,,,,,,,,,,,,,"15/Nov/12 21:41;jkreps;Here is a quick thought on how this might work to kick things off. A few things to consider:
1. Currently recovery works on a per segment basis as the index is recreated from scratch. It will be tricky to do partial segment recovery. I recommend we avoid this. If we want to speed up recovery we can always just make segment sizes smaller.
2. We need to guarantee that fsync can never happen on the active segment under any circumstances for this to really work--the syncing must be fully async since the fsync's may be very slow. To make this work we need to somehow record what has and has not been fsync'd.
3. One complicating factor is that an inactive segment may become active once again because of a truncate operation. In this case more messages will be appended and we will need to fsync it again after it is rolled again. If we crash in the intervening time we can't think the old fsync counted after the truncation and further appends.

My thought was to keep a text file containing:
  topic partition offset
This file would be populated by a background thread that would flush segments and update the file. At recovery time we would use this file to determine the latest flushed segment for each topic/partition and only recover segments newer than this. This would generally mean recovering only the last segment.
Some notes on maintaining this file:
- When a segment is flushed we would immediately append a new entry to this file without fsyncing. Doing this as appends means that we need only write out a small bit of data incrementally. Losing an entry is usually okay, it would just mean we would do an unnecessary recovery so background flush should usually be okay (exception noted before).
- We would keep an in memory map with the latest offset for each topic/partition.
- To avoid the log growing forever we would periodically write out the full contents of the map to a new file and swap this for the old log.
- We should probably keep one of these files for each data directory.
- The case of truncate (i.e. moving the offset backwards) needs to be thought through. In this case I think we may need to immediately fsync the file to avoid a case where we lose an entry in the file and therefor think we have flushed more than we actually have.

We should probably refactor the existing background flush thread so it would now handle both the time based flush and the flushing of old segments just to avoid having two threads and since they are very similar in their role. Note that it is fine to have both running, since if the data is already flushed due to a time or interval rule, then the flushing of the old segment can just proceed and will essentially be a no-op (since the file will have no dirty pages).

I recommend we leave the existing time and interval flush policies in place but now default both to infinity. These will now be rarely used options mostly useful for people who are paranoid, running only a single broker, or using a crappy filesystem where fsync locks the everything.

In terms of code structure it is quite tricky and I don't quite know how to do it. A lot of the challenge is that the flush thread and flush journal file will be global for all logs, but the roll() call is at the log level. The first question is how do we know what segments to flush? One way would be to have the background thread just periodically (say every 30 seconds) scan all logs and see if they have any new segments that need flushing. The downside of this is that it is O(n) in terms of the number of logs, which perhaps is not ideal. Another way would be to have a roll() somehow enqueue a flush operation for the background thread to carry out. The later case may be more efficient but tangles the log with the layers above it. It would be good to work out the details of how this would work up front.","06/Jul/13 20:41;jkreps;Attached a draft patch for a first version of this for early feedback. A few details remain to work out.

This patch removes the per-data-directory .kafka_cleanshutdown file as well as the concept of a ""clean shutdown"". The concept of clean shutdown is replaced with the concept of ""recovery point"". The recovery point is the offset from which the log must be recovered. Recovery points are checkpointed in a per-data-directory file called recovery-point-offset-checkpoint. This uses normal offset checkpoint file format.

Previously we always recovered the last log segment unless a clean shutdown was recorded. Now we recover from the recovery point--which may mean recovering many segments. We do not, however, recover partial segments: if the recovery point falls in the middle of a segment we recover that segment from the beginning.

On shutdown we force a flush and checkpoint which has the same effect as the cleanshutdown file did before.

Deleting the recovery-point-offset-checkpoint file will cause running full recovery on your log on restart which is kind of a nice feature if you suspect any kind of corruption in the log.

Log.flush now takes an offset argument and flushes from the recovery point up to the given offset. This allows more granular control to avoid syncing (and hence locking) the active segment.

Log.roll() now uses the scheduler to make its flush asynchronous. This flush now only covers up to the segment that is just completed, not the newly created segment, so there should be no locking of the active segment any more.

The per-topic flush policy based on # messages and time still remains but now it defaults to off so we rely only on 

I did some preliminary performance testing and we can indeed run with no application-level flush policy with reasonable latency which is both convenient (no tuning to do) and yields much better throughput. I will do more testing and report results.","08/Jul/13 18:49;jkreps;New patch with a couple of improvements:
1. Found and fixed a bug in recovery that lead to recovering logs even in clean shutdown case.
2. Now we always resize indexes for all segments during recovery as the index size may change. Not doing this was a bug in the previous patch.
3. I now force a checkpoint of the recovery points in ReplicaManager.becomeLeaderOrFollower() to handle crashes after log truncate.
4. Added a unit test that intentionally corrupts a log and checks recovery.

I also did some performance testing on my desktop machine. We can sustain very high throughput, but as we approach the maximum throughput of the drive latency will get worse and worse.

But as one data point I could do 75Mb/sec sustained writes across 500 logs on a single drive machine that can do a peek of 120MB/sec with avg write latency of < 1ms and maximum latency of about 350ms.","11/Jul/13 15:35;jkreps;Patch version v3:
- Found a call to flush the index in Log.roll(). Removed this.","17/Jul/13 21:50;jkreps;Rebased patch to trunk.","03/Aug/13 05:10;jkreps;Attach updated patch v5. Rebased against trunk and with added support for compression in the write throughput test.","04/Aug/13 17:26;nehanarkhede;Thanks for all the patches. Overall, it's a good patch. Few minor comments -

Log

1. Remove recoverFrom in the comments for Log
2. Remove count from comments for LogAppendInfo
3. In recoverLog(), it seems that it makes sense to break out of the while loop as soon as you find a segment that needs to be truncated. Right now, it goes back and tries to iterate through remaining unflushed segments which have been deleted","04/Aug/13 21:07;jkreps;Updated patch:
- Removed bad scaladoc
- Improved log corruption test to cover corruption in a non-final segment to show that the existing logic works

Actually the recoverLog method is right. It loops through the unflushed segments validating them. When it finds a bad one it truncates to the right position in that segment and then loops over all remaining segments and deletes them. The confusing part, I think, is that unflushed is an iterator so unflushed.foreach(deleteSegment) actually ends the loop because a post condition of that is that unflushed.hasNext is false. I agree that is kind of tricky. Not sure if there is a more clear way to do that (I tried, that was what I came up with...wish we had break).","04/Aug/13 22:02;junrao;Thanks for patch v5. Some comments:

50. Log:
50.1 recoveryLog(): It seems that recoveryPoint can be > lastOffset due to truncation on unclean shutdown. See the comment in 52.2.

50.2 The comment in the following code is no longer correct since it's not just recovering the active segment. Also, it seems that if we hit the exception, we should delete the rest of the segments after resetting current segment to startOffset.
        } catch {
          case e: InvalidOffsetException => 
            val startOffset = curr.baseOffset
            warn(""Found invalid offset during recovery of the active segment for topic partition "" + dir.getName +"". Deleting the segment and "" +
                 ""creating an empty one with starting offset "" + startOffset)
            // truncate the active segment to its starting offset
            curr.truncateTo(startOffset)
        }

50.3 the log flusher scheduler is multi-threaded. I am wondering if that guarantees that the flushes on the same log will complete in recovery point order, which is important?

51. LogSegment.recover(): the comment for the return value is incorrect. We return truncated bytes, not messages.

52. ReplicaManager:
52.1 The checkpointing of recovery point can be done once per LeaderAndIsr request, not per partition.
52.2 There is this corner case that I am not sure how to handle. Suppose that we truncate a log and immediately crash before flushing the recovery points. During recovery, we can happen is that a recovery point may be larger than logEndOffset. However, the log may need recovery since we don't know whether the flushing on truncated data succeeded or not. So, perhaps what we can do is that in recoveryLog(), if (lastOffset <= this.recoveryPoint), we force recover the last segment?

53. Could you verify that the basic system test works?
","05/Aug/13 03:11;jkreps;Ah thanks for the detailed review:
50.2 Yes, nice.
50.3 I thought of this but don't think it is a problem. Flushes are always up to a particular recovery point. So let us say that we are flushing on every offset and flush(100) and flush(101) are reordered since they are async. That is actually okay, flush(100) will not actually write any data, and the check to update the recovery point is always done in a lock to ensure it doesn't get clobbered by out of order flushes. Let me know if you see something I am missing.
51. Yup.
52.1 Good point
52.2 Yeah I thought of this too. My claim is that it is okay as long as the usage is something like (1) stop writes, (2) flush the checkpoints, (3) take new writes.  If we do this then there are two cases (1) the metadata write that truncated the file occurred, or (2) it did not occur. If it did not occur then it is no different then if we crashed prior to the truncate. If it did occur and if the log end is before the recovery point then that is still fine because that is stable storage (we just masked off the end of the log) so we don't need to recover that. The troublesome case is if we take writes before flushing the checkpoint, then we are in trouble. The question is whether those assumptions actually hold? However, I think there is one bug at least that you lead me to which is that I need to ensure the recovery point is then reduced to the log end, or else after we append data we would think it was flushed. Let me know if you buy this analysis.
FWIW I actually think our truncate logic may have a hole in 0.8 because we always recover the last segment only. However consider a case where we we truncate off the last segment, making a previous segment active, then we take some writes (but no flush) and then crash. In this case it is possible that the segment we truncated off reappears and also that we have partial writes and old data in the prior segment. But on recovery we will only check the zombie segment and ignore the prior segment.
One way to simplify some of the reasoning would just be to fsync on truncate which it doesn't look like we do now. That would help us out of a lot of corner cases. The downside is it may add a lot of time to the becomeFollower because of the burst of writes.
53 Will do

Let me know what you think about those two discussion points. I would rather fully think this through now than chase 1 in a million bugs later.","05/Aug/13 15:05;junrao;50.3 Yes, I think your reasoning is correct. I didn't look at the code carefully enough.

52.2 For the first part, that was my initial analysis too. Then, I was thinking the file system has to flush both the metadata and the data. During a crash, could the last segment be in a state that it's metadata (and thus length) is flushed, but the actual data is not. Does flush guarantee data is flushed before the metadata? Forcing a flush on every truncate is safe, but will delay the processing of the LeaderAndIsr request and it's probably too pessimistic. That's why I was thinking of running recovery on the last segment during startup if lastOffset < this.recoveryPoint.

For the second part, the hole that you described in current 0.8 won't happen since we force a flush on log rolling.","05/Aug/13 17:05;sriramsub;1. Log.scala
1.1 Doc fix. I see two of this - * @param recoveryPoint The offset at which to begin recovery--i.e. the first offset which has not been flushed to disk

2. ReplicaManager.scla
2.1 How does this help? We could always crash before calling this and we should still do the right thing on recovery.
  
3. TestLogPerformance
3.1 Should we make the tool consistent like the rest (w.r.t args)?
3.2 I am assuming we are testing with the default pdflush interval. Should we be able to control the flush interval to test performance? 
3.3 Could you add a comment on what this tool does and how is it different from the LinearWriteTest tool
","05/Aug/13 17:28;jkreps;1.1 Good catch
2.1 My desire is to ensure that checkpoints are flushed prior to taking new writes.
3.1 TestLogPerformance is deleted in this patch. Previously TestLogPerformance tested the log and LinearWriteTest tested writing to files. But as I did the latency testing LinearWriteTest got better and better. Finally I just added the log testing into that tool. I deleted it as part of this patch because it was a subset of the functionality in LinearWriteTest.
3.2 In all our tests we test with the filesystem, mount options, and configuration you have. I think trying to control OS-specific thing from the tool is overkill.","05/Aug/13 17:39;jkreps;Jun. 
52.2 In most filesystems there is no guarantee that metadata is flushed before/after/atomically with data. Ext3/4 has some guarantee with data=ordered, but this has other issues and we should not rely on it, and we don't want to require users run a particular mount option with their fs. What I am saying is that I think we cover that case. Casewise:
1. If the  truncation point is after than the recovery point, then the recovery point remains valid.
2. If the truncation point is before the recovery point then the log is stable up to the truncation point. So in a crash either
    a. The metadata flush occurs and the log is correctly truncated, or else
    b. The metadata flush doesn't occur and we regain segments of log. This is just the same as if we hadn't truncated the log. The log contents remain stable.

What I think is a problematic case is if
a. We truncate to to offset T  where T is less than the recovery point R.
b. We take new writes at offset T, T+1, T+2
c. Then we checkpoint the recovery point at T+2
If this occurred then we have messages T, T+1, etc which have not been flushed but which are below the recovery point. The question I was asking is, can this happen? I don't fully understand how the fetching restarts in the leader change so I am not sure.","05/Aug/13 19:13;jkreps;Okay it looks like the fetch requests are only stopped in Partition.makeFollower, so though my reasoning may be right the assumption is wrong.

Here is my proposal:
1. I change this patch to checkpoint the recovery point in Partition.makeFollower. This will be inefficient since we will do this checkpoint once per partition that we become a slave for.
2. I open a second JIRA to optimize the checkpoint logic.

Here is the proposal:
1. We add ReplicaManager.addFetchers and .removeFetchers which adds or removes a bunch of fetchers all at once
2. We add LogManager.truncateTo(m: Map[TopicAndPartition, Long). This method will first checkpoint the recovery point, then do the truncates. This is better because it gets the recovery point stuff out of ReplicaManager.
3. We change ReplicaManager.becomeLeaderOrFollower to stop fetchers for all slaves undergoing the change all at once, do the truncate, then start all those fetchers. This may be faster than what we currently have due to no longer having to fight for the fetcher lock many times while IO and network is happening.","05/Aug/13 19:55;jkreps;Patch which uses the correct but slow approach of synchronously committing the checkpoint each time we truncate before fetching restarts.","05/Aug/13 21:03;sriramsub;Do you not want to reset the recoveryPoint to the logEndOffset on startup? If logEndOffset is less than the recoveryPoint on startup, I think we could end up getting writes to the truncated offsets and we would not flush them. No?","05/Aug/13 21:33;jkreps;Ack, yes, I did mean to fix the recoveryPoint/logEndOffset issue, I just forgot. Attached v8 which includes that. The fix is as you describe--I just reset the recovery point to the end of the log.","05/Aug/13 21:38;sriramsub;+1","05/Aug/13 23:50;jkreps;Filed KAFKA-1001 to handle the follow-up work to optimize the checkpointing during failover.","05/Aug/13 23:50;jkreps;Also, Jun, basic system tests all pass with v8.","06/Aug/13 04:16;junrao;Thanks for patch v8. Just the following comment. Otherwise, +1.

In 50.2, the warning message still refers to active segment.","06/Aug/13 04:41;jkreps;Ah, nice catch. Fixed the warning message and committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tool for checking the consistency among replicas,KAFKA-1117,12677297,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,04/Nov/13 00:30,21/Nov/13 05:43,12/Jan/21 10:06,21/Nov/13 05:43,0.8.1,,,,,,,0.8.1,,,core,,,,,,0,,,,,,,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/13 00:34;junrao;KAFKA-1117.patch;https://issues.apache.org/jira/secure/attachment/12611845/KAFKA-1117.patch","11/Nov/13 16:44;junrao;KAFKA-1117_2013-11-11_08:44:25.patch;https://issues.apache.org/jira/secure/attachment/12613175/KAFKA-1117_2013-11-11_08%3A44%3A25.patch","12/Nov/13 16:35;junrao;KAFKA-1117_2013-11-12_08:34:53.patch;https://issues.apache.org/jira/secure/attachment/12613393/KAFKA-1117_2013-11-12_08%3A34%3A53.patch","14/Nov/13 16:24;junrao;KAFKA-1117_2013-11-14_08:24:41.patch;https://issues.apache.org/jira/secure/attachment/12613864/KAFKA-1117_2013-11-14_08%3A24%3A41.patch","18/Nov/13 17:58;junrao;KAFKA-1117_2013-11-18_09:58:23.patch;https://issues.apache.org/jira/secure/attachment/12614433/KAFKA-1117_2013-11-18_09%3A58%3A23.patch",,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2013-11-20 21:27:36.775,,,false,,,,,,,,,,,,,,,,,,356672,,,Thu Nov 21 05:43:44 UTC 2013,,,,,,,"0|i1phkv:",356962,,,,,,,,,,,,,,,,"04/Nov/13 00:34;junrao;Created reviewboard https://reviews.apache.org/r/15201/
","11/Nov/13 16:44;junrao;Updated reviewboard https://reviews.apache.org/r/15201/
 against branch origin/trunk","12/Nov/13 16:35;junrao;Updated reviewboard https://reviews.apache.org/r/15201/
 against branch origin/trunk","14/Nov/13 16:24;junrao;Updated reviewboard https://reviews.apache.org/r/15201/
 against branch origin/trunk","18/Nov/13 17:58;junrao;Updated reviewboard https://reviews.apache.org/r/15201/
 against branch origin/trunk","20/Nov/13 17:21;junrao;Thanks for the reviews. Committed to trunk.","20/Nov/13 21:27;swapnilghike;Hey Jun, after committing this patch, builds with scala 2.10.* are breaking, could you please take a look:

[error] /home/sghike/kafka-server/kafka-server_trunk/kafka/core/src/main/scala/kafka/tools/ReplicaVerificationTool.scala:364: ambiguous reference to overloaded definition,
[error] both constructor FetchResponsePartitionData in class FetchResponsePartitionData of type (messages: kafka.message.MessageSet)kafka.api.FetchResponsePartitionData
[error] and  constructor FetchResponsePartitionData in class FetchResponsePartitionData of type (error: Short, hw: Long, messages: kafka.message.MessageSet)kafka.api.FetchResponsePartitionData
[error] match argument types (messages: kafka.message.ByteBufferMessageSet) and expected result type kafka.api.FetchResponsePartitionData
[error]         replicaBuffer.addFetchedData(topicAndPartition, sourceBroker.id, new FetchResponsePartitionData(messages = MessageSet.Empty))
[error]     ","21/Nov/13 05:43;junrao;This is resolved now. It seems that unit tests were already broken because of this issue. It's just that we never run unit tests under 2.10.1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add server config parameter to separate bind address and ZK hostname,KAFKA-1092,12674414,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,theduderog,theduderog,theduderog,17/Oct/13 21:57,31/Oct/13 04:08,12/Jan/21 10:06,31/Oct/13 04:08,0.8.1,,,,,,,0.8.1,,,config,,,,,,0,,,,,"Currently, in server.properties, you can configure host.name which gets used for two purposes: 1) to bind the socket 2) to publish the broker details to ZK for clients to use.

There are times when these two settings need to be different.  Here's an example.  I want to setup Kafka brokers on OpenStack virtual machines in a private cloud but I need producers to connect from elsewhere on the internal corporate network.  With OpenStack, the virtual machines are only exposed to DHCP addresses (typically RFC 1918 private addresses).  You can assign ""floating ips"" to a virtual machine but it's forwarded using Network Address Translation and not exposed directly to the VM.  Also, there's typically no DNS to provide hostname lookup.  Hosts have names like ""fubar.novalocal"" that are not externally routable.

Here's what I want.  I want the broker to bind to the VM's private network IP but I want it to publish it's floating IP to ZooKeeper so that producers can publish to it.

I propose a new optional parameter, ""listen"", which would allow you to specify the socket address to listen on.  If not set, the parameter would default to host.name, which is the current behavior.

#Publish the externally routable IP in ZK
host.name = <floating ip>
#Accept connections from any interface the VM knows about
listen = *",,junrao,theduderog,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/13 21:42;theduderog;KAFKA-1092.patch;https://issues.apache.org/jira/secure/attachment/12611182/KAFKA-1092.patch","28/Oct/13 21:29;theduderog;KAFKA-1092.patch;https://issues.apache.org/jira/secure/attachment/12610686/KAFKA-1092.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2013-10-30 17:33:56.887,,,false,,,,,,,,,,,,,,,,,,354036,,,Thu Oct 31 04:08:27 UTC 2013,,,,,,,"0|i1p1db:",354328,,,,,,,,,,,,,,,,"28/Oct/13 21:30;theduderog;Added two new configuration parameters:

advertise.host.name - Hostname the broker will advertise to producers and consumers. If not set, it uses the value for ""host.name"" if configured.  Otherwise, it will use the value returned from java.net.InetAddress.getCanonicalHostName().

advertise.port - The port to publish to ZooKeeper for clients to use. If this is not set, it will publish the same port that the broker binds to.","28/Oct/13 21:35;theduderog;Note: this change is backward compatible.  Not specifying the two new optional parameters will yield the same behavior as before","30/Oct/13 17:33;junrao;Thanks for the patch. 

1. Could you change the property name to advertised.host.name and advertised.port?

2. All unit tests in kafka.producer.ProducerTest seem to fail with this patch.","30/Oct/13 21:42;theduderog;Updated parameter names to ""advertised.host.name"" and ""advertised.port"" and fixed broken ProducerTest","30/Oct/13 21:50;theduderog;The reason the kafka.producer.ProducerTest was broken was because that test was creating an anonymous subclass of KafkaConfig and overriding a couple of members.  I don't know Scala that well but discovered that statically initialized variables that depend on other statically initialized variables behave unexpectedly.  To fix this, I just set properties in the test and do not subclass KafkaConfig, as users would do.

{code}
class A() {
     val x = 1
     val y = x
}

class B() extends A {
     override val x = 2
}

val b = new B()
b.y //this is 0, not 1 or 2
{code}","31/Oct/13 04:08;junrao;Thanks for the patch. +1. Committed to trunk after adding the ASF header to the new test file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka patch review tool,KAFKA-1053,12668120,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,11/Sep/13 19:41,18/Sep/13 03:59,12/Jan/21 10:06,18/Sep/13 03:55,,,,,,,,,,,tools,,,,,,0,,,,,Created a new patch review tool that will integrate JIRA and reviewboard - https://cwiki.apache.org/confluence/display/KAFKA/Kafka+patch+review+tool,,jjkoshy,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/13 16:40;nehanarkhede;KAFKA-1053-2013-09-15_09:40:04.patch;https://issues.apache.org/jira/secure/attachment/12603239/KAFKA-1053-2013-09-15_09%3A40%3A04.patch","13/Sep/13 22:34;nehanarkhede;KAFKA-1053-followup.patch;https://issues.apache.org/jira/secure/attachment/12603133/KAFKA-1053-followup.patch","13/Sep/13 23:33;nehanarkhede;KAFKA-1053-followup2.patch;https://issues.apache.org/jira/secure/attachment/12603160/KAFKA-1053-followup2.patch","12/Sep/13 17:04;nehanarkhede;KAFKA-1053-v1.patch;https://issues.apache.org/jira/secure/attachment/12602827/KAFKA-1053-v1.patch","11/Sep/13 20:44;nehanarkhede;KAFKA-1053-v1.patch;https://issues.apache.org/jira/secure/attachment/12602662/KAFKA-1053-v1.patch","11/Sep/13 19:55;nehanarkhede;KAFKA-1053-v1.patch;https://issues.apache.org/jira/secure/attachment/12602650/KAFKA-1053-v1.patch","12/Sep/13 21:22;nehanarkhede;KAFKA-1053-v2.patch;https://issues.apache.org/jira/secure/attachment/12602880/KAFKA-1053-v2.patch","12/Sep/13 21:23;nehanarkhede;KAFKA-1053-v3.patch;https://issues.apache.org/jira/secure/attachment/12602882/KAFKA-1053-v3.patch","16/Sep/13 03:28;nehanarkhede;KAFKA-1053_2013-09-15_20:28:01.patch;https://issues.apache.org/jira/secure/attachment/12603269/KAFKA-1053_2013-09-15_20%3A28%3A01.patch","16/Sep/13 21:40;nehanarkhede;KAFKA-1053_2013-09-16_14:40:15.patch;https://issues.apache.org/jira/secure/attachment/12603453/KAFKA-1053_2013-09-16_14%3A40%3A15.patch",,,,,,,10.0,,,,,,,,,,,,,,,,,,,,2013-09-11 19:51:13.408,,,false,,,,,,,,,,,,,,,,,,348054,,,Wed Sep 18 03:55:08 UTC 2013,,,,,,,"0|i1o0l3:",348350,,,,,,,,,,,,,,,,"11/Sep/13 19:51;tejasp;(1) In [0], ""Setup"" -> hyperlinks on steps 1 and 2 loop to the same webpage.

(2) I don't have much idea about the right place where the "".reviewboardrc"" file should be, but it would be a good idea to commit it in the codebase like [1]. Also, add it to .gitignore (like [2]).

(3) How about adding ""kafka-rb.py"" to kafka codebase ? With that *maybe* there won't be any need for JIRA_CMDLINE_HOME.
 
(4) In kafka-rb.py:

>  popt.add_argument('-s', '--summary', action='store', dest='summary', required=False, help='Summary for the reviewboard')
>  popt.add_argument('-d', '--description', action='store', dest='description', required=False

I am wondering if someone doesn't provide a summary and as its an optional param, the script won't complain. Eventually, RB dashboard would end up having a bunch of tickets with no summary or title.

(6) >     print 'Creating reviewboard'
Could this message sound good: ""Generating a new review board ticket"" ?

(7) Is there a way to specify the ""Testing Done"" text of RB through this script ?

[0] : https://cwiki.apache.org/confluence/display/KAFKA/Kafka+patch+review+tool
[1] : https://issues.apache.org/jira/browse/GIRAPH-331
[2] : https://issues.apache.org/jira/browse/TAJO-69","11/Sep/13 20:18;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/14091/","11/Sep/13 20:43;nehanarkhede;Thanks for the quick review Tejas

1. I believe Guozhang fixed this
2. Updated the reviewboard to include the .reviewboardrc file for checkin to the codebase
3. Added kafka-patch-review.py to the reviewboard for checkin to the codebase
4. Added default summary ""Patch for KAFKA-...""
6. There are only 2 review board tasks a) Creating a new reviewboard b) Updating an existing reviewboard. Hopefully ""Creating a new reviewboard"" explains that better
7. Added a ""--testing-done"" option to the script","11/Sep/13 20:44;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/14091/","12/Sep/13 17:04;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/14091/","12/Sep/13 17:06;nehanarkhede;Any feedback from other committers [~junrao], [~jjkoshy] ?","12/Sep/13 18:43;jjkoshy;I'll take a look today - would like to try it out as well","12/Sep/13 21:22;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/14091/
","12/Sep/13 21:23;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/14091/
","13/Sep/13 22:34;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/14091/
","13/Sep/13 23:33;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/14091/
","14/Sep/13 00:11;jjkoshy;Ran into this while following the instructions. May be some python version
conflict but throwing it out there in case someone encountered this and
worked around it:

[1709][jkoshy@jkoshy-ld:~]$ sudo easy_install -U setuptools
Traceback (most recent call last):
  File ""/usr/bin/easy_install"", line 9, in <module>
    load_entry_point('distribute', 'console_scripts', 'easy_install')()
  File ""/usr/lib/python2.6/site-packages/setuptools-1.1.5-py2.6.egg/pkg_resources.py"", line 357, in load_entry_point
    return get_distribution(dist).load_entry_point(group, name)
  File ""/usr/lib/python2.6/site-packages/setuptools-1.1.5-py2.6.egg/pkg_resources.py"", line 2393, in load_entry_point
    raise ImportError(""Entry point %r not found"" % ((group,name),))
ImportError: Entry point ('console_scripts', 'easy_install') not found

","14/Sep/13 00:22;jjkoshy;Probably because I did both yum install python-setuptools and easy_install -U setuptools","14/Sep/13 00:26;nehanarkhede;Ya, I think the post review page is a bit confusing. I updated the wiki with the precise installation instructions for review board - https://cwiki.apache.org/confluence/display/KAFKA/Kafka+patch+review+tool#Kafkapatchreviewtool-1.Installthepostreviewtool

","15/Sep/13 16:40;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/14091/
","15/Sep/13 16:43;nehanarkhede;Thanks for the review comments, [~swapnilghike]. Were you able to setup the tool correctly and use it to upload a patch and rb?","15/Sep/13 16:49;nehanarkhede;I think this tool will be easier to use if it is checked in. That will also simplify the wiki. Can I get a +1 from one of the committers?","16/Sep/13 00:37;swapnilghike;Hmm, tried setting up the tool according to the instruction for RHEL. Ran into this:
~/kafka/kafka$ python kafka-patch-review.py --help
Traceback (most recent call last):
  File ""kafka-patch-review.py"", line 3, in <module>
    import argparse
ImportError: No module named argparse

Does the easy_install think work only on Mac? (jira-python and RBTools are installed using easy_install)?

On the Mac, I got this:
~/kafka-local/kafka$ echo $JIRA_CMDLINE_HOME 
.
~/kafka-local/kafka$ python kafka-patch-review.py -b 0.8 -j KAFKA-1003 -db
Jira Home= .
git diff 0.8 > KAFKA-1003.patch
Creating diff against 0.8 and uploading patch to JIRA KAFKA-1003
Creating a new reviewboard
post-review --publish --tracking-branch 0.8 --target-groups=kafka --bugs-closed=KAFKA-1003 --summary ""Patch for KAFKA-1003""
There don't seem to be any diffs!

rb url= 

If you take a look at KAFKA-1003, it has appended my diffs, it just did not crete a review board. I guess this is expected.
","16/Sep/13 01:04;nehanarkhede;Thanks for giving it a spin, though I'm not sure you are using the latest version of the tool. 

1. For the argparse error, I'm not too sure if it is because argparse requires Python 2.7.x ? Could you try upgrading Python to 2.7.x and see if it works?
2. To upload a patch using the tool, the code must be committed to the local branch else the diff is empty. But I think that points to a potential improvement to the tool. If the diff is empty, it can skip uploading the patch and creating the rb.","16/Sep/13 01:08;swapnilghike;1. RHEL machine is on python 2.7.2. Maybe the libraries are not in the standard place or something.
2. Made a local commit, tried again, same error. I am using the latest diff (the one with the timestamps). Is JIRA_CMDLINE_HOME pointing to a wrong directory? I set it to .","16/Sep/13 03:28;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/14091/
","16/Sep/13 16:11;nehanarkhede;Updated the Setup section of the wiki with instructions on installing argparse - https://cwiki.apache.org/confluence/display/KAFKA/Kafka+patch+review+tool#Kafkapatchreviewtool-1.Setup
Also, JIRA_CMDLINE_HOME is defunct. 
I improved the error reporting of the tool to handle empty diffs. Could you give the latest version a try?","16/Sep/13 17:05;swapnilghike;After installing arg_parse and using origin/0.8 instead of 0.8 as the branch name, it worked like a charm!","16/Sep/13 17:15;nehanarkhede;Cool! Swapnil, could you help me add a FAQ to this wiki. It will be great if you could list the issues you ran into with the error messages.","16/Sep/13 21:02;swapnilghike;Saw new issue on RHEL when I tried 'python kafka-patch-review.py -b origin/trunk -j KAFKA-42 -r 14081':

Enter authorization information for ""Web API"" at reviews.apache.org

Your review request still exists, but the diff is not attached.

Creating diff against origin/trunk and uploading patch to JIRA KAFKA-42
Created a new reviewboard  


It attached the diff, but did not create a RB.","16/Sep/13 21:40;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/14091/
","18/Sep/13 01:09;jjkoshy;Nice - I tried this on KAFKA-1049 (as a test - that patch does not work) and it worked great!

+1

I did not get time to dig into the issue I ran into on Linux but the steps worked on my laptop. I can look into that and update the wiki with a work-around if I find one.

Minor comment: the direct Python API is interesting http://www.reviewboard.org/docs/rbtools/dev/api/overview (I'm in general wary of popen/subprocess); but it is probably more work than its worth to interface with that and post-review likely wraps that anyway and is a well-maintained tool. Also, would prefer to have the tool create a os.tmpfile as opposed to leaving around a patch file but not a big deal.
","18/Sep/13 03:55;nehanarkhede;Thanks for the reviews. 

Joel -

Moved the patch to tempdir. Moving to the python API for reviewboard would be great, filed KAFKA-1058 to address that.

Checked in the tool with the tempdir fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add optional partition key override in producer,KAFKA-925,12649948,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,29/May/13 15:49,12/Sep/13 16:31,12/Jan/21 10:06,29/Jul/13 22:38,0.8.1,,,,,,,0.8.1,,,producer ,,,,,,0,,,,,"We have a key that is used for partitioning in the producer and stored with the message. Actually these uses, though often the same, could be different. The two meanings are effectively:
1. Assignment to a partition
2. Deduplication within a partition

In cases where we want to allow the client to take advantage of both of these and they aren't the same it would be nice to allow them to be specified separately.

To implement this I added an optional partition key to KeyedMessage. When specified this key is used for partitioning rather than the message key. This key is of type Any and the parametric typing is removed from the partitioner to allow it to work with either key.

An alternative would be to allow the partition id to specified in the KeyedMessage. This would be slightly more convenient in the case where there is no partition key but instead you know a priori the partition number--this case must be handled by giving the partition id as the partition key and using an identity partitioner which is slightly more roundabout. However this is inconsistent with the normal partitioning which requires a key in the case where the partition is determined by a key--in that case you would be manually calling your partitioner in user code. It seems best to me to either use a key or always a partition and since we currently take a key I stuck with that.",,criccomini,guozhang,jjkoshy,jkreps,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/13 19:01;jkreps;KAFKA-925-post-commit-v1.patch;https://issues.apache.org/jira/secure/attachment/12599841/KAFKA-925-post-commit-v1.patch","29/May/13 15:52;jkreps;KAFKA-925-v1.patch;https://issues.apache.org/jira/secure/attachment/12585228/KAFKA-925-v1.patch","17/Jul/13 21:16;jkreps;KAFKA-925-v2.patch;https://issues.apache.org/jira/secure/attachment/12592856/KAFKA-925-v2.patch",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2013-07-17 21:49:41.109,,,false,,,,,,,,,,,,,,,,,,330275,,,Thu Sep 12 16:31:55 UTC 2013,,,,,,,"0|i1kzan:",330609,,,,,,,,,,,,,,,,"17/Jul/13 21:16;jkreps;Updated patch--rebased to trunk.","17/Jul/13 21:49;criccomini;Hey Jay,

Seems pretty reasonable to me. Is the reason for the type change in the Partitioner so that you can handle either keys of type K (key) or keys of any type (part key) using the same partitioner?

Cheers,
Chris","17/Jul/13 22:01;guozhang;Hi Jay,

In the DefaultEventHandler, only the key is serialized and sent. The partition key is used to determine the partition and then dropped. So the consumers would not be able to read this partition key. Will this be a problem for, for example MirrorMaker?

Guozhang","17/Jul/13 22:12;jkreps;Yes the idea of this feature is to make it possible to partition by something other than the stored key.","17/Jul/13 22:13;jkreps;It is definitely true that downstream consumers cannot use the same key, though a generic tool can always just retain the partition by setting the partition number as the partition key and using a partitioner which just uses that number.","23/Jul/13 16:55;jjkoshy;+1 , looks good to me.

DefaultPartitioner: Do we need the type parameter anymore?

Guozhang has a good point about tools such as mirror maker not having access to the original partitioning key.
However, I can see that it would be clunky as we would then need a partition key serializer as well. Also,
for something like offset-preserving mirrors we would anyway have the source cluster's partition available,
so I don't see it as a major issue.

ConsoleProducer: the enqueue timeout change seems reasonable - I'm assuming it was done to avoid dropping
messages when piping into ConsoleProducer. Correct?
","29/Jul/13 22:38;jkreps;Committed.","29/Jul/13 22:38;jkreps;Good point, type parameter is not needed in DefaultPartitioner.

Yes, mirror maker should always partition with the partition number rather than trying to reverse engineer the client partitioning logic. This additional key is specifically for the case where you want to partition by something OTHER than the stored key.

For console producer, yes, we should default to not losing data. Not really related to this issue, but a good change.","25/Aug/13 16:24;junrao;In ConsoleProducer, shouldn't we default ""queue-enqueuetimeout-ms"" to -1, instead of Int.MaxValue, if we want to make the producer block when queue is full?","25/Aug/13 19:01;jkreps;Yeah fair point. I didn't realize we had a special infinity value so I used Int.MaxValue to be ""a long time"". Attached is a patch that fixes that and cleans up some of the cli docs.","12/Sep/13 16:31;junrao;Thanks for the post commit patch. Looks good. +1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement log compaction,KAFKA-631,12617709,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,26/Nov/12 22:30,03/Jul/13 04:04,12/Jan/21 10:06,29/Jan/13 16:11,0.8.1,,,,,,,,,,core,,,,,,0,,,,,"Currently Kafka has only one way to bound the space of the log, namely by deleting old segments. The policy that controls which segments are deleted can be configured based either on the number of bytes to retain or the age of the messages. This makes sense for event or log data which has no notion of primary key. However lots of data has a primary key and consists of updates by primary key. For this data it would be nice to be able to ensure that the log contained at least the last version of every key.

As an example, say that the Kafka topic contains a sequence of User Account messages, each capturing the current state of a given user account. Rather than simply discarding old segments, since the set of user accounts is finite, it might make more sense to delete individual records that have been made obsolete by a more recent update for the same key. This would ensure that the topic contained at least the current state of each record.",,brugidou,criccomini,jkreps,junrao,nehanarkhede,prashanth.menon,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/13 20:05;jkreps;KAFKA-631-v1.patch;https://issues.apache.org/jira/secure/attachment/12564992/KAFKA-631-v1.patch","21/Jan/13 00:26;jkreps;KAFKA-631-v2.patch;https://issues.apache.org/jira/secure/attachment/12565719/KAFKA-631-v2.patch","21/Jan/13 18:48;jkreps;KAFKA-631-v3.patch;https://issues.apache.org/jira/secure/attachment/12565822/KAFKA-631-v3.patch","21/Jan/13 23:14;jkreps;KAFKA-631-v4.patch;https://issues.apache.org/jira/secure/attachment/12565871/KAFKA-631-v4.patch","24/Jan/13 01:25;jkreps;KAFKA-631-v5.patch;https://issues.apache.org/jira/secure/attachment/12566230/KAFKA-631-v5.patch","26/Jan/13 04:58;jkreps;KAFKA-631-v6.patch;https://issues.apache.org/jira/secure/attachment/12566610/KAFKA-631-v6.patch","27/Jan/13 04:41;jkreps;KAFKA-631-v7.patch;https://issues.apache.org/jira/secure/attachment/12566656/KAFKA-631-v7.patch","28/Jan/13 21:42;jkreps;KAFKA-631-v8.patch;https://issues.apache.org/jira/secure/attachment/12566827/KAFKA-631-v8.patch","28/Jan/13 22:58;jkreps;KAFKA-631-v9.patch;https://issues.apache.org/jira/secure/attachment/12566844/KAFKA-631-v9.patch",,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,2012-12-04 21:13:54.44,,,false,,,,,,,,,,,,,,,,,,292224,,,Tue Jan 29 16:10:52 UTC 2013,,,,,,,"0|i0rsnz:",160288,,,,,,,,,,,,,,,,"26/Nov/12 22:32;jkreps;Here is a specific proposal:

We will retain the existing settings that retain segments based on bytes and time, with data prior to these limits left unmolested. We will introduce a new setting for each topic ""cleanup.policy""={delete, dedupe}. cleanup.policy=delete will correspond to the current behavior. cleanup.policy=dedupe will correspond to the new behavior described in this JIRA. As now, data that falls inside the retention window will not be touched, but data that is outside that window will be deduplicated rather than deleted. It is intended that this be a per-topic setting specified at topic creation time. As a short-cut for the purpose of this ticket I will just add a configuration map setting the policy in the way we have for other topic-level settings, these can all be refactored into something set in the create/alter topic command as a follow-up item.

Topics getting dedupe will be processed by a pool of background ""cleaner"" threads. These threads will periodically recopy old segment files removing obsolete messages and swapping in these new deduplicated files in place of the old segments. These sparse files should already be well-supported by the logical and sparse offset work in 0.8.

Here are the specific changes intended:
- Add a few new configs: 
   - topic.cleanup.policy={delete,dedupe} // A map of cleanup policies, defaults to delete
   - cleaner.thread.pool.size=# // The number of background threads to use for cleaning 
   - cleaner.buffer.size.bytes=# // The maximum amount of heap memory per cleaner thread that can be used for log deduplication
   - cleaner.max.{read,write}.throughput=# // The maximum bytes per second the cleaner can read or write
- Add a new method Log.replaceSegments() that replaces one or more old segments with a new segment while holding the log lock
- Implement a background cleaner thread that does the recopying. This thread will be owned and maintained by LogManager
- Add a new file per data directory called cleaner-metadata that maintains the cleaned section of the logs in that directory that have dedupe enabled. This allows the cleaner to restart cleaning from the same point upon restart.

The cleaning algorithm for a single log will work as follows:
1. Scan the head of the log (i.e. all messages since the last cleaning) and create a Map of key => offset for messages in the head of the log. If the cleaner buffer is too small to scan the full head of the log then just scan whatever fits going from oldest to newest.
2. Sequentially clean segments from oldest to newest.
3. To clean a segment, first create a new empty copy of the segment file with a temp name. Check each message in the original segment. If it is contained in the map with a higher offset, ignore it; otherwise recopy it to the new temp segment. When the segment is complete swap in the new file and delete the old. 

The threads will iterate over the logs and clean them periodically (not sure the right frequency yet). 

Some Nuances:
1. The above tends to lead to smaller and smaller segment files in the tail of the log as records are overwritten. To avoid this we will combine files; that is, we will always collect the largest set of files that together are smaller than the max segment size into a single segment. Obviously this will be based on the starting sizes, so the resulting segment will likely still be smaller than the resulting segment.
2. The recopying procedure depends on the property that logs are immutable. However our logs are only mostly immutable. It is possible to truncate a log to any segment. It is important that the cleaner respect this and not have a race condition with potential truncate operations. But likewise we can't lock for the duration of the cleaning as it may be quite slow. To work around this I will add a generation counter to the log. Each truncate operation will increment this counter. The cleaner will record the generation before it begins cleaning and the swap operation that swaps in the new, cleaned segment will only occur if the generations match (i.e. if no truncates happened in that segment during cleaning). This will potentially result in some wasted cleaner work when truncatations collide with cleanings, but since truncates are rare and truncates deep enough into the log to interact with cleaning very rare this should almost never happen.","04/Dec/12 21:13;prashanth.menon;Hey Jay, is this something you wanted to take on?","06/Dec/12 05:09;jkreps;Yes, I am actually working on it now (forgot to assign to myself). If you are looking for a cool project I actually have a number of ideas...","06/Dec/12 14:49;prashanth.menon;Sure, pass the ideas along onto the distribution list and I'll see what chunks I can bite off :-)","06/Dec/12 18:09;junrao;Prashanth,

Will you be interested in KAFKA-330 (delete topic)? I added some comments on how to approach this.","15/Jan/13 20:05;jkreps;This patch implements more or less what was described above.

Specific Changes:
- OffsetCheckpoint.scala: Generalize HighwaterMarkCheckpoint to OffsetCheckpoint for use in tracking the cleaner point. In the future we would use this for flush point too, if possible.
- Move configuration parameters in Log to a single class, LogConfig, to prepare for dynamically changing log configuration (also a nice cleanup)
- Implement a cleaner process in LogCleaner.scala that cleans logs, this is mostly standalone code. It is complicated but doesn't really touch anything else.
- Implement an efficient OffsetMap (and associated tests) for log deduplication
- Add an API in Log.scala that allows swapping in segments. This api is fairly specific to the cleaner for now and is not a public api.
- Refactor segment delete in Log.scala to allow reuse of the async delete functionality in segment swap
- Add logic in log recovery (Log.scala) to handle the case of a crash in the middle of cleaning or file swaps.
- Add a set of unit tests on cleaner logic (CleanerTest.scala), an integration test (LogCleanerIntegrationTest.scala) for the cleaner, and a torture test to run against a standalone server (TestLogCleaning.scala). The torture test produces a bunch of messages to a server over a long period of time and simultaneously logs them out to a text file. Then it uses unix sort to deduce this text file and compares the result to the result of consuming from the topic (if the unique key-set isn't the same for both it throws an error). It also measures the log size reduction.

New configuration parameters:

# should we default to delete or deduce for the cleanup policy?
log.cleanup.policy = delete/dedupe

# per-topic override for cleanup policy
topic.log.cleanup.policy = topic:delete/dedupe, … 

# number of background threads to use for log cleaning
log.cleaner.threads=1

# maximum I/O the cleaner is allowed to do (read & write combined)
log.cleaner.io.max.bytes.per.second=Double.MaxValue

# the maximum memory the cleaner can use
log.cleaner.buffer.size=100MB

# the amount of time to sleep when there is no cleaning to do
log.cleaner.backoff.ms=30secs

# minimum ratio of new to old messages the log must have for cleaning to proceed
log.cleaner.min.cleanable.ratio=0.5

I also changed the configuration log.cleanup.interval.mins to log.retention.check.interval.ms because the word ""cleanup"" is confusing.

New Persistent Data

This patch adds a new persistent data structure, a per-data directory file 'cleaner-offset-checkpoint'. This is the exact same format and code as the existing 'replication-offset-checkpoint'. The contents of the file is the position in the log up to which the cleaner has cleaned.

Current State

This patch is mostly functional with a number of known limitations:
1. It is a lot of code, so there are likely bugs. I think most bugs would only effect log cleaning.
2. The cleaner is somewhat inefficient. Current it does about 11MB/sec. I suspect this can be increased to around 70-100MB/sec by implementing batching of writes. I will do this as a follow-up ticket.
3. I do not properly handle compressed logs. Cleaning will work correctly but all messages are written uncompressed. The reason for this is that logically it is pretty complex to figure out what codec messages should be written with (since there may be a mixture of compression types in the log). Rather then try to handle this now, I think it makes more sense to implement dynamic config and then add a new config for log compression so that each topic has a single compression type that all messages are written with.
4. It would be nice to seed the hash with a different seed for each run so that collisions would get handled in the next run. This will also be done in a follow-up patch.
5. It would be nice to integrate the torture test into the nightly integration test framework (since it is a pass/fail test). I will work to do this as a separate item.

I would like to get this in in the current state and work on making log config dynamic. Without that feature this is not very useful since you have to bounce the server every time you add a new topic to set the cleanup policy. Once that is done we can use it for real features which will likely uncover more issues then further testing now.

Status of Testing

- There is reasonable unit test coverage but I will likely add additional tests as real usage uncovers corner cases
- I can run the torture test for many hours on a few dozen gb of data and get correct results.
","16/Jan/13 16:01;junrao;Thanks for the patch. Do you know the revision of trunk on which this patch will apply? I can take a look before you rebase.","16/Jan/13 21:18;jkreps;362eba981de40a69ae509a291649531ead6f6aee","21/Jan/13 00:26;jkreps;New patch, only minor changes:
1. Rebased against trunk at 9ee795ac563c3ce4c4f03e022c7f951e065ad1ed
2. Implemented seeding for the offset map hash so that now a different hash is used on each iteration so collisions between cleaning iterations should be independent.
3. Implemented batching in the cleaner's writes. This improves the per-thread performance from about 11MB/sec to about 64MB/sec on my laptop.
4. Add a special log4j log for cleaner messages since they are kind of verbose.","21/Jan/13 18:34;jkreps;I did some testing on the I/O throttling and verified that this does indeed maintain the expected I/O rate. Two gotchas in this, first you can't look at iostat because the OS will batch up writes and then asynchronously flush them out at a rate greater than what we requested. Second since the limit is on read and write combined a limit of 5MB/sec will lead to the offset map building happening at exactly 5MB/sec but the cleaning will be closer to 2.5MB/sec because cleaning involves first reading in messages then writing them back out so 1MB of cleaning does 2MB of I/O (assuming 100% retention).","21/Jan/13 18:48;jkreps;Attached patch v3. Two small changes:
1. Make memory usage more intuitive now that there is a read and write buffer for each cleaner thread. These are both fixed at 1MB per thread and taken out of the total buffer size given to cleaning.
2. Ensure that each new log segment is flushed before it is swapped into the log. Without this we can swap in a segment that is not on disk at all, delete the old segment, and then lose both in the event of a crash.","21/Jan/13 23:14;jkreps;Did some testing with multithreading, resulting in...

Patch v4:
1. Bug: Log selection wasn't eliminating logs already being cleaned which could lead to two simultaneous cleaner threads both cleaning the same log.
2. Improve logging to always include the thread number.","23/Jan/13 18:20;nehanarkhede;Reviewed patch v4 -

1. CleanerConfig
1.1 Typos - enableClenaer, dedupeBufferLoadFactor (probably dedupBufferLoadFactor is better?)

2. VerifiableProperties
""If the given key is not present"" -> ""If the given property is not present""

3. KafkaConfig
3.1 The comment for explaining log.cleaner.min.cleanable.ratio is confusing 
""/* the minimum ratio of bytes of log eligible for cleaning to bytes to total bytes which a log must
     contain to be eligible for cleaning */""
3.2 The config ""log.retention.check.interval.ms"" says the retention check is in milliseconds, but the name of the config is logCleanupIntervalMinutes and we multiple this value by 60K before passing it into LogManager
3.3 Can we document the different values for log.cleanup.policy in the comment ?

4. OffsetMap
4.1 Remove unused import ""import java.util.concurrent._""
4.2 entries should be updated in put() API

5. Log
5.1 Rolling new log segment in %s (log = %d/%d, index = %d/%d, age = %d/%d)
This log statement got a little confusing but sofisticated. The last part of the statement should be index and last but one should be age

6. LogCleaner
6.1 In the cleanSegments() API, we pass in SystemTime to the LogSegment. However, we already pass in a Time instance to LogCleaner. In order to test it independently, we can pass in MockTime to LogCleaner but we should pass in the same instance to LogSegment for it to work correctly.
6.2 In the cleanInto() API, we log a custom message in the IllegalArgumentException. I'm not sure I quite understood that. Aren't the log segments to be cleaned a mix of previously cleaned segments and yet to be cleaned ones ? Why not just use ""require"" like we did while building the offsetmap ?
6.3 If the server crashes in replaceSegments() after addSegment() and before asyncDeleteSegment() and let's say 2 log segments (xxxx.log,yyyy.log) were replaced with one new log segment(xxxx.log). Now, when this server restarts, the loadSegments() API will swap in the new xxxx.log.swap as xxxx.log, but it will leave yyyy.log. 
6.4 Do we have a unit test to cover the grouping logic in groupSegmentsBySize() API ? It looks correct to me, but I've been bitten by several scala collection append nuances before.
6.5 Remove unused import ""import java.util.concurrent.locks.ReentrantLock""
6.6 allCleanerCheckpoints() is only called from within LogCleaner. Can we make this private ?

7. CleanerConfig
7.1 Typo in API doc ""enableClenaer"" and ""clenaer""
7.2 Why 3MB for the minimum buffer space per thread ? Can we keep this configurable as well ?

8. LogManager
8.1 Can we rename configs to topicConfigs or topicOverrides ?

9. LogSegment
9.1 Fix log4j statement for the .log renaming - ""Failed to change the index file suffix""
 
10. ReplicaManager
In checkpointHighWatermarks(), it is better to use fatal(""Error writing to highwatermark file: "", e)

11. MockScheduler
Even though this is not introduced in this patch, while reading the code, realized that the MockScheduler actually executes tasks before their nextExecution time is reached. This is because we just check if the nextExecutionTime <= now and then call task.fun() without waiting until nextExecution time.
","24/Jan/13 01:25;jkreps;Great review. Attached patch v5 that addresses most of these issues:

1.1 Fixed ""enableClenaer"", dedupe is actually a word and is spelled dedupe, though, I think…
2. Changed
3.1 This is hard to explain, but changed it to ""the minimum ratio of dirty log to total log for a log to eligible for cleaning""
3.2 Changed to ms.
3.3 Done
4.1. Done
4.2 Ah, nice catch. Fixed. Added test for it.
5.1 ""Confusing but sophisticated"" is my middle name. Basically I didn't like the code duplication and it seemed nice to see all the criteria whenever we roll. Fixed the ordering.
6.1 Fixed
6.2 I think you are saying we could change this to a require() call, right? Made that change.
6.3 Argh, you're right. I didn't think of that problem. It isn't easily fixable. Let's continue the review and I will think of a fix for this as a follow-up item. It isn't a critical problem because effectively you just duplicate a segment of the log needlessly (but with very low probability). The old segment will mask that portion of the new segment, but I don't think there is any other bad thing that happens.
6.4 CleanerTest.testSegmentGrouping() is a beast
6.5 Done
6.6 It can but it seems reasonable to ask for the last known cleaner point?
7.1 Fixed
7.2 3MB should be enough for anyone. :-) No the real reason is because I require you to have a 1MB read buffer, a 1MB write buffer which I cleverly subtract from the cleaner total buffer size. I don't think we need to make these configurable since 1MB is a good size (bigger won't help, and smaller will hurt). So you must have at least 2MB, but if you are trying to set a dedupe buffer that is less than 1MB well that is crazy. Maybe this is just me being a little too accurate about memory accounting and a better approach would just be to not count the I/O buffers at all. In that case the question is what is the minimum we should set for the cleaner buffer?
8.1 We can't do topicOverrides since these are full log configs not overrides. I suppose topicConfigs is better in case there was a question of what the String in the map was. Changed that.
9.1 Fixed.
10.1 Fixed
11. Not sure I follow. If you update the time manually and then call tick() we basically do ""catch up"" executing the tasks in order of next execution and cycling their period until we are caught up. The key point is that the user is the one who advances the time not the scheduler. That is the user says ""it is now 12:15"" and we execute our backlog of tasks. Perhaps you are saying that it should work the other way where the scheduler advances the clock rather than vice versa?

","24/Jan/13 19:07;sriramsub;Good stuff.

My feedbacks below - 

1. Throttling

   The current scheme of throttling will work only if there is one physical disk that kafka uses which I guess is not going to be the case. For multiple disks, the single throttler is not going to prevent some disks from getting saturated. A more accurate but complex solution is to do the following - 
  - Query the number of physical disks on the machine on startup. Divide the total bytes / sec allowed for the cleaner by this number (This is the value per disk).
  - Create a throttler per disk. 
  - Have a background thread that refreshes the log directory mapping to the physical disk (this is in cases when the log directory gets moved to a different disk)
  - Use the appropriate throttler based on the mapping above
This would be one of the ways you can control any single disk from getting saturated.

2. Grouping segments by size

   The way the current grouping of segments is done based on size I think will not solve the problem of preventing very small segments. We decide the grouping even before deduplicating. I would assume somebody would choose dedup based GC only if they have lots of updates instead of new records. In such a scenario, all the old segments will eventually tend to 0 after dedup. If you were to calculate the total number of segments based on a max size before dedup, you could end up having very few records in that new segment. A more deterministic way to ensure each segment has a min size is to check the size as you append to the segment. If it has crossed the maxsize and is at the end of a segment boundary, do the swap with the segments read.

3. Determination of end offset
 
The code currently decides the end offset based on the map capacity. Consider the example you had quoted in the wiki about user updates. Very few active users would generate new events regularly and the majority would have unique records in a given time period. If many of the records within the dirty set gets duplicated you would not be making optimum use of your memory (map would end up being partially filled). I dont have a good answer for this but something to note.One option would be to keep reading the dirty set till you hit the map capacity or the beginning of the active segment.

4. Script to find actual deduplication

As part of your tests do you plan to measure the expected dedup level Vs the actual gain? As long as the gain is close to the expected value it is fine but we do not want it to be way off.

5. Integration tests

Should the integration tests use more than one cleaner thread to catch any corner cases? I could have missed it but I did not find any test that does a sanity check of multiple cleaner threads functioning correctly.

6. Salt generation

Should the salt be randomly generated instead of being monotonically increasing. Empirically I have found it to perform better in terms of providing more uniform distribution given a key namespace.","24/Jan/13 21:54;jkreps;Sriram--These are great suggestions. For the most part I am taking these to be ""future work"" because I don't think they block a ""minimally viable product"" which is what I hope to get in first. My intuition is to avoid doing anything hard or complicated until we have real operational experience with this functionality because otherwise we end up building a ton of fancy stuff that solves the wrong problems. Patches would be gladly accepted, though. :-)

1. This is a good suggestion. There is an additional assumption which is combining read and write I/O. Read I/O may be coming out of pagecache (shared) or from disks (not shared). Likewise it isn't really the number of disks per se since a RAID setup would effectively pool the I/O of all the disks (making the global throttler correct). We support multiple data directories with the recommendation that each data directory be a disk. We also know the mapping of log->data_directory. If we relied on this assumption we could do the throttling per data directory without too much difficulty. Of course that creates another additional scheduling problem which is that we should ideally choose a cleaning schedule that balances load over data directories. In any case, I think the global throttle, while not as precise as it could be, is pretty good. So I am going to add this to the ""future work"" page.

2. Yes. In fact the current code can generate segments with size 0. This is okay though. There is nothing too bad about having a few small files. We just can't accumulate an unbounded number of small files that never disappear (some combining must occur). Small files will get cleaned up in the next run. So I knowingly chose this heuristic rather than doing dynamic grouping because it made the code easier and simpler to test (i.e. I can test grouping separate from cleaning).

3. Since you have to size your heap statically in the case of a single thread shrinking the map size doesn't help anyone. Having a very sparse map just makes duplicates unlikely. However in the case where you had two threads it would be possible to schedule cleanings in such a way that you allocated small buffers for small logs and big buffers for big logs instead of medium buffers for both. Since these threads progress independently, though, it would be a bit complicated. Probably the small log would finish soon, so you would have to keep finding more small logs for the duration of the cleaning of the large log. And when the large cleaning did happen, you would probably have a small cleaning in progress so you would have to start another cleaning with the same large buffer size if you wanted memory to remain fixed. However one thing this brings up is that if your logs are non-uniform having non-uniform buffers (even if they are statically sized) could make it so you were able to efficiently clean large logs with less memory provided your scheduling was sophisticated enough. There are a number of gotchas here though.

4. I created a cleaner log and after each cleaning I log the full cleaner stats (time, mb/sec, size reduction, etc).

5. There are three tests in the patch. A simple non-threaded method-by-method unit test. A junit integration test of the full cleaner running as a background thread with concurrent appends. Finally a stand-alone torture test that runs against an arbitrary broker by producing to N topics and recording all its produced messages, then consuming from the broker to a file, then sorting and deduplicating both files by brute force and comparing them exactly. This later test is very comprehensive and runs over many hours and can test any broker configuration. I ran it with multiple threads to validate that case (and found some bugs, that i fixed). I think a third thing that could be done (but which I haven't done) is to build a stand-alone log duplication checker that consumes a topic/partition and estimates the duplication of keys using a bloom filter or something like that. I haven't done the later.

5. Intuitively this should not be true. By definition ""independent"" means that sequential salt should perform as well as well as any other salt or else that would be an attack on md5, no?","26/Jan/13 04:58;jkreps;Updated patch v6:

- Fixed a bug: messages larger than the I/O buffer would lead to an infinite loop. Now we raise the I/O buffer size when needed.
- Made I/O buffer configurable instead of hardcoding to 1MB. I also no longer subtract this from overall buffer size. The two configurations are now:
log.cleaner.dedupe.buffer.size
log.cleaner.io.buffer.size
Both give the size over all threads so the per-thread size is divided by log.cleaner.threads.

Neha, I think this addresses all your concerns.","27/Jan/13 04:41;jkreps;Patch v7, rebased.
Minor changes:
- Rebased
- Removed some printlns that slipped in in the last patch
- Improved logging for buffer size growing
- Changed Logging.msgIdent to only do string concatenation if there is a message set. Not really related to this patch but seems like a good thing to do.","28/Jan/13 06:04;junrao;Thanks for patch v7. Looks good overall. Some comments:

70. LogCleaner:
70.1 buildOffsetMap(): need to consider grow readBuffer to accomodate for maxMessageSize.
70.2 celanInto(): Can the payload ever be null?
        val retainRecord = lastOffset < 0 || (entry.offset >= lastOffset && entry.message.payload != null)
70.3 CleanerThread.run(): Should we catch all Throwables, instead of Exceptions?

71. Log:
71.1 loadSegments(): The following comment is no longer true since it can happen to a segment with SwapFileSuffix.
        if(!hasIndex) {
          // this can only happen if someone manually deletes the index file
71.2 maybeRoll(): move .format in debug to a separate line.
71.3 truncateFullyAndStartAt(): This one behaves in the same way as truncateTo and is called directly from ReplicaFetcherThread. So need to increment truncates here too.

72. KafkaConfig: Why do we have log.cleaner.enable? Shouldn't log cleaner be automatically enabled if logCleanupPolicy is dedup?

","28/Jan/13 16:32;junrao;A few more comments:

73. LogOffsetTest.createBrokerConfig(): We should set log.retention.check.interval.ms to 5 mins, instead of 5ms.

74. CleanerConfig, LogConfig, TestLogCleaning: missing Apache header.

75. TestLogCleaning: Could you write in the comment how this test works?

76. In SkimpyOffsetMap, we use only the first 4 bytes (out of 16 byts of MD5) to calculate the array position of the hash. Would it be better to use all of the 16 bytes?

","28/Jan/13 21:42;jkreps;Patch v8 includes Jun's comments. Specifically:
Cleaner:
70.1 Nice catch. Buffers now grow in offset map building. Also changed both offset map building and cleaning to keep the same buffer size for the duration of the segment to avoid growing and shrinking too frequently.
70.2 The message payload can be null and this is used to indicate a delete (note that null messages do go into the offset map but never survive a cleaning). Currently though there is no way to set the payload to null and a number of bugs around null payloads. I will be opening a ticket to solve those.
70.3 Usually catching Throwable is a mistake, I think. I.e. if we are out of memory, the thread should die.
Log:
71.1 Removed the comment about rebuilding indexes.
71.2 Improved formatting for log statement in maybeRoll()
71.3 Nice catch. Incrementing truncates count in truncateFullyAndStartAt()
KafkaConfig
72. It would be easily to implement something where the log cleaner starts only if we have a log with dedupe. However it is a little trickier with topics that are dynamically added or for which the config is changed dynamically. I would like to leave it simple/stupid for now and when we have the config change stuff ironed out make the cleaner dynamically start when the first log becomes dedupe-enabled.
LogOffsetTest:
73. Changed log.retention.check.interval.ms to 5*60*1000LogOffsetTest.createBrokerConfig()
74. Added apache header to CleanerConfig, LogConfig, TestLogCleaning.
75. Added a comment for TestLogCleaning.
76. The bytes of a cryptographic hash are supposed to be uniformly distributed, so just using the first 4 bytes should be fine (I have previously tested this and it works well empirically too).","28/Jan/13 22:58;jkreps;Oops, dropped one change I mentioned in the v8 patch. V9 only restores the read and write buffers at the end of the segment to avoid churning on memory allocation (one liner).","29/Jan/13 01:17;nehanarkhede;+1 on v9. Some minor changes before you check it in -

13. KafkaConfig
Typo - accross -> across
14. LogCleaner
Typo: ellapsed -> elapsed
15. We talked about this offline, but regarding review comment 6.3, I personally like the renaming the .swap file to contain the names of the files it has cleaned, but there might be nuances. e.g. there is a OS limit to the length of a file name. Would you mind filing another bug to track that change ?","29/Jan/13 16:10;jkreps;Cool, checked in with those fixes. Filed KAFKA-739, KAFKA-740, KAFKA-741. Jun if you have any follow-up comments I will do those as a second checkin.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a key-based log retention strategy,KAFKA-555,12611015,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,jkreps,jkreps,jkreps,09/Oct/12 16:09,03/Jul/13 04:04,12/Jan/21 10:06,03/Jul/13 04:04,,,,,,,,,,,,,,,,,0,,,,,"Currently we have two log retention strategies: one based on time and one based on log size. These work well for ""event"" type data--i.e. data that consists only of appends. However if the events model changes to an underlying keyed data set, a more convenient retention strategy would delete keys that had been overwritten rather than retaining whole segments.

The proposed implementation would be a background process that scanned log segments and recopied only keys that hadn't been overwritten. Some more details are in this wiki:
https://cwiki.apache.org/confluence/display/KAFKA/Keyed+Messages+Proposal",,Guo Ziang,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-631,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,246187,,,Wed Jul 03 04:04:03 UTC 2013,,,,,,,"0|i07i0f:",41685,,,,,,,,,,,,,,,,"03/Jul/13 04:04;jkreps;Did it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka intra-cluster replication support,KAFKA-50,12514687,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,,19/Jul/11 21:32,13/Jun/13 15:34,12/Jan/21 10:06,13/Jun/13 15:34,,,,,,,,0.8.0,,,,,,,,,3,,,,,"Currently, Kafka doesn't have replication. Each log segment is stored in a single broker. This limits both the availability and the durability of Kafka. If a broker goes down, all log segments stored on that broker become unavailable to consumers. If a broker dies permanently (e.g., disk failure), all unconsumed data on that node is lost forever. Our goal is to replicate every log segment to multiple broker nodes to improve both the availability and the durability. 

We'd like to support the following in Kafka replication: 

1. Configurable synchronous and asynchronous replication 
2. Small unavailable window (e.g., less than 5 seconds) during broker failures 
3. Auto recovery when a failed broker rejoins 
4. Balanced load when a broker fails (i.e., the load on the failed broker is evenly spread among multiple surviving brokers)

Here is a complete design proposal for Kafka replication - https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Replication",,adenysenko,alexfo,bmatheny,bryanduxbury,cagatayk,cm,davelatham,erikvanoosten,felixgv,heavydawson,jdanbrown,junrao,kzadorozhny,lianhuiwang,mjuarez,patricioe,prashanth.menon,rangadi,sharadag,slon,svenkat,thinker0,zayeem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6350400,6350400,,,,,,,,,,,,,,,,,,KAFKA-352,,,,,,,,,"05/Jan/12 17:52;junrao;kafka_replication_detailed_design_v2.pdf;https://issues.apache.org/jira/secure/attachment/12509575/kafka_replication_detailed_design_v2.pdf","20/Jul/11 15:53;junrao;kafka_replication_highlevel_design.pdf;https://issues.apache.org/jira/secure/attachment/12487175/kafka_replication_highlevel_design.pdf",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-10-18 08:09:48.042,,,false,,,,,,,,,,,,,,,,,,67073,,,Mon May 07 17:56:59 UTC 2012,,,,,,,"0|i09m7z:",54033,,,,,,,,,,,,,,,,"18/Oct/11 08:09;sharadag;Sorry for coming late to this.

I read the design document and it looked like it is quite a complex feature to implement and maintain going forward. The sheer amount of complexity in managing the replicas and partition just bothers me. I am wondering why can't we use HDFS which is hardened over a number of years. Obviously I may be missing some subtle things. Would be great if folks shed light on this.","18/Oct/11 15:33;junrao;Sharad,

Good question. HDFS is a great system and is the very first thing that we thought about when looking at Kafka replication. The pros are that (1) we can offload the replication complexity to another system and (2) HDFS can recover various data failure very effectively. Some of the cons are:

1. HDFS only provides data redundancy, but not computational redundancy. If at any given point of time, there is only one broker that can serve the data, the availability is not going to be high. When a broker is down, we need to elect another broker to take over its data. Even though data doesn't have to be physically moved, this process may require a little a bit of recovery of each partition and may take some time to complete. In that window, some partitions become unavailable. Further, data logically moved to the new broker is initially cold.

2. HDFS currently is not a highly available system. The namenode is a SPOF and you need something like Avatar namenode to make it HA. It's not clear when this feature is going to be generally available and used.

3. Using HDFS brings in another complex system, it's not clear how easy it is to operate, especially for an online system.

4. HDFS is not a true POSIX file system. The append/truncate support is relatively new. This may force us to redesign some of the things that currently require in-place update (e.g., during recovery).

5. HDFS manages its data at block level. Kafka replication can manage data at the partition level (a partition can be 3 orders of magnitude bigger than a block). This means we can manage much less meta data and therefore, potentially have a simpler design and implementation.

","18/Oct/11 16:02;tgautier;I must agree completely with Jun here.  The beauty of Kafka lies in it's simplicity.  To add another piece to the puzzle such as HDFS would break this and diminish the value.  I would even argue that if possible Kafka might consider removing Zookeeper as a dependency - or at least make it optional.  

I would also add that it's not clear that HDFS would actually exhibit the write/read performance that Kafka achieves using NIO.  And because there is basically zero copy, Kafka's memory and cpu overhead is incredibly low for what it does.

These two main factors - it's incredible performance with very low overhead using commodity components are Kafka's strengths.  Adding HDFS would eliminate them.

I would suggest that any replication strategy should focus on non-guaranteed delivery.  The Kafka clients already do not provide a guaranteed delivery mechanism, and as such Kafka should only be used in a setting where it's tolerable to have some amount of message loss during a failure event.  Minimizing this loss is a reasonable goal to achieve, but it should not compromise the simplicity and performance of Kafka in any way.","18/Oct/11 16:05;tgautier;To put it another way, in terms of what I want from replication - currently if I have a failure event I will currently lose the history of all of my messages.  I would like Kafka to preserve as much of those messages as possible in a failure event.  It's ok if not every message that appeared to be delivered was delivered.

This is a classic CAP tradeoff - does Kafka provide C or A?  I propose it continue to focus on A.","18/Oct/11 16:22;cburroughs;> I would even argue that if possible Kafka might consider removing Zookeeper as a dependency - or at least make it optional. 

It's already optional (enable.zookeeper=false), but you loose a lot if you disable it.  Taylor, maybe you could elaborate in the mailing list or another ticket what subset of functionality you would be willing to give up to not use ZK?","18/Oct/11 16:33;junrao;We do plan to offer both async replication and sync replication. In async mode, the latency should still remain low since the client doesn't wait for the data to hit all replicas. However, a small amount data may not be replicated to the followers during a failure and will be lost. Sync mode gives you more or less the opposite. This could be useful for people who want to use Kafka as traditional messaging systems like ActiveMQ.

This is another difference with HDFS, which only has a sync replication mode.","19/Oct/11 06:56;sharadag;Thanks Jun for the comments.

> HDFS only provides data redundancy, but not computational redundancy. 

If data resides in HDFS, theoretically it can be served by any broker. The default could be being served from the broker which has hot data. (the data being written to)

> The namenode is a SPOF 
True, but namenode going down doesn't let you loose the data, yes the cluster is not accessible for that period. However if kafka has acks and producer side spooling (which anyway should be there IMO for data durability), no data would be lost.

> The append/truncate support is relatively new.
Its been used by Hbase folks for quite sometime.

> HDFS manages its data at block level. 
Doesn't really matter as users of hdfs care least about blocks. They have a file view of things.


That all said, I don't want to derail this work with any kind of debate here. I was just thinking to get production quality replication quickly. Look forward to having the replication in. Thanks!



","19/Oct/11 16:43;jkreps;Hey Sharad, your comments are all correct. I think using HDFS would certainly require the least implementation effort and contains a mature replication system tested at large scale. The downside is that HDFS is fairly complex in its own right, and has a number of drawbacks for high-availability, low-latency cases (spof is one but not the only one). Also many use cases do not need replication, but supporting hdfs and local fs efficiently probably means two pretty different implementations. We felt that this kind of multi-subscriber log is a really important abstraction in its own right for systems of all kinds and so our thought was to just kind of suck it up and do the full implementation since we thought the end result would be better.","20/Oct/11 04:01;sharadag;make sense. Thanks Jay and Jun.","14/Nov/11 00:33;junrao;The dependencies of the sub-jiras look like the following:

48
49
47 <-- 46,44/45  <-- 43,42,41
  
This means that initially, 47,48,49 can be worked on independently.","05/Jan/12 17:52;junrao;Attach v2 of the detailed design doc. Made 2 minor changes:

1. V1 design has 2 separate ZK paths for a broker, one registered and one alive. Simplify it to have just 1 ZK path for live broker. The implication is that if a topic is created while a broker is down, no partition will be assigned to that broker. Since topic creation is infrequent, this is likely not a big issue.

2. Use broker id as replica id for each partition, instead of using an explicit replica id.
","05/Jan/12 19:05;junrao;Here is a breakdown of all the jiras and their dependencies:

1. kafka-47: create/delete data structures in ZK, automatically create topic, and use them (making partitions logical, supporting only 1 replica, no failure support). ﻿﻿(L1)
  1.1 kafka-237: create/delete ZK path for a topic in an admin tool (L0)
  1.2 kafka-238: add a getTopicMetaData method in broker and expose it to producer
  1.3 kafka-239: Wire existing producer and consumer to use the new ZK data structure
2. kafka-202: decouple request handler and socket sever; enabling long poll in the consumer (L1)
3. kafka-240: implement new producer and consumer request format (L1) 
4. kafka-49: add ack to ProduceRequest (L2). depending on #3
5. kafka-48: long poll in consumer (L2). depending on #2
6. kafka-44: commit thread, replica fetcher thread (L3). depending on #1, #4, #5
7. kafka-45: broker starting up, leader election (L3). depending on #1
8. kafka-46: various ZK listeners (L3). depending on #1
9. kafka-43: move master to preferred replica when possible (L4). optimization
10. kafka-42: rebalance partition with new brokers (L4). extra feature
","01/Mar/12 19:10;nehanarkhede;Can the design be moved to the Kafka wiki instead of a non-editable pdf attached here ? It will make it much easier to discuss missing details in the current design.","01/Mar/12 19:41;junrao;Yes, we can start a new wiki for each of the sub-jiras, if needed.","01/Mar/12 21:59;nehanarkhede;Cool, so I was thinking if the original design was on a wiki too, it will be much easier to point to sections of it, to make discussions easier. If you have a text copy of it, would you mind pasting it in a Kafka JIRA page ? It would be very useful.","02/Mar/12 16:24;junrao;Attach the word doc of the design.","12/Mar/12 22:46;nehanarkhede;Moving the kafka replication design docs to a wiki. This includes both the high-level as well as the low level design details. The following changes are made on the wiki -

1. The state machine will be maintained and changed only by the leader for a partition. The leader co-ordinates each state change by requesting the followers to act on state change requests. This ensures that we don't have a split-brain problem during state changes amongst the replicas for a partition.
2. More details are added for the various algorithms
","07/May/12 17:56;junrao;I have updated the replication V3 design wiki: https://cwiki.apache.org/confluence/display/KAFKA/kafka+Detailed+Replication+Design+V3 by incorporating the content from V2. We plan to do the implementation based on the V3 design. If there are concerns, please add your commend to the design wiki.

The following is a list of open jiras, roughly in order of dependency and importance.

Phase 2 (Basic message replication, with testing and tools and minimum fancy features)
KAFKA-335: Embedded controller (1w)

KAFKA-336: Admin RPC between controller and broker (1.5w)

KAFKA-337: Upgrade ZKClient to allow conditional updates through ZK (0.5w)

KAFKA-342: Broker startup (revisit based on v3.E design, 1.5w)

KAFKA-343: Leader election, become leader, become follower (revisit based on v3. A,C; 2.5w)
   Depends on KAFKA-301

KAFKA-329: Create topic support (revisit based on v3 design, 1w)

KAFKA-46:  Replica fetch, leader commit (v3.G design, 2.5w)
  Depends on KAFKA-301
  Depends on KAFKA-302

KAFKA-338: controller failover (V3. D 2w)

KAFKA-339: Multifetch for follower (1.5w)

KAFKA-306: Fix broker failure test on 0.8 branch (1w)

KAFKA-330: Delete topic support (1w)
  Depends on KAFKA-301

KAFKA-327 Monitoring and tooling support (2w)

Phase 3 (System tests and more advanced features like preferred replica leadership transfer, online partition reassignment)
KAFKA-174 Performance suite for Kafka (2w)
KAFKA-42 Online partition reassignment (3w)
KAFKA-43 Preferred replica leadership transfer (1w)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a 'log.file.age' configuration parameter to force rotation of log files after they've reached a certain age,KAFKA-235,12537334,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,herberts,herberts,05/Jan/12 14:46,13/Jun/13 14:51,12/Jan/21 10:06,06/Dec/12 08:00,0.7,,,,,,,0.7.2,0.8.0,,core,,,,,,0,,,,,"The Kafka client has the ability to start consuming at an offset before or after a given point in time. The granularity of this offset is the log file as the Kafka servers do not keep track of arrival time of various messages.

This means that the granularity of offsets relative to time depends on arrival rate of messages and thus of log file rotation. A topic with lots of messages will have its log files rotated very often (thus each spans a short time interval) whereas a topic with very few messages might not see its log files rotated for hours.

In order to circumvent this granularity disparity, having a parameter that would force log file rotation after a certain delay (xxx ms) would allow for pretty much constant time granularity to be available at the cost of more file descriptor being used.",,jkreps,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-01-05 17:07:04.525,,,false,,,,,,,,,,,,,,,,,,222842,,,Thu Dec 06 07:57:29 UTC 2012,,,,,,,"0|i1486f:",232788,,,,,,,,,,,,,,,,"05/Jan/12 17:07;junrao;This seems like a useful feature, in particular for kafka-236. What's the age granularity that you are looking for? We probably don't want to create too many files for a topic.","06/Dec/12 05:15;jkreps;Didn't we do this?","06/Dec/12 07:57;swapnilghike;Yes, KAFKA-475 added a feature to 0.7 and 0.8 that rolls a new log segment based on the age of the old segment. So in essence, a new log segment is rolled when the old segment exceeds its max permissible size or max permissible age. These maximum size and age limits can be configured in KafkaConfig.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move all per-topic configuration into ZK and add to the CreateTopicCommand,KAFKA-554,12611012,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,09/Oct/12 16:05,12/Mar/13 17:35,12/Jan/21 10:06,08/Mar/13 23:10,,,,,,,,0.8.1,,,,,,,,,0,project,,,,"We have a number of per-topic configurations that control message retention and flush interval. Here is the list of properties I find in KafkaConfig that appear to be per-topic:
  topic.log.file.size
  topic.log.roll.hours
  topic.log.retention.hours
  topic.log.retention.size
  topic.flush.intervals.ms
Currently we specify these in server.properties. This is not a good solution as it requires a rolling bounce of the cluster to make a change, which just doesn't scale to having hundreds of topics. Also the map encoded in a CSV string is kind of hacky.

We should move these into ZK in some kind of JSON blob that allows easily adding new per-topic configs and we should remove these from server.properties.

It would be good to start with a wiki design and get consensus on that first.",,criccomini,demaagd,jkreps,joestein,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/13 22:55;jkreps;KAFKA-554-v1.patch;https://issues.apache.org/jira/secure/attachment/12568654/KAFKA-554-v1.patch","07/Mar/13 21:37;jkreps;KAFKA-554-v2.patch;https://issues.apache.org/jira/secure/attachment/12572616/KAFKA-554-v2.patch","08/Mar/13 00:14;jkreps;KAFKA-554-v3.patch;https://issues.apache.org/jira/secure/attachment/12572653/KAFKA-554-v3.patch","08/Mar/13 22:47;jkreps;KAFKA-554-v4.patch;https://issues.apache.org/jira/secure/attachment/12572850/KAFKA-554-v4.patch",,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2013-03-04 04:41:04.279,,,false,,,,,,,,,,,,,,,,,,246184,,,Tue Mar 12 17:35:23 UTC 2013,,,,,,,"0|i07hzr:",41682,,,,,,,,,,,,,,,,"08/Feb/13 22:55;jkreps;This patch does two things:
1. Implement a dynamic configuration mechanism for topics
2. Remove the scripts bin/kafka-list-topic.sh, bin/kafka-delete-topic.sh, bin/kafka-create-topic.sh and create a new more powerful tool:
jay@ahab:kafka> bin/kafka-topics.sh
Command must include exactly one action: --list, --describe, --create, --delete, or --alter
Option                                  Description                            
------                                  -----------                            
--alter                                 Alter the configuration for the topic. 
--config <name=value>                   A topic configuration for this topic.  
--create                                Create a new topic.                    
--delete                                Delete the topic.                      
--describe                              List details for the given topics.     
--help                                  Print usage information.               
--list                                  List all available topics.             
--partitions <Integer: # of partitions> The number of partitions for the topic.
--replica-assignment                    A list of manual partition-to-broker   
  <broker_id_for_part1_replica1 :         assignments.                         
  broker_id_for_part1_replica2 ,                                               
  broker_id_for_part2_replica1 :                                               
  broker_id_for_part2_replica2 , ...>                                          
--replication-factor <Integer:          The replication factor for each        
  replication factor>                     partition in the topic.              
--topic <topic>                         The topic to be created.               
--zookeeper <urls>                      REQUIRED: The connection string for    
                                          the zookeeper connection in the form 
                                          host:port. Multiple URLS can be      
                                          given to allow fail-over.  

This command line tool can either list topics, describe topics, create topics, delete topics, or change the configuration for topics.

Here is an example of creating two topics with overrides:
./bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic first_topic --topic second_topic --replication-factor 1 --partitions 4 --config segment.bytes=1073741824 --config retention.ms=1000000
Created topic ""first_topic"".
Created topic ""second_topic"".

(Any command that takes a topic option can run on a list of topics by giving more than one topic flag.)

./bin/kafka-topics.sh  --zookeeper localhost:2181 --list
first_topic
second_topic

./bin/kafka-topics.sh  --zookeeper localhost:2181 --describe --topic second_topic
second_topic
	configs: segment.bytes = 1073741824, retention.ms = 1000000
	partitions: 4
		partition 0
		leader: 0 (ahab.linkedin.biz:9092)
		replicas: 0 (ahab.linkedin.biz:9092)
		isr: 0 (ahab.linkedin.biz:9092)
		partition 1
		leader: 0 (ahab.linkedin.biz:9092)
		replicas: 0 (ahab.linkedin.biz:9092)
		isr: 0 (ahab.linkedin.biz:9092)
		partition 2
		leader: 0 (ahab.linkedin.biz:9092)
		replicas: 0 (ahab.linkedin.biz:9092)
		isr: 0 (ahab.linkedin.biz:9092)
		partition 3
		leader: 0 (ahab.linkedin.biz:9092)
		replicas: 0 (ahab.linkedin.biz:9092)
		isr: 0 (ahab.linkedin.biz:9092)

This configuration could be changed later for a topic by running
./bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic first_topic --config segment.bytes=673741824 --config retention.ms=500000
Updated config for topic ""first_topic"".

The implementation of the dynamic config is to add a new zookeeper path
  /config
This path has two subdirectories
  /config/topics/<topic_name>
and
  /config/changes
The per-topic path contains any override properties specified for the topic stored in java.util.Properties format. If no overrides are given then that znode will not exist. The defaults are still taken from the server.properties file.

The /config/changes path is used to reduce the number of watches required. Instead of keeping a watch on each config override znode, whenever we update a config entry we add a sequential entry under the changes directory containing the name of the topic whose config changed. Each broker keeps a watch on this directory and caches the last change it has executed. When the watch fires it executes any new config changes. Old change entries are garbage collected after 10 minutes. The config changes are managed by a new class TopicConfigManager which executes these changes.

This patch also has two refactorings:
1. Renamed KafkaZookeeper to KafkaHealthcheck
2. Moved logic for creating topics out of CreateTopicCommand and replaced it with two utilities in AdminUtils:
       def createTopic(zkClient: ZkClient,
                  topic: String,
                  partitions: Int, 
                  replicationFactor: Int, 
                  topicConfig: Properties = new Properties)
       def createTopicWithAssignment(zkClient: ZkClient, 
                                topic: String, 
                                partitionReplicaAssignment: Map[Int, Seq[Int]], 
                                config: Properties = new Properties)
The first method will choose a partition assignment, and the second just sanity checks the assignment it is given.

I had originally planned to implement an RPC api to create and delete and alter topics, but I backed away from this since we don't seem to have a sane way to organize admin functionality yet.

I think the first step in cleaning up is probably to refactor AdminUtils into a sane Admin client with methods that match the high-level administrative operations. This will still directly interact with zookeeper. This would be a reasonable starting point since one could at least then implement a web console that used this class even if the functionality was not available to other languages. But in any case this is beyond the scope of this patch.","04/Mar/13 04:41;junrao;Thanks for patch v1. Looks good overall. Some comments:

1. TopicCommand: Some options are only available for certain actions. Should we explain that in the description of those options?

2. AdminUtils:
2.1 comment: ""is there is any"" => ""if there is any""
2.2 We have standardized non-singleton values in ZK to JSON. Should the values stored in the topic config path be JSON too?

3. TopicConfigManager:
3.1 add the missing > in the following comment.
 *   /brokers/topics/<topic_name/config
3.2 startup(): It seems there is no need to make sure TopicConfigChangesPath exists here since that's covered in initZk() in KafkaServer.startup().
3.3 processConfigChanges():
3.3.1 How about using ""processing config change notifications"" in the following logging to make it more specific.
      info(""Processing %d change notifications..."".format(notifications.size))
3.3.2 Reading the config from ZK can be done only if changeId > lastExecutedChange
3.3.3 Not sure why we don't delete the sequential node corresponding to lastChangeId from ZK.
3.3.4 It seems that sequential nodes under /brokers/config_changes are only deleted when there is a new config change. So, they are not always deleted after the configured expiration time.
3.4. ConfigChangeListener.handleChildChange(): chillins are obtained from zk.getChildren(). There is no guarantee that the list is sorted. So, you will need to sort it yourself since ordering is important here.

4. KafkaApis.handleOffsetCommitRequest(): The following statements 
    val responseInfo = offsetCommitRequest.requestInfo.map( t => {
      val (topicAndPartition, metadataAndError) = t
can be simplified to 
    val responseInfo = offsetCommitRequest.requestInfo.map( case (topicAndPartition, metadataAndError) => {

5. LogConfig: Can we define the two retentionPolicies ""delete"" and ""dedupe"" as contants and reuse them in LogConfig and KafkaConfig?

6. RequestKeys: ModifyTopicKey is not used.

7. ZkUtils: remove the following unused imports
import java.util.Properties
import java.io.{StringReader, StringWriter}

8. PrimitiveApiTest: Instead of commenting out the following lines, should we just remove them?
    // temporarily set request handler logger to a higher level
    //requestHandlerLogger.setLevel(Level.FATAL)

9. ReplicaFetchTest.logsMatch(): tandp is a bit confusing. Could we rename it to topicAndPart?

The patch needs to be rebased. Some of the changes are no longer necessary after the patch that standardizes the ZK paths/values.
","05/Mar/13 01:04;jkreps;Yeah I thought about json vs properties. The advantage i am getting out of properties is the ability to have non-stored defaults. Properties have the concept of defaults which are used but not written out. I could probably replicate that with some wrapper class but that might make the api a bit weird. The other option would be to use Properties in the api but just serialize it as JSON. That is the option I am leaning towards.","06/Mar/13 20:47;nehanarkhede;I'm a little late to this review and seems like trunk has moved a lot. Will probably just wait for the rebased version.","07/Mar/13 21:37;jkreps;New patch v2. Addresses Jun's comments and rebases.

1. TopicCommand: Made it a bit more clear which options apply to which actions.

2. AdminUtils:
2.1 Fixed typo
2.2 Moved serialization to JSON. The APIs still take Properties instances as that is a higherarchical map. However the data stored in ZK is now JSON. One question is what the format of the change notifications should be. I added a proposal to the kafka zk wiki, please take a look.

3. TopicConfigManager:
3.1 add the missing > in the following comment.
 * /brokers/topics/<topic_name/config
3.2 Yeah but this component should stand alone ideally.
3.3.1 Done
3.3.2 Good call, added that optimization.
3.3.3 I wasn't sure if deleting all znodes would lead to the sequence starting over. I am told it won't so removed this.
3.3.4 ""It seems that sequential nodes under /brokers/config_changes are only deleted when there is a new config change. So, they are not always deleted after the configured expiration time."" Yes. My rational is that I want to clean up just to prevent notifications from piling up forever. But if no new notifications come in then they can't keep piling up. I could also do this by scheduling the deletion but the problem with that is knowing whether you have already scheduled it (you don't want to schedule it over and over). I could also add a background thread dedicated to cleanup, but it doesn't seem worth it for the added complexity.
3.4. Nice call, sorted it
4. Hmm, that doesn't actually compile for me. 
5. Not sure that helps. Config is a contract so we can't change these values in the future so just having the value is more straight-forward (one fewer layers of indirection)
6. Removed ModifyTopicKey
7. ZkUtils: removed imports
8. Yeah, that slipped in. Removed.
9. ReplicaFetchTest.logsMatch(): renamed tandp it to topicAndPart?","08/Mar/13 00:14;jkreps;Erp, that last patch was screwed up. This one should be right.","08/Mar/13 18:16;junrao;For 4, you have to do the following: Instead of using ( for map, you have to use {.
   val responseInfo = offsetCommitRequest.requestInfo.map{ case (topicAndPartition, metadataAndError) => { 
      ...
    }","08/Mar/13 18:42;nehanarkhede;Json.scala
- It seems like the map and sequence case inserts an extra "","" for the last element as well. This will cause invalid json
KafkaServer
- Does it make sense to start the health check at the end after the request handlers and replica manager has started up ?
- In shutdown(), you are closing zkclient before controller, replica manager, log manager which use zkclient. I think we should close it at the very end","08/Mar/13 22:47;jkreps;V4 addresses Neha and Jun's comments.

Jun, nice. Made that change.

Neha, I think the sequence case is okay (.mkString() does the right thing). Added a unit test to cover that and other cases. Wrt the two ordering things, yes you are right, switched.","08/Mar/13 22:59;nehanarkhede;+1 on patch v4","08/Mar/13 23:10;jkreps;Cool, checked in. Jun let me know if you have any further comments and I will follow-up.","12/Mar/13 17:35;junrao;For debugging purpose, is it useful to be able to get the runtime config values used by each broker (e.g., through jmx)?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Efficient polling mechanism for many topics,KAFKA-208,12531721,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,tgautier,tgautier,17/Nov/11 01:05,20/Feb/13 17:27,12/Jan/21 10:06,20/Feb/13 17:27,,,,,,,,,,,,,,,,,0,,,,,"Currently, the way to poll many topics is to submit a request for each one in turn, and read the responses.  Especially if there are few messages delivered on many topics, the network overhead to implement this scheme can far outweigh the bandwidth of the actual messages delivered.

Effectively, for topics A, B, C the request/response scheme is the following:

-> Request A offset a
-> Request B offset b
-> Request C offset c
<- no messages
<- 1 message offset b
<- no messages
-> Request A offset a
-> Request B offset b'
-> Request C offset c
<- no messages
<- no messages
<- no messages
etc.

I propose a more efficient mechanism which works a bit like epoll in that the client can register interest in a particular topic.  There are many options for the implementation, but the one I suggest goes like so:

-> Register interest A offset a
-> Register interest B offset b
-> Register interest C offset c
-> Next message (null)
<- messages for B (1 message)
-> Next message (topic B offset b')
<- no messages
-> Unregister Interest C
...

It is possible to implement the ""Next Message"" request as either blocking or non-blocking.  I suggest that the request format include space for the timeout, which if set to 0 will be a nonblocking response, and if set to anything other than 0, will block for at most timeout ms. ",,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-11-17 03:09:24.864,,,false,,,,,,,,,,,,,,,,,,217457,,,Wed Feb 20 17:27:37 UTC 2013,,,,,,,"0|i029z3:",11217,,,,,,,,,,,,,,,,"17/Nov/11 03:09;jkreps;Hey Taylor how does this relate to KAFKA-48 and KAFKA-170? I think you are proposing something slightly different but I am not sure the exact relationship.","17/Nov/11 05:42;tgautier;Hi Jay.  Good question.

KAFKA-48 in its current description would allow a long poll for one topic at a time.  If a client is interested in only a few topics, it is conceivable that the client could use one TCP connection per topic, and issue a blocking request for each topic it is interested in, one topic per TCP connection.  But this strategy rapidly becomes undesirable as the number of topics one is interested in increases.

As we discussed in the comments for KAFKA-48 it's possible to consider the use case for multi-fetch and long polling together.  However there wasn't a conclusion to that discussion in terms of a direction.  The proposal that we discussed would allow for an interest set to be requested, and a long poll to then take place over all of the topics.  This is a reasonable solution to the problem, but it's not really going to address my needs - and thus why I submitted this feature request.

The reasoning is simple - if I have 10,000 topics I am interested in, and only a handful of those topics get messages, then KAFKA-48 is not going to address my signal to noise problem.  I will still have to submit - rather poll - for 10,000 topics and then get a few messages at a time - this means I have something like - depending on the numbers - 10kx20 bytes (assuming average topic name length of 20) 200kbyte requests which maybe return a handful of messages - lets say 10 at at time averaging say 200 bytes which means 200kbytes to receive 2kbytes or a 1:200 signal to noise ratio.  Ack.

KAFKA-170 only addresses the issue of non-blocking consumption in the consumer which is an implementation issue in the client only.  I know that KAFKA-170 can be done in the client using the current TCP protocol because I've already done it using a NodeJS client.

","17/Nov/11 05:43;tgautier;All that being said, if you'd like to combine KAFKA-48 and KAFKA-208 together I am fine with that - as long as we address the network efficiency issue at the same time in KAFKA-48.","17/Nov/11 07:31;tgautier;Jay,

I think I just realized how the solution you proposed in KAFKA-48 can be used efficiently for my use case.  I suppose this was the solution you had proposed but maybe I did not understand it fully.  

If I understand it right, your proposal is that long poll requests can be pipelined in the server, and once they are satisified they do not stick around, they should be polled again.  So to use my example from above, the way it would work would be:

-> request messages topic A offset a
-> request messages topic B offset b
-> request messages topic C offset c
(time passes)
<- messages topic B 
-> request messages topic B offset b'
(time passes)
-> request messages topic D offset D
(time passes)
<- messages topic C
-> request messages topic C offset c'

etc.

If this is the vision - then let's close this topic and fold it into KAFKA-48 - I think this will suffice for everything I need.
","17/Nov/11 08:12;jkreps;Yes, but I was actually hoping to even do a multifetch version. So it would be

request A:a, B:b, C:c, timeout:500ms
immediately get data:B:b', C:c'
request A:a, B:b', C:c', timeout:500ms
no data for any topics, request blocks
message on C
get data for C:c''
request A:a, B:b', C:c'', timeout:500ms

Etc.

I think this multifetch approach simplifies things for the consumer implementation. So I think that might cover what you want, I think the primary differences is that there is no separate registration of interest and polling for data, which i think makes sense.

The only question I think was left open was whether, in addition to a timeout, we could also support a min_bytes parameter so that the response would wait for at least that much data to accumulate. This is slightly more complex to implement on the server side but could improve efficiency.","17/Nov/11 08:44;tgautier;Hmm,

Your version seems that it would be inefficient for large # of topics with a (relative to the overall #) small # of topics that receive messages.  The version I posted is much better in this regard, the amount of data actually transmitted is relative not to the # of topics requested, but to the # that receive traffic.

I don't think my version is very hard to implement at all, in fact it would be some very small changes to the current code I have running in Node (which admittedly is event based and a natural fit for async behavior)

I don't personally have any current need for min bytes nor does it seem that I would need it in upcoming use cases.","17/Nov/11 16:23;jkreps;Ah, I see, now. So if I understand your need you actually have a very large number of topics (say a thousand) so the reason for splitting the register and poll calls is to avoid sending the topic names each time. I think the problem with a separate register call is how to manage that metadata on the server. I would be concerned about something that had to be manually deallocated because it is possible for the client to leak server resources. Presumably with enough support it could somehow be tied to the life of the socket that created it...so maybe each socket could only register at most one interest set and that would be deallocated if that socket closed.","17/Nov/11 16:51;tgautier;That's right :)  And yes, I was assuming that a single socket would retain an interest set and de-allocate it on close.

However, it might be easier to use my other proposal most recently stated.   It just builds on the existing polling mechanism but allows for requests to be blocking.  

To implement it, responses have to be able to come back out of order compared to the requests order, and that requires that the response must contain either a request id, or the topic name (but this is pretty much required no matter what we do).  

In this case, the client simply re-polls for any responses that are received (or any new topics, if the client so desires)

So to re-iterate and add a timeout parameter to illustrate how that also works (where C and S are client and server):

C->S fetch messages topic A offset a timeout 500
C->S fetch messages topic B offset b timeout 500
C->S request messages topic C offset c timeout 500
(250 ms passes, server receives 3 messages for topic B) 
C<-S topic B messages size b' (3) messages
C->S request messages topic B offset (b+b') timeout 500
(200 ms passes) 
C->S request messages topic D offset d timeout 500
(50 ms passes, no messages received for A or C) 
C<-S topic A messages size 0 (0) messages
C<-S topic C messages size 0 (0) messages
C->S fetch messages topic A offset a timeout 500
C->S request messages topic C offset c timeout 500


... and so on.

Both of these solutions are pretty similar to one another, in the above case there is still pretty much an ""interest set"" however it's implementation might differ in that it might be considered as outstanding requests. 

What I like about the above solution is that there is no magic going on at the server side.  In the registered interests implementation, the server also has to keep the current offset of the registered interest set to continue advancing it on the client's behalf, and (other than the automatic storage of the offset in ZK by the high level consumer) this idea goes against the design principles of Kafka as I understand them (move the tracking of offsets to the client to make Kafka simple).

","17/Nov/11 16:52;tgautier;Btw, much like KAFKA-101, I may need to implement this in a very short time.","20/Feb/13 17:24;lanzaa;Polling for many topics in one request seems to be handled by the new 0.8 protocol. Should this bug be closed?","20/Feb/13 17:27;jkreps;This is fixed in the new 0.8 protocol 
https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a test jar,KAFKA-676,12624488,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,18/Dec/12 16:36,18/Dec/12 17:50,12/Jan/21 10:06,18/Dec/12 17:50,0.8.0,,,,,,,,,,,,,,,,0,,,,,"It's nice to package up a seperate test jar so people can use our test classes to instantiate test harnesses for zk, kafka, etc.

Better still, of course, would be to provide a real in-memory ""mock kafka"", but that is more work.",,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/12 16:37;jkreps;KAFKA-676.patch;https://issues.apache.org/jira/secure/attachment/12561510/KAFKA-676.patch",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-12-18 16:47:19.856,,,false,,,,,,,,,,,,,,,,,,300037,,,Tue Dec 18 16:47:19 UTC 2012,,,,,,,"0|i165g7:",244015,,,,,,,,,,,,,,,,"18/Dec/12 16:37;jkreps;Trivial change to add a test jar.","18/Dec/12 16:47;nehanarkhede;+1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support for mirroring from multiple sources,KAFKA-201,12531194,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjkoshy,pquerna,pquerna,11/Nov/11 23:08,06/Dec/12 18:44,12/Jan/21 10:06,06/Dec/12 18:43,,,,,,,,,,,,,,,,,0,,,,,"Currently the EmbeddedConsumer is configured against a single source mirror.

We have a use case of consuming from multiple sources clusters.

Simple example, we have 3 datacenters which are collecting data: A, B, C.

We want all 3 to get full copies of the data eventually, by using a  on a whitelist of topics, and having the topics include the source data centers.

So, we would like to be able to:

Configure A to mirror B, and C with a whitelist of topics B,C
Configure B to mirror A, and C with a whitelist of topics A,C
Configure C to mirror A, and B with a whitelist of topics A,B

",,jjkoshy,jkreps,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-11-12 01:46:46.452,,,false,,,,,,,,,,,,,,,,,,216931,,,Thu Dec 06 18:43:56 UTC 2012,,,,,,,"0|i029zb:",11218,,,,,,,,,,,,,,,,"12/Nov/11 01:46;jkreps;This would be a great feature, we would gladly take a patch!","14/Nov/11 03:28;junrao;This should be relatively easy.  can just extend KafkaServerStartble to take an array of ConsumerConfig and create multiple instances of EmbeddedConsumer.","06/Dec/12 05:22;jkreps;Mirror maker does this, no?","06/Dec/12 18:43;jjkoshy;Yes - this should be addressed by MirrorMaker although the user should ensure that the consumption graph for every topic is a DAG.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generalize the getOffsets call,KAFKA-629,12616865,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Duplicate,,jkreps,jkreps,20/Nov/12 00:33,06/Dec/12 16:31,12/Jan/21 10:06,06/Dec/12 16:31,,,,,,,,,,,,,,,,,0,,,,,"This API is a little odd. Currently it gets a list of the first offset for each segment as well as the log end offset. It lets you filter by timestamp, though this is a bit broken because it uses the timestamp of the segment file which doesn't correspond to the first offset but rather the first offset int he next segment.

People have asked for a way to correlate time and offset. A complete mapping for this would be a bit difficult to implement (it would be a whole new persistent data structure) but a compromise would be to just replace this API with something a bit more generic.

This would differ from the getMetadata api in that it would go to the master for a given partition, so it can give info on the Log. That API answers questions that any broker can answer.

This could be called something like getPartitionMetadata. It would go only to the master for a given partition. It would give out data on each segment in the log, including (offset, timestamp, size_in_bytes) as well as having a separate entry for the logEndOffset.",,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-252,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,258737,,,2012-11-20 00:33:11.0,,,,,,,"0|i0l2vb:",121115,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
embed offset inside Message class,KAFKA-71,12514708,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Invalid,,,,19/Jul/11 21:32,15/Nov/12 03:59,12/Jan/21 10:06,15/Nov/12 03:59,,,,,,,,,,,,,,,,,0,,,,,"introduce Message.getOffst() that returns the offset this message is read from. 

I am using SimpleConsumer to manage offset myself, I would need to do: 

offset+=MessageSet.entrySize(msg); 

and keep track of the offset myself. 

This would my life easier if Message contains offset information.",,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,67459,,,Thu Nov 15 03:59:57 UTC 2012,,,,,,,"0|i02a3z:",11239,,,,,,,,,,,,,,,,"15/Nov/12 03:59;jkreps;In 0.8 the iterator returns the offset.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix commit() in zk consumer for compressed messages,KAFKA-546,12610380,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,jkreps,jkreps,04/Oct/12 19:40,08/Nov/12 06:20,12/Jan/21 10:06,08/Nov/12 06:20,0.8.0,,,,,,,0.8.0,,,,,,,,,0,,,,,"In 0.7.x and earlier versions offsets were assigned by the byte location in the file. Because it wasn't possible to directly decompress from the middle of a compressed block, messages inside a compressed message set effectively had no offset. As a result the offset given to the consumer was always the offset of the wrapper message set.

In 0.8 after the logical offsets patch messages in a compressed set do have offsets. However the server still needs to fetch from the beginning of the compressed messageset (otherwise it can't be decompressed). As a result a commit() which occurs in the middle of a message set will still result in some duplicates.

This can be fixed in the ConsumerIterator by discarding messages smaller than the fetch offset rather than giving them to the consumer. This will make commit work correctly in the presence of compressed messages (finally).",,jkreps,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/12 01:31;swapnilghike;kafka-546-v1.patch;https://issues.apache.org/jira/secure/attachment/12551806/kafka-546-v1.patch","02/Nov/12 15:36;swapnilghike;kafka-546-v2.patch;https://issues.apache.org/jira/secure/attachment/12551872/kafka-546-v2.patch","06/Nov/12 02:54;swapnilghike;kafka-546-v3.patch;https://issues.apache.org/jira/secure/attachment/12552205/kafka-546-v3.patch","07/Nov/12 23:08;swapnilghike;kafka-546-v4.patch;https://issues.apache.org/jira/secure/attachment/12552557/kafka-546-v4.patch","07/Nov/12 23:33;swapnilghike;kafka-546-v5.patch;https://issues.apache.org/jira/secure/attachment/12552562/kafka-546-v5.patch",,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2012-11-02 01:31:19.014,,,false,,,,,,,,,,,,,,,,,,241115,,,Thu Nov 08 06:20:41 UTC 2012,,,,,,,"0|i0191j:",5234,,,,,,,,,,,,,,,,"02/Nov/12 01:31;swapnilghike;1. ConsumerIterator skips messages that have already been fetched.

2. consumer.PartitionTopicInfo.enqueue()
- Made a change to pass the starting offset of a messageSet instead of the current fetchedOffset to FetchedDataChunk(). The current code expects the fetchedOffset to be the same as the starting offset of the incoming messageSet. But if a messageSet was partially consumed and fetched again, the fetchedOffset that goes into the FetchedDataChunk will be greater than the starting offset in the incoming messageSet. The fix takes care of this situation. The fix also does not disturb consumption under normal sequential fetches, because in this situation the starting offset of incoming messageSet will actually be the same as the fetchedOffset recorded in partitionTopicInfo.

3. Added a unit test to test de-deduplication of messages in ConsumerIterator.","02/Nov/12 15:21;junrao;Can't see to apply the patch cleanly to 0.8. Could you rebase?

$ patch -p0 < ~/Downloads/kafka-546-v1.patch 
patching file core/src/test/scala/unit/kafka/consumer/ConsumerIteratorTest.scala
patching file core/src/main/scala/kafka/message/ByteBufferMessageSet.scala
Reversed (or previously applied) patch detected!  Assume -R? [n] ^C
","02/Nov/12 15:36;swapnilghike;Rebased.","05/Nov/12 22:08;junrao;Thanks for patch v2. Some comments:

20. ConsumerIterator.next(): The following code depends on no gaps in offsets. This is true at this moment, but may not be true in the future when we have a different retention policy. A safer way is to keep iterating the messageSet until we get an offset that reaches or passes ctiConsumeOffset.
        for (i <- 0L until (ctiConsumeOffset - cdcFetchOffset)) {
          localCurrent.next()

21. PartitionTopicInfo: In startOffset(), unfortunately, we can't use the shallow iterator. This is because when messages are compressed, the offset of the top level message has the offset of the last message (instead of the first one) in the compressed unit. Also, iterating messages here may not be ideal since it forces us to decompress. An alternative way is to do the logic in ConsumerIterator.next(). Everytime that we get a new chunk of messageset, we keep iterating it until the message offset reaches or passes the consumeroffset. This way, if we are doing shallow iteration, we don't have to decompress messages.

22. ConsumerIteratorTest: 
22.1 zkConsumerConnector is not used.
22.2 We probably should set consumerOffset to a value >0 in PartitionTopicInfo.
22.3 Also, could we add a test that covers compressed messageset?
","06/Nov/12 02:54;swapnilghike;Thanks for the comments. The fixes are as follows:

20. Changed ConsumerIterator to iterator until it reaches or passes ctiConsumerOffset.

21. Reverted this change because as discussed: 
i. On commit of a part of a compressed message, the fetchOffset that will be checkpointed will be the actual fetchOffset, and not the offset of the last message in the compressed message set.
ii. We need to keep fetchOffset to make sure that ShallowIterator works fine under normal conditions.

22.1 Remove zkConsumerConnector.
22.2 The comments in the test case should be helpful in this regard.
22.3 Changed the test to use deep iterator over compressed message set.

Other random changes:
1. Imports optimized over changes that were pulled in via rebase.
2. I had missed removing the calls to toInt() at a couple places in KAFKA-556 for FileMessageSet.sizeInBytes(). Fixing this.","07/Nov/12 18:01;junrao;Thanks for patch v3. A couple of more comments:
30. ConsumerIterator.next(): In the following code, to be safe, we need to check if localCurrent hasNext in the while loop.
    // reject the messages that have already been consumed
    while (item.offset < currentTopicInfo.getConsumeOffset) {
      item = localCurrent.next()

31. ConsumerIteratorTest: Not sure if the test really does what it intends to. To simulate reading from the middle of a compressed messageset, we need to put in a consume offset larger than 0 in PartitionTopicInfo, right?","07/Nov/12 23:08;swapnilghike;30. Fixed.

31. The test in v3 patch would've worked too, but changed it for clarity in this patch.","07/Nov/12 23:33;swapnilghike;A small addition to the unit test to make sure that the iterator does not have any extra elements. ","08/Nov/12 06:20;junrao;Thanks for patch v5. +1. Committed to 0.8.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve Kafka internal metrics,KAFKA-203,12531357,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,jkreps,jkreps,14/Nov/11 18:52,13/Sep/12 04:32,12/Jan/21 10:06,13/Sep/12 04:32,0.8.0,,,,,,,0.8.0,,,core,,,,,,1,tools,,,,"Currently metrics in kafka are using old-school JMX directly. This makes adding metrics a pain. It would be good to do one of the following:
1. Convert to Coda Hale's metrics package (https://github.com/codahale/metrics)
2. Write a simple metrics package

The new metrics package should make metrics easier to add and work with and package up the common logic of keeping windowed gauges, histograms, counters, etc. JMX should be just one output of this.

The advantage of the Coda Hale package is that it exists so we don't need to write it. The downsides are (1) introduces another client dependency which causes conflicts, and (2) seems a bit heavy on design. The good news is that the metrics-core package doesn't seem to bring in a lot of dependencies which is nice, though the scala wrapper seems to want scala 2.9. I am also a little skeptical of the approach for histograms--it does sampling instead of bucketing though that may be okay.


",,cburroughs,jcreasy,jjkoshy,jkreps,junrao,kamaradclimber,nehanarkhede,otis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-174,KAFKA-440,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/12 17:02;junrao;kafka-203_v1.patch;https://issues.apache.org/jira/secure/attachment/12543706/kafka-203_v1.patch","12/Sep/12 16:11;junrao;kafka-203_v2.patch;https://issues.apache.org/jira/secure/attachment/12544836/kafka-203_v2.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-11-14 19:34:19.051,,,false,,,,,,,,,,,,,,,,,,217093,,,Thu Sep 13 04:32:20 UTC 2012,,,,,,,"0|i15zan:",243018,,,,,,,,,,,,,,,,"14/Nov/11 19:34;tgautier;FYI we are using the JMX metrics currently.","23/Nov/11 17:17;cburroughs;I *think* metrics is source compatible with 2.8,  but I would need to investigate more.","11/Jan/12 01:02;charmalloc;I have implemented Coda Hale's Metric package and have it running in production.

pros:
- good coverage for typical needs with not much extra effort once it is integrated
- ganglia context 
- graphite context (I don't use but it is a pro for someone)

cons:
- package causes a dependency
- the MetricsServlet is only JSON so some extra effort to have a version that also can be better human readable (better for debugging something I found often)
- builds are 2.9.1 (I run kafka in 2.9.1 build with it and it works fine though porting it back to 2.8 may get hair and tricky)


IMHO 

- since there is no compatible Scala version in maven we would basically have to checkout or fork the code, build it and include it (or publish the build ourselves to maven). 
- we should have our own version of MetricsServlet ... if someone wants to implement the http embedded jetty server it should be configurable which then makes the jetty jar not required

So I am +1 in using source from Coda Hale and keeping that layer as the layer for metrics (we could check in the specific files we need/want to a package kafka.metrics and then make changes as we need (license = ASL 2.0 https://github.com/codahale/metrics/blob/master/LICENSE)

I am comfortable with codahale/metrics source and I would contribute this by pulling the parts in we are looking to use with an initial implementation for using them allowing (replacing anything existing JMX as this would handle that and other context we include like the servlet, ganglia, whatever else we want to make) others to-do so for other things moving forward.  ","18/Jul/12 20:18;cburroughs;I also have metrics all over the place in production as well.

The scala compatibility problem only exists if we use metrics-scala, which is just a  pretty wrapper around the pure java stuff http://metrics.codahale.com/manual/scala/.  If we don't use that all of the dependency complications go away.  Is using the java interface palatable?

For mx4j we made it optional on the classpath.  We can follow the same pattern with metrics reporters to minimize dependency complications for kafak-as-library.

Off Topic:  Json is totally human readable https://addons.mozilla.org/EN-us/firefox/addon/jsonview/","08/Aug/12 21:00;nehanarkhede;This probably is unblocked by KAFKA-385. It will be good to convert all existing Kafka metrics to the new metrics package","08/Aug/12 21:02;nehanarkhede;This ticket blocks metrics collection and graphing.","28/Aug/12 17:12;junrao;I propose that we add/keep the following set of metrics. Anything missed?

Server side:
A. Requests:
A1. produceRequestRate (meter, total)
A2. fetchRequestRate (meter, follower/non-follower)
A3. getMetadataRate (meter, total)
A4. getOffsetRate (meter, total)
A5. leaderAndISRRate (meter, total)
A6. stopReplicaRate (meter, total)
A7. produceRequestSizeHist (hist, total)
A8. fetchResponseSizeHist (hist, total)
A9. produceFailureRate (meter, topic/total)
A10. fetchFailureRate (meter, topic/total)
A11. produceRequestTime (timer, total)
A12. fetchRequestTime (timer, total)
A13. messagesInRate (meter, topic/total)
A14. messagesOutRate (meter, topic/total)
A15. messagesBytesInRate (meter, topic/total)
A16. messagesBytesOutRate (meter, topic/total)

B. Log:
B1. logFlushTime (timer, total)

C. Purgatory:
Produce:
C1. expiredRequestMeter (meter, partition/total)
C2. satisfactionTimeHist (hist, total)

Fetch:
C3. expiredRequestMeter (meter, follower/non-follower)
C4. satisfactionTimeHist (hist, follower/non-follower)

Both:
C5. delayedRequests (gauge, Fetch/Produce)

D. ReplicaManager:
D1. leaderPartitionCounts (gauge, total)
D2. underReplicatedPartitionCounts (|ISR| < replication factor, gauge, total)
D3. ISRExpandRate (meter, partition/total)
D4. ISRShrinkRate (meter, partition/total)

E. Controller:
E1. requestRate (meter, total)
E2. requestTimeHist (hist, total)
E3. controllerActiveCount (gauge, total)

Clients:
F. Producer:
F1. messageRate (meter, topic/total)
F2. byteRate (meter, topic/total)
F3. droppedEventRate (meter, total)
F4. requestRate (meter, total)
F5. requestSizeHist (hist, total)
F6. requestTimeHist (hist, total)
F7. resendRate (meter, total)
F8. failedSendRate (meter, total)
F9. getMetadataRate (meter, total) 

G. Consumer:
G1. messageRate (meter, topic/total)
G2. byteRate (meter, topic/total)
G3. requestRate (meter, total)
G4. requestSizeHist (hist, total)
G5. requestTimeHist (hist, total)
G6. lagInBytes (gauge, partition)

Also, I propose that we remove the following metrics since they are either not very useful or are redundant.
Purgatory:
Produce:
* caughtUpFollowerFetchRequest (meter, partition/total): not very useful
* followerCatchupTime (hist, total): not very useful
* throughputMeter (meter, partition/total): same as bytesIn
* satisfiedRequestMeter (meter, total): not very useful

Fetch:
* satisfiedRequestMeter (meter, total): not very useful
* throughputMeter (meter, partition/total): same as bytesOut

Both
* satisfactionRate (meter, Fetch/Produce): not very useful
* expirationRate (meter, Fetch/Produce/topic): already at Produce/Fetch leve
","28/Aug/12 21:23;jkreps;There are some details around how we track request-level stats. The idea was to begin and end request measurement at the socket server so it includes the full lifecycle. Each request would have the following phases:
  read time - time spent reading data off the network
  queue time - time spent waiting for a processing thread to handle the request
  api time - time spent in the api layer
  queue time - time spent waiting for a network thread to handle the request
  write time - time spent writing data (which would include the sendfile time)

The implementation could just be to add to the Request object we have so that it has something like
   request.begin(""read"")
   // do some reading
   request.end(""read"")

It would be nice to have these stats at the client id level, but there are two problems:
1. That would be a lot of histogram data
2. Currently the socket server is not aware of the client id.

So I recommend we just track this info on a per-api basis for now. We can revisit in the future to get more in-depth instrumentation.","04/Sep/12 17:02;junrao;Attach patch v1. 

Patch overview:
1. Added general support to collect time breakdown (queueTime, localTime, remoteTime, sendTime, totalTime) for all types of requests. Need to refactor RequestChannel.Request a bit to include deserialized request object.
2. removed some metrics in DelayedRequestMetrics since they are now covered by #1.
3. Fixed Pool.getMaybePut() to make sure that the new object is only created once.
4. Converted all existing metrics to use coda hale and added some new metrics.

The list of new and converted metrics is the following.
Server side:
A. Requests 
for each request type: 
A1. requestRate (meter, total)
A2. queueTime (hist, total)
A3. localTime (hist, total)
A4. remoteTime (hist, total)
A5. sendTime (hist, total)
A6. totalTime (hist, total)
For Fetch/Produce
A7. produceFailureRate (meter, topic/total)
A8. fetchFailureRate (meter, topic/total)
A9. messagesInRate (meter, topic/total)
A10. messagesOutRate (meter, topic/total)
A11. messagesBytesInRate (meter, topic/total)
A12. messagesBytesOutRate (meter, topic/total)
All
A13. requestQueueSize (gauge, total)

B. Log:
B1. logFlushTime (timer, total)
B2. logSegments (gauge, per log)
B3. logEndOffset (gauge, per log)

C. Purgatory:
Produce:
C1. expiredRequestMeter (meter, partition/total)

Fetch:
C2. expiredRequestMeter (meter, follower/non-follower)

Both:
C3. delayedRequests (gauge, Fetch/Produce)

D. ReplicaManager:
D1. leaderPartitionCounts (gauge, total)
D2. underReplicatedPartitionCounts (|ISR| < replication factor, gauge, total)
D3. ISRExpandRate (meter, partition/total)
D4. ISRShrinkRate (meter, partition/total)

E. Controller:
E1. controllerActiveCount (gauge, total)

Clients:
F. Producer:
F1. messageRate (meter, topic/total)
F2. byteRate (meter, topic/total)
F3. droppedEventRate (meter, total)
F4. producerQueueSize (gauge, per send thread)
F5. requestSizeHist (hist, total)
F6. requestTimeAndRate (timer, total)
F7. resendRate (meter, total)
F8. failedSendRate (meter, total)

G. Consumer:
G1. messageRate (meter, topic/total)
G2. byteRate (meter, topic/total)
G3. requestSizeHist (hist, total)
G4. requestTimeAndRate (timer, total)
G5. lagInBytes (gauge, partition)","06/Sep/12 18:34;jjkoshy;I think this patch looks great and this list of stats is a good start. I
have some minor comments:

1) Rebase - the latest patch applies cleanly to r1378264.

2) The following are just my preferences on naming. What you have should be
   fine, but we should make sure the stat names are as intuitive as
   possible. We should come up with a naming convention for stats and add it
   to our coding convention.

   a) Some timer stats may be better named. E.g., SimpleConsumer
   ConsumerRequestTime will include both request rate and request duration
   which is not very intuitive. OTOH I'm having trouble thinking of a naming
   convention: I would suggest just ConsumerRequestStats - but the size stat
   would be outside then.

   b) Partition.scala:
      ISRExpandRate -> ISRExpandEventRate
      ISRShrinkRate -> ISRShrinkEventRate

   c) Log.scala:
      ""LogSegments"" -> ""NumLogSegments""

   d) ConsumerTopicStat.scala:
      ""Total"" -> ""AllTopics"" Also, what if there's a topic called ""Total""?
      :) We may want to name this label such that it is an illegal topic
      name (KAFKA-495) - say, ""All/Topics"".

   e) SimpleConsumer.scala:
      ""ConsumerRequestTime"" -> see above.

   f) FileMessageSet.scala:
      ""LogFlush"" -> ""LogFlushStats""

   g) RequestChannel.scala:

      i) Instead of ""regular"" and ""follower"" how about ""consumer"" and
      ""replica""?

      ii) endRequestTracking -> updateRequestMetrics

      iii) responseComplet (typo)

      iv) For timing stats, may be better to include the unit as part of the
      metric names (e.g., TotalTimeNs).

      v) SendTime -> ResponseSendTime(Ns)

      vi) May be useful to add a comment that simply lays out the phases to
      make the code clearer:
      /* received (start time) -> in queue (queue time) -> dequeued for
      api-local processing -> [api remote processing] -> send response */

   h) AsyncProducerStats.scala:
      DroppedEvent -> DroppedEventsPerSec
      Resentevent -> ResendEventsPerSec
      resents -> resends
      FailedSend -> FailedSendsPerSec
      (or maybe we should just follow a convention: <stat>Rate which
      defaults to <stat> per sec)
      FailedSendtRate (typo)

   i) KafkaApis.scala
      byteInRate -> bytesInRate; byteOutRate -> bytesOutRate
      ExpiresPerSecond -> ExpirationsPerSec

   j) KafkaRequestHandlers.scala
      MessageInPerSec -> IncomingMessagesPerSec

3) There are some places (SimpleConsumer, FileMessageSet, SyncProducer)
   where you use metrics timers. Instead of this:

   val timer = newTimer(...)
   ...
   val ctx = timer.time()
   try {
     // do something
   }
   finally {
     ctx.stop()
   }

   You can use the following equivalent pattern:
   val timer = new KafkaTimer(underlying)
   timer.time {
     // do something
   }

4) ZookeeperConsumerConnector: These JMX operations are actually useful to
   consumers right?

5) DefaultEventHandler: should byte rate be updated here or only after
   sending? Although it does seem useful to have the global byte rate even
   for those that are subsequently dropped.

6) SyncProducer.scala: use KafkaTimer. Also, same comment on naming for
   timers described above.

7) AbstractFetcherThread.scala: FetcherLagMetrics.lock unused.

8) KafkaApis.scala:
   a) Line 108 unused
   b) One caveat in removing the per key ProducerRequestPurgatory stats is
   if there is a key that has an intermittently slow follower you won't be
   able to narrow it down very easily (since the entire request will
   expire). OTOH you will have that stat available from the follower - it's
   just that you will need to ""search"" for the follower that is causing the
   expirations. So I think it's fine to remove it as it makes the code a lot
   simpler.

9) Pool.scala: good idea.
","06/Sep/12 22:25;nehanarkhede;It's great to see a patch that fixes metrics. 

1. Partition
In the isUnderReplicated API, shouldn't the in sync replicas size be compared to the replication factor for that partition and not the default replciation factor ?

2. ZookeeperConsumerConnector
There are a bunch of interesting metrics here that are very useful while troubleshooting. For example,
2.1 Queue size per topic and consumer thread id: When the consumer client's processing slows down, the consumer's queues back up. Currently, to troubleshoot this issue, we need to take thread dumps. If we had the right monitoring on the
consumer, we could just look at the metrics to figure out the problem.
2.2 Fetch requests per second per fetcher: Useful to know the progress of the fetcher thread. In this, the bean might probably be named after the broker id that the fetcher is connected to, somewhere along the lines of per key purgatory metrics.
 
3. KafkaController
3.1 We need a way to tell if a partition is offline. If all replicas of a partition go offline, no leader can be elected for that partition and an alert would have to be raised.
3.2 We also need to be able to measure - 
3.2.1 leader election latency
3.2.2 Leader election rate 

4. ReplicaManager
4.1 Rename ISRExpandRate to isrExpandRate
4.2 Rename ISRShrinkRate to isrShrinkRate
4.3 I'm not sure how useful it is to have a count for leaders and under replicated partitions. We however, do need a per partition status that tells if the partition is offline or under replicated.

5. TopicMetadataTest
Wrap the long line

6. system_test
We have to add the new metrics to the metrics.json file to that we can view the metrics on every test run. Not sure if you want to push that to a separate JIRA or not ?
","12/Sep/12 16:11;junrao;Attache patch v2. Overview of additional changes.

1. rebased.

2. DefaultEventHandler: Added a metric for serlializationErrorRate. Also changed the serialization error handling a bit depending on whether the send is sync or async.

3. Use the following convention to distinguish AllTopic metrics data and the per topic one: for metrics X, AllTopics => AllTopicsX; topic Y => Y-X.

4. Made a pass of metrics names and tried to keep them consistent.

5. Updated metrics.json with the new metrics. Right now we start a separate jmx tool for each jmx bean. Too many beans will slow down the system test. So, I only exposed a subset of the metrics. Once the jmx tool issue is resolved, we can add more beans for collection and graphing. 

Review comments:
Joel:
4) Consumer lag is the most useful metrics, on which an alert can be set. LogEndOffset and ConsumerOffset are less useful and can be obtained from tools. 

5) The problem is that we only know the size of a message after it is serialized, since message itself can be of any type.

8) If a follower is slow, it will be eventually dropped out of ISR and we have metrics on both ISRShrink rate and underreplicated partitions to track this.

Neha:
1. For that, the broker needs to know the replication factor of a partition. This information needs to be sent from broker on LeaderAndISRRequest. I will leave that in kafka-340.

4.3 LeaderCount is useful to see if client loads are balanced among brokers and a global ""under replicated partition count"" is convenient for setting up a alert (otherwise, one has to do that on each partition).

The rest of the review comments are all addressed.
","12/Sep/12 20:04;jjkoshy;+1 on v2.

The following are all minor comments.

2.1 - For the TODO in Partition.scala, can you add a comment in the jira
  where it will be addressed (or create a separate jira) so we don't lose
  track of it.

2.2 - SimpleConsumer.scala: FetchRequestRateAndTimeMs ->
  FetchRequestRateAndDurationMs. Similar edits in FileMessageSet.scala for
  LogFlushRateAndTimeMs; SyncProducer.scala for
  ProduceRequestRateAndTimeMs

2.3 - ProducerTopicStat.scala: resents -> resends

2.4 - DefaultEventHandler.scala:
      a - val isSync = ""sync"".equals(config.producerType)
      b - So exceptions are no longer thrown for async producers on a
        serialization error. This will have an impact on KAFKA-496. Can you
        add a comment there after check-in?

2.5 - ReplicaManager: the meter name is inconsistent with the convention
  used elsewhere.

","13/Sep/12 04:32;junrao;Thanks for the review. Committed to 0.8.

2.1 created kafka-510 to track it.

2.2 left the name as it is since it's shorter

Addressed the rest of the comments.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose JMX operation to set logger level dynamically,KAFKA-429,12600877,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jjkoshy,jjkoshy,31/Jul/12 15:59,25/Aug/12 01:16,12/Jan/21 10:06,25/Aug/12 01:16,0.7.2,0.8.0,,,,,,0.7.2,0.8.0,,core,,,,,,0,,,,,"With KAFKA-16, we can change logger levels dynamically, but that is a global setting - i.e., it affects logging in all classes. It would be useful to expose a setLoggerLevel/add appenders operation that can be applied to classes/packages.

We should also add this for the tools (and not just the broker) - e.g., mirror-maker, console consumer, etc.",,jjkoshy,jkreps,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/12 21:35;jjkoshy;KAFKA-429-0.8-v1.patch;https://issues.apache.org/jira/secure/attachment/12540762/KAFKA-429-0.8-v1.patch","13/Aug/12 21:35;jjkoshy;KAFKA-429-trunk-v1.patch;https://issues.apache.org/jira/secure/attachment/12540763/KAFKA-429-trunk-v1.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-08-14 00:08:10.788,,,false,,,,,,,,,,,,,,,,,,292179,,,Sat Aug 25 01:16:28 UTC 2012,,,,,,,"0|i0rsa7:",160226,,,,,,,,,,,,,,,,"13/Aug/12 21:35;jjkoshy;Patch for 0.8 and trunk.

This adds a new log4j controller mbean that allows getting/setting log4j levels dynamically.

The logging trait forces initialization of the companion object. This means tools such as console consumer and mirror maker get the mbean automatically (since most of the tools use the logging trait).

Also, I removed the older LoggerDynamicMbean which is superseded by the new mbean.

Finally (unrelated): log the throwable on producer send exception.
","14/Aug/12 00:08;jkreps;I thought log4j supported this out of the box http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/jmx/package-summary.html","14/Aug/12 00:16;jjkoshy;Yes - but afaik it is very limited. For e.g., we used LoggerDynamicMBean (which this patch removes) but it is a global setting and you cannot add new loggers with it.","14/Aug/12 18:29;jkreps;Makes sense.","25/Aug/12 01:16;junrao;Thanks for the patch. Committed to trunk and 0.8.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Time based log segment rollout,KAFKA-475,12604162,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,swapnilghike,swapnilghike,21/Aug/12 02:16,25/Aug/12 00:35,12/Jan/21 10:06,25/Aug/12 00:35,0.7.1,,,,,,,0.7.2,0.8.0,,,,,,,21/Aug/12 00:00,0,features,,,,Some applications might want their data to be deleted from the Kafka servers earlier than the default retention time. ,,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/12 23:00;swapnilghike;kafka-475-0.8-v1.patch;https://issues.apache.org/jira/secure/attachment/12542361/kafka-475-0.8-v1.patch","21/Aug/12 02:19;swapnilghike;kafka-475-v1.patch;https://issues.apache.org/jira/secure/attachment/12541699/kafka-475-v1.patch","22/Aug/12 21:57;swapnilghike;kafka-475-v2.patch;https://issues.apache.org/jira/secure/attachment/12542039/kafka-475-v2.patch","22/Aug/12 22:04;swapnilghike;kafka-475-v3.patch;https://issues.apache.org/jira/secure/attachment/12542040/kafka-475-v3.patch","23/Aug/12 22:48;swapnilghike;kafka-475-v4.patch;https://issues.apache.org/jira/secure/attachment/12542201/kafka-475-v4.patch","24/Aug/12 19:03;swapnilghike;kafka-475-v5.patch;https://issues.apache.org/jira/secure/attachment/12542316/kafka-475-v5.patch",,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,2012-08-21 15:07:46.599,,,false,,,,,,,,,,,,,,,,,,249042,,,Sat Aug 25 00:35:23 UTC 2012,,,,,,,"0|i0a42v:",56934,,,,,,,,,,,,,,,,"21/Aug/12 02:19;swapnilghike;To facilitate this, we can roll out a new log segment whenever a time threshold is reached if the size limit has not been reached already. We can fix this time limit for segment roll out as the same as retention time limit. These values will make sure that the number of open file handles at any point in the system cannot more than double.","21/Aug/12 15:07;junrao;Thanks for patch v1. Some comments:

1. The condition for testing whether we should roll a new log segment doesn't seem right. Currently, it will roll a new segment if the last segment hasn't been updated for retention time. What we should do is to roll a new segment every retention interval independent of the last update time, as long as (a) no segment has been rolled since the last retention interval; (b) the last segment has a size larger than 0.

2. We should add a unit test to test rolling a new segment by time. ","21/Aug/12 17:57;nehanarkhede;If you roll log segments based on retention time, seems like you can have only one segment for that log at any point of time. If you want to roll 5 minute segments, it means that you can only have 5 minute worth of data for that partition. On the contrary, if I choose size based rolling and size based retention, I can have multiple log segments each of a specific size. What seems desirable is to have time based rolling + retention also behave the same way. I would imagine applications wanting to roll segments every 1 hour and retain 24 hours worth of data. This is an advantage for applications using getOffsetsBefore() to do some time indexed fetch of the data, since getOffsetsBefore only returns offsets at the log segment granularity. And it also gives applications a way to reason about the time window of the data retained for a partition. One potential downside is that, you can end up creating large number of log segments for your partition, if you choose too small a value for log.file.time.ms. But this problem exists today with size based log segment rolling too. So we are not introducing any regression in behavior.

Other review comments -

1. Log
1.1 Rename currentMS to currentMs (Follow camel case convention).
1.2 How about renaming retentionMSInterval to retentionIntervalMs to be consistent with naming convention ?
1.3 In maybeRoll, looks like currentMS is unused apart from being used to compute the time difference. How about removing currentMS ?

2. LogManager
2.1 This is unrelated to your patch, but lets also rename logRetentionMSMap to logRetentionMsMap

","21/Aug/12 19:36;swapnilghike;Jun: Thanks for pointing out the mistake. I could not see why (a) in your suggestions is important though. Could you please elaborate if it makes a difference if we did not implement (a)?

Neha: Please correct me if I failed to see your point. In this proposed scheme, a new segment will be rolled out depending on whichever of the size limit or the time limit is hit first. So, if a producer produces data fast enough, it can still create multiple segments due to the size limit on each segment. I have set the time interval of rolling = retention time interval. In this case, if the segments don't hit the size limit within the retention time (due to aggressive retention time or slow production of data), then what you said will be true and there will be at most two active segments in the log at any point of time. In the first case, the application indeed wanted its data cleaned up fast and in the second case, hopefully the number of segments should not matter. 

Including your other suggestions in the patch.","22/Aug/12 21:57;swapnilghike;Patch attached:
1. Time based log segment rollout added. As discussed with Neha, the values of config.logRollHours and config.logRetentionHours are decoupled now.

2. Moved the position of maybeRoll(segment) call in the Log to make sure that a new message does not get appended to a segment that has expired in time. 
   i. Accordingly modified the testCleanupSegmentsToMaintainSizeWithSizeBasedLogRoll

3. I have currently set the range of logRetentionHours and logRollHours to (1, 24 * 7). An upper cap on the value of hours is necessary because a very high value of hours can overflow and become negative when converted to milliseconds. 

4. Unit tests added in LogTest 
    i.testTimeBasedLogRoll 
    ii. testSizeBasedLogRoll

5. Unit tests added in LogManagerTest (sorry couldn't come up with more concise names :\ )
    i. testCleanupSegmentsToMaintainSizeWithTimeBasedLogRoll
    ii. testCleanupExpiredSegmentsWithTimeBasedLogRoll","22/Aug/12 22:04;swapnilghike;Removed an unnecessary assert statement. Please view v3 of patch.","23/Aug/12 14:42;junrao;Thanks for patch v3. A few other comments:
20. KafkaConfig:
20.1 To be consistent, we probably should add topic level log file size for rolling.
20.2 We probably don't need to cap logRoll and logRetention hours at 24*7 since we store ms in long, which has 2^^63 millseconds.

21. LogSegment: Unlike java, we can just have ""val startTime"" and use it directly. Scala already wraps the val with a public getter.

22. LogManagerTest: It seems to me that we can test log rolling (covered in LogTest) and log cleanup (covered in LogManager) independently. Is there any value in testing all 4 combination of log rolling and log cleanup?
","23/Aug/12 18:28;swapnilghike;1. Similarly, should we also add topic level log retention size?
2. Ok.
3. Ok. I am actually changing it to a var because there is one small change to be made to the rolling policy - We don't roll a new log when the previous segment which has expired in time is empty. When a new message is finally appended to this empty expired segment, its timeOfCreation should also be reset to a new value.
4. I implemented the new tests to make sure that the independent mechanisms of roll and recovery don't interfere with each other. But now that I look at them, they indeed look like a working module of rolling followed by a working model of recovery. We can either remove them, or I can try to combine all modes of roll and recovery in one new test to check for any interference.

Also, should we have a check for illegal values in getTopic* methods in Utils?","23/Aug/12 19:06;junrao;1. Yes, adding a topic level log retention size will be useful.

3. Yes, we can make timeOfCreation and Option. Initially, it will be none. It becomes a non-empty value on next append.

4. It doesn't seem that rolling logs are interfering with log cleanup. So, removing those tests should be fine.

","23/Aug/12 22:48;swapnilghike;1. Topic level log roll size and retention size limits added. 
2. Removed the cap on logRoll and logRetention Hours. 
3. Created an Option for the timeOfFirstAppend. 
4. Removed the unnecessary unit tests. 

Created kafka-481 for adding require() to getTopic* methods.","24/Aug/12 15:02;junrao;Thanks for patch v4. A couple more comments:

30. LogSegment.updateFirstAppendTime(): Could we instead add an LogSegment.append(ByteBufferMessageSet) which appends the data and updates the timestamp? Also, if ByteBufferMessageSet.sizeInBytes is less than 0, in addition to not updating the time, we can avoid appending the messageset.

31. Could you rebase?
","24/Aug/12 19:03;swapnilghike;Made the changes.","24/Aug/12 20:39;junrao;Thanks for patch v5. Committed to trunk. Could you port to 0.8 too?","24/Aug/12 23:00;swapnilghike;Patch for 0.8 attached. After rebasing, I can apply patch for 481 to 0.8.","25/Aug/12 00:35;junrao;Thanks for the patch. Committed to 0.8.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Investigate removing the synchronization in Log.flush,KAFKA-191,12530469,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,jkreps,jkreps,06/Nov/11 02:29,12/Jul/12 21:42,12/Jan/21 10:06,12/Jul/12 21:42,0.8.0,,,,,,,,,,core,,,,,,0,,,,,"Currently we have the following synchronization in Log.scala:
1. append, roll, and flush all share a write lock
2. read is non-blocking

Unfortunately this means that flush time latency is added to appends (even if the flush is done by a background thread). To fix this we should investigate a scheme to make append and flush not block each other.",,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/11 03:22;jkreps;KAFKA-191.patch;https://issues.apache.org/jira/secure/attachment/12502635/KAFKA-191.patch",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-11-06 02:56:30.457,,,false,,,,,,,,,,,,,,,,,,216207,,,Thu Jul 12 21:42:08 UTC 2012,,,,,,,"0|i09m3b:",54012,,,,,,,,,,,,,,,,"06/Nov/11 02:56;nehanarkhede;I'm interested in investigating this and run some perf tests to see change in IO performance","06/Nov/11 03:21;jkreps;Neha--Sounds good. I have a patch (since it is just deleting and reordering, the code change itself is trivial), I will attach. Here are my thoughts. I think we can just remove the synchronization and re-order things so that the unflushed counter and lastFlushTime both remain valid lower bounds. It is possible that the time we set could get overwritten by another thread but it is unlikely to make any practical difference. See if you agree with that logic, I am not 100% positive.

I am not at all sure that this will actually help performance though for two reasons. First, I think it is possible that the file itself may be synchronized. Either at the java level or the OS level. So I am not sure if one can write to the file while a flush is occurring in another thread. This may take some research to understand. 

Second, if it is possible to do parallel write and flush, I think this still may not be ideal (though maybe a good short term hack). My reasoning is that this patch only fixes the blocking behavior for the time-based flush, but my question is why would I ever want to block?

I really see two use cases:
1. I want every message I write immediately flushed to disk in a blocking fashion before the append() is considered completed. This corresponds to flush.interval=1 in the current system.
2. I want to periodically flush data, which could be based on the number of messages, or time, (or theoretically based on unflushed bytes, though we haven't implemented that).

So what I am thinking is that case (1) clearly needs to be blocking to make sense. But for any periodic flush I don't see a reason to block appends. It is true that this makes the intervals inexact, but I think that is probably fine.. For example, even if I set flush.interval=5, it is unlikely that I could actually care that it is exactly 5, I just want to flush often, say ~5 messages. (Even if I did want it exact, since we always write the full messageset, I still might not get that). So I am thinking a better long-term approach might be to have a central threadpool that handles all flushing and have that always be asynchronous. So if I set flush.interval=5, then that means the background thread is triggered every 5 messages BUT no one blocks on this. In addition to this we add an immediate.commit=true/false option to force data to be flushed in a blocking way as part of the append.

Obviously the above only works if a parallel append and flush are possible.","06/Nov/11 15:44;jkreps;Looks like fsync does block writes, which is kind of weak. This thread is really helpful:
  http://antirez.com/post/fsync-different-thread-useless.html

The comments do mention that fsyncdata() (aka FileChannel.force(false)) somehow doesn't interfere, which doesn't make sense to me, as for use since we always append, FileChannel.force should do the same thing regardless of whether we force metadata or not because we are changing the file size which must be flushed. ","07/Nov/11 05:41;junrao;So, I guess If append and fsync can't run in parallel, the patch won't help much. ","12/Jul/12 21:42;jkreps;This is not needed in post 0.8 world since flush becomes less important with replication.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make the request processing in kafka asynchonous,KAFKA-202,12531209,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,jkreps,jkreps,12/Nov/11 03:20,13/Jan/12 22:41,12/Jan/21 10:06,13/Jan/12 22:41,,,,,,,,0.8.0,,,,,,,,,0,,,,,"We need to handle long-lived requests to support replication. To make this work we need to make the processing mechanism asynchronous from the network threads.

To accomplish this we will retain the existing pool of network threads but add a new pool of request handling threads. These will do all the disk I/O. There will be a queue mechanism to transfer requests to and from this secondary pool.",,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-48,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/11 18:41;jkreps;KAFKA-202-v2.patch;https://issues.apache.org/jira/secure/attachment/12503650/KAFKA-202-v2.patch","05/Jan/12 22:01;jkreps;KAFKA-202-v3.patch;https://issues.apache.org/jira/secure/attachment/12509610/KAFKA-202-v3.patch","10/Jan/12 07:18;jkreps;KAFKA-202-v4.patch;https://issues.apache.org/jira/secure/attachment/12510014/KAFKA-202-v4.patch","10/Jan/12 07:59;jkreps;KAFKA-202-v5.patch;https://issues.apache.org/jira/secure/attachment/12510017/KAFKA-202-v5.patch","10/Jan/12 21:38;jkreps;KAFKA-202-v6.patch;https://issues.apache.org/jira/secure/attachment/12510106/KAFKA-202-v6.patch","12/Nov/11 03:24;jkreps;KAFKA-48-socket-server-refactor-draft.patch;https://issues.apache.org/jira/secure/attachment/12503472/KAFKA-48-socket-server-refactor-draft.patch",,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,2011-11-20 06:11:17.895,,,false,,,,,,,,,,,,,,,,,,216946,,,Fri Jan 13 22:41:23 UTC 2012,,,,,,,"0|i09man:",54045,,,,,,,,,,,,,,,,"14/Nov/11 18:41;jkreps;This patch is ready for review. Note the new config I added for controlling the number of I/O threads.

Also, this patch removes all JMX monitoring from the socket server. Temporarily ignore this fact. The issue was our JMX was tangling together the socket server and the request handling. I want to generalize our stat collection and will open a second JIRA for that.","20/Nov/11 06:11;tgautier;Unfortunately it seems this patch doesn't apply cleanly against the latest code.","05/Jan/12 22:01;jkreps;Updated patch to current trunk.","06/Jan/12 06:02;junrao;Overall, the patch looks good. Some comments:

1. KafkaServer.startup doesn't have to capture exception and shutdown. The caller in KafkaServerStarable already does that. Plus, it shuts down embedded consumer appropriately if needed.

2. There is KafkaRequestHandlers.scala.rej in the patch.

3. Unit test seems to fail occasionally, giving the following error.
[info] == core-kafka / kafka.integration.LazyInitProducerTest ==
[2012-01-05 21:57:38,773] ERROR Error processing MultiProducerRequest on test:0 (kafka.server.KafkaApis:82)
java.nio.channels.ClosedChannelException
	at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:184)
	at kafka.message.ByteBufferMessageSet.writeTo(ByteBufferMessageSet.scala:75)
	at kafka.message.FileMessageSet.append(FileMessageSet.scala:161)
	at kafka.log.Log.append(Log.scala:215)
	at kafka.server.KafkaApis.kafka$server$KafkaApis$$handleProducerRequest(KafkaApis.scala:71)
	at kafka.server.KafkaApis$$anonfun$handleMultiProducerRequest$1.apply(KafkaApis.scala:64)
	at kafka.server.KafkaApis$$anonfun$handleMultiProducerRequest$1.apply(KafkaApis.scala:64)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
	at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
	at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34)
	at kafka.server.KafkaApis.handleMultiProducerRequest(KafkaApis.scala:64)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:43)
	at kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)
	at kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:35)
	at java.lang.Thread.run(Thread.java:662)
[2012-01-05 21:57:38,773] ERROR Error processing ProduceRequest on test:0 (kafka.server.KafkaApis:82)
java.nio.channels.ClosedChannelException
	at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:184)
	at kafka.message.ByteBufferMessageSet.writeTo(ByteBufferMessageSet.scala:75)
	at kafka.message.FileMessageSet.append(FileMessageSet.scala:161)
	at kafka.log.Log.append(Log.scala:215)
	at kafka.server.KafkaApis.kafka$server$KafkaApis$$handleProducerRequest(KafkaApis.scala:71)
	at kafka.server.KafkaApis.handleProducerRequest(KafkaApis.scala:55)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:40)
	at kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)
	at kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:35)
	at java.lang.Thread.run(Thread.java:662)
[2012-01-05 21:57:38,775] FATAL Halting due to unrecoverable I/O error while handling producer request: null (kafka.server.KafkaApis:92)
java.nio.channels.ClosedChannelException
	at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:184)
	at kafka.message.ByteBufferMessageSet.writeTo(ByteBufferMessageSet.scala:75)
	at kafka.message.FileMessageSet.append(FileMessageSet.scala:161)
	at kafka.log.Log.append(Log.scala:215)
	at kafka.server.KafkaApis.kafka$server$KafkaApis$$handleProducerRequest(KafkaApis.scala:71)
	at kafka.server.KafkaApis.handleProducerRequest(KafkaApis.scala:55)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:40)
	at kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)
	at kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:35)
	at java.lang.Thread.run(Thread.java:662)
[2012-01-05 21:57:38,775] FATAL Halting due to unrecoverable I/O error while handling producer request: null (kafka.server.KafkaApis:92)
java.nio.channels.ClosedChannelException
	at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:184)
	at kafka.message.ByteBufferMessageSet.writeTo(ByteBufferMessageSet.scala:75)
	at kafka.message.FileMessageSet.append(FileMessageSet.scala:161)
	at kafka.log.Log.append(Log.scala:215)
	at kafka.server.KafkaApis.kafka$server$KafkaApis$$handleProducerRequest(KafkaApis.scala:71)
	at kafka.server.KafkaApis$$anonfun$handleMultiProducerRequest$1.apply(KafkaApis.scala:64)
	at kafka.server.KafkaApis$$anonfun$handleMultiProducerRequest$1.apply(KafkaApis.scala:64)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
	at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
	at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34)
	at kafka.server.KafkaApis.handleMultiProducerRequest(KafkaApis.scala:64)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:43)
	at kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)
	at kafka.server.KafkaServer$$anonfun$startup$2.apply(KafkaServer.scala:70)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:35)
	at java.lang.Thread.run(Thread.java:662)
[info] Test Starting: testProduceAndFetch(kafka.integration.LazyInitProducerTest)
","10/Jan/12 07:18;jkreps;Fixed random jvm halts in tests. Removed stray file in patch.","10/Jan/12 07:59;jkreps;Improved version of patch. Fixes a bug where a response for a closed socket can throw an uncaught exception.","10/Jan/12 19:07;nehanarkhede;I tried to apply the v5 patch and couldn't find RequestChannel.scala in there. Would you mind uploading a patch with that included ?

Also, if the kafka cluster is experiencing either network or IO bottleneck, it slows processing down and backs up the producer queue causing QueueFullException. To detect this, seems like it will be helpful to expose the number of queued requests in the SocketServerStats ? This could either be a separate JIRA or part of this one, your call.","10/Jan/12 21:38;jkreps;Missed some files in the last patch, checked that this one actually builds correctly on a clean checkout.","12/Jan/12 01:06;junrao;unit tests now pass. +1 on the patch.

We should probably commit this patch to an 0.8 branch.","13/Jan/12 00:03;nehanarkhede;Created the 0.8 branch and committed this on the branch. ","13/Jan/12 20:49;jkreps;Hey Neha, 0.8 doesn't build for me and is missing a few files in the patch. Did those get missed in the checkin?

[info] == core-kafka / compile ==
[info]   Source analysis: 134 new/modified, 0 indirectly invalidated, 0 removed.
[info] Compiling main sources...
[error] /Users/jkreps/work/kafka-git/core/src/main/scala/kafka/network/SocketServer.scala:45: not found: type RequestChannel
[error]   val requestChannel = new RequestChannel(numProcessorThreads, maxQueuedRequests)
[error]                            ^
[error] /Users/jkreps/work/kafka-git/core/src/main/scala/kafka/network/SocketServer.scala:190: not found: type RequestChannel
[error]                                val requestChannel: RequestChannel,
[error]                                                    ^
[error] /Users/jkreps/work/kafka-git/core/src/main/scala/kafka/network/SocketServer.scala:301: not found: value RequestChannel
[error]       val req = RequestChannel.Request(processor = id, requestKey = key, request = request, start = time.nanoseconds)
[error]                 ^
[error] /Users/jkreps/work/kafka-git/core/src/main/scala/kafka/server/KafkaServer.scala:26: RequestChannel is not a member of kafka.network
[error] import kafka.network.{SocketServerStats, SocketServer, RequestChannel}
[error]        ^
[error] /Users/jkreps/work/kafka-git/core/src/main/scala/kafka/server/KafkaServer.scala:40: not found: type KafkaRequestHandlerPool
[error]   var requestHandlerPool: KafkaRequestHandlerPool = null
[error]                           ^
[error] /Users/jkreps/work/kafka-git/core/src/main/scala/kafka/server/KafkaServer.scala:69: not found: type KafkaRequestHandlerPool
[error]     requestHandlerPool = new KafkaRequestHandlerPool(socketServer.requestChannel, new KafkaApis(logManager).handle, config.numIoThreads)
[error]                              ^
[error] 6 errors found
[info] == core-kafka / compile ==
[error] Error running compile: Compilation failed
[info] 
[info] Total time: 21 s, completed Jan 13, 2012 12:46:07 PM
[info] 
[info] Total session time: 24 s, completed Jan 13, 2012 12:46:07 PM
[error] Error during build","13/Jan/12 21:16;nehanarkhede;My bad. Forgot to do the svn add on those. Fixed that, should build fine now. ","13/Jan/12 22:41;jkreps;Thanks Neha! Working great now!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a tool to jump JMX data to a csv file to help build out performance tests,KAFKA-166,12528393,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,22/Oct/11 20:58,24/Oct/11 18:52,12/Jan/21 10:06,24/Oct/11 18:52,0.8.0,,,,,,,,,,core,,,,,,0,,,,,"In order to get sane performance stats we need to be able to integrate the values we keep in JMX. To enable this it would be nice to have a generic tool that dumped JMX stats to a csv file. We could use this against the producer, consumer, and broker to collect kafka metrics while the tests were running.

",,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/11 18:19;jkreps;KAFKA-166-v2.patch;https://issues.apache.org/jira/secure/attachment/12500509/KAFKA-166-v2.patch","22/Oct/11 21:00;jkreps;KAFKA-166.patch;https://issues.apache.org/jira/secure/attachment/12500335/KAFKA-166.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-10-24 05:05:21.467,,,false,,,,,,,,,,,,,,,,,,214253,,,Mon Oct 24 18:48:52 UTC 2011,,,,,,,"0|i09mcv:",54055,,,,,,,,,,,,,,,,"22/Oct/11 21:04;jkreps;Added draft patch.

jkreps-mn:kafka-trunk jkreps$ bin/kafka-run-class.sh kafka.tools.JmxTool --help
Option                                  Description                            
------                                  -----------                            
--help                                  Print usage information.               
--jmx-url <service-url>                 The url to connect to to poll JMX      
                                          data. See http://http://download.    
                                          oracle.                              
                                          com/javase/7/docs/api/javax/management/remote/JMXServiceURL.
                                          html for details. (default: service: 
                                          jmx:rmi:///jndi/rmi://:9999/jmxrmi)  
--object-name <name>                    A JMX object name to query. If no      
                                          objects are specified all objects    
                                          will be queried.                     
--reporting-interval <Integer: ms>      Interval in MS with which to poll jmx  
                                          stats. (default: 5000) 

Example:
jkreps-mn:kafka-trunk jkreps$ bin/kafka-run-class.sh kafka.tools.JmxTool --object-name 'kafka:*' --reporting-interval 1000
""time"", ""kafka:type=kafka.KafkaLog4j:priority"", ""kafka:type=kafka.SocketServerStats:NumFetchRequests"", ""kafka:type=kafka.SocketServerStats:NumProduceRequests"", ""kafka:type=kafka.SocketServerStats:FetchRequestsPerSecond"", ""kafka:type=kafka.SocketServerStats:TotalBytesRead"", ""kafka:type=kafka.KafkaLog4j:appender=stdout"", ""kafka:type=kafka.SocketServerStats:BytesWrittenPerSecond"", ""kafka:type=kafka.SocketServerStats:AvgProduceRequestMs"", ""kafka:type=kafka.KafkaLog4j:name"", ""kafka:type=kafka.SocketServerStats:MaxFetchRequestMs"", ""kafka:type=kafka.SocketServerStats:BytesReadPerSecond"", ""kafka:type=kafka.SocketServerStats:TotalProduceRequestMs"", ""kafka:type=kafka.SocketServerStats:ProduceRequestsPerSecond"", ""kafka:type=kafka.SocketServerStats:MaxProduceRequestMs"", ""kafka:type=kafka.SocketServerStats:TotalFetchRequestMs"", ""kafka:type=kafka.SocketServerStats:TotalBytesWritten"", ""kafka:type=kafka.SocketServerStats:AvgFetchRequestMs""
1319317539516, INFO, 0, 0, -0.0, 0, log4j:appender=stdout, 0.0, 0.0, root, 0.0, 0.0, 0, -0.0, 0.0, 0, 0, 0.0
1319317540523, INFO, 0, 0, -0.0, 0, log4j:appender=stdout, 0.0, 0.0, root, 0.0, 0.0, 0, -0.0, 0.0, 0, 0, 0.0
1319317541523, INFO, 0, 0, -0.0, 0, log4j:appender=stdout, 0.0, 0.0, root, 0.0, 0.0, 0, -0.0, 0.0, 0, 0, 0.0
1319317542522, INFO, 0, 0, -0.0, 0, log4j:appender=stdout, 0.0, 0.0, root, 0.0, 0.0, 0, -0.0, 0.0, 0, 0, 0.0
1319317543521, INFO, 0, 0, -0.0, 0, log4j:appender=stdout, 0.0, 0.0, root, 0.0, 0.0, 0, -0.0, 0.0, 0, 0, 0.0
...","24/Oct/11 05:05;junrao;1. If object-name is not specified, should we include all jmx beans?
2. Maybe we should order the output by bean names.
3. The description of jmx-url has double http: in it. ","24/Oct/11 13:20;cburroughs;Coda Hale's metrics package has built in support for export:

https://github.com/codahale/metrics/commit/fce41d75046129a72e3582ae24ac698812d0fe53

Since we will inevitable want more than just average throughput (such as percentiles for latency to judge jitter).  We might want to consider switching to that.","24/Oct/11 17:16;jkreps;Yeah, I am not opposed to moving to that metrics package if it doesn't bring in a bunch of dependencies.

I think that is a separate issue, though. Right now I am just trying to support scripting up performance tests using our existing jmx metrics. The goal is to be able to dump out perf statistics in CSV for some post analysis and graphs. I wrote a strawman wiki on the goal here: 
  https://cwiki.apache.org/confluence/display/KAFKA/Performance+testing

The goal is be able to do nightly performance and integration runs. Neha had done the majority of the work so far, and I was just helping to gather more stats and do the R graphs.","24/Oct/11 18:19;jkreps;Updated version of the patch. Allows date formatting for the time stamp.

Jun--I implemented your idea to sort the columns. I am leaving time as the first column always, though, for readability. I also fixed the doc string for --jmx-url. The default was always to query all jmx beans, though the formatting for some of the more complex java.lang beans is a little crazy.","24/Oct/11 18:24;nehanarkhede;Minor things -

Is it HH:mm:ss:SSS or HH:mm:ss.SSS ?
Also, can we take the TODO off since the date formatting is in ?

+1. Good to see this patch.","24/Oct/11 18:48;jkreps;Yeah, I can change the default to use a dot, I think that is prolly more standard. Good point about the TODO.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade python producer to the new message format version,KAFKA-162,12527669,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,18/Oct/11 22:17,19/Oct/11 22:15,12/Jan/21 10:06,19/Oct/11 22:15,0.7,,,,,,,0.7,,,clients,,,,,18/Oct/11 00:00,0,,,,,The python producer is still on the old message format version (pre-compression). This can cause some issues when integrating with consumers that don't support this. It would be good to bump the python producer up to the current version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/11 22:18;jkreps;KAFKA-162.patch;https://issues.apache.org/jira/secure/attachment/12499606/KAFKA-162.patch",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-10-18 22:56:24.317,,,false,,,,,,,,,,,,,,,,,,88929,,,Tue Oct 18 22:56:24 UTC 2011,,,,,,,"0|i15z8n:",243009,,,,,,,,,,,,,,,,"18/Oct/11 22:56;nehanarkhede;+1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce the compression feature in Kafka,KAFKA-79,12516851,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,02/Aug/11 00:17,06/Oct/11 21:18,12/Jan/21 10:06,06/Oct/11 17:57,0.6,,,,,,,0.7,,,,,,,,,0,,,,,"With this feature, we can enable end-to-end block compression in Kafka. The idea is to enable compression on the producer for some or all topics, write the data in compressed format on the server and make the consumers compression aware. The data will be decompressed only on the consumer side. Ideally, there should be a choice of compression codecs to be used by the producer. That means a change to the message header as well as the network byte format. On the consumer side, the state maintenance behavior of the zookeeper consumer changes. For compressed data, the consumed offset will be advanced one compressed message at a time. For uncompressed data, consumed offset will be advanced one message at a time. ",,cscotta,sharadag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-08-12 14:51:49.41,,,false,,,,,,,,,,,,,,,,,,41583,,,Thu Oct 06 21:18:04 UTC 2011,,,,,,,"0|i15ytz:",242943,,,,,,,,,,,,,,,,"12/Aug/11 14:51;cburroughs;- Have you done any performance comparisons?
- Do you think you could produce a diff (link to github is fine) that shows all of the compression changes? 
- My reading of CompressionCodec is that getCompressionCodec would have to be edited to add a new codec (so it would not be user plugable).  Do you think thats something we should support (and in that case I guess a separate ticket)","12/Aug/11 16:37;jkreps;We have some performance comparisons, we should include that information on the performance page at least by the time this is released. Of course our primary concern is interdatacenter bandwidth rather than performance per se. We see a ~30% compression ratio on our Avro tracking data.

Neha should be able to give a diff. I think it was the last checkin on github before the cutover.

It is important that decompression always happen with the codec used for compression, so it can't just be the case that there is some property compression.codec=org.apache.kafka.GzipCompressor in the config because a mismatch on producer and consumer would lead to unreadable data, and if two people send messages with different codecs you would be totally screwed. This means the codec used must be maintained with the message set. We do this by having a compression id where 0=none, 1=gzip, etc. This doesn't lend itself to extensability since that list has to be predetermined, but we could reserve a codec id for ""user defined"" codec and leave it up to the user to configure it right.

My intuition is that most people just want a good compression implementation included out of the box and don't want to fiddle with it so i think it would be best to get that right. I think even in the long run there are really only 2-3 algorithms that have a reasonable cpu/size compression/decompression tradeoff so it makes sense to just implement and fully test those for perf and correctness and include those in a way that can't break.","15/Sep/11 16:12;nehanarkhede;The github diff URL is here - https://github.com/nehanarkhede/kafka/compare/8db50143d7362750225e...4c49c4e2a9490255c35e21f89a6f4545de87cc5a

This diff url will capture the major changes, though some more minor fixes went in the Apache SVN repo after this. 

I started writing a wiki page for the compression feature here - https://cwiki.apache.org/confluence/display/KAFKA/Compression

Comments and suggestions are welcome.","30/Sep/11 17:26;cscotta;This looks like an excellent feature, Neha - thanks for working on it. We push a lot of highly compressible data into Kafka. Trading a bit of CPU for reduced disk and network activity sounds excellent.

Would you be willing to accept a patch that implements support for http://code.google.com/p/snappy in addition to (or instead of) GZip? When consuming high-data-rate streams, we quickly peg the core on GZip decoding and have switched to Snappy (specifically, this implementation: http://code.google.com/p/snappy-java/) as a result.

If you have a chance, take a quick look at this JVM de/compressor throughput comparison: https://github.com/ning/jvm-compressor-benchmark/wiki -- these results mirror ours pretty closely. On a 36GB dataset of serialized data, we see an 89% compression ratio out of Snappy and 95% out of GZip. At least in our case, the slightly lower compression ratio still left us with a huge win in terms of codec throughput (and reducing the CPU burden on consuming / producing applications).

– Scott","02/Oct/11 15:15;jkreps;Scott, we would gladly take that feature! The compression is pluggable so adding that should not be too hard, we just need another compression id for snappy (gzip=1, so snappy=2). We should still keep gzip, I think it has a place, for example in our usage our biggest bottleneck is inter-datacenter bandwidth, so we will probably stay with gzip I think, plus it has no native depenencies so it is a little better ""out-of-the-box"" experience.

One thing we should think through is how this is packaged. Typically we make these things contrib/ packages to avoid adding to the main dependencies. It doesn't appear that the snappy jar has any external dependencies though and it looks like it packages up the .so/.dll files for all platforms in its jar, which is nice. I think it might be better just to add it in the main code, and ensure that none of the snappy classes are loaded unless the user selects snappy as the compression type (this will make the snappy jar optional for people--you only need it if you use it).

Any other thoughts from people?
","04/Oct/11 00:38;cburroughs;- I think we should have a clear convention for ids. For example: core < 10000, contrib < 20000, HERE-BE-DRAGONS > 20000. 
- I think there is room for gzip, and something else in the  LZF/Snappy area in the default kafka install.
- I'm mildly uncomfortable with native code dependencies, but the Hadoop guys seem to have gotten something working.","04/Oct/11 04:43;junrao;Chris, which ids are you referring to?","04/Oct/11 12:59;cburroughs;What Jay called ""compression ids"" (ie 1==gzip).","04/Oct/11 17:51;nehanarkhede;Scott,

Thanks for pointing us to Snappy. I took a brief look at the benchmarks for Snappy, and it does look promising to me. As Jay mentioned, GZIP buys us increased throughput and better utilization of the network bandwidth, due to relatively high compression ratio. Though, its decompression cost, in terms of both TPS and CPU usage is not very low. According to preliminary Kafka compression performance benchmarks, with fetch size of 1MB, the consumer throughput doubled, while consuming a GZIP compressed topic. When the consumer is fully caught up, the CPU usage is ~45%, as compared to ~12% when the same consumer is consuming uncompressed data. On the producer side, for a batch size of 200, message size of 200, the producer throughput for generating compressed data is 1/2 the throughput when producing uncompressed data. That is the cost of compression for GZIP. Though this is tolerable for inter-DC replication, we could do better for more real-time applications that care about TPS more than the compression ratio. I see Snappy fitting well here (http://ning.github.com/jvm-compressor-benchmark/results/canterbury-roundtrip-2011-07-28/index.html).

The compression ratio that we see (for a producer batch size of 200) is 3x for GZIP on our typical tracking data set. I wonder how low this will be for Snappy. It will be good to check. 

It will be great to see a Snappy integration patch with some Kafka performance benchmarks that measure compression/decompression overhead, compression ratio, effect on producer/consumer throughput. 

- Neha","06/Oct/11 21:18;cscotta;Jay and Neha,

Fantastic - thanks for taking a look. Your GZip choice makes sense - for inter-DC transport, totally agree with pushing the CPU a little harder to drive down the wire size. Most of our current use is within the same rack, so we've been optimizing for codec throughput. That said, I agree and like the idea of pluggable compressors.

I'll clone, apply Neha's patch, and drop that in this weekend. I don't expect that it will be too complicated (it's just an InputStream; our local use is just a couple lines).

(Apologies for the slow reply; I had mistakenly disabled e-mail notifications).

– Scott",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose total metrics through MBeans as well,KAFKA-140,12525145,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,pyritschard,pyritschard,29/Sep/11 07:02,30/Sep/11 19:37,12/Jan/21 10:06,30/Sep/11 19:37,,,,,,,,0.7,,,core,,,,,,0,,,,,"Mathias Herberts suggested on the ML that Kafka get the following additional metrics exposed through a bean

totalflushms
totalbytesread
totalbyteswritten
totalfetchrequestms
totalproducerequestms",,herberts,pyritschard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/11 06:57;pyritschard;0001-Patch-for-KAFKA-140-Exposing-Total-Metrics.patch;https://issues.apache.org/jira/secure/attachment/12497119/0001-Patch-for-KAFKA-140-Exposing-Total-Metrics.patch","29/Sep/11 07:08;pyritschard;0002-Patch-for-KAFKA-140-Exposing-Total-Metrics.patch;https://issues.apache.org/jira/secure/attachment/12496978/0002-Patch-for-KAFKA-140-Exposing-Total-Metrics.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-09-29 15:50:31.382,,,false,,,,,,,,,,,,,,,,,,39351,,,Fri Sep 30 19:37:20 UTC 2011,,,,,,,"0|i15z4f:",242990,,,,,,,,,,,,,,,,"29/Sep/11 07:06;pyritschard;The attached files fixes the issue","29/Sep/11 07:08;pyritschard;fix for kafka-140","29/Sep/11 15:50;junrao;Pierre-Yves, thanks for creating the jira and submitting the patch.

I wonder if SnapshotStats.getTotalMetric really gives what you want. This method currently gives you the aggregated metric for the latest 30-second window. My understanding is that you want to have an aggregated metric since the server is started. Is that right? If so, what you need to do is to create a new Stats in SnapshotStats that keeps tracks of the aggregated metric for the whole history.","29/Sep/11 15:56;pyritschard;I indeed got that wrong, will update the patch.","30/Sep/11 06:57;pyritschard;Updated patch","30/Sep/11 19:37;junrao;Thanks Pierre-Yves, I just committed this patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce retention setting that depends on space,KAFKA-70,12514707,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,,19/Jul/11 21:32,21/Sep/11 16:44,12/Jan/21 10:06,17/Sep/11 03:23,0.7,,,,,,,0.7,,,,,,,,,0,log,roll,,,"Currently there is this setting: 

log.retention.hours 

introduce: 

log.retention.size 

Semantic would be, either size is reached or time is reached, oldest messages will be deleted. 

This does not break back-ward compatibility and would make the system robust under scenarios where message size is not deterministic over time.",,prashanth.menon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/11 01:00;prashanth.menon;KAFKA-70.patch;https://issues.apache.org/jira/secure/attachment/12493706/KAFKA-70.patch","13/Sep/11 02:18;prashanth.menon;KAFKA-70_v2.patch;https://issues.apache.org/jira/secure/attachment/12494165/KAFKA-70_v2.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-09-09 00:58:05.182,,,false,,,,,,,,,,,,,,,,,,59903,,,Wed Sep 21 16:07:25 UTC 2011,,,,,,,"0|i15ysv:",242938,,,,,,,,,,,,,,,,"07/Sep/11 15:29;junrao;Prashanth,

Not sure if you still have those questions in the deleted comment. In any case, for your first question, the size limit can be checked every time we roll the log. For the second question, the default value can be no size limit to keep it consistent with the old behavior.","09/Sep/11 00:58;prashanth.menon;Hi Jun,

Nope, answered them myself (had a brain-blank moment), hence why I deleted it.  In the mean time, I've attached a patch I think should enable this feature.  I originally did this on GitHub with a pull request but was redirected here :)  

Let me know what you think or if this isn't quite what was expected.  

- Prashanth","09/Sep/11 20:44;jjkoshy;Prashanth,

Thanks a lot for the patch. cleanUpSegmentsToMaintainSize will walk through the segments in whatever order they were loaded. That is done in loadSegments which (due to java.io.File.listFiles) does not guarantee any particular order. We need to do a numeric sort of the segments and then start deleting from the segment with the smallest offset.

Joel
","10/Sep/11 21:49;prashanth.menon;Hi Joel,

Looking through Log.loadSegments it looks like the segments (if any were found) are already sorted in order of ascending offset.  This effectively solves the problem, no?

- Prashanth","11/Sep/11 17:00;jkreps;I agree the delete logic is correct. Log.scala keeps all segments in sorted order, and enforces the behavior that segments can only be deleted in reverse order (it uses takeWhile on the list) so this should guarantee that segments are deleted from oldest to newest. When we take the patch we should add a comments in our existing code to the effect that the list is guaranteed to be in sorted order, since that is kind of implicit now but it is important.

Prashanth, one corner case here is when the segment size < log.retention.size. One thing to note is that if the currently active segment is marked for deletion, even if that segment is not full, it will dutifully be deleted and a new, empty segment created. The cleanup based on modified time always guarantees that the last message happened AT LEAST log.retention.hours ago because it uses the modification time of the file. This means the client always has at least that much time to pick up their messages. This gets weird, though, when we delete the active segment based on size. It looks to me that if I have a segment size of 100MB but a log.retention.size of 50MB, then when my single-segment log gets to 50MB we will delete the active segment, effectively all messages in the log, even though likely it contains as-yet undelivered messages which may have arrived only ms ago. I think this will be confusing for people. Two ways to fix it: one is to require that log.retention.size > log.file.size, the other is to change the delete logic so that it deletes all but the most recent segment. Another way to say this is that since our granularity of cleanup is log.file.size we have to either overshoot or undershoot their given retention size by as much as log.file.size-1. It is probably safer to overshoot rather than undershoot what they give. Another alternative would be to phrase the retention configuration in terms of the number of files rather than the size in bytes which would be more exact, but I think the way you have done it is more intuitive.

Another feature request: would it be possible to add a per-topic config similar to topic.log.retention.hours (say topic.log.retention.size) to allow configuring the log size at the topic level? If not I think we can take a patch that fixes the above issue and just open a bug for that.","12/Sep/11 01:25;prashanth.menon;Haha, this is quite uncanny.  I had the same discussion regarding log.file.size and log.retention.size after I made the patch and concluded (I believe incorrectly) that the retention size should always be larger than the segment size.  I see both sides of the argument but I'm not really in a position to make a decision (due to me being rather new and lacking the knowledge of the breadth of use cases it's being used under).  From my point of view having the retention size be larger than the log file size is the natural solution; implementing logic to trim the log to log.retention.size but leaving the active segment undeleted is confusing since it doesn't really live by the rule its namesake configuration suggests.  As a user, I will still see the log file greater than the retention size and be confused (perhaps this can be solved with a simple rename of the configuration?).  Just my two cents.  I'd agree with you though, dealing with bytes rather than files is a more intuitive way.

In any case, let me know which solution you would like to go with.  As it is right now, I made the retention size > log size rule implicit, but I can add it into the code and throw an error or warning.

I'd definitely like to work on the per-topic retention feature request, it'll give me a better chance to dig through the code :)  My preference would be to create a new bug for it (assign it to me?).

Cheers,
Prashanth  ","12/Sep/11 04:35;junrao;Prashanth, thanks for the patch.

I suggest that we enforce retention size > log size and disallow the broker to start if the rule is violated.","12/Sep/11 11:44;prashanth.menon;Okay, expect a patch with the added rule enforcement later today.","12/Sep/11 14:49;jkreps;Actually, now that I think about it, not sure if just limiting the config to the size of one segment is enough. Intuitively here is what I think we want the config to mean: ""Keep the least amount of data possible to guarantee a buffer for the consumer of at least N bytes"". If we don't give that guarantee of that form there is no way to reason about the SLA for the consumer to pick up their data. For example if retention_size is 50*1024*1024+10 and segment size is 5050*1024*1024, then the log may actually be trimmed as small as 10 bytes.

So Jun, I think the config approach doesn't work. I think we need to tweak the delete rule slightly.","12/Sep/11 15:32;junrao;One possibility is to have the following semantic: delete segment files as much as possible with at least retention_size amount of data remaining (instead of bringing the total data size to below the retention_size).","12/Sep/11 17:22;jkreps;Jun, yeah, exactly, I think that is what I was trying to say but said more clearly.","13/Sep/11 02:18;prashanth.menon;Hi all, I've attached an updated patch for your consideration.  It now deletes segments leaving at least logRetentionSize space in the log.","16/Sep/11 01:03;prashanth.menon;Hi guys, had a chance to look at the newest patch?","16/Sep/11 01:53;nehanarkhede;+1. Thanks for the patch !","17/Sep/11 03:22;jkreps;Applied, thanks! I will open a JIRA to add per-topic overrides for the retention size.","21/Sep/11 13:18;cburroughs;Trunk == 0.7 not 0.8, right?","21/Sep/11 16:07;nehanarkhede;You are right Chris. Trunk is on 0.7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
trait Serializer should be broken down to WriterSerializer and ReadSerializer,KAFKA-69,12514706,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,12/Jan/21 10:06,19/Jul/11 21:32,,,,,,,,,,,,,,,,,0,,,,,"Create 2 extra traits: ReadSerializer and WriterSerializer and have Serializer be empty traits depending on both. 

This is a backward compatible change and would make the api much more flexible, here is an example: 

KafkaAppender should only take WriteSerializer which makes using KafkaAppender much simpler.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,67460,,,2011-07-19 21:32:27.0,,,,,,,"0|i15ysn:",242937,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
This is a test to see if the kafka-dev mailing list gets this notification,KAFKA-68,12514705,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,12/Jan/21 10:06,19/Jul/11 21:32,,,,,,,,,,,,,,,,,0,,,,,Hi guys!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,67462,,,2011-07-19 21:32:27.0,,,,,,,"0|i15ysf:",242936,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speed up recovery,KAFKA-56,12514693,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,12/Jan/21 10:06,19/Jul/11 21:32,,,,,,,,,,,,,,,,,0,,,,,"Currently we run recovery on the last segment of each log. This ensures that the log is correct, but requires reading the complete segment which can be slow. 

One approach would be to record graceful shutdown using some .no_recovery_needed file, and on the next startup remove this file and open logs without running recovery. This will speed up the common case of a graceful recovery. 

There may be some approach that writes known-valid offsets to a checkpoint file and allows recovery to proceed from there.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,67475,,,2011-07-19 21:32:24.0,,,,,,,"0|i15yq7:",242926,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New producer API,KAFKA-36,12514673,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,12/Jan/21 10:06,19/Jul/11 21:32,,,,,,,,0.6,,,,,,,,,0,,,,,"We need to introduce a new Producer API that wraps the 2 low-level producer APIs - kafka.producer.SyncProducer and kafka.producer.async.AsyncProducer. The goal is to expose all the producer functionalities through a single API to the client. The new producer should be able to - 

1. handle queueing/buffering of multiple producer requests and asynchronous dispatch of the batched data 
2. handle the serialization of data through a user-specified Encoder 
3. provide zookeeper based automatic broker discovery 
4. provide software load balancing through an optionally user-specified Partitioner",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,67483,,,2011-07-19 21:32:18.0,,,,,,,"0|i15ynr:",242915,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow to provide producer ID,KAFKA-10523,13329240,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,luigiberrettini,,24/Sep/20 15:43,30/Dec/20 18:30,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,"I read about the implementation of idempotence and saw that it is only guaranteed within a producer session, since it depends on a PID reassigned every time the producer (re)start.

The PID is probably assigne relying on ZooKeeper, but I was wondering if it could be possible to support providing a PID externally to gain idempotence across restrarts e.g. having the producing application read the PID from a configuration file.",,chia7712,luigiberrettini,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 25 08:30:09 UTC 2020,,,,,,,"0|z0iw0o:",9223372036854775807,,,,,,,,,,,,,,,,"25/Sep/20 08:30;luigiberrettini;I saw that an ID can be provided for transactions: supposing I need to publish only on one topic would idempotence be guaranteed across producer restarts calling producer.initTransactions() and the performing the send out of the transaction (i.e. without calling begin or commit).

Essentially I would use producer.initTransactions() just to acquire the same PID so that I have idempotency across restarts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Kafka connect converter for AVRO,KAFKA-10715,13340374,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,blacktooth,blacktooth,12/Nov/20 23:02,07/Dec/20 20:38,12/Jan/21 10:06,,,,,,,,,,,,KafkaConnect,,,,,,0,,,,,"I want to add support for Avro data format converter to Kafka Connect. Right now, Kafka connect supports [JSON converter|[https://github.com/apache/kafka/tree/trunk/connect].] Since, Avro is a commonly used data format with Kafka, it will be great to have support for it. 

 

Confluent Schema Registry libraries have [support|https://github.com/confluentinc/schema-registry/blob/master/avro-converter/src/main/java/io/confluent/connect/avro/AvroConverter.java] for it. The code seems to be pretty generic and can be used directly with Kafka connect without schema registry. They are also licensed under Apache 2.0.

 

Can they be copied to this repository and made available for all users of Kafka Connect?",,blacktooth,ChrisEgerton,ivanyu,ravindhranath,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 20:12:13 UTC 2020,,,,,,,"0|z0kjq0:",9223372036854775807,,,,,,,,,,,,,,,,"07/Dec/20 20:12;blacktooth;Can I contribute this change?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support AdminClient Example,KAFKA-7038,13165357,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,darion,darion,11/Jun/18 16:02,04/Nov/20 07:13,12/Jan/21 10:06,,,,,,,,,,,,admin,,,,,,0,,,,,,,darion,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-06-12 15:05:25.468,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 12 15:05:25 UTC 2018,,,,,,,"0|i3uqg7:",9223372036854775807,,,,,,,,,,,,,,,,"12/Jun/18 15:05;githubbot;darionyaphet opened a new pull request #5200: [KAFKA-7038] Support AdminClient Example
URL: https://github.com/apache/kafka/pull/5200
 
 
   Add AdminClient Example 
   include `describeCluster` , `createTopics` , `listTopics` , `describeTopics` and `deleteTopics` . 
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-517: Add consumer metrics to observe user poll behavior,KAFKA-8874,13254883,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,lu.kevin,lu.kevin,lu.kevin,05/Sep/19 01:03,16/Oct/20 22:57,12/Jan/21 10:06,17/Oct/19 09:17,,,,,,,,2.4.0,,,consumer,metrics,,,,,0,,,,,"[https://cwiki.apache.org/confluence/display/KAFKA/KIP-517%3A+Add+consumer+metrics+to+observe+user+poll+behavior]

It would be beneficial to add a metric to record the average/max time between calls to poll as it can be used by both Kafka application owners and operators to:
 * Easily identify if/when max.poll.interval.ms needs to be changed (and to what value)
 * View trends/patterns
 * Verify max.poll.interval.ms was hit using the max metric when debugging consumption issues (if logs are not available)
 * Configure alerts to notify when average/max time is too close to max.poll.interval.ms",,githubbot,lu.kevin,omkreddy,tombentley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-09-26 05:46:47.968,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 17 09:17:43 UTC 2019,,,,,,,"0|z06c28:",9223372036854775807,,,,,,,,,,,,,,,,"26/Sep/19 05:46;githubbot;KevinLiLu commented on pull request #7395: [WIP] KAFKA-8874: Add consumer metrics to observe user poll behavior
URL: https://github.com/apache/kafka/pull/7395
 
 
   **NOT READY TO MERGE**
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","05/Oct/19 00:40;lu.kevin;[https://github.com/apache/kafka/pull/7395]","17/Oct/19 09:17;githubbot;omkreddy commented on pull request #7395: KAFKA-8874: Add consumer metrics to observe user poll behavior (KIP-517)
URL: https://github.com/apache/kafka/pull/7395
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","17/Oct/19 09:17;omkreddy;Issue resolved by pull request 7395
[https://github.com/apache/kafka/pull/7395]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Calendar based windows,KAFKA-10408,13323037,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,astubbs,astubbs,17/Aug/20 11:22,19/Aug/20 23:07,12/Jan/21 10:06,,2.6.0,,,,,,,,,,streams,,,,,,0,needs-kip,,,,"A date based window, for example aggregate all payments made until each month date of the 15th, or all payments made each year until April 1st.

Should handle time zones ""properly"", e.g. allow user to specify which time zone to base it on.

Also should support setting a time cut off, and just simply ""midnight"" in the given zone. (.e.g 6pm April 15th). 

Ideally will also support day offsets, e.g. last day of every month, first Tuesday of each week, last Friday of the month.

Example implementation of a specific aggregator, with a window implementation implicitly embedded:

[https://github.com/astubbs/ks-tributary/blob/denormalisation-base-cp-libs/streams-module/src/main/java/io/confluent/ps/streams/processors/YearlyAggregator.java]

 ",,ableegoldman,astubbs,cadonna,mjsax,vvcephei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-08-17 14:43:40.329,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 19 23:07:42 UTC 2020,,,,,,,"0|z0htsw:",9223372036854775807,,,,,,,,,,,,,,,,"17/Aug/20 14:43;vvcephei;Thanks for the report [~astubbs] . Note: this feature is mildly incompatible with the current definition of TimeWindows, but it would be fixed in [https://github.com/apache/kafka/pull/9031] . The PR is a PoC for KIP-645.","19/Aug/20 23:07;mjsax;Another example implementation is this one: [https://github.com/confluentinc/kafka-streams-examples/blob/5.5.0-post/src/test/java/io/confluent/examples/streams/window/DailyTimeWindows.java]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disabling JmxReporter registration ,KAFKA-10360,13320961,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,romain.quinio,romain.quinio,04/Aug/20 20:37,05/Aug/20 01:19,12/Jan/21 10:06,,,,,,,,,,,,clients,,,,,,0,,,,,"In Kafka client applications, JMX usage is often being replaced in favor of frameworks like micrometer or microprofile-metrics.

It would be nice to be able to disable the JmxReporter that is today built-in with KafkaProducer/KafkaConsumer/KafkaStreams

[https://github.com/apache/kafka/blob/783a6451f5f8c50dbe151caf5e76b74917690364/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java#L355-L357]

[https://github.com/apache/kafka/blob/ffdec02e25bb3be52ee5c06fe76d388303f6ea43/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L869-L871]

[https://github.com/apache/kafka/blob/42f46abb34a2b29993b1a8e6333a400a00227e30/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java#L685-L687]

Example of issue in Quarkus: https://github.com/quarkusio/quarkus/issues/9799",,aweise,chia7712,romain.quinio,Steiner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-08-05 01:19:18.222,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 05 01:19:18 UTC 2020,,,,,,,"0|z0hh34:",9223372036854775807,,,,,,,,,,,,,,,,"05/Aug/20 01:19;Steiner;I have ever see the same warn log,  provide a config key to disable JMXRepoter is more reasonable

and i'd like to work on this if we want the changes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Consumer Group Management with Regex,KAFKA-7817,13209343,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,alex.dunayevsky,alex.dunayevsky,alex.dunayevsky,14/Jan/19 09:04,25/Jul/20 09:36,12/Jan/21 10:06,,2.1.0,,,,,,,,,,tools,,,,,,0,,,,,"//TODO:

New feature: Provide ConsumerGroupCommand with ability to query/manage multiple consumer groups using a single regex pattern. 

 ",,alex.dunayevsky,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-05-08 07:55:39.126,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 08 07:55:39 UTC 2019,,,,,,,"0|u00sxk:",9223372036854775807,,,,,,,,,,,,,,,,"08/May/19 07:55;githubbot;rootex- commented on pull request #6700: KAFKA-7817 ConsumerGroupCommand Regex Feature
URL: https://github.com/apache/kafka/pull/6700
 
 
   *KAFKA-7817 ConsumerGroupCommand Regex Feature
   Description: *Add ability to select a subset of consumer groups using regex for operations: --describe, --delete and --reset-offsets*
   JIRA: [Multiple Consumer Group Management with Regex](https://issues.apache.org/jira/browse/KAFKA-7817)
   Discussion 1. [Multiple Consumer Group Management](https://www.mail-archive.com/dev@kafka.apache.org/msg93781.html)
   Discussion 2. [Re: ConsumerGroupCommand tool improvement?](https://www.mail-archive.com/dev@kafka.apache.org/msg90561.html)
   
   *Unit tests implemented*
   
   ### Committer Checklist (excluded from commit message)
   - [v] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MM2 to provide configuration parameter to specify NIC/IP address to use for produce/consume requests,KAFKA-10178,13311980,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,sudarshan@confluent.io,sudarshan@confluent.io,17/Jun/20 15:11,17/Jun/20 15:14,12/Jan/21 10:06,,,,,,,,,,,,mirrormaker,,,,,,0,,,,,"Mirror maker when deployed in dual homed machines set up across network boundary requires routes to be set up in the host to allow MM process to fetch data from source cluster in one network and deliver it to destination cluster in another network.

Allowing MM configs to specify the NIC and/or IP address to used to source the produce and consume requests will simplify operations.

Dual homed configuration is preferred for policy/security/change management reasons",,sudarshan@confluent.io,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-17 15:11:34.0,,,,,,,"0|z0fxxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connect connectors api doesn't show versions of connectors,KAFKA-6942,13161786,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Invalid,,astubbs,astubbs,24/May/18 14:34,11/Jun/20 04:55,12/Jan/21 10:06,11/Jun/20 04:55,1.1.0,,,,,,,,,,KafkaConnect,,,,,,0,needs-kip,,,,Would be very useful to have the connector list API response also return the version of the installed connectors.,,astubbs,cricket007,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-06-18 21:26:47.747,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 04:55:56 UTC 2020,,,,,,,"0|i3u4gf:",9223372036854775807,,,,,,,,,,,,,,,,"18/Jun/18 21:26;cricket007;Doesn't this already exist?

GET /connector-plugins
{code}
[
  {
    ""class"": ""io.confluent.connect.replicator.ReplicatorSourceConnector"",
    ""type"": ""source"",
    ""version"": ""4.0.1""
  },
  {
    ""class"": ""org.apache.kafka.connect.file.FileStreamSinkConnector"",
    ""type"": ""sink"",
    ""version"": ""1.0.0-cp1""
  },
  {
    ""class"": ""org.apache.kafka.connect.file.FileStreamSourceConnector"",
    ""type"": ""source"",
    ""version"": ""1.0.0-cp1""
  }
]
{code}","11/Jun/20 04:55;rhauch;I'm going to close this as INVALID because the versions are available in the API, as noted above.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add KStream#repartition operation,KAFKA-8611,13242129,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,lkokhreidze,lkokhreidze,lkokhreidze,28/Jun/19 09:40,08/Jun/20 22:54,12/Jan/21 10:06,09/Apr/20 23:53,,,,,,,,2.6.0,,,streams,,,,,,0,kip,,,,"When using DSL in Kafka Streams, data re-partition happens only when key-changing operation is followed by stateful operation. On the other hand, in DSL, stateful computation can happen using _transform()_ operation as well. Problem with this approach is that, even if any upstream operation was key-changing before calling _transform()_, no auto-repartition is triggered. If repartitioning is required, a call to _through(String)_ should be performed before _transform()_. With the current implementation, burden of managing and creating the topic falls on user and introduces extra complexity of managing Kafka Streams application.

KIP-221: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-221%3A+Enhance+DSL+with+Connecting+Topic+Creation+and+Repartition+Hint]",,ableegoldman,githubbot,lkokhreidze,mjsax,roxton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6182,KAFKA-6037,KAFKA-7608,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-06-28 16:55:29.225,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 01:18:07 UTC 2020,,,,,,,"0|z046zk:",9223372036854775807,,,,,,,,,,,,,,,,"28/Jun/19 16:55;mjsax;Thanks the ticket. Seems to overlap with https://issues.apache.org/jira/browse/KAFKA-6037 and https://issues.apache.org/jira/browse/KAFKA-4835 (ie, [https://cwiki.apache.org/confluence/display/KAFKA/KIP-221%3A+Enhance+KStream+with+Connecting+Topic+Creation+and+Repartition+Hint])

I think we should consolidate the work. Nodody is actively working on KIP-221, thus I think you could take over all if it. \cc [~bchen225242], would this be ok with you?","28/Jun/19 17:20;lkokhreidze;Happy to work on that as well, can be really useful feature.

In terms of organization, what would you suggest work on them separately or unite all three issues (KAFKA-8611, KAFKA-6037, KAFKA-4835) under one umbrella KIP?","28/Jun/19 17:32;mjsax;Might be easiest to do all at once with one umbrella KIP. KIP-221 has broader scope, so I might be easiest to discard KIP-485, and embed it in KIP-221 – if fact, the idea to allow removing the name requirement was already considered in the discussion of KIP-221.","28/Jun/19 17:35;mjsax;[~lkokhreidze], I added you to the list of contributors and assigned this ticket to you. You can now self-assign tickets. too. Feel free to reassign the other two tickets to yourself, and add this ticket to the KIP-221 wiki page. Thanks for contributing to Kafka!","28/Jun/19 17:37;lkokhreidze;Great!

Thank you, will do,","06/Aug/19 18:20;githubbot;lkokhreidze commented on pull request #7170: KAFKA-8611 / Add KStream#repartition operation
URL: https://github.com/apache/kafka/pull/7170
 
 
   # KIP-221: Enhance DSL with Connecting Topic Creation and Repartition Hint
   
   ## Description
   This is first PR for [KIP-221](https://cwiki.apache.org/confluence/display/KAFKA/KIP-221%3A+Enhance+DSL+with+Connecting+Topic+Creation+and+Repartition+Hint). Goal of this PR is to introduce new `KStream#repartition` operator that can be used to for triggering repartitioning on `KStream` instance.
   
   ## Notable Changes
   - Introduced `org.apache.kafka.streams.processor.internals.InternalTopicProperties` class that can be used for capturing repartition topic configurations passed via DSL operations
   - Enhanced `OptimizableRepartitionNode` with `StreamPartitioner<K,V> partitioner` and `InternalTopicProperties internalTopicProperties` fields
   - Added `org.apache.kafka.streams.processor.internals.InternalTopologyBuilder#internalTopicNamesWithProperties` map for storing mapping between internal topics and their corresponding configuration. If configuration is present `RepartitionTopicConfig` is enriched with configurations passed via DSL operations (In this case via `org.apache.kafka.streams.kstream.Repartitioned` class).
   - Added `KStreamRepartitionIntegrationTest` for testing different scenarios of `KStream#repartition`
   - - Repartition topic shouldn't be created when key changing operation wasn't performed and `Repartitioned#numberOfPartitions` was not specified
   - - Repartition topic should be created when key changing operation was performed
   - - Repartition topic should be created when key changing operation wasn't performed, but `Repartitioned#numberOfPartitions` was specified
   - - Repartition topic name should be picked up from `Repartitioned#name` configuration
   - - Repartition topic name should be generated automatically if `Repartitioned#name` is not specified.
   - - `KStream#repartition(KeyValueMapper, Repartitioned)` works as expected with `KStream#groupByKey` (There should be only one repartition topic).
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","12/Apr/20 09:40;githubbot;lkokhreidze commented on pull request #8470: KAFKA-8611 / Refactor KStreamRepartitionIntegrationTest
URL: https://github.com/apache/kafka/pull/8470
 
 
   Follow-up PR on https://github.com/apache/kafka/pull/7170 that implements @mjsax 's [suggestions](https://github.com/apache/kafka/pull/7170#discussion_r406535443) regarding integration test parameters.
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","15/Apr/20 01:18;githubbot;mjsax commented on pull request #8470: KAFKA-8611 / Refactor KStreamRepartitionIntegrationTest
URL: https://github.com/apache/kafka/pull/8470
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include rolledup segment size stats via jmx,KAFKA-106,12519017,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,utkarshcmu,cburroughs,cburroughs,16/Aug/11 16:24,27/May/20 20:54,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,"We already have size for a each topic-partition pair.  From the user list it looks like it would be helpful to also include entire size for a topic, and size for all topics.",,junrao,mjsax,utkarshcmu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-25 05:07:45.85,,,false,,,,,,,,,,,,,,,,,,64894,,,Wed May 27 20:54:56 UTC 2020,,,,,,,"0|i02a27:",11231,,,,,,,,,,,,,,,,"25/Sep/15 05:07;utkarshcmu;[~junrao] - I would like to submit a PR for this. Can you please assign this to me?

Also, I have written KafkaWriter for JMXTrans:
https://github.com/jmxtrans/jmxtrans/tree/master/jmxtrans-output/jmxtrans-output-kafka","25/Sep/15 16:17;junrao;[~utkarshcmu], just added you to the contributor list and you should be able to assign the jira to yourself.","27/May/20 20:54;mjsax;Is this old ticket still valid? Can/should we close it?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Redo config objects with default arguments,KAFKA-72,12514709,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Abandoned,,,,19/Jul/11 21:32,27/May/20 20:51,12/Jan/21 10:06,19/Jul/11 21:32,,,,,,,,,,,,,,,,,0,,,,,It would be nice to experiment with redoing the config objects we use for the various clients using scala 2.8 default arguments in addition to the java.util.Properties arguments.,,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,67458,,,Wed May 27 20:51:39 UTC 2020,,,,,,,"0|i02a3r:",11238,,,,,,,,,,,,,,,,"27/May/20 20:51;mjsax;Closing this old ticket as abandoned.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Time based log rolling,KAFKA-40,12514677,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Implemented,cburroughs,,,19/Jul/11 21:32,27/May/20 20:50,12/Jan/21 10:06,27/May/20 20:50,,,,,,,,,,,,,,,,,0,,,,,"In some cases we know that consumers are interested in data on time boundaries (for example, hourly), and when a new consumer of the type is spun up it wants to consume data since the last boundary (start at 12 noon). 

OffsetRequest can do this now, but Log:getOffsetsBefore is ""very approximate"" and it would be nice for the consumers to not have to iterate over unneeded data that (being older) is less likely to be in the page cache. 

Proposal: Optional argument to roll log file if it contains more than n seconds of data. I think this is reasonable, but wanted to create a ticket for comments in-case I've missed a reason this infeasible or otherwise a bad idea.",,cburroughs,mjsax,sharadag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,67393,,,Wed May 27 20:50:44 UTC 2020,,,,,,,"0|i02a3j:",11237,,,,,,,,,,,,,,,,"27/May/20 20:50;mjsax;It seem the time-index as introduced via KIP-33 ([https://cwiki.apache.org/confluence/display/KAFKA/KIP-33+-+Add+a+time+based+log+index]) addressed this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Branch one Stream in multiple Streams,KAFKA-7445,13187829,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Not A Problem,,kollgandren,kollgandren,27/Sep/18 08:31,15/May/20 03:01,12/Jan/21 10:06,15/May/20 03:01,,,,,,,,,,,streams,,,,,,0,,,,,"Hi,

I need to branch/split KStreams in multiple independent KStreams. I thought, {{org.apache.kafka.streams.kstream.internals.KStreamImpl#branch}} is the right one but in fact, its designed for another purpose.
In contrast to {{branch}} I need to assign the record to *all* matching streams, not only one stream.

Speaking in code ({{org.apache.kafka.streams.kstream.internals.KStreamBranch}}):
{code:java}
if (predicates[i].test(key, value)) {
   // use forward with childIndex here
   // and pipe the record to multiple streams
   context().forward(key, value, i);
}
{code}

My question: is this still possible with features already included in Streams? Or shall I propose a change?

Thanks in advance
Dennis",,astein,guozhang,kollgandren,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-09-29 19:34:32.82,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 01 12:52:18 UTC 2018,,,,,,,"0|i3yk3r:",9223372036854775807,,,,,,,,,,,,,,,,"29/Sep/18 19:34;mjsax;I guess, you can write the following code to get what you need:
{noformat}
KStream stream = ...
KStream stream1 = stream.filter(...);
KStream stream2 = stream.filter(...);
KStream stream3 = stream.filter(...);{noformat}
This code applies each `Predicate` to all records. Reusing the same `KStream` variable (ie, `stream`) multiple times is a logical broadcast, ie, each record will be passed into each `filter()`.

It's up to you if you think it would be helpful to have a single operator for this. The question is, how this operator could be named?","30/Sep/18 17:32;guozhang;[~kollgandren] It's by-design of `branch` to only allow a record to be ""branched"" to at most one of the downstream operators, if you want to let a record to be sent to more than one of the downstream operators, I think using [~mjsax]'s suggestion is a better choice.","01/Oct/18 12:52;kollgandren;[~mjsax]: Thanks for the hint! I will give it a try.

[~guozhang]: I didn't want to change the branching logic, just using it as an example.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable --if-exists and --if-not-exists for AdminClient in TopicCommand,KAFKA-9862,13298151,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,d8tltanc,d8tltanc,d8tltanc,13/Apr/20 22:34,14/Apr/20 18:47,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,"In *kafka-topic.sh*, we expect to use --if-exists to ensure that the topic to create or change exists. Similarly, we expect to use --if-not-exists to ensure that the topic to create or change does not exist. Currently, only *ZookeeperTopicService* supports these two options and we want to introduce them to *AdminClientTopicService.*",,d8tltanc,sliebau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-04-14 07:20:05.259,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 18:47:20 UTC 2020,,,,,,,"0|z0dlds:",9223372036854775807,,,,,,,,,,,,,,,,"14/Apr/20 07:20;sliebau;Hi [~d8tltanc], that sounds useful, thanks! 

Just one comment, is this really a critical piece of work? Personally I think we should rather call this ""minor"".","14/Apr/20 18:47;d8tltanc;Thanks. I put critical because it blocks the --zookeeper flag removal. Some system tests are testing this functionality with Zookeeper. Just changed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-519: Make SSL context/engine configuration extensible,KAFKA-8890,13255749,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,maulin.vasavada,maulin.vasavada,09/Sep/19 22:32,08/Apr/20 14:23,12/Jan/21 10:06,08/Apr/20 14:23,,,,,,,,2.6.0,,,security,,,,,,0,,,,,This is to track changes for KIP-519: Make SSL context/engine configuration extensible ([https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=128650952]),,fml2,githubbot,maulin.vasavada,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-24 06:59:56.165,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 14:20:37 UTC 2020,,,,,,,"0|z06heo:",9223372036854775807,,rajinisivaram@gmail.com,,,,,,,,,,,,,,"27/Jan/20 07:08;maulin.vasavada;Hi all

 

I've a suggested code changes done for my local fork at: [https://github.com/maulin-vasavada/kafka/pull/4.] 

 

Will raise the official PR once the KIP moves in Accepted. 

 

Thanks

Maulin","24/Mar/20 06:59;githubbot;maulin-vasavada commented on pull request #8338: KAFKA-8890: KIP-519- Make SSL context/engine configuration extensible
URL: https://github.com/apache/kafka/pull/8338
 
 
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","24/Mar/20 07:01;maulin.vasavada;Raised pull request to apache kafka trunk: [https://github.com/apache/kafka/pull/8338]. Please review.","03/Apr/20 06:20;maulin.vasavada;Hi [~rajinisivaram@gmail.com] Can you please review this?","08/Apr/20 14:20;githubbot;rajinisivaram commented on pull request #8338: KAFKA-8890: KIP-519- Make SSL context/engine configuration extensible
URL: https://github.com/apache/kafka/pull/8338
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consider addition of leader quorum in replication model,KAFKA-9733,13292514,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,Yohan123,Yohan123,18/Mar/20 15:41,22/Mar/20 16:10,12/Jan/21 10:06,,,,,,,,,,,,clients,core,,,,,0,,,,,"Kafka's current replication model (with its single leader and several followers) is somewhat similar to the current consensus algorithms being used in databases (RAFT) with the major difference being the existence of the ISR. Consequently, Kafka suffers from the same fault tolerance issues as does other distributed systems which rely on RAFT: the leader tends to be the chokepoint for failures i.e. if it goes down, it will have a brief stop-the-world effect. 

In contrast, giving all replicas the power to write and read to other replicas is also difficult to accomplish (as emphasized by the complexity of the Egalitarian Paxos algorithm), since consistency is so hard to maintain in such an algorithm, plus very little gain compared to the overhead. 

Therefore, I propose that we have an intermediate plan in between these two algorithms, and that is the leader replica quorum. In essence, there will be multiple leaders (which have the power for both read and writes), but the number of leaders will not be excessive (i.e. maybe three at max). How we achieve consistency is simple:
 * Any leader has the power to propose a write update to other replicas. But before passing a write update to a follower, the other leaders must elect if such an operation is granted.
 * In principle, a leader will propose a write update to the other leaders, and once the other leaders have integrated that write update into their version of the stored data, they will also give the green light. 
 * If say, more than half the other leaders have agreed that the current change is good to go, then we can forward the change downstream to the other replicas.

 The algorithm for maintaining consistency between multiple leaders will still have to be worked out in detail. However, there would be multiple gains from this design over the old model:
 # The single leader failure bottleneck has been alleviated to a certain extent, since there are now multiple leader replicas.
 # Write updates will potentially no longer be bottlenecked at one single leader (since there are multiple leaders available). On a related note, there has been a KIP that allows clients to read from non-leader replicas. (will add the KIP link soon).

Some might note that the overhead from maintaining consistency among multiple leaders might offset these gains. That might be true, with a large number of leaders, but with a small number then (capped at 3 as mentioned above), the overhead will also be correspondingly small. (How latency will be affected is unknown until further testing, but more than likely, this option will probably be. configurable depending on user requirements).

 

 ",,Yohan123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 22 16:10:42 UTC 2020,,,,,,,"0|z0co1s:",9223372036854775807,,,,,,,,,,,,,,,,"22/Mar/20 16:10;Yohan123;[~bchen225242] Do you know how much use this has for Kafka? The design and implementation for this issue would no doubt be horrendously complex, so whats your take? ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add connector level configurability for producer/consumer client configs,KAFKA-6890,13158309,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Duplicate,,Natengall,Natengall,09/May/18 22:13,21/Mar/20 23:38,12/Jan/21 10:06,17/Oct/19 02:27,,,,,,,,,,,KafkaConnect,,,,,,0,,,,,"Right now, each source connector and sink connector inherit their client configurations from the worker properties. Within the worker properties, all configurations that have a prefix of ""producer."" or ""consumer."" are applied to all source connectors and sink connectors respectively.

We should also provide connector-level overrides whereby connector properties that are prefixed with ""producer."" and ""consumer."" are used to feed into the producer and consumer clients embedded within source and sink connectors respectively. The prefixes will be removed via a String#substring() call, and the remainder of the connector property key will be used as the client configuration key. The value is fed directly to the client as the configuration value.",,cricket007,githubbot,jsnipes,klafferty,Natengall,petermelias,rhauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-8265,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-10 15:42:46.402,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 21 23:38:56 UTC 2020,,,,,,,"0|i3tjhb:",9223372036854775807,,,,,,,,,,,,,,,,"10/May/18 15:42;githubbot;natengall opened a new pull request #4997: KAFKA-6890: Add connector-level configurability for client configs
URL: https://github.com/apache/kafka/pull/4997
 
 
   Introduced capability for individual connectors to override client config defaults.
   
   Connector properties that are prefixed with ""producer."" and ""consumer."" are now used to feed into the producer and consumer clients embedded within source and sink connectors respectively.
   
   Author: Allen Tang <natengall@gmail.com>
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","25/Oct/18 17:04;cricket007;[~rhauch], nice to meet you at Kafka Summit, 
When we spoke, I had mentioned that we currently have one large connect cluster, primarily for S3 Connect, in each of our development environments, and for some of the 100+ connector configurations that we have loaded, the topics they are sinking have variable amounts of throughput, so I think it would be beneficial to be able to tune at least some of the properties for those APIs. 

I know you brought up the concerns about at least being able to set ""consumer.bootstrap.servers"", for example, so perhaps setting ""bootstrap.servers"" (and other, similar properties, such as SSL certs for those servers) could be blacklisted in some way?

Also, as I mentioned on GitHub, this looks like it might duplicate KAFKA-4159 ","26/Oct/18 14:41;Natengall;I personally believe in exposing as much capability out of Kafka Connect as possible and enabling users to tinker with whatever they choose to, but they should exercise caution and be aware of the impact in full in doing so. As such, I'd hate to see blacklisting of any client configs, especially ""consumer.bootstrap.servers"" and ""producer.bootstrap.servers""; I've been using connector-level overrides for these two configs in our prod environment to great effect to allow a single cluster of Kafka Connect to write/read data across multiple Kafka clusters.","01/Nov/18 18:30;rhauch;First of all, we're going to need a KIP for this, since it will change the configurations exposed to users. We need to make sure that this doesn't break backward compatibility, etc.

Second, I like this change and the ability to override some of the producer/consumer behavior. I also think that generally it's better to give users more control, but I'm actually very concerned about allowing connector configurations to override {{bootstrap servers}}. Yes, I can totally see several valid use cases and benefits, but this also gives users that don't know what they're doing to really mess things up. Some people will undoubtably think they *have* to define these properties in every connector, and then when they change their bootstrap URL in the worker they might forget it in their connector configs. Or, worse yet, having the internal topics in one cluster and the connector records in a different cluster will very much conflict with any hopes of adding EOS to source connectors.","03/Nov/18 03:49;petermelias;I found this issue after reading through the source to determine if this was possible and imagined a very similar patch as is proposed here.

We have a variety of use cases for this capability. They include throughput tuning, variations in max message sizes, and overriding partitioner implementations.

I took a look at the current implementation and the first thing that jumped out was that in order to support this, the broker connection model changes from a single I/O thread per worker to needing a dedicated producer/consumer instance for each overridden connector.

While I don't think this is necessarily a bad tradeoff for the config flexibility, it does mean more I/O threads and more network connections and while a small deployment with few connectors may not notice a difference, a larger deployment choosing to run many overridden connectors may need to be warned of the possible increase in overhead.


In regards to the possible duplication [~cricket007] has mentioned for KAFKA-4159, I found [this line|https://github.com/apache/kafka/pull/2548/files#diff-316d2c222b623ee65e8065863bf4b9ceR368] on the proposed implementation to be less elegant than [~Natengall]'s approach as it causes state to accumulate on the `producerProps` map which could cause nondeterministic application of prior configuration to connectors not intended to receive said configuration. Isolating the specific overrides and dedicating the particular producer or consumer instance to the connector strikes me as the safest way to do it as proposed here.

I am also in favor of blacklisting critical configuration values such as `bootstrap.servers` from being overridden. If that strikes people as too heavy-handed than perhaps a loud warning; after all, the worst case outcome is that the particular connector will fail. Any other connectors will simply continue to use the worker-level configured values assuming we continue with the approach of isolating the overrides to the source/sink task.","06/Nov/18 17:32;Natengall;I really appreciate your input, Randall! 

The KIP to accompany this JIRA is here: https://cwiki.apache.org/confluence/display/KAFKA/KIP-296%3A+Connector+level+configurability+for+client+configs
To respond to your concerns, I should also add that defining bootstrap.servers at the connector level and then subsequently changing the bootstrap.servers configuration at the worker level is actually something that we have performed in the past as it was the most graceful solution to what we were trying to achieve. We've had an occasion where we were tasked with provisioning an entirely new Kafka cluster, migrating all topics from the old cluster to the new cluster, and reconfiguring all producers and consumers that were interfacing with the old cluster to instead point to the new cluster -- Kafka Connect connectors included.

In the absence of a bootstrap.servers overriding capability on a connector-by-connector basis, we would only have the worker bootstrap.servers to go by, which meant we would have to face a highly coordinated inter-departmental effort with over forty connectors within the Kafka Connect cluster to account for, along with all of their downstream business-facing real-time implications after flipping the cluster-wide switch. 

By providing configurability of bootstrap.servers at the connector-level, connectors became decoupled from one another, and they were no longer required to read/write data where the internal topics live, allowing for customers to migrate their connectors over to the new Kafka cluster at their own pace and on their own schedule. Eventually, the three internal topics used for Kafka Connect were mirrored over to the new Kafka cluster and the cluster-wide boostrap.servers configuration change was applied, thereby fully decoupling Kafka Connect from the older Kafka cluster.

If blacklisting overrides for specific client configs, like bootstrap.servers, is something you feel strongly about, we may be able to achieve this via whitelisting of client configs, defined by administrators of Kafka Connect clusters, at the cluster-level within worker properties. Let me know your thoughts on this.","07/Nov/18 15:50;rhauch;[~Natengall], apologies for not associating KIP-296 with this issue. Hopefully the link to the KIP here will help people from not making that same mistake. :-)

I do understand the use cases for allowing all client configs to be overridden in connectors, but for me it's a matter of usability. In particular, how easy is it to allow users to get into trouble by doing something they don't understand? Do users think they *have* to provide all of the `producer.*` properties in every connector? Ideally the KIP would address these concerns as well as those mentioned above.

The KIP is a good start, but there are a few things that it should address. Perhaps the KIP should explain the potential risks of this change (usability, complexity), and address why the benefits outweigh Connect breaking its current abstraction where connectors don't know about Kafka clients. One way to do that is to go into more detail about _multiple_ use cases and to explain what is required today to achieve them and how this proposal enables/simplifies each use case. Consider including the following use cases, though there may be more:
* security: the connector uses a specific principal (different from what's in the worker config)
* custom partitioners: allow a source connector to use a specific partitioner
* tuning: the connector tunes some of the consumer or producer configs for buffers, max bytes, max poll times, etc.
* multi-cluster: the connector reads/writes to a different broker cluster than where the connect internal information is stored (including source connector offsets)

I think the KIP should also address:
# How does someone creating a connector configuration know which `consumer.` or `producer.` properties to specify in a connector config versus which don't need to be defined and inherited instead? Perhaps I missed it, but the KIP should explicitly specify that the `consumer.` and `producer.` properties in the connector configurations _override_ the `consumer.` and `producer.` properties in the worker configuration. 
# What happens to the `producer.` and `consumer.` properties in a connector configuration? In particular I'm concerned that the connector see and use them inappropriately. It's one thing for Connect's abstraction to be broken at the configuration level, but it's something altogether different to break this abstraction such that the connector implementations start using their own producer and consumer clients. If we were to pass them through to the connector, then any connector that already defines `producer.` or `consumer.` properties might be negatively affected by this change (which would need to be addressed in the compatibility section).
# You've mentioned the difference between a Connect operator team and the people that are deploying connectors. I think it is useful for the Connect worker configs to explicitly allow writing to different clusters, so that by default the Connect cluster enforces using only the same cluster. Or can we instead allow the worker to define which ""passthrough modes"" are allowed, such as a combination of ""security"", ""broker"", ""tuning"", etc.?  Or should this be a whitelist regex for all properties that can be overridden? Which approach is more usable?
# How does this change affect the actual worker? Does this change the number of producers and consumers that are created for each connector task? (I think the answer is no; this simply changes the properties used to create those clients.) This needs to be outlined in the KIP to allay any concerns about changing behaviors.
# ""Connector properties that are prefixed with `producer.` and `consumer.` are now used to feed into the producer and consumer clients embedded within source and sink connectors respectively. The prefixes will be removed via a String::substring() invocation, and the remainder of the configuration key will be used as the client configuration key."" We should never use ""substring"" to do this kind of operation. The ConnectorConfig and WorkerConfigs all extend AbstractConfig, which provides an {{originalsWithPrefix(String prefix)}} method that does this, but importantly it internally uses a RecordingMap to record which properties end up being used for later reporting. Look at the [existing code|https://github.com/apache/kafka/blob/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java#L143] for how the `producer.` properties are extracted from the worker config.
# Is it possible to define `consumer.` properties in a source connector, or `producer.` properties in a sink connector? IMO, Connect should prevent these, but it might be tricky to implement this check correctly such that the validation of the connector config fails in the normal way. Also, is Connect responsible for validating the properties prefixed with `producer.` and `consumer.`?

If this KIP is going to allow a connector to override all configurations, then the rejected alternatives section should describe why only allowing a connector to override a subset of properties was rejected. And, a few of the items I mentioned above have multiple options, and anything approach not used should be described in the rejected alternatives section.

Keep up the good work.","17/Oct/19 02:27;rhauch;This has been implemented via [KIP-458|https://cwiki.apache.org/confluence/display/KAFKA/KIP-458%3A+Connector+Client+Config+Override+Policy] and added into AK 2.3.0.

Therefore, marking this as a duplicate (even though this issue is older and was used as the origin for KAFKA-8265).","21/Mar/20 23:38;githubbot;kkonstantine commented on pull request #4997: KAFKA-6890: Add connector-level configurability for client configs
URL: https://github.com/apache/kafka/pull/4997
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-486: Support custom way to load KeyStore and TrustStore,KAFKA-8621,13242665,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,thomas930410,maulin.vasavada,maulin.vasavada,02/Jul/19 07:25,05/Mar/20 01:15,12/Jan/21 10:06,,,,,,,,,,,,security,,,,,,0,,,,,https://cwiki.apache.org/confluence/display/KAFKA/KIP-486%3A+Support+custom+way+to+load+KeyStore+and+TrustStore,,fml2,Matthias Bechtold,maulin.vasavada,yujhe.li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-02 12:33:11.192,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 01:15:28 UTC 2020,,,,,,,"0|z04aao:",9223372036854775807,,,,,,,,,,,,,,,,"17/Jul/19 17:37;maulin.vasavada;[~gshapira_impala_35cc] Can you please review this?","02/Mar/20 12:33;fml2;I would very much welcome this change. Our application is deployed to the cloud as a docker image. Keystore and Truststore files are NOT part of the image. We provide the keystore and truststore (base64 encoded) as environment variables when the image is started. The env vars are then assigned (via spring) to variables in the program. As of now, we have (at startup) to write the values of these variables to files so that the Kafka infrastructure can read them in from there.

This is cumbersome. A much better solution for us would be a custom implementation of the proposed {{KeyStoreLoader}} interface which would load the keystore ""from memory"". I.e. no files at all would be needed.","05/Mar/20 00:08;maulin.vasavada;Thanks AI. We are doing the changes as part of KIP-519 since those are more generic than the changes suggested in KIP-486. However you should be able to achieve what you are looking for with that also. Hopefully soon our KIP-519 will get accepted. ","05/Mar/20 01:12;fml2;Thanks for letting me know. I've read the KIP-519 and must say that the proposal of KIP-486 would better fit my needs. It is less flexible than what is proposed in KIP-519, but it adresses exactly the problem we have.

Please make sure that KIP-519 makes simple things simple. I only need to let the engine consume the keystore and truststore from variables, not from files. If that will be possible (in a simple way!) with KIP-519 then it's fine. Otherwise I'd work more on the design.

I like this: ""Make simple things simple; make complex things possible"". The second part is in focus; don't forget the first part! :)","05/Mar/20 01:15;fml2;Just to have a link in Jira as well: KIP-519 is tracked as KAFKA-8890.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Allow use of RDMA for publishing, replicating, and consuming log entries",KAFKA-9640,13289320,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,stevebyan,stevebyan,03/Mar/20 19:20,03/Mar/20 19:26,12/Jan/21 10:06,,2.2.2,,,,,,,,,,clients,consumer,core,,,,0,pull-request-available,,,,"Work-in-progress. Experimental modification to Apache Kafka to use RDMA for publishing, replicating, and consuming log entries. Our evaluation of the resulting performance is ongoing, but we expect to see significant improvements in end-to-end latency, lower CPU utilization on the brokers, and greater scalability for consumers of a single topic partition.",RDMA network,githubbot,prudenko,stevebyan,xiaotao183,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-03 19:26:31.335,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 19:26:31 UTC 2020,,,,,,,"0|z0c4ls:",9223372036854775807,,,,,,,,,,,,,,,,"03/Mar/20 19:26;githubbot;stevebyan commented on pull request #8210: KAFKA-9640[WIP]: Allow use of RDMA for publishing, replicating, and consuming log entries
URL: https://github.com/apache/kafka/pull/8210
 
 
   Experimental modification to Apache Kafka to use RDMA for publishing, replicating, and consuming log entries. Our evaluation of the resulting performance is ongoing, but we expect to see significant improvements in end-to-end latency, lower CPU utilization on the brokers, and greater scalability for consumers of a single topic partition.
   
   This contribution is principally the work of Konstantin Taranov <ktaranov@ethz.ch>, formerly with Oracle Labs and currently at ETH Zurich.
   
   This is a work in progress, not ready to be merged.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add zookeeper.ssl.context.supplier.class config if/when adopting ZooKeeper 3.6,KAFKA-9469,13281210,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,rndgstn,rndgstn,rndgstn,23/Jan/20 14:33,10/Feb/20 16:21,12/Jan/21 10:06,,,,,,,,,,,,config,,,,,,0,,,,,"The ""zookeeper.ssl.context.supplier.class"" configuration doesn't actually exist in ZooKeeper 3.5.6.  The ZooKeeper admin guide documents it as being there, but it doesn't appear in the code.  This means we can't support it in KIP-515, and it has been removed from that KIP.
I checked the latest ZooKeeper 3.6 SNAPSHOT, and it has been added.  So this config could probably be added to Kafka via a new, small KIP if/when we upgrade to ZooKeeper 3.6 (which looks to be in release-candidate stage at the moment).",,rndgstn,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 10 16:21:28 UTC 2020,,,,,,,"0|z0aswg:",9223372036854775807,,,,,,,,,,,,,,,,"10/Feb/20 16:21;rndgstn;Just checked and ""zookeeper.ssl.context.supplier.class"" did not make it into ZooKeeper 3.5.7.  So we must still wait for v3.6.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add TimestampedSessionStore,KAFKA-8382,13234005,New Feature,In Progress,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,high.lee,mjsax,mjsax,17/May/19 16:52,30/Jan/20 23:22,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,kip,,,,"Follow up to KIP-258, to complete the KIP by adding TimestampedSessionStores.

[https://cwiki.apache.org/confluence/display/KAFKA/KIP-258%3A+Allow+to+Store+Record+Timestamps+in+RocksDB]",,high.lee,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-01-30 23:17:42.299,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 30 23:17:42 UTC 2020,,,,,,,"0|z02szs:",9223372036854775807,,,,,,,,,,,,,,,,"30/Jan/20 23:17;high.lee;[~mjsax]

Can I ask you to review it?! thank you!!

 

[https://github.com/apache/kafka/pull/8022]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EndToEndLatency: Add support for printing all latencies,KAFKA-9456,13280587,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,ffosilva,ffosilva,ffosilva,20/Jan/20 15:54,20/Jan/20 17:17,12/Jan/21 10:06,,,,,,,,,,,,tools,,,,,,0,pull-request-available,,,,"The EndToEndLatency tool already stores all the latencies but it only prints a brief report containing the mean latency and some percentiles. The main idea of this feature is to have a flag to enable printing all the latencies instead of a report.

Printing all the latencies is important if the goal of testing is to plot a chart (such a boxplot) for example, extract confidence intervals, wherever.",,ffosilva,githubbot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-01-20 16:06:02.888,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 20 16:06:02 UTC 2020,,,,,,,"0|z0ap28:",9223372036854775807,,,,,,,,,,,,,,,,"20/Jan/20 16:06;githubbot;ffosilva commented on pull request #7987: KAFKA-9456: Add support for printing all the latencies
URL: https://github.com/apache/kafka/pull/7987
 
 
   There's a new optional argument called 'latencies_csv'. The allowed values is ""0"" (disabled) or ""1"" (enabled).
   
   If enabled, after execution, it will print to stdout all the latencies stored instead of a brief report. I've also changed the partial latency prints during the test to 'stderr'.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka streams: add possibility to choose multiple output topics ,KAFKA-7578,13195498,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,tdanylchuk,tdanylchuk,31/Oct/18 21:30,25/Nov/19 00:46,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,needs-kip,user-experience,,,"KIP draft: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-422%3A+%5BStreams%5D+Support+multiple+topic+resolution+in+TopicNameExtractor]

There is an awesome feature which was added in 2.0 kafka stream - possibility to choose dynamically the output topic for topology, but in some cases it could be useful to chose several topics withing the same cluster.

Personally me - I met such case: I needed to route message based on its content and by routes configuration to several topics.

I've made a 'proposal' PR for this, unfortunately I couldn't find better way to implement this:

[https://github.com/apache/kafka/pull/5801]

If this approach is OK, and improvement could be done in future versions, please let me know and I'll finish PR code.",,mjsax,tdanylchuk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-4936,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-11-01 21:01:13.683,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 04 05:45:25 UTC 2019,,,,,,,"0|i3zv6n:",9223372036854775807,,,,,,,,,,,,,,,,"01/Nov/18 21:01;mjsax;Thanks for creating this ticket. Note, that this is a public API change, and thus requires a KIP: [https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals]

Let us know if you have any questions regarding the KIP process.","04/Jun/19 05:45;mjsax;[~tdanylchuk] Was wondering about this ticket. You started to write a KIP, but it's still in DRAFT status and you never started a DISCUSS thread on the dev list. Are you still interested in working on this KIP?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add gradle task for dependency listing,KAFKA-7356,13182001,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,alopresto,alopresto,29/Aug/18 21:26,19/Nov/19 05:39,12/Jan/21 10:06,19/Nov/19 05:38,,,,,,,,,,,build,packaging,,,,,0,,,,,"I needed to examine the dependency list to confirm/deny use of a specific dependency. Running {{gradle -q dependencies}} in the root directory only lists the {{rat}} dependencies. Adding a custom section to *build.gradle* allows for a complete listing of the dependencies from the command line. 

{code}
subprojects {
    task allDeps(type: DependencyReportTask) {}
}
{code}

To invoke: {{gradle allDeps}}",,alopresto,githubbot,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-08-29 21:34:26.917,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 19 05:39:12 UTC 2019,,,,,,,"0|i3xkhb:",9223372036854775807,,,,,,,,,,,,,,,,"29/Aug/18 21:32;alopresto;I do not have permissions to assign this task to myself, but I will post a PR shortly. ","29/Aug/18 21:34;githubbot;alopresto opened a new pull request #5589: KAFKA-7356 Added allDeps task to generate complete dependency report.
URL: https://github.com/apache/kafka/pull/5589
 
 
   This task allows a user to examine the dependency list to confirm/deny use of a specific dependency. Running `$ gradle -q dependencies` in the root directory only lists the `rat` dependencies. Adding a custom section to _build.gradle_ allows for a complete listing of the dependencies from the command line. 
   
   ```
   subprojects {
       task allDeps(type: DependencyReportTask) {}
   }
   ```
   
   To invoke: `$ gradle allDeps`
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","29/Aug/18 21:34;alopresto;GitHub PR 5589 addresses this Jira. ","19/Nov/19 05:38;omkreddy;Fixed in https://github.com/apache/kafka/pull/7694","19/Nov/19 05:39;githubbot;omkreddy commented on pull request #5589: KAFKA-7356 Added allDeps task to generate complete dependency report.
URL: https://github.com/apache/kafka/pull/5589
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consider introducing numberOfPartitions field to Grouped configuration class,KAFKA-9197,13268789,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,lkokhreidze,lkokhreidze,16/Nov/19 11:58,16/Nov/19 20:55,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,needs-kip,,,,"In the KIP-221 there was an idea of introducing number of partitions field to Grouped config class. During the discussion in the mailing list, couple of valid concerns were raised against this approach. 

Main argument against it was that, whenever user specifies number of partitions for internal, repartition topics, he/she really cares that those configurations will be applied. Case with group by is that, repartitioning will not happen, if key changing operation isn't performed. Therefore, number of partitions configuration specified by the user will never be applied. Alternatively, if user cares about manual repartitioning, one may do following in order to scale up/down sub topologies:

 
{code:java}
builder
  .stream(""topic"")
  .repartition((key, value) -> value.newKey(), Repartitioned.withNumberOfPartitions(5))       
  .groupByKey()       
  .count();
{code}
 

On the other hand, there were other valid arguments for adding numberOfPartitions field to Grouped config class. It was raised in the mailing list that, we should treat `numberOfPartitions` field as ""desired"" number of partitions specified by the user, so that _if repartitioning is required_, Kafka Streams must use value specified in there.

 

Idea of this ticket is to follow-up on this discussion and implement this feature if there's an actual need from the Kafka Streams users.

 ",,ableegoldman,lkokhreidze,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-11-16 11:58:01.0,,,,,,,"0|z08ozs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make Merger extend Aggregator,KAFKA-5648,13090225,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Won't Fix,cvaliente,cvaliente,cvaliente,26/Jul/17 15:07,16/Oct/19 14:19,12/Jan/21 10:06,16/Oct/19 14:19,0.11.0.0,,,,,,,,,,streams,,,,,,0,,,,,"Hi,

I suggest that Merger<K,V> should extend Aggregator<K,V,V>.
reason:
Both classes usually do very similar things. A merger takes two sessions and combines them, an aggregator takes an existing session and aggregates new values into it.
in some use cases it is actually the same thing, e.g.:
<null, log_event> -> .map() to <session_id,SingletonList<log_event>> -> .groupByKey().aggregate() to <session_id, List<log_event>>
In this case both merger and aggregator do the same thing: take two lists and combine them into one.
With the proposed change we could pass the Merger as both the merger and aggregator to the .aggregate() method and keep our business logic within one merger class.

Or in other words: The Merger is simply an Aggregator that happens to aggregate two objects of the same class
",,cvaliente,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-07-26 19:00:05.094,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 27 15:09:12 UTC 2017,,,,,,,"0|i3i1gf:",9223372036854775807,,,,,,,,,,,,,,,,"26/Jul/17 19:00;mjsax;Your observation is correct, that {{Merger}} and {{Aggregator}} are similar. You also stated correctly, that the types are different though, as the {{Merger}} merges two aggregates of same type, while the Aggregator in general merged a single value (of type-A) merges the value into an aggregate (of type-B). Thus, {{Merger<K,V> extends Aggregator<K,V,V}} is a hierarchy one could do. But the question is, what do you really gain? Also, if you want to share business logic, you can simply define one class that implements both interfaces at the same time, and do an private method that does the actual merging and call it from {{apply}}. ATM, I don't see the value and this would be an breaking change so we should really gain something out if it.","27/Jul/17 13:09;cvaliente;[~mjsax]
Thanks for your feedback!
What things would this change break? At the moment, any existing {{Merger<K,V>}} class would already automatically have implemented {{Aggregator<K,V,V>}}, since their method {{apply(K key, V aggOne, V aggTwo)}} also implements the Aggregator interfaces' {{apply}} method.

It's true that I can just create {{MyMerger implements Merger<K,V>, Aggregator<K,V,V>}} and use that (I can actually put my logic in the {{apply}} method and don't need a private one). But with the suggested change the relationship would probably be more explicit for the user and help them realize they might actually only need one class for both.
The way I see it the change would come at no additional cost or overhead","27/Jul/17 15:09;mjsax;The issue is, if one implements old {{Merger}} and does a hotswap of the Streams library without recompiling the code. The class ID will not match as the class hierarchy did change. The existing class would only implement {{Merger}} but not {{Merger extends Aggregator}}.

You are right, that if one recompiles the code, not code change is required to pass the build.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement Plain Ldap Authentication,KAFKA-5050,13062941,New Feature,Patch Available,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,in-park,nihed,nihed,10/Apr/17 16:24,08/Oct/19 09:53,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,"Implement LDAP based authentication for Plaintext authentication 
",,githubbot,nihed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-10-08 09:52:58.151,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 08 09:52:58 UTC 2019,,,,,,,"0|i3dfzb:",9223372036854775807,,,,,,,,,,,,,,,,"08/Oct/19 09:52;githubbot;in-park commented on pull request #7465: KAFKA-5050: SASL/PLAIN server callback handler which authenticates against LDAP
URL: https://github.com/apache/kafka/pull/7465
 
 
   This is an implementation of PlainServerCallbackHandler which allows Kafka clients to authenticate against LDAP through SASL/PLAIN. 
   Following is a sample Kafka client properties which will allow LDAP authentication through SASL/PLAIN:
   **sasl.mechanism=PLAIN
   security.protocol=SASL_SSL
   sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=""ldap_uid""  password=ldap_password"""";
   ssl.truststore.location=...... **
   
   The following is a sample KafkaServer jaas config: 
   org.apache.kafka.common.security.plain.PlainLoginModule required ldap_url=""ldaps://ldapserver:636""
   user_dn_template=""uid={0},cn=users,cn=accounts,dc=firm,dc=site"";
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema Inferencing for JsonConverter,KAFKA-6895,13158590,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,Natengall,Natengall,10/May/18 21:51,12/Sep/19 13:52,12/Jan/21 10:06,,,,,,,,,,,,KafkaConnect,,,,,,0,,,,,"Though there does exist a converter in the connect-json library called ""JsonConverter"", there are limitations as to the domain of JSON payloads this converter is compatible with on the Sink Connector side when serializing them into Kafka Connect datatypes; When reading byte arrays from Kafka, the JsonConverter expects its inputs to be a JSON envelope that contains the fields ""schema"" and ""payload"", otherwise it'll throw a DataException reporting:

 ??JsonConverter with schemas.enable requires ""schema"" and ""payload"" fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.??
 (when schemas.enable is true) or

 ??JSON value converted to Kafka Connect must be in envelope containing schema??
 (when schemas.enable is false)

 For example, if your JSON payload looks something on the order of:


{ ""c1"": 10000, ""c2"": ""bar"", ""create_ts"": 1501834166000, ""update_ts"": 1501834166000 }


This will not be compatible for Sink Connectors that require the schema for data ingest when mapping from Kafka Connect datatypes to, for example, JDBC datatypes. Rather, that data is expected to be structured like so:


{ ""schema"": \{ ""type"": ""struct"", ""fields"": [{ ""type"": ""int32"", ""optional"": true, ""field"": ""c1"" }, \{ ""type"": ""string"", ""optional"": true, ""field"": ""c2"" }, \{ ""type"": ""int64"", ""optional"": false, ""name"": ""org.apache.kafka.connect.data.Timestamp"", ""version"": 1, ""field"": ""create_ts"" }, \{ ""type"": ""int64"", ""optional"": false, ""name"": ""org.apache.kafka.connect.data.Timestamp"", ""version"": 1, ""field"": ""update_ts"" }], ""optional"": false, ""name"": ""foobar"" }, ""payload"": \{ ""c1"": 10000, ""c2"": ""bar"", ""create_ts"": 1501834166000, ""update_ts"": 1501834166000 } }


The ""schema"" is a necessary component in order to dictate to the JsonConverter how to map the payload's JSON datatypes to Kafka Connect datatypes on the consumer side.

Introduce a new configuration for the JsonConverter class called ""schemas.infer.enable"". When this flag is set to ""false"", the existing behavior is exhibited. When it's set to ""true"", infer the schema from the contents of the JSON record, and return that as part of the SchemaAndValue object for Sink Connectors.",,fdesing,githubbot,Natengall,oweiler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-10 22:35:31.287,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 12 13:52:07 UTC 2019,,,,,,,"0|i3tl73:",9223372036854775807,,,,,,,,,,,,,,,,"10/May/18 22:35;githubbot;natengall opened a new pull request #5001: KAFKA-6895: Schema Inferencing for JsonConverter
URL: https://github.com/apache/kafka/pull/5001
 
 
   Introduce a new configuration for the JsonConverter class called ""schemas.infer.enable"". When this flag is set to ""false"", the existing behavior is exhibited. When it's set to ""true"", infer the schema from the contents of the JSON record, and return that as part of the SchemaAndValue object for Sink Connectors.
   
   Author: Allen Tang <natengall@gmail.com>
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","12/Sep/19 13:52;oweiler;Hi, how is this supposed to work with timestamps? From my understanding, this information is impossible to deduce from the payload alone.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-512: Adding headers to RecordMetaData,KAFKA-8830,13252732,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,rmetukuru,rmetukuru,23/Aug/19 18:35,23/Aug/19 20:00,12/Jan/21 10:06,,,,,,,,,,,,clients,,,,,,1,,,,,[https://cwiki.apache.org/confluence/display/KAFKA/KIP-512%3AAdding+headers+to+RecordMetaData],,flashmouse,guilhermealcantara@gmail.com,rmetukuru,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-08-23 18:35:15.0,,,,,,,"0|z05zns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Timezone Support for Windowed Aggregations,KAFKA-7911,13214751,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,mjsax,mjsax,09/Feb/19 00:03,02/Aug/19 18:12,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,1,needs-kip,,,,"Currently, Kafka Streams only support UTC timestamps. The impact is, that `TimeWindows` are based on UTC time only. This is problematic for 24h windows, because windows are build aligned to UTC-days, but not your local time zone.

While it's possible to ""shift"" timestamps as a workaround, it would be better to allow native timezone support.",,dongjin,harshil_shah,hartmutcouk,hustclf,mjsax,speleomaniac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-02-10 08:44:35.656,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 02 18:12:08 UTC 2019,,,,,,,"0|yi0tao:",9223372036854775807,,,,,,,,,,,,,,,,"10/Feb/19 08:44;dongjin;[~mjsax] So... something like the following is needed. Right?

{{kStream}}
{{.windowedBy(TimeZone.getTimeZone(""America/Los_Angeles""), TimeWindows.of(Duration.ofDays(7)).advanceBy(Duration.ofSeconds(1)))}}

or,

{{kStream}}
{{.withTimeZone(TimeZone.getTimeZone(""America/Los_Angeles""))}}
{{.windowedBy(TimeWindows.of(Duration.ofDays(7)).advanceBy(Duration.ofSeconds(1)))}}

The first approach is easy to implement, but the second one has a little bit more clear syntax. Which one would be better?","10/Feb/19 09:00;mjsax;I did not think about this in detail yet. I guess there are some open design question. Setting the timezone in `windowedBy` would imply that one might need to specify it multiple times if there are multiple windowed operations.

For the second proposal, it seems you set it for a specific input stream. This might allow for some flexibility, however, I am wonder on the impact of a stream-stream join if both streams have different timezones set?

Also, could we assume that all input records use the same timestamp encoding or not? If yes, we could just use  config to set the timezone.

Looking at the it from 10,000ft, is seems that the timezone is a per-stream information. Thus, does it make sense to set it on an operator?

As pointed out, this Jira requires a KIP, thus, it would be good to think about those question (and maybe more), and write a concrete proposal (including rejected alternatives) so the community can discuss. I can imagine that this ticket does not describe all use cases and corner cases yet.","10/Feb/19 12:04;dongjin;Understood. Thank you roe your response. Let's continue discussing in the mailing thread.","07/Jun/19 09:56;speleomaniac;Hi,

 

I have the same problem in my Project and I tried to solving following way and I like to ask , is this acceptable or it is too naive...

I modified the following Java Class....

 

org\apache\kafka\streams\kstream\TimeWindows.java

 

@Override
public Map<Long, TimeWindow> windowsFor(final long timestamp) {
 TimeZone tz = TimeZone.getDefault();
 int offset = tz.getOffset(System.currentTimeMillis());
 long windowStart = (( Math.max(0, offset + timestamp - sizeMs + advanceMs) / advanceMs) * advanceMs) - offset;
 final Map<Long, TimeWindow> windows = new LinkedHashMap<>();
 while (windowStart <= timestamp) {
 final TimeWindow window = new TimeWindow(windowStart, windowStart + sizeMs);
 windows.put(windowStart, window);
 windowStart += advanceMs;
 }
 return windows;
}

 

 

This part 

 

TimeZone tz = TimeZone.getDefault();
int offset = tz.getOffset(System.currentTimeMillis());
long windowStart = (( Math.max(0, offset + timestamp - sizeMs + advanceMs) / advanceMs) * advanceMs) - offset;

 

calculate the local time zone, calculates the offset, then adds to timestamp to be able to find number of days from epoch start then substract it to be able to find ""00:00"" hour...

 

What do you think?","09/Jun/19 06:08;mjsax;Passing in a different `TimeWindow` seems like a good approach.","01/Aug/19 22:49;harshil_shah;Hey folks,

 

We stumbled upon similar issue, can you please shed more light on workaround to allow native timezone support ?

 

Thank you","02/Aug/19 18:12;mjsax;There are multiple options. The simplest one seem to be to ""shift"" the timestamps, for example, by using a custom `TimestampExtractor` and shift them back before you write the result using a custom `transformer()` (ie, `context.forward(..., To.all().withTimestamp(...))`)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide low-level Processor API meta data in DSL layer,KAFKA-4125,13002723,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,mjsax,mjsax,05/Sep/16 12:34,01/Aug/19 18:05,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,2,kip,,,,"For Processor API, user can get meta data like record offset, timestamp etc via the provided {{Context}} object. It might be useful to allow uses to access this information in DSL layer, too.

The idea would be, to do it ""the Flink way"", ie, by providing
RichFunctions; {{mapValue()}} for example.

Is takes a {{ValueMapper<V1, V2>}} that only has method

{noformat}
V2 apply(V1 value);
{noformat}

Thus, you cannot get any meta data within apply (it's completely ""blind"").

We would add two more interfaces: {{RichFunction}} with a method
{{open(Context context)}} and

{noformat}
RichValueMapper<V1, V2> extends ValueMapper<V1, V2>, RichFunction
{noformat}

This way, the user can chose to implement Rich- or Standard-function and
we do not need to change existing APIs. Both can be handed into
{{KStream.mapValues()}} for example. Internally, we check if a Rich
function is provided, and if yes, hand in the {{Context}} object once, to
make it available to the user who can now access it within {{apply()}} -- or
course, the user must set a member variable in {{open()}} to hold the
reference to the Context object.

KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-159%3A+Introducing+Rich+functions+to+Streams",,ableegoldman,astubbs,bbejeck,guanacobe,guozhang,hakim.acharifi,im_flog,jbrouwers,kiril_p,mjsax,nimfadora,scosenza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-5267,,,,,,,,,,,,,,,,,KAFKA-5632,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-09 19:17:36.539,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 01 18:05:43 UTC 2019,,,,,,,"0|i338yv:",9223372036854775807,,,,,,,,,,,,,,,,"09/Jan/17 19:17;bbejeck;Is this task still viable/desired? If so I'll pick it up.","09/Jan/17 21:35;mjsax;I think we need a KIP for this -- we did a lot of API changes/improvements recently and we got lot's of helpful feedback via KIP. (\cc [~guozhang] WDYT ?)

Writing the KIP should not be a big deal in this case: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals Let me know if you need any help with it (matthias@confluent.io) and I can guide you through the process (in case we need a KIP -- let's see what [~guozhang] thinks).","09/Jan/17 23:21;guozhang;[~bbejeck] Could you lead a KIP proposal / discussion on this change, since it proposes to add some new APIs? A recent example can be seen here: https://cwiki.apache.org/confluence/display/KAFKA/KIP-100+-+Relax+Type+constraints+in+Kafka+Streams+API

Since this is a straight-forward addition I won't expect it to incur much overhead.","10/Jan/17 13:46;bbejeck;[~guozhang] [~mjsax]  Thanks for the response.  Sure I'll give the KIP proposal a shot.","13/Jan/17 18:19;mjsax;Hey [~bbejeck]. I thus thought about this, and we should have some API discussion before you get started -- don't want you to waste your time.

The point is, that we do have {{transform}}, and {{transformValues}} in the API already, and we consider adding {{flatTransform}} and {{flatTransformValues}} similar to {{map}}, {{flatMap}} and {{mapValues}}, {{flatMapValues}}. Those functions allow to access all record meta data -- thus adding {{RichFunctions}} might be redundant. Even if ""transform"" was added to support stateful operators, you can use the in a stateless fashion, too.

This relates also to the idea to add parameter {{key}} to {{mapValues}} -- not sure if we need/want to add this.

I think it would be helpful to start a general API design discussion to see what we actually want to add and what not. WDYT? \cc [~guozhang] Right now, all those ideas are spread out over multiple JIRAs and I think we should consolidate all those ideas to get a sound API change instead of ""fixing"" random stuff here and there.","14/Jan/17 15:04;bbejeck;

[~mjsax] - Sounds reasonable to me.  With the information you have presented, I agree that adding {{RichFunctions}} could be redundant as it seems users already have access to record metadata in the methods you mentioned.  I think taking a step back to consider future API changes is a good idea.  Thanks for the follow-up, I'll hold off on doing anything for now.  ","14/Jan/17 15:37;guozhang;Thanks for bringing this up [~mjsax]. I do agree that we have multiple API discussions spread in different places now. Maybe you can create a wiki in AK summarizing all those proposals and we can then discuss them together and perhaps propose a single KIP wrapping them all? Current ones I can think of are:

1. Rich functions.
2. Add {{key}} to {{mapValues}} / {{transformValues}}.
3. Add {{flatTransform}} and {{flatTransformValues}}.
4. Separate {{RecordContext}} from {{ProcessorContext}}.
5. Deprecate not user-facing functions from {{TopologyBuilder}}.
6. Remove {{loggingEnabled}} parameter in {{ProcessorContext.register}}.
7. Remove {{disableLogging}} from {{Stores}}.
8. Change return type of {{Transfomer.punctuate()}} from {{R}} to {{null}}.","17/Feb/19 19:16;mjsax;Moving all major/minor/trivial tickets that are not merged yet out of 2.2 release.","01/Aug/19 10:05;astubbs;Can we add 2.3 as an affected version?","01/Aug/19 18:05;mjsax;We can, but because all versions are affected, it does not seem to be required?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Control over standby tasks host assignment,KAFKA-8727,13247683,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Duplicate,,lkokhreidze,lkokhreidze,29/Jul/19 15:24,29/Jul/19 17:36,12/Jan/21 10:06,29/Jul/19 17:36,,,,,,,,,,,streams,,,,,,0,,,,,"*Motivation*

As of now, Kafka Streams user has no control over to which host Kafka Streams application will create standby task. In production deployments (especially in Kubernetes) it's quite common to have multiple instances of the same Kafka Streams application deployed across more than one ""cluster"" in order to have high availability of the system.

For example, if we have 6 Kafka Streams instances deployed across two clusters, we'll get 3 Kafka Streams instances per cluster. With the current implementation, Kafka Streams application may create ""standby task"" in the same cluster as the active task. This is not the most optimal solution, since, in case of cluster failure recovery time will be much bigger. This is especially problematic for Kafka Streams application that manages large state.

 

*Possible Solution*

It would be great if in the Kafka Streams configuration we could have a possibility to inject dynamic environment variables and use that environment variables to control over where standby task should be created.

For example, suppose I have active task *1_1* with environment variable: *CLUSTER_ID: main01* then stnadby task for *1_1* should be created where *CLUSTER_ID* *!=* *main01*",,ableegoldman,lkokhreidze,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-07-29 15:42:02.603,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 29 17:36:39 UTC 2019,,,,,,,"0|z054ns:",9223372036854775807,,,,,,,,,,,,,,,,"29/Jul/19 15:26;lkokhreidze;I haven't looked at the code yet and have no idea how much work this gonna be or if it's even possible to do. But as a Kafka Streams user who manages large states, I clearly would love to see this feature.","29/Jul/19 15:42;mjsax;Thanks for creating this ticket [~lkokhreidze]. It is certainly possible (and not too hard) to add this feature.

Is there any difference to https://issues.apache.org/jira/browse/KAFKA-6718 ? Wondering if we should close this ticket as a duplicate?","29/Jul/19 16:06;lkokhreidze;[~mjsax] right, my Jira search is pretty bad :) sorry. Yes, seems like the same ticket. ","29/Jul/19 17:36;lkokhreidze; Duplicate of KAFKA-6718",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Feature to enable json field order retention in the JsonConverter,KAFKA-7948,13216509,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,joncourt,joncourt,19/Feb/19 04:39,22/Jul/19 15:21,12/Jan/21 10:06,,2.1.1,,,,,,,,,,config,KafkaConnect,,,,,0,needs-kip,,,,"We need to maintain the order of fields in json structures that pass through kafka connect. To achieve this a new configuration item has been added to the JsonConverter to engage the retention of field order in jsons between the input and output.

While the json spec doesn't require fields to be ordered it is required in instances where the parsers of json are primitive and difficult to correct - i.e. our mainframe.

The new config item is:
{code:java}
json.field.order = none|retained{code}
where the default is none and maintains the current functionality, and the option of 'retained' causes the underlying converter to use a LinkedHashMap in place of a HashMap and keeps the json fields in the order they're received during processing.",,githubbot,joncourt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-02-19 04:42:02.17,,,false,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,Tue Feb 19 04:42:02 UTC 2019,,,,,,,"0|yi142o:",9223372036854775807,,,,,,,,,,,,,,,,"19/Feb/19 04:42;githubbot;joncourt commented on pull request #6289: KAFKA-7948: Feature to enable json field order retention in the JsonConverter
URL: https://github.com/apache/kafka/pull/6289
 
 
   This change affects the JsonConverter.
   
   Some json parsers have particular requirements for field order in a json message. While this is not a part of the json spec and shouldn't really be necessary it is a reality for parsers we have on our mainframe. I have made this a configuration setting with a default of 'none' to retain the current functionality as is while giving users the option of enabling field order retention.
   
   3 new test methods have been added to verify the behaviour of the setting is as expected when not set, set to 'none' and set to 'retained'. 
   
   To engage the behaviour use:
   `json.field.order=retained`
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-419 Safely notify Kafka Connect SourceTask is stopped,KAFKA-7841,13210451,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,schofielaj,schofielaj,18/Jan/19 18:08,22/Jul/19 15:20,12/Jan/21 10:06,,2.2.0,,,,,,,,,,KafkaConnect,,,,,,0,needs-kip,,,,"Implements KIP 419.

https://cwiki.apache.org/confluence/display/KAFKA/KIP-419%3A+Safely+notify+Kafka+Connect+SourceTask+is+stopped",,githubbot,schofielaj,slachiewicz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/19 15:47;Thatboix45;The.txt;https://issues.apache.org/jira/secure/attachment/12955568/The.txt",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2019-04-08 19:00:07.327,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 08 19:00:07 UTC 2019,,,,,,,"0|yi02uo:",9223372036854775807,,,,,,,,,,,,,,,,"08/Apr/19 19:00;githubbot;AndrewJSchofield commented on pull request #6551: KAFKA-7841: Implement KIP-419 adding SourceTask.stopped method
URL: https://github.com/apache/kafka/pull/6551
 
 
   Add a new SourceTask.stopped method called as the last method in the lifecycle of a SourceTask.
   Called just as the resources are cleaned up in the Kafka Connect runtime.
   
   Testing by adding checks that the new method is called as expected in the existing Kafka Connect runtime tests.
   
   The contribution is my original work and I license the work to the project under the project's open source license.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add an ability to backup log segment files on truncation,KAFKA-8395,13234348,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,axiak,axiak,20/May/19 15:38,04/Jun/19 22:15,12/Jan/21 10:06,,2.2.0,,,,,,,,,,core,,,,,,1,needs-kip,,,,"At HubSpot, we believe we hit a combination of bugs [1] [2], which may have caused us to lose data. In this scenario, as part of metadata conflict resolution a slowly starting up broker recovered an offset of zero and truncated segment files.

As part of a belt-and-suspenders approach to reducing this risk in the future, I propose adding the ability to rename/backup these files and allowing kafka to move on. Note that this breaks the ordering guarantees, but allows one to recover the data and decide later how to approach it.

This feature should be turned off by default but enabled with a configuration option.

(A pull request is following soon on Github)

1: https://issues.apache.org/jira/browse/KAFKA-2178
 2: https://issues.apache.org/jira/browse/KAFKA-1120

 ",,axiak,githubbot,leoxlin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-05-20 15:53:28.918,,,false,,,,,,,,,Patch,,,,,,,,,9223372036854775807,,,Tue Jun 04 21:18:02 UTC 2019,,,,,,,"0|z02v4g:",9223372036854775807,,,,,,,,,,,,,,,,"20/May/19 15:53;githubbot;axiak commented on pull request #6772: KAFKA-8395: Add the ability to back up segment files on truncation
URL: https://github.com/apache/kafka/pull/6772
 
 
   This adds the ability to rename/back up segment files on truncation
   to zero. This is useful in the rare case that offset conflict resolution
   results in an offset reset, which can result in data loss.
   
   To enable, turn set `segment.backup.on.truncate.to.zero` to true
   in the configuration.
   
   We have unit tests included here and are starting
   to stress test a working cluster to see if we can
   reproduce the issue in a live environment.
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","04/Jun/19 21:18;axiak;Hey [~hachikuji] I was wondering if you could look and/or opine on this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add possibility to do repartitioning on KStream,KAFKA-8413,13235153,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Not A Problem,,lkokhreidze,lkokhreidze,23/May/19 14:10,23/May/19 17:59,12/Jan/21 10:06,23/May/19 17:59,,,,,,,,,,,streams,,,,,,0,,,,,"Consider following code:
{code:java}
final KStream<String, String> streamByProfileId = streamsBuilder
   .stream(""input-topic"", Consumed.with(Serdes.String(), Serdes.String()))
   .selectKey((key, value) -> value);

streamByProfileId
   .groupByKey()
   .aggregate(
      () -> 0d,
      (key, value, aggregate) -> aggregate,
      Materialized.as(""store-1"")
   );

streamByProfileId
   .groupByKey()
   .aggregate(
      () -> 0d,
      (key, value, aggregate) -> aggregate,
      Materialized.as(""store-2"")
   );
{code}
 

This code will generate following topology:
{code:java}
Topologies:
 Sub-topology: 0
 Source: KSTREAM-SOURCE-0000000000 (topics: [input-topic])
 --> KSTREAM-KEY-SELECT-0000000001
 Processor: KSTREAM-KEY-SELECT-0000000001 (stores: [])
 --> KSTREAM-FILTER-0000000004, KSTREAM-FILTER-0000000008
 <-- KSTREAM-SOURCE-0000000000
 Processor: KSTREAM-FILTER-0000000004 (stores: [])
 --> KSTREAM-SINK-0000000003
 <-- KSTREAM-KEY-SELECT-0000000001
 Processor: KSTREAM-FILTER-0000000008 (stores: [])
 --> KSTREAM-SINK-0000000007
 <-- KSTREAM-KEY-SELECT-0000000001
 Sink: KSTREAM-SINK-0000000003 (topic: store-1-repartition)
 <-- KSTREAM-FILTER-0000000004
 Sink: KSTREAM-SINK-0000000007 (topic: store-2-repartition)
 <-- KSTREAM-FILTER-0000000008
Sub-topology: 1
 Source: KSTREAM-SOURCE-0000000005 (topics: [store-1-repartition])
 --> KSTREAM-AGGREGATE-0000000002
 Processor: KSTREAM-AGGREGATE-0000000002 (stores: [store-1])
 --> none
 <-- KSTREAM-SOURCE-0000000005
Sub-topology: 2
 Source: KSTREAM-SOURCE-0000000009 (topics: [store-2-repartition])
 --> KSTREAM-AGGREGATE-0000000006
 Processor: KSTREAM-AGGREGATE-0000000006 (stores: [store-2])
 --> none
 <-- KSTREAM-SOURCE-0000000009
 
{code}
Kafka Streams creates two repartition topics for each `groupByKey` operation. In this example, two repartition topics are not really necessary and processing can be done with one sub-topology.

 

Kafka Streams user, in DSL, may specify repartition topic manually using *KStream#through* method:
{code:java}
final KStream<Object, Object> streamByProfileId = streamsBuilder
   .stream(""input-topic"")
   .selectKey((key, value) -> value)
   .through(""repartition-topic"");

streamByProfileId
   .groupByKey()
   .aggregate(
      () -> 0d,
      (key, value, aggregate) -> aggregate,
      Materialized.as(""store-1"")
   );

streamByProfileId
   .groupByKey()
   .aggregate(
      () -> 0d,
      (key, value, aggregate) -> aggregate,
      Materialized.as(""store-2"")
   );
{code}
 

 
{code:java}
Topologies:
Sub-topology: 0
Source: KSTREAM-SOURCE-0000000000 (topics: [input-topic])
--> KSTREAM-KEY-SELECT-0000000001
Processor: KSTREAM-KEY-SELECT-0000000001 (stores: [])
--> KSTREAM-SINK-0000000002
<-- KSTREAM-SOURCE-0000000000
Sink: KSTREAM-SINK-0000000002 (topic: repartition-topic)
<-- KSTREAM-KEY-SELECT-0000000001

Sub-topology: 1
Source: KSTREAM-SOURCE-0000000003 (topics: [repartition-topic])
--> KSTREAM-AGGREGATE-0000000004, KSTREAM-AGGREGATE-0000000005
Processor: KSTREAM-AGGREGATE-0000000004 (stores: [store-1])
--> none
<-- KSTREAM-SOURCE-0000000003
Processor: KSTREAM-AGGREGATE-0000000005 (stores: [store-2])
--> none
<-- KSTREAM-SOURCE-0000000003
{code}
  

While this gives possibility to optimizes Kafka Streams application, user still has to manually create repartition topic with correct number of partitions based on input topic. It would be great if in DSL we could have something like *repartition()* operation on *KStream* which can generate repartition topic based on user command.",,ableegoldman,bbejeck,lkokhreidze,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/19 14:07;lkokhreidze;topology-1.png;https://issues.apache.org/jira/secure/attachment/12969521/topology-1.png","23/May/19 14:07;lkokhreidze;topology-2.png;https://issues.apache.org/jira/secure/attachment/12969520/topology-2.png",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2019-05-23 15:33:06.923,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 23 17:58:51 UTC 2019,,,,,,,"0|z0303c:",9223372036854775807,,,,,,,,,,,,,,,,"23/May/19 14:10;lkokhreidze;Happy to work on KIP if this feature makes sense.","23/May/19 15:33;mjsax;This should already be fixed. You need to turn on topology optimization though. Compare https://issues.apache.org/jira/browse/KAFKA-6761

Seems we can close this ticket as ""invalid"" ?","23/May/19 16:27;lkokhreidze;Hi Matthias,

I've enabled topology optimization, but in this particular example, there're still 2 repartition topics created (Kafka Streams version 2.2.0).
{code:java}
Properties props = new Properties();
props.put(StreamsConfig.APPLICATION_ID_CONFIG, UUID.randomUUID().toString());
props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, ""localhost:9092"");
props.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.OPTIMIZE);
props.put(StreamsConfig.STATE_DIR_CONFIG, ""/tmp/kafka-streams"");{code}
Topics created:
{code:java}
867bef21-1e56-4ef4-918d-f701fe6000bc-store-1-changelog
867bef21-1e56-4ef4-918d-f701fe6000bc-store-1-repartition
867bef21-1e56-4ef4-918d-f701fe6000bc-store-2-changelog
867bef21-1e56-4ef4-918d-f701fe6000bc-store-2-repartition {code}
 

Actually, one other thing why introducing additional manual repartitoin may be valuable - correct me if I'm wrong, but Kafka Streams will try to optimize when key operation is followed by stateful operation, like *groupByKey().aggregate(...)* but there's may be the case, that in DSL user may be using stateful *transform(...)* operation for aggregation. Consider following example: 
{code:java}
final KStream<String, String> streamByProfileId = streamsBuilder
 .stream(""input.topic"", Consumed.with(Serdes.String(), Serdes.String()))
 .selectKey((key, value) -> value);

streamByProfileId.transform(...) // stateful tranformer with aggregation
streamByProfileId.transform(...) // stateful transformer with aggregation
{code}
 In this example there's no repartition topic created, one the other hand if had something like `repartition()` operation on KStream we could write something like this, which would be pretty cool imho:
{code:java}
final KStream<String, String> streamByProfileId = streamsBuilder
 .stream(""input.topic"", Consumed.with(Serdes.String(), Serdes.String()))
 .repartitionBy((key, value) -> new_key);

streamByProfileId.transform(...) // stateful transformer with aggregation
streamByProfileId.transform(...) // stateful transformer with aggregation{code}
 ","23/May/19 16:37;mjsax;{quote}I've enabled topology optimization, but in this particular example, there're still 2 repartition topics created (Kafka Streams version 2.2.0).
{quote}
\cc [~bbejeck] – Can you look into this? This would be a bug.

About the other request: I agree that this might be helpful, and in fact there is a similar ticket, including a KIP draft for this:
 * https://issues.apache.org/jira/browse/KAFKA-6037
 * [https://cwiki.apache.org/confluence/display/KAFKA/KIP-221%3A+Enhance+KStream+with+Connecting+Topic+Creation+and+Repartition+Hint]

The KIP is inactive, to feel free to pick it up.

I would not add `repartition()` operation though, but stick with `through()` and make the topic-name optional to let KS manage the topic.

 ","23/May/19 16:40;lkokhreidze;Thanks. I'll look into it","23/May/19 17:36;bbejeck;Hi, [~lkokhreidze] thanks for reporting this.

 

When you build the topology can you confirm for me that you are calling {{StreamBuilder#build(properties)}}? To optimize the topology you need to pass in the properties to the {{StreamBuilder}} as well as set it in the configuration.

 

Thanks,

Bill","23/May/19 17:44;lkokhreidze;Hi [~bbejeck],

I wasn't passing properties to StreamBuilder before, missed that part. Can confirm, after applying your suggestion there's only one reparation topic. Sorry about the confusion.

 ","23/May/19 17:58;bbejeck;Hi [~lkokhreidze],

No problem at all!  It's a subtle point and we could probably do a better job of making sure of that step isn't overlooked. I've created https://issues.apache.org/jira/browse/KAFKA-8416 to help improve the documentation for enabling optimizations.

I'll go ahead and close this ticket then.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KIP-427: Add AtMinIsr topic partition category (new metric & TopicCommand option),KAFKA-7904,13214297,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,lu.kevin,lu.kevin,lu.kevin,07/Feb/19 02:25,05/Apr/19 16:16,12/Jan/21 10:06,05/Apr/19 16:16,,,,,,,,2.3.0,,,,,,,,,0,,,,,https://cwiki.apache.org/confluence/display/KAFKA/KIP-427%3A+Add+AtMinIsr+topic+partition+category,,githubbot,lu.kevin,sliebau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-02-11 23:18:37.748,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 05 16:16:52 UTC 2019,,,,,,,"0|yi0qig:",9223372036854775807,,gwenshap,,,,,,,,,,,,,,"11/Feb/19 23:18;sliebau;Hi [~lu.kevin], could you please add some more detail to this ticket and the KIP? Both of them pretty much only consist of the headline.","11/Feb/19 23:33;lu.kevin;Hi [~sliebau], I am close to finishing up the draft and will publish it this week! Thanks~","11/Mar/19 05:24;githubbot;KevinLiLu commented on pull request #6421: KAFKA-7904: Add AtMinIsr partition metric and TopicCommand option
URL: https://github.com/apache/kafka/pull/6421
 
 
   
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
","11/Mar/19 05:26;lu.kevin;https://github.com/apache/kafka/pull/6421","05/Apr/19 16:16;githubbot;gwenshap commented on pull request #6421: KAFKA-7904: Add AtMinIsr partition metric and TopicCommand option (KIP-427)
URL: https://github.com/apache/kafka/pull/6421
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
To add kafka data at rest encryption,KAFKA-8170,13224577,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,ashelke,ashelke,28/Mar/19 12:26,28/Mar/19 13:39,12/Jan/21 10:06,,,,,,,,,,,,log,,,,,,0,features,security,,,"Kafka have mechanism for wire encryption of data.
But the kafka data at rest which exist in <log.dir>/<topic-name>-<partition> is still unencrypted.
This directories now have log files with actual messages embedded metadata, but unauthorised user can still recover messages from this files
Addiding encryption for this data would be valuable for preventing message protection from disk theft, unauthorised user access on servers.",,ashelke,rndgstn,Sandeep Nemuri,sliebau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-03-28 13:39:36.338,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 28 13:39:36 UTC 2019,,,,,,,"0|z017bs:",9223372036854775807,,,,,,,,,,,,,,,,"28/Mar/19 13:39;sliebau;Hi [~ashelke], 

I've proposed [KIP-317|https://cwiki.apache.org/confluence/display/KAFKA/KIP-317%3A+Add+transparent+data+encryption+functionality] a while ago, which would probably cover this - though it would do so client-side.

It has been dormant for a while now due to lots of other things to do on my end to be honest, but I plan on reviving this very shortly.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add schema.namespace support to SetSchemaMetadata SMT in Kafka Connect,KAFKA-7883,13212596,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Workaround,,zThulj,zThulj,29/Jan/19 16:32,04/Feb/19 18:50,12/Jan/21 10:06,04/Feb/19 18:50,2.1.0,,,,,,,,,,KafkaConnect,,,,,,0,features,,,,"When using a connector with AvroConverter & SchemaRegistry, users should be able to specify the namespace in the SMT.

Currently, only ""schema.version"" and ""schema.name"" can be specified.

This is needed because if not specified, generated classes (from avro schema)  are in the default package and not accessible.

Currently, the workaround is to add a Transformation implementation to the connect classpath.

It should be native.

 ",,zThulj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 04 18:49:46 UTC 2019,,,,,,,"0|yi0g1c:",9223372036854775807,,,,,,,,,,,,,,,,"04/Feb/19 18:49;zThulj;Actually, there is a native way to do it :

 ""transforms"" : ""AddNamespace"", 
 ""transforms.AddNamespace.type"" : ""org.apache.kafka.connect.transforms.SetSchemaMetadata$Value"", 
 ""transforms.AddNamespace.schema.name"" : ""my.namespace.NameOfTheSchema""

 

""my.namespace"" will become the namespace

""NameOfTheSchema"" the name",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
distinct count kafka streams api,KAFKA-7820,13209484,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,vinubarro,vinubarro,14/Jan/19 19:56,17/Jan/19 03:50,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,needs-kip,,,,"we are using Kafka streams for our real-time analytic use cases. most of our use cases involved with doing distinct count on certain fields.

currently we do distinct count by storing the hash map value of the data in a set and do a count as event flows in. There are lot of challenges doing this using application memory, because storing the hashmap value and counting them is limited by the allotted memory size. When we get high volume  or spike in traffic hash map of the distinct count fields grows beyond allotted memory size leading to issues.

other issue is when  we scale the app, we need to use global ktables so we get all the values for doing distinct count and this adds back pressure in the cluster or we have to re-partition the topic and do count on the key.

Can we have feature, where the distinct count is supported by through streams api at the framework level, rather than dealing it with application level.",,bchen225242,guozhang,mjsax,vinubarro,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-01-14 22:37:31.142,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 17 03:50:00 UTC 2019,,,,,,,"0|u00tsw:",9223372036854775807,,,,,,,,,,,,,,,,"14/Jan/19 22:37;bchen225242;Hey Vinoth,

thanks for proposing this! Based on your use case, I'm wondering whether we could repartition the input with all the cared fields are a compound key, and aggregate based on the key? That should be able to fulfill your requirement.","15/Jan/19 20:25;vinubarro;Hey Boyang,

That's a good idea and we considered that option too. And that comes with a trade-off allocating more space in the cluster for distinct count itself with re-partitioning. we have large schema with around 15-20 fields qualify for distinct counts based on the use cases for making near-real time decisions. re-partitioning for those many fields we have to trade-off on the space. this will be challenging at enterprise level, since many teams share the same cluster.

There are other ways where we can perform distinct count for a field using statetore or ktables. In this scenario we have too maintain too many tables at the application level. 

This is a nice to have feature for apps that uses Kafka streams for analytical use cases making real-time decisions. since many other streaming frameworks supports this feature, we thought it would be a very useful feature in the streams for many use cases, if this is something that can be handled at framework level .

Would implementing this feature at the framework level going to be a heavy-lift?

 ","16/Jan/19 00:09;bchen225242;Thanks [~vinubarro] for more details. I think we need to further understand the use case before we decide whether we need to add a new API. 
 # About repartition, since KStream does aggregation on partition level, so it is required to have the same key hashing to the same partition. My question is that how many unique keys we are having here (the combo out of all 15-20 fields)? If the total number of the keys are not that big, it should be ok 
 # We don't need to have multiple KTables to solve the problem. We could just get a common aggregation key and do the counts, if you are referring to one single streaming instance. At Pinterest, we are building a generic API on top of #aggregate() by extracting necessary fields to generate a superset join key through our thrift structure.

In conclusion, with proper repartition applied, count() should be suffice for most use cases. To add support for field extraction on framework level, we need to have the API support multiple data types (Json, Avro, Thrift). Let me know if this answers your question. ","17/Jan/19 03:50;guozhang;[~vinubarro] Thanks for sharing your use case. I think the proposal 2) from [~bchen225242] may well fit your needs. To be more specific: say you need 10-20 fields that require distinct counts, you can create a repartition key which is a combo of all of these fields via a single repartition topic. For example, if your interested fields are A,B,C, and you create a combo key is (A,B,C), the semantics of a co-partition key is that ""all the records with the same values in A,B,C will go to the same partition"", which inplies ""all the records with the same values of A will go to the same partition"" (same for B, C), so after you've done the repartitioning, say to distinctly count on field A, you can aggregate on B/C and count on A, and aggregate on A/C to count on B etc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition assignment strategy that distributes lag evenly across consumers in each group,KAFKA-5337,13075324,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,grantneale,grantneale,27/May/17 05:25,09/Jan/19 09:36,12/Jan/21 10:06,,0.10.2.1,,,,,,,,,,consumer,,,,,,2,,,,,"Existing partition assignment strategies (RangeAssignor and RoundRobinAssignor) do not account for the current consumer group lag on each partition.  This can result in sub-optimal assignments when the distribution of lags for a given topic and consumer group is skewed.

The LagBasedAssignor operates on a per-topic basis, and attempts to assign partitions such that lag is distributed as evenly across a consumer group.

h4. Algorithm:

For each topic, we first obtain the lag on all partitions. Lag on a given partition is the difference between the end offset and the last offset committed by the consumer group. If no offsets have been committed for a partition we determine the lag based on the code auto.offset.reset property. If auto.offset.reset=latest, we assign a lag of 0. If auto.offset.reset=earliest (or any other value) we assume assign lag equal to the total number of message currently available in that partition.

We then create a map storing the current total lag of all partitions assigned to each member of the consumer group. Partitions are assigned in decreasing order of lag, with each partition assigned to the consumer with least total number of assigned partitions, breaking ties by assigning to the consumer with the least total assigned lag.

Distributing partitions evenly across consumers (by count) ensures that the partition assignment is balanced when all partitions have a current lag of 0 or if the distribution of lags is heavily skewed. It also gives the consumer group the best possible chance of remaining balanced if the assignment is retained for a long period.",,astubbs,baz33,githubbot,grantneale,jeffwidman,mgarbis,scosenza,umesh9794@gmail.com,vahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-05-27 05:39:55.709,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 09 09:36:20 UTC 2019,,,,,,,"0|i3fjpj:",9223372036854775807,,,,,,,,,,,,,,,,"27/May/17 05:32;grantneale;Currently working on this and intend to raise a PR soon.  Unfortunately, I don't have permission to assign the issue to myself.","27/May/17 05:39;githubbot;GitHub user grantneale opened a pull request:

    https://github.com/apache/kafka/pull/3158

    KAFKA-5337: LagBasedAssignor partition assignment strategy

    Existing partition assignment strategies (RangeAssignor and RoundRobinAssignor) do not account for the current consumer group lag on each partition. This can result in sub-optimal assignments when the distribution of lags for a given topic and consumer group is skewed.
    
    The LagBasedAssignor operates on a per-topic basis, and attempts to assign partitions such that lag is distributed as evenly across a consumer group.
    
    ## Algorithm
    
    For each topic, first obtain the lag on all partitions. Lag on a given partition is the difference between the end offset and the last offset committed by the consumer group. If no offsets have been committed for a partition we determine the lag based on the code auto.offset.reset property. If auto.offset.reset=latest, we assume a lag of 0. If auto.offset.reset=earliest (or any other value) we assume lag equal to the total number of message currently available in that partition.
    
    Next, create a map storing the current total lag of all partitions assigned to each member of the consumer group. Partitions are assigned in decreasing order of lag, with each partition assigned to the consumer with least total number of assigned partitions, breaking ties by assigning to the consumer with the least total currently assigned lag.
    
    Assigning partitions evenly across consumers (by partition count) ensures that the assignment is reasonably balanced (by partition count) when all partitions have a current lag of 0 or if the distribution of lags is heavily skewed. It also gives the consumer group the best possible chance of remaining balanced if the assignment is retained for a long period (assuming throughput is consistent across members of the consumer group).

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/grantneale/kafka feature/kafka-5337-lag-based-assignor

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3158.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3158
    
----

----
","27/May/17 05:42;grantneale;PR raised: https://github.com/apache/kafka/pull/3158","28/May/17 23:04;githubbot;Github user grantneale closed the pull request at:

    https://github.com/apache/kafka/pull/3158
","18/Jun/17 08:38;grantneale;A corresponding KIP has been raised at:
 https://cwiki.apache.org/confluence/display/KAFKA/KIP-169+-+Lag-Aware+Partition+Assignment+Strategy?flashId=471405093","02/Apr/18 19:55;mgarbis;Is this dead? This would be a great new feature to have out of the box, instead of having to implement it ourselves.","05/Apr/18 10:10;grantneale;It would appear so, the KIP didn't generate any interest.  If you're interested, there is a third-party implementation available here: https://github.com/grantneale/kafka-lag-based-assignor","09/Jan/19 09:36;baz33;I think it's an interesting feature to add.

When a consumer group read multiple topics with lots of offsets differences it can be lead to a unique worker to read the ""big"" topics.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support different topic name in source and destination server in Mirrormaker,KAFKA-7615,13197675,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,adeetikaushal,adeetikaushal,11/Nov/18 23:03,15/Nov/18 02:54,12/Jan/21 10:06,,,,,,,,,,,,mirrormaker,,,,,,0,,,,,"Currently mirrormaker only supports same topic name in source and destination broker. Support for different topic names in source and destination brokers is needed.

 

source broker : topic name -> topicA

destination broker: topic name -> topicB

 

MirrorData from topicA to topicB",,adeetikaushal,ryannedolan,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-11-12 18:48:06.855,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 15 02:54:42 UTC 2018,,,,,,,"0|s00dc8:",9223372036854775807,,,,,,,,,,,,,,,,"12/Nov/18 05:50;adeetikaushal;Pull request: https://github.com/apache/kafka/pull/5902","12/Nov/18 18:48;ryannedolan;[https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0] (under discussion) supports this via Connect transformations (SMTs)","15/Nov/18 02:54;wushujames;[~adeetikaushal]: This can be implemented by supplying a MessageHandler to mirrormaker. See [https://github.com/gwenshap/kafka-examples/tree/master/MirrorMakerHandler] for an example.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a tool to delete kafka based consumer offsets for a given group,KAFKA-6314,13123165,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Duplicate,,tom@confluent.io,tom@confluent.io,06/Dec/17 11:00,23/Aug/18 06:19,12/Jan/21 10:06,23/Aug/18 06:19,,,,,,,,,,,consumer,core,tools,,,,0,,,,,"Add a tool to delete kafka based consumer offsets for a given group similar to the reset tool. It could look something like this:

kafka-consumer-groups --bootstrap-server localhost:9092 --delete-offsets --group somegroup

The case for this is as follows:

1. Consumer group with id: group1 subscribes to topic1
2. The group is stopped 
3. The subscription changed to topic2 but the id is kept as group1

Now the out output of kafka-consumer-groups --describe for the group will show topic1 even though the group is not subscribed to that topic. This is bad for monitoring as it will show lag on topic1.




",,bobrik,omkreddy,tom@confluent.io,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-12-08 09:06:59.433,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 23 06:19:58 UTC 2018,,,,,,,"0|i3nlkv:",9223372036854775807,,,,,,,,,,,,,,,,"08/Dec/17 09:06;wushujames;FYI, kafka-consumer-groups.sh already has --delete support, but it only works for zookeeper-based offsets. 
{noformat}
$ ~/kafka_2.11-1.0.0/bin/kafka-consumer-groups.sh
List all consumer groups, describe a consumer group, delete consumer group info, or reset consumer group offsets.
Option                                  Description
------                                  -----------
--delete                                Pass in groups to delete topic
                                          partition offsets and ownership
                                          information over the entire consumer
                                          group. For instance --group g1 --
                                          group g2
                                        Pass in groups with a single topic to
                                          just delete the given topic's
                                          partition offsets and ownership
                                          information for the given consumer
                                          groups. For instance --group g1 --
                                          group g2 --topic t1
                                        Pass in just a topic to delete the
                                          given topic's partition offsets and
                                          ownership information for every
                                          consumer group. For instance --topic
                                          t1
                                        WARNING: Group deletion only works for
                                          old ZK-based consumer groups, and
                                          one has to use it carefully to only
                                          delete groups that are not active.
{noformat}

So this JIRA should say that the RFE is to let us delete kafka-based offsets.","08/Dec/17 09:51;tom@confluent.io;thank, I've modified it to reference kafka based offsets","06/Jan/18 00:23;bobrik;Is there a workaround that allows universal alerting for lagging consumers?","19/Jun/18 18:36;omkreddy;""–delete"" works with java consumer groups also. This option just deletes all the group information and associated offsets.","23/Aug/18 06:19;omkreddy;Resolving as duplicate of KAFKA-6275",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add KafkaConsumer fetch-error-rate and fetch-error-total metrics ,KAFKA-7300,13179261,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,lu.kevin,lu.kevin,lu.kevin,16/Aug/18 06:16,16/Aug/18 06:39,12/Jan/21 10:06,,,,,,,,,,,,clients,consumer,metrics,,,,0,,,,,"[https://cwiki.apache.org/confluence/display/KAFKA/KIP-356%3A+Add+KafkaConsumer+fetch-error-rate+and+fetch-error-total+metrics]

 

The KafkaConsumer is a complex client that requires many different components to function properly. When a consumer is not operating properly, it can be difficult to identify the root cause and which component is causing issues (ConsumerCoordinator, Fetcher, ConsumerNetworkClient, etc).

 

This aims to improve the monitoring and detection of KafkaConsumer’s Fetcher component.

 

Fetcher will send a fetch request for each node that the consumer has assigned partitions for.

 

This fetch request may fail under the following cases:
 * Intermittent network issues (goes to onFailure)
 * Node sent an invalid full/incremental fetch response (FetchSessionHandler’s handleResponse returns false)
 * FetchSessionIdNotFound
 * InvalidFetchSessionEpochException

 

These cases are logged, but it would be valuable to provide a corresponding metric that allows for monitoring and alerting.",,CDanielC,lu.kevin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-08-16 06:16:10.0,,,,,,,"0|i3x3mf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New KafkaSpoutConfig(Scheme)-ByteArrayKeyValueScheme,KAFKA-5799,13097893,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Auto Closed,,NamGung,NamGung,28/Aug/17 07:11,25/Jul/18 16:28,12/Jan/21 10:06,25/Jul/18 16:28,0.11.0.0,,,,,,,,,,,,,,,,0,,,,,"I try to integrate Kafka with Apache Strom.
I want to get data from Kafka, using KafkaSpout in Apache Storm. 

To get data from Kafka using KafkaSpout, SpoutConfig-scheme must be setting. (Scheme is an interface that dictates how the ByteBuffer consumed from Kafka gets transformed into a storm tuple)
I want to get both key and value in Kafka, so I used to KafkaSpoutConfig ‘KeyValueSchemeAsMultiScheme’.

KeyValueSchemeAsMultiScheme’s Constructor is as follows.
[^2.JPG]
But, as you can see in the picture, implementing classes of Interface KeyValueScheme are only StringKeyValueScheme.
[^1.JPG]

Using StringKeyValueShceme causes problems when importing Integer data from Kafka. Because StringKeyValueScheme deserialize Bytebuffer to String.

So I implement ByteArrayKeyValueScheme that deserialize ByteBuffer to ByteArray.
ByteArrayKeyValueScheme imports data as BtyeArray.
If you use ByteArrayKeyValueScheme, you can import data regardless of data type from Kafka without error.
(But, you should convert data type ByteArray to data type that you want(e.g. String, Integer...))

[^bakvs.JPG]
{code:java}
// Some comments here
import java.nio.ByteBuffer;
import java.util.List;

import org.apache.storm.kafka.KeyValueScheme;
import org.apache.storm.spout.RawScheme;
import org.apache.storm.tuple.Values;
import com.google.common.collect.ImmutableMap;

public class ByteArrayKeyValueScheme extends RawScheme implements KeyValueScheme {

	@Override
	public List<Object> deserializeKeyAndValue(ByteBuffer key, ByteBuffer value) {
		// TODO Auto-generated method stub
		if (key == null) {
			return deserialize(value);
		}
		Object keytuple = deserialize(key).get(0);
		Object valuetuple = deserialize(value).get(0);

		return new Values(ImmutableMap.of(keytuple, valuetuple));
	}
}
{code}


",apache-storm 1.1.0,ewencp,NamGung,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/17 07:16;NamGung;1.JPG;https://issues.apache.org/jira/secure/attachment/12884004/1.JPG","28/Aug/17 07:16;NamGung;2.JPG;https://issues.apache.org/jira/secure/attachment/12884003/2.JPG","28/Aug/17 07:17;NamGung;bakvs.JPG;https://issues.apache.org/jira/secure/attachment/12884002/bakvs.JPG",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2018-03-07 05:54:20.893,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 25 16:28:02 UTC 2018,,,,,,,"0|i3jc7r:",9223372036854775807,,,,,,,,,,,,,,,,"07/Mar/18 05:54;ewencp;Is there an actual Kafka issue here? It looks to me like this is just an issue with the interfaces/types used in Storm. Kafka allows you to deserialize to whatever types you like and the KeyValueScheme issues seem to be limitations of Storm interfaces. Perhaps this is better filed against Storm?","25/Jul/18 16:28;omkreddy;Closing Apache Storm - Kafka Spout related query.  If this still issue, please contact storm mailing list.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zookeeper client setting in server-properties,KAFKA-7090,13167636,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,ctr,ctr,22/Jun/18 11:57,15/Jul/18 13:21,12/Jan/21 10:06,,,,,,,,,,,,config,documentation,,,,,1,,,,,"There are several Zookeeper client settings that may be used to connect to ZK.

Currently, it seems only very few zookeeper.* settings are supported in Kafka's server.properties file. Wouldn't it make sense to support all zookeeper client settings there or where would that need to go?

I.e. for using Zookeeper 3.5 with TLS enabled, the following properties are required:

zookeeper.clientCnxnSocket
zookeeper.client.secure
zookeeper.ssl.keyStore.location
zookeeper.ssl.keyStore.password
zookeeper.ssl.trustStore.location
zookeeper.ssl.trustStore.password

It's obviously possible to pass them through ""-D"", but especially for the keystore password, I'd be more comfortable with this sitting in the properties file than being visible in the process list...",,bgummalla,ctr,gquintana,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-06-27 20:24:22.693,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 15 13:21:34 UTC 2018,,,,,,,"0|i3v4rr:",9223372036854775807,,,,,,,,,,,,,,,,"27/Jun/18 20:24;bgummalla;Hi, is this still an issue that needs to be taken a look at?

 ","28/Jun/18 14:37;bgummalla;[~ctr] Can you please tell me if this needs to be worked on?","28/Jun/18 14:53;omkreddy;Kafka still uses Zookeeper 3.4 client dependency. Currently Zookeeper 3.5 is in beta release. We may update the dependency after stable Zookeeper 3.5 release. We may need to do this change, after Kafka upgrades Zookeeper's dependency to 3.5

Related Issue: KAFKA-3287","15/Jul/18 13:21;ctr;Wouldn't it make sense to have the required configuration *before* the dependeny is changed to 3.5?

Having said that, I think this request is independent of the Kafka dependency (you can already run Kafka with ZK 3.5) and there are other, non ZK3.5 and/or TLS/SSL related Zookeeper configuration directives where it would make sense to store them in the server.properties rather than in environment.

 

What would be wrong with passing any zookeeper.* configuration directive to Zookeeper library?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a new tool to loading data from file,KAFKA-6954,13162239,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,darion,darion,26/May/18 15:31,26/May/18 15:31,12/Jan/21 10:06,,1.1.0,,,,,,,,,,tools,,,,,,0,,,,,"Sometimes we will append data from a file or files , I write a small tool to loading data from file and write to Kafka. I think this is very useful and could make as a tool .

",,darion,Rajkumar Singh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-26 15:31:33.0,,,,,,,"0|i3u787:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a script for dump segment tool,KAFKA-189,12530463,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Duplicate,,tgautier,tgautier,06/Nov/11 00:01,17/Mar/18 06:25,12/Jan/21 10:06,17/Mar/18 06:25,,,,,,,,,,,core,,,,,,0,newbie,patch,,,"In the kafka jar there is a useful segment dump tool.  It would be useful to expose this as a script.  The current usage is:

bin/kafka-run-class.sh kafka.tools.DumpLogSegments logfilename -noprint

Change this to:

bin/kafka-dump-log-segments.sh logfilename <opts>
",,andrew.musselman,jozi-k,omkreddy,queinnec,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/12 14:20;queinnec;KAFKA-189.diff;https://issues.apache.org/jira/secure/attachment/12541058/KAFKA-189.diff",,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2012-08-15 14:19:23.318,,,false,,,,,,,,,,,,,,,,,,216201,,,Sat Mar 17 06:25:37 UTC 2018,,,,,,,"0|i029nz:",11167,,,,,,,,,,,,,,,,"15/Aug/12 14:19;queinnec;Hi, here's a patch against the trunk. Thanks for Kafka, it rocks!","17/Oct/16 18:45;andrew.musselman;Any interest in this; been open a while.","17/Mar/18 06:25;omkreddy;Fixed in  KAFKA-6615",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
automatic migration of log dirs to new locations,KAFKA-1689,12746657,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,fjavieralba,fjavieralba,08/Oct/14 08:42,20/Feb/18 23:48,12/Jan/21 10:06,20/Feb/18 23:48,0.8.1.1,,,,,,,1.1.0,,,config,core,,,,,1,newbie++,,,,"There is no automated way in Kafka 0.8.1.1 to make a migration of log data if we want to reconfigure our cluster nodes to use several data directories where we have mounted new disks instead our original data directory.

For example, say we have our brokers configured with:

  log.dirs = /tmp/kafka-logs

And we added 3 new disks and now we want our brokers to use them as log.dirs:

  logs.dirs = /srv/data/1,/srv/data/2,/srv/data/3

It would be great to have an automated way of doing such a migration, of course without losing current data in the cluster.

It would be ideal to be able to do this migration without losing service.


",,airbots,boniek,fjavieralba,jozi-k,kamal_ebay,noxis,sliebau,soumyajitsahu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-12-10 19:02:28.913,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 20 23:48:10 UTC 2018,,,,,,,"0|i20xbz:",9223372036854775807,,,,,,,,,,,,,,,,"10/Dec/14 19:02;airbots;Just a newbie to the Kafka community, maybe this one is a toy that I can play with. ","20/Feb/18 23:48;sliebau;The requested functionality is available as part of KAFKA-5163",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evaluate mmap-based writes for Log implementation,KAFKA-414,12600100,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Won't Fix,,jkreps,jkreps,25/Jul/12 00:13,16/Nov/17 16:45,12/Jan/21 10:06,16/Nov/17 16:45,,,,,,,,,,,,,,,,,0,,,,,"Working on another project I noticed that small write performance for FileChannel is really very bad. This likely effects Kafka in the case where messages are produced one at a time or in small batches. I wrote a quick program to evaluate the following options:
raf = RandomAccessFile
mmap = MappedByteBuffer
channel = FileChannel
For both of the later two I tried both direct-allocated and non-direct allocated buffers (direct allocation is supposed to be faster).

Here are the results I saw:

[jkreps@jkreps-ld valencia]$ java -XX:+UseConcMarkSweepGC -cp target/test-classes -server -Xmx1G -Xms1G valencia.TestLinearWritePerformance $((256*1024)) $((1*1024*1024*1024)) 2
              file_length              size (bytes)              raf (mb/sec)   channel_direct (mb/sec)      mmap_direct (mb/sec)     channel_heap (mb/sec)        mmap_heap (mb/sec)
                  1000000                         1                       0.60                      0.52                     28.66                      0.55                     50.40
                  2000000                         2                       1.18                      1.16                     67.84                      1.13                     74.17
                  4000000                         4                       2.33                      2.26                    121.52                      2.23                    122.14
                  8000000                         8                       4.72                      4.51                    228.39                      4.41                    175.20
                 16000000                        16                       9.25                      8.96                    393.24                      8.88                    314.11
                 32000000                        32                      18.43                     17.93                    601.83                     17.28                    482.25
                 64000000                        64                      36.25                     35.21                    799.98                     34.39                    680.39
                128000000                       128                      69.80                     67.52                    963.30                     66.21                    870.82
                256000000                       256                     134.24                    129.25                   1064.13                    129.01                   1014.00
                512000000                       512                     247.38                    238.24                   1124.71                    235.57                   1091.81
               1024000000                      1024                     420.42                    411.43                   1170.94                    406.57                   1138.80
               1073741824                      2048                     671.93                    658.96                   1133.63                    650.39                   1151.81
               1073741824                      4096                    1007.84                    989.88                   1165.60                    976.10                   1158.49
               1073741824                      8192                    1137.12                   1145.01                   1189.38                   1128.30                   1174.66
               1073741824                     16384                    1172.63                   1228.33                   1192.19                   1206.58                   1156.37
               1073741824                     32768                    1221.13                   1295.37                   1170.96                   1262.28                   1156.65
               1073741824                     65536                    1255.23                   1306.33                   1160.22                   1268.24                   1142.52
               1073741824                    131072                    1240.65                   1292.06                   1101.90                   1269.00                   1119.14

The size column gives the size of the write, and the length column gives the total length of the file written. 

Now over a period of time the 1GB/sec performance is unsustainable because the disk on my machine would not be able to keep up. Nonetheless it is worth noting that even up to 256 byte writes that is not the bottleneck, the bottleneck is the write overhead.

This would indicate that a better strategy for the log would be to pre-allocate the segment and mmap it. Then use the memory map for writes and continue to use the filechannel for reads. ",,ijuma,jkreps,sliebau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/12 00:16;jkreps;TestLinearWritePerformance.java;https://issues.apache.org/jira/secure/attachment/12537772/TestLinearWritePerformance.java","25/Jul/12 00:16;jkreps;linear_write_performance.txt;https://issues.apache.org/jira/secure/attachment/12537773/linear_write_performance.txt",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2017-11-16 10:04:09.624,,,false,,,,,,,,,,,,,,,,,,241661,,,Thu Nov 16 16:45:23 UTC 2017,,,,,,,"0|i029of:",11169,,,,,,,,,,,,,,,,"25/Jul/12 00:16;jkreps;Attached test code to generate writes and a better-formatted version of the data.","16/Nov/17 10:04;sliebau;The switch to using MappedByteBuffers was made as part of KAFKA-506 as far as I can tell, so we can probably close this issue.","16/Nov/17 10:30;ijuma;We are still using `FileChannel` for log appends (see `FileRecords.append`)","16/Nov/17 15:52;sliebau;True, I was looking at the index file code, sorry for the mixup!
Is this still relevant and should be reopened then, or has there been a decision to not go down this road at some point in time? I couldn't find anything in jira or the wiki on this topic when researching..","16/Nov/17 15:53;ijuma;I don't know to be honest. The JIRA is more than 5 years old. :)","16/Nov/17 16:45;jkreps;This was meant as more of a ""memo to myself"". No reason to leave it open, the core observation remains true but I don't think it is really the biggest bottleneck and pre-allocation has other downsides.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ConsumerConnector has no access to ""getOffsetsBefore"" ",KAFKA-232,12537084,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Won't Fix,,appodictic,appodictic,03/Jan/12 18:09,29/Oct/17 09:13,12/Jan/21 10:06,29/Oct/17 09:13,,,,,,,,,,,,,,,,,0,,,,,"kafka.javaapi.SimpleConsumer has ""getOffsetsBefore"". I would like this ability in KafkaMessageStream or in ConsumerConnector. In this way clients can access their current position.",,appodictic,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-01-04 01:40:21.245,,,false,,,,,,,,,,,,,,,,,,222594,,,Sun Oct 29 09:13:27 UTC 2017,,,,,,,"0|i029xj:",11210,,,,,,,,,,,,,,,,"04/Jan/12 01:40;junrao;Could you explain in a bit more detail how you plan to use this method? Today, ConsumerConnector doesn't support consuming from an arbitrary offset.","04/Jan/12 03:06;appodictic;Sure. I am using Kafka to build aggregations of logs. These aggregations are going to be minutely. I want to record clientid and my log position externally. If my consumer goes down 30 seconds into the minute I want to replay the entire minute over. I might also want to bring this client up at position x on another node.","05/Jan/12 21:10;junrao;Do you think you can add meta data in the message itself so that you know whether the consumed data has moved to the next minute? If so, you can turn off auto commit and call commitOffset() directly.","05/Jan/12 21:27;appodictic;Interesting idea. I am not sure that will work well, many producers will be logging and I would expect that some messages will arrive out of order. Especially as the minute turns over. In the worst case scenario a producer might have an unexpected shutdown and wake up at some time later. Ideally I would like the producers to not have to write special headers etc. I thought of recording the last message seen assuming that each record is more or less unique but there are edge cases here as well.

I opened this issue up because I am wondering why the two consumers have different functionality. Which one should be used for which cases? It is not entirely clear to a new user like myself. 

From a mile-high view it seems like if the ConsumerConnection has access to commitOffset() it should be able to call getOffset(). Is there some technical complexity in implementing this?
","12/Jan/12 19:58;junrao;We discussed in the mailing before about allowing the ZK-based consumer to consume from an arbitrary offset during startup time. Here is the main complexity. Multiple consumers in the same group can consume a topic jointly. When they start up, which consumer sets the offset for which partitions? How do we prevent 2 consumers from setting the offset for the same partition?","12/Jan/12 20:46;nehanarkhede;I think here is how that can work -

Say we enable a ""custom.beginning.offsets"" option in the zk consumer. What that means is that this particular group id has chosen to work with custom beginning offsets. 

The requirement is to make sure that only one consumer in the group specifies this option. And if more than one consumer in a group tries to do the same, we throw an exception. This can be implemented via zookeeper locks, where the first consumer id that specifies this option writes the custom offsets in the /consumers/groups/[group_id]/custom_offsets path. Following that any other consumer trying to do the same will get back an exception stating the consumer id that currently has the lock.  

On the first successful rebalance after startup, the consumer ids can check for this path and reset the consumed offset for the partitions they own to that value. 

This is just a cursory explanation of the idea and many details would have to be worked out. This is very tricky with the current implementation of the consumer rebalancing logic. I feel that this will be much easier to implement if we move to the co-ordinator approach for the consumer rebalancing. I've been meaning to write up a proposal for that. Will write up my ideas and add some ideas for this JIRA to it.","29/Oct/17 09:13;omkreddy;Closing inactive issue. The old consumer is no longer supported. This jira requirement can be implemented with java consumer API.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Output generated by kafka_acls, kafka_topics. kafka_topics should be easily usable in a pipe",KAFKA-5923,13102969,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,hrauch,hrauch,18/Sep/17 14:20,18/Sep/17 14:20,12/Jan/21 10:06,,0.11.0.0,,,,,,,,,,tools,,,,,,0,,,,,"The output produced by {{kafka_topics}}, {{kafka_acls}}, and {{kafka_configs}} (or rather, their corresponding, underlying classes) should be suitable for use in a pipe (e.g. to be piped into {{grep}} when these commands are used in an Ansible playbook/role).

AFAIK, the current implementations produce free form text. Using that inside a grep can be error prone IMHO (especially in case the output should change from one release to the next).

A more reliable approach would be to provide a cmd line switch to enable machine parseable output suitable for use in pipes.",Linux,hrauch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-09-18 14:20:52.0,,,,,,,"0|i3k6uf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose states of active tasks to public API,KAFKA-4819,13047100,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,fhussonnois,fhussonnois,fhussonnois,28/Feb/17 21:26,14/Sep/17 18:40,12/Jan/21 10:06,14/Sep/17 18:40,0.10.2.0,,,,,,,1.0.0,,,streams,,,,,,0,kip,,,,https://cwiki.apache.org/confluence/display/KAFKA/KIP+130%3A+Expose+states+of+active+tasks+to+KafkaStreams+public+API,,fhussonnois,githubbot,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-28 21:46:00.127,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 05 21:13:34 UTC 2017,,,,,,,"0|i3ar5j:",9223372036854775807,,,,,,,,,,,,,,,,"28/Feb/17 21:46;githubbot;GitHub user fhussonnois opened a pull request:

    https://github.com/apache/kafka/pull/2612

    KAFKA-4819: Expose states for active tasks to public API

    Simple implementation of the feature : [KAFKA-4819](https://issues.apache.org/jira/browse/KAFKA-4819)
     KAFKA-4819
    
    This PR adds a new method `threadStates` to public API of `KafkaStreams` which returns all currently states of running threads and active tasks.
    
    Below is a example for a simple topology consuming from topics; test-p2 and test-p4.
    
    [{""name"":""StreamThread-1"",""state"":""RUNNING"",""activeTasks"":[{""id"":""0_0"", ""assignments"":[""test-p4-0"",""test-p2-0""], ""consumedOffsetsByPartition"":[{""topicPartition"":""test-p2-0"",""offset"":""test-p2-0""}]}, {""id"":""0_2"", ""assignments"":[""test-p4-2""], ""consumedOffsetsByPartition"":[]}]}, {""name"":""StreamThread-2"",""state"":""RUNNING"",""activeTasks"":[{""id"":""0_1"", ""assignments"":[""test-p4-1"",""test-p2-1""], ""consumedOffsetsByPartition"":[{""topicPartition"":""test-p2-1"",""offset"":""test-p2-1""}]}, {""id"":""0_3"", ""assignments"":[""test-p4-3""], ""consumedOffsetsByPartition"":[]}]}]

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fhussonnois/kafka KAFKA-4819

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2612.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2612
    
----
commit 0f8b8123cabdbfcfb44fe59b9be20e13ac253c95
Author: Florian Hussonnois <florian.hussonnois@gmail.com>
Date:   2017-02-23T22:08:01Z

    KAFKA-4819: Expose states for active tasks to public API

----
","28/Feb/17 22:43;mjsax;[~fhussonnois] Thanks for the JIRA an PR. All public API changes require a KIP: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals","05/Sep/17 21:13;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/2612
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Count of bytes or messages of a topic stored in kafka,KAFKA-1197,12686995,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,hanish.bansal.agarwal,hanish.bansal.agarwal,02/Jan/14 16:45,07/Sep/17 17:41,12/Jan/21 10:06,07/Sep/17 17:41,0.7.2,0.8.0,,,,,,,,,core,,,,,,0,,,,,"There should be direct way of measuring count of messages or bytes for a topic stored in Kafka.

There are already some very useful metrics like byteRate and messageRate using what we can see count of bytes/messages coming into Kafka broker.
I was looking for some jmx metrics that can give count of messages/bytes stored in kafka.

If we look into data stores like hbase we can see  how many messages are  stored in hbase or if we look into search engine like elasticsearch then also we can see how many messages are stored/indexed in elasticsearch. In similar way i was expecting that there should be some way to see count of  messages or bytes for a topic stored in kafka without using any external tool.

It will be really helpful if there is some support for this using some jmx metric or by script.",,aperepel,hanish.bansal.agarwal,omkreddy,omnomnom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-01-13 13:34:03.266,,,false,,,,,,,,,,,,,,,,,,365990,,,Thu Sep 07 17:41:11 UTC 2017,,,,,,,"0|i1r35b:",366297,,,,,,,,,,,,,,,,"13/Jan/14 13:34;omnomnom;Is it really an issue to sum up offsets for topic's partitions? This will give you bytes (pre-0.8.x) / messages (post-0.8.x) count. ","13/Jan/14 14:01;hanish.bansal.agarwal;Not an issue. In case kafka is not restarted we can get idea of message count using ""MessageIn"" metrix. But all metrics are reset to zero when kafka is restarted so its not possible to see message count stored in kafka after restart kafka node. So there can be some script which can give count of messages stored in a topic.","07/Sep/17 17:41;omkreddy;Size, LogStartOffset, LogEndOffset are exposed as metrics in newer verions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for JMXMP,KAFKA-5622,13088906,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,vprakash,vprakash,20/Jul/17 21:42,20/Jul/17 21:42,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,It doesn't seem to be possible to access Kafka metrics through JMX using the JMXMP protocol. JMXMP uses less ports and has better security options than RMI and should be supported. ,,vprakash,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-07-20 21:42:31.0,,,,,,,"0|i3htpj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide utilities for polling source connectors,KAFKA-3819,12977382,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,ewencp,ewencp,09/Jun/16 20:55,31/May/17 16:37,12/Jan/21 10:06,,,,,,,,,,,,KafkaConnect,,,,,,1,needs-kip,,,,"Source connectors that need to poll for data are currently responsible for managing their own sleeping/backoff if they don't have any new data available. This is becoming a very common pattern. It's also easy to implement it incorrectly, e.g. by using Thread.sleep and not properly interrupting on stop().

We should probably provide some utilities, maybe just exposed via the Context object to implement this for connector developers.",,ewencp,rhauch,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-06-09 20:55:26.0,,,,,,,"0|i2z8zj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include producing principal and host in messages,KAFKA-4947,13058917,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,madhadron,madhadron,24/Mar/17 15:01,24/Mar/17 15:01,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,,,,,"For a couple of topics, we need to be able to audit the hosts and principals that are sending messages. We would like to have Kafka include the host and principal that produced a message to a topic in the message when we consume it.",,madhadron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-03-24 15:01:52.0,,,,,,,"0|i3cr5r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RESTful proxy,KAFKA-639,12618097,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Duplicate,,mumrah,mumrah,29/Nov/12 01:03,10/Mar/17 09:44,12/Jan/21 10:06,10/Mar/17 09:44,,,,,,,,,,,contrib,,,,,,1,,,,,"An issue to track work on a RESTful proxy 

See initial discussion here: http://mail-archives.apache.org/mod_mbox/incubator-kafka-dev/201211.mbox/%3C00AF3218-0AA9-42EA-B601-F54B167FDCC0%40gmail.com%3E",,anandriyer,lanzaa,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,292704,,,Thu Nov 29 01:17:15 UTC 2012,,,,,,,"0|i0sbev:",163325,,,,,,,,,,,,,,,,"29/Nov/12 01:17;mumrah;I've committed some work on this to a personal fork in GitHub: https://github.com/mumrah/kafka/tree/rest/contrib/rest-proxy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka producer implementation without additional threads and control of when data is sent to kafka (similar to sync producer of 0.8.),KAFKA-4822,13047200,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,mgiri935,mgiri935,01/Mar/17 06:32,02/Mar/17 07:21,12/Jan/21 10:06,,0.10.0.0,0.10.0.1,0.10.1.0,0.10.1.1,0.9.0.0,0.9.0.1,,,,,producer ,,,,,,0,,,,,,,huxi_2b,ijuma,mgiri935,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-03-01 09:52:41.108,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 01 13:38:19 UTC 2017,,,,,,,"0|i3arrr:",9223372036854775807,,,,,,,,,,,,,,,,"01/Mar/17 09:52;huxi_2b;For KafkaProducer, all writes are asynchronous by default. Snippets below implements synchronous writes:
{code}
Future<RecordMetadata> future = producer.send(record);
RecordMetadata metadata = future.get();
{code}

","01/Mar/17 13:10;mgiri935;i understand that the sync can be achieved by the above workaround, but still another thread will be spawned per producer to send the data in background.

i have a case where a predefined set of threads (40 to 80 depending on the machine conf) receive data from a persistent medium and this data is sent to kafka (1 producer per thread) and i have to commit the position of the persistent medium to protect against restarts. I have achieved this in 0.8 sync producer by storing the data from persistent medium in list and after some content is cached i sent to kafka as batch and committed the position in the persistent medium. But now i do not have explicit control over when the data will be sent to kafka as data is completely handled by the new network thread and also the batch full and new batch are not visible to user (lost at KafkaProducer.doSend()). ","01/Mar/17 13:15;mgiri935;A producer implementation with finer control will be helpful in the above like cases.","01/Mar/17 13:38;ijuma;The data is sent as soon as possible by default (linger.ms=0), so if you wait for the Future to complete or use a callback, it should behave the same as before (the fact that the send is being done by a background thread is an implementation detail).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide access to low-level Metrics in ProcessorContext,KAFKA-3537,12957806,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,enothereska,mdcoon,mdcoon,11/Apr/16 18:25,22/Feb/17 00:57,12/Jan/21 10:06,13/Jan/17 17:05,0.10.0.0,0.10.0.1,0.10.1.0,0.10.1.1,,,,0.10.2.0,,,streams,,,,,,0,semantics,,,,"It would be good to have access to the underlying Metrics component in StreamMetrics. StreamMetrics forces a naming convention for metrics that does not fit our use case for reporting. We need to be able to convert the stream metrics to our own metrics formatting and it's cumbersome to extract group/op names from pre-formatted strings the way they are setup in StreamMetricsImpl. If there were a ""metrics()"" method of StreamMetrics to give me the underlying Metrics object, I could register my own sensors/metrics as needed.",,enothereska,githubbot,guozhang,mdcoon,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-12-06 14:14:30.655,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 13 17:06:49 UTC 2017,,,,,,,"0|i2vy1r:",9223372036854775807,,,,,,,,,,,,,,,,"06/Dec/16 14:14;githubbot;GitHub user enothereska opened a pull request:

    https://github.com/apache/kafka/pull/2215

    KAFKA-3537: Expose metrics registry

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/enothereska/kafka KAFKA-3537-access-low-level-metrics

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/2215.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #2215
    
----
commit f212c3630dfbaf0c666a5e78d095d20776065bef
Author: Eno Thereska <eno.thereska@gmail.com>
Date:   2016-12-06T14:14:01Z

    Expose metrics registry

----
","06/Dec/16 14:16;enothereska;Note PR only exposes metrics to be consumed as read-only. KAFKA-3701 will allow registering of metrics.","06/Dec/16 14:18;enothereska;[~mdcoon1] could you have a look at PR. Sorry for long delay.","09/Jan/17 10:19;githubbot;Github user enothereska closed the pull request at:

    https://github.com/apache/kafka/pull/2215
","13/Jan/17 17:05;guozhang;Resolved as part of PR 1446.","13/Jan/17 17:06;guozhang;[~mdcoon1] In the coming 0.10.2.0 release users can get the metrics registry from {{context().metrics().metrics()}}, just FYI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add metrics ability for streams serde components,KAFKA-3535,12957687,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,mdcoon,mdcoon,11/Apr/16 12:30,22/Feb/17 00:57,12/Jan/21 10:06,,0.10.0.0,0.10.0.1,0.10.1.0,0.10.1.1,0.10.2.0,,,,,,streams,,,,,,0,user-experience,,,,"Add the ability to record metrics in the serializer/deserializer components. As it stands, I cannot record latency/sensor metrics since the API does not provide the context at the serde levels. Exposing the ProcessorContext at this level may not be the solution; but perhaps change the configure method to take a different config or init context and make the StreamMetrics available in that context along with config information.",,imandhan,mdcoon,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-04-11 12:30:40.0,,,,,,,"0|i2vxbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add event-based session windows,KAFKA-4731,13040186,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,bbejeck,mjsax,mjsax,03/Feb/17 18:00,10/Feb/17 03:19,12/Jan/21 10:06,,,,,,,,,,,,streams,,,,,,0,needs-kip,,,,"Kafka Streams allows to define session windows based on an time-based inactivity gap. This can be used for _session detection_.

However, some data streams do contain event like ""start session"" and ""end session"" to mark the begin and end of a session. For this use case, it is not required to _detect_ session because session boundaries are known and not based on time -- but on events.

This Jira is about adding support for those event-based session windows. To add this feature, a KIP (https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals) is required to discuss the proposed design.",,ableegoldman,astubbs,bbejeck,mjsax,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-02-07 02:54:27.59,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 07 04:09:18 UTC 2017,,,,,,,"0|i39l7r:",9223372036854775807,,,,,,,,,,,,,,,,"07/Feb/17 02:54;bbejeck;[~mjsax] Is this Jira pending any further API redesign discussions? If not I'd like to give this a shot.","07/Feb/17 04:09;mjsax;Feel free to take it. I don't see any conflict with the ongoing API improvements. You can just follow the current API pattern, and if we decide to change some API parts we can rework the API for this later on, too. Looking forward to your KIP -- did not put any thought into how the internal design could look like. Curious to see what you will come up with :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for batch/scheduled Copycat connectors,KAFKA-2483,12859656,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,ewencp,ewencp,27/Aug/15 17:11,20/Jan/17 19:12,12/Jan/21 10:06,,,,,,,,,,,,KafkaConnect,,,,,,0,needs-kip,,,,"A few connectors may not perform well if run continuously; for example, HDFS may not handle a task holding a file open for very long periods of time well.

These connectors will work better if they can schedule themselves to be executed periodically. Note that this cannot currently be implemented by the connector itself because in sink connectors get data delivered to them as it streams in. However, it might be possible to make connectors handle this themselves given the combination of KAFKA-2481 and KAFKA-2482 would make it possible, if inconvenient, to implement this in the task itself.",,ewencp,kzadorozhny-tubemogul,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-08-27 17:11:38.0,,,,,,,"0|i2jghr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement a default queue for expired messages,KAFKA-3895,12982404,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,krish7919,krish7919,23/Jun/16 17:32,29/Sep/16 16:16,12/Jan/21 10:06,,,,,,,,,,,,core,,,,,,0,,,,,"As discussed in the mailing list, kafka currently does not support a dead letter queue-like feature where all the expired messages can be sent to.

A high-level design of such a feature can be as follows:
When kafka needs to expire a message from topic 'X', it sends the message to 'X-expired' topic, rather than deleting it. If 'X' is 'X-expired', dequeue and requeue the message in the expired topic.
Given that this topic can receive duplicate messages (same expired message multiple times), the consumer of this topic needs to ensure duplicate message handling.

Open question:
Does the message need a timestamp to figure out when did it move from a normal queue to a dead letter queue. Does it need a flag to specify whether it had expired from the X-expired topic itself?

",,jeff.klukas@gmail.com,krish7919,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-09-29 16:16:01.711,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 29 16:16:01 UTC 2016,,,,,,,"0|i2zzhr:",9223372036854775807,,,,,,,,,,,,,,,,"29/Sep/16 16:16;jeff.klukas@gmail.com;It's not clear to me how the concepts here map onto the Kafka consumer model.

What do you consider an ""expired"" message?

I've been in some discussions recently with colleagues considering how we might get dead letter-like functionality in Kafka. In our case, we're usually concerned with a badly formed message that doesn't deserialize correctly or otherwise causes a non-retriable error in the consumer.

An idea we have considered is creating a single deadletter topic for the cluster. When a consumer gets an unretriable error for a message, it would produce a message to that deadletter topic where the key would be group.id and the value would be a tuple of (topic, partition, offset) that failed to be consumed. We'd then be able to go back within the retention period, figure out why the message failed to be consumed, change our consumer logic, and reconsume that particular message if needed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a throttling option to the Kafka replication tool,KAFKA-1464,12715806,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,benstopford,mjuarez,mjuarez,21/May/14 17:33,16/Sep/16 05:28,12/Jan/21 10:06,16/Sep/16 05:28,0.8.0,,,,,,,0.10.1.0,,,replication,,,,,,2,replication,replication-tools,,,"When performing replication on new nodes of a Kafka cluster, the replication process will use all available resources to replicate as fast as possible.  This causes performance issues (mostly disk IO and sometimes network bandwidth) when doing this in a production environment, in which you're trying to serve downstream applications, at the same time you're performing maintenance on the Kafka cluster.

An option to throttle the replication to a specific rate (in either MB/s or activities/second) would help production systems to better handle maintenance tasks while still serving downstream applications.",,becket_qin,chaitanyap,enothereska,ijuma,jkreps,jonbringhurst,jruckman,jthakrar,junrao,kzakee,maysamyabandeh,mjuarez,mwol,nehanarkhede,nico.meyer,r.weires,sam.obeid@shopify.com,stevenz3wu,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-05-21 18:30:45.871,,,false,,,,,,,,,,,,,,,,,,394091,,,Fri Sep 16 05:28:53 UTC 2016,,,,,,,"0|i1vut3:",394229,,junrao,,,,,,,,,,,,,,"21/May/14 18:30;jonbringhurst;Although this would be nice to have, something similar exists as part of linux. You can accomplish the same type of thing by first adding the process into a net_cls cgroup. Then, you can use the tc command to classify the marked packets into an htb qdisc (possibly with an sfq further down the tree to completely prevent starvation) to throttle the packets coming from kafka.

* https://www.kernel.org/doc/Documentation/cgroups/net_cls.txt
* http://linux.die.net/man/8/tc
* http://lartc.org/manpages/tc-htb.html
* http://lartc.org/manpages/tc-sfq.html

The blkio cgroup works in a similar way to throttle disk io.","04/Aug/15 17:36;ijuma;I'd like to take a look at this. In a separate conversation, [~junrao] suggested that the throttling should perhaps only happen for out of sync replicas.","04/Aug/15 17:58;junrao;Another thing that we need to be a bit careful is that typically throttling just slows down a request. However, in our case, a single replica fetch request may have multiple partitions and we don't want to slow down the in-sync replicas. Perhaps we should always respond asap but just gives back less data for out-of-sync replicas.","18/Jan/16 18:41;enothereska;An alternative to throttling background maintenance traffic is to use a priority levels (just two: foreground and background). This has the advantage of being fairly simple and allows for important replication work to proceed fast if there is little or no foreground traffic. If most of the contention happens at the disk (as [~mjuarez] seems to indicate) then priorities implemented as two queues at the receiving end could be sufficient. However, if the network is a problem as well, then throttling would probably work best since it limits background traffic at the source.","28/Jan/16 15:28;jkreps;I agree that the key difference is in-sync vs out-of-sync replicas. In-sync replicas add to the commit time so they are really the highest priority and generally should add much load anyway. Out-of-sync replicas are the catch up case that add load.

Blindly reducing the fetch size for out-of-sync partitions probably would make things worse though. Large fetch size is actually good for efficiency and shrinking it will add overhead (more physical I/O, more FS reads, more requests overall, etc).

However it should be possible to throttle dynamically at the partition level for out of sync partitions. This could be done by dynamically omitting partitions that have exceeded their throttle rate from either the fetch request that the follower sends or from the fetch response the leader constructs. For example when handling follower fetch requests the leader could check the observed fetch rate for that follower and whether it is in sync or not; if the rate exceeds the configured maximum for catch-up traffic the leader would ignore that partition and only answer for other partitions (if there are no other partitions the purgatory time would need to be calculated to be no greater than the time in which the fetch rate might come down below the throttle). This would allow for dynamically throttling down the catch up traffic without reducing efficiency.","28/Jan/16 15:32;jkreps;Another issue this raises is that a partition might have a natural rate of new data coming in that is higher than the catch-up rate in which case if it ever falls out of sync it can never catch up. This is possible today to some extent but not a common problem since the followers are, if anything, a bit faster than the leader and have no throttle.","29/Jan/16 15:45;ijuma;Thanks for your input [~jkreps].

With regards to the issue where a replica may never catch up, it is a good point that came up previously. One option may be to disable throttling (or increase the catch-up rate) in the case where the replica is falling further behind.

One important question is whether users have enough information to be able to configure an appropriate throttling/catch-up rate that takes into account both disk IO and network bandwidth while keeping resource utilisation at an appropriate level. Thoughts? (the log cleaner has a similar config: `log.cleaner.io.max.bytes.per.second`, although it seems simpler to figure out).","29/Jan/16 18:40;becket_qin;It looks our purpose is to minimize user impact during replica catching up. From broker point of view, as long as client request latency is acceptable we should fully utilize the bandwidth we have to let replicas keep up. We should be able to measure the user experience by checking Queuing time of requests from and response to clients.

If that is the case, maybe we can let user set an SLA for latency. And we will not throttle replication as long as the user ProduceRequest / FetchRequest queuing time. Otherwise, we will throttle the fetching from out of sync replica (We probably don't want to throttle in-sync replicas).","13/Feb/16 23:36;nehanarkhede;The most useful resource to throttle for is network bandwidth usage by replication, as measured by the rate of total outgoing replication data on every leader. Adding the ability on every leader to cap data transferred under an upper limit is what we are looking for. This can be a config option similar to the one we have for the log cleaner. It seems to be that it is better to have the leader send less instead of have the replica fetch less as the leader has a holistic view of the total amount of data being transferred out.
Data transferred from a leader includes
# Fetch requests from an in-sync replica
# Fetch requests from an out-of-sync replica of a partition being reassigned
# Fetch requests from an out-of-sync replica of a partition not being reassigned

Data transferred across 1+2+3 should stay roughly within the configured upper limit. If the limit is crossed, we want to start throttling requests, all except the ones that fall under #1. The leader can assign the remaining available bandwidth amongst partitions that fall under #2 and #3 by allowing more bandwidth to #3 since presumably it is fine to let partitions being reassigned to catch up slower than the rest. Throttling could involve returning fewer bytes as determined by this computation for each such partition as Jay suggests.","14/Apr/16 19:32;jruckman;Hello Neha, 

One problem we've run into, is we run a system where sometimes we replace brokers completely, in an automated fashion, and rebalance leadership and replicas across them.  When we bring a new broker online, we move some partitions to it.  What we see is something like this:

Consider topics A, B, C with replication factors of 3
Consider brokers 1,2,3 as serving topics A,B,C

A new broker 4 is replacing 1 (maybe the machine died, or whatever)

A and B are relatively small, but C is large

1. Move some leaders and replicas to 4 for A and B from 2 and 3.  Everything is good up until now
2. Move some leaders and replicas to 4 for C from 2 and 3. 

At this point, broker 4 is pegged, since it's trying to pull in data from 2 and 3 (the other two replicas) trying to catch up, so it causes timeouts for partitions it is the leader for.  Brokers 2 and 3 are ok because 4 can only use 1/2 of their bandwidth to replicate, since they still have some bandwidth available to serve requests.","13/Jun/16 15:10;r.weires;We have similar problems as described by Jason above, in our case usually when taking a broker offline due to hardware failure (broken HD, with each broker being equipped with 2 HDs / log directories in our case). If the broker gets back online with one fresh disk and corresponding missing data (i.e. half of the partitions of that broker missing), its network link is saturated for some time by inbound traffic to catch up with replication.

While the broker is re-streaming all the missing data, we are additionally experiencing problems with consumers as well. After the broker has caught up with it's missing data, the situation normalizes again quickly.

To me it seems as if the partitions for which the broker already catches up soon after restart (esp. the ones from non-broken HD which just had little data missing) are causing issues if the broker becomes leader for them, while it is otherwise still clogging its incoming link with replication of the remaining data.

In this scenario, I would actually prefer to just let the broker catch up with any replication it still needs to do, without it becoming leader for any partition it has. Isn't there actually a way to achieve this? I.e. just keeping a broker online with replication and all, but not having it take over any partition leadership (at least so long as there are other candidates available for leadership). Being able to toggle that behavior at run-time would be ideal, so that we would just explicitly activate it again after the maintenance interval, once the node has caught up the bulk of necessary replication. Could IMO be an alternative to any throttling approach.","11/Jul/16 18:54;kzakee;I agree with Ralph. 
Lets say, we have a high produce rate and a system failure (as long as the kafka retention period itself), there is a lot of data to catchup and as fast as it could. Throttling catching up of out-of-sync replicas in this case may become a ""chase-your-own-tail thing"" and these may never be able to catchup with their leader or take days depending on produce-rate and throttle limit. Suppressing new replicas taking the leadership until the time they have all caught up sounds a better idea.","11/Jul/16 20:47;wushujames;[~r.weires], [~kzakee], you might be able to control this a little by setting auto.leader.rebalance.enable=false. If you it to false, then the broker would come up but would not assume leadership for any partitions at all, unless manually told to. You would then have to use the kafka-preferred-replica-election.sh tool [https://cwiki.apache.org/confluence/display/KAFKA/Replication+tools#Replicationtools-1.PreferredReplicaLeaderElectionTool], to allow it to assume leadership.

This would mean that you wouldn't have the problem you described. But the downside is that you are now in charge of handling rebalancing on your own.

The auto.leader.rebalance.enable flag is not changeable during runtime, tho. I think it is only read at startup time.
","11/Jul/16 20:48;junrao;Currently, our leader balancing logic happens automatically on a per partition basis. Turning this off requires a restart of all brokers.

I am not sure if we always want to disable leader balancing during catch up though. Balancing the leaders as the replicas catching up allows us to balance the client traffic to more brokers. Doing this may slow down the catch-up traffic a bit. However, this is probably fine if we do the throttling properly.","11/Jul/16 22:20;r.weires;Thanks a lot for the input - so if I understand this right, the config setting James proposed would not work for me if I only set this on a single node (i.e. the node under maintenance) before starting it up again, correct? Otherwise, that would have been the perfect solution for me. I wouldn't mind running the node with the custom setting during recovery, and just restarting it again once more in the end without the setting.

If this won't work, what would even happen if this setting is defined differently on various nodes in the cluster? Anyhow, alternatively I'd still even consider using that option along with a full cluster restart before (and disabling with another cluster restart afterwards), since a maintenance scenario as described happens every now and then for us, and currently really causes us major hassle for many hours, every time.

Jun - I'm also not be sure if disabling leader balancing during catch up would necessarily be a good idea in general - but having / allowing the possibility to configure this some way would be a nice option to have IMO.","12/Jul/16 01:36;junrao;The controller does leader balancing. So, auto.leader.rebalance.enable needs to be set on the controller. However, controller can move.","12/Jul/16 13:25;r.weires;Another related idea then, since those consumer rebalancing issues that result during maintenance for us drove me up the walls yesterday... Just desperately looking for a way to get this stabilized (on our v0.8.2.1) ;)

Wouldn't a (manual and temporary) modification of the partition assignment also be a viable option, to prevent a given node from becoming leader for any partitions?

I mean, could I issue kafka-reassign-partitions.sh with a customized partition assignment, that wouldn't actually re-assign any partitions to different brokers, but would merely change the replica *order* for several of the partitions - such that the node in question no longer is first replica for any partition? If I understand it right, the controller will always prefer the first replica as leader in balancing, so I'd just need to make sure that my node won't be the first replica for anything. All this temporarily of course, so after the maintenance I'd restore the original partition assignment back again.

Should this work, or would you expect specific problems with this workaround...?

Also: Let me know if this rather belongs onto the mailing list, since admittedly it isn't really related to throttling... But as a side-remark in this regard, I also tried throttling outside kafka (i.e. on side of the network, tried via wondershaper) in our problem case, but that didn't help. I'd agree this would need to be within kafka, i.e. to be able to separate out-of-sync replica recovery traffic from the rest.","15/Sep/16 21:43;becket_qin;It seems the PR title did not start with ""KAFKA-1464"" so the PR link is not updated. Anyway, the PR link is https://github.com/apache/kafka/pull/1776","16/Sep/16 05:28;junrao;Issue resolved by pull request 1776
[https://github.com/apache/kafka/pull/1776]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Message Listener(that can hit callback registered),KAFKA-3970,12990486,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,raj.ver903@gmail.com,raj.ver903@gmail.com,18/Jul/16 17:46,19/Jul/16 16:23,12/Jan/21 10:06,,,,,,,,,,,,,,,,,,0,features,,,,"At my current project I am writing a Message Listener for kafka.

-Message listener can register a callable (domain url , headers, schema of domain end point)

-Initially wrote Message listener as java Callable later used akka Futures (as akka promises to scale well)

-Attaching classes that i am using in my current project.These are POC not production ready


Please review and let me know if this is a good idea as it looks promising 


",,raj.ver903@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/16 17:47;raj.ver903@gmail.com;ConsumerEntry.java;https://issues.apache.org/jira/secure/attachment/12818592/ConsumerEntry.java","18/Jul/16 17:47;raj.ver903@gmail.com;KafkaAkkaConsumerQueueManager.java;https://issues.apache.org/jira/secure/attachment/12818593/KafkaAkkaConsumerQueueManager.java","18/Jul/16 17:47;raj.ver903@gmail.com;KafkaCallable.java;https://issues.apache.org/jira/secure/attachment/12818594/KafkaCallable.java",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-07-18 17:46:21.0,,,,,,,"0|i315hz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide utilities for tracking source offsets,KAFKA-3820,12977388,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,liquanpei,ewencp,ewencp,09/Jun/16 21:00,15/Jun/16 18:20,12/Jan/21 10:06,,,,,,,,,,,,KafkaConnect,,,,,,0,needs-kip,,,,"OffsetStorageReader does not (and is not expected to) be immediately updated when a SourceRecord is returned from poll(). However, this can be a bit confusing to connector developers as they may return that data, then expect a subsequent read from OffsetStorageReader should match that. In other words, rather than tracking which offset they are at themselves in variables maintained by the task implementation, the connector developer expected OffsetStorageReader to do this for them.

Part of the confusion comes from the fact that data is sent asynchronously after returned from poll(), which explains the semantics we have. However, it does also mean many connectors have similarly structured code where they keep track of the current offset themselves. It might be nice to provide some utilities, probably through the Context object, to get the last returned offset for each source partition being processed by a task.",,diederik,ewencp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-06-09 21:00:19.0,,,,,,,"0|i2z90v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Notification upon unclean leader election,KAFKA-3271,12941511,New Feature,Open,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,,,yasuhiro.matsuda,yasuhiro.matsuda,23/Feb/16 19:33,23/Feb/16 22:03,12/Jan/21 10:06,,,,,,,,,,,,clients,core,,,,,0,,,,,"It is a legitimate restriction that unclean leader election results in some message loss. That said, it is always good to try to minimize the message loss. A notification of unclean leader election can reduce message loss in the following scenario.

1. The latest offset is L.
2. A consumer is at C, where C < L
3. A slow broker (not in ISR) is at S, where S < C
4. All brokers in ISR die.
5. The slow broker becomes a leader by unclean leader election.
6. Now the offset of S.
7. The new messages get offsets S, S+1, S+2, and so on.

Currently the consumer won't receive new messages of offsets between S and C. However, if the consumer is notified when unclean leader election happened and resets its offset to S, it can receive new messages between S and C.
",,mdaxini,TaoFeng,yasuhiro.matsuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-02-23 22:03:03.632,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 23 22:03:03 UTC 2016,,,,,,,"0|i2t8hj:",9223372036854775807,,,,,,,,,,,,,,,,"23/Feb/16 22:03;mdaxini;If there is an api to query a broker (leader of a partition) what the offset is when it became a leader is good enough, even if the consumer is not notified. When an invalid offset exception is encountered a client can make a leader offset metadata request for the specific topic and partition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add IP Filtering / Whitelists-Blacklists ,KAFKA-1810,12760221,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Won't Fix,jholoman,jholoman,jholoman,08/Dec/14 17:56,10/Nov/15 18:59,12/Jan/21 10:06,21/Apr/15 23:09,,,,,,,,,,,core,network,security,,,,0,,,,,"While longer-term goals of security in Kafka are on the roadmap there exists some value for the ability to restrict connection to Kafka brokers based on IP address. This is not intended as a replacement for security but more of a precaution against misconfiguration and to provide some level of control to Kafka administrators about who is reading/writing to their cluster.

1) In some organizations software administration vs o/s systems administration and network administration is disjointed and not well choreographed. Providing software administrators the ability to configure their platform relatively independently (after initial configuration) from Systems administrators is desirable.
2) Configuration and deployment is sometimes error prone and there are situations when test environments could erroneously read/write to production environments
3) An additional precaution against reading sensitive data is typically welcomed in most large enterprise deployments.

",,benoyantony,blue20080,bosco,jholoman,joestein,mherstine,nehanarkhede,tongli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/15 17:01;jholoman;KAFKA-1810.patch;https://issues.apache.org/jira/secure/attachment/12690887/KAFKA-1810.patch","16/Jan/15 00:47;jholoman;KAFKA-1810_2015-01-15_19:47:14.patch;https://issues.apache.org/jira/secure/attachment/12692649/KAFKA-1810_2015-01-15_19%3A47%3A14.patch","15/Mar/15 05:13;jholoman;KAFKA-1810_2015-03-15_01:13:12.patch;https://issues.apache.org/jira/secure/attachment/12704639/KAFKA-1810_2015-03-15_01%3A13%3A12.patch",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2014-12-09 04:58:23.892,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 15 05:13:08 UTC 2015,,,,,,,"0|i236vj:",9223372036854775807,,gwenshap,,,,,,,,,,,,,,"08/Dec/14 18:08;jholoman;There was a discussion in KAFKA-1512 regarding the capability for limiting connections by setting max connections to 0. This gives a cleaner way to perform similar functionality","09/Dec/14 04:58;nehanarkhede;[~jholoman] Not sure I understood what you are proposing. Can you be more specific about the changes you are proposing?","09/Dec/14 05:54;jholoman;[~nehanarkhede]  Sure no problem. I had a request to provide the ability to specify a range of IP addresses to either include or exclude. I was thinking the easiest way would be to specify IP addresses in CIDR notation and include them in the server.properties such as 192.168.2.0/24:allow, 192.168.1.0/16:deny. This would allow an administrator to accept/deny connections based on ip ranges. Does that clarify?
","09/Dec/14 18:25;bosco;[~jholoman], there is a JIRA for supporting authorization. https://issues.apache.org/jira/browse/KAFKA-1688. The current plan is to support user, group and IP. https://cwiki.apache.org/confluence/display/KAFKA/Security

Would it address your issue?

","10/Dec/14 01:38;jholoman;[~bosco] ,

I did note the broader security-related JIRA and have been following it somewhat closely. I'm keenly interested in this topic generally. I think this proposal makes sense for a number of reasons

1) While this feature is certainly related to security its intent is really more to insulate against configuration issues and give software administrators the ability manage incoming connections by network range. For example, make sure my QA isn't writing to Prod. 
2) This is a relatively simple change that doesn't require dependencies on a much larger security initiative.
3) There's no reason why this feature couldn't be worked into the upcoming permissions manager. 
4) The configuration is very minor, a list / range of IP addresses to allow/deny

Essentially this would cover the following scenarios
1) Allow any incoming connections but deny certain IP blocks
2) The inverse of the above
3) Some combination of the two, where certain subnets were allowed/denied, e.g. allow all traffic from 192.168.2.0/12 but deny 192.168.2.0/28.

This concept is probably most related to IPTables or AWS security groups… What I mean here, we have other more sophisticated methods of authorizing access in other systems but the ability to filter simple network requests from a given IP range I think provides a valuable tool in the administrator's kit. As such I think this would provide this particular capability to restrict access in the near term until a more robust implementation is completed, or in conjunction with that initiative.  As far as I know, the rollout for 1688 is TBD. ","14/Dec/14 16:29;joestein;+1 I like this approach because you can better manage the brokers to have resilience in their network environment for what hosts can connect to them. This is an implementation of what KAFKA-1688 will be layering and making pluggable. I also see overlap with https://issues.apache.org/jira/browse/KAFKA-1786 and might be a good place to start building that out too.","08/Jan/15 17:01;jholoman;Created reviewboard https://reviews.apache.org/r/29714/diff/
 against branch origin/trunk","08/Jan/15 17:08;jholoman;This patch is a first pass at implementing IP Filtering logic. It requires defining two additional properties: 
security.ip.filter.rule.type
security.ip.filter.list. 

The list of IP's are specified in CIDR notation: http://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing. This implementation supports whitelisting (""allow"" config value) or blacklisting (""deny""), mutually exclusive. The parameters are passed to socket server and validated upon startup.(I'd like to move most of the validation logic per KAFKA-1845). An exception is thrown and the server shutdown in the event of misconfiguration of these parameters. The check against the list is in the Acceptor thread, and if the rule check fails, the socket is closed. There are a lot of tests included in the patch but if there are suggestions for more please let me know.","16/Jan/15 00:47;jholoman;Updated reviewboard https://reviews.apache.org/r/29714/diff/
 against branch origin/trunk","22/Jan/15 01:35;jholoman;The current plan is to rework the configuration portion of this patch once KAFKA-1845 is committed (ConfigDef)","30/Jan/15 20:45;tongli;rather than add specific security measures, can we add some kind of plugin point so that any plugins can be configured to do that type of work. Either it is a IP filter or certificate filter or basic authentication filter we can simply enable these plugins according to our own needs. This way, kafka only provide the plugin point, nothing else, how the plugin gets developed , performs, are not really the concern of the kafka community, we can have a clear separation of concerns. This has been done in many other successful projects, new to kafka, just saying we can do some thing like middle ware (in python term) or servlet filter in java world. The point of doing this is to have the security measure become a configuration matter. One can choose any available plugins appropriate for their own purposes by changing configurations. ","15/Mar/15 05:13;jholoman;Updated reviewboard https://reviews.apache.org/r/29714/diff/
 against branch origin/trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logging,KAFKA-2497,12861020,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ewencp,zheolong,zheolong,01/Sep/15 14:05,01/Sep/15 17:38,12/Jan/21 10:06,01/Sep/15 17:38,,,,,,,,,,,,,,,,,0,,,,,"I have create one log collecting tool named ‘logkafka’, please add it's link to ecosystem/Logging.

Github url： https://github.com/Qihoo360/logkafka/",,ewencp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-09-01 17:38:49.476,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 01 17:38:49 UTC 2015,,,,,,,"0|i2jnjb:",9223372036854775807,,,,,,,,,,,,,,,,"01/Sep/15 17:38;ewencp;Added, thanks for the contribution [~zheolong]!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce merge-kafka-pr.py script,KAFKA-2187,12829110,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ijuma,ijuma,ijuma,12/May/15 07:28,10/Jul/15 18:43,12/Jan/21 10:06,09/Jul/15 22:25,,,,,,,,0.9.0.0,,,,,,,,,0,,,,,"This script will be used to merge GitHub pull requests and it will pull from the Apache Git repo to the current branch, squash and merge the PR, push the commit to trunk, close the PR (via commit message) and close the relevant JIRA issue (via JIRA API).

Spark has a script that does most (if not all) of this and that will be used as the starting point:

https://github.com/apache/spark/blob/master/dev/merge_spark_pr.py",,gwenshap,ijuma,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-2321,KAFKA-2328,,,,,,,,,,,,,,,,,,,,,,,"20/May/15 22:08;ijuma;KAFKA-2187.patch;https://issues.apache.org/jira/secure/attachment/12734250/KAFKA-2187.patch","20/May/15 22:14;ijuma;KAFKA-2187_2015-05-20_23:14:05.patch;https://issues.apache.org/jira/secure/attachment/12734253/KAFKA-2187_2015-05-20_23%3A14%3A05.patch","02/Jun/15 19:06;ijuma;KAFKA-2187_2015-06-02_20:05:50.patch;https://issues.apache.org/jira/secure/attachment/12736985/KAFKA-2187_2015-06-02_20%3A05%3A50.patch",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2015-06-01 02:19:46.88,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 22:27:41 UTC 2015,,,,,,,"0|i2elv3:",9223372036854775807,,nehanarkhede,,,,,,,,,,,,,,"20/May/15 22:02;ijuma;Issue resolved by pull request 3
[https://github.com/ijuma/kafka/pull/3]","20/May/15 22:03;ijuma;I was just testing the merge script and it seems to work great. :)","20/May/15 22:08;ijuma;Created reviewboard https://reviews.apache.org/r/34502/diff/
 against branch origin/trunk","20/May/15 22:14;ijuma;Updated reviewboard https://reviews.apache.org/r/34502/diff/
 against branch origin/trunk","20/May/15 22:24;ijuma;The following environment variables are of note:

{quote}
# Remote name which points to the GitHub site
PR_REMOTE_NAME = os.environ.get(""PR_REMOTE_NAME"", ""apache-github"")
# Remote name which points to Apache git
PUSH_REMOTE_NAME = os.environ.get(""PUSH_REMOTE_NAME"", ""apache"")
# ASF JIRA username
JIRA_USERNAME = os.environ.get(""JIRA_USERNAME"", """")
# ASF JIRA password
JIRA_PASSWORD = os.environ.get(""JIRA_PASSWORD"", """")
GITHUB_USER = os.environ.get(""GITHUB_USER"", ""apache"")
{quote}","28/May/15 08:14;ijuma;[~nehanarkhede], please let me know if there's anything I can do to make it easier for you to review and test this.","01/Jun/15 02:19;nehanarkhede;[~ijuma] This looks great. I can check it in, would you mind updating the wiki too https://cwiki.apache.org/confluence/display/KAFKA/Patch+submission+and+review ?
","01/Jun/15 07:44;ijuma;Sure, that was indeed my plan. Will do it soon.","02/Jun/15 17:59;nehanarkhede;[~ijuma] As I went through the patch again, I caught a few places that still refer to Spark instead of Kafka. If you can fix those, I'll check it in. ","02/Jun/15 19:06;ijuma;Updated reviewboard https://reviews.apache.org/r/34502/diff/
 against branch upstream/trunk","02/Jun/15 19:13;ijuma;Thanks [~nehanarkhede], I changed a couple of places (one of them was not strictly wrong, but better to avoid confusion). The other places are crediting the original script to Spark and linking to it. ","07/Jul/15 20:21;ijuma;[~nehanarkhede], I added a section to Patch Submission and Review page:

https://cwiki.apache.org/confluence/display/KAFKA/Patch+submission+and+review#Patchsubmissionandreview-MergingGitHubPullRequests

I also added a new page that targets contributors:

https://cwiki.apache.org/confluence/display/KAFKA/Contributing+Code+Changes

Feedback is welcome.","07/Jul/15 20:25;ijuma;Once we transition to the new way, we also need to update the ""Contributing"" page of the website.","09/Jul/15 20:35;gwenshap;I think that per Apache, we need to acknowledge Spark somewhere since we essentially lifted their code. LICENSE file, maybe?","09/Jul/15 20:42;ijuma;It is mentioned in the script itself (near the top). Happy to add it in LICENSE too if that's the right place.","09/Jul/15 20:48;gwenshap;Sorry, I missed that. I think its good and I don't see any legal verbiage about having in in LICENSE. 
(http://www.apache.org/dev/licensing-howto.html)","09/Jul/15 20:51;gwenshap;One more thing:
If I start a merge and cancel (say, by choosing 'n' when asked if I want to proceed), I'm left on a detached branch. Any chance the script can put me back in the original branch? or in trunk?","09/Jul/15 21:00;ijuma;Thanks for checking regarding the license.

Good question. I didn't add new functionality in the initial version of the script under the assumption that it's been in use for a while and is robust. I can check tomorrow if there's a good reason for the current behaviour under that scenario.","09/Jul/15 21:02;ijuma;In the meantime `git checkout -` should take you to the previous branch.","09/Jul/15 22:25;gwenshap;I accidentally committed this script as part of PR 73 (I did a git add, intending to commit this separately, and then up committing PR 73 from same branch).

I think the script is in good condition, so I'm not taking it out. Just file a new JIRA to fix the branch thing when you have a chance.","09/Jul/15 22:27;ijuma;OK, thank you.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inbuilt consumer offset management feature for kakfa,KAFKA-1000,12661913,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,tejasp,tejasp,tejasp,05/Aug/13 16:13,16/Jun/15 18:23,12/Jan/21 10:06,16/Jun/15 18:23,0.8.1,,,,,,,0.8.2.0,,,consumer,,,,,,5,features,,,,"Kafka currently stores offsets in zookeeper. This is a problem for several reasons. First it means the consumer must embed the zookeeper client which is not available in all languages. Secondly offset commits are actually quite frequent and Zookeeper does not scale this kind of high-write load. 

This Jira is for tracking the phase #2 of Offset Management [0]. Joel and I have been working on this. [1] is the overall design of the feature.

[0] : https://cwiki.apache.org/confluence/display/KAFKA/Offset+Management
[1] : https://cwiki.apache.org/confluence/display/KAFKA/Inbuilt+Consumer+Offset+Management
",,austinonthenet,diederik,eidi,jghoman,jmcnulty,junz,kieren,llai,noslowerdna,rangadi,rbinion,rektide,srikanth18,sslavic,tejasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,341917,,,2013-08-05 16:13:46.0,,,,,,,"0|i1myuf:",342223,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New metrics: ResponseQueueSize and BeingSentResponses,KAFKA-1597,12734263,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,alexismidon,alexismidon,alexismidon,14/Aug/14 21:42,14/Nov/14 19:20,12/Jan/21 10:06,16/Sep/14 03:36,,,,,,,,0.8.2.0,,,core,,,,,,0,features,,,,"This patch adds two metrics:

h3. ResponseQueueSize
As of 0.8.1, the sizes of the response queues are [reported as different metrics|https://github.com/apache/kafka/blob/0.8.1/core/src/main/scala/kafka/network/RequestChannel.scala#L127-L134] - one per processor thread. This is not very ideal for different reasons:
* charts have to sum the different metrics
* the metrics collection system might not support 'wild card queries' like {{sum:kafka.network.RequestChannel.Processor_*_ResponseQueueSize}} in which case monitoring now depends on the number of configured network threads
* monitoring the response by thread is not very valuable. However the global number of responses is useful.

* proposal*
So this patch exposes the total number of queued responses as a metric {{ResponseQueueSize}}

*implementation*
In {{RequestChannel}}, create a Gauge that adds up the size of the response queues.


h3. BeingSentResponses
As of 0.8.1, the processor threads will poll responses from the queues and attach them to the SelectionKey as fast as possible. The consequence of that is that the response queues are not a good indicator of the number of ""in-flight"" responses. The {{ServerSocketChannel}} acting as another queue of response to be sent.
The current metrics don't reflect the size of this ""buffer"", which is an issue.

*proposal*
This patch adds a gauge that keeps track of the number of responses being handled by the {{ServerSocketChannel}}.
That new metric is named ""BeingSentResponses"" (who said naming was hard?)

*implementation*
To calculate that metric, the patch adds up the number of SelectionKeys interested in writing, across processor threads.

Another approach could be to keep all in-flight responses in a data structure (let's say a map) shared by the processor threads. A response will be added to that map when dequeued from the response queue, and removed when the write is complete. The gauge will simply report the size of that map. I decided against that second approach as it is more intrusive and requires some additional bookkeeping to gather information already available through the {{SelectionKey}}'s

",,alexismidon,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1594,,,,,,,,,,,,,,,,,,,,"20/Aug/14 23:53;alexismidon;ResponseQueueSize.patch;https://issues.apache.org/jira/secure/attachment/12663275/ResponseQueueSize.patch","20/Aug/14 23:53;alexismidon;ResponsesBeingSent.patch;https://issues.apache.org/jira/secure/attachment/12663276/ResponsesBeingSent.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2014-08-18 22:28:43.092,,,false,,,,,,,,,,,,,,,,,,412203,,,Tue Sep 16 03:36:11 UTC 2014,,,,,,,"0|i1ywuv:",412192,,nehanarkhede,,,,,,,,,,,,,,"18/Aug/14 22:28;junrao;Thanks for the patch. For ResponseQueueSize, I am not sure how intuitive it is to report the total size across queues. Would reporting a max and avg across all response queues be better?

I want to clarify on BeingSentResponses. Is there a particular reason that you want to track this? I was thinking that we can track an InflightRequests on the broker that measures # of requests not completed by the broker.","18/Aug/14 23:21;alexismidon;* regarding {{ResponseQueueSize}}, I think I'm missing some context. Why is an analysis by response thread useful? Your experience might help me understand. It seems to me that the total number of queued responses is the only metric that helps understand the load on the Socket server (assuming it has some kind of back-pressure mechanism). 

* with {{BeingSentResponses}}, my goal was to avoid having a blind spot with the Socket server. Every element on the ""request handling chain"" is monitored except the last, the socket server. I considered tracking {{InflightResponses}} which would be {{ResponseQueueSize + BeingSentResponses}} but I decided against since having 2 metrics helps in monitoring 2 different component of the ""response chain"". 
I think that  {{InflightRequests}} will have the same issue: it would be a superset of already available metrics {{InflightRequest = RequestQueueSize + number of I/O threads+ ResponseQueueSize + BeingSentResponses}}. And I think the break down would be more useful.","19/Aug/14 14:34;junrao;The load on the broker is currently measured by NetworkProcessorAvgIdlePercent and RequestHandlerAvgIdlePercent. These are more direct measures of the load on the broker. Both ResponseQueueSize and BeingSentResponses measures the effect of the load. ResponseQueueSize could be useful to understand the memory footprint. From this perspective, I think the sum across all queues could make sense. However, I am still not sure how useful BeingSentResponses is. The breakdown of InflightRequest is a bit more complicated and looks like the following. I am not sure we need to measure every part of it.

InflightRequest = RequestQueueSize + RequestBeingProcessed (but response is not generated yet) + number of I/O threads+ ResponseQueueSize + BeingSentResponses","19/Aug/14 17:11;alexismidon;
# NetworkProcessorAvgIdlePercent and RequestHandlerAvgIdlePercent don't give information regarding response processing
# could you explain what RequestBeingProcessed is? how different is it from the number of I/O threads? My understanding of the code is that requests are processed by the I/O threads process requests.
https://github.com/apache/kafka/blob/0.8.1/core/src/main/scala/kafka/server/KafkaServer.scala#L90
https://dchtm6r471mui.cloudfront.net/hackpad.com_SIX7UAFo7hB_p.186494_1405370457117_KafkaThreadingModel.png
# Regarding InflightRequests. A request flows through a sequence of transformations/states. To me, measuring every component of that pipeline is the most basic and reliable way of understanding the behavior of the system. It also gives users the option to use the data in any way they might want. So i'd that exposing these root metrics would be great.  Considering that only BeingSentResponses is missing, the implementation cost is pretty low.



","19/Aug/14 18:25;nehanarkhede;+1 on measuring the responses being sent and also the ResponseQueueSize, can we please rename BeingSentResponses to ResponsesInFlight or ResponsesBeingSent?
","20/Aug/14 23:50;alexismidon;rename {{BeingSentResponses}} into {{ResponsesBeingSent}}","20/Aug/14 23:53;alexismidon;[~nehanarkhede] patch updated. thanks for your feedback!","16/Sep/14 03:33;nehanarkhede;+1. Thanks for the updated patch.","16/Sep/14 03:36;nehanarkhede;Pushed to trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2 new metrics,KAFKA-1594,12733930,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Duplicate,junrao,alexismidon,alexismidon,14/Aug/14 00:53,14/Aug/14 21:48,12/Jan/21 10:06,14/Aug/14 05:08,,,,,,,,,,,core,,,,,,2,features,,,,"This patch adds two metrics:

h3. ResponseQueueSize
As of 0.8.1, the sizes of the response queues are [reported as different metrics|https://github.com/apache/kafka/blob/0.8.1/core/src/main/scala/kafka/network/RequestChannel.scala#L127-L134] - one per processor thread. This is not very ideal for different reasons:
* charts have to sum the different metrics
* the metrics collection system might not support 'wild card queries' like {{sum:kafka.network.RequestChannel.Processor_*_ResponseQueueSize}} in which case monitoring now depends on the number of configured network threads
* monitoring the response by thread is not very valuable. However the global number of responses is useful.

* proposal*
So this patch exposes the total number of queued responses as a metric {{ResponseQueueSize}}

*implementation*
In {{RequestChannel}}, create a Gauge that adds up the size of the response queues.


h3. BeingSentResponses
As of 0.8.1, the processor threads will poll responses from the queues and attach them to the SelectionKey as fast as possible. The consequence of that is that the response queues are not a good indicator of the number of ""in-flight"" responses. The {{ServerSocketChannel}} acting as another queue of response to be sent.
The current metrics don't reflect the size of this ""buffer"", which is an issue.

*proposal*
This patch adds a gauge that keeps track of the number of responses being handled by the {{ServerSocketChannel}}.
That new metric is named ""BeingSentResponses"" (who said naming was hard?)

*implementation*
To calculate that metric, the patch adds up the number of SelectionKeys interested in writing, across processor threads.

Another approach could be to keep all in-flight responses in a data structure (let's say a map) shared by the processor threads. A response will be added to that map when dequeued from the response queue, and removed when the write is complete. The gauge will simply report the size of that map. I decided against that second approach as it is more intrusive and requires some additional bookkeeping to gather information already available through the {{SelectionKey}}'s

",,alexismidon,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/14 01:04;alexismidon;KAFKA-1594_BeingSentResponses.patch;https://issues.apache.org/jira/secure/attachment/12661602/KAFKA-1594_BeingSentResponses.patch","14/Aug/14 01:04;alexismidon;KAFKA-1594_ResponseQueueSize.patch;https://issues.apache.org/jira/secure/attachment/12661603/KAFKA-1594_ResponseQueueSize.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2014-08-14 05:08:35.389,,,false,,,,,,,,,,,,,,,,,,411958,,,Thu Aug 14 21:48:13 UTC 2014,,,,,,,"0|i1yvdb:",411947,,,,,,,,,,,,,,,,"14/Aug/14 01:04;alexismidon;adding 2 patch files","14/Aug/14 05:08;nehanarkhede;Duplicate of KAFKA-1593","14/Aug/14 15:48;alexismidon;Hi [~nehanarkhede]

could you please re-open this issue, it has more details than KAFKA-1593 and patches are also attached.
I closed KAFKA-1593.

Sorry for the confusion,

Alexis","14/Aug/14 21:34;nehanarkhede;[~alexismidon], that makes sense. However, it doesn't let me reopen the ticket :-(
We could either move the patches and info to KAFKA-1593 or just keep it here and follow up. Sorry.","14/Aug/14 21:43;alexismidon;you know what, I closed KAFKA-1593 thinking you could re-open KAFKA-1594. The irony.. ;)

I created a third ticket, KAFKA-1597.
","14/Aug/14 21:48;nehanarkhede;Assigning to [~junrao] for review.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
new metrics: ResponseQueueSize and BeingSentResponses,KAFKA-1593,12733929,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Duplicate,,alexismidon,alexismidon,14/Aug/14 00:53,14/Aug/14 15:45,12/Jan/21 10:06,14/Aug/14 15:45,,,,,,,,,,,core,,,,,,0,,,,,"
This patch adds two metrics:

h3. ResponseQueueSize
As of 0.8.1, the sizes of the response queues are [reported as different metrics|https://github.com/apache/kafka/blob/0.8.1/core/src/main/scala/kafka/network/RequestChannel.scala#L127-L134] - one per processor thread. This is not very ideal for different reasons:
* charts have to sum the different metrics
* the metrics collection system might not support 'wild card queries' like {{sum:kafka.network.RequestChannel.Processor_*_ResponseQueueSize}} in which case monitoring now depends on the number of configured network threads
* monitoring the response by thread is not very valuable. However the global number of responses is useful.

* proposal*
So this patch exposes the total number of queued responses as a metric {{ResponseQueueSize}}

*implementation*
In {{RequestChannel}}, create a Gauge that adds up the size of the response queues.


*BeingSentResponses
As of 0.8.1, the processor threads will poll responses from the queues and attach them to the SelectionKey as fast as possible. The consequence of that is that the response queues are not a good indicator of the number of ""in-flight"" responses. The {{ServerSocketChannel}} acting as another queue of response to be sent.
The current metrics don't reflect the size of this ""buffer"", which is an issue.

*proposal*
This patch adds a gauge that keeps track of the number of responses being handled by the {{ServerSocketChannel}}.
That new metric is named ""BeingSentResponses"" (who said naming was hard?)

*implementation*
To calculate that metric, the patch adds up the number of SelectionKeys interested in writing across processor threads.

An another approach could be to keep all in-flight responses in a data structure (let's say a map) shared by the processor thread. A response will be added to that map when dequeued from the response queue, and removed when the write is complete. I decided agains that second approach as it is more intrusive and requires some additional bookkeeping to gather information already available through the {{SelectionKey}}'s

",,alexismidon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,411957,,,Thu Aug 14 15:45:49 UTC 2014,,,,,,,"0|i1yvd3:",411946,,,,,,,,,,,,,,,,"14/Aug/14 15:45;alexismidon;created by error as a duplicate of KAFKA-1594",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Kafka StatsD metric reporter,KAFKA-1572,12732074,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Not A Problem,,jackiewang518,jackiewang518,05/Aug/14 17:12,05/Aug/14 20:59,12/Jan/21 10:06,05/Aug/14 20:57,0.8.0,,,,,,,,,,tools,,,,,,0,,,,,"Myfitnesspal.com uses statsD for data collection and aggregation. Since statsD is pretty popular, adding a kafka statsD metric reporter seems needed.

I have implemented one and open source it at https://github.com/myfitnesspal/kafka-statsd-reporter

We are using it in production environment at myfitnespal.com.

Can you refer this statsd reporter in your general jmx-reporter page: https://cwiki.apache.org/confluence/display/KAFKA/JMX+Reporters
so that other people can use it?

",,guozhang,jackiewang518,joestein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-08-05 20:48:40.675,,,false,,,,,,,,,,,,,,,,,,410103,,,Tue Aug 05 20:59:51 UTC 2014,,,,,,,"0|i1yk47:",410097,,,,,,,,,,,,,,,,"05/Aug/14 20:48;guozhang;Hi Jianwen,

After Kafka 0.9 we will be moving to the SLF4J logging framework, which supports a variable of logging libraries such as log4j, jdk logging, etc.

http://www.slf4j.org/

Is statsD compatible with this framework?
","05/Aug/14 20:56;joestein;[~jackiewang518] you don't need to open a ticket for wiki changes, what is your username for the wiki? I will grant you access you can update it yourself.  moving forward feel free to send this on the dev list","05/Aug/14 20:59;joestein;[~jackiewang518] I updated the page, i actually had another reporter to update and added yours too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move to java CRC32 implementation,KAFKA-374,12595927,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jkreps,jkreps,jkreps,26/Jun/12 15:37,19/Jun/14 05:17,12/Jan/21 10:06,16/Dec/12 19:42,0.8.0,,,,,,,,,,core,,,,,,0,newbie,,,,"We keep a per-record crc32. This is fairly cheap algorithm, but the java implementation uses JNI and it seems to be a bit expensive for small records. I have seen this before in Kafka profiles, and I noticed it on another application I was working on. Basically with small records the native implementation can only checksum < 100MB/sec. Hadoop has done some analysis of this and replaced it with a Java implementation that is 2x faster for large values and 5-10x faster for small values. Details are here HADOOP-6148.

We should do a quick read/write benchmark on log and message set iteration and see if this improves things.",,jkreps,junrao,mumrah,nehanarkhede,scott_carey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/12 00:01;jkreps;KAFKA-374-draft.patch;https://issues.apache.org/jira/secure/attachment/12533564/KAFKA-374-draft.patch","07/Dec/12 18:33;mumrah;KAFKA-374.patch;https://issues.apache.org/jira/secure/attachment/12559921/KAFKA-374.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2012-06-27 16:47:11.984,,,false,,,,,,,,,,,,,,,,,,241660,,,Sun Dec 16 19:42:15 UTC 2012,,,,,,,"0|i029nr:",11166,,,,,,,,,,,,,,,,"27/Jun/12 00:01;jkreps;The following is a draft patch that transliterates the Hadoop CRC32 implementation into scala and swaps it in for our own crc32 implementation. I ran a sanity check on a few hundred million values to check I got the same crcs.

The CRC code is essentially unreadable. So it is not something you can really edit.

I wrote a simple test that just sequentially creates messages (which requires computing the checksum). On this test I see about a 30-40% improvement, roughly across the board on Linux. I will include full results when it completes.

I suspect this really only helps very trivial cases: benchmarks, mirroring, or other applications which do no real processing. It should speed up both the broker, the producer, and the consumer.","27/Jun/12 16:42;jkreps;Here are full performance results

The size is in bytes and the value for native/java is the nanoseconds per message averaged over a large number of messages:

size	native	java	improvement
16	149.47	108.11	27.7%
32	197.8	149.78	24.3%
64	291.01	219.89	24.4%
128	487.36	357.64	26.6%
256	892.78	631.15	29.3%
512	1774.22	1251.4	29.5%
1024	3412.79	2470.58	27.6%
2048	6594.28	4421.38	33.0%
4096	13121.85	8751.19	33.3%
8192	25689.03	18173.61	29.3%
16384	51258.21	36278.3	29.2%
32768	103584.61	73240.5	29.3%
65536	207569.05	146748.51	29.3%
131072	415893.86	292083.12	29.8%

I suspect there is still some scala numeric boxing magic happening here that would be good to get rid of.","27/Jun/12 16:47;junrao;Should the native and the java column be reversed? Now, it reads that java is faster than native.","27/Jun/12 17:07;jkreps;Yeah, that's a little unclear. The ""java"" column is the ""pure jvm"" scala implementation in the patch and the native version is java.util.zip.CRC32 which uses JNI to call a C CRC library.","07/Dec/12 04:30;mumrah;I pulled in the pure-Java implementation from Hadoop for comparison:

https://docs.google.com/spreadsheet/pub?key=0AksaPvYfWJQFdG5fZFNyWnpOUzZfZEtnVl9YZ21FWUE&output=html

Pure Java has a slight advantage over pure Scala, both have good speedup over JNI. This was done against 0.8 using the same test code in the patch","07/Dec/12 06:20;jkreps;Ooo very nice.","07/Dec/12 18:38;mumrah;Not sure how you guys feel about having Java in the source tree, but I attached a patch with the pure Java implementation (and the other stuff from [~jkreps] original patch).","11/Dec/12 05:42;junrao;Should this be a post 0.8 item?","11/Dec/12 13:10;jkreps;Yes, this should go on trunk.","13/Dec/12 03:59;scott_carey;Awesome.  I was just profiling some Kafka 0.7.1 stuff and noticed the CRC eating up time, and since I co-authored the pure java hadoop one, was just about to file a JIRA here....


On a related note, it would be nice to offload the CRC and decompression to a different thread than the user's thread.  Does 0.8's client do all of this work on the user thread like 0.7.1 ?  Our 0.7.1 consumers are often throughput bound by CPU including kafka code.    If the client is currently single-threaded I can file a JIRA with some ideas.","13/Dec/12 04:52;jkreps;Hey Scott, currently the implementation of the consumer is that there is a background thread that fetches data and populates a queue of data chunks which are handed off to the user's iterators. A single consumer client can feed many iterators in (potentially) many threads. The goal of this design was specifically to support a large thread pool of processors while still maintaining ordered consumption on a per-partition basis. So although a background thread would be one way to increase parallelism I think it is actually the harder one to reason about since now there are multiple thread pools to tune. I think just increasing the number of user threads/iterators is probably the best approach. ","13/Dec/12 18:52;scott_carey;I was thinking of using Akka Actors, as they can provide the necessary ordering guarantees, do not require thread pool tuning, and should be easy to reason about.  The performance would likely be better than producer/consumer thread chains too, since chained actors often run in the same thread and avoid processor cache thrashing.","14/Dec/12 16:09;mumrah;Akka seems a bit overkill for this (although it does have some nice properties). It would be interesting to refactor the threading in Kafka with Akka and see what kind of performance differences there are (certainly beyond the scope of this JIRA).

As for the CRC implementation, is there consensus of what do here - Java or Scala?

I say +1 for Java since no one will need to modify this code and it doesn't really matter that it's not Scala.","16/Dec/12 19:42;jkreps;Checked in the java version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ruby client needs to support new compression byte,KAFKA-163,12527691,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Won't Fix,,araddon,araddon,19/Oct/11 02:10,04/Apr/13 20:43,12/Jan/21 10:06,04/Apr/13 20:43,,,,,,,,,,,clients,,,,,,0,,,,,Ruby client updates,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/11 02:12;araddon;KAFKA-158.patch;https://issues.apache.org/jira/secure/attachment/12499626/KAFKA-158.patch","19/Oct/11 16:54;araddon;KAFKA-163.patch;https://issues.apache.org/jira/secure/attachment/12499712/KAFKA-163.patch","20/Oct/11 16:49;nehanarkhede;rat.out;https://issues.apache.org/jira/secure/attachment/12499885/rat.out",,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,2011-10-19 16:11:30.694,,,false,,,,,,,,,,,,,,,,,,89011,,,Thu Apr 04 19:46:56 UTC 2013,,,,,,,"0|i029zj:",11219,,,,,,,,,,,,,,,,"19/Oct/11 02:12;araddon;tested against the consumer/producer shell (scala clients) as well as go producer/consumers.","19/Oct/11 16:11;junrao;This seems to be a patch for 158, not 163.","19/Oct/11 16:54;araddon;Try2","20/Oct/11 16:49;nehanarkhede;-1. Rat fails. Attaching the output here.","12/Nov/11 21:19;jkreps;Hi Aaron, I think there is missing licensing info. Can you update that and trying running the rat tool under bin/ to verify it is all apache compliant?","04/Apr/13 19:46;lanzaa;Old bug. Client libraries, such as this Ruby library, are no longer being maintained in the main project.

Please close. Won't Fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a default producer for receiving messages from STDIN,KAFKA-130,12521107,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jkreps,felixgv,felixgv,01/Sep/11 20:19,12/Oct/11 01:22,12/Jan/21 10:06,12/Oct/11 01:22,0.6,,,,,,,0.7,,,,,,,,,0,,,,,"It would be useful to provide a default producer we can fire up that reads from STDIN and sends one message per line to the broker.

The most obvious use case for this is to pipe a tail -f command into it, to tail log files as they're generated. Making it depend on STDIN seems more flexible than a producer that just tails files though.",,felixgv,sharadag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/11 03:59;jkreps;kafka-console-producer.patch;https://issues.apache.org/jira/secure/attachment/12497421/kafka-console-producer.patch","11/Sep/11 15:06;jkreps;kafka-console-producer.patch;https://issues.apache.org/jira/secure/attachment/12493954/kafka-console-producer.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-09-11 15:06:08.531,,,false,,,,,,,,,,,,,,,,,,43431,,,Mon Oct 03 04:54:11 UTC 2011,,,,,,,"0|i15z2v:",242983,,,,,,,,,,,,,,,,"11/Sep/11 15:06;jkreps;Add a console producer for kafka that reads from standard in.","11/Sep/11 15:13;jkreps;Usage is the following:

jkreps-mn:kafka-git jkreps$ bin/kafka-run-class.sh kafka.producer.ConsoleProducer 
Missing required argument ""[topic]""
Option                                  Description                            
------                                  -----------                            
--batch-size <Integer: size>            Number of messages to send in a single 
                                          batch if they are not being sent     
                                          synchronously. (default: 200)        
--compress                              If set, messages batches are sent      
                                          compressed                           
--line-reader <reader_class>            The class name of the class to use for 
                                          reading lines from standard in. By   
                                          default each line is read as a       
                                          seperate message. (default: kafka.   
                                          producer.                            
                                          ConsoleProducer$LineMessageReader)   
--message-encoder <encoder_class>       The class name of the message encoder  
                                          implementation to use. (default:     
                                          kafka.serializer.StringEncoder)      
--property <prop>                                                              
--sync                                  If set message send requests to the    
                                          brokers are synchronously, one at a  
                                          time as they arrive.                 
--timeout <Long: timeout_ms>            If set and the producer is running in  
                                          asynchronous mode, this gives the    
                                          maximum amount of time a message     
                                          will queue awaiting suffient batch   
                                          size. The value is given in ms.      
                                          (default: 1000)                      
--topic <topic>                         REQUIRED: The topic id to produce      
                                          messages to.                         
--zookeeper <connection_string>         REQUIRED: The zookeeper connection     
                                          string for the kafka zookeeper       
                                          instance in the form HOST:PORT       
                                          [/CHROOT].

It reads from standard input so it can take interactive input or else you can pipe stuff to it:
cat ~/some_log_file.log| bin/kafka-run-class.sh kafka.producer.ConsoleProducer --topic test --zookeeper localhost:2181

By default it reads lines from the log and sends them as string messages to Kafka.

Both the class used to read objects and the Kafka producer serializer can be customized from the arguments. The serializer is the normal kafka.serializer.Encoder interface, and the class used to read messages from System.in should extend the MessageReader abstract class (trait), which has the following methods:

  trait MessageReader { 
    def init(inputStream: InputStream, props: Properties) {}
    def readMessage(): AnyRef
    def close() {}
  }

AKA:

interface MessageReader {
  public void init(InputStream in, Properties props);
  public Object readMessage();
  public void close();
}","20/Sep/11 17:43;junrao;Thanks for the patch Jay. It looks good. About the propertyOpt, could you add a description about its purpose and format? Also, it's not used in the init method of LineMessageReader.","02/Oct/11 15:33;jkreps;Hey Jun, the idea of the properties option is this: we want to give a way for the user to plug in code for parsing the input file, and that code may require some options of its own. For example you might need to give the charset of the input file or some other info. To make this possible there needs to be a way to pass config through this code to the user's plug-in message reader. Obviously for our usage, the thing I had in mind was being able to pass in an avro schema or url to get one. This is all fairly advanced functionality, so the user doesn't need to mess with it unless they want to read in messages a particular way.

LineMessageReader is a default implementation of this interface that just turns one line into an input to the producer as a string, used in combination with the StringEncoder it does the normal logging thing of sending one line as a message.

Hopefully that makes sense...","02/Oct/11 15:55;junrao;Ok, that makes sense. Just document the format in the description of the property and we can commit the patch.","03/Oct/11 03:55;jkreps;I will add some docs, can I get a +1 on this?","03/Oct/11 03:59;jkreps;Added to the --help info on --property.","03/Oct/11 04:54;junrao;+1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add optional mx4j support to expose jmx over http,KAFKA-78,12515871,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,cburroughs,cburroughs,cburroughs,31/Jul/11 16:09,06/Oct/11 05:12,12/Jan/21 10:06,06/Oct/11 05:11,,,,,,,,0.7,,,,,,,,,0,,,,,In the fun world of operations and monitoring sometimes HTTP is a better choice than 'real' jmx for alerting.  mx4j exposes jmx beans over HTTP (sadly with xml).  Attached patch is a scala port of CASSANDRA-1068 that optionally loads mx4j if it is present on the classpath.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/11 16:10;cburroughs;k78-v1.txt;https://issues.apache.org/jira/secure/attachment/12488359/k78-v1.txt","08/Aug/11 19:42;cburroughs;k78-v2.txt;https://issues.apache.org/jira/secure/attachment/12489732/k78-v2.txt","04/Oct/11 02:05;cburroughs;k78-v3.txt;https://issues.apache.org/jira/secure/attachment/12497586/k78-v3.txt","04/Oct/11 22:58;cburroughs;k78-v4.txt;https://issues.apache.org/jira/secure/attachment/12497723/k78-v4.txt",,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2011-08-02 21:02:12.413,,,false,,,,,,,,,,,,,,,,,,43851,,,Thu Oct 06 05:11:58 UTC 2011,,,,,,,"0|i0s94v:",162956,,,,,,,,,,,,,,,,"02/Aug/11 21:02;jkreps;This is nice. Couple of things:
- Do we have anyone who would directly make use of this? Without that these things tend to rot...
- Jun/Neha, we are already using mx4j what happens to someone already using mx4j if this code runs too?
- We have tried to make the kafka broker easily embeddable. I wonder if we should not just let people add additional ""container"" functionality that way rather than us trying to add it all into the kafka itself. What I mean is you can easily make an XyzKafkaWrapper.java that does whatever custom logic you want in addition to starting the Kafka broker.","02/Aug/11 21:15;bmatheny;I would use it. Right now all of our production JVM code exposes stats over HTTP, we mostly avoid JMX.

On the embeddable broker, I'd sign up to help spec/dev it if needed. I asked if that was possible in the IRC channel last week and the short answer was, ""not really"".","05/Aug/11 01:03;jkreps;The kafka broker is just an object that takes a java.util.Properties in its constructor so it should be very easy to embed. Was there a reason it couldn't be embedded? We do this in tests and indeed this is how linkedin does it so i think it does work...

I think this is an important use case, the question is do we want to try to build this in or let other people create their own wrappers. I could see going either way, but my experience is that a lot of this packaging stuff tends to be fairly environment specific so even if someone else is using mx4j they may have a different incompatible version so it is just better to let people embed and do their own thing. (Actually LinkedIn is using mx4j in this very way and I am a little concerned this would conflict with out usage...so it would be good if there was an option so it could be disabled at the least).

So I think we should either:
1. Document how to embed the broker to make it easy for people to do these kinds of things OR
2. Make it so that this can be disabled","05/Aug/11 01:27;junrao;I think if this is disabled by default through a config, then it should be fine. Some minor comments on the patch:
1. please remove unused import javax.management.MBeanServer
2. fix typo scale -> scala","08/Aug/11 19:41;cburroughs;I like having a clean separation between ""this is the kafka class""  and ""this is the class for the kafka daemon (and will instantiate the kafka server class)"".

If you try to start mx4j twice you will get something like this in the logs, but I think everything will function correctly.

[2011-08-08 15:34:43,470] WARN Could not start register mbean in JMX (kafka.utils.Mx4jLoader$)
javax.management.InstanceAlreadyExistsException: system:name=http
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:453)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1484)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:963)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)
	at kafka.utils.Mx4jLoader$.maybeLoad(Mx4jLoader.scala:47)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:74)
	at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:40)
	at kafka.Kafka$.main(Kafka.scala:56)
	at kafka.Kafka.main(Kafka.scala)
","08/Aug/11 19:42;cburroughs;v2 fixes typos and imports","12/Aug/11 13:07;cburroughs;Jay, are you satisfied with the multiple invitations of mx4j case?","12/Aug/11 16:26;jkreps;Shouldn't the mx4j port come from kafka config, why make it a system property?

What happens if two kafka servers are instantiated in the same jvm? We do this for testing quite a lot.

We should also have a config to disable this feature.","03/Oct/11 17:10;junrao;It doesn't looks like that we have a patch that addresses all the concerns here. Perhaps we can defer this to 0.8?","04/Oct/11 02:05;cburroughs;Sorry for the delay.

 - License header format updated.
 - Added -Dmx4jdisable to disable mx4j even if it's on the classpath.

They are system properties because it only makes sense to have one mx4j port per JVM (like jmx ports), not one per embedded broker.  If there is a case there you have a lot of embedded brokers, and mx4j (ie not a test), and the log messages are annoying I can add some more complicated ""do only once"" code.

Unit tests should be unaffected since we don't have a hard dep on mx4j.","04/Oct/11 17:46;junrao;Chris, thanks for the patch. Do you think that we can change mx4jdisable to mx4jenable so that if the user doesn't specify the property, the mx4j part is not touched? This way, users who already use mx4j but don't want mx4j to be loaded in the embedded Kafka broker are not affected by default.","04/Oct/11 22:58;cburroughs;v4 defaults to off (even with mx4j is on the classpath).","05/Oct/11 01:03;junrao;+1. Thanks for the patch, Chris.","06/Oct/11 05:11;junrao;Committed this with a minor change (rename property mx4jenable to kafka_mx4jenable). Thanks for the patch, Chris.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tool to watch consumer offsets and lag,KAFKA-127,12520993,New Feature,Resolved,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,jjkoshy,jjkoshy,01/Sep/11 00:01,10/Sep/11 00:37,12/Jan/21 10:06,10/Sep/11 00:37,,,,,,,,,,,,,,,,,0,,,,,"I have a simple script to watch consumer offsets and lag that was useful to us in production debugging. Unfortunately, it is slow and does not work on Macs due to some non-portable awk. I will write up a scala implementation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/11 01:25;jjkoshy;KAFKA-127_v1.patch;https://issues.apache.org/jira/secure/attachment/12493711/KAFKA-127_v1.patch","09/Sep/11 20:52;jjkoshy;KAFKA-127_v2.patch;https://issues.apache.org/jira/secure/attachment/12493852/KAFKA-127_v2.patch",,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2011-09-09 23:50:26.523,,,false,,,,,,,,,,,,,,,,,,61695,,,Fri Sep 09 23:50:26 UTC 2011,,,,,,,"0|i15z27:",242980,,,,,,,,,,,,,,,,"09/Sep/11 01:26;jjkoshy;Running this tool spits out some zk/zkclient logging. Not sure if we want to default to warn level for org.apache.zookeeper and org.I0Itec","09/Sep/11 20:52;jjkoshy;After trying this out a bit more I decided to incorporate a small formatting improvement.","09/Sep/11 23:50;junrao;+1. Thanks, Joel. Just committed this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Increase ProducerPerformance precision by using nanoTime,KAFKA-7722,13203847,New Feature,In Progress,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,,lu.kevin,lu.kevin,lu.kevin,11/Dec/18 22:54,11/Dec/18 22:54,12/Jan/21 10:06,,,,,,,,,,,,tools,,,,,,0,,,,,https://cwiki.apache.org/confluence/display/KAFKA/KIP-403%3A+Increase+ProducerPerformance+precision+by+using+nanoTime,,lu.kevin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-12-11 22:54:52.0,,,,,,,"0|s01f4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fork or create link to node kafka client,KAFKA-80,12517814,New Feature,Closed,KAFKA,Kafka,software,junrao,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,,marcuswestin,marcuswestin,03/Aug/11 17:32,04/Apr/13 20:42,12/Jan/21 10:06,04/Apr/13 19:44,,,,,,,,,,,clients,,,,,,0,client,node,nodejs,,"https://github.com/marcuswestin/node-kafka has a working implementation of simple client in nodejs. It would be nice if it can be listed among the known kafka clients to avoid reinventing the wheel if someone needs a node implementation.

Cheers!
Marcus",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-08-12 13:15:45.487,,,false,,,,,,,,,,,,,,,,,,65113,,,Thu Apr 04 19:41:42 UTC 2013,,,,,,,"0|i02a33:",11235,,,,,,,,,,,,,,,,"12/Aug/11 13:15;cburroughs;Marcus I'm not sure what you mean by fork. Are you requesting that we include this in the main Kafka tree (in which case ASF policy would require you to attach it at a patch and click the legal checkbox), or would you prefer to have just a link in the README & wiki and manage the code yourself  on github?","12/Aug/11 17:34;marcuswestin;Hey Chris,

Whichever is the project maintainers' preference. If you like to have the clients in the kafka tree then I'll submit it as a patch; if not then a link in the README sounds great too.

Cheers!","12/Aug/11 19:33;jkreps;Hi Marcus, thanks for the great contribution!

I think if the code is in reasonably good shape we prefer to keep it with the main code base unless you are doing very active development and would prefer to have your own repo. This helps people find the code, send us patches, and will help us to add integration tests that run all the clients against the server.

One thing we need to be able to do this though is make sure we have someone who will maintain the code on an ongoing basis (adapt to any binary format changes, fix bugs, etc). If you are up for doing that then I would vote we keep it with the main code.

","13/Aug/11 00:56;marcuswestin;Yeah, if protocol changes are described well and there's working example code I'm happy to keep the node client up to date. Ok, I'll get a formal patch together.

Cheers!
Marcus","04/Apr/13 19:41;lanzaa;There is a link to the Node.JS client on the wiki page.
https://cwiki.apache.org/confluence/display/KAFKA/Clients#Clients-Node.js

This bug should be closed as fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
