Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Supercedes),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (Hadoop Flags),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Tags),Custom field (Tags),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Added ignored test for async connection that runs retries just so can check how long it takes and that retrying is happening,HBASE-25561,13357619,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Trivial,,,stack,stack,08/Feb/21 20:04,08/Feb/21 21:11,18/Feb/21 10:10,,,,,,,,,,test,,,,,,0,,,"There's an old @Ignored test in TestClientNoCluster that has been useful from time to time figuring how long N retries takes and reproducing certain production 'Connection rejected', etc., stack traces. This issue adds an @Ignored method for the async connection case so can do similar for async (I needed it just now because it looked like retries at a certain juncture were not happening -- turns out they are... this addition helped me prove this the case).",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2021-02-08 20:04:04.0,,,,,,,"0|z0ni80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestMasterRegionCompaction is flaky,HBASE-25157,13333801,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,ndimiduk,ndimiduk,05/Oct/20 21:34,05/Oct/20 21:34,18/Feb/21 10:10,,3.0.0-alpha-1,,,,,,,,master,test,,,,,0,,,"{noformat}
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 10.298 s <<< FAILURE! - in org.apache.hadoop.hbase.master.region.TestMasterRegionCompaction
[ERROR] org.apache.hadoop.hbase.master.region.TestMasterRegionCompaction.test  Time elapsed: 10.269 s  <<< FAILURE!
java.lang.AssertionError: expected:<1> but was:<4>
	at org.apache.hadoop.hbase.master.region.TestMasterRegionCompaction.test(TestMasterRegionCompaction.java:136)
{noformat}

This test uses calls to {{Thread.sleep}} of fixed durations instead of the {{waitFor}} pattern of retries. It's looking for side-effects of a background thread that may not be scheduled on precisely the cadence that the test expects, especially in an overloaded test environment.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2020-10-05 21:34:39.0,,,,,,,"0|z0jfnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The postAppend method in SimpleRegionObserver should return result instead of null,HBASE-24959,13324786,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,wenbang,wenbang,wenbang,27/Aug/20 07:27,27/Aug/20 07:29,18/Feb/21 10:10,,1.4.13,1.5.0,2.3.1,3.0.0-alpha-1,,,,,,,,,,,0,,,"I found this error when testing batch append with SimpleRegionObserver：

java.lang.RuntimeException: java.lang.IllegalStateException: actions.getActionCount=2, actionResult.getResultOrExceptionCount=0 for region type: REGION_NAME

 

The postAppend method in SimpleRegionObserver should return result instead of null.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2020-08-27 07:27:09.0,,,,,,,"0|z0i4kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky Test TestRegionReplicas#testVerifySecondaryAbilityToReadWithOnFiles,HBASE-24708,13316011,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,huaxiangsun,huaxiangsun,10/Jul/20 00:13,18/Jul/20 04:38,18/Feb/21 10:10,,2.3.0,,,,,,,,test,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-07-18 04:38:04.083,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 18 04:38:04 UTC 2020,,,,,,,"0|z0gmlc:",9223372036854775807,,,,,,,,,,,,,,,,,"18/Jul/20 04:38;liuml07;Do you have links to builds / stack trace?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Flakey] TestVerifyReplication.testVerifyRepJob & TestVerifyReplicationAdjunct.testVerifyReplicationWithSnapshotSupport,HBASE-24677,13314904,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,vjasani,vjasani,03/Jul/20 14:17,03/Jul/20 14:23,18/Feb/21 10:10,,2.3.0,,,,,,,,,,,,,,0,flakey,test,"Attaching sunfire reports xml for details.

For both reports, on high level, I see some hanging threads for ReadOnlyZKClient and Replica not found for a datablock related Exceptions.

[^TEST-org.apache.hadoop.hbase.replication.TestVerifyReplication.xml]",,,,,,,,,,,,,,"03/Jul/20 14:16;vjasani;TEST-org.apache.hadoop.hbase.replication.TestVerifyReplication.xml;https://issues.apache.org/jira/secure/attachment/13007014/TEST-org.apache.hadoop.hbase.replication.TestVerifyReplication.xml","03/Jul/20 14:16;vjasani;TEST-org.apache.hadoop.hbase.replication.TestVerifyReplicationAdjunct.xml;https://issues.apache.org/jira/secure/attachment/13007013/TEST-org.apache.hadoop.hbase.replication.TestVerifyReplicationAdjunct.xml",,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2020-07-03 14:17:01.0,,,,,,,"0|z0gft4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[fakey test] TestInfoServersACL testJmxAvailableForAdmin,HBASE-24330,13302903,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,ndimiduk,ndimiduk,05/May/20 15:34,05/May/20 15:34,18/Feb/21 10:10,,2.3.0,,,,,,,,test,,,,,,0,,,"There was a failure on branch-2.3 last night, https://builds.apache.org/job/HBase%20Nightly/job/branch-2.3/65/testReport/

This test failed for JDK11 but not JDK8.

Before the failed assertion, I do see a log from the loop on line 312:

{noformat}
2020-05-05 10:57:45,323 INFO  [Time-limited test] http.TestInfoServersACL$11(312): Hadoop:service=HBase,name=StartupProgress
{noformat}

This indicates to me that the HBase portion of the process hadn't finished coming online yet, and so the precondition was not yet satisfied. Just speculation though; I don't understand why the same bean wouldn't be available to the admin user as well.

Furthermore, looks like the test does not clean up after itself correctly. On retry, the subsequent attempt flat-out failed with

{noformat}
Failed to load or create keytab /home/jenkins/jenkins-slave/workspace/HBase_Nightly_branch-2.3/component/hbase-server/target/test-data/f2e84dcf-0d1f-fe85-3519-4fcb37058d39/keytab

org.apache.kerby.kerberos.kerb.KrbException: Failed to load or create keytab /home/jenkins/jenkins-slave/workspace/HBase_Nightly_branch-2.3/component/hbase-server/target/test-data/f2e84dcf-0d1f-fe85-3519-4fcb37058d39/keytab
	at org.apache.hadoop.hbase.http.TestInfoServersACL.beforeClass(TestInfoServersACL.java:114)
Caused by: java.io.IOException: No such file or directory
	at org.apache.hadoop.hbase.http.TestInfoServersACL.beforeClass(TestInfoServersACL.java:114)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2020-05-05 15:34:55.0,,,,,,,"0|z0eed4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestJMXConnectorServer can fail to start the minicluster due to it's port already having been chosen by another test.,HBASE-24325,13302772,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,markrmiller,markrmiller,05/May/20 01:20,05/May/20 05:16,18/Feb/21 10:10,,,,,,,,,,test,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2020-05-05 01:20:59.0,,,,,,,"0|z0edk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Switch to `RawLocalFileSystem` as default FS for tests,HBASE-24127,13296660,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,ndimiduk,ndimiduk,06/Apr/20 21:01,07/Apr/20 14:16,18/Feb/21 10:10,,,,,,,,,,test,,,,,,0,,,"In the PR discussion on HBASE-24106, [~elserj] [mentioned|https://github.com/apache/hbase/pull/1422#discussion_r403789851] that another project has had success with switching its unit test suite to use {{RawLocalFileSystem}} from {{LocalFileSystem}}. This is discussed in the context of the availability of the {{hsync}} and {{hflush}} stream capabilities, which are not available on {{LocalFileSystem}}.

Perhaps we should also consider doing the same for our default {{standalone}} user experience. I don't know if doing so should be a separate task from this one, or if completing one is effectively performing the implementation for the other.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-04-07 01:44:42.653,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 07 14:16:10 UTC 2020,,,,,,,"0|z0dc68:",9223372036854775807,,,,,,,,,,,,,,,,,"07/Apr/20 01:44;zhangduo;It seems to be a bug of LocalFileSystem? LocalFileSystem is just a wrapper on the RawLocalFileSystem, so it does not make sense that RawLocalFileSystem support hflush/hsync while LocalFileSystem does not...","07/Apr/20 14:16;elserj;I couldn't tell you why it is the way it is, just that it's been this way since Hadoop 2.6 times :)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[flakey test] TestAccessController3,HBASE-24046,13293901,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,ndimiduk,ndimiduk,25/Mar/20 18:24,25/Mar/20 18:24,18/Feb/21 10:10,,2.4.0,,,,,,,,test,,,,,,0,,,"Saw this failure on JDK8/Hadoop2, https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2565/

Not much to go on.

{noformat}
java.lang.AssertionError: region server should have aborted due to FaultyAccessController
	at org.apache.hadoop.hbase.security.access.TestAccessController3.tearDownAfterClass(TestAccessController3.java:205)
{noformat}",,,,,,,,,,,,,,"25/Mar/20 18:24;ndimiduk;TEST-org.apache.hadoop.hbase.security.access.TestAccessController3.xml;https://issues.apache.org/jira/secure/attachment/12997712/TEST-org.apache.hadoop.hbase.security.access.TestAccessController3.xml",,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2020-03-25 18:24:27.0,,,,,,,"0|z0cwls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rubocop configuration needs updated,HBASE-24044,13293873,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,ndimiduk,ndimiduk,25/Mar/20 17:00,25/Mar/20 23:14,18/Feb/21 10:10,,2.4.0,,,,,,,,build,,,,,,0,,,"From https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2565/consoleFull

{noformat}
15:30:40  Running rubocop against identified ruby scripts.
15:30:40  The following cops were added to RuboCop, but are not configured. Please set Enabled to either `true` or `false` in your `.rubocop.yml` file:
15:30:40   - Style/HashEachMethods
15:30:40   - Style/HashTransformKeys
15:30:40   - Style/HashTransformValues
{noformat}

Is out rubocop versions somehow not pinned? HBASE-23943 was just fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-25 22:53:44.521,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 25 23:14:07 UTC 2020,,,,,,,"0|z0cwfk:",9223372036854775807,,,,,,,,,,,,,,,,,"25/Mar/20 22:53;janh;It's the same Rubocop version. Haven't seen this messages on master yet. Probably would be good to go against master first and then push down the changes. Does this problem also exist in branch-1?","25/Mar/20 23:14;ndimiduk;rubocop.yaml and Dockerfiles are identical on master, branch-2. Why do we see it on branch-2 but not master?

branch-1 has it's own dockerfile with its own rubocop install path, and looks like it's working there.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestUsersOperationsWithSecureHadoop fails when an existing ticket is present,HBASE-23970,13291184,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,ndimiduk,ndimiduk,11/Mar/20 21:54,08/Feb/21 11:32,18/Feb/21 10:10,,2.3.0,3.0.0-alpha-1,,,,,,,security,test,,,,,0,,,"{{TestUsersOperationsWithSecureHadoop}} makes assumptions about the state of the machine environment. When i have an existing ticket issued and active (shown via {{klist}}), this test fails. Signing out of all active sessions allows the test to again pass.

{noformat}
Caused by: org.apache.hadoop.security.authentication.util.KerberosName$NoMatchingRule: No rules applied to me@EXAMPLE.COM
        at org.apache.hadoop.hbase.security.TestUsersOperationsWithSecureHadoop.testAuthUtilLogin(TestUsersOperationsWithSecureHadoop.java:146)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-13 10:10:11.332,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 29 07:15:42 UTC 2020,,,,,,,"0|z0cfu8:",9223372036854775807,,,,,,,,,,,,,,,,,"13/Mar/20 10:10;wenfeiyi666;Hello, can you be specific? I tested it and nothing happened","13/Mar/20 15:26;ndimiduk;I authenticate myself to a kerberos authority and have a local ticket issued, then run the test; the test fails. When i forcefully expire said ticket, and the output of {{klist}} is empty, the test passes.","21/Mar/20 03:37;wenfeiyi666; help me review this PR，thanks!","22/Mar/20 23:13;stack;When TestUsersOperationsWithSecureHadoop runs, it leaves behind a ticket so if you rerun tests, they fail w/ this complaint.","22/Mar/20 23:25;stack;[~wenfeiyi666] Did you see the comments on https://github.com/apache/hbase/pull/1293 ?","24/Mar/20 03:43;wenfeiyi666;sorry, I didn't notice it, Thanks for reminding. I commit a new patch.","29/Sep/20 07:15;wenfeiyi666;ping [~ndimiduk] and [~stack]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Split TestFromClientSide; it takes too long to complete timing out",HBASE-23963,13290922,Test,Reopened,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,stack,stack,stack,10/Mar/20 21:55,08/Sep/20 18:00,18/Feb/21 10:10,,1.7.0,,,,2.3.0,3.0.0-alpha-1,,,test,,,,,,0,,,The TestFromClientSide test was parameterized recently so we'd run full sweet with one of three registries. Test now often takes longer than max 13 minutes allowed. Split the test.,,,,,,,,,,,,,,"11/Mar/20 14:43;stack;Screen Shot 2020-03-11 at 7.43.27 AM.png;https://issues.apache.org/jira/secure/attachment/12996432/Screen+Shot+2020-03-11+at+7.43.27+AM.png",,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2020-03-11 02:08:43.639,,,false,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Tue Sep 08 17:55:16 UTC 2020,,,,,,,"0|z0ce80:",9223372036854775807,,,,,,,,,,,,,,,,,"11/Mar/20 02:08;zhangduo;+1 on the proposal. Let me check the patch.","11/Mar/20 05:14;stack;Merged to branch-2 and master. Thanks for the review [~zhangduo]","11/Mar/20 05:16;stack;Hmm... maybe I should wait before resolving. Currently master and branch-2 are solid blue for timed out in the second panel on the flakies report https://builds.apache.org/view/H-L/view/HBase/job/HBase-Find-Flaky-Tests/job/master/lastSuccessfulBuild/artifact/dashboard.html","11/Mar/20 12:11;hudson;Results for branch master
	[build #1661 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/master/1661/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(x) {color:red}-1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/master/1661//General_Nightly_Build_Report/]




(x) {color:red}-1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1661//JDK8_Nightly_Build_Report_(Hadoop2)/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1661//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 jdk11 hadoop3 checks{color}
-- For more information [see jdk11 report|https://builds.apache.org/job/HBase%20Nightly/job/master/1661//JDK11_Nightly_Build_Report/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}
","11/Mar/20 14:44;stack;Here is from master flakies list. See how the end has turned green since this was committed. Similar is happening on branch-2 but signal is cleaner on master branch.","11/Mar/20 20:24;hudson;Results for branch branch-2
	[build #2543 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2543/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2543//General_Nightly_Build_Report/]




(x) {color:red}-1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2543//JDK8_Nightly_Build_Report_(Hadoop2)/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2543//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 jdk11 hadoop3 checks{color}
-- For more information [see jdk11 report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2543//JDK11_Nightly_Build_Report/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}
","03/Jun/20 22:38;hudson;Results for branch branch-2.2
	[build #884 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/884/]: (/) *{color:green}+1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/884//General_Nightly_Build_Report/]




(/) {color:green}+1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/884//JDK8_Nightly_Build_Report_(Hadoop2)/]


(/) {color:green}+1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/884//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}
","16/Jul/20 22:44;hudson;Results for branch branch-2.2
	[build #914 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/914/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/914//General_Nightly_Build_Report/]




(/) {color:green}+1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/914//JDK8_Nightly_Build_Report_(Hadoop2)/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/914//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}
","08/Sep/20 17:55;bharathv;Re-opening for branch-1 back port. TestFromClientSide is now heavy given it will be parameterized to run with multiple registry implementations. [~apurtell]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add CI check that builds dockerfiles,HBASE-23961,13290904,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,ndimiduk,ndimiduk,10/Mar/20 19:23,17/Mar/20 18:22,18/Feb/21 10:10,,,,,,,,,,build,,,,,,0,,,"I stumbled into some bit-rot in a {{Dockerfile}} recently. The job that powers the flakey detector apparently only runs because the build machines have layers cached. We could detect these failures earlier by rebuilding docker environments with {{--no-cache}} on a regular interval. Nightly is probably overkill, but it's easy enough to tack on an additional parallel stage to the existing build.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2020-03-10 19:23:26.0,,,,,,,"0|z0ce40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Jenkins Nightly] shelldocs executable unavailable within docker context,HBASE-23942,13289892,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,ndimiduk,ndimiduk,05/Mar/20 19:20,05/Mar/20 19:34,18/Feb/21 10:10,,3.0.0-alpha-1,,,,,,,,build,,,,,,0,,,"I'm not sure if this is an HBase build bug or a Yetus bug. Looking more closely at the yetus output, I noticed this in the general check section on [nightlies/master|https://builds.apache.org/blue/organizations/jenkins/HBase%20Nightly/detail/master/1654/pipeline/96].

{noformat}
[2020-03-05T14:56:51.559Z] executable '/home/jenkins/jenkins-slave/workspace/HBase_Nightly_master/yetus-0.11.1/bin/shelldocs' for 'shelldocs' does not exist.
{noformat}

Of course the executable is available in the yetus distro unpacked into the workspace. However, we're running in the docker context, and apparently the yetus runtime does not make itself available to the running container.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-03-05 19:34:00.457,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 05 19:34:00 UTC 2020,,,,,,,"0|z0c7v4:",9223372036854775807,,,,,,,,,,,,,,,,,"05/Mar/20 19:23;ndimiduk;I can imagine a handful of ways to address this within HBase's builds: cooking up additional docker steps or passing parameters to mount the local yetus path as a container volume. Neither of which I find palatable. Maybe we're just using Yetus wrong? Or is this a bug in Yetus's docker mode?

cc [~busbey], [~aw].","05/Mar/20 19:30;ndimiduk;Or maybe this is a bug local to the shelldoc module in yetus? The {{shellcheck}} plugin seems to manage this scenario (not having Yetus mounted inside the docker container) just fine – it's able to work with scripts from the host environment.
{noformat}
[2020-03-05T15:40:23.561Z] ============================================================================
[2020-03-05T15:40:23.561Z] ============================================================================
[2020-03-05T15:40:23.561Z]                           shellcheck plugin: full
[2020-03-05T15:40:23.561Z] ============================================================================
[2020-03-05T15:40:23.561Z] ============================================================================
[2020-03-05T15:40:23.561Z] 
[2020-03-05T15:40:23.561Z] 
[2020-03-05T15:40:23.561Z] Running shellcheck against all suspected shell scripts
[2020-03-05T15:40:23.561Z] /home/jenkins/jenkins-slave/workspace/HBase_Nightly_master/output-general/precommit/test-patch.d/shellcheck.sh: line 108: warning: command substitution: ignored null byte in input
[2020-03-05T15:40:23.561Z] /home/jenkins/jenkins-slave/workspace/HBase_Nightly_master/output-general/precommit/test-patch.d/shellcheck.sh: line 108: warning: command substitution: ignored null byte in input
[2020-03-05T15:40:23.561Z] /home/jenkins/jenkins-slave/workspace/HBase_Nightly_master/output-general/precommit/test-patch.d/shellcheck.sh: line 108: warning: command substitution: ignored null byte in input
{noformat}","05/Mar/20 19:34;busbey;precommit docker support doesn't mount the entire yetus assembly AFAIK. I believe the assumption is that you would either mount those bits beyond precommit that you want yourself or you would include them in your docker image.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Flakey Test] TestQuotaObserverChoreRegionReports.testReportExpiration,HBASE-23907,13288101,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,ndimiduk,ndimiduk,27/Feb/20 17:03,27/Feb/20 17:05,18/Feb/21 10:10,,3.0.0-alpha-1,,,,,,,,test,,,,,,0,,,"{noformat}
at org.apache.hadoop.hbase.quotas.TestQuotaObserverChoreRegionReports.testReportExpiration(TestQuotaObserverChoreRegionReports.java:118)
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException: 
org.apache.hadoop.hbase.TableNotFoundException: hbase:quota
	at java.lang.Thread.getStackTrace(Thread.java:1559)
	at org.apache.hadoop.hbase.util.FutureUtils.setStackTrace(FutureUtils.java:130)
	at org.apache.hadoop.hbase.util.FutureUtils.rethrow(FutureUtils.java:145)
	at org.apache.hadoop.hbase.util.FutureUtils.get(FutureUtils.java:168)
	at org.apache.hadoop.hbase.client.TableOverAsyncTable.get(TableOverAsyncTable.java:180)
	at org.apache.hadoop.hbase.quotas.QuotaTableUtil.doGet(QuotaTableUtil.java:843)
	at org.apache.hadoop.hbase.quotas.QuotaTableUtil.getQuotas(QuotaTableUtil.java:162)
	at org.apache.hadoop.hbase.quotas.QuotaTableUtil.getQuotas(QuotaTableUtil.java:150)
	at org.apache.hadoop.hbase.quotas.QuotaTableUtil.getTableQuota(QuotaTableUtil.java:124)
	at org.apache.hadoop.hbase.quotas.MasterQuotaManager$4.fetch(MasterQuotaManager.java:276)
	at org.apache.hadoop.hbase.quotas.MasterQuotaManager.setQuota(MasterQuotaManager.java:467)
	at org.apache.hadoop.hbase.quotas.MasterQuotaManager.setTableQuota(MasterQuotaManager.java:272)
	at org.apache.hadoop.hbase.quotas.MasterQuotaManager.setQuota(MasterQuotaManager.java:160)
	at org.apache.hadoop.hbase.master.MasterRpcServices.setQuota(MasterRpcServices.java:1720)
	at org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:374)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:133)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:338)
	at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:318)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2020-02-27 17:03:44.0,,,,,,,"0|z0byww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[flakey] TestThriftHttpServer fails to bind to port,HBASE-23903,13287813,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,ndimiduk,ndimiduk,26/Feb/20 18:08,28/Feb/20 00:09,18/Feb/21 10:10,,2.3.0,3.0.0-alpha-1,,,,,,,test,Thrift,,,,,0,,,"We see thrift tests fail occasionally in Jenkins due to failure to bind to port.

{noformat}
2020-02-26 10:24:07,725 INFO  [ThriftServer-httpServer] http.HttpServer (HttpServer.java:start(1063)) - HttpServer.start() threw a non Bind IOException
java.net.BindException: Port in use: 0.0.0.0:54902
	at org.apache.hadoop.hbase.http.HttpServer.openListeners(HttpServer.java:1126)
	at org.apache.hadoop.hbase.http.HttpServer.start(HttpServer.java:1060)
	at org.apache.hadoop.hbase.http.InfoServer.start(InfoServer.java:148)
	at org.apache.hadoop.hbase.thrift.ThriftServer.startInfoServer(ThriftServer.java:308)
	at org.apache.hadoop.hbase.thrift.ThriftServer.run(ThriftServer.java:849)
	at org.apache.hadoop.hbase.thrift.TestThriftHttpServer.lambda$startHttpServerThread$0(TestThriftHttpServer.java:121)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.eclipse.jetty.server.ServerConnector.openAcceptChannel(ServerConnector.java:351)
	at org.eclipse.jetty.server.ServerConnector.open(ServerConnector.java:319)
	at org.apache.hadoop.hbase.http.HttpServer.openListeners(HttpServer.java:1120)
	... 6 more
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-02-26 23:43:39.986,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 28 00:09:16 UTC 2020,,,,,,,"0|z0bx4w:",9223372036854775807,,,,,,,,,,,,,,,,,"26/Feb/20 23:43;stack;Where'd you see that [~ndimiduk]  ?","28/Feb/20 00:09;ndimiduk;I think this was on [#1209|https://github.com/apache/hbase/pull/1209], but there's too many test runs these days to be sure.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestEntityLocks often fails in lower resource envs in testEntityLockTimeout,HBASE-23839,13285215,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,markrmiller,markrmiller,13/Feb/20 23:39,13/Feb/20 23:39,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,,,"The test waits for something to happen and if the computer is a little slow, it will fail on line 178.

Doing a check against 3 instead 2x seems to help a lot to start.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2020-02-13 23:39:49.0,,,,,,,"0|z0bhc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestFromClientSide3 and subclasses often fail on testScanAfterDeletingSpecifiedRowV2.,HBASE-23835,13285052,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,markrmiller,markrmiller,13/Feb/20 11:10,11/Aug/20 18:12,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,,,"This test method fails a fair amount on me with something like:

TestFromClientSide3WoUnsafe>TestFromClientSide3.testScanAfterDeletingSpecifiedRowV2:236 expected:<3> but was:<2>

I had a hunch that it might be due to interference from other test methods running first so I tried changing the table name for just this method to be unique - still fails.

However, when I just run testScanAfterDeletingSpecifiedRowV2 on it's own without the methods, it does not seem to fail so far.


",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 14 10:43:07 UTC 2020,,,,,,,"0|z0bgbs:",9223372036854775807,,,,,,,,,,,,,,,,,"14/Feb/20 10:43;markrmiller;I've started to dig into this test - I've played around with speeding it up and hardening it - that led to a couple other little things in other code - I'll put up a pr for this test soon and spin off a JIRA issue or two.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consider setting -XX:MaxDirectMemorySize in the root Maven pom.xml file.,HBASE-23794,13283287,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,markrmiller,markrmiller,04/Feb/20 17:02,11/Apr/20 01:43,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,,,"-XX:MaxDirectMemorySize is an artificial governor on how much off heap memory can be allocated.

It would be nice to specify explicitly because:
 # The default can vary by platform / jvm impl - some devs may see random fails
 # It's just a limiter, it won't pre allocate or anything
 # A test env should normally ensure a healthy limit as would be done in production",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-02-04 17:21:19.122,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 11 01:43:48 UTC 2020,,,,,,,"0|z0b5fc:",9223372036854775807,,,,,,,,,,,,,,,,,"04/Feb/20 17:21;ndimiduk;What's your suggested value here for the rank-and-file tests [~markrmiller]? You see where this might be added to our base pom in the {{hbase-surefire.argLine}} property?

It's quite possible we have {{LargeTest}} classes that attempt to verify our off-heap data pathways. You might have a look into how those test classes manage themselves before tweaking anything in this area.","04/Feb/20 17:50;markrmiller;I'm still working out what a good suggestion for a value might be. Very few of the tests need even more than 1g of the 2g of heap given, so I'm looking into some numbers between the two things.

Largely, it is just nice to be explicit so that all devs and CI envs get the same value. Older hotspot might default to lower explicit values depending on arch/client/server, more recent hotspot defaults to Xmx, hotspot can change again, other JVM's could do whatever. So a lot of the improvement I imagine here is just consistency of the build and knowing the value has been set high enough for the tests.

I've run into fails due to this while playing around with giving tests less resources - so I'd like to set it high enough to avoid any fails, but also remove this confusion around messing with Xmx and running into off heap allocation failures and that type of thing.","11/Apr/20 01:43;stack;This seems like good idea. Stick up a patch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Flakey Test] TestExportSnapshotNoCluster.testSnapshotWithRefsExportFileSystemState,HBASE-23792,13283283,Test,Reopened,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,ndimiduk,ndimiduk,ndimiduk,04/Feb/20 16:51,08/Sep/20 17:49,18/Feb/21 10:10,,1.7.0,2.3.0,,,2.1.9,2.2.4,2.3.0,3.0.0-alpha-1,test,,,,,,0,,,"{{TestExportSnapshotNoCluster.testSnapshotWithRefsExportFileSystemState}} fails with

{noformat}
java.lang.IllegalArgumentException: Wrong FS: file:/home/jenkins/jenkins-slave/workspace/HBase_Nightly_branch-2@2/component/hbase-mapreduce/target/test-data/878a5107-35a3-90ea-50ef-d2a3c32a50dc/.hbase-snapshot/tableWithRefsV1, expected: hdfs://localhost:44609
	at org.apache.hadoop.hbase.snapshot.TestExportSnapshotNoCluster.testSnapshotWithRefsExportFileSystemState(TestExportSnapshotNoCluster.java:110)
	at org.apache.hadoop.hbase.snapshot.TestExportSnapshotNoCluster.testSnapshotWithRefsExportFileSystemState(TestExportSnapshotNoCluster.java:90)
{noformat}",,,,,,,,,,,,,,"04/Feb/20 20:40;ndimiduk;TEST-org.apache.hadoop.hbase.snapshot.TestExportSnapshotNoCluster.xml;https://issues.apache.org/jira/secure/attachment/12992632/TEST-org.apache.hadoop.hbase.snapshot.TestExportSnapshotNoCluster.xml",,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2020-02-05 08:20:23.525,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 08 17:49:00 UTC 2020,,,,,,,"0|z0b5eg:",9223372036854775807,,,,,,,,,,,,,,,,,"04/Feb/20 20:47;ndimiduk;I can't repro this locally or find a source for the filesystem implementation getting flipped to a distributed fs. However, the only place in Hadoop code where I see this ""Wrong FS"" message thrown as an {{IllegalArgumentException}} is in {{FileSystem#checkPath}}.

Looking closer at the xml report, I see that the test failed once with the above. Surefire tried to re-run it, but it failed the rerun with

{noformat}
java.io.IOException: Target file:/home/jenkins/jenkins-slave/workspace/HBase_Nightly_branch-2@2/component/hbase-mapreduce/target/test-data/878a5107-35a3-90ea-50ef-d2a3c32a50dc/.hbase-snapshot/tableWithRefsV1/tableWithRefsV1 is a directory
	at org.apache.hadoop.hbase.snapshot.TestExportSnapshotNoCluster.testSnapshotWithRefsExportFileSystemState(TestExportSnapshotNoCluster.java:105)
	at org.apache.hadoop.hbase.snapshot.TestExportSnapshotNoCluster.testSnapshotWithRefsExportFileSystemState(TestExportSnapshotNoCluster.java:90)
{noformat}

which implies to me that when surefire reruns a test method, it does not run the BeforeClass business.

I also notice that the test method runs the same code twice, but both times it's using {{createSnapshotV2}}... I think one of the invocations is supposed to be calling {{createSnapshotV1}}.

So.
# Survive flakey rerunning by converting the static {{BeforeClass}} stuff into instance-level {{Before}}.
# Break the test method into two, one for running over each of the snapshot manifest versions.","04/Feb/20 20:58;ndimiduk;I see that [~liuml07] spent some time tracking a similar issue in the same test on HBASE-22607. Linking.","04/Feb/20 21:02;ndimiduk;[~liuml07], [~AK2019] you folks mind taking a look at https://github.com/apache/hbase/pull/1124 ?","05/Feb/20 00:14;ndimiduk;Backported to all the 2.x branches.","05/Feb/20 08:20;hudson;Results for branch branch-2
	[build #2451 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2451/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2451//General_Nightly_Build_Report/]




(x) {color:red}-1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2451//JDK8_Nightly_Build_Report_(Hadoop2)/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2451//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}
","05/Feb/20 14:43;hudson;Results for branch master
	[build #1620 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/master/1620/]: (/) *{color:green}+1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/master/1620//General_Nightly_Build_Report/]




(/) {color:green}+1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1620//JDK8_Nightly_Build_Report_(Hadoop2)/]


(/) {color:green}+1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/master/1620//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}
","06/Feb/20 01:41;hudson;Results for branch branch-2.2
	[build #776 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/776/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/776//General_Nightly_Build_Report/]




(x) {color:red}-1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/776//JDK8_Nightly_Build_Report_(Hadoop2)/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.2/776//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}
","06/Feb/20 03:41;hudson;Results for branch branch-2.1
	[build #1795 on builds.a.o|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/1795/]: (x) *{color:red}-1 overall{color}*
----
details (if available):

(/) {color:green}+1 general checks{color}
-- For more information [see general report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/1795//General_Nightly_Build_Report/]




(x) {color:red}-1 jdk8 hadoop2 checks{color}
-- For more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/1795//JDK8_Nightly_Build_Report_(Hadoop2)/]


(x) {color:red}-1 jdk8 hadoop3 checks{color}
-- For more information [see jdk8 (hadoop3) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2.1/1795//JDK8_Nightly_Build_Report_(Hadoop3)/]


(/) {color:green}+1 source release artifact{color}
-- See build output for details.


(/) {color:green}+1 client integration test{color}
","08/Feb/20 19:04;stack;I saw this just now. The patch here is in place.

 [ERROR]   TestExportSnapshotNoCluster.testSnapshotV2WithRefsExportFileSystemState:91->testSnapshotWithRefsExportFileSystemState:107 » IllegalArgument

 Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 115.741 s <<< FAILURE! - in org.apache.hadoop.hbase.snapshot.TestExportSnapshotNoCluster
 org.apache.hadoop.hbase.snapshot.TestExportSnapshotNoCluster.testSnapshotV2WithRefsExportFileSystemState  Time elapsed: 74.459 s  <<< ERROR!
 java.lang.IllegalArgumentException: Wrong FS: file:/Users/stack/checkouts/hbase.apache.git/hbase-mapreduce/target/test-data/fad01338-65ef-e973-2412-05de114016fe/.hbase-snapshot/tableWithRefsV2, expected: hdfs://localhost:58051
   at org.apache.hadoop.hbase.snapshot.TestExportSnapshotNoCluster.testSnapshotWithRefsExportFileSystemState(TestExportSnapshotNoCluster.java:107)
   at org.apache.hadoop.hbase.snapshot.TestExportSnapshotNoCluster.testSnapshotV2WithRefsExportFileSystemState(TestExportSnapshotNoCluster.java:91)

testSnapshotV1WithRefsExportFileSystemState had just run which does same thing and it was fine.

All passes locally. Let me try in a subtask a Nick suggestion getting new HBaseTestingUtility per run.","10/Feb/20 21:59;ndimiduk;Argh.","08/Sep/20 17:49;bharathv;Re-opening for branch-1 back-port. There are a lot of commits related to this (some debugging, some refactoring etc). It is not exactly clear to me what fixed the problem, need to take a closer look. The issue now happens consistently in branch-1 too. [~apurtell] FYI.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestSecureRESTServer fails due to hostname mismatch,HBASE-23769,13282280,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,ndimiduk,ndimiduk,29/Jan/20 22:54,12/Feb/20 20:01,18/Feb/21 10:10,,2.3.0,3.0.0-alpha-1,,,,,,,REST,test,,,,,0,,,"Above test fails reliably in a simpleVagrant/VirtualBox Ubuntu vm.

{noformat}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.005 s <<< FAILURE! - in org.apache.hadoop.hbase.rest.TestSecureRESTServer
org.apache.hadoop.hbase.rest.TestSecureRESTServer  Time elapsed: 0.001 s  <<< ERROR!
java.io.IOException: Failed on local exception: java.io.IOException: Couldn't setup connection for hbase/localhost@EXAMPLE.COM to localhost/127.0.0.1:45719; Host Details : local host is: ""bionic/10.0.2.15""; destination host is: ""localhost"":45719;
        at org.apache.hadoop.hbase.rest.TestSecureRESTServer.setupServer(TestSecureRESTServer.java:193)
Caused by: java.io.IOException: Couldn't setup connection for hbase/localhost@EXAMPLE.COM to localhost/127.0.0.1:45719
        at org.apache.hadoop.hbase.rest.TestSecureRESTServer.setupServer(TestSecureRESTServer.java:193)
Caused by: javax.security.sasl.SaslException: GSS initiate failed
        at org.apache.hadoop.hbase.rest.TestSecureRESTServer.setupServer(TestSecureRESTServer.java:193)
Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: Message stream modified (41) - Message stream modified)
        at org.apache.hadoop.hbase.rest.TestSecureRESTServer.setupServer(TestSecureRESTServer.java:193)
Caused by: sun.security.krb5.KrbException: Message stream modified (41) - Message stream modified
        at org.apache.hadoop.hbase.rest.TestSecureRESTServer.setupServer(TestSecureRESTServer.java:193)
Caused by: sun.security.krb5.Asn1Exception: Identifier doesn't match expected value (906)
        at org.apache.hadoop.hbase.rest.TestSecureRESTServer.setupServer(TestSecureRESTServer.java:193)
{noformat}",,,,,,,,,,,HBASE-23765,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-01-30 02:27:46.92,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 30 16:49:06 UTC 2020,,,,,,,"0|z0azi8:",9223372036854775807,,,,,,,,,,,,,,,,,"29/Jan/20 23:26;ndimiduk;Is there some assumption about hostnames that this environment is missing [~elserj]?","30/Jan/20 02:27;elserj;Set on -Dsun.security.krb5.debug=true (and maybe -Dsun.security.spnego.debug=true can't tell if this is spnego yet) and you should get a clear error message about what the client was asking for from the kdc. I've seen this before with VMs where localhost resolution is wonky as a ""feature"" so that things resolve from the host OS. Something like localhost actually resolves to something other than 127.0.0.1 but then the IP that was resolved has a reverse lookup into the fqdn.

In short, double check /etc/hosts and make sure localhost is sane.","30/Jan/20 16:49;elserj;Also, send me the box if you can and the command you're running, and I'll try to take a look rather than debug-over-async ;)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test failure due to flaky tests on ppc64le,HBASE-23366,13272185,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,AK2019,AK2019,04/Dec/19 10:56,13/Jan/20 07:51,18/Feb/21 10:10,,2.2.0,,,,,,,,,,,,,,0,,,"I have been trying to build the Apache Hbase on rhel_7.6/ppc64le. The build passes, however it leads to flaky test failures in module hbase-server.

All the tests pass most of the times when run individually.

Following is the list of the tests that fail often:
 * TestMetaTableMetrics
 * TestMasterAbortWhileMergingTable
 * TestSnapshotFromMaster
 * TestReplicationAdminWithClusters
 * TestAsyncDecommissionAdminApi
 * TestCompactSplitThread
 

   
I am on branch rel/2.2.0

{color:#172b4d}Would like some help on understanding the cause for the same . I am running it on a High end VM with good connectivity.{color}
 
 
 ","{color:#172b4d}os: rhel 7.6{color}
{color:#172b4d} arch: ppc64le{color}",,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-12-18 23:39:59.636,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 13 07:51:23 UTC 2020,,,,,,,"0|z099xk:",9223372036854775807,,,,,,,,,,,,,,,,,"18/Dec/19 07:00;AK2019;Any leads will be appreciated. 
[~liuml07] could you please look into this.
Thank you

","18/Dec/19 23:39;liuml07;[~AK2019] I'm not seeing this errors in our daily build because we are not on this 2.2 release. Tests are failing for different reasons, so you can check status of each failing test by searching in JIRA. Perhaps they have been addressed in later releases, for e.g. https://issues.apache.org/jira/browse/HBASE-22637?jql=text%20~%20TestMetaTableMetrics%20ORDER%20BY%20created%20DESC If they have been fixed in later releases, you can either backport to your fork, or upgrade your version if that fits.","20/Dec/19 03:46;AK2019;Each time different tests are failing and running the same again either builds successfully or gives some different failures.
And the failing tests are many a times not repeated.
","13/Jan/20 07:51;rashmi_sakhalkar;I am facing the similar issue with hbase-server project. I used the latest code from master branch but still the tests failed intermittently.

[ERROR] Failures:
[ERROR] TestCompactingToCellFlatMapMemStore.testForceCopyOfBigCellIntoImmutableSegment:900 i=1 expected:<8389924> but was:<8389992>
[ERROR] TestRegionReplicasWithRestartScenarios.testWhenRestart:135->assertReplicaDistributed:141
[ERROR] TestReplicationStatus.testReplicationStatus:98 failed to get ReplicationLoadSourceList

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Test failure timing out, failed to replicate all edits ",HBASE-23344,13270691,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,lysannef,lysannef,26/Nov/19 10:51,26/Nov/19 10:53,18/Feb/21 10:10,,2.2.0,,,,,,,,test,,,,,,0,,,"Following tests failing if run as part of the Test suite:
[INFO] Results:
[INFO]
[ERROR] Failures:
[ERROR]   TestRegionAssignedToMultipleRegionServers.test:177
[ERROR]   TestReplicationEndpointWithMultipleWAL>TestReplicationEndpoint.testInterClusterReplication:236 Waiting timed out after [30,000] msec Failed to replicate all edits, expected = 2500 replicated = 1969
[INFO]
[ERROR] Tests run: 2131, Failures: 2, Errors: 0, Skipped: 22
[INFO]

However running it individually it passes.
I have tried increasing the time out to double the current value which allows the test to pass, However , we would like to know if there's any  minimum performance requirements at which we could make the failing tests pass.
Kindly let me know if you have any issues.
 ","OS: RHEL7.6

Arch: ppc64le

Version 2.2.0

 ",,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2019-11-26 10:51:11.0,,,,,,,"0|z090q0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insufficient set up in TestSecureWALReplay  because of test fixture inheritance,HBASE-22814,13249411,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,zhouyapengzi,zhouyapengzi,07/Aug/19 17:03,23/Aug/19 02:40,18/Feb/21 10:10,,2.1.5,,,,,,,,regionserver,,,,,,0,test,,"Description: 

In _TestSecureWALReplay.java,_ it does not call the test fixture method from the base class _TestWALReplay.java_ while the test fixture in the all other sibling test cases do (e.g., _TestWALReplayBoundedLogWriterCreation.java_)

Suggestion:

should call *TestWALReplay.setUpBeforeClass()* in @BeforeClass

or use test utilities to remove dependencies of the tests.
{code:java}
public class TestSecureWALReplay extends TestWALReplay {
    @BeforeClass
    public static void setUpBeforeClass() throws Exception {
	Configuration conf = AbstractTestWALReplay.TEST_UTIL.getConfiguration();
	conf.set(HConstants.CRYPTO_KEYPROVIDER_CONF_KEY, KeyProviderForTesting.class.getName());
	conf.set(HConstants.CRYPTO_MASTERKEY_NAME_CONF_KEY, ""hbase"");
	conf.setClass(""hbase.regionserver.hlog.reader.impl"", SecureProtobufLogReader.class,
	Reader.class);
	conf.setClass(""hbase.regionserver.hlog.writer.impl"", SecureProtobufLogWriter.class,
	Writer.class);
	conf.setBoolean(HConstants.ENABLE_WAL_ENCRYPTION, true);
	AbstractTestWALReplay.setUpBeforeClass();
    }
}{code}",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-08-23 02:40:52.526,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 23 02:40:52 UTC 2019,,,,,,,"0|z05faw:",9223372036854775807,,,,,,,,,,,,,,,,,"23/Aug/19 02:40;jingjing;thanks for reporting,could u submit a pull request to fix this issue?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integration Test for BulkLoad replication,HBASE-22668,13243905,Test,In Progress,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,nkalmar,nkalmar,nkalmar,09/Jul/19 14:54,09/Jul/19 14:54,18/Feb/21 10:10,,,,,,,,,,Replication,,,,,,0,,,"Add Integration Test to check if BulkLoad replication works, data is available on a sink cluster once BulkLoad is complete on master.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2019-07-09 14:54:12.0,,,,,,,"0|z04hx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a REST server integration test that uses a cluster for simple operations,HBASE-22587,13239595,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,sakthi,sakthi,14/Jun/19 15:49,30/Jan/20 17:11,18/Feb/21 10:10,,,,,,,,,,integration tests,REST,test,,,,0,jdk11,,"Let's add rest server to the nightly test that uses a cluster for simple operations to catch any possible rest server issues related to dependencies, cnfe, npe etc.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-01-29 19:31:51.238,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 29 23:11:22 UTC 2020,,,,,,,"0|z03re0:",9223372036854775807,,,,,,,,,,,,,,,,,"29/Jan/20 19:31;stack;Can we resolve this parent issue now [~sakthi] given all subtasks are done?","29/Jan/20 19:42;sakthi;All the tasks listed here issues that are related to getting our rest server work fine with jdk 11 I guess. But the parent issue hasn't yet been started/completed (No one has taken up the task yet). The motto was for us to continuously test our rest server by doing some simple cluster operations through the rest endpoint. [~stack]","29/Jan/20 19:55;stack;Oh. I should have read the description. Thanks [~sakthi]","29/Jan/20 21:39;ndimiduk;Existing tests using {{HBaseRESTTestingUtility}} don't accomplish what you're seeking here, [~sakthi]? We need some kind of integration test that works by shelling out to {{bin/hbase rest start}} ?","29/Jan/20 23:11;sakthi;Yeah you are right [~ndimiduk] . I was thinking from an integration testing point of view.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestBlocksRead is flaky,HBASE-22126,13224706,Test,In Progress,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,sandeep.pal,apurtell,apurtell,28/Mar/19 22:26,13/Feb/20 19:53,18/Feb/21 10:10,,1.5.0,,,,1.7.0,,,,test,,,,,,0,branch-1,,"TestBlocksRead does not fail when invoked by itself but is flaky when run as part of the suite.

Some kind of race during setup.

[ERROR] testBlocksStoredWhenCachingDisabled(org.apache.hadoop.hbase.regionserver.TestBlocksRead)  Time elapsed: 0.19 s  <<< ERROR!
java.net.ConnectException: Call From $HOST/$IP to localhost:59658 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
    at org.apache.hadoop.hbase.regionserver.TestBlocksRead.initHRegion(TestBlocksRead.java:112)
    at org.apache.hadoop.hbase.regionserver.TestBlocksRead.testBlocksStoredWhenCachingDisabled(TestBlocksRead.java:389)
Caused by: java.net.ConnectException: Connection refused
    at org.apache.hadoop.hbase.regionserver.TestBlocksRead.initHRegion(TestBlocksRead.java:112)
    at org.apache.hadoop.hbase.regionserver.TestBlocksRead.testBlocksStoredWhenCachingDisabled(TestBlocksRead.java:389)

 ",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2019-03-28 22:26:12.0,,,,,,,"0|z01848:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestSimpleRpcScheduler is still flaky,HBASE-21904,13215843,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,apurtell,apurtell,14/Feb/19 22:29,06/Mar/20 23:47,18/Feb/21 10:10,,1.5.0,,,,1.7.0,,,,test,,,,,,0,branch-1,,"Flaky wait condition, unclear if it's the wait condition or the underlying functionality that is the problem.

[ERROR] testSoftAndHardQueueLimits(org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler)  Time elapsed: 0.228 s  <<< FAILURE!
java.lang.AssertionError
        at org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler.testSoftAndHardQueueLimits(TestSimpleRpcScheduler.java:380)",,,,,,,,,,,,,,"14/Feb/19 22:29;apurtell;org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler-output.txt;https://issues.apache.org/jira/secure/attachment/12958792/org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler-output.txt",,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2019-02-17 07:18:28.41,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 17 20:10:50 UTC 2019,,,,,,,"0|yi100g:",9223372036854775807,,,,,,,,,,,,,,,,,"14/Feb/19 22:30;apurtell;Fails on Linux hosts, not seen on MacOS","17/Feb/19 07:18;xucang;Test passed on my linux box.
My env:

openjdk version ""1.8.0_191""
OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-2ubuntu0.16.04.1-b12)
OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)","17/Feb/19 20:10;apurtell;Run it up to 100 times. it will eventually fail.

By the way my Linux test environment is based on CentOS 6.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestCompactionWithCoprocessor doesn't invoke coprocessor logic,HBASE-21902,13215801,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,dcz99,dcz99,14/Feb/19 18:27,13/Feb/20 19:28,18/Feb/21 10:10,,2.0.4,,,,,,,,test,,,,,,0,,,"TestCompactionWithCoprocessor is designed to invoke NoOpScanPolicyObserver which implements default behavior.  in reality NoOpScanPolicyObserver isn't instantiated, TestCompactionWithCoprocessor runs/passes trivially, without increasing test coverage beyond TestCompaction.

See patch which passes TestCompactionWithCoprocessor.",,,,,,,,,,,,,,"14/Feb/19 18:27;dcz99;hbase-test.patch;https://issues.apache.org/jira/secure/attachment/12958762/hbase-test.patch",,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2020-01-27 19:02:59.787,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 13 19:28:27 UTC 2020,,,,,,,"0|yi0zr4:",9223372036854775807,,,,,,,,,,,,,,,,,"27/Jan/20 19:02;xucang;Hi [~dcz99] thanks for working on this JIRA.  Please see this doc how to submit patch correctly. [https://hbase.apache.org/book.html#developing]

And by looking at your patch, if you think NoOpScanPolicyObserver is not instantiated, why does throwing the RuntimeException in that class help?  thanks.

 ","13/Feb/20 19:28;dcz99;IIRC throwing the RuntimeException just exposes the test passes (false-negative).  It isn't intended as a fix.  I can provide a fix if you'd like, I recall fixing this in my own branch. Just give me sometime to jog my memory.

thanks for the comment Xu Cang.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark in java examples throws 'A master URL must be set in your configuration',HBASE-21267,13189348,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,ram_krish,ram_krish,ram_krish,04/Oct/18 04:34,05/Oct/18 09:12,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,,,"Trying out some spark on java examples I found this
'A master URL must be set in your configuration'.
It comes from here
bq.    SparkConf sparkConf = new SparkConf().setAppName(""JavaHBaseBulkPutExample "" + tableName);
Adding the below line solved the problem, 
bq. SparkConf sparkConf = new SparkConf().setAppName(""JavaHBaseBulkPutExample "" + tableName).setMaster(""local[2]"")
But am not sure if there is any better way.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-10-05 09:12:30.225,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 05 09:12:30 UTC 2018,,,,,,,"0|i3ytfj:",9223372036854775807,,,,,,,,,,,,,,,,,"04/Oct/18 04:44;ram_krish;[~malaskat] - You have any suggestions here. ","05/Oct/18 09:12;ashish singhi; [~ram_krish], the change you propose makes sense.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
automated tests against non-minicluster based deployment,HBASE-18148,13076594,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,busbey,busbey,01/Jun/17 17:54,05/Jun/17 17:04,18/Feb/21 10:10,,,,,,,,,,community,integration tests,,,,,0,,,we should have ITs that run automatically (i.e. nightly) against a cluster that isn't a minicluster or standalone instance.,,,,,,,,,,,,,,"01/Jun/17 17:57;busbey;HBASE-18148.0.patch;https://issues.apache.org/jira/secure/attachment/12870829/HBASE-18148.0.patch",,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2017-06-01 18:08:31.101,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 17:04:11 UTC 2017,,,,,,,"0|i3frjr:",9223372036854775807,,,,,,,,,,,,,,,,,"01/Jun/17 17:54;busbey;one way to do this that we started on in the past was to use [clusterdock|https://github.com/clusterdock/].","01/Jun/17 17:57;busbey;attaching WIP with contents of the two jobs from that last time we tried this with clusterdock.

these represent:

* [job that builds topology images|https://builds.apache.org/view/H-L/view/HBase/job/HBase-Build-clusterdock-Clusters/]
* [job that runs ITBLL against the topology for master HEAD|https://builds.apache.org/view/H-L/view/HBase/job/HBase-master-IntegrationTestBigLinkedList/]","01/Jun/17 17:57;busbey;these were also aug/sep 2016 I think.","01/Jun/17 18:08;awleblang;Could you include the default values for the parameters in the script? This would make it easier for reviewers without jenkins access to follow the script. ","01/Jun/17 18:09;busbey;sure. short term:

* DOCKER_REGISTRY_URL
** hbasejenkinsuser-docker-hbase.bintray.io
* CLUSTERDOCK_DOCKER_REGISTRY_USERNAME
** hbasejenkinsuser
* DOCKER_REGISTRY_NAMESPACE
** dev","01/Jun/17 23:05;awleblang;Have you tested this recently? I filed this clusterdock issue b/c I was unable to build the images https://github.com/clusterdock/topology_apache_hbase/issues/6","05/Jun/17 17:04;busbey;I don't think any of this has run since September.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ITAcidGuarantees should drive flushes/compactions with a monkey,HBASE-18146,13076590,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,apurtell,apurtell,01/Jun/17 17:37,02/Jun/17 00:53,18/Feb/21 10:10,,,,,,,,,,integration tests,,,,,,0,,,IntegrationTestAcidGuarantees best works with minicluster based testing because it sets up for frequent flushing using site configuration. However we may run against a distributed cluster so cannot rely on site configuration to drive desired flushing/compaction behavior. Introduce and use a new monkey policy for this purpose. ,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-06-01 19:44:02.158,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 02 00:53:55 UTC 2017,,,,,,,"0|i3friv:",9223372036854775807,,,,,,,,,,,,,,,,,"01/Jun/17 19:44;jmhsieh;IntegrationTestAcidGuarantees works against real clusters for a configurable amount of time. See: 
https://github.com/apache/hbase/blob/master/hbase-it/src/test/java/org/apache/hadoop/hbase/IntegrationTestAcidGuarantees.java#L113

This can be done via the command line like so:
hbase org.apache.hadoop.hbase.IntegrationTestAcidGuarantees -DnumWriters=5 -DnumGetters=1 -DnumScanners=1 -Dmillis=600000 -m calm

In some internal setups, we've been running it against multiple branches for 10 minute runs with each monkey enabled against various configuration settings (e.g. all writes through multiwal, through mob write path, under kerberos security etc).
 ","01/Jun/17 20:19;apurtell;Looking at the code, IntegrationTestAcidGuarantees wants to apply special configuration to the minicluster and then run three relatively short canned scenarios. 

It's good to know someone is using it on real clusters and with chaos. Do you think it sufficient or could there be improvement? I'm pretty sure there is room for improvement for testing against real clusters. One thing I was thinking regards a new IT is a new monkey policy that flushes and compacts very frequently. 

","01/Jun/17 23:09;jmhsieh;There is always room for improvement. :) For your initial specific asks, I think this is sufficient.

That suggested monkey sounds useful.  An alternative appraoch that would require config as opposed to code woudl be to flush size on the cluster under test which would to exercise flush and compact those mechanisms more frequently (would affect a few of my other favorites like ITBLL and ITIngest) 

The fact that this was filed means we should probably add a section to the docs to make it more obvious that this and other tests are runnable from the command line.  (some IT tests use the class's static main function to run, other use the ITDriver to execute some canned runs).



","01/Jun/17 23:46;apurtell;bq. An alternative appraoch that would require config as opposed to code woudl be to flush size on the cluster under test 

That's what I was getting at earlier when suggesting the test shouldn't depend on changes to deployed site configuration, and how the current test seems limited. ","02/Jun/17 00:43;jmhsieh;Ugh, I realize that was hard to understand/parse but I think you got the jist.  -- An alternative would be to change the flush size config.  

That said, a monkeyfactory that flushes frequently and causes compactions frequently (and similarly, a split/merge monkey factory) make sense.  Would it make sense to just file those as separate tickets? We coudl also either close this out as info provided or convert it to a docs improvement jira?)","02/Jun/17 00:49;apurtell;bq. That said, a monkeyfactory that flushes frequently and causes compactions frequently (and similarly, a split/merge monkey factory) make sense.

Let me repurpose this issue.","02/Jun/17 00:53;apurtell;Original description

{quote}
TestAcidGuarantees and IntegrationTestAcidGuarantees both really only work with minicluster based testing and do not run for a long duration. Consider a new integration test that makes similar atomicity checks while running for, potentially, a very long time, determined by test parameters supplied on the command line (perhaps as property definitions). The new integration test should expect to run against a distributed cluster, support specification of desired monkey policy, and not require any special non-default site configuration settings. 
{quote}

New

{quote}
IntegrationTestAcidGuarantees best works with minicluster based testing because it sets up for frequent flushing using site configuration. However we may run against a distributed cluster so cannot rely on site configuration to drive desired flushing/compaction behavior. Introduce and use a new monkey policy for this purpose.
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shorten the execution time of TestBackupMultipleDeletes,HBASE-17949,13066134,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,yuzhihong@gmail.com,yuzhihong@gmail.com,23/Apr/17 03:37,24/Sep/18 15:22,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,backup,,"On my Mac, TestBackupMultipleDeletes took 10 minutes to run.

The test performs 5 incremental backups in total for 3 tables.

We can reduce the number of incremental backups so that runtime comes down by a few minutes.",,,,,,,,,,,,,,"19/Aug/17 18:28;yuzhihong@gmail.com;testBackupMultipleDeletes-output.tar.gz;https://issues.apache.org/jira/secure/attachment/12882747/testBackupMultipleDeletes-output.tar.gz",,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2017-05-05 16:02:01.626,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 18:29:58 UTC 2017,,,,,,,"0|i3dz27:",9223372036854775807,,,,,,,,,,,,,,,,,"23/Apr/17 03:47;yuzhihong@gmail.com;Looking at https://builds.apache.org/job/HBase-TRUNK_matrix/jdk=JDK%201.8%20(latest),label=Hadoop/2901/testReport/org.apache.hadoop.hbase.backup/TestBackupMultipleDeletes/testBackupMultipleDeletes/ , it seems test progress on Jenkins was faster than that on Mac.","05/May/17 16:02;chia7712;It takes 152.883 sec on my local run. My environment is i5-4210H, 8G mem, SSD, and ubuntu-16.04.

bq. On my Mac, TestBackupMultipleDeletes took 10 minutes to run.
Would you mind telling more detail of your env?","05/May/17 18:22;yuzhihong@gmail.com;Here is my OS:

Darwin TYus-MacBook-Pro.local 15.3.0 Darwin Kernel Version 15.3.0: Thu Dec 10 18:40:58 PST 2015; root:xnu-3248.30.4~1/RELEASE_X86_64 x86_64

When I ran the test on the following OS (with SSD), it took 7 minutes:

Linux a.com 3.10.0-327.28.3.el7.x86_64 #1 SMP Thu Aug 18 19:05:49 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux","12/May/17 14:42;chia7712;My MacBook took 10 mins to run the test but the hardware of my MacBook is more powerful than my ubuntu machine. 
That is interesting..","19/Aug/17 16:34;reidchan;here is mine (one of):
{code}
Running org.apache.hadoop.hbase.backup.TestBackupMultipleDeletes
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 178.117 sec - in org.apache.hadoop.hbase.backup.TestBackupMultipleDeletes

Results :

Tests run: 1, Failures: 0, Errors: 0, Skipped: 0
{code} But sometimes, it couldn't finish itself.
My mac: 2.7 GHz Intel Core i5, 8 GB 1867 MHz DDR3.

Maybe there are unstable factors? i'm going to run more few times and look deeply in codes","19/Aug/17 18:29;yuzhihong@gmail.com;When I ran the test on MacBook, it took >5 minutes.
Here is test output.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shorten backup / restore test execution time,HBASE-16458,12998598,Test,Patch Available,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,vrodionov,yuzhihong@gmail.com,yuzhihong@gmail.com,19/Aug/16 21:32,24/Sep/18 15:22,18/Feb/21 10:10,,,,,,3.0.0-alpha-1,,,,,,,,,,0,backup,,"Below was timing information for all the backup / restore tests (today's result):
{code}
Running org.apache.hadoop.hbase.backup.TestIncrementalBackup
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 576.273 sec - in org.apache.hadoop.hbase.backup.TestIncrementalBackup
Running org.apache.hadoop.hbase.backup.TestBackupBoundaryTests
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 124.67 sec - in org.apache.hadoop.hbase.backup.TestBackupBoundaryTests
Running org.apache.hadoop.hbase.backup.TestBackupStatusProgress
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 102.34 sec - in org.apache.hadoop.hbase.backup.TestBackupStatusProgress
Running org.apache.hadoop.hbase.backup.TestBackupAdmin
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 490.251 sec - in org.apache.hadoop.hbase.backup.TestBackupAdmin
Running org.apache.hadoop.hbase.backup.TestHFileArchiving
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 84.323 sec - in org.apache.hadoop.hbase.backup.TestHFileArchiving
Running org.apache.hadoop.hbase.backup.TestSystemTableSnapshot
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 65.492 sec - in org.apache.hadoop.hbase.backup.TestSystemTableSnapshot
Running org.apache.hadoop.hbase.backup.TestBackupDescribe
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 93.758 sec - in org.apache.hadoop.hbase.backup.TestBackupDescribe
Running org.apache.hadoop.hbase.backup.TestBackupLogCleaner
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 109.187 sec - in org.apache.hadoop.hbase.backup.TestBackupLogCleaner
Running org.apache.hadoop.hbase.backup.TestIncrementalBackupNoDataLoss
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 330.539 sec - in org.apache.hadoop.hbase.backup.TestIncrementalBackupNoDataLoss
Running org.apache.hadoop.hbase.backup.TestRemoteBackup
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 84.371 sec - in org.apache.hadoop.hbase.backup.TestRemoteBackup
Running org.apache.hadoop.hbase.backup.TestBackupSystemTable
Tests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 67.893 sec - in org.apache.hadoop.hbase.backup.TestBackupSystemTable
Running org.apache.hadoop.hbase.backup.TestRestoreBoundaryTests
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 120.779 sec - in org.apache.hadoop.hbase.backup.TestRestoreBoundaryTests
Running org.apache.hadoop.hbase.backup.TestFullBackupSetRestoreSet
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 117.815 sec - in org.apache.hadoop.hbase.backup.TestFullBackupSetRestoreSet
Running org.apache.hadoop.hbase.backup.TestBackupShowHistory
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 136.517 sec - in org.apache.hadoop.hbase.backup.TestBackupShowHistory
Running org.apache.hadoop.hbase.backup.TestRemoteRestore
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 91.799 sec - in org.apache.hadoop.hbase.backup.TestRemoteRestore
Running org.apache.hadoop.hbase.backup.TestFullRestore
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 317.711 sec - in org.apache.hadoop.hbase.backup.TestFullRestore
Running org.apache.hadoop.hbase.backup.TestFullBackupSet
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 87.045 sec - in org.apache.hadoop.hbase.backup.TestFullBackupSet
Running org.apache.hadoop.hbase.backup.TestBackupDelete
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 86.214 sec - in org.apache.hadoop.hbase.backup.TestBackupDelete
Running org.apache.hadoop.hbase.backup.TestBackupDeleteRestore
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 77.631 sec - in org.apache.hadoop.hbase.backup.TestBackupDeleteRestore
Running org.apache.hadoop.hbase.backup.TestIncrementalBackupDeleteTable
Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 190.358 sec - in org.apache.hadoop.hbase.backup.TestIncrementalBackupDeleteTable
Running org.apache.hadoop.hbase.backup.TestBackupShowHistoryFromBackupDestination
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 93.726 sec - in org.apache.hadoop.hbase.backup.TestBackupShowHistoryFromBackupDestination
Running org.apache.hadoop.hbase.backup.TestFullBackup
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 177.137 sec - in org.apache.hadoop.hbase.backup.TestFullBackup
{code}
The duration was close to 1 hour:
{code}
[INFO] Apache HBase - Server .............................. SUCCESS [  01:02 h]
{code}
We should:
* merge duplicate backup / restore test cases
* shorten backup / restore test execution time (ingesting less data, etc)
* use multi wal for selected test(s) since rolling / replaying more WAL files takes more time",,,,,,,,,,,HBASE-17949,,,"07/Sep/18 22:32;yuzhihong@gmail.com;16458-v1.patch;https://issues.apache.org/jira/secure/attachment/12938910/16458-v1.patch","24/Aug/16 02:32;yuzhihong@gmail.com;16458.HBASE-7912.v3.txt;https://issues.apache.org/jira/secure/attachment/12825180/16458.HBASE-7912.v3.txt","24/Aug/16 20:37;yuzhihong@gmail.com;16458.HBASE-7912.v4.txt;https://issues.apache.org/jira/secure/attachment/12825334/16458.HBASE-7912.v4.txt","22/Aug/16 17:45;yuzhihong@gmail.com;16458.v1.txt;https://issues.apache.org/jira/secure/attachment/12824899/16458.v1.txt","08/Sep/18 04:16;yuzhihong@gmail.com;16458.v2.txt;https://issues.apache.org/jira/secure/attachment/12938936/16458.v2.txt","23/Aug/16 15:30;yuzhihong@gmail.com;16458.v2.txt;https://issues.apache.org/jira/secure/attachment/12825078/16458.v2.txt","23/Aug/16 16:48;yuzhihong@gmail.com;16458.v3.txt;https://issues.apache.org/jira/secure/attachment/12825094/16458.v3.txt","08/Sep/18 12:06;yuzhihong@gmail.com;16458.v4.txt;https://issues.apache.org/jira/secure/attachment/12938948/16458.v4.txt","08/Sep/18 16:04;yuzhihong@gmail.com;16458.v5.txt;https://issues.apache.org/jira/secure/attachment/12938959/16458.v5.txt","07/Sep/18 21:44;vrodionov;HBASE-16458-v1.patch;https://issues.apache.org/jira/secure/attachment/12938904/HBASE-16458-v1.patch","08/Sep/18 00:18;vrodionov;HBASE-16458-v2.patch;https://issues.apache.org/jira/secure/attachment/12938927/HBASE-16458-v2.patch",,11.0,,,,,,,,,,,,,,,,,,,,2016-08-19 22:52:42.901,,,false,,,,,,,,,,,,,,,,,9223372036854775807,Reviewed,,,Thu Sep 13 19:15:49 UTC 2018,,,,,,,"0|i32jjr:",9223372036854775807,,,,,,,,,,,,,,,,,"19/Aug/16 22:52;vrodionov;Interesting, last time I have run all tests they took less than 30 min. It was couple weeks ago.","19/Aug/16 22:57;yuzhihong@gmail.com;That was for single WAL, right ?

The above metrics were obtained on a Linux which is not as fast as MacBook with SSDs.","22/Aug/16 17:22;yuzhihong@gmail.com;TestBackupAdmin took 490 seconds in the above run.

BackupAdmin functionality is exercised in TestIncrementalBackup and TestIncrementalBackupDeleteTable.

Looks like we can drop TestBackupAdmin.","22/Aug/16 17:45;yuzhihong@gmail.com;Patch v1 enabled multiwal for TestIncrementalBackup only.

Also drops some duplicate test cases.","22/Aug/16 19:56;yuzhihong@gmail.com;[~vrodionov]:
Can you take a look ?","22/Aug/16 23:26;vrodionov;lgtm, but you can optimize more, I think, [~tedyu].","22/Aug/16 23:31;yuzhihong@gmail.com;Mind being a bit more specific on the optimizations ?

Thanks","23/Aug/16 00:05;vrodionov;Some tests can be simplified, others can be merged? Will you continue working on this or want to commit it, [~tedyu]?","23/Aug/16 15:30;yuzhihong@gmail.com;Patch v2 adds Durability.SKIP_WAL to table loading prior to full backup.","23/Aug/16 16:48;yuzhihong@gmail.com;Patch v3 folds TestBackupShowHistoryFromBackupDestination into TestBackupShowHistory.

The 3 sub-tests in TestBackupShowHistory are folded into one sub-test to cut down on the time table1 full backup takes.","23/Aug/16 16:49;yuzhihong@gmail.com;[~vrodionov]:
Mind taking another look ?","23/Aug/16 18:58;vrodionov;Sure, will do it today.","24/Aug/16 01:49;vrodionov;Without patch - 34 minutes
With patch      - 29 minutes

On my laptop
[~tedyu], do you have improvement numbers? Otherwise, looks good.","24/Aug/16 01:53;yuzhihong@gmail.com;I forgot to record the exact runtime without patch - on Linux.

There were several minutes of savings.","24/Aug/16 02:32;yuzhihong@gmail.com;Same v3 patch named properly - for QA.","24/Aug/16 15:14;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 20s {color} | {color:blue} Docker mode activated. {color} |
| {color:blue}0{color} | {color:blue} patch {color} | {color:blue} 0m 3s {color} | {color:blue} The patch file was not named according to hbase's naming conventions. Please see https://yetus.apache.org/documentation/0.3.0/precommit-patchnames for instructions. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 1s {color} | {color:green} HBASE-7912 passed with JDK v1.8.0_101 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 1s {color} | {color:green} HBASE-7912 passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} hbaseprotoc {color} | {color:green} 0m 1s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 1s {color} | {color:green} HBASE-7912 passed with JDK v1.8.0_101 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 2s {color} | {color:green} HBASE-7912 passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 1s {color} | {color:green} the patch passed with JDK v1.8.0_101 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 1s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 1s {color} | {color:green} the patch passed with JDK v1.7.0_101 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 1s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red} 0m 0s {color} | {color:red} The patch has 1 line(s) that end in whitespace. Use git apply --whitespace=fix. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 25m 10s {color} | {color:green} Patch does not cause any errors with Hadoop 2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.1 2.6.2 2.6.3 2.7.1. {color} |
| {color:green}+1{color} | {color:green} hbaseprotoc {color} | {color:green} 0m 3s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 3s {color} | {color:green} the patch passed with JDK v1.8.0_101 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 3s {color} | {color:green} the patch passed with JDK v1.7.0_101 {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 25m 59s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=1.11.2 Server=1.11.2 Image:yetus/hbase:date2016-08-24 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12825180/16458.HBASE-7912.v3.txt |
| JIRA Issue | HBASE-16458 |
| Optional Tests |  hadoopcheck  hbaseanti  javac  compile  javadoc  |
| uname | Linux e8a50851363d 3.13.0-92-generic #139-Ubuntu SMP Tue Jun 28 20:42:26 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | nobuild |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/component/dev-support/hbase-personality.sh |
| git revision | HBASE-7912 / 7c6b10f |
| Default Java | 1.7.0_101 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_101 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_101 |
| whitespace | https://builds.apache.org/job/PreCommit-HBASE-Build/3236/artifact/patchprocess/whitespace-eol.txt |
| modules | C: . U: . |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/3236/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","24/Aug/16 20:37;yuzhihong@gmail.com;Patch v4 drops table3 and table4 from TestIncrementalBackup since multi table operations are covered by table1 and table2.","24/Aug/16 21:05;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 16s {color} | {color:blue} Docker mode activated. {color} |
| {color:blue}0{color} | {color:blue} patch {color} | {color:blue} 0m 3s {color} | {color:blue} The patch file was not named according to hbase's naming conventions. Please see https://yetus.apache.org/documentation/0.3.0/precommit-patchnames for instructions. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 1s {color} | {color:green} HBASE-7912 passed with JDK v1.8.0_101 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 1s {color} | {color:green} HBASE-7912 passed with JDK v1.7.0_111 {color} |
| {color:green}+1{color} | {color:green} hbaseprotoc {color} | {color:green} 0m 1s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 1s {color} | {color:green} HBASE-7912 passed with JDK v1.8.0_101 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 1s {color} | {color:green} HBASE-7912 passed with JDK v1.7.0_111 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 1s {color} | {color:green} the patch passed with JDK v1.8.0_101 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 1s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 1s {color} | {color:green} the patch passed with JDK v1.7.0_111 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 1s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red} 0m 0s {color} | {color:red} The patch has 1 line(s) that end in whitespace. Use git apply --whitespace=fix. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 26m 23s {color} | {color:green} Patch does not cause any errors with Hadoop 2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.1 2.6.2 2.6.3 2.7.1. {color} |
| {color:green}+1{color} | {color:green} hbaseprotoc {color} | {color:green} 0m 2s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 2s {color} | {color:green} the patch passed with JDK v1.8.0_101 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 2s {color} | {color:green} the patch passed with JDK v1.7.0_111 {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 27m 5s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=1.11.2 Server=1.11.2 Image:yetus/hbase:date2016-08-24 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12825334/16458.HBASE-7912.v4.txt |
| JIRA Issue | HBASE-16458 |
| Optional Tests |  hadoopcheck  hbaseanti  javac  compile  javadoc  |
| uname | Linux e0ef43fa78a5 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | nobuild |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build@2/component/dev-support/hbase-personality.sh |
| git revision | HBASE-7912 / 7c6b10f |
| Default Java | 1.7.0_111 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_101 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_111 |
| whitespace | https://builds.apache.org/job/PreCommit-HBASE-Build/3251/artifact/patchprocess/whitespace-eol.txt |
| modules | C: . U: . |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/3251/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","24/Aug/16 21:32;yuzhihong@gmail.com;Patch v5 drops TestIncrementalBackupNoDataLoss which is almost identical to TestIncrementalBackup","24/Aug/16 22:05;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 15s {color} | {color:blue} Docker mode activated. {color} |
| {color:blue}0{color} | {color:blue} patch {color} | {color:blue} 0m 1s {color} | {color:blue} The patch file was not named according to hbase's naming conventions. Please see https://yetus.apache.org/documentation/0.3.0/precommit-patchnames for instructions. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 1s {color} | {color:green} HBASE-7912 passed with JDK v1.8.0_101 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 2s {color} | {color:green} HBASE-7912 passed with JDK v1.7.0_111 {color} |
| {color:green}+1{color} | {color:green} hbaseprotoc {color} | {color:green} 0m 1s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 1s {color} | {color:green} HBASE-7912 passed with JDK v1.8.0_101 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 1s {color} | {color:green} HBASE-7912 passed with JDK v1.7.0_111 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 1s {color} | {color:green} the patch passed with JDK v1.8.0_101 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 1s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 2s {color} | {color:green} the patch passed with JDK v1.7.0_111 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 2s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red} 0m 0s {color} | {color:red} The patch has 1 line(s) that end in whitespace. Use git apply --whitespace=fix. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 26m 6s {color} | {color:green} Patch does not cause any errors with Hadoop 2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.1 2.6.2 2.6.3 2.7.1. {color} |
| {color:green}+1{color} | {color:green} hbaseprotoc {color} | {color:green} 0m 3s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 3s {color} | {color:green} the patch passed with JDK v1.8.0_101 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 0m 3s {color} | {color:green} the patch passed with JDK v1.7.0_111 {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 26m 46s {color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=1.11.2 Server=1.11.2 Image:yetus/hbase:date2016-08-24 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12825341/16458.HBASE-7912.v5.txt |
| JIRA Issue | HBASE-16458 |
| Optional Tests |  hadoopcheck  hbaseanti  javac  compile  javadoc  |
| uname | Linux c343bc8abb7f 3.13.0-92-generic #139-Ubuntu SMP Tue Jun 28 20:42:26 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | nobuild |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/component/dev-support/hbase-personality.sh |
| git revision | HBASE-7912 / 7c6b10f |
| Default Java | 1.7.0_111 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_101 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_111 |
| whitespace | https://builds.apache.org/job/PreCommit-HBASE-Build/3252/artifact/patchprocess/whitespace-eol.txt |
| modules | C: . U: . |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/3252/console |
| Powered by | Apache Yetus 0.3.0   http://yetus.apache.org |


This message was automatically generated.

","25/Aug/16 20:07;yuzhihong@gmail.com;Latest changes were confirmed by Vlad offline.

Thanks for the review, Vlad.","06/Sep/18 22:52;vrodionov;Reopened.

Every test class starts a new HBase cluster, because surefire plugin was configured to run every class in a new JVM. This overhead accounts for ~ 25% of a total execution time (63 min total, 17 min cluster start up time). So it make sense to modify backup tests to be able to share single JVM and modify surefire plugin configuration to reuse JVM during B&R UTs execution.","07/Sep/18 18:15;yuzhihong@gmail.com;Assigning to Vlad who has done experiments.","07/Sep/18 21:47;vrodionov;With some tweak TestBackupBase and pom.xml now we can run tests 5x times faster. cc: [~yuzhihong@gmail.com]. 
","07/Sep/18 22:29;yuzhihong@gmail.com;On Linux, with patch, from first test output:
{code}
2018-09-07 22:06:50,491 INFO  [Time-limited test] hbase.ResourceChecker(148): before: backup.TestBackupUtils#TestGetBulkOutputDir Thread=8, OpenFileDescriptor=179, MaxFileDescriptor=32000, SystemLoadAverage=242, ProcessCount=363, AvailableMemoryMB=56614
{code}
to last:
{code}
2018-09-07 22:23:48,010 INFO  [Block report processor] blockmanagement.BlockManager(2645): BLOCK* addStoredBlock: blockMap updated: 127.0.0.1:36058 is added to blk_1073741829_1005{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-453ccfd4-ec24-490b-a51e-2b75f5b1da9f:NORMAL:127.0.0.1:36058|RBW]]} size 146414
2018-09-07 22:23:48,413 INFO  [Thread-3] regionserver.ShutdownHook$ShutdownHookThread(135): Shutdown hook finished.
{code}
That was ~17 minutes.","07/Sep/18 22:51;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 19s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green}  0m  0s{color} | {color:green} Patch does not have any anti-patterns. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m 34s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 31s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 17s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 22s{color} | {color:green} branch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 46s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 16s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 27s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 27s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 14s{color} | {color:red} hbase-backup: The patch generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 15s{color} | {color:green} patch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 10m 44s{color} | {color:green} Patch does not cause any errors with Hadoop 2.7.4 or 3.0.0. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 42s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 14s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 16m 57s{color} | {color:green} hbase-backup in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 19s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 51m 34s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hbase:b002b0b |
| JIRA Issue | HBASE-16458 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12938904/HBASE-16458-v1.patch |
| Optional Tests |  asflicense  javac  javadoc  unit  shadedjars  hadoopcheck  xml  compile  findbugs  hbaseanti  checkstyle  |
| uname | Linux fc7d12345598 3.13.0-144-generic #193-Ubuntu SMP Thu Mar 15 17:03:53 UTC 2018 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/component/dev-support/hbase-personality.sh |
| git revision | master / c3419be003 |
| maven | version: Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-17T18:33:14Z) |
| Default Java | 1.8.0_181 |
| findbugs | v3.1.0-RC3 |
| checkstyle | https://builds.apache.org/job/PreCommit-HBASE-Build/14354/artifact/patchprocess/diff-checkstyle-hbase-backup.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/14354/testReport/ |
| Max. process+thread count | 3521 (vs. ulimit of 10000) |
| modules | C: hbase-backup U: hbase-backup |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/14354/console |
| Powered by | Apache Yetus 0.7.0   http://yetus.apache.org |


This message was automatically generated.

","07/Sep/18 23:35;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 26s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green}  0m  0s{color} | {color:green} Patch does not have any anti-patterns. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 55s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 34s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 17s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 28s{color} | {color:green} branch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 39s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 14s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m  2s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 15s{color} | {color:red} hbase-backup: The patch generated 3 new + 0 unchanged - 0 fixed = 3 total (was 0) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  2s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 52s{color} | {color:green} patch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 12m 23s{color} | {color:green} Patch does not cause any errors with Hadoop 2.7.4 or 3.0.0. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 41s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 14s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 16m 36s{color} | {color:green} hbase-backup in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 20s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 54m 55s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hbase:b002b0b |
| JIRA Issue | HBASE-16458 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12938910/16458-v1.patch |
| Optional Tests |  asflicense  javac  javadoc  unit  shadedjars  hadoopcheck  xml  compile  findbugs  hbaseanti  checkstyle  |
| uname | Linux b9e3bc4c36bc 3.13.0-144-generic #193-Ubuntu SMP Thu Mar 15 17:03:53 UTC 2018 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build@2/component/dev-support/hbase-personality.sh |
| git revision | master / c3419be003 |
| maven | version: Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-17T18:33:14Z) |
| Default Java | 1.8.0_181 |
| findbugs | v3.1.0-RC3 |
| checkstyle | https://builds.apache.org/job/PreCommit-HBASE-Build/14356/artifact/patchprocess/diff-checkstyle-hbase-backup.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/14356/testReport/ |
| Max. process+thread count | 3524 (vs. ulimit of 10000) |
| modules | C: hbase-backup U: hbase-backup |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/14356/console |
| Powered by | Apache Yetus 0.7.0   http://yetus.apache.org |


This message was automatically generated.

","08/Sep/18 00:19;vrodionov;Patch v2 addresses checkstyle warnings.","08/Sep/18 01:31;hadoopqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 24s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green}  0m  0s{color} | {color:green} Patch does not have any anti-patterns. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 46s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 27s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 15s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  3m 59s{color} | {color:green} branch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 36s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 11s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 24s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 24s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  2s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  3m 51s{color} | {color:green} patch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green}  9m 42s{color} | {color:green} Patch does not cause any errors with Hadoop 2.7.4 or 3.0.0. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 40s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 11s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 18m 48s{color} | {color:green} hbase-backup in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 17s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 49m 41s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hbase:b002b0b |
| JIRA Issue | HBASE-16458 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12938927/HBASE-16458-v2.patch |
| Optional Tests |  asflicense  javac  javadoc  unit  shadedjars  hadoopcheck  xml  compile  findbugs  hbaseanti  checkstyle  |
| uname | Linux 481fb4c9fb9e 4.4.0-133-generic #159-Ubuntu SMP Fri Aug 10 07:31:43 UTC 2018 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/component/dev-support/hbase-personality.sh |
| git revision | master / 9af7bc6204 |
| maven | version: Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-17T18:33:14Z) |
| Default Java | 1.8.0_181 |
| findbugs | v3.1.0-RC3 |
|  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/14359/testReport/ |
| Max. process+thread count | 3693 (vs. ulimit of 10000) |
| modules | C: hbase-backup U: hbase-backup |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/14359/console |
| Powered by | Apache Yetus 0.7.0   http://yetus.apache.org |


This message was automatically generated.

","08/Sep/18 04:18;yuzhihong@gmail.com;Vlad:
Can you take a look at 16458.v2.txt ?

This is based on your patch, using shutdown hook to tear down the mini-cluster at the end of last test which is subclass of TestBackupBase.

","08/Sep/18 07:17;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 12s{color} | {color:blue} Docker mode activated. {color} |
| {color:blue}0{color} | {color:blue} patch {color} | {color:blue}  0m  1s{color} | {color:blue} The patch file was not named according to hbase's naming conventions. Please see https://yetus.apache.org/documentation/0.7.0/precommit-patchnames for instructions. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green}  0m  0s{color} | {color:green} Patch does not have any anti-patterns. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 24s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 53s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  2m 10s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 23s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 10s{color} | {color:green} branch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 38s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 44s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 54s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  2m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  2m 12s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 14s{color} | {color:red} hbase-backup: The patch generated 11 new + 0 unchanged - 0 fixed = 11 total (was 0) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 14s{color} | {color:green} patch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 10m 28s{color} | {color:green} Patch does not cause any errors with Hadoop 2.7.4 or 3.0.0. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 49s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}119m 10s{color} | {color:green} hbase-server in the patch passed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 13m  3s{color} | {color:green} hbase-backup in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 45s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}177m 17s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hbase:b002b0b |
| JIRA Issue | HBASE-16458 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12938936/16458.v2.txt |
| Optional Tests |  asflicense  javac  javadoc  unit  shadedjars  hadoopcheck  xml  compile  findbugs  hbaseanti  checkstyle  |
| uname | Linux d2e3b90b1b48 3.13.0-143-generic #192-Ubuntu SMP Tue Feb 27 10:45:36 UTC 2018 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/component/dev-support/hbase-personality.sh |
| git revision | master / b04b4b0fd1 |
| maven | version: Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-17T18:33:14Z) |
| Default Java | 1.8.0_181 |
| findbugs | v3.1.0-RC3 |
| checkstyle | https://builds.apache.org/job/PreCommit-HBASE-Build/14361/artifact/patchprocess/diff-checkstyle-hbase-backup.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/14361/testReport/ |
| Max. process+thread count | 5471 (vs. ulimit of 10000) |
| modules | C: hbase-server hbase-backup U: . |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/14361/console |
| Powered by | Apache Yetus 0.7.0   http://yetus.apache.org |


This message was automatically generated.

","08/Sep/18 12:08;yuzhihong@gmail.com;16458.v4.txt for checkstyle warnings.","08/Sep/18 15:12;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 14s{color} | {color:blue} Docker mode activated. {color} |
| {color:blue}0{color} | {color:blue} patch {color} | {color:blue}  0m  1s{color} | {color:blue} The patch file was not named according to hbase's naming conventions. Please see https://yetus.apache.org/documentation/0.7.0/precommit-patchnames for instructions. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green}  0m  0s{color} | {color:green} Patch does not have any anti-patterns. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 15s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 59s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  2m 11s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 23s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m  8s{color} | {color:green} branch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 50s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 54s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 14s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m 20s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  2m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  2m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 26s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 22s{color} | {color:green} patch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 10m 44s{color} | {color:green} Patch does not cause any errors with Hadoop 2.7.4 or 3.0.0. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 49s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 43s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}122m 36s{color} | {color:red} hbase-server in the patch failed. {color} |
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 13m 10s{color} | {color:green} hbase-backup in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 49s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}182m  7s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hbase.replication.TestReplicationDroppedTables |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hbase:b002b0b |
| JIRA Issue | HBASE-16458 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12938948/16458.v4.txt |
| Optional Tests |  asflicense  javac  javadoc  unit  shadedjars  hadoopcheck  xml  compile  findbugs  hbaseanti  checkstyle  |
| uname | Linux 188b5cbf8bca 3.13.0-143-generic #192-Ubuntu SMP Tue Feb 27 10:45:36 UTC 2018 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/component/dev-support/hbase-personality.sh |
| git revision | master / b04b4b0fd1 |
| maven | version: Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-17T18:33:14Z) |
| Default Java | 1.8.0_181 |
| findbugs | v3.1.0-RC3 |
| unit | https://builds.apache.org/job/PreCommit-HBASE-Build/14362/artifact/patchprocess/patch-unit-hbase-server.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/14362/testReport/ |
| Max. process+thread count | 4852 (vs. ulimit of 10000) |
| modules | C: hbase-server hbase-backup U: . |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/14362/console |
| Powered by | Apache Yetus 0.7.0   http://yetus.apache.org |


This message was automatically generated.

","08/Sep/18 17:01;hadoopqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
| {color:blue}0{color} | {color:blue} patch {color} | {color:blue}  0m  1s{color} | {color:blue} The patch file was not named according to hbase's naming conventions. Please see https://yetus.apache.org/documentation/0.7.0/precommit-patchnames for instructions. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green}  0m  0s{color} | {color:green} Patch does not have any anti-patterns. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m 45s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 33s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 16s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  5m 12s{color} | {color:green} branch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 46s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 20s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 19s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 45s{color} | {color:green} patch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} hadoopcheck {color} | {color:green} 12m  1s{color} | {color:green} Patch does not cause any errors with Hadoop 2.7.4 or 3.0.0. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 45s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 15s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green} 13m  3s{color} | {color:green} hbase-backup in the patch passed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 11s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 51m 28s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hbase:b002b0b |
| JIRA Issue | HBASE-16458 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12938959/16458.v5.txt |
| Optional Tests |  asflicense  javac  javadoc  unit  shadedjars  hadoopcheck  xml  compile  findbugs  hbaseanti  checkstyle  |
| uname | Linux 7f12ffd8f189 3.13.0-143-generic #192-Ubuntu SMP Tue Feb 27 10:45:36 UTC 2018 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/component/dev-support/hbase-personality.sh |
| git revision | master / b04b4b0fd1 |
| maven | version: Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-17T18:33:14Z) |
| Default Java | 1.8.0_181 |
| findbugs | v3.1.0-RC3 |
|  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/14364/testReport/ |
| Max. process+thread count | 3370 (vs. ulimit of 10000) |
| modules | C: hbase-backup U: hbase-backup |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/14364/console |
| Powered by | Apache Yetus 0.7.0   http://yetus.apache.org |


This message was automatically generated.

","08/Sep/18 18:15;vrodionov;[~tedyu@apache.org], you did not need to install shutdown hook. tearDown works the same way. I removed tearDown because there is no need to clean up after test, as since each test is executed in a separate JVM instance. This actually saved almost 30% of overall execution time. ","08/Sep/18 19:27;yuzhihong@gmail.com;From the test console, you would see:
{code}
17:01:53 |  +1  |           unit  |  13m  3s   | hbase-backup in the patch passed. 
{code}
There is no slowdown by tearing down cluster thru shutdown hook.

We should perform mini cluster shutdown to remove intermediate files generated during test runs.
Otherwise there is chance that such files stay on the Jenkins machine(s).","09/Sep/18 15:17;mdrob;I'd like to review this patch but I'm confused which file I should be looking at. Use of shutdown hooks caught my attention and is generally inadvisable.","09/Sep/18 15:21;yuzhihong@gmail.com;HBASE-16458-v2.patch was the last one from Vlad.

Patch v5 is from me, based on Vlad's v2.","09/Sep/18 18:34;vrodionov;[~ted_yu], cleanup must be done after *all* unit tests. Just *mvn clean* should work fine.","13/Sep/18 19:09;elserj;{quote}each test is executed in a separate JVM instance
{quote}
Interesting. Didn't realize we had resuseForks disabled in HBase :). Seems like as long as the surefire-plugin is configured this way, your change is fine.
{quote}This actually saved almost 30% of overall execution time.
{quote}
That's.. crazy. Did you dig in to see why this was the case? Curious to know how much of it is in ""our"" code compared to Hadoop's.","13/Sep/18 19:15;vrodionov;Yes, with tearDown it took 63 min, w/o - 44 min. What tearDown does is 

# Cleaning snapshots
# stopping hbase and yarn mini clusters

I did not spend time on analyzing this. Just an observation."
Write test asserting desired priority of RS->Master RPCs,HBASE-13603,12826367,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,elserj,elserj,elserj,30/Apr/15 17:31,02/Mar/18 01:28,18/Feb/21 10:10,,,,,,,,,,IPC/RPC,test,,,,,0,,,"From HBASE-13351:

{quote}
Any way we can write a FT test to assert that the RS->Master APIs are treated with higher priority. I see your UT for asserting the annotation.
{quote}

Write a test that verifies expected RPCs are run on the correct pools in as real of an environment possible.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-08-16 05:56:52.507,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 16 05:56:52 UTC 2016,,,,,,,"0|i2e5g7:",9223372036854775807,,,,,,,,,,,,,,,,,"16/Aug/16 05:56;stack;No patch. Minor issue. Moved out of 1.2.3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create/expand unit test to exercise htrace instrumentation,HBASE-13458,12820619,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,elserj,elserj,elserj,13/Apr/15 19:58,30/Jun/17 12:08,18/Feb/21 10:10,,,,,,,,,,test,,,,,,0,,,"From HBASE-13078, [~ndimiduk] suggested that we can also add a Medium/Large unit test that does some more assertions over the HTrace instrumentation. Some loose goals:

* Try to verify that spans continue from HBase into HDFS
* Ensure that user-created spans continue into HBase
* Validate expected API calls have expected instrumentation

Other ideas that people have? Any pain points experienced prior by people?",,,,,,,,,,,HBASE-13078,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2015-04-13 19:58:18.0,,,,,,,"0|i2d70v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid writing files to disk in unit tests,HBASE-13144,12778959,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,apurtell,apurtell,03/Mar/15 05:32,03/Mar/15 05:34,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,,,"We have a number of unit tests that read and write relatively small files, but do it often and run under a timeout clock. If the build or test host is virtual or IO contended or both then the test can become flaky. Even if the host is up to the IO pressure the test would run a lot faster if we were not asking the OS to persist anything. Consider both:
- If built against Hadoop 2.6+, spin up miniclusters that keep all blocks in memory-only storage
- Write a simple Hadoop filesystem implementation (or find one) that keeps all state in memory and use in lieu of LocalFileSystem for unit tests. ",,,,,,,,,,,HBASE-13143,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,2015-03-03 05:32:28.0,,,,,,,"0|i269w7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestCacheOnWrite#testStoreFileCacheOnWrite sometimes fails with HFileBlock cached assertion failure,HBASE-12789,12764326,Test,Reopened,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,yuzhihong@gmail.com,yuzhihong@gmail.com,31/Dec/14 01:03,21/May/15 22:42,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,,,"Since cacheOnWrite is INDEX_BLOCKS, ENCODED_DATA or DATA blocks should not be cached (shouldBeCached: false) but are still being cached (isCached: true).
I have also seen this happening when cacheOnWrite is BLOOM_BLOCKS.

From https://builds.apache.org/job/HBase-TRUNK/5980/testReport/junit/org.apache.hadoop.hbase.io.hfile/TestCacheOnWrite/testStoreFileCacheOnWrite_106_/ :
{code}
java.lang.AssertionError: shouldBeCached: false
isCached: true
Test description: [cacheOnWrite=INDEX_BLOCKS, compress=GZ, encoderType=BLOCK_ENCODING_EVERYWHERE, cacheCompressedData=false]
block: HFileBlock [ fileOffset=12046 headerSize()=33 blockType=ENCODED_DATA onDiskSizeWithoutHeader=1325 uncompressedSizeWithoutHeader=1591 prevBlockOffset=10457 isUseHBaseChecksum()=true checksumType=NULL bytesPerChecksum=512 onDiskDataSizeWithHeader=1346 getOnDiskSizeWithHeader()=1358 totalChecksumBytes()=12 isUnpacked()=true buf=[ java.nio.HeapByteBuffer[pos=0 lim=1636 cap=1669] ] dataBeginsWith=\x00\x02\x00\x00\x08N.\x1D\x00\x00 aaaaaaaaaaaaaaaaaaaaa fileContext=HFileContext [ usesHBaseChecksum=true checksumType=NULL bytesPerChecksum=512 blocksize=65536 encoding=NONE includesMvcc=true includesTags=true compressAlgo=NONE compressTags=false cryptoContext=[ cipher=NONE keyHash=NONE ] ] ]
encodingInCache: PREFIX
blockCacheKey: bfdf56eb18464371a17ed05500e189d6_12046
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.hadoop.hbase.io.hfile.TestCacheOnWrite.readStoreFile(TestCacheOnWrite.java:285)
	at org.apache.hadoop.hbase.io.hfile.TestCacheOnWrite.testStoreFileCacheOnWriteInternals(TestCacheOnWrite.java:244)
	at org.apache.hadoop.hbase.io.hfile.TestCacheOnWrite.testStoreFileCacheOnWrite(TestCacheOnWrite.java:239)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-12-31 01:20:00.846,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 21 22:38:47 UTC 2015,,,,,,,"0|i23vcf:",9223372036854775807,,,,,,,,,,,,,,,,,"31/Dec/14 01:20;stack;Copy/paste of our build box output with no input.","31/Dec/14 02:45;yuzhihong@gmail.com;The test failed here too:
https://builds.apache.org/job/HBase-0.98-on-Hadoop-1.1/734/testReport/junit/org.apache.hadoop.hbase.io.hfile/TestCacheOnWrite/testStoreFileCacheOnWrite_105_/","21/May/15 22:38;appy;Reopening. Updating description.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generate hbase-site2.xml from contents of hbase-site.xml when test runs,HBASE-12756,12763586,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,yuzhihong@gmail.com,yuzhihong@gmail.com,24/Dec/14 00:01,18/Sep/17 17:55,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,beginner,,"During review of HBASE-12651, Enis made the following comment:

hbase-site2.xml will diverge from hbase-site.xml very quickly since devs will not know about this file, and will not change it then they add a new conf option. Then, when this test overwrites the file, concurrently running tests will be affected. I think the unit test should be changed so that it dynamically re-writes the file with the added config option rather than committing the hbase-site2.xml file.

This JIRA is for generating hbase-site2.xml based on the contents of hbase-site.xml at runtime.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-12-24 03:04:12.797,,,false,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 24 03:04:12 UTC 2014,,,,,,,"0|i23qvb:",9223372036854775807,,,,,,,,,,,,,,,,,"24/Dec/14 03:04;stack;Can't you just load up a file that has only your differences in it (See how HBaseConfiguration constructor works loading hbase-default and hbase-site).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add unit test for the fix that sorts custom value of BUCKET_CACHE_BUCKETS_KEY,HBASE-11743,12734063,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,yuzhihong@gmail.com,yuzhihong@gmail.com,14/Aug/14 13:52,10/Nov/17 21:23,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,,,"HBASE-11550 sorts the custom value of BUCKET_CACHE_BUCKETS_KEY such that there is no wastage in bucket allocation.

This JIRA is to add unit test for the fix so that there is no regression in the future.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,412091,,,,Tue Oct 17 01:00:53 UTC 2017,,,,,,,"0|i1yw6f:",412081,,,,,,,,,,,,,,,,,"17/Oct/17 01:00;yuzhihong@gmail.com;[~gustavoanatoly]:
Are you working on this ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose PolicyBasedChaosMonkey on the integration test command line,HBASE-11680,12732147,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,apurtell,apurtell,05/Aug/14 21:43,05/Aug/14 21:50,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,beginner,,"PolicyBasedChaosMonkey can be constructed with a collection of Policies, which in turn can be composed with one or more Actions. Expose PolicyBasedChaosMonkey via MonkeyFactory. Add support for parsing a supplied policy file (we have generic support for loading a properties file) into a PolicyBasedChaosMonkey instance.",,,,,,,,,,,,,HBASE-11672,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,410176,,,,2014-08-05 21:43:50.0,,,,,,,"0|i1ykkf:",410168,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestMultiParallel#testNonceCollision occasionally fails with OperationConflictException,HBASE-10724,12700848,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,yuzhihong@gmail.com,yuzhihong@gmail.com,11/Mar/14 23:05,20/Aug/14 14:11,18/Feb/21 10:10,,,,,,,,,,,,,,,,1,,,"From https://builds.apache.org/job/HBase-0.98/220/testReport/junit/org.apache.hadoop.hbase.client/TestMultiParallel/testNonceCollision/ :
{code}
org.apache.hadoop.hbase.exceptions.OperationConflictException: org.apache.hadoop.hbase.exceptions.OperationConflictException: The operation with nonce {-1778587827371821880, 5283077739350761367} on row [xxx] may have already completed
	at org.apache.hadoop.hbase.regionserver.HRegionServer.startNonceOperation(HRegionServer.java:4159)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.increment(HRegionServer.java:4123)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.mutate(HRegionServer.java:2888)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:28452)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2012)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)
	at java.lang.Thread.run(Thread.java:662)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:95)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:284)
	at org.apache.hadoop.hbase.client.HTable$7.call(HTable.java:1053)
	at org.apache.hadoop.hbase.client.HTable$7.call(HTable.java:1043)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:114)
	at org.apache.hadoop.hbase.client.HTable.increment(HTable.java:1057)
	at org.apache.hadoop.hbase.client.TestMultiParallel.testNonceCollision(TestMultiParallel.java:516)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.exceptions.OperationConflictException): org.apache.hadoop.hbase.exceptions.OperationConflictException: The operation with nonce {-1778587827371821880, 5283077739350761367} on row [xxx] may have already completed
	at org.apache.hadoop.hbase.regionserver.HRegionServer.startNonceOperation(HRegionServer.java:4159)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.increment(HRegionServer.java:4123)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.mutate(HRegionServer.java:2888)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:28452)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2012)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)
	at java.lang.Thread.run(Thread.java:662)

	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1450)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1654)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1712)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.mutate(ClientProtos.java:28845)
	at org.apache.hadoop.hbase.client.HTable$7.call(HTable.java:1050)
{code}
Exception came from the first call to table.increment():
{code}
    try {
      Increment inc = new Increment(ONE_ROW);
      inc.addColumn(BYTES_FAMILY, QUALIFIER, 1L);
      table.increment(inc);
{code}
There seemed to be race between the first and second table.increment() calls.
In the above case, the first call received OperationConflictException",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-03-12 21:42:37.241,,,false,,,,,,,,,,,,,,,,,379191,,,,Wed Aug 20 14:11:31 UTC 2014,,,,,,,"0|i1tc5z:",379483,,,,,,,,,,,,,,,,,"12/Mar/14 21:42;stack;Patch?","20/Aug/14 14:11;torgonwoodget;Caught this problem  in a real application during a mass insert process using several hundreds of python processes to load statistical back data:

IOError(_message='org.apache.hadoop.hbase.exceptions.OperationConflictException: The operation with nonce {-2298743813494479781, -5039431054343659404} on row [0_741_2014081720_b325aea5092ebac621ab1c7d0144cf9c] may have already completed
	at org.apache.hadoop.hbase.regionserver.HRegionServer.startNonceOperation(HRegionServer.java:4199)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.increment(HRegionServer.java:4163)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.mutate(HRegionServer.java:2890)
	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:29495)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2012)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:160)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:38)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:110)
	at java.lang.Thread.run(Thread.java:745)
')",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermittent TestDistributedLogSplitting#testLogReplayForDisablingTable failure,HBASE-10404,12690531,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,yuzhihong@gmail.com,yuzhihong@gmail.com,yuzhihong@gmail.com,23/Jan/14 02:32,23/Jan/14 02:32,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,,,"Here was the assertion failure:
{code} java.lang.AssertionError: expected:&lt;1000> but was:&lt;0> at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.failNotEquals(Assert.java:743) at org.junit.Assert.assertEquals(Assert.java:118) at org.junit.Assert.assertEquals(Assert.java:555) at org.junit.Assert.assertEquals(Assert.java:542) at org.apache.hadoop.hbase.master.TestDistributedLogSplitting.testLogReplayForDisablingTable(TestDistributedLogSplitting.java:838) 
{code}
This was due to the loop starting around line 823 not picking up any edits ",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,369451,,,,2014-01-23 02:32:19.0,,,,,,,"0|i1roen:",369756,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test-patch.sh should state the reason why patch is not tested if patch filename is not recognized,HBASE-10253,12686654,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,yuzhihong@gmail.com,yuzhihong@gmail.com,29/Dec/13 05:25,29/Dec/13 05:25,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,,,"Currently if patch filename is not recognized (unknown file extension, e.g.), we would see the following in post back:
{code}
-1 overall. Here are the results of testing the latest attachment 
http://issues.apache.org
against trunk revision .
ATTACHMENT ID: http:
{code}
In this situation, post back should indicate the reason why patch is not tested.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,365647,,,,2013-12-29 05:25:34.0,,,,,,,"0|i1r113:",365954,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix EncodedSeekPerformanceTest,HBASE-10122,12683918,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,jmhsieh,jxiang,jxiang,10/Dec/13 21:12,05/Dec/14 02:13,18/Feb/21 10:10,,,,,,,,,,test,,,,,,0,,,"After HBASE-9870, the encoding in cache is always the same as that on disk. EncodedSeekPerformanceTest relies that the two encodings are different. So now, it is not working as expected. We need to fix it if the tool is still needed.",,,,,,,,,,,,,,"10/Dec/13 21:32;jxiang;trunk-10122.patch;https://issues.apache.org/jira/secure/attachment/12618102/trunk-10122.patch",,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2013-12-11 05:14:18.732,,,false,,,,,,,,,,,,,,,,,362990,,,,Wed Dec 11 16:32:35 UTC 2013,,,,,,,"0|i1qkjz:",363296,,,,,,,,,,,,,,,,,"10/Dec/13 21:32;jxiang;One option is to remove this test. To test encoded seek performance, we have to use something else, like ycsb?","11/Dec/13 05:14;jmhsieh;I have a rough patch that adds a hbase-perf module and with a little modification this could be a fine microbenchmark.  To fix this, I think we'd add methods that create properly encoded data and report its performance.  

If you want, as long as the current version compiles, you can assign it to me and I'll take care of it.
","11/Dec/13 16:32;jxiang;A microbenchmark sounds great.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for Namenode HA to test infrastructure,HBASE-10041,12681430,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,yuzhihong@gmail.com,yuzhihong@gmail.com,26/Nov/13 22:23,26/Nov/13 22:28,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,,,"Currently MiniDFSCluster is used for testing hdfs cluster.
In hadoop-2, MiniDFSCluster supports HA mode through its Builder.
MiniJournalCluster allows testing against QJM.

There is no such capability in hadoop-1

Support for Namenode HA should be added to test infrastructure.
One possibility is through hbase-hadoop-compat/src/test/java/org/apache/hadoop/hbase/HadoopShims.java",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,360695,,,,2013-11-26 22:23:35.0,,,,,,,"0|i1q6f3:",360994,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestCompaction#testMultipleCustomCompactionRequests hangs when stripe compaction is used,HBASE-9937,12678401,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,yuzhihong@gmail.com,yuzhihong@gmail.com,09/Nov/13 16:06,09/Nov/13 16:06,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,,,"When I plugin StripeStoreEngine, TestCompaction#testMultipleCustomCompactionRequests hangs.

Attached is the stack trace.",,,,,,,,,,,,,,"09/Nov/13 16:06;yuzhihong@gmail.com;testCompaction#testMultipleCustomCompactionRequests.stack;https://issues.apache.org/jira/secure/attachment/12612995/testCompaction%23testMultipleCustomCompactionRequests.stack",,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,357776,,,,2013-11-09 16:06:26.0,,,,,,,"0|i1podj:",358066,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LoadTestTool should be callable using code,HBASE-9824,12675176,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,sershe,sershe,23/Oct/13 02:34,05/Aug/14 20:06,18/Feb/21 10:10,,,,,,,,,,test,,,,,,0,beginner,,"The way we use LoadTestTool from tests now is not good. Why we have to use the class that is directly visible over in the next jar by stitching a bunch of command line options is not clear.
We need to use MTW & MTR directly from test code, or add fancy stuff like actual methods to LoadTestTool.
Table creation in LoadTestTool is also fragile and inflexible (from command line options), not sure what to do about that.",,,,,,,,,,,HBASE-9846,HBASE-9858,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,354796,,,,2013-10-23 02:34:19.0,,,,,,,"0|i1p60f:",355085,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add PerfEval tool for BlockCache,HBASE-9806,12674605,Test,Patch Available,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,ndimiduk,ndimiduk,ndimiduk,18/Oct/13 22:13,14/May/19 03:08,18/Feb/21 10:10,,,,,,,,,,BlockCache,Performance,test,,,,0,,,We have at least three different block caching layers with myriad configuration settings. Let's add a tool for evaluating memory allocations and configuration combinations with different access patterns.,,,,,,,,,,,,,HBASE-4812,"18/Oct/13 23:07;ndimiduk;HBASE-9806.00.patch;https://issues.apache.org/jira/secure/attachment/12609232/HBASE-9806.00.patch","23/Oct/13 03:08;ndimiduk;HBASE-9806.01.patch;https://issues.apache.org/jira/secure/attachment/12609789/HBASE-9806.01.patch","09/Dec/13 00:05;ndimiduk;HBASE-9806.02.patch;https://issues.apache.org/jira/secure/attachment/12617766/HBASE-9806.02.patch","03/Nov/13 04:03;ndimiduk;conf_20g.patch;https://issues.apache.org/jira/secure/attachment/12611801/conf_20g.patch","03/Nov/13 04:03;ndimiduk;conf_3g.patch;https://issues.apache.org/jira/secure/attachment/12611800/conf_3g.patch","03/Nov/13 04:03;ndimiduk;test1_run1_20g.pdf;https://issues.apache.org/jira/secure/attachment/12611803/test1_run1_20g.pdf","03/Nov/13 04:03;ndimiduk;test1_run1_3g.pdf;https://issues.apache.org/jira/secure/attachment/12611802/test1_run1_3g.pdf","03/Nov/13 04:03;ndimiduk;test1_run2_20g.pdf;https://issues.apache.org/jira/secure/attachment/12611805/test1_run2_20g.pdf","03/Nov/13 04:03;ndimiduk;test1_run2_3g.pdf;https://issues.apache.org/jira/secure/attachment/12611804/test1_run2_3g.pdf",,,,9.0,,,,,,,,,,,,,,,,,,,,2013-10-20 11:16:23.203,,,false,,,,,,,,,,,,,,,,,354227,,,,Tue May 14 03:08:25 UTC 2019,,,,,,,"0|i1p2if:",354517,,,,,,,,,,,,,,,,,"18/Oct/13 23:07;ndimiduk;Here's an initial version, based heavily on HFilerPerfEval code. Would be nice to reduce duplication between the two. Sample output, for reference.

{noformat}
$ ./bin/hbase org.apache.hadoop.hbase.BlockCachePerformanceEvaluation --help
usage: bin/hbase org.apache.hadoop.hbase.BlockCachePerformanceEvaluation
                 <options>
Options:
 -b,--blocksize <arg>    Override the default blocksize (65536).
 -h,--help               Show usage
 -i,--iterations <arg>   The number of times to run a test (default: 100).
 -s,--seed <arg>         Specify a seed value for the random generator.
$ ./bin/hbase org.apache.hadoop.hbase.BlockCachePerformanceEvaluation -i 10
2013-10-18 15:56:14,174 INFO  [main] hbase.BlockCachePerformanceEvaluation: Using random seed: 1382136974160
2013-10-18 15:56:14,994 WARN  [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2013-10-18 15:56:15,337 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 386.7 M
2013-10-18 15:56:15,362 INFO  [main] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2013-10-18 15:56:15,364 INFO  [main] util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C
2013-10-18 15:56:15,434 WARN  [main] conf.Configuration: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
2013-10-18 15:56:30,720 INFO  [main] hbase.BlockCachePerformanceEvaluation: Running UniformRandomSmallScanBenchmark 10 time(s).
2013-10-18 15:56:33,194 INFO  [main] hbase.BlockCachePerformanceEvaluation: Running UniformRandomReadBenchmark 10 time(s).
2013-10-18 15:57:51,237 INFO  [main] hbase.BlockCachePerformanceEvaluation: Running GaussianRandomReadBenchmark 10 time(s).
2013-10-18 15:58:07,599 INFO  [main] hbase.BlockCachePerformanceEvaluation: Not able to seekTo 00000-5419
2013-10-18 15:58:34,024 INFO  [main] hbase.BlockCachePerformanceEvaluation: Running SequentialReadBenchmark 10 time(s).
2013-10-18 15:58:39,664 INFO  [main] hbase.BlockCachePerformanceEvaluation: UniformRandomReadBenchmark: [7644, 10228, 8438, 8415, 8310, 7814, 7155, 7033, 6633, 6363], avg: 7803.3 ms.
2013-10-18 15:58:39,665 INFO  [main] hbase.BlockCachePerformanceEvaluation: UniformRandomSmallScanBenchmark: [832, 172, 171, 171, 171, 185, 186, 169, 174, 171], avg: 240.2 ms.
2013-10-18 15:58:39,665 INFO  [main] hbase.BlockCachePerformanceEvaluation: SequentialReadBenchmark: [300, 545, 713, 460, 512, 679, 599, 672, 515, 582], avg: 557.7 ms.
2013-10-18 15:58:39,665 INFO  [main] hbase.BlockCachePerformanceEvaluation: GaussianRandomReadBenchmark: [4017, 4175, 4790, 4346, 4242, 4690, 4092, 4142, 4217, 4068], avg: 4277.9 ms.
{noformat}","20/Oct/13 11:16;jmspaggi;Can we not do this via PE? Having a PerfTest driver might be good. See https://issues.apache.org/jira/browse/HBASE-9599 .

That way we know easily where to look at for perf testing? Instead of looking at 20 different classes?","21/Oct/13 18:17;ndimiduk;PerformanceEvaluation goes through the HTableInterface; I'm looking for a component-level benchmark. Agreed, this would qualify as a tool to register in whatever comes out of HBASE-9599.
","23/Oct/13 03:08;ndimiduk;Updated patch after running with a couple different configurations. Hopefully I'll get to run on a larger memory machine tomorrow. I have a couple more things to expose as configuration and I'd like to get it to the point where it can automatically determine the amount of data to write based on different cache:data ratios. I also tried running multiple concurrent the cache consumers, but saw warnings about double-storing blocks. Will investigate that further before adding.

[~jmspaggi] how much RAM do your test machines have?","23/Oct/13 03:20;jmspaggi;They have 16GB by default but I can get 16GB from one and put it on another one. So. Options are:
1) I can run on a 4 nodes cluster where all nodes will have 16GB
2) I can run on 2 standalone/pseudo-dist servers (1 SSD, 1SATA) with 32GB.

Of course, I also have some nodes where I have only 4GB ;) But I don't think you are interested but those one ;)","23/Oct/13 04:36;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12609789/HBASE-9806.01.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

    {color:red}-1 site{color}.  The patch appears to cause mvn site goal to fail.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/7603//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7603//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7603//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7603//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7603//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7603//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7603//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7603//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7603//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7603//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/7603//console

This message is automatically generated.","23/Oct/13 17:07;ndimiduk;[~jmspaggi]

This test runs on a single host, not a cluster. I've been running it against local fs; it's only profiling the behavior of the BlockCache implementation, so nothing else is required.

I just got my 24GB machines back, so I'll start with one of those. If you'd like to create a frankenbox with 32GB and run some tests, that would be cool too. Since you have SSD, you could also profile the bucketcache, which would be cool. I can attach some reference configurations I've been playing with to get you started.","23/Oct/13 19:11;ndimiduk;[~tlipcon] [~li] [~jdcryans]

Any chance one of you has the configs laying around from the benchmarks run on the slabcache [announcement post|http://blog.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/]? I'm attempting to reproduce and slabcache looks pretty unstable. I wonder if I'm exercising some edge-case based on my configuration or if there's some bit-rot happened.

Specifically, I'm looking for values for HBASE_HEAPSIZE, -XX:MaxDirectMemorySize, hfile.block.cache.size, hbase.regionserver.global.memstore.upperLimit, hbase.regionserver.global.memstore.lowerLimit, hbase.offheapcache.percentage, hbase.offheapcache.slab.proportions, and anything else that was using a non-default value.

Thanks.","24/Oct/13 14:06;jmspaggi;Sure! I will create a ""frankenbox"" and try that. I will have physical access to the computers only Friday evening. So I might be able to configure that this week-end.

Also, I found a motherboard where I can put 64GB!!!! A bit expensive, but I will most probably buy it so we can test with high memory values and see how it reacts! Donations are welcome ;) Hahaha.

I will need some details on what you want me to run 0.96.0 vs 0.96.0+9806 ? Anything else? Specific settings?","27/Oct/13 13:09;jmspaggi;Ok. Server is ready.

32GB + 1 SDD + 5 SATA.

Do you want that to run in standalone in the SSD? Or pseudo-dist with all the disks and ""namenode"" directory in SATA?

So far I'm going to run 0.96.0 and 0.96.1-SNAPSHOT+9806. Nothing else? Please advice.","01/Nov/13 18:23;vrodionov;Any results so far?","01/Nov/13 18:26;jmspaggi;Hi [~vrodionov], I was waiting to know what I should test... Server is ready, just waiting for guidelines.

question was ""Do you want that to run in standalone in the SSD? Or pseudo-dist with all the disks and ""namenode"" directory in SATA?""

And also regarding what to run with: ""So far I'm going to run 0.96.0 and 0.96.1-SNAPSHOT+9806. Nothing else?""

As soon as I know that I start the tests. As I said, server is ready and off for now. Just waiting for the work ;)","03/Nov/13 04:03;ndimiduk;Sorry for the delays, [~jmspaggi] -- Strata and all that. I wanted to add a kind of self-tuning to the patch, give it the ability to write records until it fills up the cache and no more. No luck as of yet, which means determining how many rows to write requires a little guess-work and checking the logs.

I've run the test for 3g (-r 1700000) and 20g (-r 12000000) heaps using the attached configs and 100 iterations. Attached also are the charts I generated from the logs. I tried 1000 iterations to see if anything happens over a longer interval but nothing exciting. I'm also updating my log-parsing script to overly the GC events. More to follow.

On your fancy rig, can you run for maybe 8g, 16g, 20g, and 28g? You'll have to apply one of these conf patches and then adjust the heap size yourself. You'll also need to play with the -r param to work out how many rows you can fit into the cache w.o eviction (please let me know what you end up using!) -i 100 should be a good starting point, but if you have time I'd take logs from 500 or 1000.

Thank you much!","03/Nov/13 04:13;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12611805/test1_run2_20g.pdf
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:red}-1 tests included{color}.  The patch doesn't appear to include any new or modified tests.
                        Please justify why no new tests are needed for this patch.
                        Also please list what manual steps were performed to verify this patch.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/7718//console

This message is automatically generated.","22/Nov/13 18:07;ndimiduk;Heya [~jmspaggi]. Any chance you had a moment to run this? I'm leaving on holiday next week and I'd like to tie up some loose ends. Even just running it, verifying it works for you, so I can get it committed and start improving and refactoring these perf tools when I get back.","23/Nov/13 20:28;jmspaggi;Sure! I might be able to give it a try tonight of tomorrow... I keep you posted.","07/Dec/13 13:06;jmspaggi;Working on that right now...","07/Dec/13 17:21;jmspaggi;Hum. Seems to not compile in Trunk:
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /home/jmspaggi/trunk/hbase-server/src/test/java/org/apache/hadoop/hbase/BlockCachePerformanceEvaluation.java:[327,20] error: no suitable method found for createReader(FileSystem,Path,CacheConfig)

I will take a look","07/Dec/13 17:35;jmspaggi;{code}
reader = HFile.createReader(fs, mf, cacheConfig);
{code}

Need to be
{code}
reader = HFile.createReader(fs, mf, cacheConfig, conf);
{code}","09/Dec/13 00:05;ndimiduk;bq. Seems to not compile in Trunk

Yep, there has been drift. Here's an updated patch.","09/Dec/13 01:27;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12617766/HBASE-9806.02.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop1.1{color}.  The patch compiles against the hadoop 1.1 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

    {color:red}-1 site{color}.  The patch appears to cause mvn site goal to fail.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/8092//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8092//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8092//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8092//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8092//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8092//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8092//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8092//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8092//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8092//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/8092//console

This message is automatically generated.","19/May/14 19:47;stack;Should this be in the io.hfile package since that is where BC's normally fester.

PE has more than PerfEval at the moment (histograms, rows by data size). This does gaussian rows which PE doesn't have which would be nice (but for BlockCache testing we want random-sized data I'd say).  Should this go in [~ndimiduk]?  Or want me to adopt to PE?   Add a random data size?  Will help yeah w/ looking at BC options quickly on one machine.

","20/May/14 10:52;ndimiduk;Moving to the io.hfile package makes sense. Maybe we merge this one into HFilePerfEval, as these are closely related?

I'd like to add support for more schema varieties and access distributions on PerfEval, similar to what these tools have. The whole business could use some refactor toward more code sharing. It all depends on the goal of the tool though. I like PerfEval because it's closer to what a user will see. OTOH, it makes it difficult for an hbase dev to get a sense for the IO subsystem pieces in isolation.","20/May/14 15:54;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12617766/HBASE-9806.02.patch
  against trunk revision .
  ATTACHMENT ID: 12617766

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 javadoc{color}.  The javadoc tool appears to have generated 1 warning messages.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 1 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.client.TestHCM
                  org.apache.hadoop.hbase.regionserver.TestRSKilledWhenInitializing
                  org.apache.hadoop.hbase.replication.TestReplicationSyncUpTool

     {color:red}-1 core zombie tests{color}.  There are 1 zombie test(s): 	at org.apache.hadoop.hbase.mapreduce.TestMultiTableInputFormat.testScan(TestMultiTableInputFormat.java:244)
	at org.apache.hadoop.hbase.mapreduce.TestMultiTableInputFormat.testScanYZYToEmpty(TestMultiTableInputFormat.java:195)

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/9548//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9548//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9548//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9548//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9548//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9548//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9548//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9548//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9548//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/9548//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/9548//console

This message is automatically generated.","22/May/14 00:17;stack;Linking an issue that has simple BC toolset superceded by work here.","14/May/19 03:08;HBaseQA;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 20s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} hbaseanti {color} | {color:green}  0m  0s{color} | {color:green} Patch does not have any anti-patterns. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  4m 22s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 51s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m 17s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} shadedjars {color} | {color:green}  4m 35s{color} | {color:green} branch has no errors when building our shaded downstream artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  3m  8s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 30s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:red}-1{color} | {color:red} mvninstall {color} | {color:red}  2m 16s{color} | {color:red} root in the patch failed. {color} |
| {color:red}-1{color} | {color:red} compile {color} | {color:red}  0m 15s{color} | {color:red} hbase-server in the patch failed. {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red}  0m 15s{color} | {color:red} hbase-server in the patch failed. {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  1m 13s{color} | {color:red} hbase-server: The patch generated 7 new + 0 unchanged - 0 fixed = 7 total (was 0) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:red}-1{color} | {color:red} shadedjars {color} | {color:red}  3m  8s{color} | {color:red} patch has 10 errors when building our shaded downstream artifacts. {color} |
| {color:red}-1{color} | {color:red} hadoopcheck {color} | {color:red}  1m 39s{color} | {color:red} The patch causes 10 errors with Hadoop v2.7.4. {color} |
| {color:red}-1{color} | {color:red} hadoopcheck {color} | {color:red}  3m 19s{color} | {color:red} The patch causes 10 errors with Hadoop v3.0.0. {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red}  0m 14s{color} | {color:red} hbase-server in the patch failed. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 29s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}  0m 16s{color} | {color:red} hbase-server in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m  8s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 26m 49s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce base: https://builds.apache.org/job/PreCommit-HBASE-Build/310/artifact/patchprocess/Dockerfile |
| JIRA Issue | HBASE-9806 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12617766/HBASE-9806.02.patch |
| Optional Tests |  dupname  asflicense  javac  javadoc  unit  findbugs  shadedjars  hadoopcheck  hbaseanti  checkstyle  compile  |
| uname | Linux 8d8f7260c8e9 4.4.0-143-generic #169~14.04.2-Ubuntu SMP Wed Feb 13 15:00:41 UTC 2019 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | dev-support/hbase-personality.sh |
| git revision | master / 0b8493f886 |
| maven | version: Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-17T18:33:14Z) |
| Default Java | 1.8.0_181 |
| findbugs | v3.1.11 |
| mvninstall | https://builds.apache.org/job/PreCommit-HBASE-Build/310/artifact/patchprocess/patch-mvninstall-root.txt |
| compile | https://builds.apache.org/job/PreCommit-HBASE-Build/310/artifact/patchprocess/patch-compile-hbase-server.txt |
| javac | https://builds.apache.org/job/PreCommit-HBASE-Build/310/artifact/patchprocess/patch-compile-hbase-server.txt |
| checkstyle | https://builds.apache.org/job/PreCommit-HBASE-Build/310/artifact/patchprocess/diff-checkstyle-hbase-server.txt |
| shadedjars | https://builds.apache.org/job/PreCommit-HBASE-Build/310/artifact/patchprocess/patch-shadedjars.txt |
| hadoopcheck | https://builds.apache.org/job/PreCommit-HBASE-Build/310/artifact/patchprocess/patch-javac-2.7.4.txt |
| hadoopcheck | https://builds.apache.org/job/PreCommit-HBASE-Build/310/artifact/patchprocess/patch-javac-3.0.0.txt |
| findbugs | https://builds.apache.org/job/PreCommit-HBASE-Build/310/artifact/patchprocess/patch-findbugs-hbase-server.txt |
| unit | https://builds.apache.org/job/PreCommit-HBASE-Build/310/artifact/patchprocess/patch-unit-hbase-server.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HBASE-Build/310/testReport/ |
| Max. process+thread count | 85 (vs. ulimit of 10000) |
| modules | C: hbase-server U: hbase-server |
| Console output | https://builds.apache.org/job/PreCommit-HBASE-Build/310/console |
| Powered by | Apache Yetus 0.9.0 http://yetus.apache.org |


This message was automatically generated.

",,,,,,,,,,,,,,,
create integration or unit test for scanner lease expiration,HBASE-9713,12672442,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,sershe,sershe,05/Oct/13 01:16,07/Oct/13 18:41,18/Feb/21 10:10,,0.96.0,,,,,,,,,,,,,,0,,,"See HBASE-8935.

I see similar issue again in 96 in IntegrationTestLoadAndVerify, it seems that when scanner lease expires, MR HBase reader may occasionally skip a bunch of rows.

We need some sort of test to verify it works correctly, some CP or hook may be necessary to create well-defined lease expiration.
",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-10-05 01:20:23.547,,,false,,,,,,,,,,,,,,,,,352069,,,,Mon Oct 07 18:41:19 UTC 2013,,,,,,,"0|i1op9b:",352357,,,,,,,,,,,,,,,,,"05/Oct/13 01:20;ndimiduk;Think this is related to HBASE-9582?","07/Oct/13 18:41;sershe;Maybe; the error is different though. There's no root cause there anyway, so that is why we might need the test if we cannot get the reliable repro otherwise",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Promote CM SnapshotTableAction to a command,HBASE-9674,12671043,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,santoshv98,ndimiduk,ndimiduk,27/Sep/13 23:25,27/Jul/15 21:11,18/Feb/21 10:10,,,,,,,,,,test,,,,,,0,beginner,,"SnapshotTableAction is an action, which means it runs as the user running the test. On secure clusters, snapshots can only be taken by admins, which means for this action to run, it the test user needs to be an admin. This is a bit of over-kill. Re-implement this as a command so that it can be executed as a user other than the test user.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350872,,,,2013-09-27 23:25:33.0,,,,,,,"0|i1ohwv:",351163,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SnapshotReferenceUtil#snapshot should catch RemoteWithExtrasException,HBASE-9629,12669933,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,yuzhihong@gmail.com,yuzhihong@gmail.com,22/Sep/13 20:55,18/Nov/13 22:39,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,,,"From https://builds.apache.org/job/PreCommit-HBASE-Build/7329//testReport/org.apache.hadoop.hbase.snapshot/TestFlushSnapshotFromClient/testTakeSnapshotAfterMerge/ :
{code}
org.apache.hadoop.hbase.snapshot.HBaseSnapshotException: org.apache.hadoop.hbase.snapshot.HBaseSnapshotException: Snapshot { ss=snapshotAfterMerge table=test type=FLUSH } had an error.  Procedure snapshotAfterMerge { waiting=[] done=[] }
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.translateException(RpcRetryingCaller.java:208)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.translateException(RpcRetryingCaller.java:219)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:123)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:94)
	at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable(HBaseAdmin.java:3156)
	at org.apache.hadoop.hbase.client.HBaseAdmin.snapshot(HBaseAdmin.java:2705)
	at org.apache.hadoop.hbase.client.HBaseAdmin.snapshot(HBaseAdmin.java:2638)
	at org.apache.hadoop.hbase.client.HBaseAdmin.snapshot(HBaseAdmin.java:2645)
	at org.apache.hadoop.hbase.snapshot.SnapshotTestingUtils.snapshot(SnapshotTestingUtils.java:260)
	at org.apache.hadoop.hbase.snapshot.TestFlushSnapshotFromClient.testTakeSnapshotAfterMerge(TestFlushSnapshotFromClient.java:318)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException: org.apache.hadoop.hbase.snapshot.HBaseSnapshotException: Snapshot { ss=snapshotAfterMerge table=test type=FLUSH } had an error.  Procedure snapshotAfterMerge { waiting=[] done=[] }
	at org.apache.hadoop.hbase.master.snapshot.SnapshotManager.isSnapshotDone(SnapshotManager.java:365)
	at org.apache.hadoop.hbase.master.HMaster.isSnapshotDone(HMaster.java:2878)
	at org.apache.hadoop.hbase.protobuf.generated.MasterAdminProtos$MasterAdminService$2.callBlockingMethod(MasterAdminProtos.java:32890)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:1979)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:90)
	at org.apache.hadoop.hbase.ipc.FifoRpcScheduler$1.run(FifoRpcScheduler.java:73)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException via Failed taking snapshot { ss=snapshotAfterMerge table=test type=FLUSH } due to exception:Missing parent hfile for: 9592c67505ab4cdc9d95a9437068b093.1782a7f8ce9085c1d201635936b15366:org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException: Missing parent hfile for: 9592c67505ab4cdc9d95a9437068b093.1782a7f8ce9085c1d201635936b15366
	at org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher.rethrowException(ForeignExceptionDispatcher.java:83)
	at org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.rethrowExceptionIfFailed(TakeSnapshotHandler.java:318)
	at org.apache.hadoop.hbase.master.snapshot.SnapshotManager.isSnapshotDone(SnapshotManager.java:355)
	... 11 more
Caused by: org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException: Missing parent hfile for: 9592c67505ab4cdc9d95a9437068b093.1782a7f8ce9085c1d201635936b15366
	at org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier.verifyStoreFile(MasterSnapshotVerifier.java:224)
	at org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier.access$000(MasterSnapshotVerifier.java:82)
	at org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier$1.storeFile(MasterSnapshotVerifier.java:210)
	at org.apache.hadoop.hbase.util.FSVisitor.visitRegionStoreFiles(FSVisitor.java:115)
	at org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.visitRegionStoreFiles(SnapshotReferenceUtil.java:123)
	at org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier.verifyRegion(MasterSnapshotVerifier.java:207)
	at org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier.verifyRegions(MasterSnapshotVerifier.java:175)
	at org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier.verifySnapshot(MasterSnapshotVerifier.java:120)
	at org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.process(TakeSnapshotHandler.java:189)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:131)
	... 3 more

	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1430)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1634)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1691)
	at org.apache.hadoop.hbase.protobuf.generated.MasterAdminProtos$MasterAdminService$BlockingStub.isSnapshotDone(MasterAdminProtos.java:34923)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$5.isSnapshotDone(HConnectionManager.java:2130)
	at org.apache.hadoop.hbase.client.HBaseAdmin$24.call(HBaseAdmin.java:2708)
	at org.apache.hadoop.hbase.client.HBaseAdmin$24.call(HBaseAdmin.java:2705)
	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:116)
	... 16 more
{code}
The CorruptedSnapshotException is wrapped in RemoteWithExtrasException.
SnapshotReferenceUtil#snapshot should catch RemoteWithExtrasException and check for CorruptedSnapshotException.",,,,,,,,,,,,,,"22/Sep/13 20:56;yuzhihong@gmail.com;9629.txt;https://issues.apache.org/jira/secure/attachment/12604470/9629.txt",,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2013-09-23 02:04:25.669,,,false,,,,,,,,,,,,,,,,,349767,,,,Tue Sep 24 04:52:22 UTC 2013,,,,,,,"0|i1ob5b:",350065,,,,,,,,,,,,,,,,,"23/Sep/13 02:04;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12604470/9629.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:red}-1 release audit{color}.  The applied patch generated 1 release audit warnings (more than the trunk's current 0 warnings).

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/7336//testReport/
Release audit warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7336//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7336//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7336//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7336//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7336//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7336//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7336//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7336//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7336//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7336//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/7336//console

This message is automatically generated.","23/Sep/13 09:29;mbertozzi;looks like other HBaseAdmin calls like createTable(), deleteTable(), flush(), compact(), ...
are using ProtobufUtil.getRemoteException(e) or e.unwrapRemoteException() to unwrap and rethrow the exception.
(and in 94 the snapshots calls seems to catch and unwrap the exception)","23/Sep/13 14:24;yuzhihong@gmail.com;ProtobufUtil.getRemoteException(e) is for ServiceException.

RemoteWithExtrasException is RemoteException, not ServiceException.","23/Sep/13 16:55;mbertozzi;ok, what is the rationale behind throwing the RemoteExceptions to the user instead of unwrapping them and throw the proper exception? the user will probably have to do the unwrap anyway if wants to handler a particular exception.

like in the patch attached you've a catch for a CorruptedSnapshotException and a catch for a RemoteException with an if for the CorruptedSnapshotException case... so what is different between the two? why I've two write two code path for the same exception?","24/Sep/13 04:52;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12604470/9629.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/7347//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7347//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7347//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7347//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7347//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7347//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7347//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7347//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7347//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/7347//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/7347//console

This message is automatically generated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[types] extend the breadth and depth of test coverage,HBASE-9572,12669275,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,ndimiduk,ndimiduk,18/Sep/13 16:01,18/Sep/13 16:01,18/Feb/21 10:10,,0.96.0,,,,,,,,,,,,,,0,,,"Direct test coverage over the data types API is minimal. Existing tests cover the underlying encoding mechanisms and the {{Struct}} components, but that's about it. We should expand on the test coverage, something like what Orderly accomplishes with its api-driven harness for generating random values and verifying them.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349207,,,,2013-09-18 16:01:26.0,,,,,,,"0|i1o7ov:",349505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LocalHBaseCluster.shutdown hangs for regionserver thread wait zk deleteMyEphemeralNode packet. ,HBASE-9526,12668410,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,,liushaohui,liushaohui,13/Sep/13 02:55,16/Sep/13 06:06,18/Feb/21 10:10,,0.94.3,,,,,,,,,,,,,,0,,,"When LocalHBaseCluster is shutdowned, it will join all regionserver threads.
Regionserver thread will try to delete EphemeralNode and wait zk packet, but the sendThread has been not existed, so no one notify the regionserver thread. 

{noformat}

""RegionServer:0;10.237.14.236,43311,1378958529812"" prio=10 tid=0x00007f0a9d020000 nid=0x18db in Object.wait() [0x00007f0a8aa35000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1309)
        - locked <0x000000008ece3de8> (a org.apache.zookeeper.ClientCnxn$Packet)
        at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:866)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.delete(RecoverableZooKeeper.java:127)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1038)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1027)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.deleteMyEphemeralNode(HRegionServer.java:1073)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:851)
        at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.runRegionServer(MiniHBaseCluster.java:147)
        at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.access$000(MiniHBaseCluster.java:100)
        at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer$1.run(MiniHBaseCluster.java:131)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:337)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1340)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.util.Methods.call(Methods.java:37)
        at org.apache.hadoop.hbase.security.User.call(User.java:603)
        at org.apache.hadoop.hbase.security.User.access$700(User.java:50)
        at org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:443)
        at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.run(MiniHBaseCluster.java:129)

{noformat}

This situation emerges randomly. If i rerun the test, the tests may pass.",,,,,,,,,,,,,,"13/Sep/13 02:57;liushaohui;stack.log;https://issues.apache.org/jira/secure/attachment/12602949/stack.log",,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2013-09-13 17:38:54.915,,,false,,,,,,,,,,,,,,,,,348344,,,,Mon Sep 16 06:06:37 UTC 2013,,,,,,,"0|i1o2db:",348641,,,,,,,,,,,,,,,,,"13/Sep/13 02:57;liushaohui;The full stack log","13/Sep/13 17:38;larsh;Interesting. Thanks [~liushaohui].
Do you have an idea for a patch?","16/Sep/13 06:06;liushaohui;[~lhofhansl] Sorry, I have no idea about how it happens. 
It's hard to reproduce it in our ci env and i am trying to find some ideas from the code. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBaseTestingUtility#cleanupDataTestDirOnTestFS() doesn't close the FileSystem,HBASE-9355,12665758,Test,Patch Available,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Minor,,rekhajoshm,yuzhihong@gmail.com,yuzhihong@gmail.com,27/Aug/13 18:14,05/Mar/14 06:23,18/Feb/21 10:10,,0.92.2,,,,,,,,,,,,,,0,,,"Here is related code:
{code}
  public boolean cleanupDataTestDirOnTestFS() throws IOException {
    boolean ret = getTestFileSystem().delete(dataTestDirOnTestFS, true);
    if (ret)
      dataTestDirOnTestFS = null;
    return ret;
  }
{code}
The FileSystem returned by getTestFileSystem() is not closed.",,,,,,,,,,,,,,"04/Mar/14 12:22;rekhajoshm;HBASE-9355.1.patch;https://issues.apache.org/jira/secure/attachment/12632494/HBASE-9355.1.patch",,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2014-03-04 12:21:25.077,,,false,,,,,,,,,,,,,,,,,345697,,,,Wed Mar 05 04:55:11 UTC 2014,,,,,,,"0|i1nm3j:",345998,,,,,,,,,,,,,,,,,"04/Mar/14 12:21;rekhajoshm;Attached patch.
Think the configuration parameter - fs.automatic.close set to true seems better, rather than the boilerplate.What do you think?","04/Mar/14 12:22;rekhajoshm;Attached patch.
Think the configuration parameter - fs.automatic.close set to true seems better, rather than the boilerplate.What do you think?","04/Mar/14 14:07;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12632494/HBASE-9355.1.patch
  against trunk revision .
  ATTACHMENT ID: 12632494

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop1.1{color}.  The patch compiles against the hadoop 1.1 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:red}-1 findbugs{color}.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.zookeeper.lock.TestZKInterProcessReadWriteLock

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/8880//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8880//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8880//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8880//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8880//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8880//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8880//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8880//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8880//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8880//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/8880//console

This message is automatically generated.","05/Mar/14 04:20;zjushch;Is there any reason to close the file system？
From the method name, it just need clean up data.","05/Mar/14 04:27;yuzhihong@gmail.com;When getTestFileSystem().delete() returns true, dataTestDirOnTestFS is set to null.
Test file system should be closed in the above case.","05/Mar/14 04:55;zjushch;lgtm
+1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Introduce master failover into a new AccessController unit test,HBASE-8492,12646100,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,yuzhihong@gmail.com,yuzhihong@gmail.com,04/May/13 23:26,27/Nov/13 23:26,18/Feb/21 10:10,,,,,,,,,,security,,,,,,0,,,"Currently TestAccessController starts a cluster with one master.

This JIRA introduces two masters in the cluster where the active master is stopped. Expectation is that standby master should take over and perform permission verification.",,,,,,,,,,,,,,"05/May/13 00:30;yuzhihong@gmail.com;8492-0.94-v1.txt;https://issues.apache.org/jira/secure/attachment/12581815/8492-0.94-v1.txt",,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2013-05-05 00:00:19.188,,,false,,,,,,,,,,,,,,,,,326458,,,,Wed Nov 27 23:26:44 UTC 2013,,,,,,,"0|i1kbjr:",326803,,,,,,,,,,,,,,,,,"04/May/13 23:28;yuzhihong@gmail.com;Patch for 0.94

TestAccessController#testBulkLoad fails","05/May/13 00:00;apurtell;TestAccessController is already a fat test. Do this new test case as a new unit test? Maybe we should break out the cases related to that Endpoint also?","05/May/13 00:16;yuzhihong@gmail.com;We have two options:
1. parameterize TestAccessController so that there is one and two masters, respectively
2. introduce master failover into a new test

Looks like #2 is better.
","06/May/13 21:31;yuzhihong@gmail.com;Looking at TestAccessController in trunk, permission granting is done in setUp() method. This is different from the setup in 0.94 branch.
I wonder if permission granting can be lifted to setupBeforeClass(). This way, stopping the active master and letting standby master take over would happen only once instead of for every test.","06/May/13 21:36;mbertozzi;{quote}I wonder if permission granting can be lifted to setupBeforeClass().{quote}
if you fix the tests that change the permissions you can move back the granting to setupBeforeClass().
Take a look at HBASE-8122, some tests revoke rights from the ones setup in the beginning","06/May/13 21:52;yuzhihong@gmail.com;In testGrantRevoke(), READ permission was taken away from USER_RO for TEST_TABLE :
{code}
          ProtobufUtil.revoke(protocol, USER_RO.getShortName(), TEST_TABLE,
            TEST_FAMILY, null, Action.READ);
{code}
But the verification seems to be at the global level:
{code}
    verifyAllowed(revokeAction, SUPERUSER, USER_ADMIN, USER_OWNER);
    verifyDenied(revokeAction, USER_CREATE, USER_RW, USER_RO, USER_NONE);
{code}
Should verification on the TEST_TABLE be added ?","06/May/13 22:19;apurtell;We should be hesitant to change a unit test that has a lot of cases and has had flaky test fixes in the past. Why start mucking around and destabilize it again?

bq. This way, stopping the active master and letting standby master take over would happen only once instead of for every test.

-1, this is why I suggest a separate unit test. Leave TestAccessController alone.","06/May/13 22:21;apurtell;Changed the JIRA description. Feel free to change back if there is disagreement.","12/May/13 16:54;yuzhihong@gmail.com;@Andy:
I agree with creating new test for the master failover scenario.","27/Nov/13 23:26;apurtell;No viable patch as of yet, unscheduling.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HBaseTestingUtility.startMiniDFSClusterForTestHLog should handle ""Address already in use"" and retry automatically",HBASE-8154,12637964,Test,Patch Available,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,yuzhihong@gmail.com,xieliang007,xieliang007,20/Mar/13 12:15,29/Jan/14 09:27,18/Feb/21 10:10,,0.98.0,,,,,,,,test,,,,,,0,,,"see https://builds.apache.org/job/PreCommit-HBASE-Build/4906//testReport/org.apache.hadoop.hbase.regionserver.wal/TestHLog/testAppendClose/
and https://issues.apache.org/jira/browse/HBASE-7845?focusedCommentId=13607468&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13607468
",,,,,,,,,,,,,,"10/May/13 16:56;yuzhihong@gmail.com;8154-v2.txt;https://issues.apache.org/jira/secure/attachment/12582638/8154-v2.txt","10/May/13 17:37;yuzhihong@gmail.com;8154-v3.txt;https://issues.apache.org/jira/secure/attachment/12582645/8154-v3.txt","10/May/13 22:44;yuzhihong@gmail.com;8154-v4.txt;https://issues.apache.org/jira/secure/attachment/12582720/8154-v4.txt","29/Jan/14 07:20;stack;HBASE-8154-v5.txt;https://issues.apache.org/jira/secure/attachment/12625809/HBASE-8154-v5.txt","28/Jan/14 02:29;xieliang007;HBASE-8154-v5.txt;https://issues.apache.org/jira/secure/attachment/12625504/HBASE-8154-v5.txt",,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2013-03-20 14:55:53.484,,,false,,,,,,,,,,,,,,,,,318442,,,,Wed Jan 29 09:27:51 UTC 2014,,,,,,,"0|i1iy33:",318783,,,,,,,,,,,,,,,,,"20/Mar/13 14:55;yuzhihong@gmail.com;How about this patch ?","20/Mar/13 15:00;jmspaggi;[~tedyu@apache.org]

Few things.

Will the break not let you try only once? It should be moved in the try after, no?

Also, are all those new imports required?","20/Mar/13 15:05;yuzhihong@gmail.com;Jean-Marc:
You're right about the location of break statement.","20/Mar/13 15:08;jmspaggi;Thanks. And what's about those imports?
{code}
+import org.apache.hadoop.hbase.HBaseTestingUtility;
+import org.apache.hadoop.hbase.HColumnDescriptor;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HTableDescriptor;
+import org.apache.hadoop.hbase.KeyValue;
+import org.apache.hadoop.hbase.LargeTests;
+import org.apache.hadoop.hbase.ServerName;
{code}
Are they really required?","20/Mar/13 16:06;yuzhihong@gmail.com;Take a look at the first line below:
{code}
-import org.apache.hadoop.hbase.*;
+import org.apache.hadoop.hbase.Coprocessor;
+import org.apache.hadoop.hbase.HBaseTestingUtility;
{code}
It is good practice to limit the imports.","20/Mar/13 16:17;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12574555/8154.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/4910//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4910//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4910//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4910//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4910//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4910//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4910//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4910//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/4910//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/4910//console

This message is automatically generated.","20/Mar/13 18:41;jmspaggi;bq. Take a look at the first line below:
bq.-import org.apache.hadoop.hbase.*;
bq.+import org.apache.hadoop.hbase.Coprocessor;
bq.+import org.apache.hadoop.hbase.HBaseTestingUtility;
bq.It is good practice to limit the imports.

Got it! Thanks for the clarification.
","10/May/13 16:29;jmspaggi;One last comment here.

Whould we limit the number of retries? Let's say the address is used but another process which is stuck, or anything like that. Should we try foreever? Or shouldwe fail after a certain number of tries?","10/May/13 16:56;yuzhihong@gmail.com;Patch v2 limited the number of retries.","10/May/13 17:06;jmspaggi;I like the approach, but should we not throw an expection after the 30 tries if it's still failing?","10/May/13 17:37;yuzhihong@gmail.com;Patch v3 throws the last exception caught if MiniDFS cluster cannot be started within retry limits.","10/May/13 17:40;jmspaggi;Cool! I like this version! 

Thanks Ted. I'm +1.","10/May/13 18:14;jeffreyz;A minor comment since it's a public utility method, it'd be better that we can put the retry logic inside the function like ""startMiniDFSClusterForTestHLog(int namenodePort, int retries)"" so new client code won't do the same when calling the function.

Other than that +1 from me! Thanks.","10/May/13 18:30;yuzhihong@gmail.com;Good idea, Jeff.

Here is patch v4.","10/May/13 18:47;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12582645/8154-v3.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/5623//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5623//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5623//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5623//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5623//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5623//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5623//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5623//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5623//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/5623//console

This message is automatically generated.","10/May/13 19:03;jeffreyz;+1 on the v4 patch. Thanks!","10/May/13 20:01;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12582660/8154-v4.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 6 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
     

     {color:red}-1 core zombie tests{color}.  There are 1 zombie test(s): 

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/5624//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5624//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5624//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5624//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5624//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5624//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5624//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5624//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5624//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/5624//console

This message is automatically generated.","10/May/13 22:15;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12582688/8154-v4.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 6 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
                       org.apache.hadoop.hbase.regionserver.wal.TestHLog

     {color:red}-1 core zombie tests{color}.  There are 1 zombie test(s): 

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/5627//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5627//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5627//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5627//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5627//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5627//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5627//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5627//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5627//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/5627//console

This message is automatically generated.","11/May/13 00:06;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12582720/8154-v4.txt
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 6 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop2.0{color}.  The patch compiles against the hadoop 2.0 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/5630//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5630//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5630//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5630//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5630//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop1-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5630//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5630//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5630//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/5630//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/5630//console

This message is automatically generated.","27/Jan/14 18:04;apurtell;This has been through review and has a +1, shouldn't it be committed?","27/Jan/14 18:15;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12582720/8154-v4.txt
  against trunk revision .
  ATTACHMENT ID: 12582720

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 6 new or modified tests.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/8527//console

This message is automatically generated.","28/Jan/14 02:31;xieliang007;Rebased patch v5, let's see QA result again.","28/Jan/14 04:06;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12625504/HBASE-8154-v5.txt
  against trunk revision .
  ATTACHMENT ID: 12625504

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop1.1{color}.  The patch compiles against the hadoop 1.1 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

     {color:red}-1 core tests{color}.  The patch failed these unit tests:
     

     {color:red}-1 core zombie tests{color}.  There are 1 zombie test(s): 	at org.apache.hadoop.hbase.regionserver.wal.TestLogRolling.testLogRollOnDatanodeDeath(TestLogRolling.java:355)

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/8532//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8532//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8532//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8532//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8532//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8532//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8532//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8532//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8532//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8532//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/8532//console

This message is automatically generated.","29/Jan/14 07:20;stack;Retry. See if zombie is related.","29/Jan/14 09:27;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12625809/HBASE-8154-v5.txt
  against trunk revision .
  ATTACHMENT ID: 12625809

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 3 new or modified tests.

    {color:green}+1 hadoop1.0{color}.  The patch compiles against the hadoop 1.0 profile.

    {color:green}+1 hadoop1.1{color}.  The patch compiles against the hadoop 1.1 profile.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 lineLengths{color}.  The patch does not introduce lines longer than 100

  {color:green}+1 site{color}.  The mvn site goal succeeds with this patch.

    {color:green}+1 core tests{color}.  The patch passed unit tests in .

Test results: https://builds.apache.org/job/PreCommit-HBASE-Build/8549//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8549//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop2-compat.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8549//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-prefix-tree.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8549//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-client.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8549//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-common.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8549//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-protocol.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8549//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-server.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8549//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-examples.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8549//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-thrift.html
Findbugs warnings: https://builds.apache.org/job/PreCommit-HBASE-Build/8549//artifact/trunk/patchprocess/newPatchFindbugsWarningshbase-hadoop-compat.html
Console output: https://builds.apache.org/job/PreCommit-HBASE-Build/8549//console

This message is automatically generated.",,,,,,,,,,,,,,,,
Update config parameter name for enabling append support in tests,HBASE-7943,12634289,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,yuzhihong@gmail.com,yuzhihong@gmail.com,26/Feb/13 21:20,26/Feb/13 22:21,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,,,"When I worked on HBASE-7878, I found that we're using the following code to enable append in our tests:
{code}
    TEST_UTIL.getConfiguration().setBoolean(""dfs.support.append"", true);
{code}
However, in hadoop 1.0, here is the code from FSNamesystem.java for checking append support:
{code}
    this.allowBrokenAppend = conf.getBoolean(""dfs.support.broken.append"", false);
...
  LocatedBlock appendFile(String src, String holder, String clientMachine
      ) throws IOException {
    if (!allowBrokenAppend) {
      throw new IOException(""Append is not supported. "" +
          ""Please see the dfs.support.append configuration parameter"");
    }
{code}
I think we should pass true for ""dfs.support.broken.append"" so that the append functionality can be verified for hadoop 1.0",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,314782,,,,Tue Feb 26 22:21:52 UTC 2013,,,,,,,"0|i1ibiv:",315126,,,,,,,,,,,,,,,,,"26/Feb/13 22:21;yuzhihong@gmail.com;Devaraj pointed out the correct config parameter during investigation.

Thanks to him.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restored snapshot replay problem,HBASE-7346,12623773,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Critical,,,jmhsieh,jmhsieh,13/Dec/12 16:11,21/Feb/13 04:33,18/Feb/21 10:10,,hbase-6055,,,,hbase-6055,,,,Client,master,regionserver,snapshots,Zookeeper,,0,,,"The situation is a coarse-grained problem. The key problem is that writes that shouldn't be replayed (since they don't belong to the restored image), would not normally get replayed, but would potentially get replayed if recovery was triggered.
Previously, without restore, we could depend on the timestamps – if something was replayed but there was newer data, the newer data would win. In a restore situation, the ""newer"" data is has the old time stamps from before recovery, and new data that shouldn't get replayed could be.
ex: 
1) write 100 rows
2) ss1 (with logs)
3) write 50 rows
4) restore ss1
5) crash
6) writes from 1 and 3 both get replayed in log splitting recovery. Oops.

",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-12-13 19:16:49.809,,,false,,,,,,,,,,,,,,,,,297473,,,,Thu Dec 13 19:56:19 UTC 2012,,,,,,,"0|i14oxr:",235505,,,,,,,,,,,,,,,,,"13/Dec/12 16:15;jmhsieh;It was suggested by [~mbertozzi] that we probably can adding a restore timestamp somewhere to a restored snapshot and have log splitting not restore data before that timestamp in that case.  Since we require a disable before restore, this may be able to avoid restore ragged edges due to having many rs's with different timestamps.","13/Dec/12 19:16;jesse_yates;When we do restore a table, it needds be disabled first (no inflight restores!), which flushes everything to disk and rolls the WAL. This seems as simple as just removing the existing recovered.edits directory under a table before we restore the table.

As an aside, offline snapshots don't actually reference any of the WALs and just copy the recovered.edits directory over, just incase something is in there. This should only apply to the online snapshot cases.","13/Dec/12 19:56;jmhsieh;This crash I'm talking about is after the restore has successfully completed (after step 4).  

Does the disable table flushing + log rolling guarantee that logs are moved out of the way so that they won't be replayed?  

Let's double check and add a test case for this.  That is the easiest way to convince that this is or isn't a problem on the offline or online cases.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure that compactions use already cached blocks but do not cache new data blocks,HBASE-5375,12542069,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,mikhail,mikhail,10/Feb/12 04:20,08/Aug/14 18:50,18/Feb/21 10:10,,,,,,,,,,BlockCache,Performance,,,,,0,,,"Create a unit test to verify that compactions reuse existing cached blocks but do not thrash the cache with newly read blocks. Also need to verify that we only read every data block once, e.g. that we don't re-read the block on every next() operation. HBASE-1597 did not seem to include a unit test, so we need to add a test now. This and HBASE-4683 (the unit test that was not checked in) are the remaining missing pieces before we can close HBASE-3976.",,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,227356,,,,2012-02-10 04:20:57.0,,,,,,,"0|i0ht9j:",102003,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MapReduce testing changes for Hadoop 0.23,HBASE-4813,12531847,Test,Open,HBASE,HBase,software,stack,"Apache HBase is an open-source, distributed, versioned, non-relational
database. Apache HBase gives you low latency random access to billions of
rows with millions of columns atop non-specialized hardware.",http://hbase.apache.org,Major,,,apurtell,apurtell,17/Nov/11 21:27,02/May/13 02:30,18/Feb/21 10:10,,,,,,,,,,,,,,,,0,,,"MiniMRCluster went away in Hadoop 0.23.

MAPREDUCE-3169 seeks to provide a MiniMRCluster equivalent, which we may be able to use without significant changes to HBase test code.

It might be good to take this opportunity to rewrite tests as appropriate to mock MR using MRUnit or mockito.",,,,,,,,,,HBASE-4924,HBASE-4966,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,217583,,,,2011-11-17 21:27:56.0,,,,,,,"0|i0hsa7:",101844,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
