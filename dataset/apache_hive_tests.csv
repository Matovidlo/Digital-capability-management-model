Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Description,Environment,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Child-Issue),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Tags),Custom field (Tags),Custom field (Target Version/s),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Create description annotations for vectorized UDF,HIVE-4937,12659956,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Blocker,,vjk1688,appodictic,appodictic,26/Jul/13 03:04,25/Apr/16 07:53,18/Feb/21 09:57,,,,,,vectorization-branch,,,,0,,"Vectorized UDFs should technically be close to the same as normal UDFs, but that is not guaranteed. For example a standard UDF might have multiple overloads that the vectorized version does not.

When users run things like 'describe function' they may not be getting the correct information depending if they are in vectorized mode or not.

[~ehans] I assigned this to you feel free to unassign it, but I think we need some internal documentation for vectorized UDFs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,340148,,,Fri Jul 26 03:16:01 UTC 2013,,,,,,,"0|i1mo0n:",340466,,,,,,,,,,,,,,,,,,"26/Jul/13 03:16;appodictic;Marked this as a blocker because we should not do a release until this is sorted out, but this does not need to be handler right away.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.thrift.transport.TTransportException when insert data in hive3.1.2,HIVE-24576,13348661,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,,appleyuchi,appleyuchi,01/Jan/21 11:55,01/Jan/21 11:56,18/Feb/21 09:57,,3.1.2,,,,,,,,0,,"my error log is:

https://paste.ubuntu.com/p/y4N7RTtBz5/

when i try to insert some data in to internal table.

my hive-site.xml is

<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>
<configuration>
        <property>
                <name>javax.jdo.option.ConnectionURL</name>
                <value>jdbc:mysql://Desktop:3306/hive?useUnicode=true&amp;characterEncoding=utf8</value>（mysql地址localhost）
        </property>


        <property>
                <name>javax.jdo.option.ConnectionDriverName</name>（mysql的驱动）
                <value>com.mysql.cj.jdbc.Driver</value>
        </property>

        <property>
                <name>javax.jdo.option.ConnectionUserName</name>（用户名）
                <value>appleyuchi</value>
        </property>

        <property>
                <name>javax.jdo.option.ConnectionPassword</name>（密码）
                <value>appleyuchi</value>
        </property>
        <property>
                <name>datanucleus.schema.autoCreateAll</name>
                <value>true</value>
        </property>

        <property>
            <name>hive.metastore.uris</name>
            <value>thrift://Desktop:9083</value>
                <description>Thrift uri for the remote metastore. Used by metastore client to connect to remote metastore.</description>
        </property>

        <property>
                <name>hive.metastore.schema.verification</name>
                <value>false</value>
        </property>


        <property>
                <name>hive.map.aggr.hash.percentmemoryn</name>
                <value>0.25</value>
        </property>

        


<!-- 这是hiveserver2 -->
<property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
</property>
 
<property>
        <name>hive.server2.thrift.bind.host</name>
        <value>Desktop</value>
</property>



<property>  
        <name>javax.jdo.option.ConnectionUserName</name>  
        <value>appleyuchi</value>  
        <description>ername to use against metastoredatabase</description>  
</property>  

<property>  
        <name>javax.jdo.option.ConnectionPassword</name>  
        <value>appleyuchi</value>  
        <description>password to use against metastoredatabase</description>  
</property>


<property>
    <name>hive.metastore.event.db.notification.api.auth</name>
    <value>false</value>
  </property>



   <property>
    <name>hive.server2.active.passive.ha.enable</name>
    <value>true</value>
  </property>

<property>
    <name>hive.support.concurrency</name>
    <value>true</value>
</property>


<property>
    <name>hive.txn.manager</name>
    <value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
</property>





<!-- 下面是给tez设置的 -->
<!-- <property>
    <name>hive.execution.engine</name>
    <value>tez</value>
</property>
 -->



<!-- 下面是给hive支持事务操作而设置的 -->

<property>
<name>hive.enforce.bucketing</name>
<value>true</value>
</property>
<property>
<name>hive.exec.dynamic.partition.mode</name>
<value>nonstrict</value>
</property>
<!-- <property>
<name>hive.txn.manager</name>
<value>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</value>
</property> -->
<property>
<name>hive.compactor.initiator.on</name>
<value>true</value>
</property>
<property>
<name>hive.compactor.worker.threads</name>
<value>1</value>
</property>

<!-- <property>
<name>hive.in.test</name>
<value>true</value>
</property>  -->

<property>
<name>hive.auto.convert.join.noconditionaltask.size</name>
<value>10000000</value>
</property>


<property>
<name>hive.server2.logging.operation.enabled</name>
<value>false</value>
</property>

</configuration>

I have googled relevant information,but no luck

need support,
thanks for your help~


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-01-01 11:55:22.0,,,,,,,"0|z0lyso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TestAcidOnTez#testNonStandardConversion01,HIVE-19550,13159295,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,,jcamachorodriguez,jcamachorodriguez,15/May/18 03:05,18/Dec/20 18:49,18/Feb/21 09:57,,3.1.0,,,,,Test,,,0,,,,,,,,,,,,,,,,,,,,HIVE-19509,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-15 03:05:01.0,,,,,,,"0|i3tphz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to start Hive - java.lang.RuntimeException: java.lang.NoClassDefFoundError: org/apache/hadoop/crypto/key/KeyProvider,HIVE-23778,13314109,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,,sathyatsr2002,sathyatsr2002,29/Jun/20 17:22,29/Jun/20 17:22,18/Feb/21 09:57,,2.3.7,,,,,CLI,,,0,,"My Hadoop version is Hadoop 2.3.0-cdh5.0.0 and my Hive 2.3.7, while executive Hive, i'm getting the following error message. I could sense that there is some JAR file missing in the classpath. But could not get the relevant classpath.

 

Logging initialized using configuration in jar:file:/usr/local/hive/apache-hive-2.3.7-bin/lib/hive-common-2.3.7.jar!/hive-log4j2.properties Async: true

Exception in thread ""main"" java.lang.RuntimeException: java.lang.NoClassDefFoundError: org/apache/hadoop/crypto/key/KeyProvider

 at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:91)

 at org.apache.hadoop.hive.ql.exec.Utilities.createDirsWithPermission(Utilities.java:3323)

 at org.apache.hadoop.hive.ql.session.SessionState.createRootHDFSDir(SessionState.java:709)

 at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:654)

 at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:586)

 at org.apache.hadoop.hive.ql.session.SessionState.beginStart(SessionState.java:553)

 at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:750)

 at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)

 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)

 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

 at java.lang.reflect.Method.invoke(Method.java:498)

 at org.apache.hadoop.util.RunJar.main(RunJar.java:212)

Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/crypto/key/KeyProvider

 at java.lang.Class.forName0(Native Method)

 at java.lang.Class.forName(Class.java:264)

 at org.apache.hadoop.hive.shims.ShimLoader.createShim(ShimLoader.java:129)

 at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:124)

 at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:88)

 ... 12 more

Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.crypto.key.KeyProvider

 at java.net.URLClassLoader.findClass(URLClassLoader.java:382)

 at java.lang.ClassLoader.loadClass(ClassLoader.java:418)

 at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)

 at java.lang.ClassLoader.loadClass(ClassLoader.java:351)

 ... 17 more","Hadoop 2.3.0-cdh5.0.0
Hive 2.3.7
VM - Oracle Linux 7

Bash profile 
-------------
export JAVA_HOME=/usr/local/java/jdk1.8.0_251/
export PATH=$PATH:$JAVA_HOME/bin 
export HADOOP_HOME=/usr/local/hadoop/hadoop-2.3.0-cdh5.0.0
export PATH=$PATH:$HADOOP_HOME/bin

export HADOOP_HOME=/usr/local/hadoop/hadoop-2.3.0-cdh5.0.0
export HADOOP_MAPRED_HOME=$HADOOP_HOME 
export HADOOP_COMMON_HOME=$HADOOP_HOME 

export HADOOP_HDFS_HOME=$HADOOP_HOME 
export YARN_HOME=$HADOOP_HOME 
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native 
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 
export HADOOP_INSTALL=$HADOOP_HOME 

export HIVE_HOME=/usr/local/hive/apache-hive-2.3.7-bin
export PATH=$PATH:$HIVE_HOME/bin
export CLASSPATH=$CLASSPATH:/usr/local/hive/apache-hive-2.3.7-bin/lib/
export HADOOP_CLASSPATH=/usr/local/hive/apache-hive-2.3.7-bin/lib/hive-common-2.3.7.jar
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/usr/local/hive/apache-hive-2.3.7-bin/lib/*.jar
export HIVE_CLASSPATH=/usr/local/hive/apache-hive-2.3.7-bin/lib/:/usr/local/hadoop/hadoop-2.3.0-cdh5.0.0/share/hadoop/mapreduce1/lib/*.jar

export DERBY_HOME=/usr/local/derby/db-derby-10.4.2.0-bin

export PATH=$PATH:$DERBY_HOME/bin

export CLASSPATH=$CLASSPATH:$DERBY_HOME/lib/derby.jar:$DERBY_HOME/lib/derbytools.jar
[root@hadhiv bin]# 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-29 17:22:44.0,,,,,,,"0|z0gb1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Enable TestStats#partitionedTableDeprecatedCalls, TestStats#partitionedTableInHiveCatalog, and TestStats#partitionedTableOtherCatalog",HIVE-19546,13159290,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,jfs,jcamachorodriguez,jcamachorodriguez,15/May/18 03:01,16/Jun/20 15:21,18/Feb/21 09:57,,3.1.0,,,,,Test,,,0,,,,,,,,,,,,,,,,,,,,HIVE-19509,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-15 03:01:58.0,,,,,,,"0|i3tpgv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TestSSL#testSSLFetchHttp,HIVE-19548,13159293,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,,jcamachorodriguez,jcamachorodriguez,15/May/18 03:03,29/May/20 11:19,18/Feb/21 09:57,,3.1.0,,,,,Test,,,0,,,,,,,,,,,,,,,,,,,,HIVE-19509,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-05-29 11:19:57.47,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 11:19:57 UTC 2020,,,,,,,"0|i3tphj:",9223372036854775807,,,,,,,,,,,,,,,,,,"29/May/20 11:19;dlavati;I ran into this in a much older version as well, my findings might help someone fixing this:

Error was:
{code:java}
 testSSLFetchHttp(org.apache.hive.jdbc.TestSSL)  Time elapsed: 12.568 sec  <<< ERROR!
org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException [Error 10072]: Database does not exist: default
	at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:279)
	at org.apache.hive.jdbc.Utils.verifySuccessWithInfo(Utils.java:265)
	at org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:304)
	at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:245)
	at org.apache.hive.jdbc.TestSSL.setupTestTableWithData(TestSSL.java:492)
	at org.apache.hive.jdbc.TestSSL.testSSLFetchHttp(TestSSL.java:369)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:367)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:274)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException [Error 10072]: Database does not exist: default{code}
 

This happens during the create table command in the data setup:
{code:java}
Statement stmt = hs2Conn.createStatement();
stmt.execute(""set hive.support.concurrency = false"");

stmt.execute(""drop table if exists "" + tableName);
stmt.execute(""create table "" + tableName
    + "" (under_col int comment 'the under column', value string)""); {code}
This is probably some kind of concurrency/timing issue.

If I debug this and check {{show databases}} before the create command, I do get back the default value. Also I had a passing test when run alone.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TestDanglingQOuts#checkDanglingQOut,HIVE-19554,13159299,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,kgyrtkirk,jcamachorodriguez,jcamachorodriguez,15/May/18 03:08,26/Jun/18 08:25,18/Feb/21 09:57,,3.1.0,,,,,Test,,,0,,,,,,,,,,,,,,,,,,,,HIVE-19509,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-15 03:08:36.0,,,,,,,"0|i3tpiv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TestBeeLineWithArgs#testQueryProgress and TestBeeLineWithArgs#testQueryProgressParallel,HIVE-19551,13159296,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,,jcamachorodriguez,jcamachorodriguez,15/May/18 03:05,15/May/18 04:28,18/Feb/21 09:57,,3.1.0,,,,,Test,,,0,,,,,,,,,,,,,,,,,,,,HIVE-19509,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-15 03:05:45.0,,,,,,,"0|i3tpi7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TestOldSchema#testPartitionOps,HIVE-19547,13159292,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,,jcamachorodriguez,jcamachorodriguez,15/May/18 03:02,15/May/18 03:03,18/Feb/21 09:57,,3.1.0,,,,,Test,,,0,,,,,,,,,,,,,,,,,,,,HIVE-19509,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-15 03:02:42.0,,,,,,,"0|i3tphb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TestMiniLlapLocalCliDriver#schema_evol_orc_acidvec_part.q and TestMiniLlapLocalCliDriver#schema_evol_orc_vec_part_llap_io.q,HIVE-19544,13159287,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,,jcamachorodriguez,jcamachorodriguez,15/May/18 02:58,15/May/18 02:59,18/Feb/21 09:57,,3.1.0,,,,,Test,,,0,,,,,,,,,,,,,,,,,,,,HIVE-19509,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-15 02:58:37.0,,,,,,,"0|i3tpg7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TestMiniLlapLocalCliDriver#tez_smb_1.q,HIVE-19542,13159285,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,,jcamachorodriguez,jcamachorodriguez,15/May/18 02:56,15/May/18 02:56,18/Feb/21 09:57,,3.1.0,,,,,Test,,,0,,,,,,,,,,,,,,,,,,,,HIVE-19509,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-15 02:56:31.0,,,,,,,"0|i3tpfr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TestMiniLlapLocalCliDriver#bucket_map_join_tez1.q,HIVE-19539,13159281,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,,jcamachorodriguez,jcamachorodriguez,15/May/18 02:52,15/May/18 02:53,18/Feb/21 09:57,,3.1.0,,,,,Test,,,0,,,,,,,,,,,,,,,,,,,,HIVE-19509,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-15 02:52:55.0,,,,,,,"0|i3tpev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TestNegativeCliDriver#mm_concatenate.q,HIVE-19538,13159280,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,,jcamachorodriguez,jcamachorodriguez,15/May/18 02:51,15/May/18 02:52,18/Feb/21 09:57,,3.1.0,,,,,Test,,,0,,,,,,,,,,,,,,,,,,,,HIVE-19509,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-15 02:51:39.0,,,,,,,"0|i3tpen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TestNegativeCliDriver#merge_negative_5.q,HIVE-19537,13159279,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,,jcamachorodriguez,jcamachorodriguez,15/May/18 02:49,15/May/18 02:50,18/Feb/21 09:57,,3.1.0,,,,,Test,,,0,,,,,,,,,,,,,,,,,,,,HIVE-19509,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-15 02:49:53.0,,,,,,,"0|i3tpef:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TestSequenceFileReadWrite#testSequenceTableWriteReadMR and TestSequenceFileReadWrite#testTextTableWriteReadMR,HIVE-19536,13159278,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,,jcamachorodriguez,jcamachorodriguez,15/May/18 02:47,15/May/18 02:48,18/Feb/21 09:57,,3.1.0,,,,,Test,,,0,,,,,,,,,,,,,,,,,,,,HIVE-19509,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-15 02:47:17.0,,,,,,,"0|i3tpe7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add testcases for concurrent query execution,HIVE-80,12409298,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Critical,,,rsm,rsm,26/Nov/08 19:22,06/Dec/14 09:59,18/Feb/21 09:57,,,,,,,Query Processor,Server Infrastructure,,6,concurrency,Can use one driver object per query.,,,,,,,,,,,,,,,,,,HIVE-2935,,,,,"08/May/09 19:20;neilconway;hive_input_format_race-2.patch;https://issues.apache.org/jira/secure/attachment/12407649/hive_input_format_race-2.patch",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2009-03-13 01:12:26.093,,,false,,,,,,,,,,,,,,,,,,42903,,,Sun Apr 08 03:11:16 UTC 2012,,,,,,,"0|i0l7pz:",121901,,,,,,,,,,,,,,,,,,"13/Mar/09 01:12;athusoo;Downgrading to critical as HiveServer has a bunch of other fixes and is at best experimental right now.","20/Apr/09 21:40;neilconway;A reasonable way to implement this might be as follows:

* Change the HiveServer#execute() method to return a unique ID for each active query (this can just be QueryPlan#getQueryId()).
* Change the rest of the HiveServer methods to be parameterized by the query ID
* Inside HiveServer, create a separate Driver object for each active query
* Perhaps add a HiveServer#close() method that clients can use when they're finished executing a query

One issue is that if a client dies spontaneously, it might not call close(), which would leak resources at the server.

Comments? If you're not working on this right now Raghu, I'd be happy to take a crack at it.","20/Apr/09 22:07;rsm;Neil, please go ahead and take this over. Your plan sounds good.

A few comments:
- This change would mean breaking compatibility of the hive service. It might be better to just add additional methods that use the queryId and deprecate the old methods.
- Inside the HiveServerHandler, I am assuming, you would just use a map of queryid to driver (instead of a single driver object)
- If a client dies, thrift will time out and release the handler. So, that should not cause problems.","20/Apr/09 22:25;neilconway;Raghu, thanks for the feedback.

Maintaining backward compatibility for the hive service is important, then? Okay, I can just add new methods rather than changing the existing ones.

WRT the map of queryId => driver, that's correct.","21/Apr/09 00:12;neilconway;BTW, is there any documentation on how to regenerate the Thrift-generated source files? I made a trivial change to service/if/hive_service.thrift, and then reran

$ cd service
$ ant thriftif

However, the resulting Thrift-generated code fails to compile. The first few of the many compile errors are:

    [javac] Compiling 4 source files to /Users/neilconway/hive-trunk/build/service/classes
    [javac] /Users/neilconway/hive-trunk/service/src/gen-javabean/org/apache/hadoop/hive/service/ThriftHive.java:46: cannot find symbol
    [javac] symbol  : constructor Client(org.apache.thrift.protocol.TProtocol,org.apache.thrift.protocol.TProtocol)
    [javac] location: class org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.Client
    [javac]       super(iprot, oprot);
    [javac]       ^
    [javac] /Users/neilconway/hive-trunk/service/src/gen-javabean/org/apache/hadoop/hive/service/ThriftHive.java:57: writeMessageBegin(com.facebook.thrift.protocol.TMessage) in com.facebook.thrift.protocol.TProtocol cannot be applied to (org.apache.thrift.protocol.TMessage)
    [javac]       oprot_.writeMessageBegin(new TMessage(""compile"", TMessageType.CALL, seqid_));
    [javac]             ^
    [javac] /Users/neilconway/hive-trunk/service/src/gen-javabean/org/apache/hadoop/hive/service/ThriftHive.java:60: write(org.apache.thrift.protocol.TProtocol) in org.apache.hadoop.hive.service.ThriftHive.compile_args cannot be applied to (com.facebook.thrift.protocol.TProtocol)
    [javac]       args.write(oprot_);
    [javac]           ^

So I'm guessing I need to be using an old version of Thrift? Any info on which version to use or which procedure to follow would be very helpful.","21/Apr/09 01:07;rsm;We have not really recorded the thrift version used to generate the files. Can you try one of the instant releases at: http://instant.thrift-rpc.org ? I was able to run ant thriftif with r760184 of thrift at http://gitweb.thrift-rpc.org/?p=thrift.git;spfx=thrift-instant-r760184;a=snapshot;h=b1139424416009c980a9634c44f2806f469f8c1c;sf=tgz.","21/Apr/09 04:49;neilconway;Raghu: Well, running ""ant thriftif"" works with the trunk release of thrift, as well -- but the generated code doesn't compile; same results for r760184 and r758922. I'll do a binary search tomorrow to find a version of Thrift that works with hive, unless anyone knows of one offhand ...?","21/Apr/09 09:42;rsm;Actually, right after I commented on this jira, I realized that the problem is with the incompatible change between thrift in apache vs thrift we had used originally. The following changes have to be made in hive to get it working with any thrift in apache.

- change libthrift.jar
- change com.facebook.thrift to org.apache.thrift
- handle some incompatible changes 
- fix some of the warnings

I'll create a new jira for this.","23/Apr/09 10:45;rsm;Neil, you can get the patch that Zheng posted to HIVE-438 and copy over the libthrift.jar and libfb303.jar (also attached). That should get you moving with this jira. ","27/Apr/09 23:48;neilconway;Raghu, thanks for your work on 438. In the short term, we're planning to implement the MQO prototype using multiple Drivers embedded directly in a client program. That means there's no short-term dependency on getting this ticket resolved. I should have the cycles to look at this in ~2 weeks -- but if someone else would like to do it first, by all means go ahead.","05/May/09 23:34;neilconway;BTW, one issue we've run into when running multiple queries by using multiple Drivers in a client program is that HiveInputFormat seems to be dependent on a static ""JobConf"" variable, so there's a race condition when running multiple queries concurrently.","08/May/09 19:01;athusoo;hmm...

We do generate a new JobConf in ExecDriver. I think the static can be dropped in HiveInputFormat. I don't think that is needed at all. Have you done that to get around this issue?
","08/May/09 19:20;neilconway;I quickly hacked this patch together to see if it fixed the problem by removing the static JobConf variable. It seemed to fix the races, but it looked like the execution of multiple queries was still serialized. I haven't had a chance to look into it further...","12/May/09 17:38;namit;The patch looks OK - I don't know why job was static to start with.","31/Aug/09 03:29;cresnick;I'm attaching a patch to org.apache.hadoop.hive.ql.exec.Utilities. Currently this class has a static field instance of type mapredWork. Changing the reference to ThreadLocal has eliminated the race conditions we found while executing several concurrent queries through a simple HiveConnection pool. 
","09/Sep/09 14:05;mpest06;Patch Attached.","17/Sep/09 20:36;cresnick;This fixes a broken patch previously submitted","18/Sep/09 16:39;namit;The unit tests are failing with this patch","18/Sep/09 22:27;cresnick;The ThreadLocal fix won't work with the unit tests, which is run an embedded mapreduce on a separate thread. In a real-world scenario perhaps it's a worthwhile hack - it does work for us -  but there are certainly better options. 

After 0.4 release, if HiveConnection is still not threadsafe I will delve more into this. In the meantime I'm removing the patch.","14/May/10 21:53;aprabhakar;I wanted to fix this JIRA and so started looking at it. From what I have observed it appears that the {{HiveServer}} *is* multi-thread capable. Specifically:

* The {{HiveServer}} is using a {{TThreadPoolServer}} which is multi-threaded.
* The {{ThriftHiveProcessorFactory}} overrides the {{getProcessor()}} call and returns a new instance of {{HiveServerHandler}} on every invokation.
* Every instance of {{HiveServerHandler}} has its own thread local session state and a private driver instance.
* Query execution is thread safe thanks to HIVE-77.

Give the above, I believe that this JIRA should be marked closed and resolved. If you think I missed something in my analysis, can you please point that out?","26/May/10 22:39;jvs;From internal discussions at Facebook, the best I can gather is that there may still be some thread-unsafe code, but no one knows for sure.  Given that, the only approach may be to do as much review as possible (e.g. grep for statics that shouldn't be there), ask everyone to add any known issues here, and then set up a testbed and see what turns up.
","26/May/10 23:53;nzhang;I think after HIVE-549 (parallel execution) was committed, Driver allows more than one tasks running in parallel. So this JIRA may be fixed as a by-product. But we never tried it yet. It may make sense to try it on trunk or release 0.5. ","27/May/10 15:01;athusoo;yes I think what Ning is saying is correct. We should however add a test case to the unit tests to check that. I am not sure that we added a test case for the parallel execution stuff.","27/May/10 15:24;nzhang;Yes we should add more test cases for parallel execution. There is an open issue HIVE-1019 for parallel execution. The HIVE_PLAN* file name need to be unique rather than relying on timestamp. ","27/May/10 16:39;aprabhakar;This sounds like a good plan. If Neil is not actively working on this issue, I can move this to my queue and start working on it. ","27/May/10 17:26;neilconway;Arvind, I'm not actively working on it, so please go ahead.","18/Nov/10 09:53;zjffdu;Does anyone know whether Driver is thread-safe ? If so, each query can been executed by one Driver.
And I notice that in HWI for each session there is one Driver, it seems Driver should be thread-safe, just want to ensure about it.

","18/Nov/10 10:28;svenkat;> Does anyone know whether Driver is thread-safe?
Driver is not thread safe.","27/Jan/11 15:38;fdiaconeasa;Hello,

I'm thinking about writing a Hive client in order to handle some of the queries that i wish to run and the dependencies between them.

From what i read on the Hive wiki ( http://wiki.apache.org/hadoop/Hive/HiveServer ) it appears that the hive server is single threaded.

I might be mistaking, but this is what i understand from that text: if i launch 2 requests from my client towards the hive server, the hive server will not handle them in parallel and the hadoop requests done by hive itself will run one after another, not in parallel.

Is this right? If (hopefully) not :) , could someone please clarify that for me, if possible?

Thank you.","29/Jun/11 18:49;franklovecchio;We have tested multiple client connections submitting jobs rapidly to a single hiveserver (running on a Brisk implementation); I would not recommend doing this unless you have a que of some sort on your end.  Otherwise, you will see this:

ERROR in runJob
org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused

Has anyone used multiple hiveservers for managing multiple connections?  (something like Amazon's cloud map/reduce, where they spin up a temporary instance?)  

Thanks","09/Jul/11 21:15;franklovecchio;Edit: I put up some test results for different scenarios, including multiple connections threaded, here:  https://github.com/franklovecchio/hiveserver-loadtest .  Multi-threaded jobs = wonky.","09/Jul/11 21:18;alexvk@cloudera.com;I will be out of office July 9-22, 2011.  If you have anything urgent,
please contact my manager Omer Trajman omer@cloudera.com.

-- 
--
Alex Kozlov
Solutions Architect
Cloudera, Inc

Hadoop World 2011 in New York
City<http://www.cloudera.com/company/events/hadoop-world-2011/>
<http://www.cloudera.com/company/press-center/hadoop-world-nyc/>
","15/Nov/11 02:25;cos;Perhaps my question is bigger than this issue or shall be asked somewhere else, but looking at Hive code I couldn't help noticing that it is essentially build as a singleton i.e. only a single instance of Hive object is allowed to exist. 

What was/is a design motivation behind of this? Why Hive can't be instantiated at will by the client so different instances can be independently for query analysis, job submissions, etc.? This will make Hive much more flexible and extendable from Hive client applications perspective, won't it?","08/Apr/12 03:11;cwsteinbach;HiveServer can't support concurrent connections due to a limitation of the current HiveServer Thrift API. There's a proposal for a new HiveServer2 Thrift API which fixes these problems located here:

https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Thrift+API
"
Enable flaky cbo_rp_limit.q in TestMiniLlapLocalCliDriver,HIVE-21658,13230419,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,jcamachorodriguez,jcamachorodriguez,27/Apr/19 01:23,12/Feb/21 14:57,18/Feb/21 09:57,,,,,,,Hive,,,0,,"Fails intermittently with diff:

{code}
Client Execution succeeded but contained differences (error code = 1) after executing cbo_rp_limit.q 
11c11
<  1  4 2
---
>  1 4 2
{code}",,,,,,,,,,,,,,,,,,HIVE-21657,HIVE-20959,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-04-27 01:23:03.0,,,,,,,"0|z026w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Qtest to simulate query-based compaction,HIVE-24459,13343578,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,klcopp,klcopp,klcopp,01/Dec/20 14:31,01/Dec/20 14:31,18/Feb/21 09:57,,,,,,,,,,0,,"AFAIK all compaction tests run on a local filesystem, and none run on HDFS. Since HDFS and local filesystem behavior differs sometimes, it would be good to test query-based compaction on HDFS.

Compaction threads don't run in the qtest environment so this qtest would simulate QB compaction by running the queries that QB compaction runs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-12-01 14:31:41.0,,,,,,,"0|z0l3g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"GBK encoded data is imported into hive, and the last column is garbled during query",HIVE-23812,13315520,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,bitao,bitao,08/Jul/20 01:47,08/Jul/20 01:47,18/Feb/21 09:57,,,,,,,,,,0,,"I created a hive table and specified the encoding format.as follows:
CREATE TABLE person(id INT, name STRING, {color:#910091}level{color} STRING)ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES(""serialization.encoding""='GBK');
But when importing data, the last column of the query result is garbled，as attachment,Can anyone help me to solve this problem?thank you.
",,,,,,,,,,,,,,,,,,,,,,,"08/Jul/20 01:46;bitao;WechatIMG1.jpeg;https://issues.apache.org/jira/secure/attachment/13007246/WechatIMG1.jpeg",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-07-08 01:47:33.0,,,,,,,"0|z0gjls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tests for 'external.table.purge' and 'auto.purge',HIVE-23705,13311791,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,hsnusonic,hsnusonic,hsnusonic,16/Jun/20 18:11,16/Jun/20 18:11,18/Feb/21 09:57,,,,,,,Standalone Metastore,,,0,,The current unit tests did not include an external table with setting 'external.table.purge' or 'auto.purge'. It should be added into TestTableCreateDropAlterTruncate.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-16 18:11:41.0,,,,,,,"0|z0fws0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dummy jira for modifying ptest configuration - before the actual patch,HIVE-23203,13298368,Test,Patch Available,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,mgergely,mgergely,14/Apr/20 18:32,18/Apr/20 16:12,18/Feb/21 09:57,,,,,,,Hive,,,0,,"If the ptest configuration is required to be modified, then the server must be restarted, which causes the actually processed patch to fail. In order not to cause trouble to others, this dummy Jira can be used by queueing this first, then the actual modification, finally the pair of this Jira.

When the patch for this one is is processed, the ptest configuration can be modified, and the server can be restarted, thus this dummy jira's patch processing will fail. Then the actual modification comes, which will result in some way or the other. Then the other dummy Jira ([HIVE-23204|https://issues.apache.org/jira/browse/HIVE-23204]) comes, and the ptest modifications can be reverted, and the server can be restarted again.",,,,,,,,,,,,,,,,,,,,,,,"18/Apr/20 09:12;mgergely;HIVE-23203.patch;https://issues.apache.org/jira/secure/attachment/13000365/HIVE-23203.patch",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2020-04-18 16:05:20.826,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 16:12:57 UTC 2020,,,,,,,"0|z0dmqo:",9223372036854775807,,,,,,,,,,,,,,,,,,"18/Apr/20 16:05;hiveqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 10m 34s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  7m 24s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  7m 36s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  8m 26s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  7m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  7m 35s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:red}-1{color} | {color:red} xml {color} | {color:red}  0m  1s{color} | {color:red} The patch has 1 ill-formed XML file(s). {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  7m 30s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 14s{color} | {color:red} The patch generated 2 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 49m 46s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  xml  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2+deb8u5 (2017-09-19) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-21752/dev-support/hive-personality.sh |
| git revision | master / 15ebf9e |
| Default Java | 1.8.0_111 |
| xml | http://104.198.109.242/logs//PreCommit-HIVE-Build-21752/yetus/xml.txt |
| asflicense | http://104.198.109.242/logs//PreCommit-HIVE-Build-21752/yetus/patch-asflicense-problems.txt |
| modules | C: . U: . |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-21752/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","18/Apr/20 16:12;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/13000365/HIVE-23203.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 17131 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.ql.schq.TestScheduledQueryStatements.testExecuteImmediate (batchId=294)
org.apache.hive.jdbc.TestJdbcWithMiniLlapRow.testComplexQuery (batchId=214)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/21752/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/21752/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-21752/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 13000365 - PreCommit-HIVE-Build",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dummy jira for modifying ptest configuration - after the actual patch,HIVE-23204,13298369,Test,Patch Available,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,mgergely,mgergely,14/Apr/20 18:32,15/Apr/20 13:28,18/Feb/21 09:57,,,,,,,Hive,,,0,,"If the ptest configuration is required to be modified, then the server must be restarted, which causes the actually processed patch to fail. In order not to cause trouble to others, the pair of this dummy Jira can be used by queueing this first, then the actual modification, finally this Jira.

When the patch for the other one ([https://issues.apache.org/jira/browse/HIVE-23203]) is is processed, the ptest configuration can be modified, and the server can be restarted, thus the other dummy jira's patch processing will fail. Then the actual modification comes, which will result in some way or the other. Then this dummy Jira comes, and the ptest modifications can be reverted, and the server can be restarted again.",,,,,,,,,,,,,,,,,,HIVE-23203,,,,,"14/Apr/20 19:01;mgergely;HIVE-23204.patch;https://issues.apache.org/jira/secure/attachment/12999946/HIVE-23204.patch",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2020-04-15 13:06:47.929,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 13:28:00 UTC 2020,,,,,,,"0|z0dmqw:",9223372036854775807,,,,,,,,,,,,,,,,,,"15/Apr/20 13:06;hiveqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  1s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  9m 35s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  6m 34s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  6m 45s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 39s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  6m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  6m 32s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:red}-1{color} | {color:red} xml {color} | {color:red}  0m  1s{color} | {color:red} The patch has 1 ill-formed XML file(s). {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  6m 45s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 14s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 44m 26s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  xml  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.43-2+deb8u5 (2017-09-19) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-21673/dev-support/hive-personality.sh |
| git revision | master / 8e3ce09 |
| Default Java | 1.8.0_111 |
| xml | http://104.198.109.242/logs//PreCommit-HIVE-Build-21673/yetus/xml.txt |
| modules | C: . U: . |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-21673/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","15/Apr/20 13:28;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12999946/HIVE-23204.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 17128 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[materialized_view_create_rewrite_4] (batchId=95)
org.apache.hadoop.hive.metastore.TestMetastoreHousekeepingLeaderEmptyConfig.testHouseKeepingThreadExistence (batchId=173)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/21673/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/21673/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-21673/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12999946 - PreCommit-HIVE-Build",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
typeCast issue while constant propagation when cbo is disabled,HIVE-22999,13290496,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,dkuzmenko,dkuzmenko,09/Mar/20 10:04,09/Mar/20 10:05,18/Feb/21 09:57,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,HIVE-22945,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-03-09 10:04:35.0,,,,,,,"0|z0cblc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
materialized_view_partitioned_3.q relies on hive.optimize.sort.dynamic.partition property,HIVE-22921,13286876,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,vgarg,jcamachorodriguez,jcamachorodriguez,21/Feb/20 19:17,21/Feb/20 19:17,18/Feb/21 09:57,,,,,,,Materialized views,,,0,,{{hive.optimize.sort.dynamic.partition}} was deprecated in favor of {{hive.optimize.sort.dynamic.partition.threshold}} in HIVE-20703. {{materialized_view_partitioned_3.q}} specifically tests SortedDynPartitionOptimizer for MVs. We need to update the q test.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-02-21 19:17:18.0,,,,,,,"0|z0brkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hcat command to import jars in auxlib ,HIVE-22732,13279640,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,amaharek,amaharek,15/Jan/20 17:52,16/Jan/20 08:24,18/Feb/21 09:57,,,,,,,HCatalog,,,0,,"Importing the HIVE_AUX_JARS_PATH/*jar doesn't work.

By using the command:

> sqoop import --connect jdbc:mysql://xxxxxxxx/database_name --table <table_name> --username <user_name> --hive-import --create-hive-table --hive-table <hive_table_name> -P

which needs a jar in the auxlib isn't working. However, from hive CLI it works.",,,,,,,,,,,,,,,,,,,,,,,"15/Jan/20 17:55;amaharek;hcat;https://issues.apache.org/jira/secure/attachment/12991020/hcat",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-01-15 17:52:35.0,,,,,,,"0|z0ajls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Umbrella : branch-2 failing tests,HIVE-17436,13099491,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,vihangk1,vihangk1,03/Sep/17 23:24,09/Dec/19 14:15,18/Feb/21 09:57,,,,,,,,,,0,,JIRA to track branch-2 failing tests which can be cited before committing to branch-2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2019-12-09 14:15:49.919,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 09 14:15:49 UTC 2019,,,,,,,"0|i3jlhz:",9223372036854775807,,,,,,,,,,,,,,,,,,"09/Dec/19 14:15;abstractdog;in 2019-12 I got these:
https://issues.apache.org/jira/browse/HIVE-22579?focusedCommentId=16991486&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16991486
{code}
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_queries] (batchId=227)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[avro_tableproperty_optimize] (batchId=22)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[explaindenpendencydiffengs] (batchId=38)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[table_nonprintable] (batchId=140)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[join_acid_non_acid] (batchId=158)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_fast_stats] (batchId=153)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vectorized_parquet_types] (batchId=155)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[merge_negative_5] (batchId=88)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=100)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[explaindenpendencydiffengs] (batchId=115)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorization_input_format_excludes] (batchId=117)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[vectorized_ptf] (batchId=125)
org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationReads.testReadTableFailure (batchId=217)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tests for dynamic semijoins reduction,HIVE-22581,13272312,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,jcamachorodriguez,odraese,odraese,04/Dec/19 20:29,04/Dec/19 20:31,18/Feb/21 09:57,,,,,,,Query Planning,,,0,,"There don't seem to be tests for the TezCompiler. A test suite should be added and a test for the error scenario of HIVE-22572 should be implemented there.

 

This is a follow-up action for:

https://issues.apache.org/jira/browse/HIVE-22572",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-12-04 20:29:25.0,,,,,,,"0|z09aps:",9223372036854775807,,,,,,,,,,,,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IO exception in Hive Spark Remote Client sub-module,HIVE-22564,13271334,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,AK2019,AK2019,29/Nov/19 08:11,29/Nov/19 08:11,18/Feb/21 09:57,,3.1.2,,,,,,,,0,,"I have been trying to build the Apache Hive on rhel_7.6/ppc64le. The build passes, however it leads to a test failure giving the following error:

[ERROR] testServerPort(org.apache.hive.spark.client.rpc.TestRpc) Time elapsed: 0.021 s <<< ERROR!

java.io.IOException: Incorrect RPC server port configuration for HiveServer2

I am on branch release-3.1.2-rc0

{color:#172b4d}Would like some help on understanding the cause for the same . I am running it on a High end VM with good connectivity.{color}","os: rhel:7.6 
 architecture: ppc64le",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-11-29 08:11:00.0,,,,,,,"0|z094ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive Llap Tez test fails giving a null pointer exception,HIVE-22543,13270687,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,AK2019,AK2019,26/Nov/19 10:47,26/Nov/19 10:47,18/Feb/21 09:57,,3.1.2,,,,,,,,0,,"{color:#172b4d}I have been trying to build the Test suite of Apache Hive on rhel_7.6/ppc64le, however it leads to test failure in ""hive-llap-tez"" sub-module giving the following error:{color}

{color:#172b4d}[ERROR]   TestLlapTaskCommunicator.testFinishableStateUpdateFailure:142 ? NullPointer{color}

I{color:#172b4d} am on branch {color}rel/release-3.1.2

{color:#172b4d}Would like some help on understanding the cause for the same . I am running it on a High end VM with good connectivity.{color}

 ","{color:#172b4d}os: rhel:7.6 {color}
{color:#172b4d} architecture: ppc64le{color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-11-26 10:47:14.0,,,,,,,"0|z090p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky dummy,HIVE-15078,13015584,Test,Patch Available,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,kgyrtkirk,kgyrtkirk,kgyrtkirk,26/Oct/16 21:05,22/Nov/19 20:08,18/Feb/21 09:57,,,,,,,,,,0,,"I think it would be intresting to see what will happen if all currently known flaky test would be ignored...
",,,,,,,,,,,,,,,,,,,,,,,"29/May/18 18:36;kgyrtkirk;HIVE-15078.01-branch-3.patch;https://issues.apache.org/jira/secure/attachment/12925615/HIVE-15078.01-branch-3.patch","28/Oct/16 20:33;kgyrtkirk;HIVE-15078.1.patch;https://issues.apache.org/jira/secure/attachment/12835882/HIVE-15078.1.patch","26/Oct/16 23:02;kgyrtkirk;HIVE-15078.1.patch;https://issues.apache.org/jira/secure/attachment/12835427/HIVE-15078.1.patch","10/Dec/18 16:08;kgyrtkirk;some.file.patch;https://issues.apache.org/jira/secure/attachment/12951221/some.file.patch",,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2016-10-27 00:53:45.653,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 22 20:08:42 UTC 2019,,,,,,,"0|i35g4n:",9223372036854775807,,,,,,,,,,,,,,,,,,"27/Oct/16 00:53;sseth;The next set of flaky tests will surface :) Honestly, I think some of these actually point to a product bug - potential races. It could ofcourse also be bugs in the tests / test framework.
","27/Oct/16 01:02;sseth;[~kgyrtkirk] - as a data point. When I changed the ptest configuration to remove some isolated tests, and convert them over to just skipBatching, as well as some batchSize changes - the number of failures changed. Nothing should have changed with this config change, except for test runs being several minutes faster.
I'm going to try that change again one of these days.","27/Oct/16 01:59;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12835427/HIVE-15078.1.patch

{color:green}SUCCESS:{color} +1 due to 6 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 10530 tests executed
*Failed tests:*
{noformat}
TestSemanticAnalysis - did not produce a TEST-*.xml file (likely timed out) (batchId=171)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_bucket_pruning] (batchId=29)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/1833/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/1833/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-1833/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12835427 - PreCommit-HIVE-Build","28/Oct/16 20:45;kgyrtkirk;i've forgot to re-attach the patch...

very intresting...i suspect that there are a few problematic cases in there which when get into the same batch will fail - but these unstable tests may even help bugs to play hide and seek ;)

I will start some standalone mvn test executions...which will probably take a few days - but there outputs might be intresting...
until then...i will reschedule this a few times...because this doesn't contain any real change; just some disabled tests - this may detect some fluctuating tests","28/Oct/16 21:45;sseth;For CliDriver tests - Tez/Spark sessions are re-used across tests. I believe the Hive Client session gets resset each time - and this is supposed to make sure tests start with a clean base, and whatever settings are in the qfile get applied. I wonder if this is not working as it should.
Also, I read a comment somewhere about tests modifying and writing the configuration back to disk. That would cause all kinds of problems when running in a batch.

Thanks for looking into this.","01/Jun/18 11:30;hiveqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m  7s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  4m 27s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  5m  6s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  5m 28s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  4m 27s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  4m 27s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:red}-1{color} | {color:red} xml {color} | {color:red}  0m  1s{color} | {color:red} The patch has 1 ill-formed XML file(s). {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  5m  0s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 10s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 31m  0s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  xml  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-11407/dev-support/hive-personality.sh |
| git revision | master / 28779d2 |
| Default Java | 1.8.0_111 |
| xml | http://104.198.109.242/logs//PreCommit-HIVE-Build-11407/yetus/xml.txt |
| modules | C: . U: . |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-11407/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","01/Jun/18 12:04;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12925615/HIVE-15078.01-branch-3.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 14371 tests executed
*Failed tests:*
{noformat}
TestUpgradeTool - did not produce a TEST-*.xml file (likely timed out) (batchId=309)
org.apache.hadoop.hive.cli.TestMiniDruidCliDriver.testCliDriver[druidkafkamini_basic] (batchId=253)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[insertsel_fail] (batchId=95)
org.apache.hive.jdbc.TestJdbcWithMiniLlapArrow.testDataTypes (batchId=240)
org.apache.hive.spark.client.rpc.TestRpc.testServerPort (batchId=304)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/11407/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/11407/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-11407/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12925615 - PreCommit-HIVE-Build","10/Dec/18 18:39;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12951221/some.file.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/15245/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/15245/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-15245/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2018-12-10 18:38:30.034
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-15245/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2018-12-10 18:38:30.037
+ cd apache-github-source-source
+ git fetch origin
From https://github.com/apache/hive
   b42fdc2..9493dcf  master     -> origin/master
+ git reset --hard HEAD
HEAD is now at b42fdc2 HIVE-21018: Grouping/distinct on more than 64 columns should be possible (Zoltan Haindrich reviewed by Jesus Camacho Rodriguez)
+ git clean -f -d
Removing standalone-metastore/metastore-server/src/gen/
+ git checkout master
Already on 'master'
Your branch is behind 'origin/master' by 1 commit, and can be fast-forwarded.
  (use ""git pull"" to update your local branch)
+ git reset --hard origin/master
HEAD is now at 9493dcf HIVE-21007: Semi join + Union can lead to wrong plans (Vineet Garg, reviewed by Jesus Camacho Rodriguez)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2018-12-10 18:38:31.369
+ rm -rf ../yetus_PreCommit-HIVE-Build-15245
+ mkdir ../yetus_PreCommit-HIVE-Build-15245
+ git gc
+ cp -R . ../yetus_PreCommit-HIVE-Build-15245
+ mkdir /data/hiveptest/logs/PreCommit-HIVE-Build-15245/yetus
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
fatal: unrecognized input
fatal: unrecognized input
fatal: unrecognized input
The patch does not appear to apply with p0, p1, or p2
+ result=1
+ '[' 1 -ne 0 ']'
+ rm -rf yetus_PreCommit-HIVE-Build-15245
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12951221 - PreCommit-HIVE-Build","22/Nov/19 20:08;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12951221/some.file.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/19553/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/19553/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-19553/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2019-11-22 20:07:56.179
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-19553/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2019-11-22 20:07:56.183
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at d6ae486 HIVE-22514: HiveProtoLoggingHook might consume lots of memory (Attila Magyar via Slim Bouguerra)
+ git clean -f -d
Removing standalone-metastore/metastore-server/src/gen/
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at d6ae486 HIVE-22514: HiveProtoLoggingHook might consume lots of memory (Attila Magyar via Slim Bouguerra)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2019-11-22 20:07:57.417
+ rm -rf ../yetus_PreCommit-HIVE-Build-19553
+ mkdir ../yetus_PreCommit-HIVE-Build-19553
+ git gc
+ cp -R . ../yetus_PreCommit-HIVE-Build-19553
+ mkdir /data/hiveptest/logs/PreCommit-HIVE-Build-19553/yetus
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
fatal: unrecognized input
fatal: unrecognized input
fatal: unrecognized input
The patch does not appear to apply with p0, p1, or p2
+ result=1
+ '[' 1 -ne 0 ']'
+ rm -rf yetus_PreCommit-HIVE-Build-19553
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12951221 - PreCommit-HIVE-Build",,,,,,,,,,,,,,,,,,,,,,,,,
Clean up intermittently failing unit tests,HIVE-17325,13094863,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,gates,gates,gates,15/Aug/17 21:02,01/Oct/19 22:07,18/Feb/21 09:57,,,,,,,Tests,,,0,,We have a number of intermittently failing tests.  I propose to disable these so that we can get clean (or at least cleaner) CI runs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-16 10:44:42.585,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 16 22:12:39 UTC 2017,,,,,,,"0|i3iton:",9223372036854775807,,,,,,,,,,,,,,,,,,"15/Aug/17 21:27;gates;In the last 10 CI runs, the following tests have failed:
* TestBeeLineDriver.testCliDriver.insert_overwrite_local_directory_1 6 times
* TestCliDriver.testCliDriver.union36 3 times
* TestMiniLlapCliDriver.testCliDriver.orc_ppd_basic 3 times
* TestMiniLlapLocalCliDriver.testCliDriver.vector_if_expr 3 times
* TestPerfCliDriver.testCliDriver.query14 7 times
* TestPerfCliDriver.testCliDriver.query16 3 times
* TestPerfCliDriver.testCliDriver.query23 5 times
* TestPerfCliDriver.testCliDriver.query94 3 times
* TestBlobstoreCliDriver.testCliDriver.insert_overwrite_dynamic_partitions_merge_move 6 times
* TestBlobstoreCliDriver.testCliDriver.insert_overwrite_dynamic_partitions_merge_only 6 times
* TestBlobstoreCliDriver.testCliDriver.insert_overwrite_dynamic_partitions_move_only 6 times
* TestMiniSparkOnYarnCliDriver.testCliDriver.spark_dynamic_partition_pruning_mapjoin_only 6 times
* TestMiniSparkOnYarnCliDriver.testCliDriver.spark_vectorized_dynamic_partition_pruning 7 times
* TestHCatClient.testPartitionRegistrationWithCustomSchema 7 times
* TestHCatClient.testPartitionSpecRegistrationWithCustomSchema 7 times
* TestHCatClient.testTableSchemaPropagation 7 times

All of these should be disabled until the reason for their flakiness can be determined.","16/Aug/17 10:44;pvary;Thanks [~gates] to bring this up!
I really hate these continuously failing tests so I agree that we should solve them. I have information about some of them:
- TestBeeLineDriver.testCliDriver.insert_overwrite_local_directory_1 - For the BeeLine tests [~zsombor.klara] has a patch which most probably solves the problem: HIVE-17322
- TestMiniSparkOnYarnCliDriver.testCliDriver.spark_dynamic_partition_pruning_mapjoin_only - I have a patch (HIVE-17292) where I will be update the golden files. It was missing from HIVE-16948.
- TestMiniSparkOnYarnCliDriver.testCliDriver.spark_dynamic_partition_pruning - I have committed HIVE-17305 yesterday which should solve this issue

I know about these, and not sure about how to handle them. Why we are not reverting the patch which caused the failures?
- TestMiniSparkOnYarnCliDriver.testCliDriver.spark_vectorized_dynamic_partition_pruning - Most probably cause by HIVE-16273
- TestHCatClient - Caused by HIVE-16844

Thanks,
Peter","16/Aug/17 11:40;zsombor.klara;Just a small clarification HIVE-17305  is fixing 3 tests:
-TestBlobstoreCliDriver.testCliDriver.insert_overwrite_dynamic_partitions_merge_move 
-TestBlobstoreCliDriver.testCliDriver.insert_overwrite_dynamic_partitions_merge_only
-TestBlobstoreCliDriver.testCliDriver.insert_overwrite_dynamic_partitions_move_only
These 3 are not failing anymore, sorry that it took so long to fix.
Unfortunately it will *not* fix TestMiniSparkOnYarnCliDriver.testCliDriver.spark_dynamic_partition_pruning.","16/Aug/17 11:59;pvary;You are right [~zsombor.klara], made a copy-paste mistake :)","16/Aug/17 22:12;gates;Thanks for tracking down all these things.

I'm all for fixing these issues and reverting those we can't fix over just turning off the tests.  Let's use this as an umbrella JIRA to track those others and get them in, then we'll turn off whatever tests we can't fix or revert.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix branch-3 metastore test timeouts,HIVE-21180,13212636,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,vihangk1,vihangk1,29/Jan/19 19:05,29/Mar/19 18:01,18/Feb/21 09:57,,3.2.0,,,,,,,,0,,"The module name below is wrong since metastore-server doesn't exist on branch-3. This is most likely the reason why test batches are timing out on branch-3

{noformat}
2019-01-29 00:32:17,765  INFO [HostExecutor 3] HostExecutor.executeTestBatch:262 Drone [user=hiveptest, host=104.198.216.224, instance=0] executing UnitTestBatch [name=228_UTBatch_standalone-metastore__metastore-server_20_tests, id=228, moduleName=standalone-metastore/metastore-server, batchSize=20, isParallel=true, testList=[TestPartitionManagement, TestCatalogNonDefaultClient, TestCatalogOldClient, TestHiveAlterHandler, TestTxnHandlerNegative, TestTxnUtils, TestFilterHooks, TestRawStoreProxy, TestLockRequestBuilder, TestHiveMetastoreCli, TestCheckConstraint, TestAddPartitions, TestListPartitions, TestFunctions, TestGetTableMeta, TestTablesCreateDropAlterTruncate, TestRuntimeStats, TestDropPartitions, TestTablesList, TestUniqueConstraint]] with bash /home/hiveptest/104.198.216.224-hiveptest-0/scratch/hiveptest-228_UTBatch_standalone-metastore__metastore-server_20_tests.sh
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 29 18:01:40 UTC 2019,,,,,,,"0|yi0ga8:",9223372036854775807,,,,,,,,,,,,,3.2.0,,,,,"31/Jan/19 18:30;vihangk1;It looks like the test batches are created based of the master branch. Many of these supposedly timed-out tests don't even exist on branch-3","29/Mar/19 18:01;vihangk1;Don't think I will have enough time to work on this. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Re-enable TestCliDriver#vector_groupby_reduce,HIVE-21535,13224664,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,vihangk1,vihangk1,28/Mar/19 18:08,28/Mar/19 18:08,18/Feb/21 09:57,,,,,,,Tests,,,0,,The test was disabled since it was flaky in HIVE-21396. Creating this JIRA to re-enable the test by fixing the rounding logic.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-03-28 18:08:23.0,,,,,,,"0|z017v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test : TestActivePassiveHA,HIVE-21534,13224659,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,vihangk1,vihangk1,28/Mar/19 17:51,28/Mar/19 17:51,18/Feb/21 09:57,,,,,,,,,,0,,"Failed in https://issues.apache.org/jira/browse/HIVE-21484?focusedCommentId=16798031&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16798031

Works locally as well in the subsequent run of precommit later in the patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-03-28 17:51:54.0,,,,,,,"0|z017u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix TestJdbcWithDBTokenStoreNoDoAs flakiness,HIVE-20845,13195407,Test,Patch Available,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,pvary,pvary,pvary,31/Oct/18 15:46,19/Nov/18 20:05,18/Feb/21 09:57,,,,,,,,,,0,,"Previously did a dirty fix for TestJdbcWithDBTokenStoreNoDoAs and TestJdbcWithDBTokenStore
Found out the issue is that we do not wait enough for HS2 to come up.
Need to fix in MiniHS2.waitForStartup()",,,,,,,,,,,,,,,,,,,,,,,"09/Nov/18 05:55;pvary;HIVE-20845.2.patch;https://issues.apache.org/jira/secure/attachment/12947526/HIVE-20845.2.patch","19/Nov/18 09:10;pvary;HIVE-20845.3.patch;https://issues.apache.org/jira/secure/attachment/12948675/HIVE-20845.3.patch","19/Nov/18 16:04;pvary;HIVE-20845.4.patch;https://issues.apache.org/jira/secure/attachment/12948739/HIVE-20845.4.patch","07/Nov/18 23:58;pvary;HIVE-20845.patch;https://issues.apache.org/jira/secure/attachment/12947304/HIVE-20845.patch",,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,2018-11-09 01:42:43.478,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 19 20:05:37 UTC 2018,,,,,,,"0|i3zumn:",9223372036854775807,,,,,,,,,,,,,,,,,,"08/Nov/18 00:06;pvary;The patch contains 2 changes:
* Fix for {{TestJdbcWithDBTokenStore}} - Several more configuration changes done by MiniHS2 init (USE_THRIFT_SASL, KERBEROS_PRINCIPAL, KERBEROS_KEYTAB_FILE) should be set in System properties, so the DriverManager can use them. This fixes the long outstanding issue, that 2 HMS instances is used here
* The MiniHS2 {{waitForStartup}} should wait not only for the possibility to open the session, but for the possibility to open the transport as well. This could be done by using {{DriverManager.getConnection}} instead of {{hs2Client.openSession}}. The {{DriverManager.getConnection}} will open a HiveConnection which calls {{openTransport}} and {{openSession}} as well.

Running tests to see what will happen with the HA passive tests :)","08/Nov/18 07:43;pvary;[~szita]: Could you please take a look?
Thanks,
Peter","09/Nov/18 01:42;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12947304/HIVE-20845.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/14820/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/14820/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-14820/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2018-11-09 01:41:49.365
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-14820/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2018-11-09 01:41:49.367
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 5aac805 HIVE-20782: Clean unused code to improve redability (Slim B, reviewed by Teddy Choi)
+ git clean -f -d
Removing ${project.basedir}/
Removing itests/${project.basedir}/
Removing standalone-metastore/metastore-server/src/gen/
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at 5aac805 HIVE-20782: Clean unused code to improve redability (Slim B, reviewed by Teddy Choi)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2018-11-09 01:41:49.990
+ rm -rf ../yetus_PreCommit-HIVE-Build-14820
+ mkdir ../yetus_PreCommit-HIVE-Build-14820
+ git gc
+ cp -R . ../yetus_PreCommit-HIVE-Build-14820
+ mkdir /data/hiveptest/logs/PreCommit-HIVE-Build-14820/yetus
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
Going to apply patch with: git apply -p0
+ [[ maven == \m\a\v\e\n ]]
+ rm -rf /data/hiveptest/working/maven/org/apache/hive
+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven
protoc-jar: executing: [/tmp/protoc8902627240627946349.exe, --version]
libprotoc 2.5.0
protoc-jar: executing: [/tmp/protoc8902627240627946349.exe, -I/data/hiveptest/working/apache-github-source-source/standalone-metastore/metastore-common/src/main/protobuf/org/apache/hadoop/hive/metastore, --java_out=/data/hiveptest/working/apache-github-source-source/standalone-metastore/metastore-common/target/generated-sources, /data/hiveptest/working/apache-github-source-source/standalone-metastore/metastore-common/src/main/protobuf/org/apache/hadoop/hive/metastore/metastore.proto]
ANTLR Parser Generator  Version 3.5.2
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (process-resource-bundles) on project hive-shims-0.23: Execution process-resource-bundles of goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process failed. ConcurrentModificationException -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-shims-0.23
+ result=1
+ '[' 1 -ne 0 ']'
+ rm -rf yetus_PreCommit-HIVE-Build-14820
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12947304 - PreCommit-HIVE-Build","10/Nov/18 04:25;hiveqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 33s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 27s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 53s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 24s{color} | {color:green} master passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m 43s{color} | {color:blue} itests/util in master has 48 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 35s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 23s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 51s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 12s{color} | {color:red} itests/util: The patch generated 2 new + 16 unchanged - 1 fixed = 18 total (was 17) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 35s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 12s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 15m  7s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-14849/dev-support/hive-personality.sh |
| git revision | master / 7ae4a2c |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-14849/yetus/diff-checkstyle-itests_util.txt |
| modules | C: itests/util itests/hive-minikdc U: itests |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-14849/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","10/Nov/18 05:23;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12947526/HIVE-20845.2.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 14 failed/errored test(s), 15529 tests executed
*Failed tests:*
{noformat}
TestSSL - did not produce a TEST-*.xml file (likely timed out) (batchId=257)
org.apache.hive.beeline.hs2connection.TestBeelineConnectionUsingHiveSite.testBeelineConnectionSSL (batchId=253)
org.apache.hive.beeline.hs2connection.TestBeelineWithUserHs2ConnectionFile.testBeelineConnectionSSL (batchId=253)
org.apache.hive.jdbc.TestActivePassiveHA.testActivePassiveHA (batchId=258)
org.apache.hive.jdbc.TestActivePassiveHA.testClientConnectionsOnFailover (batchId=258)
org.apache.hive.jdbc.TestActivePassiveHA.testConnectionActivePassiveHAServiceDiscovery (batchId=258)
org.apache.hive.jdbc.TestActivePassiveHA.testManualFailover (batchId=258)
org.apache.hive.jdbc.TestActivePassiveHA.testManualFailoverUnauthorized (batchId=258)
org.apache.hive.jdbc.TestActivePassiveHA.testNoConnectionOnPassive (batchId=258)
org.apache.hive.jdbc.TestNoSaslAuth.org.apache.hive.jdbc.TestNoSaslAuth (batchId=261)
org.apache.hive.jdbc.TestXSRFFilter.testFilterEnabledNoInjection (batchId=256)
org.apache.hive.jdbc.miniHS2.TestHs2ConnectionMetricsBinary.testOpenConnectionMetrics (batchId=262)
org.apache.hive.jdbc.miniHS2.TestHs2ConnectionMetricsHttp.testOpenConnectionMetrics (batchId=262)
org.apache.hive.minikdc.TestSSLWithMiniKdc.org.apache.hive.minikdc.TestSSLWithMiniKdc (batchId=273)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/14849/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/14849/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-14849/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 14 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12947526 - PreCommit-HIVE-Build","10/Nov/18 08:38;pvary;The funny thing is that all of them running locally.
[~sershe]: Any fast clues how did you reproduce the issue what made you add these lines to MiniHS2.waitForStartup:
{code}
        if (e.getMessage().contains(""Cannot open sessions on an inactive HS2"")) {
          // Passive HS2 has started. TODO: seems fragile
          return;
        }
{code}
Using Linux instead of Mac?","12/Nov/18 20:12;sershe;[~pvary] that is specific to some unit tests that start HS2 in passive mode (HA tests). 
HS2 that comes up as passive will not accept the session, so starting one is not a valid liveness check. Not sure why it won't repro... I think I hit it on the Mac before fixing.","17/Nov/18 06:34;pvary;Thanks [~sershe]!

Will try again then 😀","19/Nov/18 09:20;pvary;There are strange errors in the hive.log when trying to connect to an active-passive HS2 with DriverManager.getConnection(...).
{code:java}
2018-11-19T00:00:17,408  INFO [Thread-9] jdbc.ZooKeeperHiveClientHelper: Found HS2 Active Host: localhost Port: 50666 Identity: 2ca89854-eecd-447b-b5b3-8882af5f44ac Mode: binary
2018-11-19T00:00:17,408 DEBUG [Thread-9] jdbc.ZooKeeperHiveClientHelper: Configurations applied to JDBC connection params. {hive.server2.instance.uri=localhost:50666, hive.server2.authentication=NONE, hive.server2.transport.mode=binary, hive.server2.thrift.sasl.qop=auth, hive.server2.thrift.bind.host=localhost, hive.server2.thrift.port=50666, hive.server2.use.SSL=false, registry.unique.id=2ca89854-eecd-447b-b5b3-8882af5f44ac, hive.server2.webui.port=50665}
2018-11-19T00:00:17,408 DEBUG [Thread-9] jdbc.Utils: Resolved authority: localhost:50666
2018-11-19T00:00:18,501  INFO [Thread-9] jdbc.HiveConnection: Connected to localhost:50666
2018-11-19T00:00:18,671 ERROR [HiveServer2-Handler-Pool: Thread-130] server.TThreadPoolServer: Error occurred during processing of message.
java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: Invalid status 80
        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:219) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:269) [libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_172]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_172]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_172]
Caused by: org.apache.thrift.transport.TTransportException: Invalid status 80
        at org.apache.thrift.transport.TSaslTransport.sendAndThrowMessage(TSaslTransport.java:232) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:184) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.transport.TSaslServerTransport.handleSaslStartMessage(TSaslServerTransport.java:125) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:271) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216) ~[libthrift-0.9.3.jar:0.9.3]
        ... 4 more
2018-11-19T00:00:18,681 ERROR [Thread-9] jdbc.HiveConnection: Error opening session
org.apache.thrift.transport.TTransportException: org.apache.http.client.ClientProtocolException
        at org.apache.thrift.transport.THttpClient.flushUsingHttpClient(THttpClient.java:297) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.transport.THttpClient.flush(THttpClient.java:313) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:73) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.hive.service.rpc.thrift.TCLIService$Client.send_OpenSession(TCLIService.java:170) ~[hive-service-rpc-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hive.service.rpc.thrift.TCLIService$Client.OpenSession(TCLIService.java:162) ~[hive-service-rpc-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hive.jdbc.HiveConnection.openSession(HiveConnection.java:809) [hive-jdbc-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:309) [hive-jdbc-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107) [hive-jdbc-4.0.0-SNAPSHOT.jar:?]
        at java.sql.DriverManager.getConnection(DriverManager.java:664) [?:1.8.0_172]
        at java.sql.DriverManager.getConnection(DriverManager.java:270) [?:1.8.0_172]
        at org.apache.hive.jdbc.miniHS2.MiniHS2.waitForStartup(MiniHS2.java:622) [hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hive.jdbc.miniHS2.MiniHS2.start(MiniHS2.java:402) [hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hive.jdbc.TestActivePassiveHA.testManualFailoverUnauthorized(TestActivePassiveHA.java:406) [test-classes/:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_172]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_172]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_172]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_172]
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) [junit-4.11.jar:?]
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.11.jar:?]
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) [junit-4.11.jar:?]
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.11.jar:?]
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74) [junit-4.11.jar:?]
Caused by: org.apache.http.client.ClientProtocolException
        at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:186) ~[httpclient-4.5.2.jar:4.5.2]
        at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:117) ~[httpclient-4.5.2.jar:4.5.2]
        at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55) ~[httpclient-4.5.2.jar:4.5.2]
        at org.apache.thrift.transport.THttpClient.flushUsingHttpClient(THttpClient.java:251) ~[libthrift-0.9.3.jar:0.9.3]
        ... 22 more
Caused by: org.apache.http.ProtocolException: The server failed to respond with a valid HTTP response
        at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:151) ~[httpclient-4.5.2.jar:4.5.2]
        at org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:57) ~[httpclient-4.5.2.jar:4.5.2]
        at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:261) ~[httpcore-4.4.4.jar:4.4.4]
        at org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:165) ~[httpcore-4.4.4.jar:4.4.4]
        at org.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:167) ~[httpclient-4.5.2.jar:4.5.2]
        at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:272) ~[httpcore-4.4.4.jar:4.4.4]
        at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:124) ~[httpcore-4.4.4.jar:4.4.4]
        at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:271) ~[httpclient-4.5.2.jar:4.5.2]
        at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184) ~[httpclient-4.5.2.jar:4.5.2]
        at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88) ~[httpclient-4.5.2.jar:4.5.2]
        at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) ~[httpclient-4.5.2.jar:4.5.2]
        at org.apache.http.impl.execchain.ServiceUnavailableRetryExec.execute(ServiceUnavailableRetryExec.java:84) ~[httpclient-4.5.2.jar:4.5.2]
        at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184) ~[httpclient-4.5.2.jar:4.5.2]
        at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:117) ~[httpclient-4.5.2.jar:4.5.2]
        at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55) ~[httpclient-4.5.2.jar:4.5.2]
        at org.apache.thrift.transport.THttpClient.flushUsingHttpClient(THttpClient.java:251) ~[libthrift-0.9.3.jar:0.9.3]
        ... 22 more
2018-11-19T00:00:18,683  WARN [Thread-9] jdbc.HiveConnection: Failed to connect to localhost:50666
{code}

And later:
{code:java}
2018-11-19T00:00:38,026  INFO [Thread-9] server.HS2ActivePassiveHARegistryClient: Returning cached registry client for namespace: hs2ActivePassiveHATest-unsecure
2018-11-19T00:00:38,026  INFO [Thread-9] jdbc.ZooKeeperHiveClientHelper: Found HS2 Active Host: localhost Port: 50668 Identity: 8b4ca1b4-ade6-47d9-ba00-f3e5f6dd4ece Mode: http:/clidriverTest
2018-11-19T00:00:38,026 DEBUG [Thread-9] jdbc.ZooKeeperHiveClientHelper: Configurations applied to JDBC connection params. {hive.server2.instance.uri=localhost:50668, hive.server2.authentication=NONE, hive.server2.transport.mode=http, hive.server2.thrift.http.path=clidriverTest, hive.server2.thrift.http.port=50668, hive.server2.thrift.bind.host=localhost, hive.server2.use.SSL=false, registry.unique.id=8b4ca1b4-ade6-47d9-ba00-f3e5f6dd4ece, hive.server2.webui.port=50669}
2018-11-19T00:00:38,026  INFO [Thread-9] jdbc.Utils: Selected HiveServer2 instance with uri: jdbc:hive2://localhost:50668/default;serviceDiscoveryMode=zooKeeperHA;zooKeeperNamespace=hs2ActivePassiveHATest;transportMode=http;httpPath=cliservice;
2018-11-19T00:00:38,026  WARN [Thread-9] jdbc.HiveConnection: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:50668/default;serviceDiscoveryMode=zooKeeperHA;zooKeeperNamespace=hs2ActivePassiveHATest;transportMode=http;httpPath=cliservice;: Could not establish connection to jdbc:hive2://localhost:50668/default;serviceDiscoveryMode=zooKeeperHA;zooKeeperNamespace=hs2ActivePassiveHATest;transportMode=http;httpPath=cliservice;: HTTP Response code: 405 Retrying 0 of 1
2018-11-19T00:00:38,028  INFO [Thread-9] jdbc.HiveConnection: Connected to localhost:50668
2018-11-19T00:00:38,030 ERROR [Thread-9] jdbc.HiveConnection: Error opening session
org.apache.thrift.transport.TTransportException: HTTP Response code: 405
        at org.apache.thrift.transport.THttpClient.flushUsingHttpClient(THttpClient.java:262) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.transport.THttpClient.flush(THttpClient.java:313) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:73) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.hive.service.rpc.thrift.TCLIService$Client.send_OpenSession(TCLIService.java:170) ~[hive-service-rpc-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hive.service.rpc.thrift.TCLIService$Client.OpenSession(TCLIService.java:162) ~[hive-service-rpc-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hive.jdbc.HiveConnection.openSession(HiveConnection.java:809) [hive-jdbc-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:309) [hive-jdbc-4.0.0-SNAPSHOT.jar:?]
        at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107) [hive-jdbc-4.0.0-SNAPSHOT.jar:?]
        at java.sql.DriverManager.getConnection(DriverManager.java:664) [?:1.8.0_172]
        at java.sql.DriverManager.getConnection(DriverManager.java:270) [?:1.8.0_172]
        at org.apache.hive.jdbc.miniHS2.MiniHS2.waitForStartup(MiniHS2.java:622) [hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hive.jdbc.miniHS2.MiniHS2.start(MiniHS2.java:402) [hive-it-util-4.0.0-SNAPSHOT.jar:4.0.0-SNAPSHOT]
        at org.apache.hive.jdbc.TestActivePassiveHA.testManualFailoverUnauthorized(TestActivePassiveHA.java:406) [test-classes/:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_172]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_172]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_172]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_172]
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) [junit-4.11.jar:?]
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.11.jar:?]
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) [junit-4.11.jar:?]
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.11.jar:?]
        at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74) [junit-4.11.jar:?]
2018-11-19T00:00:38,030  WARN [Thread-9] jdbc.HiveConnection: Failed to connect to localhost:50668
{code}

Might indicate some bug in HA implementation? Any thoughts [~sershe]?

For the tests I just fell back to using the original method for checking the status of the MiniHS2 instances in HA mode.","19/Nov/18 10:32;hiveqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 33s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 22s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 52s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 24s{color} | {color:green} master passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m 40s{color} | {color:blue} itests/util in master has 48 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 34s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 26s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 24s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 49s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 34s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 12s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 15m  4s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-14983/dev-support/hive-personality.sh |
| git revision | master / fd5f34f |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| modules | C: itests/util itests/hive-minikdc U: itests |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-14983/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","19/Nov/18 11:29;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12948675/HIVE-20845.3.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 15536 tests executed
*Failed tests:*
{noformat}
TestMiniDruidCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=197)
	[druidmini_masking.q,druidmini_joins.q,druid_timestamptz.q]
TestSSL - did not produce a TEST-*.xml file (likely timed out) (batchId=258)
org.apache.hive.beeline.hs2connection.TestBeelineConnectionUsingHiveSite.testBeelineConnectionSSL (batchId=254)
org.apache.hive.beeline.hs2connection.TestBeelineWithUserHs2ConnectionFile.testBeelineConnectionSSL (batchId=254)
org.apache.hive.jdbc.TestNoSaslAuth.org.apache.hive.jdbc.TestNoSaslAuth (batchId=262)
org.apache.hive.jdbc.TestXSRFFilter.testFilterEnabledNoInjection (batchId=257)
org.apache.hive.jdbc.miniHS2.TestHs2ConnectionMetricsBinary.testOpenConnectionMetrics (batchId=263)
org.apache.hive.jdbc.miniHS2.TestHs2ConnectionMetricsHttp.testOpenConnectionMetrics (batchId=263)
org.apache.hive.minikdc.TestSSLWithMiniKdc.org.apache.hive.minikdc.TestSSLWithMiniKdc (batchId=274)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/14983/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/14983/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-14983/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12948675 - PreCommit-HIVE-Build","19/Nov/18 12:42;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12948675/HIVE-20845.3.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/14985/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/14985/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-14985/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Tests exited with: Exception: Patch URL https://issues.apache.org/jira/secure/attachment/12948675/HIVE-20845.3.patch was found in seen patch url's cache and a test was probably run already on it. Aborting...
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12948675 - PreCommit-HIVE-Build","19/Nov/18 19:07;hiveqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 38s{color} | {color:blue} Maven dependency ordering for branch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 30s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 52s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 22s{color} | {color:green} master passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m 44s{color} | {color:blue} itests/util in master has 48 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 36s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 25s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 53s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 24s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 13s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 15m 20s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-14990/dev-support/hive-personality.sh |
| git revision | master / ea1173a |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| modules | C: itests/util itests/hive-minikdc U: itests |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-14990/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","19/Nov/18 20:05;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12948739/HIVE-20845.4.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 15526 tests executed
*Failed tests:*
{noformat}
TestBeelinePasswordOption - did not produce a TEST-*.xml file (likely timed out) (batchId=254)
TestSSL - did not produce a TEST-*.xml file (likely timed out) (batchId=258)
org.apache.hive.beeline.hs2connection.TestBeelineConnectionUsingHiveSite.testBeelineConnectionSSL (batchId=254)
org.apache.hive.beeline.hs2connection.TestBeelineWithUserHs2ConnectionFile.testBeelineConnectionSSL (batchId=254)
org.apache.hive.jdbc.TestNoSaslAuth.org.apache.hive.jdbc.TestNoSaslAuth (batchId=262)
org.apache.hive.jdbc.TestXSRFFilter.testFilterEnabledNoInjection (batchId=257)
org.apache.hive.jdbc.miniHS2.TestHs2ConnectionMetricsBinary.testOpenConnectionMetrics (batchId=263)
org.apache.hive.jdbc.miniHS2.TestHs2ConnectionMetricsHttp.testOpenConnectionMetrics (batchId=263)
org.apache.hive.minikdc.TestSSLWithMiniKdc.org.apache.hive.minikdc.TestSSLWithMiniKdc (batchId=274)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/14990/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/14990/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-14990/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12948739 - PreCommit-HIVE-Build",,,,,,,,,,,,,,,,,,,,
Some replication tests are super slow and cause batch timeouts,HIVE-20697,13189772,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,vihangk1,vihangk1,05/Oct/18 18:11,18/Oct/18 05:37,18/Feb/21 09:57,,,,,,,,,,0,,"Some of these tests are taking a long time and can cause test batch timeouts given that we only give 40 min for a batch to complete. We should speed these tests up.

TestReplicationScenarios	20 min
TestReplicationScenariosAcidTables	11 min
TestReplicationScenariosAcrossInstances	5 min 14 sec
TestReplicationScenariosIncrementalLoadAcidTables	20 min",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-10-09 09:13:17.467,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 18 05:37:08 UTC 2018,,,,,,,"0|i3yw1r:",9223372036854775807,,,,,,,,,,,,,,,,,,"05/Oct/18 18:14;vihangk1;More runtime history
https://builds.apache.org/job/PreCommit-HIVE-Build/14249/testReport/org.apache.hadoop.hive.ql.parse/TestReplicationScenarios/history/
https://builds.apache.org/job/PreCommit-HIVE-Build/14249/testReport/org.apache.hadoop.hive.ql.parse/TestReplicationScenariosAcidTables/history/
https://builds.apache.org/job/PreCommit-HIVE-Build/14249/testReport/org.apache.hadoop.hive.ql.parse/TestReplicationScenariosAcrossInstances/history/
https://builds.apache.org/job/PreCommit-HIVE-Build/14249/testReport/org.apache.hadoop.hive.ql.parse/TestReplicationScenariosIncrementalLoadAcidTables/history/","05/Oct/18 18:24;vihangk1;cc [~sankarh] [~anishek] [~thejas] Can you please take a look since you are more familiar with this code? Can we disable the tests until this is fixed?","09/Oct/18 09:13;anishek;[~vihangk1] do you know how the batch is decided, is there a way to force batches with only couple of tests in batch above ?
","10/Oct/18 17:33;vihangk1;I think we can control that to reduce batch size or isolate them. Let me see how can we do that.  It may cause a increase in the overall test run time though. Is it not possible to reduce the time taken for these tests? 20 min for a unit test seems very high.","15/Oct/18 10:03;anishek;i tried to skip batching of these tests via the master-mr2.porperties but that didnot help, All f this is in HIVE-20679 runs however.
run times.
The reason the tests take a longer time is because there are various uses cases that are tried with test trying to do it as a end user, there are commands that needs processing in hive like insert / delete / create followed by repl dump / load commands which effectively does the same thing twice with some metadata work in between, hence the longer run times. ","17/Oct/18 17:30;vihangk1;Hi [~anishek] Sorry couldn't take a look at this. In fact I had tried adding them to the skipBatching list on the ptest server and I thought it should work. But apparently not. Let me take a look again.","17/Oct/18 18:06;vihangk1;this is what I had added to master-mr2.properties file on ptest server

{noformat}
ut.ql.skipBatching=TestDbTxnManager2 TestTxnCommands2WithSplitUpdateAndVectorization TestTxnCommands2WithSplitUpdate TestReplicationScenarios TestReplicationScenariosAcidTables TestReplicationScenariosAcrossInstances TestReplicationScenariosIncrementalLoadAcidTables
{noformat}

But it should have been in the itests/hive-unit since these tests are in itests/hive-unit module. I have updated the configs now. Hopefully the next precommit job should skip batching for these tests.","17/Oct/18 19:49;vihangk1;{noformat}
2018-10-17 19:16:04,963 DEBUG [TestExecutor] ExecutionPhase.execute:98 PBatch: UnitTestBatch [name=243_TestReplicationScenarios, id=243, moduleName=itests/hive-unit, batchSize=1, isParallel=true, testList=[TestReplicationScenarios]]
2018-10-17 19:16:04,963 DEBUG [TestExecutor] ExecutionPhase.execute:98 PBatch: UnitTestBatch 
2018-10-17 19:16:04,963 DEBUG [TestExecutor] ExecutionPhase.execute:98 PBatch: UnitTestBatch [name=245_TestReplicationScenariosAcrossInstances, id=245, moduleName=itests/hive-unit, batchSize=1, isParallel=true, testList=[TestReplicationScenariosAcrossInstances]]
2018-10-17 19:16:04,963 DEBUG [TestExecutor] ExecutionPhase.execute:98 PBatch: UnitTestBatch [name=246_TestReplicationScenariosAcidTables, id=246, moduleName=itests/hive-unit, batchSize=1, isParallel=true, testList=[TestReplicationScenariosAcidTables]]
2018-10-17 19:16:04,963 DEBUG [TestExecutor] ExecutionPhase.execute:98 PBatch: UnitTestBatch [name=247_TestReplicationScenariosIncrementalLoadAcidTables, id=247, moduleName=itests/hive-unit, batchSize=1, isParallel=true, testList=[TestReplicationScenariosIncrementalLoadAcidTables]]
{noformat}

These tests are not batched anymore. They run in batches of 1 test each.","18/Oct/18 04:38;anishek;Thanks [~vihangk1], does this mean skipbatching is available for all commits running / submitted ? There is no patch that needs to be uploaded here for now for this correct ?","18/Oct/18 05:37;vihangk1;yeah, nothing to be done with regards to batching of these tests.",,,,,,,,,,,,,,,,,,,,,,,,
Acid tests with multiple splits,HIVE-17296,13093946,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,ekoifman,ekoifman,ekoifman,10/Aug/17 23:28,17/Oct/18 21:28,18/Feb/21 09:57,,3.0.0,,,,,Transactions,,,0,,"data files in an Acid table are ORC files which may have multiple stripes
for such files in base/ or delta/ (and original files with non acid to acid conversion) are split by OrcInputFormat into multiple (stripe sized) chunks.
There is additional logic in in OrcRawRecordMerger (discoverKeyBounds/discoverOriginalKeyBounds) that is not tested by any E2E tests since none of the have enough data to generate multiple stripes in a single file.

testRecordReaderOldBaseAndDelta/testRecordReaderNewBaseAndDelta/testOriginalReaderPair
in TestOrcRawRecordMerger has some logic to test this but it really needs e2e tests.

With ORC-228 it will be possible to write such tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 17 21:28:53 UTC 2018,,,,,,,"0|i3io33:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,"15/Aug/17 19:01;ekoifman;ORC-228 is in ORC 1.5.  Note that MemoryManager is a ThreadLocal so changing this property may affect other tests.
See if this will actually work before backporting","04/Oct/18 22:24;ekoifman;Hive is now on ORC 1.5.3","17/Oct/18 21:28;ekoifman;see HIVE-20694 and TestVectorizedOrcAcidRowBatchReader",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add blobstore tests for MSCK REPAIR TABLE,HIVE-20724,13190777,Test,Patch Available,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,junyangl,junyangl,junyangl,10/Oct/18 22:35,17/Oct/18 06:38,18/Feb/21 09:57,,,,,,2.4.0,Tests,,,0,,This patch introduces a regression test into the hive-blobstore qtest module: recover_partitions.q.,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/18 18:58;junyangl;HIVE-20724.patch;https://issues.apache.org/jira/secure/attachment/12944192/HIVE-20724.patch",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2018-10-12 21:55:50.085,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 17 06:38:14 UTC 2018,,,,,,,"0|i3z25r:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,"12/Oct/18 21:55;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12943318/HIVE-20724.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/14393/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/14393/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-14393/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2018-10-12 21:54:48.934
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-14393/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2018-10-12 21:54:48.938
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at beccce3 HIVE-20590 : Allow merge statement to have column schema (Miklos Gergely via Ashutosh Chauhan)
+ git clean -f -d
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at beccce3 HIVE-20590 : Allow merge statement to have column schema (Miklos Gergely via Ashutosh Chauhan)
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2018-10-12 21:54:49.659
+ rm -rf ../yetus_PreCommit-HIVE-Build-14393
+ mkdir ../yetus_PreCommit-HIVE-Build-14393
+ git gc
+ cp -R . ../yetus_PreCommit-HIVE-Build-14393
+ mkdir /data/hiveptest/logs/PreCommit-HIVE-Build-14393/yetus
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
Going to apply patch with: git apply -p0
+ [[ maven == \m\a\v\e\n ]]
+ rm -rf /data/hiveptest/working/maven/org/apache/hive
+ mvn -B clean install -DskipTests -T 4 -q -Dmaven.repo.local=/data/hiveptest/working/maven
protoc-jar: executing: [/tmp/protoc6933125909630608223.exe, --version]
libprotoc 2.5.0
protoc-jar: executing: [/tmp/protoc6933125909630608223.exe, -I/data/hiveptest/working/apache-github-source-source/standalone-metastore/metastore-common/src/main/protobuf/org/apache/hadoop/hive/metastore, --java_out=/data/hiveptest/working/apache-github-source-source/standalone-metastore/metastore-common/target/generated-sources, /data/hiveptest/working/apache-github-source-source/standalone-metastore/metastore-common/src/main/protobuf/org/apache/hadoop/hive/metastore/metastore.proto]
ANTLR Parser Generator  Version 3.5.2
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process (process-resource-bundles) on project hive-shims: Execution process-resource-bundles of goal org.apache.maven.plugins:maven-remote-resources-plugin:1.5:process failed. ConcurrentModificationException -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-shims
+ result=1
+ '[' 1 -ne 0 ']'
+ rm -rf yetus_PreCommit-HIVE-Build-14393
+ exit 1
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12943318 - PreCommit-HIVE-Build","17/Oct/18 05:34;hiveqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 44s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}  1m 16s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-14515/dev-support/hive-personality.sh |
| git revision | master / fa97c67 |
| modules | C: itests/hive-blobstore U: itests/hive-blobstore |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-14515/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","17/Oct/18 06:38;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12944192/HIVE-20724.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 15094 tests executed
*Failed tests:*
{noformat}
TestMiniDruidCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=196)
	[druidmini_masking.q,druidmini_test1.q,druidkafkamini_basic.q,druidmini_joins.q,druid_timestamptz.q]
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/14515/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/14515/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-14515/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12944192 - PreCommit-HIVE-Build",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add blobstore tests for ALTER TABLE CONCATENATE,HIVE-20403,13179409,Test,Patch Available,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,junyangl,junyangl,junyangl,16/Aug/18 17:12,12/Oct/18 21:31,18/Feb/21 09:57,,,,,,2.4.0,Tests,,,0,,This patch introduces a regression test into the hive-blobstore qtest module: alter_table_concatenate.q.,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/18 22:32;junyangl;HIVE-20403.patch;https://issues.apache.org/jira/secure/attachment/12943317/HIVE-20403.patch",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2018-10-12 20:26:05.51,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 12 21:31:57 UTC 2018,,,,,,,"0|i3x4jb:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,"12/Oct/18 20:26;hiveqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  1m  0s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}  1m 39s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-14391/dev-support/hive-personality.sh |
| git revision | master / beccce3 |
| modules | C: itests/hive-blobstore U: itests/hive-blobstore |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-14391/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","12/Oct/18 21:31;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12943317/HIVE-20403.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 15078 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[udaf_corr] (batchId=84)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/14391/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/14391/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-14391/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12943317 - PreCommit-HIVE-Build",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TestMiniLlapLocalCliDriver.testCliDriver[load_dyn_part3],HIVE-20663,13188558,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,kgyrtkirk,klcopp,klcopp,01/Oct/18 15:42,02/Oct/18 08:41,18/Feb/21 09:57,,,,,,,,,,0,,"results/clientpositive/llap/load_dyn_part3.q.out currently requires 

Column stats: COMPLETE

but test outputs

Column stats: PARTIAL",,,,,,,,,,,,,,,,,,HIVE-20585,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-10-01 18:22:32.322,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 02 08:41:44 UTC 2018,,,,,,,"0|i3yokn:",9223372036854775807,,,,,,,,,,,,,,,,,,"01/Oct/18 18:22;kgyrtkirk;this is most probably the same as HIVE-20585 ....the symptom occurs in case of a specific set of metastore call sequence....","02/Oct/18 08:41;klcopp;Many thanks for picking this up, [~kgyrtkirk]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable druidmini_test1,HIVE-20429,13180057,Test,Patch Available,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,vgarg,vgarg,vgarg,20/Aug/18 20:24,21/Aug/18 00:19,18/Feb/21 09:57,,,,,,,,,,0,,This test is very flaky,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/18 20:24;vgarg;HIVE-20429.1.patch;https://issues.apache.org/jira/secure/attachment/12936322/HIVE-20429.1.patch",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2018-08-20 23:34:37.634,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 21 00:19:51 UTC 2018,,,,,,,"0|i3x8iv:",9223372036854775807,,,,,,,,,,,,,4.0.0,,,,,"20/Aug/18 23:34;hiveqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  8m 22s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 30s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 13s{color} | {color:green} master passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  0m 45s{color} | {color:blue} itests/util in master has 52 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 21s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 14s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  0m 54s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 21s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 13s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 13m 17s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-13357/dev-support/hive-personality.sh |
| git revision | master / 20baf49 |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| modules | C: itests/util U: itests/util |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-13357/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","21/Aug/18 00:19;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12936322/HIVE-20429.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:green}SUCCESS:{color} +1 due to 14884 tests passed

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/13357/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/13357/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-13357/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12936322 - PreCommit-HIVE-Build",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix test flakyness masked by HIVE-19730,HIVE-19795,13164105,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,pvary,pvary,05/Jun/18 08:04,05/Jun/18 08:04,18/Feb/21 09:57,,,,,,,,,,0,,"In HIVE-19730 we added a temporary fix for the metastore.client tests, so if closing the HMSClient throws an exception it does not cause the tests to fail.

It would be good to fix the root cause",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-06-05 08:04:01.0,,,,,,,"0|i3uiqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Precommits generating 20G of logs,HIVE-19487,13158498,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,vihangk1,vihangk1,10/May/18 15:06,30/May/18 15:36,18/Feb/21 09:57,,,,,,,,,,0,,Precommit jobs are generating huge logs. The disk gets full pretty quickly. I am not sure what was the log size but I think it was definitely not a problem previously. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-30 15:32:13.65,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 30 15:36:02 UTC 2018,,,,,,,"0|i3tkmn:",9223372036854775807,,,,,,,,,,,,,,,,,,"12/May/18 18:39;vihangk1;Following for the largest logs 
{noformat}
-rw-r--r-- 1 tomcat hiveptest 1169724848 May 11 21:11 ./succeeded/173-TestMiniLlapLocalCliDriver-vector_windowing_expressions.q-auto_join1.q-ins
ert_values_partitioned.q-and-27-more/logs/hive.log
-rw-r--r-- 1 tomcat hiveptest 1702616342 May 11 20:43 ./succeeded/35-TestCliDriver-constantPropWhen.q-udf_floor.q-join20.q-and-27-more/logs/hive
.log
-rw-r--r-- 1 tomcat hiveptest 4231255074 May 11 21:12 ./succeeded/221_UTBatch_service_8_tests/logs/hive.log
-rw-r--r-- 1 tomcat hiveptest 1062224231 May 11 21:08 ./succeeded/169-TestMiniLlapLocalCliDriver-union_top_level.q-vector_create_struct_table.q-
schema_evol_text_vecrow_part_all_primitive.q-and-27-more/logs/hive.log
{noformat}","12/May/18 18:41;vihangk1;2018-05-11 20:30:43,168 DEBUG [TestExecutor] ExecutionPhase.execute:96 PBatch: UnitTestBatch [name=221_UTBatch_service_8_tests, id=221, moduleName=service, batchSize=8, isParallel=true, testList=[TestCustomQueryFilter, TestUserFilter, TestUserSearchFilter, TestQuery, TestLdapAtnProviderWithMiniDS, TestHiveSQLException, TestCLIServiceConnectionLimits, TestCLIServiceRestore]]","30/May/18 15:32;stakiar;Is this a duplicate of HIVE-19514?","30/May/18 15:36;vihangk1;HIVE-19514 disabled fetching logs for successful tests. It still is generating 20G of logs if we start collecting all the logs. Currently its not a problem after HIVE-19514 since each run now generates ~2G of logs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create utility method to set staging tmp directory correctly,HIVE-19732,13162636,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,jcamachorodriguez,jcamachorodriguez,29/May/18 16:24,29/May/18 16:25,18/Feb/21 09:57,,,,,,,Test,,,0,,"Many tests are flaky because they point to same default staging tmp directory. For instance:
{noformat}
org.apache.hadoop.util.Shell$ExitCodeException: chmod: cannot access ‘/tmp/hadoop/mapred/staging/hiveptest985275899/.staging/job_local985275899_0088’: No such file or directory

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:1009) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell.run(Shell.java:902) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1227) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1321) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1303) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:840) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:508) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:489) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:511) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:727) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobResourceUploader.mkdirs(JobResourceUploader.java:658) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:172) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:133) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:102) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_102]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_102]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.submit(ControlledJob.java:336) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_102]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_102]
	at org.apache.pig.backend.hadoop23.PigJobControl.submit(PigJobControl.java:128) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:194) [pig-0.16.0-h2.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_102]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:276) [pig-0.16.0-h2.jar:?]
{noformat}

Till now, those tests have been fixed individually. It would be useful to create a utility method that given the conf object and the test name, sets the correct property values correctly so staging directories do not clash. ",,,,,,,,,,,,,,,,,,HIVE-19509,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-29 16:24:11.0,,,,,,,"0|i3u9of:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TestJdbcWithMiniHS2#testHttpRetryOnServerIdleTimeout,HIVE-19707,13161961,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,jcamachorodriguez,jcamachorodriguez,25/May/18 02:20,25/May/18 02:20,18/Feb/21 09:57,,3.1.0,,,,,,,,0,,,,,,,,,,,,,,,,,,,,HIVE-19509,HIVE-19706,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-25 02:20:22.0,,,,,,,"0|i3u5in:",9223372036854775807,,,,,,,,,,,,,3.1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Enable TestCliDriver#sample2, TestCliDriver#sample4, and TestCliDriver#sample6",HIVE-19657,13161142,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,jcamachorodriguez,jcamachorodriguez,22/May/18 17:29,25/May/18 01:21,18/Feb/21 09:57,,3.1.0,,,,,,,,0,,Disabled by HIVE-19617 in the context of HIVE-19509.,,,,,,,,,,,,,,,,,,HIVE-19509,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-22 17:29:44.0,,,,,,,"0|i3u0hr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable TestAutoPurgeTables test,HIVE-19616,13160469,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,jcamachorodriguez,jcamachorodriguez,19/May/18 00:22,19/May/18 00:43,18/Feb/21 09:57,,3.1.0,,,,,Test,,,0,,Disabled by HIVE-19589.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-19 00:22:11.0,,,,,,,"0|i3twq7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestDbNotificationListener is timing out on precommits,HIVE-19461,13157968,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,vihangk1,vihangk1,08/May/18 18:03,08/May/18 18:03,18/Feb/21 09:57,,,,,,,,,,0,,{{TestDbNotificationListener}} test is timing out on pre-commit consistently,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-08 18:03:29.0,,,,,,,"0|i3thdj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Umbrella: branch-3 failing tests,HIVE-19142,13151229,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,vgarg,vgarg,09/Apr/18 23:21,26/Apr/18 09:20,18/Feb/21 09:57,,,,,,,,,,0,,"This is the list [~alangates] specified on HIVE-19135 which are non-oom test failures:

*Errors*:
TestAcidOnTez.testGetSplitsLocks
TestJdbcWithLocalClusterSpark.testSparkQuery
TestJdbcWithLocalClusterSpark.testTempTable
TestJdbcWithMiniLlap.testLlapInputFormatEndToEnd
TestMTQueries.testMTQueries1
TestMultiSessionsHS2WithLocalClusterSpark.testSparkQuery
TestNegativeCliDriver.alter_notnull_constraint_violation
TestNegativeCliDriver.insert_into_acid_notnull
TestNegativeCliDriver.insert_into_notnull_constraint
TestNegativeCliDriver.insert_multi_into_notnull
TestNegativeCliDriver.insert_overwrite_notnull_constraint
TestNegativeCliDriver.update_notnull_constraint

*Failures*:
TestBlobstoreCliDriver.insert_into_dynamic_partitions
TestBlobstoreCliDriver.insert_overwrite_directory
TestBlobstoreCliDriver.insert_overwrite_dynamic_partitions
-TestCliDriver.acid_table_stats-
TestCliDriver.auto_sortmerge_join_2
TestCliDriver.avro_alter_table_update_columns
TestCliDriver.avrotblsjoin
TestCliDriver.dbtxnmgr_showlocks
TestCliDriver.orc_merge10
TestCliDriver.orc_schema_evolution_float
TestCliDriver.parquet_ppd_multifiles
TestCliDriver.schema_evol_par_vec_table_dictionary_encoding
TestCliDriver.schema_evol_par_vec_table_non_dictionary_encoding
TestCliDriver.selectindate
-TestCliDriver.statsoptimizer-
TestCliDriver.vector_bround
TestCliDriver.vector_case_when_1
TestCliDriver.vector_coalesce_2
TestCliDriver.vector_coalesce_3
TestCliDriver.vector_interval_1
TestCliDriver.vectorized_parquet_types
TestMetastoreVersion.testMetastoreVersion
TestMetastoreVersion.testVersionMatching
TestMiniDruidCliDriver.druidkafkamini_basic
TestMiniLlapCliDriver.llap_smb
TestMiniLlapCliDriver.unionDistinct_1
TestMiniTezCliDriver.explainanalyze_5
-TestNegativeCliDriver.authorization_caseinsensitivity-
-TestNegativeCliDriver.authorization_fail_1-
-TestNegativeCliDriver.authorization_grant_table_dup-
-TestNegativeCliDriver.authorization_role_case-
-TestNegativeCliDriver.authorization_role_grant_nosuchrole-
-TestNegativeCliDriver.authorization_table_grant_nosuchrole-
TestNegativeCliDriver.subquery_subquery_chain
TestSessionState.testCreatePath
TestSessionState.testCreatePath
TestSparkStatistics.testSparkStatistics
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-04-26 09:20:43.629,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 26 09:20:43 UTC 2018,,,,,,,"0|i3scfb:",9223372036854775807,,,,,,,,,,,,,,,,,,"10/Apr/18 00:56;vgarg;authorization tests are fixed by HIVE-19143","11/Apr/18 20:07;vgarg;testLlapInputFormatEndToEnd should be fixed by HIVE-17645","26/Apr/18 09:20;kgyrtkirk;there are some test breaks on master; which got backported to branch-3 also: HIVE-19137, HIVE-18739 ; they'll be probably fixed soon",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add sampling.q test for blobstores,HIVE-17819,13109754,Test,Patch Available,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,rangu,rangu,rangu,16/Oct/17 17:49,29/Mar/18 07:45,18/Feb/21 09:57,,,,,,,Tests,,,0,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/17 18:07;rangu;HIVE-17819.patch;https://issues.apache.org/jira/secure/attachment/12892434/HIVE-17819.patch",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2017-10-19 23:46:51.928,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Thu Mar 29 07:45:36 UTC 2018,,,,,,,"0|i3lbhj:",9223372036854775807,,,,,,,,,,,,,2.4.0,,,,,"16/Oct/17 18:08;rangu;Attached patch","19/Oct/17 23:46;jcamachorodriguez;Changing targeted fix version to 2.3.2 as this is not a blocker and we are releasing 2.3.1.","01/Nov/17 17:32;vihangk1;+1","09/Nov/17 16:29;stakiar;Moving to 2.3.3 release as this isn't a release blocker","27/Mar/18 20:42;daijy;I am about to release 2.3.3. Moving it to 2.4.","29/Mar/18 06:31;hiveqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 46s{color} | {color:blue} Maven dependency ordering for branch {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 17s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:red}-1{color} | {color:red} whitespace {color} | {color:red}  0m  0s{color} | {color:red} The patch has 1 line(s) that end in whitespace. Use git apply --whitespace=fix <<patch_file>>. Refer https://git-scm.com/docs/git-apply {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red}  0m 12s{color} | {color:red} The patch generated 49 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}  2m 27s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-9898/dev-support/hive-personality.sh |
| git revision | master / 1b01f9c |
| whitespace | http://104.198.109.242/logs//PreCommit-HIVE-Build-9898/yetus/whitespace-eol.txt |
| asflicense | http://104.198.109.242/logs//PreCommit-HIVE-Build-9898/yetus/patch-asflicense-problems.txt |
| modules | C: . itests/hive-blobstore U: . |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-9898/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","29/Mar/18 07:45;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12892434/HIVE-17819.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 84 failed/errored test(s), 13454 tests executed
*Failed tests:*
{noformat}
TestMinimrCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=93)
	[infer_bucket_sort_num_buckets.q,infer_bucket_sort_reducers_power_two.q,parallel_orderby.q,bucket_num_reducers_acid.q,infer_bucket_sort_map_operators.q,infer_bucket_sort_merge.q,root_dir_external_table.q,infer_bucket_sort_dyn_part.q,udf_using.q,bucket_num_reducers_acid2.q]
TestNegativeCliDriver - did not produce a TEST-*.xml file (likely timed out) (batchId=96)
	[udf_invalid.q,authorization_uri_export.q,default_constraint_complex_default_value.q,druid_datasource2.q,check_constraint_max_length.q,view_update.q,default_partition_name.q,authorization_public_create.q,load_wrong_fileformat_rc_seq.q,default_constraint_invalid_type.q,altern1.q,describe_xpath1.q,drop_view_failure2.q,temp_table_rename.q,invalid_select_column_with_subquery.q,udf_trunc_error1.q,insert_view_failure.q,dbtxnmgr_nodbunlock.q,authorization_show_columns.q,cte_recursion.q,merge_constraint_notnull.q,load_part_nospec.q,clusterbyorderby.q,orc_type_promotion2.q,ctas_noperm_loc.q,udf_min.q,udf_instr_wrong_args_len.q,invalid_create_tbl2.q,part_col_complex_type.q,authorization_drop_db_empty.q,smb_mapjoin_14.q,subquery_scalar_multi_rows.q,alter_partition_coltype_2columns.q,subquery_corr_in_agg.q,insert_overwrite_notnull_constraint.q,authorization_show_grant_otheruser_wtab.q,regex_col_groupby.q,ptf_negative_DuplicateWindowAlias.q,exim_22_export_authfail.q,udf_likeany_wrong1.q,groupby_key.q,ambiguous_col.q,groupby3_multi_distinct.q,authorization_alter_drop_ptn.q,invalid_cast_from_binary_5.q,show_create_table_does_not_exist.q,invalid_select_column.q,exim_20_managed_location_over_existing.q,interval_3.q,authorization_compile.q,join35.q,udf_concat_ws_wrong3.q,create_or_replace_view8.q,create_external_with_notnull_constraint.q,split_sample_out_of_range.q,materialized_view_no_transactional_rewrite.q,authorization_show_grant_otherrole.q,create_with_constraints_duplicate_name.q,invalid_stddev_samp_syntax.q,authorization_view_disable_cbo_7.q,autolocal1.q,avro_non_nullable_union.q,load_orc_negative_part.q,drop_view_failure1.q,columnstats_partlvl_invalid_values_autogather.q,exim_13_nonnative_import.q,alter_table_wrong_regex.q,udf_next_day_error_2.q,authorization_select.q,udf_trunc_error2.q,authorization_view_7.q,udf_format_number_wrong5.q,touch2.q,orc_type_promotion1.q,lateral_view_alias.q,show_tables_bad_db1.q,unset_table_property.q,alter_non_native.q,nvl_mismatch_type.q,load_orc_negative3.q,authorization_create_role_no_admin.q,invalid_distinct1.q,authorization_grant_server.q,orc_type_promotion3_acid.q,show_tables_bad1.q,macro_unused_parameter.q,drop_invalid_constraint3.q,drop_partition_filter_failure.q,char_pad_convert_fail3.q,exim_23_import_exist_authfail.q,drop_invalid_constraint4.q,authorization_create_macro1.q,archive1.q,subquery_multiple_cols_in_select.q,change_hive_hdfs_session_path.q,udf_trunc_error3.q,invalid_variance_syntax.q,authorization_truncate_2.q,invalid_avg_syntax.q,invalid_select_column_with_tablename.q,mm_truncate_cols.q,groupby_grouping_sets1.q,druid_location.q,groupby2_multi_distinct.q,authorization_sba_drop_table.q,dynamic_partitions_with_whitelist.q,compare_string_bigint_2.q,udf_greatest_error_2.q,authorization_view_6.q,show_tablestatus.q,duplicate_alias_in_transform_schema.q,create_with_fk_uk_same_tab.q,udtf_not_supported3.q,alter_table_constraint_invalid_fk_col2.q,udtf_not_supported1.q,dbtxnmgr_notableunlock.q,ptf_negative_InvalidValueBoundary.q,alter_table_constraint_duplicate_pk.q,udf_printf_wrong4.q,create_view_failure9.q,udf_elt_wrong_type.q,selectDistinctStarNeg_1.q,invalid_mapjoin1.q,load_stored_as_dirs.q,input1.q,udf_sort_array_wrong1.q,invalid_distinct2.q,invalid_select_fn.q,authorization_role_grant_otherrole.q,archive4.q,load_nonpart_authfail.q,recursive_view.q,authorization_view_disable_cbo_1.q,desc_failure4.q,create_not_acid.q,udf_sort_array_wrong3.q,char_pad_convert_fail0.q,udf_map_values_arg_type.q,alter_view_failure6_2.q,alter_partition_change_col_nonexist.q,update_non_acid_table.q,authorization_view_disable_cbo_5.q,ct_noperm_loc.q,interval_1.q,authorization_show_grant_otheruser_all.q,authorization_view_2.q,show_tables_bad2.q,groupby_rollup2.q,truncate_column_seqfile.q,create_view_failure5.q,authorization_create_view.q,ptf_window_boundaries.q,ctasnullcol.q,input_part0_neg_2.q,create_or_replace_view1.q,udf_max.q,exim_01_nonpart_over_loaded.q,msck_repair_1.q,orc_change_fileformat_acid.q,udf_nonexistent_resource.q,msck_repair_3.q,exim_19_external_over_existing.q,serde_regex2.q,msck_repair_2.q,exim_06_nonpart_noncompat_storage.q,illegal_partition_type4.q,udf_sort_array_by_wrong1.q,windowing_leadlag_in_udaf.q,avro_decimal.q,materialized_view_update.q,illegal_partition_type2.q,invalid_varchar_length_1.q,authorization_view_5.q,nested_complex_neg.q,lockneg_try_drop_locked_db.q,constraint_partition_columns.q,authorization_insertpart_noinspriv.q,avro_add_column_extschema.q,udf_sort_array_wrong2.q,drop_database_cascade.q,archive3.q,alter_view_failure5.q,load_orc_negative1.q,create_external_acid.q,check_constraint_temporary_udf.q,file_with_header_footer_negative.q,alter_view_failure6.q,create_view_failure6.q,char_pad_convert_fail1.q,invalid_var_samp_syntax.q,update_partition_col.q,database_create_already_exists.q,union2.q,windowing_invalid_udaf.q,authorization_public_drop.q,truncate_table_failure5.q,alter_view_failure2.q,udf_reflect_neg.q,interval_2.q,column_rename2.q,set_hiveconf_validation0.q,materialized_view_drop2.q,repl_dump_requires_admin.q,alter_view_failure7.q,alter_view_failure4.q,alter_view_failure3.q,udf_map_keys_arg_type.q,alter_partition_with_whitelist.q,authorization_show_role_principals_no_admin.q,table_nonprintable_negative.q,clusterbydistributeby.q,authorization_rolehierarchy_privs.q,alter_notnull_constraint_violation.q,check_constraint_aggregate.q,script_broken_pipe3.q,column_rename3.q,authorization_fail_create_db.q,analyze.q,compute_stats_long.q,sortmerge_mapjoin_mismatch_1.q,insert_into6.q,select_charliteral.q,fs_default_name2.q,check_constraint_qual_name.q,archive_multi5.q,input4.q,create_udaf_failure.q,create_table_failure1.q,regex_col_1.q,materialized_view_authorization_no_select_perm.q,decimal_precision_1.q,columnstats_partlvl_multiple_part_clause.q,udf_if_not_bool.q,materialized_view_replace_with_view.q,invalid_cast_to_binary_6.q,lockneg5.q,database_drop_not_empty.q,smb_bucketmapjoin.q,alter_rename_partition_failure2.q,invalid_cast_to_binary_4.q,decimal_precision.q,create_view_failure10.q,invalid_cast_to_binary_3.q,groupby_invalid_position.q,load_wrong_fileformat_txt_seq.q,exchange_partition_neg_partition_exists.q,authorization_not_owner_alter_tab_serdeprop.q,alter_rename_partition_failure.q,merge_negative_5.q,authorization_invalid_priv_v2.q,bucket_mapjoin_wrong_table_metadata_1.q,lockneg2.q,join28.q,exchange_partition_neg_test.q,lockneg_try_lock_db_in_use.q,udf_assert_true.q,udf_coalesce.q,join29.q,archive_partspec4.q,exim_24_import_part_authfail.q,exim_08_nonpart_noncompat_serde.q,udtf_explode_not_supported4.q,create_view_failure4.q,default_constraint_invalid_default_value_type.q,invalid_char_length_1.q,udf_in.q,create_view_failure3.q,strict_pruning_2.q,insertexternal1.q,alter_partition_change_col_dup_col.q,authorization_ctas2.q,uniquejoin.q,authorization_role_grant_nosuchrole.q,column_rename4.q,create_with_fk_wrong_ref2.q,authorization_role_case.q,udf_if_wrong_args_len.q,create_skewed_table_col_name_value_no_mismatch.q,compare_string_bigint.q,archive_insert2.q,authorization_grant_table_fail1.q,exim_02_all_part_over_overlap.q,archive_multi1.q,subquery_in_groupby.q,authorization_role_grant.q,insert_into_with_schema3.q,create_with_fk_constraint.q,materialized_view_insert.q,orc_replace_columns1.q,updateBasicStats.q,udf_likeall_wrong1.q,udf_map_keys_arg_num.q,subquery_scalar_corr_multi_rows.q,exchange_partition_neg_partition_exists2.q,udf_test_error.q,distinct_windowing_failure2.q,authorization_caseinsensitivity.q,update_bucket_col.q,create_or_replace_view3.q,lockneg_query_tbl_in_locked_db.q,authorization_select_view.q,create_or_replace_view4.q,disallow_incompatible_type_change_on2.q,ptf_negative_DistributeByOrderBy.q,desc_failure3.q,create_insert_outputformat.q,alter_table_wrong_location2.q,groupby_grouping_sets5.q,authorization_uri_create_table_ext.q,invalid_t_create2.q,archive_corrupt.q,groupby_grouping_sets6.q,subquery_corr_grandparent.q,archive_insert3.q,authorization_alter_table_exchange_partition_fail2.q,invalid_cast_from_binary_2.q,authorize_grant_public.q,authorization_fail_drop_db.q,insert_into_with_schema4.q,parquet_alter_part_table_drop_columns.q,unionSortBy.q,invalid_char_length_2.q,insert_multi_into_notnull.q,groupby_grouping_id1.q,strict_orderby_2.q,udf_next_day_error_1.q,authorize_revoke_public.q,load_part_authfail.q,drop_default_partition_filter.q,orc_replace_columns2.q,alter_file_format.q,authorization_alter_db_owner_default.q,authorization_uri_createdb.q,authorization_import_ptn.q,selectDistinctWithoutAggr.q,split_sample_wrong_format.q,authorization_fail_8.q,regex_col_2.q,ptf_negative_PartitionBySortBy.q,stats_aggregator_error_2.q,drop_table_failure1.q,stats_aggregator_error_1.q,udf_concat_ws_wrong1.q,exim_12_nonnative_export.q,clustern4.q,strict_orderby.q,authorization_insertoverwrite_nodel.q,clustern3.q,exim_18_part_spec_missing.q,invalid_t_transform.q,columnstats_tbllvl_complex_type.q,authorization_grant_table_allpriv.q,msck_repair_4.q,set_hiveconf_validation1.q,authorization_alter_table_exchange_partition_fail.q,ctas_noemptyfolder.q,clustern2.q,set_hiveconf_validation2.q,bucket_mapjoin_mismatch1.q,mm_convert.q,orc_change_fileformat.q,truncate_bucketed_column.q,druid_datasource.q,udf_assert_true2.q,strict_join_2.q,udf_format_number_wrong3.q,default_constraint_invalid_default_value_length.q,masking_acid_update.q,materialized_view_no_transactional_rewrite_2.q,materialized_view_authorization_drop_other.q,dyn_part1.q,default_constraint_invalid_default_value2.q,select_star_suffix.q,subquery_select_distinct.q,authorization_uri_create_table1.q,special_character_in_tabnames_1.q,archive_partspec2.q,analyze_non_existent_tbl.q,create_with_pk_constraints_enforced.q,orderby_position_unsupported.q,subquery_subquery_chain_exists.q,orc_reorder_columns1_acid.q,subquery_notin_implicit_gby.q,notable_alias4.q,semijoin2.q,desc_failure1.q,ptf_negative_HavingLeadWithPTF.q,alter_tableprops_external_with_default_constraint.q,create_unknown_udf_udaf.q,materialized_view_authorization_create_no_grant.q,orc_change_serde.q,uniquejoin3.q,stats_publisher_error_1.q,authorization_part.q,alter_table_constraint_invalid_fk_tbl1.q,fileformat_void_input.q,truncate_table_failure3.q,alter_table_constraint_invalid_fk_tbl2.q,mismatch_columns_insertion.q,repl_load_requires_admin.q]
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[acid_table_stats] (batchId=54)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[vectorized_bucketmapjoin1] (batchId=25)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=153)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucket_map_join_tez_empty] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[groupby_groupingset_bug] (batchId=173)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[mergejoin] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[tez_smb_main] (batchId=160)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[update_access_time_non_current_db] (batchId=171)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vectorization_div0] (batchId=170)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vectorized_dynamic_semijoin_reduction] (batchId=154)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[bucketizedhiveinputformat] (batchId=182)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_5] (batchId=105)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.org.apache.hadoop.hive.cli.TestNegativeCliDriver (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[insert_into_acid_notnull] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[insert_into_notnull_constraint] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[subquery_in_implicit_gby] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[subquery_subquery_chain] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[truncate_column_list_bucketing] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[truncate_nonexistant_column] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[truncate_partition_column2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[truncate_partition_column] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_field_wrong_args_len] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_field_wrong_type] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_format_number_wrong1] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_format_number_wrong2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_format_number_wrong4] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_format_number_wrong7] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_greatest_error_1] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_in_2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_instr_wrong_type] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_last_day_error_1] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_last_day_error_2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_locate_wrong_args_len] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_map_values_arg_num] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_printf_wrong1] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_printf_wrong2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_printf_wrong3] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_qualified_name] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_size_wrong_args_len] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_sort_array_by_wrong2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_sort_array_by_wrong3] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_test_error_reduce] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udf_when_type_wrong] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udtf_explode_not_supported1] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udtf_explode_not_supported2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udtf_explode_not_supported3] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[udtf_invalid_place] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[union22] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[unionClusterBy] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[unionDistributeBy] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[unionLimit] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[unionOrderBy] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[uniquejoin2] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[unset_view_property] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[update_no_such_table] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[update_not_acid] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[update_notnull_constraint] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[windowing_after_orderby] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[windowing_ll_no_neg] (batchId=95)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[windowing_ll_no_over] (batchId=95)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_scalar] (batchId=125)
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testMetastoreVersion (batchId=226)
org.apache.hadoop.hive.metastore.TestMetastoreVersion.testVersionMatching (batchId=226)
org.apache.hadoop.hive.metastore.client.TestAppendPartitions.testAppendPartitionNullPartValues[Embedded] (batchId=208)
org.apache.hadoop.hive.metastore.client.TestAppendPartitions.testAppendPartitionNullPartValues[Remote] (batchId=208)
org.apache.hadoop.hive.ql.TestAcidOnTez.testGetSplitsLocks (batchId=227)
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1 (batchId=230)
org.apache.hive.jdbc.TestActivePassiveHA.testManualFailover (batchId=238)
org.apache.hive.jdbc.TestJdbcWithMiniLlap.testLlapInputFormatEndToEnd (batchId=238)
org.apache.hive.jdbc.TestTriggersMoveWorkloadManager.testTriggerMoveAndKill (batchId=241)
org.apache.hive.jdbc.TestTriggersMoveWorkloadManager.testTriggerMoveBackKill (batchId=241)
org.apache.hive.jdbc.TestTriggersWorkloadManager.testMultipleTriggers2 (batchId=238)
org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerCustomCreatedDynamicPartitions (batchId=238)
org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerCustomCreatedDynamicPartitionsMultiInsert (batchId=238)
org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerCustomCreatedDynamicPartitionsUnionAll (batchId=238)
org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerCustomCreatedFiles (batchId=238)
org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerCustomNonExistent (batchId=238)
org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerHighBytesRead (batchId=238)
org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerHighShuffleBytes (batchId=238)
org.apache.hive.jdbc.TestTriggersWorkloadManager.testTriggerVertexRawInputSplitsNoKill (batchId=238)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/9898/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/9898/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-9898/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 84 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12892434 - PreCommit-HIVE-Build",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add compression related tests for blobstores,HIVE-18650,13136991,Test,Patch Available,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,yuzhousun,yuzhousun,yuzhousun,07/Feb/18 23:10,10/Feb/18 23:00,18/Feb/21 09:57,,2.3.2,,,,,Tests,,,0,,"This patch introduces the following tests and files into the hive-blobstore module:
 * bzip_read.q -> Test read bzip compressed input data
 * bzip_write.q -> Test write gzip compressed output table
 * gzip_read.q -> Test read gzip compressed input data
 * gzip_write.q -> Test write gzip compressed output table
 * gzip_splits.q -> Test read splited gzip compressed input data
 * over10k.bz2 -> Bzip compressed file of over10k
 * gzip_splits/ -> 10 gzip files of 21-22 KB",,,,,,,,,,,,,,,,,,,,,,,"09/Feb/18 19:09;yuzhousun;HIVE-18650.patch;https://issues.apache.org/jira/secure/attachment/12909970/HIVE-18650.patch","07/Feb/18 23:04;yuzhousun;over10k.bz2;https://issues.apache.org/jira/secure/attachment/12909680/over10k.bz2","07/Feb/18 23:03;yuzhousun;s_aa.gz;https://issues.apache.org/jira/secure/attachment/12909690/s_aa.gz","07/Feb/18 23:03;yuzhousun;s_ab.gz;https://issues.apache.org/jira/secure/attachment/12909689/s_ab.gz","07/Feb/18 23:03;yuzhousun;s_ac.gz;https://issues.apache.org/jira/secure/attachment/12909688/s_ac.gz","07/Feb/18 23:03;yuzhousun;s_ad.gz;https://issues.apache.org/jira/secure/attachment/12909687/s_ad.gz","07/Feb/18 23:03;yuzhousun;s_ae.gz;https://issues.apache.org/jira/secure/attachment/12909686/s_ae.gz","07/Feb/18 23:03;yuzhousun;s_af.gz;https://issues.apache.org/jira/secure/attachment/12909685/s_af.gz","07/Feb/18 23:03;yuzhousun;s_ag.gz;https://issues.apache.org/jira/secure/attachment/12909684/s_ag.gz","07/Feb/18 23:03;yuzhousun;s_ah.gz;https://issues.apache.org/jira/secure/attachment/12909683/s_ah.gz","07/Feb/18 23:03;yuzhousun;s_ai.gz;https://issues.apache.org/jira/secure/attachment/12909682/s_ai.gz","07/Feb/18 23:03;yuzhousun;s_aj.gz;https://issues.apache.org/jira/secure/attachment/12909681/s_aj.gz",,,,,,,12.0,,,,,,,,,,,,,,,,,,,,2018-02-09 05:25:09.667,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 10 23:00:06 UTC 2018,,,,,,,"0|i3px7b:",9223372036854775807,,,,,,,,,,,,,,,,,,"09/Feb/18 05:25;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12909693/HIVE-18650.patch

{color:red}ERROR:{color} -1 due to build exiting with an error

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/9108/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/9108/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-9108/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Tests exited with: NonZeroExitCodeException
Command 'bash /data/hiveptest/working/scratch/source-prep.sh' failed with exit status 1 and output '+ date '+%Y-%m-%d %T.%3N'
2018-02-09 05:23:25.575
+ [[ -n /usr/lib/jvm/java-8-openjdk-amd64 ]]
+ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
+ export PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ PATH=/usr/lib/jvm/java-8-openjdk-amd64/bin/:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games
+ export 'ANT_OPTS=-Xmx1g -XX:MaxPermSize=256m '
+ ANT_OPTS='-Xmx1g -XX:MaxPermSize=256m '
+ export 'MAVEN_OPTS=-Xmx1g '
+ MAVEN_OPTS='-Xmx1g '
+ cd /data/hiveptest/working/
+ tee /data/hiveptest/logs/PreCommit-HIVE-Build-9108/source-prep.txt
+ [[ false == \t\r\u\e ]]
+ mkdir -p maven ivy
+ [[ git = \s\v\n ]]
+ [[ git = \g\i\t ]]
+ [[ -z master ]]
+ [[ -d apache-github-source-source ]]
+ [[ ! -d apache-github-source-source/.git ]]
+ [[ ! -d apache-github-source-source ]]
+ date '+%Y-%m-%d %T.%3N'
2018-02-09 05:23:25.578
+ cd apache-github-source-source
+ git fetch origin
+ git reset --hard HEAD
HEAD is now at 3464df4 Revert ""HIVE-18350 : load data should rename files consistent with insert statements. (Deepak Jaiswal, reviewed by Sergey Shelukhin and Ashutosh Chauhan)""
+ git clean -f -d
+ git checkout master
Already on 'master'
Your branch is up-to-date with 'origin/master'.
+ git reset --hard origin/master
HEAD is now at 3464df4 Revert ""HIVE-18350 : load data should rename files consistent with insert statements. (Deepak Jaiswal, reviewed by Sergey Shelukhin and Ashutosh Chauhan)""
+ git merge --ff-only origin/master
Already up-to-date.
+ date '+%Y-%m-%d %T.%3N'
2018-02-09 05:23:28.085
+ rm -rf ../yetus
+ mkdir ../yetus
+ git gc
+ cp -R . ../yetus
+ mkdir /data/hiveptest/logs/PreCommit-HIVE-Build-9108/yetus
+ patchCommandPath=/data/hiveptest/working/scratch/smart-apply-patch.sh
+ patchFilePath=/data/hiveptest/working/scratch/build.patch
+ [[ -f /data/hiveptest/working/scratch/build.patch ]]
+ chmod +x /data/hiveptest/working/scratch/smart-apply-patch.sh
+ /data/hiveptest/working/scratch/smart-apply-patch.sh /data/hiveptest/working/scratch/build.patch
fatal: git diff header lacks filename information when removing 0 leading pathname components (line 4)
error: cannot apply binary patch to 'data/files/gzip_splits/s_aa.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'data/files/gzip_splits/s_aa.gz' without full index line
error: data/files/gzip_splits/s_aa.gz: patch does not apply
error: cannot apply binary patch to 'data/files/gzip_splits/s_ab.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'data/files/gzip_splits/s_ab.gz' without full index line
error: data/files/gzip_splits/s_ab.gz: patch does not apply
error: cannot apply binary patch to 'data/files/gzip_splits/s_ac.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'data/files/gzip_splits/s_ac.gz' without full index line
error: data/files/gzip_splits/s_ac.gz: patch does not apply
error: cannot apply binary patch to 'data/files/gzip_splits/s_ad.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'data/files/gzip_splits/s_ad.gz' without full index line
error: data/files/gzip_splits/s_ad.gz: patch does not apply
error: cannot apply binary patch to 'data/files/gzip_splits/s_ae.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'data/files/gzip_splits/s_ae.gz' without full index line
error: data/files/gzip_splits/s_ae.gz: patch does not apply
error: cannot apply binary patch to 'data/files/gzip_splits/s_af.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'data/files/gzip_splits/s_af.gz' without full index line
error: data/files/gzip_splits/s_af.gz: patch does not apply
error: cannot apply binary patch to 'data/files/gzip_splits/s_ag.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'data/files/gzip_splits/s_ag.gz' without full index line
error: data/files/gzip_splits/s_ag.gz: patch does not apply
error: cannot apply binary patch to 'data/files/gzip_splits/s_ah.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'data/files/gzip_splits/s_ah.gz' without full index line
error: data/files/gzip_splits/s_ah.gz: patch does not apply
error: cannot apply binary patch to 'data/files/gzip_splits/s_ai.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'data/files/gzip_splits/s_ai.gz' without full index line
error: data/files/gzip_splits/s_ai.gz: patch does not apply
error: cannot apply binary patch to 'data/files/gzip_splits/s_aj.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'data/files/gzip_splits/s_aj.gz' without full index line
error: data/files/gzip_splits/s_aj.gz: patch does not apply
error: cannot apply binary patch to 'data/files/over10k.bz2' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'data/files/over10k.bz2' without full index line
error: data/files/over10k.bz2: patch does not apply
error: cannot apply binary patch to 'files/gzip_splits/s_aa.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'files/gzip_splits/s_aa.gz' without full index line
error: files/gzip_splits/s_aa.gz: patch does not apply
error: cannot apply binary patch to 'files/gzip_splits/s_ab.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'files/gzip_splits/s_ab.gz' without full index line
error: files/gzip_splits/s_ab.gz: patch does not apply
error: cannot apply binary patch to 'files/gzip_splits/s_ac.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'files/gzip_splits/s_ac.gz' without full index line
error: files/gzip_splits/s_ac.gz: patch does not apply
error: cannot apply binary patch to 'files/gzip_splits/s_ad.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'files/gzip_splits/s_ad.gz' without full index line
error: files/gzip_splits/s_ad.gz: patch does not apply
error: cannot apply binary patch to 'files/gzip_splits/s_ae.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'files/gzip_splits/s_ae.gz' without full index line
error: files/gzip_splits/s_ae.gz: patch does not apply
error: cannot apply binary patch to 'files/gzip_splits/s_af.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'files/gzip_splits/s_af.gz' without full index line
error: files/gzip_splits/s_af.gz: patch does not apply
error: cannot apply binary patch to 'files/gzip_splits/s_ag.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'files/gzip_splits/s_ag.gz' without full index line
error: files/gzip_splits/s_ag.gz: patch does not apply
error: cannot apply binary patch to 'files/gzip_splits/s_ah.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'files/gzip_splits/s_ah.gz' without full index line
error: files/gzip_splits/s_ah.gz: patch does not apply
error: cannot apply binary patch to 'files/gzip_splits/s_ai.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'files/gzip_splits/s_ai.gz' without full index line
error: files/gzip_splits/s_ai.gz: patch does not apply
error: cannot apply binary patch to 'files/gzip_splits/s_aj.gz' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'files/gzip_splits/s_aj.gz' without full index line
error: files/gzip_splits/s_aj.gz: patch does not apply
error: cannot apply binary patch to 'files/over10k.bz2' without full index line
Falling back to three-way merge...
error: cannot apply binary patch to 'files/over10k.bz2' without full index line
error: files/over10k.bz2: patch does not apply
'
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12909693 - PreCommit-HIVE-Build","09/Feb/18 19:11;yuzhousun;Will update it with binary diff patch","09/Feb/18 19:13;yuzhousun;Update with binary patch","10/Feb/18 21:53;hiveqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  1m 43s{color} | {color:blue} Maven dependency ordering for branch {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:blue}0{color} | {color:blue} mvndep {color} | {color:blue}  0m 25s{color} | {color:blue} Maven dependency ordering for patch {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 10s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}  2m 44s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus/dev-support/hive-personality.sh |
| git revision | master / ddd4c9a |
| modules | C: . itests/hive-blobstore U: . |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-9145/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","10/Feb/18 23:00;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12909970/HIVE-18650.patch

{color:green}SUCCESS:{color} +1 due to 5 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 24 failed/errored test(s), 13160 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_queries] (batchId=241)
org.apache.hadoop.hive.cli.TestBlobstoreCliDriver.testCliDriver[gzip_write] (batchId=252)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[mapjoin_hook] (batchId=13)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join5] (batchId=36)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[row__id] (batchId=79)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_move_tbl] (batchId=175)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucket_map_join_tez1] (batchId=172)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid] (batchId=171)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid_fast] (batchId=162)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[resourceplan] (batchId=164)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=161)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[schemeAuthority] (batchId=180)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_opt_shuffle_serde] (batchId=180)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ppd_join5] (batchId=122)
org.apache.hadoop.hive.cli.TestSparkPerfCliDriver.testCliDriver[query1] (batchId=251)
org.apache.hadoop.hive.cli.control.TestDanglingQOuts.checkDanglingQOut (batchId=222)
org.apache.hadoop.hive.metastore.client.TestTablesCreateDropAlterTruncate.testAlterTableNullStorageDescriptorInNew[Embedded] (batchId=206)
org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.testWrite (batchId=257)
org.apache.hive.beeline.cli.TestHiveCli.testNoErrorDB (batchId=188)
org.apache.hive.jdbc.TestSSL.testConnectionMismatch (batchId=235)
org.apache.hive.jdbc.TestSSL.testConnectionWrongCertCN (batchId=235)
org.apache.hive.jdbc.TestSSL.testMetastoreConnectionWrongCertCN (batchId=235)
org.apache.hive.jdbc.authorization.TestCLIAuthzSessionContext.testAuthzSessionContextContents (batchId=239)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/9145/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/9145/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-9145/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 24 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12909970 - PreCommit-HIVE-Build",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update q.out for TestSparkCliDriver.testCliDriver[ppd_join5],HIVE-18640,13136642,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,stakiar,stakiar,stakiar,06/Feb/18 18:56,07/Feb/18 00:45,18/Feb/21 09:57,,,,,,,Test,,,0,,TestSparkCliDriver.testCliDriver[ppd_join5] has been consistently failing for 166 builds with a q-file diff.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 07 00:45:10 UTC 2018,,,,,,,"0|i3pv1r:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,"07/Feb/18 00:45;stakiar;ummm really not sure what is going on here. Test works locally on OSX. Tried on Ubuntu still works. Tried on an actual ptest-slave and it still works. Tried the entire q-test batch, still works. So not sure why this is failing. JVM version maybe?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark tar is downloaded every time for itest,HIVE-18282,13125102,Test,Patch Available,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,lirui,lirui,15/Dec/17 05:21,20/Dec/17 04:24,18/Feb/21 09:57,,,,,,,,,,0,,"Seems we missed the md5 file for spark-2.2.0?
cc [~kellyzly], [~stakiar]",,,,,,,,,,,,,,,,,,,,,,,"15/Dec/17 06:05;lirui;HIVE-18282.1.patch;https://issues.apache.org/jira/secure/attachment/12902220/HIVE-18282.1.patch",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2017-12-15 05:35:33.976,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 20 04:24:47 UTC 2017,,,,,,,"0|i3nxhz:",9223372036854775807,,,,,,,,,,,,,,,,,,"15/Dec/17 05:35;kellyzly;[~stakiar]: http://d3jw87u4immizc.cloudfront.net/spark-tarball/spark-2.0.0-bin-hadoop2-without-hive.tgz.md5sum exists. But http://d3jw87u4immizc.cloudfront.net/spark-tarball/spark-2.2.0-bin-hadoop2-without-hive.tgz.md5sum does not exist. Can you help upload the md5sum for spark2.2 package otherwise spark tar will be downloaded even the package has been downloaded?
$HIVE_SOURCE/itests/pom.xml#Line 82
{code}
    download() {
                        url=$1;
                        finalName=$2
                        tarName=$(basename $url)
                        rm -rf $BASE_DIR/$finalName
                        if [[ ! -f $DOWNLOAD_DIR/$tarName ]]
                        then
                         curl -Sso $DOWNLOAD_DIR/$tarName $url
                        else
                          local md5File=""$tarName"".md5sum
                          curl -Sso $DOWNLOAD_DIR/$md5File ""$url"".md5sum
                          cd $DOWNLOAD_DIR
                          if type md5sum >/dev/null &amp;&amp; ! md5sum -c $md5File; then  // use md5sum to check if need to update package.
                            curl -Sso $DOWNLOAD_DIR/$tarName $url || return 1
                          fi  

                          cd -
                        fi  
                        tar -zxf $DOWNLOAD_DIR/$tarName -C $BASE_DIR
                        mv $BASE_DIR/spark-${spark.version}-bin-hadoop2-without-hive $BASE_DIR/$finalName
                      }   


{code}","15/Dec/17 06:06;lirui;Meanwhile, I think it's better to avoid the re-download when we can't download the checksum.","15/Dec/17 10:09;hiveqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 21s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m  0s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m  0s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} xml {color} | {color:green}  0m  1s{color} | {color:green} The patch has no ill-formed XML file. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m  0s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 12s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}  7m 41s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  xml  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus/dev-support/hive-personality.sh |
| git revision | master / 856d88d |
| Default Java | 1.8.0_111 |
| modules | C: itests U: itests |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-8265/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","15/Dec/17 11:08;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12902220/HIVE-18282.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 17 failed/errored test(s), 11527 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[auto_join25] (batchId=72)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_join5] (batchId=35)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucket_map_join_tez1] (batchId=170)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[bucketsortoptimize_insert_2] (batchId=152)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[hybridgrace_hashjoin_2] (batchId=157)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[insert_values_orig_table_use_metadata] (batchId=165)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[llap_acid_fast] (batchId=160)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[mergejoin] (batchId=165)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[quotedid_smb] (batchId=157)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sysdb] (batchId=160)
org.apache.hadoop.hive.cli.TestNegativeCliDriver.testCliDriver[authorization_part] (batchId=93)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[auto_sortmerge_join_10] (batchId=138)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[bucketsortoptimize_insert_7] (batchId=128)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[ppd_join5] (batchId=120)
org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver[subquery_multi] (batchId=113)
org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.testConstraints (batchId=226)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/8265/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/8265/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-8265/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 17 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12902220 - PreCommit-HIVE-Build","15/Dec/17 18:41;stakiar;Whoops, I forgot to upload it previously. I just uploaded the md5sum file so it should be working now.","20/Dec/17 04:24;lirui;Thanks [~stakiar] for uploading the file. Do you think we can make the code change? So that we don't get similar problem in the future.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Import hive data to solr,HIVE-18081,13118894,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,jaymin009,jaymin009,16/Nov/17 12:19,16/Nov/17 12:22,18/Feb/21 09:57,,2.3.2,,,,0.10.1,Database/Schema,,,0,,"I want to import data from Hive database to solr so i can direct call solr API for search indexing result.

I tried this: https://github.com/lucidworks/hive-solr
But not get any solution. please update me ASAP.


Appreciate in advance for support.
","Its local environment. with centos 7 , Mongodb 2.4, hadoop  2.7 and hive 2.1.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-11-16 12:19:47.0,,,,,,,"0|hzzy9h:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix test failure TestAccumuloCliDriver caused from the accumulo version upgrade,HIVE-17583,13104365,Test,Patch Available,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,aihuaxu,aihuaxu,aihuaxu,22/Sep/17 18:31,14/Nov/17 17:57,18/Feb/21 09:57,,3.0.0,,,,,Test,,,0,,HIVE-17373 increases the accumulo version and it's causing the test failure for TestAccumuloCliDriver,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/17 18:43;aihuaxu;HIVE-17583.1.patch;https://issues.apache.org/jira/secure/attachment/12888544/HIVE-17583.1.patch",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2017-09-23 00:38:03.24,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 23 00:38:03 UTC 2017,,,,,,,"0|i3kffb:",9223372036854775807,,,,,,,,,,,,,,,,,,"22/Sep/17 18:45;aihuaxu;New version of accumulo uses thrift 0.93. Change the thrift version in the test to 0.93 as well.","23/Sep/17 00:38;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12888544/HIVE-17583.1.patch

{color:red}ERROR:{color} -1 due to no test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 8 failed/errored test(s), 11060 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_joins] (batchId=231)
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_predicate_pushdown] (batchId=231)
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_queries] (batchId=231)
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_single_sourced_multi_insert] (batchId=231)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[union_fast_stats] (batchId=156)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_explainuser_1] (batchId=170)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=235)
org.apache.hadoop.hive.ql.security.TestMetastoreAuthorizationProvider.testSimplePrivileges (batchId=221)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6949/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6949/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6949/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 8 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12888544 - PreCommit-HIVE-Build",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add Type2 SCD merge tests,HIVE-15898,13042702,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,ekoifman,ekoifman,ekoifman,13/Feb/17 22:36,17/Oct/17 18:38,18/Feb/21 09:57,,,,,,,Transactions,,,0,,,,,,,,,,,,,,,,,,,,,,,,HIVE-10924,"14/Feb/17 01:53;ekoifman;HIVE-15898.01.patch;https://issues.apache.org/jira/secure/attachment/12852459/HIVE-15898.01.patch","14/Feb/17 04:57;ekoifman;HIVE-15898.02.patch;https://issues.apache.org/jira/secure/attachment/12852490/HIVE-15898.02.patch","28/Feb/17 21:11;ekoifman;HIVE-15898.03.patch;https://issues.apache.org/jira/secure/attachment/12855220/HIVE-15898.03.patch","01/Mar/17 01:10;ekoifman;HIVE-15898.04.patch;https://issues.apache.org/jira/secure/attachment/12855271/HIVE-15898.04.patch","10/Jul/17 23:06;ekoifman;HIVE-15898.05.patch;https://issues.apache.org/jira/secure/attachment/12876527/HIVE-15898.05.patch","11/Jul/17 04:23;ekoifman;HIVE-15898.06.patch;https://issues.apache.org/jira/secure/attachment/12876558/HIVE-15898.06.patch","11/Jul/17 13:53;ekoifman;HIVE-15898.07.patch;https://issues.apache.org/jira/secure/attachment/12876624/HIVE-15898.07.patch","13/Jul/17 04:18;ekoifman;HIVE-15898.08.patch;https://issues.apache.org/jira/secure/attachment/12877014/HIVE-15898.08.patch","13/Jul/17 14:55;ekoifman;HIVE-15898.09.patch;https://issues.apache.org/jira/secure/attachment/12877091/HIVE-15898.09.patch","13/Jul/17 19:49;ekoifman;HIVE-15898.10.patch;https://issues.apache.org/jira/secure/attachment/12877148/HIVE-15898.10.patch","13/Jul/17 23:10;ekoifman;HIVE-15898.11.patch;https://issues.apache.org/jira/secure/attachment/12877185/HIVE-15898.11.patch","14/Jul/17 18:10;ekoifman;HIVE-15898.12.patch;https://issues.apache.org/jira/secure/attachment/12877353/HIVE-15898.12.patch","14/Jul/17 19:53;ekoifman;HIVE-15898.13.patch;https://issues.apache.org/jira/secure/attachment/12877380/HIVE-15898.13.patch",,,,,,13.0,,,,,,,,,,,,,,,,,,,,2017-02-14 03:41:20.671,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 14 20:57:57 UTC 2017,,,,,,,"0|i3a0nz:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,"14/Feb/17 03:41;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12852459/HIVE-15898.01.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10222 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge_type2_scd] (batchId=139)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver (batchId=162)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3527/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3527/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3527/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12852459 - PreCommit-HIVE-Build","14/Feb/17 06:28;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12852490/HIVE-15898.02.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 10225 tests executed
*Failed tests:*
{noformat}
TestDerbyConnector - did not produce a TEST-*.xml file (likely timed out) (batchId=235)
org.apache.hadoop.hive.cli.TestEncryptedHDFSCliDriver.testCliDriver[encryption_join_with_different_encryption_keys] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge_type2_scd] (batchId=139)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hadoop.hive.cli.TestSparkCliDriver.org.apache.hadoop.hive.cli.TestSparkCliDriver (batchId=128)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3529/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3529/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3529/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12852490 - PreCommit-HIVE-Build","01/Mar/17 04:14;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12855271/HIVE-15898.04.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 5 failed/errored test(s), 10299 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[schema_evol_text_vec_table] (batchId=147)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge_type2_scd] (batchId=139)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=223)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=223)
org.apache.hive.beeline.TestBeeLineWithArgs.testQueryProgressParallel (batchId=211)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/3855/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/3855/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-3855/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 5 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12855271 - PreCommit-HIVE-Build","11/Jul/17 01:07;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12876527/HIVE-15898.05.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 12 failed/errored test(s), 10835 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge_type2_scd] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=99)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.testSigning (batchId=289)
org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.testDelegationTokenSharedStore (batchId=229)
org.apache.hive.beeline.TestBeeLineWithArgs.testQueryProgressParallel (batchId=220)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5944/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5944/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5944/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 12 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12876527 - PreCommit-HIVE-Build","11/Jul/17 06:53;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12876558/HIVE-15898.06.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 10835 tests executed
*Failed tests:*
{noformat}
TestJdbcWithMiniLlap - did not produce a TEST-*.xml file (likely timed out) (batchId=226)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[create_merge_compressed] (batchId=237)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[materialized_view_create_rewrite] (batchId=237)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge_type2_scd] (batchId=144)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=232)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5951/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5951/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5951/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12876558 - PreCommit-HIVE-Build","11/Jul/17 15:31;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12876624/HIVE-15898.07.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 10839 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBlobstoreCliDriver.testCliDriver[zero_rows_blobstore] (batchId=240)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge_type2_scd] (batchId=144)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/5956/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/5956/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-5956/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12876624 - PreCommit-HIVE-Build","13/Jul/17 06:50;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12877014/HIVE-15898.08.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 9 failed/errored test(s), 10890 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[create_merge_compressed] (batchId=237)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge_type2_scd] (batchId=144)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testHttpRetryOnServerIdleTimeout (batchId=226)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6001/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6001/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6001/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 9 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12877014 - PreCommit-HIVE-Build","13/Jul/17 17:56;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12877091/HIVE-15898.09.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 13 failed/errored test(s), 10890 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestAccumuloCliDriver.testCliDriver[accumulo_queries] (batchId=228)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[create_merge_compressed] (batchId=237)
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[materialized_view_create_rewrite] (batchId=237)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge] (batchId=159)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge_type2_scd] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=99)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=232)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6011/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6011/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6011/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 13 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12877091 - PreCommit-HIVE-Build","13/Jul/17 23:02;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12877148/HIVE-15898.10.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 6 failed/errored test(s), 10892 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=237)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge_type2_scd] (batchId=144)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6017/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6017/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6017/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 6 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12877148 - PreCommit-HIVE-Build","14/Jul/17 06:50;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12877185/HIVE-15898.11.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 10 failed/errored test(s), 10892 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=237)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[partition_wise_fileformat6] (batchId=7)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge_type2_scd] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=232)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=232)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=177)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=177)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6026/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6026/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6026/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 10 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12877185 - PreCommit-HIVE-Build","14/Jul/17 19:34;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12877353/HIVE-15898.12.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 16 failed/errored test(s), 10899 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[materialized_view_create_rewrite] (batchId=238)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[orc_merge_diff_fs] (batchId=1)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge_type2_scd] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning_2] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_explainuser_1] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_use_op_stats] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_use_ts_stats_for_mapjoin] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=167)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=233)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query23] (batchId=233)
org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver.org.apache.hadoop.hive.cli.TestSparkNegativeCliDriver (batchId=240)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=178)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6037/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6037/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6037/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 16 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12877353 - PreCommit-HIVE-Build","14/Jul/17 19:41;stakiar;You can ignore the spark failures, they are due to HIVE-17099 - which I'm working on fixing.","14/Jul/17 20:57;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12877380/HIVE-15898.13.patch

{color:green}SUCCESS:{color} +1 due to 2 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 19 failed/errored test(s), 10908 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestBeeLineDriver.testCliDriver[insert_overwrite_local_directory_1] (batchId=238)
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver[ppd_windowing2] (batchId=10)
org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver[llap_smb] (batchId=143)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[sqlmerge_type2_scd] (batchId=144)
org.apache.hadoop.hive.cli.TestMiniLlapLocalCliDriver.testCliDriver[vector_if_expr] (batchId=145)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_dynamic_partition_pruning_2] (batchId=169)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_explainuser_1] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_use_op_stats] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_use_ts_stats_for_mapjoin] (batchId=168)
org.apache.hadoop.hive.cli.TestMiniSparkOnYarnCliDriver.testCliDriver[spark_vectorized_dynamic_partition_pruning] (batchId=167)
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver[explainanalyze_2] (batchId=99)
org.apache.hadoop.hive.cli.TestPerfCliDriver.testCliDriver[query14] (batchId=233)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testPartitionSpecRegistrationWithCustomSchema (batchId=178)
org.apache.hive.hcatalog.api.TestHCatClient.testTableSchemaPropagation (batchId=178)
org.apache.hive.hcatalog.pig.TestTextFileHCatStorer.testWriteDecimalX (batchId=181)
org.apache.hive.hcatalog.pig.TestTextFileHCatStorer.testWriteTimestamp (batchId=181)
org.apache.hive.jdbc.TestJdbcWithMiniHS2.testHttpRetryOnServerIdleTimeout (batchId=227)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/6039/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/6039/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-6039/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 19 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12877380 - PreCommit-HIVE-Build",,,,,,,,,,,,,,,,,,,,,
Add automated tests to check backwards compatibility of core APIs,HIVE-17130,13088577,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,stakiar,stakiar,stakiar,19/Jul/17 22:14,20/Jul/17 18:12,18/Feb/21 09:57,,,,,,,,,,0,,"We should added automated tests that check we are not adding backwards incompatible changes to core APIs (e.g. HMS APIs, SerDe APIs, UDF APIs, etc.).

Other Apache components, such as HBase and Hadoop already have existing checks. They are largely based on the japi-compliance-checker: https://lvc.github.io/japi-compliance-checker/

The nice thing about the japi-compliance-checker is that it can identify an interface as ""any class with a specified Java annotation"", so we can use the compliance-checker to check for backwards compatibility of any classes annotated with InterfaceAudience.Public

Ideally, we can build this check into our pre-commit job, or get it into YETUS, since we are already working on adding YETUS support to Hive.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 20 18:07:13 UTC 2017,,,,,,,"0|i3hron:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,"20/Jul/17 18:07;stakiar;Here are the relevant JIRAs from other Apache Projects who have done similar things:

HADOOP-13583
HBASE-12808 and HBASE-18020
KUDU-1265
SPARK-1094 (Spark uses a Scala specific tool called [MiMa|https://github.com/typesafehub/migration-manager])",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a TestNegativeEncryptedCliDriver for negative tests,HIVE-16322,13059862,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,vihangk1,vihangk1,vihangk1,28/Mar/17 22:01,28/Mar/17 22:01,18/Feb/21 09:57,,,,,,,Hive,,,0,,Currently there is no negative test CLI driver for running encrypted tests. The current mechanism to test negative tests for encrypted q-file tests is to set hive.cli.ignore.error=true and let the error occur and compared in q.out file. we should create a negative CLI driver just like TestNegativeCliDriver for encrypted use-cases.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-03-28 22:01:13.0,,,,,,,"0|i3cwzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tests for partitioned acid tables with schema evolution to UTs,HIVE-15897,13042701,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,ekoifman,ekoifman,ekoifman,13/Feb/17 22:34,25/Mar/17 21:43,18/Feb/21 09:57,,,,,,,Transactions,,,0,,,,,,,,,,,,,,,,,,,,HIVE-15967,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-02-13 22:34:38.0,,,,,,,"0|i3a0nr:",9223372036854775807,,,,,,,,,,,,,3.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test: cbo_rp_TestJdbcDriver2 is not executed in precommit runs,HIVE-15015,13013505,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,kgyrtkirk,kgyrtkirk,19/Oct/16 12:48,19/Oct/16 12:49,18/Feb/21 09:57,,,,,,,,,,0,,"full path to file:
{{./itests/hive-unit/src/test/java/org/apache/hive/jdbc/cbo_rp_TestJdbcDriver2.java}}

I guess it's because the class name doesnt some pattern like: {{Test.*}} 

And it's failing of course ;)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-10-19 12:48:15.0,,,,,,,"0|i353an:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestFetchResultsOfLog failures,HIVE-14441,12995288,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,taoli-hwx,taoli-hwx,05/Aug/16 17:47,05/Aug/16 17:56,18/Feb/21 09:57,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/16 17:56;taoli-hwx;BUG-63720.1.patch;https://issues.apache.org/jira/secure/attachment/12822345/BUG-63720.1.patch",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 05 17:56:21 UTC 2016,,,,,,,"0|i31z4n:",9223372036854775807,,,,,,,,,,,,,,,,,,"05/Aug/16 17:56;taoli-hwx; These test failures happen when logging level is set to INFO for HS2. See the attached patch as a temporary workaround to fix this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive (2.0.1) with s3 - Error: java.io.IOException No FileSystem for scheme: s3n,HIVE-13996,12977879,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,ssmane3,ssmane3,11/Jun/16 18:23,16/Jun/16 14:45,18/Feb/21 09:57,,2.0.1,,,,,,,,0,,"I m trying to create table As:
{code}
CREATE EXTERNAL TABLE mydata (sessionid STRING)   LOCATION 's3n://mybucket/kafkalogs/20160411/';
{code}
I have setup all aws access keys in both hadoop (core-site.xml) and hive-site.xml file.

How to resolve this issue ?

I am getting below error:
{code}
	16/06/11 23:39:16 [main]: ERROR exec.DDLTask: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: java.io.IOException No FileSystem for scheme: s3n)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:720)
	at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4135)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:306)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1653)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1412)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Caused by: MetaException(message:Got exception: java.io.IOException No FileSystem for scheme: s3n)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:29983)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme.read(ThriftHiveMetastore.java:29951)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result.read(ThriftHiveMetastore.java:29877)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_create_table_with_environment_context(ThriftHiveMetastore.java:1075)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.create_table_with_environment_context(ThriftHiveMetastore.java:1061)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2050)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:97)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:669)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:657)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy5.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:714)
	... 21 more

{code}","hadoop 2.7.1
hive 2.0.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-06-16 14:44:33.713,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 16 14:45:17 UTC 2016,,,,,,,"0|i2zc1r:",9223372036854775807,,,,,,,,,,,,,,,,,,"16/Jun/16 14:44;spena;That happens if you're using hadoop 2.6.x

For some reason, the S3 libraries weren't included into the hadoop classpath. 
Try and add it to the classpath by adding the following line in hadoop-env.sh which is located in $HADOOP_HOME/etc/hadoop/hadoop-env.sh:

{noformat}
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/tools/lib/*
{noformat}

You can check the hadoop classpath with this:
{noformat}
bin/hadoop classpath
{noformat}

However, I got the same issue when running an MR job. Possibly due to the same classpath issue ,but I haven't solved it yet.","16/Jun/16 14:45;spena;Just noticed you're using hadoop 2.7.1
Just check the classpath to see if 2.7.x also has the same problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How to initialize hive metastore database,HIVE-13734,12967332,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,damien.carol,ljy423897608,ljy423897608,11/May/16 08:36,11/May/16 15:56,18/Feb/21 09:57,,2.0.0,,,,2.0.0,Configuration,Database/Schema,18/May/16 00:00,0,mesosphere,"When run ""hive"",there is a mistake:Exception in thread ""main"" java.lang.RuntimeException:Hive metastore database is not initializad.Please use schematool to create the schema.",,,,,,,,,1814400,1814400,,0%,1814400,1814400,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-11 15:55:27.991,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 11 15:56:51 UTC 2016,,,,,,,"0|i2xk5r:",9223372036854775807,,,,,,,,,,,,,2.0.0,,,,,"11/May/16 15:55;damien.carol;[~ljy423897608] You JIRA ticket seems to be a question of configuration, no?
Please, if it's the case use the user mailing list. You will have more feedback for this kind of questions, trust me.
If it's ok, I will chnage this JIRA into ""not a bug"".","11/May/16 15:56;damien.carol;Here => https://hive.apache.org/mailing_lists.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JMeter <> Knox <> Hive : Test not successful,HIVE-13121,12941269,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,rdonbosco,rdonbosco,23/Feb/16 03:23,05/May/16 22:16,18/Feb/21 09:57,,,,,,,JDBC,,24/Feb/16 00:00,0,,"Hello,

We tried running SQL queries from JMeter using JDBC Hive connection.

Could see authentication call in hive logs but could not see the SQL gets executed and results retrieved. 
FYI., AutoCommit=false.

Could you please share your views if anyone has similar experience?

Does JMeter support load testing Hive?

Thanks,
Donbosco",Windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2016-02-23 03:23:36.0,,,,,,,"0|i2t6zr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Unit Test to test serializability/deserializability of HCatSplits,HIVE-11697,12860755,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,sushanth,sushanth,31/Aug/15 17:55,31/Aug/15 17:56,18/Feb/21 09:57,,,,,,,,,,0,,"As HIVE-11344 found, we should have unit tests for this scenario, and we need to add one in.",,,,,,,,,,,,,,,,,,HIVE-11344,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-08-31 17:55:31.0,,,,,,,"0|i2jlyf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.q test case for HIVE-3446,HIVE-3843,12625417,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,ashutoshc,ashutoshc,28/Dec/12 14:34,07/Aug/15 08:34,18/Feb/21 09:57,,0.11.0,,,,,Tests,Types,,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,301959,,,2012-12-28 14:34:17.0,,,,,,,"0|i16xon:",248590,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add concurrency tests to HiveServer2,HIVE-5373,12670807,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,vgumashta,vgumashta,26/Sep/13 19:10,09/May/15 00:11,18/Feb/21 09:57,,,,,,,HiveServer2,JDBC,,0,,TestHiveServer2Concurrency.java has nothing as of now. It is important to have some measure of concurrency that we can support.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-09 00:11:32.795,,,false,,,,,,,,,,,,,,,,,,350636,,,Sat May 09 00:11:32 UTC 2015,,,,,,,"0|i1oggv:",350929,,,,,,,,,,,,,,,,,,"09/May/15 00:11;sushanth;Removing fix version of 1.2.0 in preparation of release, since this is not a blocker for 1.2.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Write tests for state transitions of an operation in HS2,HIVE-5200,12666777,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,vgumashta,vgumashta,03/Sep/13 22:26,09/May/15 00:11,18/Feb/21 09:57,,0.12.0,,,,,HiveServer2,,,0,,The test should assert valid transitions for both synchronous and asynchronous execution.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-09 00:11:30.798,,,false,,,,,,,,,,,,,,,,,,346715,,,Sat May 09 00:11:30 UTC 2015,,,,,,,"0|i1nscv:",347016,,,,,,,,,,,,,,,,,,"03/Sep/13 22:27;vgumashta;Depends on [HIVE-4617|https://issues.apache.org/jira/browse/HIVE-4617].","09/May/15 00:11;sushanth;Removing fix version of 1.2.0 in preparation of release, since this is not a blocker for 1.2.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup UDF testcases,HIVE-998,12443717,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,cwsteinbach,cwsteinbach,18/Dec/09 02:02,06/Dec/14 05:24,18/Feb/21 09:57,,,,,,,,,,0,,"* For every UDF x there should be a corresponding udf_x.q testcase.
* Every udf_x.q file should begin with ""DESCRIBE FUNCTION X"", and ""DESCRIBE EXTENDED FUNCTION X"".

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,42660,,,2009-12-18 02:02:27.0,,,,,,,"0|i0lcvb:",122735,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable transactional unit tests against other databases,HIVE-8721,12752625,Test,Patch Available,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,gates,gates,gates,04/Nov/14 04:13,26/Nov/14 21:08,18/Feb/21 09:57,,0.14.0,,,,,Testing Infrastructure,Transactions,,0,,Since TxnHandler and subclasses use JDBC to directly connect to the underlying database (rather than relying on DataNucleus) it is important to test that all of the operations work against different database flavors.  An easy way to do this is to enable the unit tests to run against an external database.,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/14 04:19;gates;HIVE-8721.patch;https://issues.apache.org/jira/secure/attachment/12679143/HIVE-8721.patch",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2014-11-04 09:23:13.005,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 21:08:29 UTC 2014,,,,,,,"0|i21xe7:",9223372036854775807,,,,,,,,,,,,,,,,,,"04/Nov/14 04:19;gates;This patch changes DbTxnUtil to look at the environment variables:
METASTORE_CONNECTION_DRIVER, METASTORECONNECTURLKEY, METASTORE_CONNECTION_USER_NAME, METASTOREPWD

If these are set, they will be used to determine how to connect the database.  If these are not set, it uses the embedded Derby, as before.","04/Nov/14 09:23;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12679143/HIVE-8721.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 6669 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_optimize_nullscan
org.apache.hive.hcatalog.streaming.TestStreaming.testEndpointConnection
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1627/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/1627/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1627/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12679143 - PreCommit-HIVE-TRUNK-Build","20/Nov/14 18:42;ashutoshc;It will be awesome to have this extended for full metastore. Will make testing easy for regular tables easy as well.","26/Nov/14 21:08;gates;I'm fine to move this out of TxnDbUtil and put it somewhere more generic.  But I'm not sure where.  It works in TxnDbUtil because the transaction tests always start by calling TxnDbUtil.prepDb.  Is there an equivalent place we can guarantee gets called first in all unit tests?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a hive smoke test for Apache Bigtop ,HIVE-8553,12749656,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,jayunit100,jayunit100,21/Oct/14 23:42,12/Nov/14 20:40,18/Feb/21 09:57,,,,,,,Contrib,,,0,,"Hi hive.  Here at bigtop we'd like to work with you folks on curating some tests, like this https://github.com/apache/bigtop/blob/master/bigtop-tests/smoke-tests/hive/TestHiveSimple.groovy  inside of hive/, so that bigtop and hive communities can unify our smoke testing of the hive project.

We were curating our own fork of *your tests*, but that proved to be a difficult path to go down.

So lets join forces :) 

This is part of an overall effort we are making to have a strong connection with the ecosystem components that bigtop integrates.

 
This could be an artifact that the *hive community uses internally*, and also, that the *bigtop community can use to validate our hive packaging.* 

Hope to get some positive or negative feedback on this idea soon.  *I can help curate these tests if the hive community agrees to this idea.*

Since this is partially a collaborative JIRA, not just a code drop, Possibly a google hangout or phone conversation about this topic might be in order.",,,,,,,,,,,,,,,,,,BIGTOP-1450,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2014-10-21 23:42:37.0,,,,,,,"0|i21fe7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test HiveServer2 crash based on max thrift threads,HIVE-4291,12640683,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,thiruvel,thiruvel,thiruvel,04/Apr/13 01:22,09/Oct/14 10:42,18/Feb/21 09:57,,,,,,,HiveServer2,,,0,,"This test case ensures HS2 does not shutdown/crash when the thrift threads have been depleted. This is due to an issue fixed in THRIFT-1869. This test should pass post HIVE-4224. This test case ensures, the crash doesnt happen due to any changes in Thrift behavior.",,,,,,,,,,,,,,,,,,THRIFT-1869,HIVE-4224,HIVE-4766,HIVE-8418,,"04/Apr/13 01:24;thiruvel;TestHS2ThreadAllocation.java;https://issues.apache.org/jira/secure/attachment/12576897/TestHS2ThreadAllocation.java",,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,2013-04-04 17:47:01.569,,,false,,,,,,,,,,,,,,,,,,321142,,,Tue Jun 18 00:54:59 UTC 2013,,,,,,,"0|i1jerz:",321487,,,,,,,,,,,,,,,,,,"04/Apr/13 01:24;thiruvel;A WIP patch, will clean it up and post it on review board. I tested this with a custom built Thrift 0.9.0 library with THRIFT-692 changes, will retest with THRIFT-1.0 and update.","04/Apr/13 17:47;brocknoland;Hi,

We can probably take the test down to 10 threads or so? This would make the unit test faster and less resource intensive while still testing the same change.","05/Apr/13 19:52;thiruvel;Thanks Brock, I will also reduce the time delays so the entire test runs in less than 20 seconds.","18/Jun/13 00:15;ctang;If I understand right, the test testHS2StabilityOnLargeConnections seems expecting that client is returned SQLExcpetion (line 190) when the max # of threads is reached. While the fix in Thrift-1869 lets the client wait until the next one is available, and SQLException should not be thrown out in the test.","18/Jun/13 00:54;thiruvel;I wrote the test case based on the patches in THRIFT-692. I haven't had a chance to modify the tests based on THRIFT-1869.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Format-independent fixture for testing storage formats,HIVE-7484,12729080,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,davidzchen,davidzchen,23/Jul/14 03:30,23/Jul/14 03:40,18/Feb/21 09:57,,,,,,,Serializers/Deserializers,,,0,,"HIVE-7286 and HIVE-7420 adds test fixtures for running HCatalog core and HCatalog Pig Adapter tests against all native storage formats in Hive (i.e. those registered via StorageFormatDescriptor). Currently, Serdes are scattered between the ql and serde modules and storage formats have little shared test code.

As a result, a similar format-independent test fixture should be added to provide a common set of tests for all native storage formats. These tests will also be self-documenting, making it easier to keep track of incomplete or unstable storage formats by making it possible to disable running certain tests against a given storage format.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,407154,,,2014-07-23 03:30:38.0,,,,,,,"0|i1y29z:",407170,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ROLLUP: Make HCatalog tests generic for all storage formats,HIVE-7455,12728461,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,davidzchen,davidzchen,davidzchen,21/Jul/14 03:37,21/Jul/14 03:37,18/Feb/21 09:57,,,,,,,,,,0,,"Currently, HCatalog tests (both HCatalog core and HCatalog Pig Adapter) only test against RCFile with a few tests running against ORC. These tests should be made generic and be run against all Hive storage formats.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,406537,,,2014-07-21 03:37:09.0,,,,,,,"0|i1xyif:",406557,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unit test for evoloving schema in parquet files,HIVE-6463,12695960,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,jcoffey,jcoffey,jcoffey,19/Feb/14 13:54,19/Feb/14 13:55,18/Feb/21 09:57,,,,,,,,,,0,,Unit test(s) for patch found in #HIVE-6456,,,,,,,,,,,,,,,,,,HIVE-6456,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,374438,,,2014-02-19 13:54:58.0,,,,,,,"0|i1sixr:",374738,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use of relative paths for data load in .q files breaks on Windows,HIVE-6413,12694746,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,rusanu,rusanu,12/Feb/14 14:22,13/Feb/14 08:09,18/Feb/21 09:57,,,,,,,Build Infrastructure,,,1,,"Eg. partition_type_check.q:

FAILED: SemanticException Line 2:23 Invalid path ''../../data/files/T1.txt'': Relative path in absolute URI: file:E:/HW/project/hive-monarch/data/files/T1.txt

This happens because the path is constructed in LoadSemanticAnalizer.initializeFromUri by appending the ""user.dir"" system property:  path = new Path(System.getProperty(""user.dir""), fromPath).toString(); . The resulted path is missing the leading ""/"" in front the drive letter.

This was fixed in the past with HIVE-3126 (change 39fbb41e3e96858391646c0e20897e848616e8e2) but was reverted with HIVE-6048 (change c6f3bcccda986498ecb1e8070594961203038a8b)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,373254,,,2014-02-12 14:22:53.0,,,,,,,"0|i1sbnz:",373555,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate Clover with Hive,HIVE-2991,12553439,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,iveselovsky,ashutoshc,ashutoshc,30/Apr/12 23:50,28/Jul/13 18:04,18/Feb/21 09:57,,0.10.0,0.11.0,0.9.0,,,Testing Infrastructure,,,0,,Atlassian has donated license of their code coverage tool Clover to ASF. Lets make use of it to generate code coverage report to figure out which areas of Hive are well tested and which ones are not. More information about license can be found in Hadoop jira HADOOP-1718 ,,,,,,,,,,,,,,,,,,,,,,,"01/May/12 00:15;phabricator@reviews.facebook.net;ASF.LICENSE.NOT.GRANTED--HIVE-2991.D2985.1.patch;https://issues.apache.org/jira/secure/attachment/12525141/ASF.LICENSE.NOT.GRANTED--HIVE-2991.D2985.1.patch","11/Jul/13 15:44;iveselovsky;HIVE-clover-branch-0.10--N1.patch;https://issues.apache.org/jira/secure/attachment/12591861/HIVE-clover-branch-0.10--N1.patch","11/Jul/13 15:44;iveselovsky;HIVE-clover-branch-0.11--N1.patch;https://issues.apache.org/jira/secure/attachment/12591862/HIVE-clover-branch-0.11--N1.patch","11/Jul/13 15:44;iveselovsky;HIVE-clover-trunk--N1.patch;https://issues.apache.org/jira/secure/attachment/12591863/HIVE-clover-trunk--N1.patch","13/Dec/12 07:04;ikatsov;hive-trunk-clover-html-report.zip;https://issues.apache.org/jira/secure/attachment/12560730/hive-trunk-clover-html-report.zip","06/Dec/12 13:48;ikatsov;hive.2991.1.branch-0.10.patch;https://issues.apache.org/jira/secure/attachment/12556279/hive.2991.1.branch-0.10.patch","06/Dec/12 13:48;ikatsov;hive.2991.1.branch-0.9.patch;https://issues.apache.org/jira/secure/attachment/12556278/hive.2991.1.branch-0.9.patch","06/Dec/12 13:48;ikatsov;hive.2991.1.trunk.patch;https://issues.apache.org/jira/secure/attachment/12556280/hive.2991.1.trunk.patch","13/Dec/12 11:24;ikatsov;hive.2991.2.branch-0.10.patch;https://issues.apache.org/jira/secure/attachment/12560764/hive.2991.2.branch-0.10.patch","13/Dec/12 11:24;ikatsov;hive.2991.2.branch-0.9.patch;https://issues.apache.org/jira/secure/attachment/12560763/hive.2991.2.branch-0.9.patch","05/Feb/13 07:41;ikatsov;hive.2991.2.trunk.patch;https://issues.apache.org/jira/secure/attachment/12567971/hive.2991.2.trunk.patch","14/Feb/13 08:43;ikatsov;hive.2991.3.branch-0.10.patch;https://issues.apache.org/jira/secure/attachment/12569326/hive.2991.3.branch-0.10.patch","14/Feb/13 08:43;ikatsov;hive.2991.3.branch-0.9.patch;https://issues.apache.org/jira/secure/attachment/12569327/hive.2991.3.branch-0.9.patch","14/Feb/13 08:43;ikatsov;hive.2991.3.trunk.patch;https://issues.apache.org/jira/secure/attachment/12569328/hive.2991.3.trunk.patch","03/Apr/13 06:44;ikatsov;hive.2991.4.branch-0.10.patch;https://issues.apache.org/jira/secure/attachment/12576729/hive.2991.4.branch-0.10.patch","03/Apr/13 06:44;ikatsov;hive.2991.4.branch-0.9.patch;https://issues.apache.org/jira/secure/attachment/12576728/hive.2991.4.branch-0.9.patch","03/Apr/13 06:44;ikatsov;hive.2991.4.trunk.patch;https://issues.apache.org/jira/secure/attachment/12576730/hive.2991.4.trunk.patch",,17.0,,,,,,,,,,,,,,,,,,,,2012-05-01 00:15:44.893,,,false,,,,,,,,,,,,,,,,,,237601,,,Sun Jul 28 18:04:25 UTC 2013,,,,,,,"0|i0lmav:",124263,,,,,,,,,,,,,,,,,,"01/May/12 00:15;phabricator@reviews.facebook.net;ashutoshc requested code review of ""HIVE-2991 [jira] Integrate Clover with Hive"".
Reviewers: JIRA

  https://issues.apache.org/jira/browse/HIVE-2991

  This patch integrates clover with Hive. To generate code coverage follow these steps:

  Download the clover jar and license from https://svn.apache.org/repos/private/committers/donated-licenses/clover/2.6.x/README.txt and put them in same dir
  (lets say to /home/me/clover/). Then to generate the clover reports run ant clean package test clover-reports -Dclover.jar=/home/me/clover/clover.jar

  TEST PLAN: Run ant clean package test clover-reports -Dclover.jar=/home/me/clover/clover.jar and reports should be in build/${ant.project.name}/clover/reports

  Atlassian has donated license of their code coverage tool Clover to ASF. Lets make use of it to generate code coverage report to figure out which areas of Hive are well tested and which ones are not. More information about license can be found in Hadoop jira HADOOP-1718

TEST PLAN
  EMPTY

REVISION DETAIL
  https://reviews.facebook.net/D2985

AFFECTED FILES
  build.xml
  build-common.xml

MANAGE HERALD DIFFERENTIAL RULES
  https://reviews.facebook.net/herald/view/differential/

WHY DID I GET THIS EMAIL?
  https://reviews.facebook.net/herald/transcript/6795/

Tip: use the X-Herald-Rules header to filter Herald messages in your client.
","01/May/12 01:28;appodictic;I would be interested to see this. The crazy way we use velocity templates to generate out unit tests had me thinking that clover would not work well. I would like to be pleasantly surprised.","01/May/12 03:43;ashutoshc;Here is the report I got by running it on hive trunk of last week: http://people.apache.org/~hashutosh/hive-clover/","01/May/12 13:21;appodictic;It is a nice report, as I expected I am not sure the coverage accurately depicts reality http://people.apache.org/~hashutosh/hive-clover/ql/. You would think that all the clientpositive directory would give better coverage. Still a good thing to have and improve on.","07/Dec/12 06:17;ikatsov;Apparently it makes sense to exclude auto generated and sample code from the coverage reports to obtain realistic total coverage percentage. Updated patches has been attached. ","09/Dec/12 17:44;ashutoshc;[~ikatsov] Can you link the report that you got by running clover on recent hive trunk?","13/Dec/12 07:04;ikatsov;Latest report for trunk is attached (class-level info was removed to reduce the archive size).","12/Jan/13 09:16;daisy_yu;when run “ ant clean package test clover-reports -Dclover.jar=/opt/yoo/clover-2.6.2.jar”, it meets the following exception:
/opt/trunk/build.xml:264: The following error occurred while executing this line:
/opt/trunk/build-common.xml:126: java.lang.RuntimeException: Invalid or missing License.. Please visit http://my.atlassian.com to obtain a valid license.
        at com.cenqua.clover.CloverStartup.loadLicense(CloverStartup.java:58)
        at com.cenqua.clover.CloverStartup.loadLicense(CloverStartup.java:25)
        at com.cenqua.clover.tasks.AbstractCloverTask.execute(AbstractCloverTask.java:52)
        at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
        at org.apache.tools.ant.Task.perform(Task.java:348)
        at org.apache.tools.ant.Target.execute(Target.java:392)
        at org.apache.tools.ant.Target.performTasks(Target.java:413)
        at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)
        at org.apache.tools.ant.helper.SingleCheckExecutor.executeTargets(SingleCheckExecutor.java:38)
        at org.apache.tools.ant.Project.executeTargets(Project.java:1251)
        at org.apache.tools.ant.taskdefs.Ant.execute(Ant.java:442)
        at org.apache.tools.ant.taskdefs.SubAnt.execute(SubAnt.java:303)
        at org.apache.tools.ant.taskdefs.SubAnt.execute(SubAnt.java:221)
        at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
        at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
        at org.apache.tools.ant.Task.perform(Task.java:348)
        at org.apache.tools.ant.Target.execute(Target.java:392)
        at org.apache.tools.ant.Target.performTasks(Target.java:413)
        at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1399)
        at org.apache.tools.ant.Project.executeTarget(Project.java:1368)
        at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)
        at org.apache.tools.ant.Project.executeTargets(Project.java:1251)
        at org.apache.tools.ant.Main.runBuild(Main.java:811)
        at org.apache.tools.ant.Main.startAnt(Main.java:217)
        at org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)
        at org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)

Total time: 2 seconds


The clover-2.6.x.jar needs license, and I get a try-it-free license, and add it to the Dclover.jar path. try it again, it still failed because of the above reason.

Does it have any free jar to replace the clover.jar","14/Jan/13 17:12;aklochkov;Daisy, you need to use ""clover.license.path"" JVM parameter. This is how we build:
{code}
ant clean package test clover-reports -Dtest.silent=false -Dtest.ignore.failures=true -Dtest.junit.output.format=xml -Dclover.jar=$CLOVER_HOME/lib/clover.jar -Dclover.license.path=$HOME/.clover.license
{code}","14/Jan/13 18:03;cwsteinbach;@Ashutosh: I left some comments on phabricator. Thanks.","14/Jan/13 18:04;phabricator@reviews.facebook.net;cwsteinbach has requested changes to the revision ""HIVE-2991 [jira] Integrate Clover with Hive"".

INLINE COMMENTS
  build-common.xml:112 indentation

  build-common.xml:125 indentation
  build-common.xml:118 We should exclude generated code (e.g. metastore/src/gen, metastore/src/model) from this list.
  build-common.xml:115 This should check to see if clover.license.path and clover.jar are set, and print an informative error message if they're missing.

REVISION DETAIL
  https://reviews.facebook.net/D2985

BRANCH
  svn

To: JIRA, cwsteinbach, ashutoshc
","14/Jan/13 23:59;vaidya;I'm new to Ant/Ivy based builds. Please pardon my ignorance. Could someone please explain what's the rationale behind excluding com.cenqua.clover in ivy.xml and then using -Dclover.jar to pass in the Clover JAR location? I understand -Dclover.license.path to be configurable as each user is supposed to have his/her own Clover license.

What could go wrong if we have <dependency org=""com.cenqua.clover"" name=""clover"" rev=""3.1.8""/> in the ivy.xml instead of that 'exclude?' Is this JAR not compliant/compatible with Apache License? AFAIK, maven-clover-plugin uses this same JAR and is used in other Apache Hadoop projects.","15/Jan/13 01:51;ashutoshc;I am not actively working on this one. Ashish,Ilya or any one else whoever is interested, feel free to pick it up, if interested.","16/Jan/13 08:50;ikatsov;Indentations are fixed in hive.2991.2.* patches. 

Generated code is excluded in build.xml as a part of report generation (see clover.report.filest fileset).

clover.jar property is used to enable clover, clover will be inactive if this property missed. clover.license.path is a clover's internal property and it is not mandatory if the license is placed in the clover's lib directory. If the license is missed, clover prints the error message itself.","11/Jul/13 15:44;iveselovsky;The attached patches ""HIVE-clover-xxx.patch"" are somewhat updated versions of the clovering. We used them in parallel builds.

Except clovering itself the patches introduce the following changes:
1) .q files test generator changed to split test classes by groups of tests (10 test cases) per class is the default. This is needed to avoid huge test classes -- needed for parralalized and distributed builds.
2) added ""test-lightweight"" target that allows to run a batch of tests without re-generation/re-compilation. This is badly needed in parallelized and distributed builds.
3) we introduce ""testcase-list"" parameter that allows to pass several test class names to execute. The names are to be passed in form of comma-separated list with each name in the list being in form ""**/a/b/c/TestFoo.*"". Last asterisk is needed because main project accepts .class names, while HCatalog accepts .java names.
4) + several more improvements related to clover instrumentation, reporting, etc. ","11/Jul/13 18:38;ashutoshc;[~iveselovsky] Seems like you have expanded the scope of this jira quite a bit. Your other changes (introducing targets in build system) are quite useful, but they are orthogonal to clover integration (as far as i understand). I would suggest to split the patch in three parts: one for clover integration, second for improvement in test infrastructure and third for improvements in build infra.","28/Jul/13 18:04;ashutoshc;Canceling patch for now. Breaking it down in series of patches (instead of clubbing everything together) for different issues is a good idea.",,,,,,,,,,,,,,,,,
-e -f efficiency and hive optimization rules,HIVE-4582,12648555,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,qw2qw2,qw2qw2,21/May/13 04:54,21/May/13 04:59,18/Feb/21 09:57,,,,,,,,,,0,,"I have two ways to write hive scripts:
1. Use ""hive -f "" to execute the scripts.q which have dozens of hiveQL.
2. Directly execute the scripts.sh which have dozens of hive -e ""hiveQL"".
Which way has better efficiency?
Another question, I cannot find hive optimization rules like Pig,where is hive optimization rules?",hive-0.10.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,328910,,,2013-05-21 04:54:27.0,,,,,,,"0|i1kqxr:",329252,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add an automated test for HIVE-1328,HIVE-1336,12463723,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,jvs,jvs,jvs,05/May/10 00:13,02/May/13 02:29,18/Feb/21 09:57,,0.6.0,,,,,Query Processor,,,0,,"This has to wait until we have a version of Hadoop which contains MAPREDUCE-1501.
",,,,,,,,,,,,,,,,,,,,,,HIVE-1328,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,42538,,,2010-05-05 00:13:59.0,,,,,,,"0|i0kpb3:",118917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 hive 0.9.0 error query with store as sequence file format,HIVE-4400,12644145,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,cldo,cldo,23/Apr/13 16:06,23/Apr/13 16:09,18/Feb/21 09:57,,0.9.0,,,,,,,,0,,"I create 2 table :

table page_view_1 store as text file and table page_view_2 store as squencefile. That has error when run query in table page_view_2.

""CREATE TABLE page_view_1 (viewTime INT, userid BIGINT,page_url STRING, referrer_url STRING,ip STRING COMMENT 'IP Address of the User') COMMENT 'This is the page view table' PARTITIONED BY(dt STRING, country STRING);""

And

""CREATE TABLE page_view_2 (viewTime INT, userid BIGINT,page_url STRING, referrer_url STRING,ip STRING COMMENT 'IP Address of the User') COMMENT 'This is the page view table' PARTITIONED BY(dt STRING, country STRING) STORED AS SEQUENCEFILE;""

then i run query in table page_view_2 ""select count(*) from page_view_2"" that has error

2013-04-23 10:08:52,808 FATAL org.apache.hadoop.mapred.Child: Error running child : java.lang.OutOfMemoryError: Java heap space
at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.<init>(MapTask.java:949)
at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)
at org.apache.hadoop.mapred.Child.main(Child.java:249)

But then i run query with table page_view_1 ""select count(*) from page_view_1"" that has successful.

my hadoop version is : 1.0.0 and hive version is 0.9.0 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,324512,,,2013-04-23 16:06:18.0,,,,,,,"0|i1jzjr:",324857,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve test coverage for ALTER TABLE PARTITION statements,HIVE-2394,12519361,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,cwsteinbach,cwsteinbach,19/Aug/11 01:05,19/Aug/11 01:15,18/Feb/21 09:57,,,,,,,Metastore,Query Processor,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,42138,,,Fri Aug 19 01:15:08 UTC 2011,,,,,,,"0|i0ljt3:",123859,,,,,,,,,,,,,,,,,,"19/Aug/11 01:15;cwsteinbach;It's possible to alter the properties of individual partitions using
the ALTER TABLE statement, but we currently have very little test
coverage for these scenarios.

We should add tests (both positive and negative) for the following cases:

ALTER TABLE tbl PARTITION partspec alter_op

where alter_op ==
SET FILEFORMAT
SET LOCATION
SET TBLPROPERTIES (shouldn't work?)
DROP PARTITION (shouldn't work)
ADD PARTITION (shouldn't work)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add TestRemoteJdbcDriver and TestEmbeddedJdbcDriver,HIVE-2216,12510234,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,cwsteinbach,cwsteinbach,13/Jun/11 21:36,13/Jun/11 21:36,18/Feb/21 09:57,,,,,,,JDBC,,,0,,"TestJdbcDriver currently runs in embedded mode by default, which means that the Hive regression tests don't actually cover testing the JDBC driver in remote mode. 

We should add TestRemoteJdbcDriver and TestEmbeddedJdbcDriver test suites that respectively cover the remote and embedded configuration scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,42223,,,2011-06-13 21:36:59.0,,,,,,,"0|i0lizj:",123726,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Metastore Concurrency Tests,HIVE-1710,12477276,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,cwsteinbach,cwsteinbach,13/Oct/10 20:35,14/Feb/11 21:02,18/Feb/21 09:57,,,,,,,Metastore,,,0,,"We need to provide test coverage for concurrent access to the metastore.
These tests should operate in both embedded and standalone mode.",,,,,,,,,,,,,,,,,,HIVE-1681,HIVE-1899,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,42390,,,2010-10-13 20:35:50.0,,,,,,,"0|i0kpan:",118915,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add framework for negative tests against HBase handler,HIVE-1389,12466140,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,jvs,jvs,jvs,03/Jun/10 23:05,16/Jun/10 01:59,18/Feb/21 09:57,,0.6.0,,,,,HBase Handler,,,0,,Currently only positive testcases are supported.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,42519,,,2010-06-03 23:05:08.0,,,,,,,"0|i014xj:",4568,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test coverage for ExecDriver when running tests in local mode,HIVE-1080,12446176,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,cwsteinbach,cwsteinbach,21/Jan/10 01:41,24/Feb/10 01:35,18/Feb/21 09:57,,0.6.0,,,,,Query Processor,,,0,,"Add a distributed-mode test to the test suite in order to guarantee test coverage of ExecDriver.
Filing this ticket as a follow-up to HIVE-1064.

",,,,,,,,,,,,,,,,,,HIVE-1064,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,42625,,,2010-01-21 01:41:12.0,,,,,,,"0|i0lda7:",122802,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
udf tests should include a describe and describe extended,HIVE-739,12432541,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Major,,,namit,namit,07/Aug/09 16:22,20/Aug/09 17:31,18/Feb/21 09:57,,,,,,,Testing Infrastructure,,,0,,"
There are a lot of udf tests - 

[njain@dev029 clientpositive]$ ls -lrt *udf* 
-rw-r--r--  1 njain users  975 May 27 17:58 udf_when.q
-rw-r--r--  1 njain users  382 May 27 17:58 udf_unix_timestamp.q
-rw-r--r--  1 njain users 1919 May 27 17:58 udf_round.q
-rw-r--r--  1 njain users  144 May 27 17:58 udf_lower.q
-rw-r--r--  1 njain users 1814 May 27 17:58 udf_json.q
-rw-r--r--  1 njain users  555 May 27 17:58 udf_hash.q
-rw-r--r--  1 njain users  937 May 27 17:58 udf_case_thrift.q
-rw-r--r--  1 njain users  943 May 27 17:58 udf_case.q
-rw-r--r--  1 njain users  331 May 27 17:58 udf_case_column_pruning.q
-rw-r--r--  1 njain users 1239 May 27 17:58 udf9.q
-rw-r--r--  1 njain users  293 May 27 17:58 udf8.q
-rw-r--r--  1 njain users  946 May 27 17:58 udf7.q
-rw-r--r--  1 njain users  592 May 27 17:58 udf5.q
-rw-r--r--  1 njain users  550 May 27 17:58 udf4.q
-rw-r--r--  1 njain users  448 May 27 17:58 udf3.q
-rw-r--r--  1 njain users  312 May 27 17:58 udf2.q
-rw-r--r--  1 njain users 1400 May 27 17:58 udf1.q
-rw-r--r--  1 njain users  332 May 27 17:58 udf_10_trims.q
-rw-r--r--  1 njain users  607 May 27 17:58 ppd_udf_case.q
-rw-r--r--  1 njain users 1463 Jun 11 15:08 udf_coalesce.q
-rw-r--r--  1 njain users 1726 Jun 18 16:20 udf_parse_url.q
-rw-r--r--  1 njain users  482 Jun 19 15:09 udf_size.q
-rw-r--r--  1 njain users  577 Jun 19 15:09 udf_like.q
-rw-r--r--  1 njain users  724 Jun 19 15:09 udf_isnull_isnotnull.q
-rw-r--r--  1 njain users  944 Jun 19 15:09 udf6.q
-rw-r--r--  1 njain users  994 Jul  7 09:53 udf_if.q
-rw-r--r--  1 njain users  470 Jul 14 11:27 udf_length.q
-rw-r--r--  1 njain users  558 Jul 15 14:34 udf_reverse.q
-rw-r--r--  1 njain users 1257 Jul 15 22:23 udf_locate.q
-rw-r--r--  1 njain users  921 Jul 15 22:23 udf_instr.q
-rw-r--r--  1 njain users  444 Jul 17 16:03 udf_hex.q
-rw-r--r--  1 njain users 1606 Jul 17 16:03 udf_conv.q
-rw-r--r--  1 njain users  163 Jul 17 16:03 udf_bin.q
-rw-r--r--  1 njain users  925 Jul 23 11:54 udf_elt.q
-rw-r--r--  1 njain users   93 Jul 23 13:39 udf_sin.q
-rw-r--r--  1 njain users   93 Jul 23 13:39 udf_cos.q
-rw-r--r--  1 njain users  153 Jul 23 13:39 udf_asin.q
-rw-r--r--  1 njain users  153 Jul 23 13:39 udf_acos.q
-rw-r--r--  1 njain users 1930 Jul 23 13:50 udf_substr.q
-rw-r--r--  1 njain users  257 Jul 23 13:50 udf_split.q
-rw-r--r--  1 njain users  318 Jul 23 13:50 udf_space.q
-rw-r--r--  1 njain users  225 Jul 23 13:50 udf_repeat.q
-rw-r--r--  1 njain users  387 Jul 23 13:50 udf_lpad_rpad.q
-rw-r--r--  1 njain users  153 Jul 23 13:50 udf_ascii.q
-rw-r--r--  1 njain users  397 Jul 23 13:50 udf_abs.q
-rw-r--r--  1 njain users  732 Jul 28 18:04 create_genericudf.q
-rw-r--r--  1 njain users  199 Jul 28 18:04 drop_udf.q
-rw-r--r--  1 njain users  353 Jul 28 18:05 udf_testlength2.q
-rw-r--r--  1 njain users  347 Jul 28 18:05 udf_testlength.q
-rw-r--r--  1 njain users  179 Jul 29 08:26 udf_pmod.q


They should be enhanced to include a describe/describe extended of the udf under consideration",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,613,,,Thu Aug 20 17:31:29 UTC 2009,,,,,,,"0|i0lbhj:",122511,,,,,,,,,,,,,,,,,,"20/Aug/09 17:31;namit;Since we dont support a describe functions *, it will be good to change show_functions.q and add a describe 
and describe extended for all the functions there.

Add a comment there which says that whenever the output of show functions change, add a describe also - it might be easier to remember that way
for now.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
results_cache_invalidation2.q is flaky,HIVE-24594,13350742,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,vihangk1,vihangk1,vihangk1,06/Jan/21 19:59,06/Jan/21 20:30,18/Feb/21 09:57,,,,,,,,,,0,pull-request-available,"results_cache_invalidation2.q failed for me couple of times on a unrelated PR. Here is the error log.

{noformat}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hive.cli.split19.TestMiniLlapLocalCliDriver
-------------------------------------------------------------------------------
Tests run: 90, Failures: 1, Errors: 0, Skipped: 6, Time elapsed: 450.54 s <<< FAILURE! - in org.apache.hadoop.hive.cli.split19.TestMiniLlapLocalCliDriver
org.apache.hadoop.hive.cli.split19.TestMiniLlapLocalCliDriver.testCliDriver[results_cache_invalidation2]  Time elapsed: 15.087 s  <<< FAILURE!
java.lang.AssertionError:
Client Execution succeeded but contained differences (error code = 1) after executing results_cache_invalidation2.q ^M
266a267
> #### A masked pattern was here ####
271a273
> #### A masked pattern was here ####
273c275,276
<   Stage-0 is a root stage
---
>   Stage-1 is a root stage
>   Stage-0 depends on stages: Stage-1
275a279,365
>   Stage: Stage-1
>     Tez
> #### A masked pattern was here ####
>       Edges:
>         Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 4 (SIMPLE_EDGE)
>         Reducer 3 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)
> #### A masked pattern was here ####
>       Vertices:
>         Map 1
>             Map Operator Tree:
>                 TableScan
>                   alias: tab1
>                   filterExpr: key is not null (type: boolean)
>                   Statistics: Num rows: 1500 Data size: 130500 Basic stats: COMPLETE Column stats: COMPLETE
>                   Filter Operator
>                     predicate: key is not null (type: boolean)
>                     Statistics: Num rows: 1500 Data size: 130500 Basic stats: COMPLETE Column stats: COMPLETE
>                     Select Operator
>                       expressions: key (type: string)
>                       outputColumnNames: _col0
>                       Statistics: Num rows: 1500 Data size: 130500 Basic stats: COMPLETE Column stats: COMPLETE
>                       Reduce Output Operator
>                         key expressions: _col0 (type: string)
>                         null sort order: z
>                         sort order: +
>                         Map-reduce partition columns: _col0 (type: string)
>                         Statistics: Num rows: 1500 Data size: 130500 Basic stats: COMPLETE Column stats: COMPLETE
>             Execution mode: vectorized, llap
>             LLAP IO: all inputs
>         Map 4
>             Map Operator Tree:
>                 TableScan
>                   alias: tab2
>                   filterExpr: key is not null (type: boolean)
>                   Statistics: Num rows: 500 Data size: 43500 Basic stats: COMPLETE Column stats: COMPLETE
>                   Fil^M
{noformat}

The test works for me locally. In fact the same PR had a successful run of this test in a previous commit. I think we should disable this and re-enable it after fixing the flakiness.",,"vihangk1 opened a new pull request #1837:
URL: https://github.com/apache/hive/pull/1837


   The PR disables results_cache_invalidation2.q which fails intermittently on unrelated PRs.
   
   ### What changes were proposed in this pull request?
   Disable TestMiniLlapLocalCliDriver[results_cache_invalidation2.q] due to flakiness.
   
   
   ### Why are the changes needed?
   Reduce unrelated intermittent errors on the PRs to improve dev productivity.
   
   
   ### Does this PR introduce _any_ user-facing change?
   No
   
   
   ### How was this patch tested?
   Ran the split locally and made sure that the test is not run.
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Jan/21 20:30;githubbot;600",,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-01-06 19:59:15.0,,,,,,,"0|z0mbtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deflake TestCachedStoreUpdateUsingEvents,HIVE-24561,13347424,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,vihangk1,vihangk1,vihangk1,22/Dec/20 19:58,23/Dec/20 17:59,18/Feb/21 09:57,,,,,,,,,,0,pull-request-available,"TestCachedStoreUpdateUsingEvents seems to use ""file:/tmp"" as the table and database directory. The cleanUp method will clean all the sub-directories directories in /tmp which can be error prone.

Also noticed that I see a lot NPEs from {{SharedCache#getMemorySizeEstimator}} because the {{sizeEstimators}} field is null. We should add a null check for that field.",,"vihangk1 opened a new pull request #1808:
URL: https://github.com/apache/hive/pull/1808


   ### What changes were proposed in this pull request?
   This PR modifies the test TestCachedStoreUpdateUsingEvents to not use ""file:/tmp"" which is error prone since clean up method can delete all the files in /tmp. Also fixes a harmless NPE which unnecessarily spams the log file.
   
   ### Why are the changes needed?
   I observed this test to be failing on a unrelated PR. I am hoping that these changes will help remove any flakiness if present.
   
   
   ### Does this PR introduce _any_ user-facing change?
   No
   
   ### How was this patch tested?
   Ran the test locally.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Dec/20 20:02;githubbot;600","kishendas commented on a change in pull request #1808:
URL: https://github.com/apache/hive/pull/1808#discussion_r547501638



##########
File path: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/cache/TestCachedStoreUpdateUsingEvents.java
##########
@@ -82,9 +84,9 @@ public void setUp() throws Exception {
     HiveMetaStore.HMSHandler.createDefaultCatalog(rawStore, new Warehouse(conf));
   }
 
-  private Database createTestDb(String dbName, String dbOwner) {
+  private Database createTestDb(String dbName, String dbOwner) throws IOException {
     String dbDescription = dbName;
-    String dbLocation = ""file:/tmp"";
+    String dbLocation = Files.createTempDirectory(dbName).toString();

Review comment:
       Even the dbName can be same across multiple tests, which can get cleaned up. Would it be a good idea to use the current timestamp or generate a random number and use that for the temporary directory ?

##########
File path: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/cache/TestCachedStoreUpdateUsingEvents.java
##########
@@ -94,8 +96,9 @@ private Database createTestDb(String dbName, String dbOwner) {
   }
 
   private Table createTestTblParam(String dbName, String tblName, String tblOwner,
-                              List<FieldSchema> cols, List<FieldSchema> ptnCols, Map<String, String> tblParams) {
-    String serdeLocation = ""file:/tmp"";
+                              List<FieldSchema> cols, List<FieldSchema> ptnCols, Map<String, String> tblParams)
+      throws IOException {
+    String serdeLocation = Files.createTempDirectory(dbName + ""_"" + tblName).toString();

Review comment:
       Even the dbName and tblName can be same across multiple tests, which can get cleaned up. Would it be a good idea to use the current timestamp or generate a random number and use that for the temporary directory ?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Dec/20 20:52;githubbot;600","kishendas commented on a change in pull request #1808:
URL: https://github.com/apache/hive/pull/1808#discussion_r547501798



##########
File path: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/cache/TestCachedStoreUpdateUsingEvents.java
##########
@@ -94,8 +96,9 @@ private Database createTestDb(String dbName, String dbOwner) {
   }
 
   private Table createTestTblParam(String dbName, String tblName, String tblOwner,
-                              List<FieldSchema> cols, List<FieldSchema> ptnCols, Map<String, String> tblParams) {
-    String serdeLocation = ""file:/tmp"";
+                              List<FieldSchema> cols, List<FieldSchema> ptnCols, Map<String, String> tblParams)
+      throws IOException {
+    String serdeLocation = Files.createTempDirectory(dbName + ""_"" + tblName).toString();

Review comment:
       Even the dbName and tblName can be same across multiple tests, which can get cleaned up. Would it be a good idea to use the current timestamp or generate a random number and prepend that to dbName and use that for the temporary directory ?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Dec/20 20:53;githubbot;600","kishendas commented on a change in pull request #1808:
URL: https://github.com/apache/hive/pull/1808#discussion_r547501638



##########
File path: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/cache/TestCachedStoreUpdateUsingEvents.java
##########
@@ -82,9 +84,9 @@ public void setUp() throws Exception {
     HiveMetaStore.HMSHandler.createDefaultCatalog(rawStore, new Warehouse(conf));
   }
 
-  private Database createTestDb(String dbName, String dbOwner) {
+  private Database createTestDb(String dbName, String dbOwner) throws IOException {
     String dbDescription = dbName;
-    String dbLocation = ""file:/tmp"";
+    String dbLocation = Files.createTempDirectory(dbName).toString();

Review comment:
       Even the dbName can be same across multiple tests, which can get cleaned up. Would it be a good idea to use the current timestamp or generate a random number and prepend that to dbName and use that for the temporary directory ?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Dec/20 20:53;githubbot;600","vihangk1 commented on a change in pull request #1808:
URL: https://github.com/apache/hive/pull/1808#discussion_r547513624



##########
File path: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/cache/TestCachedStoreUpdateUsingEvents.java
##########
@@ -82,9 +84,9 @@ public void setUp() throws Exception {
     HiveMetaStore.HMSHandler.createDefaultCatalog(rawStore, new Warehouse(conf));
   }
 
-  private Database createTestDb(String dbName, String dbOwner) {
+  private Database createTestDb(String dbName, String dbOwner) throws IOException {
     String dbDescription = dbName;
-    String dbLocation = ""file:/tmp"";
+    String dbLocation = Files.createTempDirectory(dbName).toString();

Review comment:
       yeah, the Files.createTempDirectory creates a temp directory by appending the timestamp. DbName is just used as a prefix for the directory name.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Dec/20 21:23;githubbot;600","vihangk1 commented on a change in pull request #1808:
URL: https://github.com/apache/hive/pull/1808#discussion_r547514156



##########
File path: itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/cache/TestCachedStoreUpdateUsingEvents.java
##########
@@ -94,8 +96,9 @@ private Database createTestDb(String dbName, String dbOwner) {
   }
 
   private Table createTestTblParam(String dbName, String tblName, String tblOwner,
-                              List<FieldSchema> cols, List<FieldSchema> ptnCols, Map<String, String> tblParams) {
-    String serdeLocation = ""file:/tmp"";
+                              List<FieldSchema> cols, List<FieldSchema> ptnCols, Map<String, String> tblParams)
+      throws IOException {
+    String serdeLocation = Files.createTempDirectory(dbName + ""_"" + tblName).toString();

Review comment:
       same comment as above. Based on the documentation of Files.createTempDirectory it returns ""the path to the newly created directory that did not exist before this method was invoked""




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Dec/20 21:24;githubbot;600","vihangk1 merged pull request #1808:
URL: https://github.com/apache/hive/pull/1808


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Dec/20 17:59;githubbot;600",,0,4200,,,0,4200,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-12-22 19:58:32.0,,,,,,,"0|z0lr60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deflake TestHivePrivilegeObjectOwnerNameAndType,HIVE-24562,13347426,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,vihangk1,vihangk1,vihangk1,22/Dec/20 20:23,22/Dec/20 22:55,18/Feb/21 09:57,,,,,,,,,,0,pull-request-available,"One of my unrelated PRs fails this test {{TestHivePrivilegeObjectOwnerNameAndType}}. The exception which I see in the logs is below:

{noformat}
Caused by: ERROR 42X05: Table/View 'TXN_LOCK_TBL' does not exist.
        at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
        at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
        at org.apache.derby.impl.sql.compile.LockTableNode.bindStatement(Unknown Source)
        at org.apache.derby.impl.sql.GenericStatement.prepMinion(Unknown Source)
        at org.apache.derby.impl.sql.GenericStatement.prepare(Unknown Source)
        at org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext.prepareInternalStatement(Unknown Source)
        ... 73 more
)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:651)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:8301)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108)
        at com.sun.proxy.$Proxy46.open_txns(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.openTxnsIntr(HiveMetaStoreClient.java:3634)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.openTxn(HiveMetaStoreClient.java:3595)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:218)
        at com.sun.proxy.$Proxy47.openTxn(Unknown Source)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.openTxn(DbTxnManager.java:243)
        at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.openTxn(DbTxnManager.java:227)
        at org.apache.hadoop.hive.ql.Compiler.openTransaction(Compiler.java:268)
        at org.apache.hadoop.hive.ql.Compiler.analyze(Compiler.java:215)

        at org.apache.hadoop.hive.ql.Compiler.compile(Compiler.java:104)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:492)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:445)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:178)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:150)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:137)
        at org.apache.hadoop.hive.ql.security.authorization.plugin.TestHivePrivilegeObjectOwnerNameAndType.runCmd(TestHivePrivilegeObjectOwnerNameAndType.java:86)
        at org.apache.hadoop.hive.ql.security.authorization.plugin.TestHivePrivilegeObjectOwnerNameAndType.beforeTest(TestHivePrivilegeObjectOwnerNameAndType.java:82)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
        at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
        at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:43)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
        at java.util.Iterator.forEachRemaining(Iterator.java:116)
        at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
        at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
        at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:82)
        at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:73)
        at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:248)
        at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$5(DefaultLauncher.java:211)
        at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:226)
        at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:199)
        at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:132)
        at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:188)
        at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
        at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
        at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
        at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)

{noformat} ",,"vihangk1 opened a new pull request #1809:
URL: https://github.com/apache/hive/pull/1809


   ### What changes were proposed in this pull request?
   This change modifies the test TestHivePrivilegeObjectOwnerNameAndType in the hope that it will reduce the intermittent failures that I saw on my unrelated PR. The full trace of the failure is available in the JIRA.
   
   ### Why are the changes needed?
   Deflake the test.
   
   
   ### Does this PR introduce _any_ user-facing change?
   No
   
   
   ### How was this patch tested?
   Ran the modified test locally.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Dec/20 20:27;githubbot;600","vihangk1 commented on pull request #1809:
URL: https://github.com/apache/hive/pull/1809#issuecomment-749792041


   > Why the test sometimes pass without the fix?
   
   Yeah, I am not sure about this. The exception trace that I see suggests that this should fail every time since datanucleus should automatically create the tables. 


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Dec/20 21:49;githubbot;600","vihangk1 merged pull request #1809:
URL: https://github.com/apache/hive/pull/1809


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Dec/20 22:55;githubbot;600",,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 22 20:24:23 UTC 2020,,,,,,,"0|z0lr6g:",9223372036854775807,,,,,,,,,,,,,,,,,,"22/Dec/20 20:24;vihangk1;I believe we should run {{TestTxnDbUtil.prepDb(conf);}} in the test setup method since it creates the transactional schema. I have seen this to be used in other tests like {{TestHiveAuthorizerCheckInvocation}} as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add randomized tests to TestArrowColumnarBatchSerDe,HIVE-20445,13180648,Test,Patch Available,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,teddy.choi,teddy.choi,teddy.choi,23/Aug/18 07:07,16/Jun/20 16:52,18/Feb/21 09:57,,,,,,,,,,0,pull-request-available,Use random schemas/data for TestArrowColumnarBatchSerDe to improve coverage of arrow conversion/serialization.,,"github-actions[bot] commented on pull request #423:
URL: https://github.com/apache/hive/pull/423#issuecomment-641144550


   This pull request has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.
   Feel free to reach out on the dev@hive.apache.org list if the patch is in need of reviews.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jun/20 16:38;githubbot;600","github-actions[bot] closed pull request #423:
URL: https://github.com/apache/hive/pull/423


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jun/20 16:52;githubbot;600",,,,,,,0,1200,,,0,1200,,,,,,,,"28/Aug/18 06:47;teddy.choi;HIVE-20445.1.patch;https://issues.apache.org/jira/secure/attachment/12937388/HIVE-20445.1.patch","03/Sep/18 00:53;teddy.choi;HIVE-20445.2.patch;https://issues.apache.org/jira/secure/attachment/12938059/HIVE-20445.2.patch","29/Aug/18 09:37;teddy.choi;HIVE-20445.2.patch;https://issues.apache.org/jira/secure/attachment/12937599/HIVE-20445.2.patch","21/Sep/18 09:45;teddy.choi;HIVE-20445.3.patch;https://issues.apache.org/jira/secure/attachment/12940759/HIVE-20445.3.patch","14/Sep/18 08:22;teddy.choi;HIVE-20445.3.patch;https://issues.apache.org/jira/secure/attachment/12939679/HIVE-20445.3.patch",,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,2018-08-28 06:48:26.047,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 28 15:27:25 UTC 2018,,,,,,,"0|i3xc5b:",9223372036854775807,,,,,,,,,,,,,,,,,,"28/Aug/18 06:48;githubbot;GitHub user pudidic opened a pull request:

    https://github.com/apache/hive/pull/423

    HIVE-20445: Add randomized tests to TestArrowColumnarBatchSerDe

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/pudidic/hive HIVE-20445

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/hive/pull/423.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #423
    
----
commit a4ef8df46a5b881dde9895aab7c60ff24185b10a
Author: Teddy Choi <pudidic@...>
Date:   2018-08-24T04:21:35Z

    Working

commit f5b7b4b7f972bae2e6d1cbb7e029b96018da973e
Author: Teddy Choi <pudidic@...>
Date:   2018-08-27T03:58:58Z

    Merge branch 'master' into HIVE-20445

commit 71d28a3d0d2ff827046c19eab2e8e5c37c08e107
Author: Teddy Choi <pudidic@...>
Date:   2018-08-28T02:36:03Z

    Working

commit c68fea097a6791819d7907708f6403943a5dd9d8
Author: Teddy Choi <pudidic@...>
Date:   2018-08-28T02:36:45Z

    Merge branch 'master' into HIVE-20445

commit afdc0fd4c0bb1fa33604ac2aecf882ff15e65609
Author: Teddy Choi <pudidic@...>
Date:   2018-08-28T06:33:43Z

    Working

commit 2e084cb455d225a8e728edce9a7b9b0212430e3b
Author: Teddy Choi <pudidic@...>
Date:   2018-08-28T06:39:17Z

    Removed unnecessary code

----
","28/Aug/18 09:35;hiveqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  8m 40s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  5s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 49s{color} | {color:green} master passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  4m  5s{color} | {color:blue} ql in master has 2310 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  0s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 30s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m  8s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 50s{color} | {color:red} ql: The patch generated 55 new + 1155 unchanged - 76 fixed = 1210 total (was 1231) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 58s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 14s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 25m  9s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-13502/dev-support/hive-personality.sh |
| git revision | master / 004ec3c |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-13502/yetus/diff-checkstyle-ql.txt |
| modules | C: ql U: ql |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-13502/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","28/Aug/18 10:08;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12937388/HIVE-20445.1.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 14892 tests executed
*Failed tests:*
{noformat}
org.apache.hive.jdbc.TestJdbcWithMiniLlapArrow.testDataTypes (batchId=252)
org.apache.hive.jdbc.TestJdbcWithMiniLlapVectorArrow.testDataTypes (batchId=252)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/13502/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/13502/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-13502/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12937388 - PreCommit-HIVE-Build","03/Sep/18 01:44;hiveqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  8m 44s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  8s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 49s{color} | {color:green} master passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  3m 58s{color} | {color:blue} ql in master has 2310 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  1s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 27s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  8s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m  8s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 49s{color} | {color:red} ql: The patch generated 64 new + 1137 unchanged - 94 fixed = 1201 total (was 1231) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m 20s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  1m  0s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 14s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 25m  4s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-13561/dev-support/hive-personality.sh |
| git revision | master / 57f40f7 |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-13561/yetus/diff-checkstyle-ql.txt |
| modules | C: ql U: ql |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-13561/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","03/Sep/18 02:17;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12938059/HIVE-20445.2.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:green}SUCCESS:{color} +1 due to 14920 tests passed

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/13561/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/13561/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-13561/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12938059 - PreCommit-HIVE-Build","15/Sep/18 08:33;hiveqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  7m 52s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 10s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 50s{color} | {color:green} master passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  3m 58s{color} | {color:blue} ql in master has 2311 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 57s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 24s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  5s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m  5s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 53s{color} | {color:red} ql: The patch generated 64 new + 1143 unchanged - 94 fixed = 1207 total (was 1237) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m 12s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 14s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 23m 56s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-13800/dev-support/hive-personality.sh |
| git revision | master / 08d9083 |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-13800/yetus/diff-checkstyle-ql.txt |
| modules | C: ql U: ql |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-13800/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","15/Sep/18 09:14;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12939679/HIVE-20445.3.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:red}ERROR:{color} -1 due to 7 failed/errored test(s), 14941 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestMiniDruidCliDriver.testCliDriver[druidmini_dynamic_partition] (batchId=193)
org.apache.hadoop.hive.cli.TestMiniDruidCliDriver.testCliDriver[druidmini_expressions] (batchId=193)
org.apache.hadoop.hive.cli.TestMiniDruidCliDriver.testCliDriver[druidmini_test1] (batchId=193)
org.apache.hadoop.hive.cli.TestMiniDruidCliDriver.testCliDriver[druidmini_test_alter] (batchId=193)
org.apache.hadoop.hive.cli.TestMiniDruidCliDriver.testCliDriver[druidmini_test_insert] (batchId=193)
org.apache.hive.jdbc.miniHS2.TestHs2ConnectionMetricsHttp.testOpenConnectionMetrics (batchId=255)
org.apache.hive.service.auth.TestCustomAuthentication.org.apache.hive.service.auth.TestCustomAuthentication (batchId=247)
{noformat}

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/13800/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/13800/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-13800/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 7 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12939679 - PreCommit-HIVE-Build","21/Sep/18 09:46;teddy.choi;The third patch doesn't produce any errors now on my laptop. I uploaded it again to re-initiate the test.","22/Sep/18 18:23;hiveqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  8m  7s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  1s{color} | {color:green} master passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 49s{color} | {color:green} master passed {color} |
| {color:blue}0{color} | {color:blue} findbugs {color} | {color:blue}  3m 55s{color} | {color:blue} ql in master has 2326 extant Findbugs warnings. {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 55s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m 23s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  2s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  1m  2s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red}  0m 48s{color} | {color:red} ql: The patch generated 63 new + 1144 unchanged - 94 fixed = 1207 total (was 1238) {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  4m  0s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 55s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 13s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 23m 37s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Optional Tests |  asflicense  javac  javadoc  findbugs  checkstyle  compile  |
| uname | Linux hiveptest-server-upstream 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u1 (2016-09-03) x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /data/hiveptest/working/yetus_PreCommit-HIVE-Build-13978/dev-support/hive-personality.sh |
| git revision | master / cdba00c |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| checkstyle | http://104.198.109.242/logs//PreCommit-HIVE-Build-13978/yetus/diff-checkstyle-ql.txt |
| modules | C: ql U: ql |
| Console output | http://104.198.109.242/logs//PreCommit-HIVE-Build-13978/yetus.txt |
| Powered by | Apache Yetus    http://yetus.apache.org |


This message was automatically generated.

","22/Sep/18 19:06;hiveqa;

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12940759/HIVE-20445.3.patch

{color:green}SUCCESS:{color} +1 due to 1 test(s) being added or modified.

{color:green}SUCCESS:{color} +1 due to 14994 tests passed

Test results: https://builds.apache.org/job/PreCommit-HIVE-Build/13978/testReport
Console output: https://builds.apache.org/job/PreCommit-HIVE-Build/13978/console
Test logs: http://104.198.109.242/logs/PreCommit-HIVE-Build-13978/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.TestCheckPhase
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.YetusPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12940759 - PreCommit-HIVE-Build","28/Sep/18 15:27;mmccline;+1 LGTM",,,,,,,,,,,,,,,,,,,,,,,
Fix TestSchemaToolCatalogOps,HIVE-21168,13211919,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,,vihangk1,vihangk1,25/Jan/19 19:47,03/May/19 18:58,18/Feb/21 09:57,,3.2.0,,,,,,,,0,,HIVE-21077 causes TestSchemaToolCatalogOps to fail on branch-3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 28 21:58:28 UTC 2019,,,,,,,"0|yi0bwg:",9223372036854775807,,,,,,,,,,,,,,,,,,"28/Jan/19 21:58;vihangk1;Looked into this and its related to HIVE-21128. Since the hiveversion is still 3.1 in the pom.xml, schemaTool is using 3.1 schema init scripts which do not have the {{CREATE_TIME}} column added in HIVE-21077",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test : TestMiniSparkOnYarnCliDriver.testCliDriver[truncate_column_buckets],HIVE-21617,13228228,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,,vihangk1,vihangk1,16/Apr/19 01:41,16/Apr/19 01:41,18/Feb/21 09:57,,,,,,,,,,0,,We should disable this test. Was seen in https://builds.apache.org/job/PreCommit-HIVE-Build/16961/testReport/,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-04-16 01:41:10.0,,,,,,,"0|z01tg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Requesting LISA access,HIVE-21276,13216009,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,,raquelmorales,raquelmorales,15/Feb/19 15:06,15/Feb/19 15:06,18/Feb/21 09:57,,2.3.4,,,,,Authentication,,27/Feb/19 00:00,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2019-02-15 15:06:37.0,,,,,,,"0|yi1114:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix or disable TestRpc.testClientTimeout,HIVE-20799,13193962,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,,vihangk1,vihangk1,24/Oct/18 20:00,24/Oct/18 20:00,18/Feb/21 09:57,,,,,,,,,,0,,Test failed without any code changes on master. See HIVE-20798,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-10-24 20:00:50.0,,,,,,,"0|i3zlpz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Tests for HIVE-12812,HIVE-20642,13187760,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,afan,afan,afan,26/Sep/18 23:49,26/Sep/18 23:49,18/Feb/21 09:57,,4.0.0,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-09-26 23:49:41.0,,,,,,,"0|i3yjof:",9223372036854775807,,,,,,,,,,,,,4.0.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metastore tests failing,HIVE-18719,13138693,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,,vihangk1,vihangk1,15/Feb/18 02:46,15/Feb/18 08:43,18/Feb/21 09:57,,,,,,,Metastore,,,0,,"Some of the recently added metastore tests are failing regularly. Possibly because the socket is in use.

Here is one such run: https://builds.apache.org/job/PreCommit-HIVE-Build/9218/

org.apache.hadoop.hive.metastore.client.TestFunctions.testGetFunctionNullDatabase[Embedded] (batchId=205)
org.apache.hadoop.hive.metastore.client.TestTablesGetExists.testGetAllTablesCaseInsensitive[Embedded] (batchId=205)
org.apache.hadoop.hive.metastore.client.TestTablesList.testListTableNamesByFilterNullDatabase[Embedded] (batchId=205)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-02-15 08:43:20.896,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 15 08:43:20 UTC 2018,,,,,,,"0|i3q7pj:",9223372036854775807,,,,,,,,,,,,,,,,,,"15/Feb/18 02:46;vihangk1;cc: [~pvary] [~szita]","15/Feb/18 08:43;pvary;[~vihangk1]: What do you mean by ""Possibly because the socket is in use.""?

 

We suspect some derby issue here, but probably wrong.

Thanks,

Peter",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestSparkCliDriver.testCliDriver[vectorized_ptf] failing on branch-2,HIVE-18103,13119524,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,vihangk1,vihangk1,vihangk1,19/Nov/17 20:45,19/Nov/17 20:45,18/Feb/21 09:57,,,,,,,,,,0,,TestSparkCliDriver.testCliDriver[vectorized_ptf.q] and TestSparkCliDriver.testCliDriver[vectorization_7.q] are failing on branch-2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-11-19 20:45:47.0,,,,,,,"0|i3mz8n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests for utf-8 support,HIVE-9272,12765238,Test,Reopened,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,asreekumar,asreekumar,asreekumar,06/Jan/15 22:49,06/Jul/15 21:17,18/Feb/21 09:57,,0.14.0,,,,,Tests,WebHCat,,0,,"Including some test cases for utf8 support in webhcat. The first four tests invoke hive, pig, mapred and streaming apis for testing the utf8 support for data processed, file names and job name. The last test case tests the filtering of job name with utf8 character",,,,,,,,,,,,,,,,,,,,,,,"08/Jan/15 03:03;asreekumar;HIVE-9272.1.patch;https://issues.apache.org/jira/secure/attachment/12690705/HIVE-9272.1.patch","14/Jan/15 18:13;asreekumar;HIVE-9272.2.patch;https://issues.apache.org/jira/secure/attachment/12692284/HIVE-9272.2.patch","15/Jan/15 23:11;asreekumar;HIVE-9272.3.patch;https://issues.apache.org/jira/secure/attachment/12692622/HIVE-9272.3.patch","20/Jan/15 19:00;ekoifman;HIVE-9272.4.patch;https://issues.apache.org/jira/secure/attachment/12693353/HIVE-9272.4.patch","26/Mar/15 21:40;asreekumar;HIVE-9272.5.patch;https://issues.apache.org/jira/secure/attachment/12707608/HIVE-9272.5.patch","27/Mar/15 21:06;asreekumar;HIVE-9272.6.patch;https://issues.apache.org/jira/secure/attachment/12707871/HIVE-9272.6.patch","21/Apr/15 02:10;asreekumar;HIVE-9272.7.patch;https://issues.apache.org/jira/secure/attachment/12726756/HIVE-9272.7.patch","01/May/15 21:04;asreekumar;HIVE-9272.8.patch;https://issues.apache.org/jira/secure/attachment/12729844/HIVE-9272.8.patch","06/Jul/15 21:16;asreekumar;HIVE-9272.9.patch;https://issues.apache.org/jira/secure/attachment/12743801/HIVE-9272.9.patch","06/Jan/15 22:51;asreekumar;HIVE-9272.patch;https://issues.apache.org/jira/secure/attachment/12690437/HIVE-9272.patch",,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,2015-01-09 01:41:59.604,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 06 21:17:48 UTC 2015,,,,,,,"0|i240dr:",9223372036854775807,,,,,,,,,,,,,,,,,,"06/Jan/15 22:51;asreekumar;Please review this patch with the test cases for utf8 support","08/Jan/15 03:03;asreekumar;Please find the updated patch for utf-8 tests for review.","09/Jan/15 01:41;ekoifman;1. could you add a comment to the change in deploy_e2e_artifacts.sh to indicate what set of tests the artifact is for?
2. I'm not sure how you are generating the patch, but it has some headers that may not apply.  I usually use ""git diff --no-prefix SHA SHA1 > foo.patch""
3. my knowledge of Perl is very limited.  Is there someone else who can review .pm changes?","13/Jan/15 23:20;sushanth;The .pm changes look reasonable to me. It's basically checking type of argument(s), and appropriately utf8-decoding them, so that they may be compared for equivalence.

I had some difficulty using git apply with this patch, but looks like standard unix patch with a ""patch -p1"" itself works. Since this is a patch that purely affects e2e tests, there are no expected changes to the unit test tree itself, but I'm still going to go ahead and set this as patch-available and let the build run, in case the presence of unicode files gives some of the build tools a hiccup. If all of that passes, I can go ahead and commit it.

Also, [~asreekumar], could you respond to Eugene's first question in the meanwhile?","14/Jan/15 18:12;asreekumar;Thanks for the comments Eugene and Sushanth. I am uploading a new patch with comment added in deploy_e2e_artifacts.sh mentioning the purpose of new jar file moved to HDFS. Please review.","15/Jan/15 11:38;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12692284/HIVE-9272.2.patch

{color:red}ERROR:{color} -1 due to 1 failed/errored test(s), 7311 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2370/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2370/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-2370/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 1 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12692284 - PreCommit-HIVE-TRUNK-Build","15/Jan/15 18:19;deepesh;We should ignore the above failures.
Add the flag to skip pre-commit tests as these are changes in the E2E test suite that doesn't get run as part of pre-commit tests.","15/Jan/15 21:26;ekoifman;+1","15/Jan/15 21:39;ekoifman;sorry, I think the +1 was premature

after applying this patch on trunk and running
mvn clean package install -Phadoop-2,dist -DskipTests

I end up with an error

[INFO] Hive Packaging ..................................... FAILURE [ 23.908 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 03:46 min
[INFO] Finished at: 2015-01-15T13:35:44-08:00
[INFO] Final Memory: 187M/1055M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.3:single (assemble) on project hive-packaging: Failed to create assembly: Error creating assembly archive src: hcatalog/src/test/e2e/templeton/inpdir/artof\344\266\264\343\204\251\351\274\276\344\270\204\347\213\234\343\200\207war.txt"" not found. -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-packaging
ekoifman:svnhive ekoifman$ 
","15/Jan/15 23:11;asreekumar;Attaching the patch with encoded characters replaced with original characters in file name. Please review the same.","16/Jan/15 23:14;ekoifman;resubmitting the patch - for some reason the buildbot is not picking it up
","17/Jan/15 00:09;sushanth;Cancelling and resubmitting after removing the sigil from the description that prevents the buildbot from picking it up.","20/Jan/15 19:00;ekoifman;patch 4 is the same as 3.  For some reason the build bot is picking this up","20/Jan/15 22:07;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12693353/HIVE-9272.4.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 7333 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby_grouping_window
org.apache.hadoop.hive.ql.TestMTQueries.testMTQueries1
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2443/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2443/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-2443/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12693353 - PreCommit-HIVE-TRUNK-Build","20/Jan/15 23:28;ekoifman;patch 4 committed to trunk.
Thanks [~asreekumar] for the contribution and [~sushanth] for the review","20/Jan/15 23:31;asreekumar;Thanks [~ekoifman] and [~sushanth] for review comments and verification.","21/Jan/15 19:13;ekoifman;dev@hive.apache.org has a thread titled ""Precommit test error with utf8""","26/Mar/15 21:47;asreekumar;Please find attached a patch with filenames with utf-8 character removed. This will avoid the svn checkout issue. The files need to be renamed with utf-8 characters before running tests (included instructions for that in readme.txt). Appreciate reviews for this patch.","27/Mar/15 20:06;ekoifman;[~asreekumar], renaming files after checkout is a good idea, but this step needs to be automated just like renaming of the jar is automated.  Generally, we want to try to minimize manual steps as much as possible.","27/Mar/15 21:10;asreekumar;[~ekoifman] Please find attached the patch with renaming of input files automated (in deploy_e2e_artifacts.sh). I was a little hesitant to add this initially as this looked like the only local file operation, but certainly will come in handy for anyone running the the test suite.","20/Apr/15 21:14;ekoifman;[~asreekumar], I think your change in #3 in README.txn is not needed since this is now automated

there is a test.sh file in the patch, what is this for?

wrt the change in deploy_e2e_artifacts.sh - this will fail if run more than once as ""mv"" removes the file which is under source control.  I would at least change it to ""cp"".  
Also, related to this piece, is it possible to mv/cp to some location not under source control?
For example, when you run the ""ant test"" you'll notice that a new testdist/ is created to which all (most) files are copied to (e.g. tests/).  Perhaps you could do the same for inpdir/ and rename files to use UTF8 there?  This is not critical but would be nice.","21/Apr/15 02:09;asreekumar;[~ekoifman] Please find attached the patch with the above suggestions included.","29/Apr/15 19:38;ekoifman;Hive_UTF8 is failing.  I think because ""row"" is a reserved keyword.
Also I'm seeing messages like “Wide character in print at TestDriverCurl.pm line 563”  (and line 901) - not sure what the significance of this is","01/May/15 21:04;asreekumar;[~ekoifman] Thanks for catching that. I changed the column name. Please review the patch attached.","06/May/15 19:55;ekoifman;[~asreekumar], 
I see errors from restart_hive_redeploy_artifacts.sh 
and the new tests fail.   
cp: /Users/ekoifman/dev/hiverwgit/hcatalog/src/test/e2e/templeton/deployers/inpdir/artofwar.txt: No such file or directory
cp: /Users/ekoifman/dev/hiverwgit/hcatalog/src/test/e2e/templeton/deployers/inpdir/PigJoinwork.pig: No such file or directory
15/05/06 12:21:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
put: `test_utf8_inpdir/*': No such file or directory","06/Jul/15 21:17;asreekumar;[~ekoifman] Could you please review the attached patch and see if it solves the issue",,,,,,,,
Add code coverage for HiveAuthFactory.java in itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHiveAuthFactory.java,HIVE-10139,12786758,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,,aihuaxu,aihuaxu,30/Mar/15 15:51,30/Mar/15 15:51,18/Feb/21 09:57,,,,,,,,,,0,,We have recently seen some issues in HiveAuthFactory. It would be nice to add unit tests so that we can capture possible issues.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-03-30 15:51:10.0,,,,,,,"0|i27jof:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable and address flakiness of hbase_bulk.m,HIVE-7197,12720135,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,,ndimiduk,ndimiduk,09/Jun/14 16:10,09/Jun/14 17:07,18/Feb/21 09:57,,,,,,,HBase Handler,,,0,,"There's a nice e2e test for existing bulkload workflow, but it's disabled. We should turn it on and fix what's broken.",,,,,,,,,,,,,,,,,,HIVE-6473,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,398334,,,2014-06-09 16:10:53.0,,,,,,,"0|i1wku7:",398461,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add query for vectorized_decimal_smbjoin,HIVE-6502,12697122,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,rusanu,rusanu,rusanu,25/Feb/14 13:42,25/Feb/14 13:42,18/Feb/21 09:57,,,,,,,,,,0,,The patch for HIVE-6345 did not contain a query for SMB join because decimal SMB join failed (HIVE-6412). I've tested vectorized decimal SMB and it works fine now. This issue is the check-in vehicle for regression testing .q and .q.out for it.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,375596,,,2014-02-25 13:42:05.0,,,,,,,"0|i1sq27:",375892,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"this.getJdbcTemplate().queryForInt(""select count(*) from user_info""); search",HIVE-2103,12503904,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Minor,,,jerry2018,jerry2018,11/Apr/11 08:04,11/Apr/11 08:47,18/Feb/21 09:57,,0.7.0,,,,,JDBC,,,0,hive,"use the following: this.getJdbcTemplate().queryForInt(""select count(*) from user_info"");
the hive.log contents like that:

2011-04-11 15:38:44,270 WARN  mapred.JobClient (JobClient.java:configureCommandLineOptions(539)) - Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
2011-04-11 15:39:03,455 ERROR ql.Driver (SessionState.java:printError(343)) - FAILED: Parse Error: line 0:-1 mismatched input '<EOF>' expecting FROM in from clause

org.apache.hadoop.hive.ql.parse.ParseException: line 0:-1 mismatched input '<EOF>' expecting FROM in from clause

        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:406)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:327)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:736)
        at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:116)
        at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.process(ThriftHive.java:699)
        at org.apache.hadoop.hive.service.ThriftHive$Processor.process(ThriftHive.java:677)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)


","Linux, jdk1.6.0_20, hive-0.7.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-04-11 08:47:17.815,,,false,,,,,,,,,,,,,,,,,,42270,,,Mon Apr 11 08:47:17 UTC 2011,,,,,,,"0|i0lifr:",123637,,,,,,,,,,,,,,,,,,"11/Apr/11 08:47;bennies;Does queryForLong work for you? It's what I use. This should of course be fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix TestSparkCliDriver.testCliDriver[vectorized_ptf] on branch-2,HIVE-18474,13131755,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,,,vihangk1,vihangk1,18/Jan/18 01:50,18/Jan/18 01:50,18/Feb/21 09:57,,,,,,,,,,0,,Looks like q.out file needs an update.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-01-18 01:50:27.0,,,,,,,"0|i3p1j3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test whether schema evolution using parquet is possible,HIVE-9595,12772933,Test,Patch Available,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,,,messner,messner,06/Feb/15 08:14,14/Sep/15 12:43,18/Feb/21 09:57,,0.13.1,0.14.0,1.0.0,1.1.0,,File Formats,Tests,,4,,"With https://issues.apache.org/jira/browse/HIVE-7554 a unfortunate else branch was introduced to org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport . That else branch fired an exception in case the parquet file schema had less fields than the table schema. So adding columns to a parquet backed table and query the added columns on old partitions wasn't possible anymore. Which was very unfortunate for my team. https://issues.apache.org/jira/browse/HIVE-7800 fixed that, see point 3 in Daniel Weeks' comment https://issues.apache.org/jira/browse/HIVE-7800?focusedCommentId=14158594&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14158594 .

However Hive-7800 did not introduce a test case for that. I'd love to see one.

Best,
Manuel
",,,,,,,,,,,,,,,,,,,,,,,"06/Feb/15 08:19;messner;HIVE-9595.01.patch;https://issues.apache.org/jira/secure/attachment/12696992/HIVE-9595.01.patch","06/Feb/15 08:46;messner;HIVE-9595.02.patch;https://issues.apache.org/jira/secure/attachment/12696995/HIVE-9595.02.patch",,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,2015-02-07 05:34:48.723,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 07 05:34:48 UTC 2015,,,,,,,"0|i259zj:",9223372036854775807,,,,,,,,,,,,,,,,,,"06/Feb/15 08:46;messner;I just realized that the test output seem to have changed. Adding a new patch to reflect that.","07/Feb/15 05:34;hiveqa;

{color:red}Overall{color}: -1 at least one tests failed

Here are the results of testing the latest attachment:
https://issues.apache.org/jira/secure/attachment/12696995/HIVE-9595.02.patch

{color:red}ERROR:{color} -1 due to 2 failed/errored test(s), 7483 tests executed
*Failed tests:*
{noformat}
org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_transform_acid
org.apache.hive.spark.client.TestSparkClient.testSyncRpc
{noformat}

Test results: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2693/testReport
Console output: http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/2693/console
Test logs: http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-2693/

Messages:
{noformat}
Executing org.apache.hive.ptest.execution.PrepPhase
Executing org.apache.hive.ptest.execution.ExecutionPhase
Executing org.apache.hive.ptest.execution.ReportingPhase
Tests exited with: TestsFailedException: 2 tests failed
{noformat}

This message is automatically generated.

ATTACHMENT ID: 12696995 - PreCommit-HIVE-TRUNK-Build",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup TestPassProperties changes introduced due to HIVE-8696,HIVE-10637,12827847,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,,thiruvel,thiruvel,thiruvel,06/May/15 22:47,06/May/15 23:05,18/Feb/21 09:57,,,,,,,HCatalog,Tests,,0,,Follow up JIRA to cleanup the test case as per recommendations from Sushanth.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-05-06 22:57:42.48,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 23:05:48 UTC 2015,,,,,,,"0|i2ee6f:",9223372036854775807,,,,,,,,,,,,,,,,,,"06/May/15 22:57;sushanth;InvocationTargetException..getTargetException() is apparently a dispreferred old use, so the ((InvocationTargetException)e.getCause().getCause().getCause()).getTargetException().getMessage() can be simplified to a continuous chain of getCause if not null

To wit, we could have something like this:

{code}
boolean exceptionChainContainsMessage(Exception e, String msg){
  if (e == null){ return false; }
  if (e.getMessage() != null){
    if (e.getMessage().contains(msg) { return true; }
  }
  return exceptionChainContainsMessage(e.getCause(),msg);
}
{code}

And we could put that in a common TestUtil area for use in other tests if need be, because I think this sort of chained exception testing is likely to become common.","06/May/15 23:05;thiruvel;Thanks for the suggestion, yes that's simple and will solve the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the ibatis isn't supported by hive,HIVE-2104,12503906,Test,Open,HIVE,Hive,software,ashutoshc,"Hive is a data warehouse infrastructure built on top of Hadoop that provides tools to enable easy data summarization, adhoc querying and analysis of large datasets data stored in Hadoop files. It provides a mechanism to put structure on this data and it also provides a simple query language called QL which is based on SQL and which enables users familiar with SQL to query this data.",http://hive.apache.org,Trivial,,,jerry2018,jerry2018,11/Apr/11 08:09,11/Apr/11 08:09,18/Feb/21 09:57,,0.7.0,,,,,JDBC,,,0,hive-appu,the ibatis isn't supported by hive. Is there any infomation about the ibatis and hive?,"jdk1.6.0_20,hive-0.7.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,42269,,,2011-04-11 08:09:06.0,,,,,,,"0|i0lifz:",123638,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
