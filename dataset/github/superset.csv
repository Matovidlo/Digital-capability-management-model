Issue id,Status,Summary,Issue Type,Created,Author,Resolution,Resolved,Description,Creator,Labels,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary
14043,OPEN,Unable to filter based on virtual columns,#bug,2021-04-09 07:56:42 +0000 UTC,ypankovych,Opened,,"I have a virtual dataset: 
```SQL
  (select *
   from public.invoice_invoice i
   left join public.attrs ia on ia.id = i.id) AS virtual_table
```
Then I try to filter/select based on column from virtual dataset, here's the query generated by superset:
```SQL
SELECT ""virtual_table"".""internal_0"" AS """"""virtual_table"""".""""internal_0""""""
FROM
  (select *
   from public.invoice_invoice i
   left join public.attrs ia on ia.id = i.id) AS virtual_table
WHERE invoice_date >= TO_DATE('2020-04-08', 'YYYY-MM-DD')
  AND invoice_date < TO_DATE('2021-04-08', 'YYYY-MM-DD')
LIMIT 10000;
```

### Expected results
Value from internal_0 column

### Actual results

`Error: Columns missing in datasource: ['""virtual_table"".""internal_0""']`

### Environment

(please complete the following information):

- superset version: `0.999.0dev` (FROM apache/superset:latest)
- python version: `3.8`

",,,,,,,,,,,,,,
14041,OPEN,Import datasets with dict in metrics: extra,#bug,2021-04-09 04:25:08 +0000 UTC,GGPay,Opened,,"If datasets has a dict in yaml file - metrics: extra  - that cause the unknown error when import datasets using UI.

![image](https://user-images.githubusercontent.com/17413180/114126680-89521c00-98be-11eb-93f6-fe3339e851c9.png)

### Actual results

Failed import datasets

#### Screenshots

![image](https://user-images.githubusercontent.com/17413180/114126842-cae2c700-98be-11eb-80d4-043487637ff8.png)

Server log

![image](https://user-images.githubusercontent.com/17413180/114127058-447ab500-98bf-11eb-86fa-1a4a9d48d698.png)

![image](https://user-images.githubusercontent.com/17413180/114127924-dafba600-98c0-11eb-8ed1-022c07c675e5.png)


#### How to reproduce the bug

1. Edit any Virtual datasets
2. Click on Metrics tab
3. Click on ""Add Item"" - create any new item - or even add item and click delete - it also create dict in extra field.
4. Save datasets
5. Click on Action - Export
6. Click Import  - choose zip file previously exported
7. enter ""OVERWRITE""
8. See error

### Environment
- superset version: `superset version 1.0.1`
- docker-compose-non-dev.yml

### Additional context

If change dict to null in yaml file - import works as expected.
",,,,,,,,,,,,,,
14037,OPEN,Package helm chart,question,2021-04-09 10:25:00 +0000 UTC,caleb15,Opened,,"**Is your feature request related to a problem? Please describe.**
The helm chart is not packaged, so it can't be installed directly

**Describe the solution you'd like**
The helm chart packaged so it can easily be installed via helm/helmfile

**Describe alternatives you've considered**
We copied https://github.com/apache/superset/tree/master/helm/superset into our own chart repo and packaged it ourselves.

**Additional context**
Add any other context or screenshots about the feature request here.
",,,amitmiran137,"
--
hey @caleb15 ,
could you elaborate what should be the steps for packaging the helm chart?
Do we need to add an helmFile.yaml?
what else?
--
",,,,,,,,,,
14034,OPEN,[Explore]Single metric selector is inoperable if the referenced metric no longer exists,#bug; good first issue; viz:explore:dataset,2021-04-08 22:22:40 +0000 UTC,john-bodley,Opened,,"If a metric is removed from a datasource the single metric widget is inoperable if saved slice or URL references the previously defined metric. Note this isn't a problem for visualization types which support multiple metrics.

### Expected results

The metric widget to render correctly.

### Actual results

The metric widget in inoperable (see attached screenshot).

#### Screenshots

<img width=""944"" alt=""Screen Shot 2021-04-09 at 8 24 04 AM"" src=""https://user-images.githubusercontent.com/4567245/114091733-220f8880-990d-11eb-8fce-1ad089bdf846.png"">

#### How to reproduce the bug

1. Using the example data visit the ""Number of Girls"" chart.
2. Edit the datasource and remove the `sum_num` metric.
3. Verify the metric widget is inoperable (see screenshot).

### Environment

(please complete the following information):

- superset version: `master`
- python version: `3.7`
- node.js version: `14`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,,,,,,,,,,,,
14032,OPEN,Helm chart errors if using a SemVer2 chart version,#bug,2021-04-08 18:03:03 +0000 UTC,thomasv314,Opened,,"A clear and concise description of what the bug is.

### Expected results

Helm chart deploys when the chart version uses SemVer2

### Actual results

```
 Error: release superset failed, and has been uninstalled due to atomic being set: Secret ""superset-config"" is invalid: metadata.labels: Invalid value: ""superset-2021.13.1-79+44068e96a"": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')
```

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

Set the chart version.yaml to include a `+`, i.e. `2021.13.1-79+44068e96a`

The `superset-config` resource errors out because it does not use the `superset.chart` function that all the other resources use

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,,,,,,,,,,,,
14029,OPEN,Redshift : unable to get table description or add a table as new dataset,#bug; data:connect:redshift,2021-04-09 06:31:38 +0000 UTC,squalou,Opened,,"When in SQL Lab selecting a schema, then a table in dropdown menu I get an error instead of colums list.

(Same thing happens when trying to 'add' a table in data/dataset tab)

### Expected results

See the table description on the left pane.

### Actual results

error is displayed, (see screenshot)

 and following errors appears in superset logs ;

`TypeError: _get_column_info() missing 1 required positional argument: 'generated' `

(note : running manual queries in sqllab works, and the table dropdown is displaying tables correctly)

More detailed stack : 

```python
ERROR:root:_get_column_info() missing 1 required positional argument: 'generated'
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/flask_appbuilder/api/__init__.py"", line 84, in wraps
    return f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/superset/utils/log.py"", line 70, in wrapper
    value = f(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/superset/views/base_api.py"", line 79, in wraps
    duration, response = time_function(f, self, *args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/superset/utils/core.py"", line 1311, in time_function
    response = func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/superset/databases/api.py"", line 454, in table_metadata
    table_info = get_table_metadata(database, table_name, schema_name)
  File ""/usr/local/lib/python3.6/site-packages/superset/databases/utils.py"", line 66, in get_table_metadata
    columns = database.get_columns(table_name, schema_name)
  File ""/usr/local/lib/python3.6/site-packages/superset/models/core.py"", line 618, in get_columns
    return self.db_engine_spec.get_columns(self.inspector, table_name, schema)
  File ""/usr/local/lib/python3.6/site-packages/superset/db_engine_specs/base.py"", line 692, in get_columns
    return inspector.get_columns(table_name, schema)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/reflection.py"", line 391, in get_columns
    self.bind, table_name, schema, info_cache=self.info_cache, **kw
  File ""<string>"", line 2, in get_columns
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/reflection.py"", line 52, in cache
    ret = fn(self, con, *args, **kw)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy_redshift/dialect.py"", line 409, in get_columns
    enums=[], schema=col.schema, encode=col.encode)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy_redshift/dialect.py"", line 614, in _get_column_info
    **kw
TypeError: _get_column_info() missing 1 required positional argument: 'generated'

```



#### Screenshots

![image](https://user-images.githubusercontent.com/4623644/114073000-1b4c2b80-98a3-11eb-9524-37653d014d8c.png)


#### How to reproduce the bug

1. Go to  SQL Lab
2. Click on Database, choose a REDSHIFT connexion
3. Choose the schema
3. Unfold the Tables dropdown : which is correclty populated
4. Choose a table
5. see error

### Environment

(please complete the following information):

- superset version: `Superset 0.38.0`
- python version: `Python 3.6.7`
- node.js version: does not seem present in Superset docker install

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Seems related to this bug https://github.com/OpenEnergyPlatform/oedialect/issues/35

",,,,,,,,,,,,,,
14024,OPEN,[Explore] Table not stretching,#bug; viz:chart-table,2021-04-08 22:30:38 +0000 UTC,michael-s-molina,Opened,,"https://user-images.githubusercontent.com/70410625/114032837-12336e00-9853-11eb-8509-8a14fec38690.mov

### Expected results

The table stretches to fill the available space.

### Actual results

The table DOES NOT stretch to fill the available space.

@junlincc 

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,junlincc,"
--
i assume it also happens to panel resizing? 

@ktmud Let us know if this is related to your recent change. I can't recall the old behavior. 
--
",,,,,,,,,,
14023,OPEN,AttributeError: module 'sqlalchemy_drill.drilldbapi' has no attribute 'Error',#bug,2021-04-08 12:15:47 +0000 UTC,elenamereloaxesor,Opened,,"I have already seen issues [#42](https://github.com/JohnOmernik/sqlalchemy-drill/issues/42), [#10032](https://github.com/apache/superset/issues/10032) and [#5063](https://github.com/apache/superset/issues/5063) but they don't solve my problem. 
I want to connect a GCS bucket to Superset so as to visualize the data and such. For that, I connected Drill to GCS, and then Drill to Superset, and when adding Drill as a database everything went ok. Everything is in a docker container, thus just doing docker-compose up makes Superset start. However, when i try to create a new dataset, `dfs.root`, `dfs.tmp`, `sys`, `information_schema` appear as schemas. If the first one is selected, in table_schema appears gcs, but amongst others i get the above-mentioned error. If i select `dfs.tmp`, i get drill as table_schema, but it doesn't work either. There are no problems selecting the other schemas, but gcs or my data is nowhere to be seen.

### Expected results

I expected gcs to appear as schema. 

### Actual results

`dfs.root`, `dfs.tmp`, `sys`, `information_schema` appear as schemas, but not `gcs`, and `gcs` only appears as table_schema in dfs.root, but doesn't work.

### Additional context

What the terminal produces: 
superset_app            | DEBUG:superset.stats_logger:[stats_logger] (incr) extra_table_metadata
superset_app            | DEBUG:superset.stats_logger:[stats_logger] (incr) DatabaseRestApi.table_metadata.init
superset_app            | DEBUG:superset.models.core:Database.get_sqla_engine(). Masked URL: drill+sadrill://drill:8047/gcs?use_ssl=False
superset_app            | DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): drill:8047
superset_app            | 172.29.0.1 - - [08/Apr/2021:09:47:53 +0000] ""GET /superset/extra_table_metadata/1/gcs/dfs.root/ HTTP/1.1"" 200 2 ""http://localhost:8088/superset/sqllab/"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36""
superset_app            | DEBUG:urllib3.connectionpool:http://drill:8047 ""POST /query.json HTTP/1.1"" 200 429
superset_app            | DEBUG:urllib3.connectionpool:http://drill:8047 ""POST /query.json HTTP/1.1"" 200 219
superset_app            | DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): drill:8047
superset_app            | DEBUG:urllib3.connectionpool:http://drill:8047 ""POST /query.json HTTP/1.1"" 200 429
superset_app            | DEBUG:urllib3.connectionpool:http://drill:8047 ""POST /query.json HTTP/1.1"" 200 219
superset_app            | DEBUG:urllib3.connectionpool:http://drill:8047 ""POST /query.json HTTP/1.1"" 200 270
superset_app            | DEBUG:root:Mapping column SCHEMA_NAME of Drill type VARCHAR to dtype string
superset_app            | DEBUG:root:Mapping column TYPE of Drill type VARCHAR to dtype string
superset_app            | DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): drill:8047
superset_app            | DEBUG:urllib3.connectionpool:http://drill:8047 ""POST /query.json HTTP/1.1"" 200 429
superset_app            | DEBUG:urllib3.connectionpool:http://drill:8047 ""POST /query.json HTTP/1.1"" 200 219
superset_app            | DEBUG:urllib3.connectionpool:http://drill:8047 ""POST /query.json HTTP/1.1"" 200 192
superset_app            | DEBUG:root:Mapping column TABLE_NAME of Drill type VARCHAR to dtype string
superset_app            | DEBUG:superset.stats_logger:[stats_logger] (incr) DatabaseRestApi.table_metadata.error
superset_app            | ERROR:root:module 'sqlalchemy_drill.drilldbapi' has no attribute 'Error'
superset_app            | Traceback (most recent call last):
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1277, in _execute_context
superset_app            |     cursor, statement, parameters, context
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute
superset_app            |     cursor.execute(statement, parameters)
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy_drill/drilldbapi/_drilldbapi.py"", line 65, in func_wrapper
superset_app            |     return func(self, *args, **kwargs)
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy_drill/drilldbapi/_drilldbapi.py"", line 165, in execute
superset_app            |     elif str(df[col_name].iloc[0]).startswith(""["") and str(df[col_name].iloc[0]).endswith(""]""):
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/pandas/core/indexing.py"", line 895, in __getitem__
superset_app            |     return self._getitem_axis(maybe_callable, axis=axis)
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1501, in _getitem_axis
superset_app            |     self._validate_integer(key, axis)
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1444, in _validate_integer
superset_app            |     raise IndexError(""single positional indexer is out-of-bounds"")
superset_app            | IndexError: single positional indexer is out-of-bounds
superset_app            | 
superset_app            | During handling of the above exception, another exception occurred:
superset_app            | 
superset_app            | Traceback (most recent call last):
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/flask_appbuilder/api/__init__.py"", line 84, in wraps
superset_app            |     return f(self, *args, **kwargs)
superset_app            |   File ""/app/superset/views/base_api.py"", line 85, in wraps
superset_app            |     raise ex
superset_app            |   File ""/app/superset/views/base_api.py"", line 82, in wraps
superset_app            |     duration, response = time_function(f, self, *args, **kwargs)
superset_app            |   File ""/app/superset/utils/core.py"", line 1400, in time_function
superset_app            |     response = func(*args, **kwargs)
superset_app            |   File ""/app/superset/utils/log.py"", line 217, in wrapper
superset_app            |     value = f(*args, **kwargs)
superset_app            |   File ""/app/superset/databases/api.py"", line 488, in table_metadata
superset_app            |     table_info = get_table_metadata(database, table_name, schema_name)
superset_app            |   File ""/app/superset/databases/utils.py"", line 66, in get_table_metadata
superset_app            |     columns = database.get_columns(table_name, schema_name)
superset_app            |   File ""/app/superset/models/core.py"", line 619, in get_columns
superset_app            |     return self.db_engine_spec.get_columns(self.inspector, table_name, schema)
superset_app            |   File ""/app/superset/db_engine_specs/base.py"", line 866, in get_columns
superset_app            |     return inspector.get_columns(table_name, schema)
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/reflection.py"", line 391, in get_columns
superset_app            |     self.bind, table_name, schema, info_cache=self.info_cache, **kw
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy_drill/base.py"", line 382, in get_columns
superset_app            |     views = self.get_view_names(connection, schema)
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy_drill/base.py"", line 337, in get_view_names
superset_app            |     curs = connection.execute(""SELECT `TABLE_NAME` FROM INFORMATION_SCHEMA.views WHERE table_schema='"" + schema + ""'"")
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 2235, in execute
superset_app            |     return connection.execute(statement, *multiparams, **params)
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1003, in execute
superset_app            |     return self._execute_text(object_, multiparams, params)
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1178, in _execute_text
superset_app            |     parameters,
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1317, in _execute_context
superset_app            |     e, statement, parameters, cursor, context
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1390, in _handle_dbapi_exception
superset_app            |     isinstance(e, self.dialect.dbapi.Error)
superset_app            | AttributeError: module 'sqlalchemy_drill.drilldbapi' has no attribute 'Error'
superset_app            | 172.29.0.1 - - [08/Apr/2021:09:47:53 +0000] ""GET /api/v1/database/1/table/gcs/dfs.root/ HTTP/1.1"" 500 31 ""http://localhost:8088/superset/sqllab/"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36""",,,,,,,,,,,,,,
14022,OPEN,Metadata browser in SQL Lab not rendering nicely in Safari,browser:safari; bug:cosmetic; preset-io,2021-04-08 12:17:44 +0000 UTC,srinify,Opened,,"## Screenshot
<img width=""3000"" alt=""Screen Shot 2021-04-08 at 8 09 38 AM"" src=""https://user-images.githubusercontent.com/801507/114024385-fc697d00-9841-11eb-8e4e-7aa8524d7cc5.png"">
<img width=""1488"" alt=""Screen Shot 2021-04-08 at 8 08 26 AM"" src=""https://user-images.githubusercontent.com/801507/114024387-fd021380-9841-11eb-823f-acc48277804b.png"">


## Description

Latest Superset master",,,,,,,,,,,,,,
14021,OPEN,"[Bar chart] ""Color by"" feature",enhancement:request; needs:design-input; viz:chart-bar,2021-04-09 06:08:24 +0000 UTC,ValentinC-BR,Opened,,"## Screenshot

![image](https://user-images.githubusercontent.com/79460908/114021786-5cc3e300-9871-11eb-990c-9855416b713d.png)


## Description

As the color/legend is based on the breakdown and the breakdown cannot be the same as the series column, there is no way to have bars with various color (one per bar) in  **Bar Charts**.

The possibility to color by 'series' instead of 'breakdown' would be very appreciated.

Use case : I want one bar per customer, and the same colors as in the rest of the dashboard, to easily see which one is the most important, and where is my customer on each chart, without reading the legend each time.

## Design input

/",,,junlincc,"
--
@ValentinC-BR 
I am curious, how would adding a color layer to a simple bar chart help user see the difference of the bar? wouldn't it be a distraction instead? 

@mihir174 @steejay wdyt? 


--
",ValentinC,"
--
As pie charts are pretty misleading, I'd rather replace them with bar charts.

However, contrary to pie chats (in Superset), I can't easily identify (i mean without reading) where is the customer A in each chart)
If customer A is always the blue bar, I find it easier to do so.

I might be wrong however.
--
",,,,,,,,
14020,OPEN,'Upload parquet' option,enhancement:request,2021-04-09 12:25:30 +0000 UTC,elenamereloaxesor,In progress,,"**Is your feature request related to a problem? Please describe.**
In my data mining department we read data from parquets, something that's quite common and widespread. However, Superset doesn't give the option to directly upload a parquet file, just csv or excel.
**Describe the solution you'd like**
I would like it to be an 'upload parquet' option, alongside the existing ones.
**Describe alternatives you've considered**
I am already using Drill with Superset since Drill supports many kind of files. It would be perfect were it to be an option like when adding the database and using Drill, of using SQLAlchemy to connect with more databases that the default ones.

",,,junlincc,"
--
Hi @elenamereloaxesor, thanks for suggesting!Took a look at Apache Parquet, it seems promising! However, we haven't received a lot of the same request so this is not in our roadmap. would you be interested in adding the support? 
Bringing our data expert @betodealmeida for additional idea. :) 

--
",nytai,"
--
Parquet is awesome and I think its something we should consider, along with an option to import a file directly from a remote server (eg, s3). Though,  if data is already in parquet format it may make more sense to import it directly into the db then connect the db to superset.

For these upload data options, superset is really just acting as a broker for the target db. @elenamereloaxesor is the motivation here due to supersets upload UI being much friendlier than going directly to the db? Or is getting direct access to the db the challenge? 
--
",elenamereloaxesor,"
--
The motivation may well be both you mention. For ease of use, and since i am having a bit of trouble accesing GCS's parquets, even if i have succesfully connected Drill as a database. Thanks for the rapid response. I would of course love to help, but im afraid im quite new to this world, getting the grasp of how everything works and where everything is, so i feel a bit at a loss as to how to contribute. 
--
",0xBADBAC0N,"
--
Hi,if parquet does gets added I also recommend to add in the same process support for ORC files. Both of these formats are quite common in the Hadoop environment and analytics engineering area.
--
",,,,
14019,OPEN,Can we embed images in Pivot table in Superset?,,2021-04-08 09:32:28 +0000 UTC,Renganayaki1,Opened,,"![image](https://user-images.githubusercontent.com/77659418/114002526-93522b80-987a-11eb-93df-f6bdb549e192.png)

As you can see above, If I add the image column in pivot table, Instead of images, I'm getting the same HTML formatted column. 

Can we add/embed image in Pivot table? 
Because the same method is working well with straight table but not working with pivot table.

First of all, can we do this? Whether Pivot table in Superset supports this or not? 
If yes, How to do it? 

Thanks in advance. 
Rens. 
 ",,,,,,,,,,,,,,
14012,OPEN,Link broken in top-level documentation,#bug,2021-04-08 12:17:26 +0000 UTC,richdevboston,Opened,,"The third bullet on Page 1 of the main Apache / Superset documentation has a broken link [Documentation | Superset](https://superset.apache.org/docs/docs/installation/installing-superset-using-docker-compose), which leads to an error ""Not Found The requested URL was not found on this server.""  I want to install Superset into Windows using Docker Desktop.   

I am using Firefox on Windows 10.

From the page [Superset Introduction](https://superset.apache.org/docs/intro)

```
What is Apache Superset?

Apache Superset is a modern, enterprise-ready business intelligence web application. It is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple pie charts to highly detailed deck.gl geospatial charts.

Here are a few different ways you can get started with Superset:

    Download the source from Apache Foundation's website
    Download the latest Superset version from Pypi here
    Setup Superset locally with one command [using Docker Compose]  
    Download the Docker image from Dockerhub
    Install bleeding-edge master version of Superset from Github

```
https://superset.apache.org/docs/docs/installation/installing-superset-using-docker-compose

### Expected results  ==> The link opens the docker compose instructions!!

",,,srinify,"
--
Thank you @richdevboston I will fix this today!
--
",,,,,,,,,,
14005,OPEN,Import dashboard can import multiple times,#bug,2021-04-07 22:21:04 +0000 UTC,samtfm,Opened,,"When importing a dashboard, the dashboard will import multiple times if you click the button multiple times before the modal closes.

#### Screenshots

![Screen Shot 2021-04-07 at 3 12 04 PM](https://user-images.githubusercontent.com/12817097/113941304-b36ae780-97b3-11eb-8c5a-c5460d7560c5.png)
![Screen Shot 2021-04-07 at 3 14 18 PM](https://user-images.githubusercontent.com/12817097/113941446-f200a200-97b3-11eb-8722-5de4eb0aba77.png)


#### How to reproduce the bug

1. Go to the Dashboards list view
2. Click the import button in the top right corner (to the right of + Dashboard)
3. Select jinja json dashboard file, and then repeatedly click the import button.
4. The dashboard has now been imported multiple times
",,,,,,,,,,,,,,
14004,OPEN,"Time Picker - manually entered custom range timestamps don't ""stick""",#bug,2021-04-07 22:15:24 +0000 UTC,rusackas,In progress,,"When entering a custom time range, I tried to type a date in the input field / time picker. When I do so, and then hit return, the calendar popover goes away, but the date I typed doesn't stick, and instead reverts to what it was before.

### Expected results

I expect hand-entered dates (when valid) to persist.

### Actual results

Cancellation of timepicker popover

#### Screenshots

https://user-images.githubusercontent.com/812905/113941397-dc8b7800-97b3-11eb-987c-1219ff88a797.mp4


#### How to reproduce the bug

In explore, choose custom time range. Enter a date by hand, and hit return

### Environment

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,rusackas,"
--
@junlincc @zhaoyongjie hopefully minor, but wanted to track it...
--
",,,,,,,,,,
14003,OPEN,[echart] Improve tooltip on line echart to represent combination of time grain and time picker values,viz:echarts,2021-04-07 22:07:00 +0000 UTC,zuzana-vej,Opened,,"Currently, users setup time grain and filter time range and these might not always match.
For example: time grain weekly, but time filter is until 03-31-2021 (which is wednesday).

In the new echart Line chart, the X axis has the time periods listed and the tooltip also lists the time period. This always says the full week though. In the case when time granularity doesn't match filter, it can happen that the last week only shows few days of data. Based on the tooltip though, this is not straighforward for the user - especially if chart is accessed in dashboard (not in explore where the time picker range is little more visible).

The proposal is for the tooltip to actually display the correct time range - this can usually apply to the first or last week (for weekly granularity) or first or last month (for monthly granularity). 

**So in this example:**
Weekly granularity.
Filter: 
![Screen Shot 2021-04-07 at 2 57 57 PM](https://user-images.githubusercontent.com/61221714/113940059-b7960580-97b1-11eb-83fd-52f4ef97f205.png)

**Tooltip for last datapoint would ideally be:** 
2021-03-29 - 2021-03-30 (because time picker is until <2021-03-31 exclusive)
Instead of current:
![Screen Shot 2021-04-07 at 2 57 50 PM](https://user-images.githubusercontent.com/61221714/113940078-bfee4080-97b1-11eb-828f-e65bb75de967.png)

Why is this important - **in case of charts displaying cumulative value, partial week or partial month usually will have a drop and this leads dashboard consumers to misinterpretting the data.** 

Chart image:
(in this case we don't face the ""drop"" issue because values are averaged, just adding a chart image for illustration - that based on that it's seems to user that last week is complete)
![Screen Shot 2021-04-07 at 2 58 15 PM](https://user-images.githubusercontent.com/61221714/113940250-00e65500-97b2-11eb-99bb-cadf6ff33747.png)

Same issue is with current charts, but might be worth it to just implement enhancement with echarts.

**Alternatives**
There are alternatives to solving this problem, like warning on the chart in case time grain mismatches time picker (or filter) similar to dashboard filter indicator. Or warning on Explore when user sets time grain and filter which don't match.",,,zuzana,"
--
FYI @junlincc @kristw 
--
",,,,,,,,,,
14002,OPEN,Superset as a JavaScript visualization library,question,2021-04-09 02:16:59 +0000 UTC,candrewtemenoscom,Opened,,Is there any way that I can use Superset on other development platforms like asp.net on windows? Is there any JavaScript version of the query building and visualization module of the Superset that I can embed in a web application? ,,,maltoze,"
--
https://github.com/apache/superset/issues/2571
--
",,,,,,,,,,
14001,OPEN,Dashboard properties do not update in dashboard list UI,#bug,2021-04-07 21:54:50 +0000 UTC,samtfm,Opened,,"Editing dashboard properties does not cause an update in the dashboard list UI until the page is refreshed.

### Expected results

Dashboard is updated in the ui with new title and owners.

### Actual results

Edits are saved, but don't show up unless you refresh the page.

#### Screenshots

![Screen Shot 2021-04-07 at 2 41 07 PM](https://user-images.githubusercontent.com/12817097/113938374-62f18b00-97af-11eb-9c78-316dd4ab9e6f.png)

#### How to reproduce the bug
1. Go to Dashboards list view.
2. Open edit modal for a dashboard (pencil icon)
3. Edit Title or Owners fields.
4. Press save.
5. See dashboard in list with outdated details

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,,,,,,,,,,,,
14000,OPEN,[Homescreen] Dashboard thumbnail is gone when toggling between favorite and mine,#bug,2021-04-07 21:29:37 +0000 UTC,rosemarie-chiu,Opened,,"Pre-Condition:
being the owner of a dashboard

Steps:
1. Open homescreen, observe in dashboard section there is a dashboard in mine
2. Toggle to Favorite
3. Toggle back to mine, observe in mine, no dashboard show up

### Expected results
Togging back to mine dashboard should show dashboard thumbnails that I own

### Actual results
No dashboard thumbnail shows up in mine unless I refresh
what actually happens.

#### Screenshots

https://user-images.githubusercontent.com/52086618/113937164-99c6a180-97ad-11eb-9ab3-083fc230b60e.mov


",,,,,,,,,,,,,,
13999,OPEN,"Profile page -> Created Content: ""Favorited"" header should probably be ""Created""",bug:cosmetic; preset-io,2021-04-07 21:26:50 +0000 UTC,rusackas,Opened,,"## Screenshot

![image](https://user-images.githubusercontent.com/812905/113936691-faa1aa00-97ac-11eb-9e9a-a972972d570f.png)

## Description

On the Profile page, the ""Created Content"" tab shows my newly created Dashboard/Chart objects (hooray!) but it also lists their creation time under a ""favorited"" header. I never ""favorited"" these objects, so I'm under the assumption that the header should instead say ""created"".

## Design input
[describe any input/collaboration you'd like from designers, and
tag accordingly. For design review, add the
label `design:review`. If this includes a design proposal,
include the label `design:suggest`]
",,,,,,,,,,,,,,
13995,OPEN,Allow password to be specified for redis,data:connect:redis,2021-04-09 02:16:04 +0000 UTC,caleb15,In progress,,"**Is your feature request related to a problem? Please describe.**
https://github.com/apache/superset/blob/master/helm/superset/values.yaml says you can set a password for redis. (search for `Redis password`)
However, the superset helm chart does not appear to accept a redis password configuration.
See https://github.com/apache/superset/blob/master/helm/superset/templates/_helpers.tpl#L60 and 
https://github.com/apache/superset/blob/master/helm/superset/templates/secret-env.yaml

**Describe the solution you'd like**
Helm chart modified to allow passwords.

**Describe alternatives you've considered**
Redis can be locked down in a VPC. This doesn't completely eliminate the need for passwords but it helps protect it.

**Additional context**
Related issue: https://github.com/apache/superset/issues/13541
",,,caleb15,"
--
Related PR: https://github.com/apache/superset/pull/8420 (for just docker though, it doesn't include the helm chart)
--
",,,,,,,,,,
13982,OPEN,cannot display label percentage value in pie chart,need:validation; viz:chart-pie,2021-04-08 13:59:30 +0000 UTC,ivanku99,In progress,,"I can't display values and percentage in pie chart
the category was fixed -Category,value,percentage but nothing changed
![WhatsApp Image 2021-04-06 at 18 55 59](https://user-images.githubusercontent.com/82075724/113811142-78ee5400-978d-11eb-9969-6e8d07ac3801.jpeg)
![WhatsApp Image 2021-04-06 at 18 55 23](https://user-images.githubusercontent.com/82075724/113811144-7a1f8100-978d-11eb-941a-e83312261ad0.jpeg)

",,,junlincc,"
--
what version are you at? i believe it's fixed a while ago. 
--
",ivanku99,"
--
Version: 1.0.1
--

--
it happend after dawnloading  a new version. the old version  worked correctly 
--

--
what kind of type is this column in russian?  varchar?
--

--
I used the same Varchar and russian font like the upper left line 
But it dosn't work now. The old version of superset did not have tis problem 
--
",octaviancorlade,"
--
I'm not sure about 1.0.1 but it works with the current master, also with non-latin alphabets:

![image](https://user-images.githubusercontent.com/15089539/113925695-9f42dc80-97eb-11eb-8336-397da050a4d5.png)


--

--
@ivanku99 yes that's right, I used the one defined [here](https://github.com/apache/superset/blob/master/superset/examples/configs/datasets/examples/unicode_test.test.yaml#L55) from the example data
--
",,,,,,
13978,OPEN,[Explore] data table unnecessary re-render,good first issue; viz:explore:perf,2021-04-08 21:34:33 +0000 UTC,michael-s-molina,Opened,,"## Description

Avoid unnecessary re-render of the data table in Explore. As we can see in the video below, the data table is re-rendered even if the control being applied is not related to the table. We can improve the user experience by only re-rendering the data table if the modified control affects the table data.

## Screenshot

https://user-images.githubusercontent.com/70410625/113782575-d1c7d900-9708-11eb-8135-b2c982073bd8.mov#13974 

@junlincc @rusackas ",,,junlincc,"
--
Make sense, thanks for suggesting! 

cc @kgabryje (not urgent, let's wait for next quality bash :) ) 

--
",,,,,,,,,,
13977,OPEN,[homescreen]500 error when deleting dashboards,global:homepage; need:validation,2021-04-07 06:21:37 +0000 UTC,jonathanmorais,In progress,,"Error 500 when trying to delete dashboards 
### Expected results

Dashboard deletion

### Actual results

There was an issue deleting [ <dashboard> ]: Fatal error

#### Screenshots

![Captura de tela de 2021-04-06 17-16-04](https://user-images.githubusercontent.com/12867382/113774036-249b9380-96fd-11eb-805d-66a1a4cf2ec2.png)

![superset_500](https://user-images.githubusercontent.com/12867382/113773922-0170e400-96fd-11eb-9795-2c4db7a3bde1.png)

### Environment


- superset version: `1.0.1`
",,,nytai,"
--
can you post the server logs for this? A stack trace would be helpful
--
",jonathanmorais,"
--
Sure, the only log that the application gives us, here is:

![superset_log](https://user-images.githubusercontent.com/12867382/113775103-988a6b80-96fe-11eb-8b18-19e025b1ace6.png)

--
",,,,,,,,
13976,OPEN,[dashboard]'go back' button causes page crash in dashboard,#bug; viz:dashboard:error,2021-04-06 19:36:47 +0000 UTC,junlincc,Opened,,"See video

https://user-images.githubusercontent.com/67837651/113766859-ca85d880-96d2-11eb-8be8-4307b552e061.mov

#### How to reproduce the bug

1. Go to Dashboards
2. Click on any dashboard
3. Click on go back button ""<- ""
4. See error

### Environment 
master

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,nytai,"
--
That exception looks related to async queries 
--
",,,,,,,,,,
13968,OPEN,[chart]Columns with all NA values should not be silently dropped,#bug; enhancement:committed; viz:echarts,2021-04-07 14:52:35 +0000 UTC,octaviancorlade,In progress,,"#8040 restored dropping columns that are all NA, but it is problematic in some visualization types. 
There is already #13612 for this issue in Bar Charts, for which I opened a PR, but the same applies to dual line charts if one of the lines has only null values. 
The error is also not informative, since it only shows the name of the column.

### Expected results

The null values should be shown as nulls, without data points on the charts

![image](https://user-images.githubusercontent.com/15089539/113723675-df259900-96f1-11eb-8dab-f30953fd44d1.png)


### Actual results

The null series is dropped by `pandas.pivot_table` calls which use `dropna=True` as default, and the user can only see the text of a `KeyError`:

![image](https://user-images.githubusercontent.com/15089539/113723786-fa90a400-96f1-11eb-96ba-19d3f3b06586.png)

The stacktrace for the dual line chart:
```
ERROR:superset.views.base:'metric1'
Traceback (most recent call last):
  File ""/app/superset/views/base.py"", line 181, in wraps
    return f(self, *args, **kwargs)
  File ""/app/superset/utils/log.py"", line 164, in wrapper
    value = f(*args, **kwargs)
  File ""/app/superset/utils/cache.py"", line 152, in wrapper
    return f(*args, **kwargs)
  File ""/app/superset/views/utils.py"", line 451, in wrapper
    return f(*args, **kwargs)
  File ""/app/superset/views/core.py"", line 619, in explore_json
    return self.generate_json(viz_obj, response_type)
  File ""/app/superset/views/core.py"", line 456, in generate_json
    payload = viz_obj.get_payload()
  File ""/app/superset/viz.py"", line 464, in get_payload
    payload[""data""] = self.get_data(df)
  File ""/app/superset/viz.py"", line 1531, in get_data
    chart_data = self.to_series(df)
  File ""/app/superset/viz.py"", line 1500, in to_series
    ys = series[m]
KeyError: 'metric1'
```

#### How to reproduce the bug

1. Open a bar chart, a dual line chart, or other charts that use `pandas.pivot_table` in `viz.py`
2. Choose a metric that only returns null values for the selected period. This is true for e.g. `sum(1)+null` in most SQL dialects

### Environment

I used the latest master with docker

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,junlincc,"
--
@octaviancorlade thank you so much for reporting! do you mind confirming what charts are being affected? 

@villebro @zhaoyongjie @kgabryje what's the behavior in echarts and pivot table do you guys know?  
--
",kgabryje,"
--
@junlincc @octaviancorlade The new (not merged yet) pivot table shows zeros as values. 
![image](https://user-images.githubusercontent.com/15073128/113839682-337f5600-9790-11eb-81bb-cc9b157b9685.png)

The old pivot table drops the column with nulls.
![image](https://user-images.githubusercontent.com/15073128/113839804-501b8e00-9790-11eb-8cfc-ff42b5ea515a.png)


--

--
In the case of Echarts, Timeseries displays null values as zeros. Pie and Graph charts display nothing (just white background) and Box chart displays an error `Error: No numeric types to aggregate`
--
",octaviancorlade,"
--
@junlincc @kgabryje here are some screenshots and links to reproduce locally with the example data of the ones using `df.pivot_table()` in `viz.py`, only Big Number and Time-series Period Pivot seem to handle the situation currently. 
I am not sure if I'm missing something since the Pivot Table generates errors here, maybe the screenshots by @kgabryje are both not yet in master?
 
[Time-series Table](http://localhost:8088/superset/explore/?form_data=%7B%22viz_type%22%3A%22time_table%22%2C%22datasource%22%3A%2219__table%22%2C%22slice_id%22%3A82%2C%22url_params%22%3A%7B%7D%2C%22time_range_endpoints%22%3A%5B%22inclusive%22%2C%22exclusive%22%5D%2C%22granularity_sqla%22%3A%22time_start%22%2C%22time_grain_sqla%22%3A%22P1D%22%2C%22time_range%22%3A%22No+filter%22%2C%22metrics%22%3A%5B%7B%22expressionType%22%3A%22SQL%22%2C%22sqlExpression%22%3A%22COUNT%28%2A%29+%2B+NULL%22%2C%22column%22%3Anull%2C%22aggregate%22%3Anull%2C%22isNew%22%3Afalse%2C%22hasCustomLabel%22%3Afalse%2C%22label%22%3A%22COUNT%28%2A%29+%2B+NULL%22%2C%22optionName%22%3A%22metric_k65my36ei5e_qiwcw5ofy4m%22%7D%5D%2C%22adhoc_filters%22%3A%5B%5D%2C%22groupby%22%3Anull%2C%22limit%22%3A100%2C%22column_collection%22%3A%5B%7B%22key%22%3A%22jTqUEXjNb%22%7D%5D%2C%22row_limit%22%3A10000%2C%22extra_form_data%22%3A%7B%22custom_form_data%22%3A%7B%7D%2C%22override_form_data%22%3A%7B%7D%2C%22append_form_data%22%3A%7B%7D%7D%7D)

![image](https://user-images.githubusercontent.com/15089539/113845157-7e4f9c80-9795-11eb-89af-74d23999fd5b.png)

[Pivot Table](http://localhost:8088/superset/explore/?form_data=%7B%22viz_type%22%3A%22pivot_table%22%2C%22datasource%22%3A%2219__table%22%2C%22slice_id%22%3A82%2C%22url_params%22%3A%7B%7D%2C%22time_range_endpoints%22%3A%5B%22inclusive%22%2C%22exclusive%22%5D%2C%22granularity_sqla%22%3A%22time_start%22%2C%22time_grain_sqla%22%3A%22P1D%22%2C%22time_range%22%3A%22No+filter%22%2C%22metrics%22%3A%5B%7B%22expressionType%22%3A%22SQL%22%2C%22sqlExpression%22%3A%22COUNT%28%2A%29+%2B+NULL%22%2C%22column%22%3Anull%2C%22aggregate%22%3Anull%2C%22isNew%22%3Afalse%2C%22hasCustomLabel%22%3Afalse%2C%22label%22%3A%22COUNT%28%2A%29+%2B+NULL%22%2C%22optionName%22%3A%22metric_k65my36ei5e_qiwcw5ofy4m%22%7D%5D%2C%22adhoc_filters%22%3A%5B%5D%2C%22groupby%22%3A%5B%22marital_status%22%5D%2C%22columns%22%3A%5B%5D%2C%22row_limit%22%3A10000%2C%22order_desc%22%3Atrue%2C%22pandas_aggfunc%22%3A%22sum%22%2C%22pivot_margins%22%3Atrue%2C%22number_format%22%3A%22SMART_NUMBER%22%2C%22date_format%22%3A%22smart_date%22%2C%22extra_form_data%22%3A%7B%22custom_form_data%22%3A%7B%7D%2C%22override_form_data%22%3A%7B%7D%2C%22append_form_data%22%3A%7B%7D%7D%7D)

![image](https://user-images.githubusercontent.com/15089539/113845377-bce55700-9795-11eb-9cf8-c9c5bbec1de2.png)

[Big Number](http://localhost:8088/superset/explore/?form_data=%7B%22viz_type%22%3A%22big_number_total%22%2C%22datasource%22%3A%2219__table%22%2C%22slice_id%22%3A82%2C%22url_params%22%3A%7B%7D%2C%22time_range_endpoints%22%3A%5B%22inclusive%22%2C%22exclusive%22%5D%2C%22granularity_sqla%22%3A%22time_start%22%2C%22time_range%22%3A%22No+filter%22%2C%22metric%22%3A%7B%22expressionType%22%3A%22SQL%22%2C%22sqlExpression%22%3A%22COUNT%28%2A%29+%2B+NULL%22%2C%22column%22%3Anull%2C%22aggregate%22%3Anull%2C%22isNew%22%3Afalse%2C%22hasCustomLabel%22%3Afalse%2C%22label%22%3A%22COUNT%28%2A%29+%2B+NULL%22%2C%22optionName%22%3A%22metric_egnne5z0946_d0cm4i01bm7%22%7D%2C%22adhoc_filters%22%3A%5B%5D%2C%22y_axis_format%22%3A%22SMART_NUMBER%22%2C%22header_font_size%22%3A0.4%2C%22subheader_font_size%22%3A0.15%2C%22extra_form_data%22%3A%7B%22custom_form_data%22%3A%7B%7D%2C%22override_form_data%22%3A%7B%7D%2C%22append_form_data%22%3A%7B%7D%7D%7D) looks OK to me

![image](https://user-images.githubusercontent.com/15089539/113847040-5b25ec80-9797-11eb-93a4-c88c910ce9da.png)

[Dual Line Chart](http://localhost:8088/superset/explore/?form_data=%7B%22viz_type%22%3A%22dual_line%22%2C%22datasource%22%3A%2219__table%22%2C%22slice_id%22%3A82%2C%22url_params%22%3A%7B%7D%2C%22time_range_endpoints%22%3A%5B%22inclusive%22%2C%22exclusive%22%5D%2C%22granularity_sqla%22%3A%22time_start%22%2C%22time_grain_sqla%22%3A%22P1D%22%2C%22time_range%22%3A%22No+filter%22%2C%22color_scheme%22%3A%22supersetColors%22%2C%22label_colors%22%3A%7B%7D%2C%22x_axis_format%22%3A%22smart_date%22%2C%22metric%22%3A%22count%22%2C%22y_axis_format%22%3A%22SMART_NUMBER%22%2C%22y_axis_bounds%22%3A%5Bnull%2Cnull%5D%2C%22metric_2%22%3A%7B%22expressionType%22%3A%22SQL%22%2C%22sqlExpression%22%3A%22COUNT%28%2A%29+%2B+NULL%22%2C%22column%22%3Anull%2C%22aggregate%22%3Anull%2C%22isNew%22%3Afalse%2C%22hasCustomLabel%22%3Afalse%2C%22label%22%3A%22COUNT%28%2A%29+%2B+NULL%22%2C%22optionName%22%3A%22metric_y1k9ywww97r_ydides4be9%22%7D%2C%22y_axis_2_format%22%3A%22SMART_NUMBER%22%2C%22y_axis_2_bounds%22%3A%5Bnull%2Cnull%5D%2C%22adhoc_filters%22%3A%5B%5D%2C%22annotation_layers%22%3A%5B%5D%2C%22extra_form_data%22%3A%7B%22custom_form_data%22%3A%7B%7D%2C%22override_form_data%22%3A%7B%7D%2C%22append_form_data%22%3A%7B%7D%7D%7D)

![image](https://user-images.githubusercontent.com/15089539/113845874-38df9f00-9796-11eb-826c-4cb6edd99e3b.png)

[Time-series Period Pivot](http://localhost:8088/superset/explore/?form_data=%7B%22viz_type%22%3A%22time_pivot%22%2C%22datasource%22%3A%2219__table%22%2C%22slice_id%22%3A82%2C%22url_params%22%3A%7B%7D%2C%22time_range_endpoints%22%3A%5B%22inclusive%22%2C%22exclusive%22%5D%2C%22granularity_sqla%22%3A%22time_start%22%2C%22time_grain_sqla%22%3A%22P1D%22%2C%22time_range%22%3A%22No+filter%22%2C%22metric%22%3A%7B%22expressionType%22%3A%22SQL%22%2C%22sqlExpression%22%3A%22COUNT%28%2A%29+%2B+NULL%22%2C%22column%22%3Anull%2C%22aggregate%22%3Anull%2C%22isNew%22%3Afalse%2C%22hasCustomLabel%22%3Afalse%2C%22label%22%3A%22COUNT%28%2A%29+%2B+NULL%22%2C%22optionName%22%3A%22metric_3k6ay6ex9gu_2er2f7kq0kc%22%7D%2C%22adhoc_filters%22%3A%5B%5D%2C%22freq%22%3A%22W-MON%22%2C%22line_interpolation%22%3A%22linear%22%2C%22color_picker%22%3A%7B%22r%22%3A0%2C%22g%22%3A122%2C%22b%22%3A135%2C%22a%22%3A1%7D%2C%22x_axis_label%22%3A%22age%22%2C%22bottom_margin%22%3A%22auto%22%2C%22x_axis_format%22%3A%22smart_date%22%2C%22y_axis_label%22%3A%22count%22%2C%22left_margin%22%3A%22auto%22%2C%22y_axis_format%22%3A%22SMART_NUMBER%22%2C%22y_axis_bounds%22%3A%5Bnull%2Cnull%5D%2C%22extra_form_data%22%3A%7B%22custom_form_data%22%3A%7B%7D%2C%22override_form_data%22%3A%7B%7D%2C%22append_form_data%22%3A%7B%7D%7D%7D) also looks OK to me

![image](https://user-images.githubusercontent.com/15089539/113846206-878d3900-9796-11eb-8985-7ce10812e0ea.png)

[Bar Chart](http://localhost:8088/superset/explore/?form_data=%7B%22viz_type%22%3A%22dist_bar%22%2C%22datasource%22%3A%2219__table%22%2C%22slice_id%22%3A82%2C%22url_params%22%3A%7B%7D%2C%22time_range_endpoints%22%3A%5B%22inclusive%22%2C%22exclusive%22%5D%2C%22granularity_sqla%22%3A%22time_start%22%2C%22time_range%22%3A%22No+filter%22%2C%22metrics%22%3A%5B%7B%22expressionType%22%3A%22SQL%22%2C%22sqlExpression%22%3A%22COUNT%28%2A%29+%2B+NULL%22%2C%22column%22%3Anull%2C%22aggregate%22%3Anull%2C%22isNew%22%3Afalse%2C%22hasCustomLabel%22%3Afalse%2C%22label%22%3A%22COUNT%28%2A%29+%2B+NULL%22%2C%22optionName%22%3A%22metric_u5cuk4h7i48_nopwaswccig%22%7D%5D%2C%22adhoc_filters%22%3A%5B%5D%2C%22groupby%22%3A%5B%22marital_status%22%5D%2C%22columns%22%3A%5B%5D%2C%22row_limit%22%3A10000%2C%22order_desc%22%3Atrue%2C%22color_scheme%22%3A%22supersetColors%22%2C%22label_colors%22%3A%7B%7D%2C%22y_axis_format%22%3A%22SMART_NUMBER%22%2C%22y_axis_label%22%3A%22count%22%2C%22y_axis_bounds%22%3A%5Bnull%2Cnull%5D%2C%22x_axis_label%22%3A%22age%22%2C%22bottom_margin%22%3A%22auto%22%2C%22x_ticks_layout%22%3A%22auto%22%2C%22extra_form_data%22%3A%7B%22custom_form_data%22%3A%7B%7D%2C%22override_form_data%22%3A%7B%7D%2C%22append_form_data%22%3A%7B%7D%7D%7D)

![image](https://user-images.githubusercontent.com/15089539/113846480-cde29800-9796-11eb-9b07-5efd0ed890b7.png)

[Paired t-test Table](http://localhost:8088/superset/explore/?form_data=%7B%22viz_type%22%3A%22paired_ttest%22%2C%22datasource%22%3A%2219__table%22%2C%22slice_id%22%3A82%2C%22url_params%22%3A%7B%7D%2C%22time_range_endpoints%22%3A%5B%22inclusive%22%2C%22exclusive%22%5D%2C%22granularity_sqla%22%3A%22time_start%22%2C%22time_range%22%3A%22No+filter%22%2C%22metrics%22%3A%5B%7B%22expressionType%22%3A%22SQL%22%2C%22sqlExpression%22%3A%22COUNT%28%2A%29+%2B+NULL%22%2C%22column%22%3Anull%2C%22aggregate%22%3Anull%2C%22isNew%22%3Afalse%2C%22hasCustomLabel%22%3Afalse%2C%22label%22%3A%22COUNT%28%2A%29+%2B+NULL%22%2C%22optionName%22%3A%22metric_u5cuk4h7i48_nopwaswccig%22%7D%5D%2C%22adhoc_filters%22%3A%5B%5D%2C%22groupby%22%3A%5B%22marital_status%22%5D%2C%22limit%22%3A100%2C%22order_desc%22%3Atrue%2C%22row_limit%22%3A10000%2C%22significance_level%22%3A0.05%2C%22pvalue_precision%22%3A6%2C%22liftvalue_precision%22%3A4%2C%22extra_form_data%22%3A%7B%22custom_form_data%22%3A%7B%7D%2C%22override_form_data%22%3A%7B%7D%2C%22append_form_data%22%3A%7B%7D%7D%7D)

![image](https://user-images.githubusercontent.com/15089539/113846620-f5396500-9796-11eb-8f76-4ea364d39f6b.png)

--
",,,,,,
13966,OPEN,[Drilldown]Support mapping of source/target columns in Cross-Filters,enhancement:committed; viz:dashboard:drill-down,2021-04-07 07:57:29 +0000 UTC,cccs-jc,Opened,,"This is mock-up of a dashboard with two Charts. The first chart is a table of flights between a originating airport and a destination airport. Columns include ORIGINATING_AIRPORT, DESTINATION_AIRPORT

The second chart gives details about a given airport. Columns include AIRPORT, LOCATION, BUILT

The idea would be to use cross-filtering in order to display the details of the airport selected in the flights table.

Notice that the datasets used in the charts do not share a common airport column name. Currently cross-filter require matching source/target column names.

![Flights](https://user-images.githubusercontent.com/56140112/113712853-a0bdb900-96b4-11eb-9702-b38b4a2bb92c.png)

You can also imagine a Slack dataset where columns might be ""USER_MENTION"", ""USER_REACTION"", ""USER_COMMENT"" and a user details table keyed on ""USER_ID"".

In the cross-filtering mechanism it would be great to have a means to map a given source column to a target filter column. 





",,,junlincc,"
--
hey jc, the feature you are proposing is called 'drilldown', similar to cross-filtering. instead of changing a 'where clause', drill down is basically changing the dimension. enabling it is now our top priority and would love to use some help from you. 
lmk your plan on contributing  @cccs-jc 
--
",,,,,,,,,,
13964,OPEN,[error]Dashboard is not shown in list but is accessible via link,#bug; P2,2021-04-07 21:23:27 +0000 UTC,nickdomnin,Opened,,"User can access dashboards in any states via link although they are not shown in user's dashboard list. In case of RLS enabled and user without permissions for any datasources used on dashboard this case should be considered as security issue.

### Expected results

Error message about denied access to that dashboard and redirection to previous page or welcome/dashboard list page.

### Actual results

Dashboard page opened with access errors on all charts.

#### How to reproduce the bug

1. Create dashboard on any datasource which is not accessible by all users of SuperSet
2. Save dashboard url
3. Relogin with user without access to used datasource
4. Open saved link

### Environment

Reproduced on 1.0.1, 1.1.0rc1 and master.

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.",,,junlincc,"
--
thanks for reporting. @nytai hey tai, were you working on the permission error page earlier?


--
",nytai,"
--
I havent worked on permissions or error pages recently. I suspect this was always an issue, as permissions are really enforced at the dataset level. 
--
",amitmiran137,"
--
If you enable DASHBOARD_RBAC FF then accessing a specific  dashboard without a  permission is denied 
--
",,,,,,
13959,OPEN,Annotation background is transparent; causing text to overlap illegibly,need:validation,2021-04-07 16:08:39 +0000 UTC,razzius,In progress,,"## Screenshot

<img width=""280"" alt=""image"" src=""https://user-images.githubusercontent.com/2244895/113651255-80571580-9646-11eb-8929-63adf6d66903.png"">

## Description

Annotations of type ""interval"", when added to a line chart, show up directly on top of the chart / legend, causing text to overlap in a way that makes it hard to read. I think in the past (pre-1.0) there was an opaque background that made annotations legible.

## Design input

Anything that makes the text legible would be welcome.",,,junlincc,"
--
I believe users can set the opacity of annotation. can you double check? thanks
--
",razzius,"
--
@junlincc it looks like the controls customize how the overlay on the graph shows up (at least this is the case with color, see screenshot), not the annotation text. Opacity doesn't seem to have any effect, though, so perhaps something strange is going on with that.

<img width=""364"" alt=""image"" src=""https://user-images.githubusercontent.com/2244895/113898473-c57f6280-9780-11eb-9c72-75de55051886.png"">

--
",,,,,,,,
13957,OPEN,Docker Compose with no example data,install:docker,2021-04-06 07:32:07 +0000 UTC,alexkreidler,In progress,,"When I followed the recommended instructions for running Superset with Docker Compose, it took quite a while to load the example data. On a subsequent try it took 6 minutes and the HTTP endpoint provided no response (it just hung).

```
curl -vv http://0.0.0.0:8088/                                                                                                                                                                           (base) 
*   Trying 0.0.0.0:8088...
* TCP_NODELAY set
* Connected to 0.0.0.0 (127.0.0.1) port 8088 (#0)
> GET / HTTP/1.1
> Host: 0.0.0.0:8088
> User-Agent: curl/7.68.0
> Accept: */*
> 
```

It would be great if there was a version of the Docker Compose with no Postgres container, no example data from the init script, just a minimal installation. I think this could really help folks get started on devices with less resources, or who might already have a database they want to use rather than examples.

As a side note, there is not much documentation on what each container does, and whether it is necessary. Adding this would really help folks like me who want to customize the docker-compose config.

Thanks a ton!
",,,alexkreidler,"
--
Ok I've realized you can set `SUPERSET_LOAD_EXAMPLES=no` in `.env-non-dev` to disable loading the examples. The `docker-compose down -v` command is also helpful to remove previous volumes.

I'll leave this open to track adding a note for this in the docs.
--

--
Sure, PR incoming.
--
",nytai,"
--
@alexkreidler docker-compose provides a simple way to bring up a fully functional version of superset with a single command. Redis and Postgres are used by the application for caching and storing metadata, if you would like to bring your own instances of these, running docker containers directly might be a better fit https://hub.docker.com/r/apache/superset. 

The docs could always be better. Having a section for ""Debugging docker-compose"" and ""Customizing docker-compose"" would be useful. Would you be interested in contributing in this area? 
--
",,,,,,,,
13954,OPEN,[Reporting][help needed]New feature request for reporting and alert,enhancement:request; global:report,2021-04-09 01:35:32 +0000 UTC,junlincc,In progress,,"Superset Roadmap item: https://github.com/apache-superset/superset-roadmap/issues/51 

Request: Users, instead of going to dashboards, would like to receive data via email: PDF, 
Screenshot, Link to dashboard or info your data is available. Based on alerts or Anomaly detection or data availability.

Gaps in current Superset reporting:

Setting up email / report schedules

- [**Onboarding**]Add tooltips to the UI - even with the improved UI/UX its unclear what some of the fields in the setup mean
- [**Timezone settings**] - currently only (default) option is UTC, ideally user can select their timezone (e.g select PST in a dropdown) and then setup time in PST

Management of email schedules

- [**Configuration**] Admin can configure whether anyone can add anyone to email schedules or limited to above points.

- [**Subscription**]User can sign up for themselves and others to receive dashboard emails (setup dashboard to receive via email) 

- [**Schedule management**]Ability for a user (consumers) to see all their scheduled charts / dashboards (somewhat possible if they filter; maybe filter on their own schedules should be default in the UI)

- [**View subscription**]Ability for chart / dashboard owners to easily see who is subscribed to their dashboard

When a change to subscription is made, the recipient & dashboard owner could be notified about the change.


Security

- [**RBAC**] Security: All emails should be generated on the recipients permissions to access the data within the dashboard / chart. 

Solution proposal: Check access to the section of dashboard which is being email; if user has access to all charts - include them in email; otherwise - just send them link to dashboard.  this addresses some of the perf. Concerns about each email being generated separately for permissions reason. 

- [**RBAC**] Dashboard owner can restrict my dashboard is never emailable  this addresses part of the Security issue.  

Defining what to send - Tabs, Filters, Recipient

- [**Tabs**]Handling dashboards with multiple tabs: The email subscription should be per tab - user can select which tab to email or even select multiple tabs (?combination of nested tabs?) 

- [**Filters**] Being able to send dashboard tab with defined filters - not just default filters

- [**Recipient**] Ability to send email to Dashboard owner - in case owners change, recipients also change

- [**PDF**] Send in PDF not screenshot (PDF is standard, Looker, Tableau both send PDF)

- [**Text**] Ability to define short text/content to be included at the top of the email - currently it includes link to dashboard, users would like to include a short message, link to documentation, contact info, etc. 

- [**Mobile**] friendly views - needed for executive reporting

Tooltips / visibility into values: Lot of charts are useful only with tooltips, since values are not displayed on the chart by default. If the chart has description it can be toggled on charts being sent via email. 

Core functionality

- [**Triggering**]Trigger not based on specified time or alert, but based on last weeks data landed

Meaning: instead of setting ""send this dashboard every Monday 10am"", set up as ""as soon as the data for week lands (week ending sunday) send this dashboard"" (it could be monday 10 am or 1pm)

- [**cleanup**] mechanism to prevent email schedules which are not longer used running

Concerns:
Performance, especially if each request is processed separately to apply users specific permissions. Can be executed on batch cluster (instead of dashboard cluster)
Caching / data freshness - by default it uses data from cache - currently expected behavior but based on the time of the report data might be a day old (should be addressed longer term) 
",,,junlincc,"
--
^^ @daniel10012 @zuzana-vej @amitmiran137 @EBoisseauSierra @willbarrett 

Do we see any above items that are aligned with our own org's goals? Can we collaborate on this project? thanks! 
--

--
@EBoisseauSierra @krsnik93 
I'm wondering if any of those interests you and your org, lmk if you wanna work on those.  PDF seems to be a small size project. 
`What was useful to me in some other BI tools was a ""Send now"" button right where the schedule/content is being defined, so that the user can immediately test out the report.` 
This should also be a small item :) 
--
",EBoisseauSierra,"
--
Wow! Thanks for summing that up, this sounds very exciting! Your summary covers already a lot (and more than what we could have imagined).

I quite like the idea to be able to send one single report, and use RBAC to automagically customise it per recipient. This suppose that each addressee is also a user of the Superset instance. This isn't a bad thing from a security perspective, yet might make it overcomplicated if people need to setup an account just to be able to receive an email. (Fall back to the `Public` role?)
A possibility _could_ be to enable sending report to non-Superset-users. In that case, a nice to have would be for the dashboard owner to whitelist which domain name(s) the report can be sent to (e.g. only to `*@mycorp.com` and `*@cooldivision.io`). 
(And, in general, it could be interesting to be able to configure Superset so that one can restrict outbound email to certain domain name(s).)

Re caching, a force refresh graphs could be added as an option.

As for attachment, we might want to be able to add PDF and/or CSV.

I'm very much looking forward to it! :rocket: 
--
",krsnik93,"
--
Email should contain an ""Unsubscribe"" link.

What was useful to me in some other BI tools was a ""Send now"" button right where the schedule/content is being defined, so that the user can immediately test out the report. Workaround is for the user to create a schedule sending in a minute or similar, but then it needs to be canceled as well, so a one time event is nicer.
--
",kamalkeshavani,"
--
One concern I have in current implementation is regarding RLS.
When I have a dashboard/chart with RLS control enabled, I can't schedule email/alerts to different users, since they are generated by admin rights showing all the data to all the users. But if we try to control this, then we have to limit the recipients to only Superset users as mentioned by @EBoisseauSierra, which can be troublesome in some cases.
--
",john,"
--
TL;DR Feature request changes to remedy security vulnerabilities 

Though the reporting and alerting feature is great and I can understand the desire for Slack integrations for alerting etc., I and others sense this exposes a fairly major data security vulnerability, i.e., users could be exposed (via email or Slack) to information which they do not have permission to via the Superset UI. 

Ensuring Superset adheres to the defined security policy should trump any desired functionality. Superset needs to have a sound concise and do the right thing, though any downstream actions taken by individuals, i.e., forwarding of emails etc., is outside of Superset's jurisdiction.

I propose the following changes should be made with the understanding that functionality will be impacted:

1. The Slack and generic email notification methods be disabled. This exposes a fairly major security vulnerability as Superset has no a priori knowledge which individuals are members of said Slack or email group.
2. Recipients should be Superset users (see screenshot). Users are emailed content individually according to the email registered in their profile.
3. The screenshots are captured on behalf of each user. This ensures what they receive is _identical_ to their interactive Superset experience. Though this requires more requests, if run sequentially the chart payloads will be cached and thus the overhead should be manageable. Note this is a more desirable user experience than checking whether said user can access the entire dashboard in question and emailing the image only if this is the case.

These changes should be fairly minor in terms of engineering work and do remedy the security issues. Down the road we can explore how we could integrate with Slack et al. in a more secure way.   

### Screenshots

#### Before 

<img width=""1137"" alt=""Screen Shot 2021-04-09 at 9 07 52 AM"" src=""https://user-images.githubusercontent.com/4567245/114099335-b9c5a480-9916-11eb-95dd-79d82ed6840f.png"">

#### After

<img width=""1139"" alt=""Screen Shot 2021-04-09 at 9 12 17 AM"" src=""https://user-images.githubusercontent.com/4567245/114099373-c813c080-9916-11eb-90cc-8b2b4d9c8093.png"">

cc: @graceguo-supercat @junlincc @nytai @willbarrett @zuzana-vej 
--
",,
13952,OPEN,[Dashboard] Tooltips disappear on loading dashboards,#bug; bug; tooltip:chart; viz:chart-line,2021-04-06 13:12:28 +0000 UTC,serenajiang,Opened,,"In a dashboard, if you mouse over a chart while *other charts* on the same tab are loading, the tooltip is visible. However, once at least one other chart loads, mousing over the original chart no longer displays the tooltip, and it won't come back unless you reload the dashboard, force refresh the chart, or inspect element.

### Expected results

Tooltip appears:
![image](https://user-images.githubusercontent.com/14146019/113604881-bbcbf280-95fa-11eb-8707-1ddc91199552.png)


### Actual results

Tooltip does not appear:
![image](https://user-images.githubusercontent.com/14146019/113604984-e3bb5600-95fa-11eb-9de1-83ed632943e5.png)


#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug
1. Open any superset Dashboard (preferably with lots of charts to load)
2. While charts are loading, mouse over an ""already loaded"" chart to engage the tooltips
3. Once another chart has loaded, the tooltip for the ""first-loaded"" chart will no longer be visible.

### Environment

(please complete the following information):

- superset version: `superset version`: up-to-date as of 2021-04-02
- python version: `python --version`: 3.7
- node.js version: `node -v`: `v14.16.0`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,junlincc,"
--
@serenajiang thanks for reporting! which charts are you seeing this issue? I assume it's NVD3 line?
--
",iercan,"
--
We also face this issue with line charts. 
--
",,,,,,,,
13949,OPEN,Google Sheet Error no such table: https://docs.google.com/spreadsheet/....,,2021-04-08 12:19:37 +0000 UTC,jessicaxyl,Opened,,"My Google Sheet is already set as anyone with the link can view it. I can query and display results from the google sheet URL in Superset's SQL Editor, but once I clicked ""explore"" and it opens up a new tab for creating a chart, I have this error of no such table, and I can't create any chart. I've also tried to link the Google Sheet URL by creating a new dataset (physical) and it's still the same, can't create a chart. Anyone knows what I need to do? Btw, I followed the steps from here as well https://preset.io/blog/2020-06-01-connect-superset-google-sheets/
![image](https://user-images.githubusercontent.com/81961241/113563910-14bb7b00-963b-11eb-87b5-1b07ecee1190.png)
",,,srinify,"
--
@betodealmeida are these instructions still relevant in the Google Sheets blog post? Should we instead point people to your new library (shillelagh) and update the post?
--
",,,,,,,,,,
13948,OPEN,Integration with OAuth provider okta,,2021-04-05 12:49:20 +0000 UTC,vinit2580,Opened,,"Hi,
   I am trying to integrate okta using OAuth but everytime it gives me invalid login. Please try again message.
My superset_config.py has below configuration: 

import os
from flask import Flask

import logging
from flask_appbuilder.security.manager import AUTH_OID, AUTH_REMOTE_USER, AUTH_DB, AUTH_LDAP, AUTH_OAUTH
from superset.security import SupersetSecurityManager
import logging
from flask_appbuilder import SQLA, AppBuilder


class CustomSsoSecurityManager(SupersetSecurityManager):

    def oauth_user_info(self, provider, response=None):
        logging.info(""Oauth2 provider: {0}."".format(provider))
        if provider == 'okta':
            # As example, this line request a GET to base_url + '/' + userDetails with Bearer  Authentication,
            # and expects that authorization server checks the token, and response with user details
            res = self.appbuilder.sm.oauth_remotes[provider].get('https://dev-514411.okta.com/oauth2/default/v1/userinfo')
            logging.info("" {0}"".format(res))
            if res.status != 200:
                logger.error('Failed to obtain user info: %s', res.data)
                return
            logging.info(""user_data: {0}"".format(res))
            return {'name': res['firstName'], 'email': res['email'], 'id': res['login'], 'username': res['login'],
                    'first_name': '', 'last_name': ''}
        #  return {'name': 'neeraj', 'email': 'neeraj@gmail.com', 'id': 'neeraj@sparkflows.io', 'username': 'neeraj@sparkflows.io',
        #         'first_name': '', 'last_name': ''}


# Superset specific config
ROW_LIMIT = 5000
AUTH_USER_REGISTRATION = True
AUTH_USER_REGISTRATION_ROLE = 'Admin'
AUTH_ROLE_ADMIN = 'Admin'
AUTH_ROLE_PUBLIC = 'Admin'
WTF_CSRF_EXEMPT_LIST = ['']
# Flask App Builder configuration
# Your App secret key
SECRET_KEY = '\2\1thisismyscretkey\1\2\e\y\y\h'

AUTH_TYPE = AUTH_OAUTH
OAUTH_PROVIDERS = [{
    'name': 'okta',
    'token_key': 'access_token', # Name of the token in the response of access_token_url
    'icon':'fa-circle-o',   # Icon for the provider
    'remote_app': {
        'client_id': '0oa8hoe9t1c8555666091z357',  # Client Id (Identify Superset application)
        'client_secret': 'b8exxJID0BQOXlvMl1234565frU4OY7FX3cXDOMLM', # Secret for this Client Id (Identify Superset application)
        'client_kwargs': {
            'scope': 'openid'
        },
        'access_token_method': 'POST',    # HTTP Method to call access_token_url
        'access_token_headers': {    # Additional headers for calls to access_token_url
                'Authorization': 'Basic MG9hOGhvZTl0MWM4THhCMXozNTc6YjhleHhKSUQwQlFPWGx2TWxRYTVUbzVmclU0T1k3RlgzY1hET01MTQ=='
            },
        'base_url': 'https://dev-514411.okta.com/oauth2/default/',
        'authorize_url': 'https://dev-514411.okta.com/oauth2/default/v1/authorize',
        'access_token_url': 'https://dev-514411.okta.com/oauth2/default/v1/token',
        'redirect_uris': ['http://127.0.0.1:8088/oauth-authorized/okta']
    }
}]

CUSTOM_SECURITY_MANAGER = CustomSsoSecurityManager

Whenever i try to login. It gives below error message : 
![image](https://user-images.githubusercontent.com/30068633/113552352-ad3e0500-9613-11eb-86c3-98cebed5edab.png)

![image](https://user-images.githubusercontent.com/30068633/113552456-ce065a80-9613-11eb-8b23-535f416c61ee.png)

I got stuck here. i followed the steps mentioned into superset configuration settings. Can someone help me here please ?

",,,dpgaspar,"
--
@vinit2580,

Note that OKTA will be supported out of the box on FAB on 3.2.2 to be release this week. So you don't have to write your own custom security class

--
",,,,,,,,,,
13944,OPEN,User can see dataset without permission in chart creating page,#bug,2021-04-06 03:00:08 +0000 UTC,thangman22,In progress,,"### Expected results

Users can access only allow data set. If you see on the screenshot it should available only campaign_30067_posts. In another screenshot, I can change only the dataset that I have permission

### Actual results

User is possible to see no access dataset in create chart menu even it disappear from dataset menu but not in another page

#### Screenshots

![Screen Shot 2564-04-03 at 22 27 54](https://user-images.githubusercontent.com/806893/113483467-bc1a9f80-94cd-11eb-8fb2-0874944dc96b.png)

![Screen Shot 2564-04-03 at 22 42 24](https://user-images.githubusercontent.com/806893/113483509-026ffe80-94ce-11eb-833f-da3ed8d5d7b3.png)

#### How to reproduce the bug

1. Go to 'Charts'
2. Click on ' + Chart'
3. Click 'Choose a dataset'
4. See not allow dataset on dropdown menu

### Environment

- superset version: `1.1.0`
- node.js version: `14`
- Run via docker-compose

",,,mayurnewase,"
--
If you try to explore, it will show ""not authorized"".
--

--
@junlincc if this is a valid issue, I can fix it by only listing data sources which user has access to.
--
",thangman22,"
--
@mayurnewase even it is not work in explore by listing the none access dataset is not secure
--
",,,,,,,,
13943,OPEN,Filters should be considered handled when jinja templates uses them,#bug,2021-04-03 14:18:50 +0000 UTC,cccs-jc,Opened,,"Virtual tables let you use jinja templates to apply your own filtering. For example take this SQL query which finds all the superiors to of an employee. The filter has to be applied inside the WITH RECURSIVE. So it's a great application of jinja templating.

```

WITH RECURSIVE superiors(employee_id, manager_id, full_name, level) AS (
	SELECT
		employee_id,
		manager_id,
		full_name,
    1 as level
	FROM
		employees
	WHERE
		full_name in ( {{ ""'"" + ""', '"".join(filter_values('full_name')) + ""'"" }} )
	UNION ALL
		SELECT
			e.employee_id,
			e.manager_id,
			e.full_name,
      s.level + 1 as level
		FROM
			employees e,
		  superiors s 
		WHERE s.manager_id = e.employee_id
) 
SELECT
	employee_id, manager_id, full_name, level, concat(repeat('....', level), full_name) as description
FROM
	superiors

```


The pie chart shows some statistics about employees. When you click on a given employee cross filtering sends the filter to the table. The jinja template inserts the user to filter inside the WITH RECURSIVE.

![jinja-filter-virtual-table](https://user-images.githubusercontent.com/56140112/113480915-d2404580-9464-11eb-89a6-eee5d15030cf.png)

However the result is incorrect. It only shows the employee and none of the superiors. Looking at the SQL generated explains why.

```

SELECT employee_id AS employee_id,
       manager_id AS manager_id,
       level AS level,
                description AS description,
                full_name AS full_name
FROM
  (WITH RECURSIVE superiors(employee_id, manager_id, full_name, level) AS
     (SELECT employee_id,
             manager_id,
             full_name,
             1 as level
      FROM employees
      WHERE full_name in ('Frank Tucker')
      UNION ALL SELECT e.employee_id,
                       e.manager_id,
                       e.full_name,
                       s.level + 1 as level
      FROM employees e,
           superiors s
      WHERE s.manager_id = e.employee_id ) SELECT employee_id,
                                                  manager_id,
                                                  full_name,
                                                  level,
                                                  concat(repeat('....', level), full_name) as description
   FROM superiors) AS virtual_table
WHERE full_name IN ('Frank Tucker')
LIMIT 10000;
```

Jinja applied the filter correctly however Superset applied the default behaviour which is to apply a filter to the result of the virtual table. I believe that in most cases if a filter is handled by jinja you probably don't want the default behaviour.

This can be worked around by renaming the column `full_name` to something like `display_full_name` and setting the sql expression to be `full_name`. Effectively hiding the column from Superset's default handling.

![trick](https://user-images.githubusercontent.com/56140112/113481043-61e5f400-9465-11eb-970f-dc5e854b24b4.png)
 
And now you get the correct results.

![correct-results](https://user-images.githubusercontent.com/56140112/113481049-6ad6c580-9465-11eb-906a-032ce7ee317d.png)

The query is properly rendered

```

SELECT employee_id AS employee_id,
       manager_id AS manager_id,
       level AS level,
                description AS description,
                full_name AS full_name
FROM
  (WITH RECURSIVE superiors(employee_id, manager_id, full_name, level) AS
     (SELECT employee_id,
             manager_id,
             full_name,
             1 as level
      FROM employees
      WHERE full_name in ('Frank Tucker')
      UNION ALL SELECT e.employee_id,
                       e.manager_id,
                       e.full_name,
                       s.level + 1 as level
      FROM employees e,
           superiors s
      WHERE s.manager_id = e.employee_id ) SELECT employee_id,
                                                  manager_id,
                                                  full_name,
                                                  level,
                                                  concat(repeat('....', level), full_name) as description
   FROM superiors) AS virtual_table
LIMIT 10000;
```

However this is a bit of a hack. You can't use the `full_name` filter in the explore because Superset does not know of it. I would propose that filters handled by jinja be marked as ""handled"" and not post processed by the sqla/models.py. Or that there be a way to mark them as ""handled"" from the jinja template.


",,,,,,,,,,,,,,
13942,OPEN,[JINJA]Filterbox should support alias columns,,2021-04-04 15:03:20 +0000 UTC,pkdotson,Opened,,"The filterbox currently has not mapping when a user uses an alias for columns for example:

```SELECT  num_boys as from_value,  num_girls as to_value, name as the_name from birth_names```

When the user creates a virtual dataset in sql lab the mapping to the alias names gets lost because the original columns aren't passed in from SQL Lab. This in turn returns a incompatible filter to the filter indicators as shown from below:

<img width=""1189"" alt=""Screen Shot 2021-04-01 at 1 06 12 AM"" src=""https://user-images.githubusercontent.com/17326228/113455156-c8fc9d80-93be-11eb-8fd2-00462ae4f690.png"">


**Describe the solution you'd like**
During dataset creation from sql lab editor we need to save the original column as well as the aliases. We need a way to map these two.

cc: @zhaoyongjie 

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
",,,zhaoyongjie,"
--
![image](https://user-images.githubusercontent.com/2016594/113512951-bee2c680-9599-11eb-9df8-c72e8c6982a6.png)

Since virtual dataset and physical dataset is different dataset, 1) we shoud create the filterbox via the virtual dataset instead of physical dataset, 2) If we need to affect the virtual dataset by creating a filterbox on the physical dataset, I need to change the virtual dataset subquery.

cc: @junlincc @pkdotson  









--
",,,,,,,,,,
13938,OPEN,"SQL Lab fails to detect select statement when using ""WITH RECURSIVE""",#bug,2021-04-08 20:38:27 +0000 UTC,cccs-jc,Opened,,"A clear and concise description of what the bug is.

In the sql lab view using the examples postgesql database. Enter the following statements:

```
select n from (values(1)) as t(n)
```
Works fine

```
WITH t(n) AS ( VALUES (1) )
SELECT n FROM t
```
Works fine

```
WITH RECURSIVE t(n) AS (
    VALUES (1)
  UNION ALL
    SELECT n+1 FROM t WHERE n < 100
)
SELECT sum(n) FROM t
```
Returns an error ""Only `SELECT` statements are allowed against this database""

Debug the code a bit. The sqlparser can't determine the type of query. sql_parser.py line 133

    def is_select(self) -> bool:
        return self._parsed[0].get_type() == ""SELECT""

The type returned is 'UNKNOWN'

Forcing the is_select(self) to return True will make the query execute correctly. So it's is just the check that is failing.

WITH RECURSIVE is very handy in parent/child and graph datasets.

",,,cccs,"
--
The same issues occurs when using `SET @myvariable` statements. Superset fails to recognize it as a select query.

```
SET @slope = (select (sum(xy)-sum(received_on)*sum(count))/(sum(x2)-power(sum(received_on),2)) as slope from
(select received_on, count, received_on * count as xy, power(received_on, 2) as x2
from table1
group by received_on) T1);

SET @offset = (select (sum(count)-sum(received_on)*@slope)/count(*) as offset
from table1);

select date(received_on) as date, @slope*received_on+@offset as trendline from table1;
```
--
",,,,,,,,,,
13935,OPEN,Issue with logs table - user_id is NULL,P1; bug:regression; infra:logging,2021-04-07 00:16:05 +0000 UTC,zuzana-vej,Opened,,"Since 03-11 we are seeing user_id in the Superset logs table is NULL. 

This regression was likely caused by https://github.com/apache/superset/pull/13441 since it correlates exactly with our release on 3-10 which included this PR.
This issue is happening when filtering on `action  = 'save_dash'`

Here is a screenshot from the logs table:
![Screen Shot 2021-04-02 at 9 36 31 AM](https://user-images.githubusercontent.com/61221714/113435114-0816f880-9397-11eb-923c-2d9598db7435.png)

The dashboard was in fact edited on the days when it says NULL and the user_id who eddited the dashboard is correctly listed in the dashboards table changed_by_fk

![Screen Shot 2021-04-02 at 9 38 46 AM](https://user-images.githubusercontent.com/61221714/113435267-4c09fd80-9397-11eb-861a-691feed69ee4.png)
",,,zuzana,"
--
Cc: @junlincc @hughhhh @betodealmeida @graceguo-supercat 
--

--
The user_id NULL only happens for few dashboard events; the save_dash is the one which is a bit problematic because it's the log of who edited the dashboard when. 
These events have NULL user_id (but some not every time)
![Screen Shot 2021-04-05 at 4 47 41 PM](https://user-images.githubusercontent.com/61221714/113639969-429ac280-962f-11eb-82f4-f87ab44b5183.png)

--
",eschutho,"
--
@hughhhh is looking into this
--
",,,,,,,,
13930,OPEN,Add support of MongoDB to Superset via a new connector,,2021-04-08 16:30:38 +0000 UTC,slim-patchy,Opened,,"I am interested in implementing a new connector for Superset to add MongoDB support, and would like to know if others have started similar work too so as to avoid duplicate effort. 

### Inspiration
@priyankajuyal I think there're 3 methods to support mongodb in superset:

- Implement a mongodb in sqlalchemy, this requires a lot of work and may not be easy to do cause sqlalchemy is a object relational mapper library, while mongodb is not relational.
- Implement a new connector for superset. Right now superset has two connectors, one is for sqlalchemy, one is for druid, and similar things can be implemented for mongodb, this should be the right choice.
- Transform mongodb to some relational db, stripe has a deprecated project for this job: https://github.com/stripe/mosql.

_Originally posted by @xiaohanyu in https://github.com/apache/superset/issues/4231#issuecomment-358608747_",,,srinify,"
--
Hey @slim-patchy right now adding new connectors to Superset requires a SQLAlchemy + Python DB-API 2.0 compatible driver. Does Mongo have one?

https://preset.io/blog/building-database-connector/
--
",slim,"
--
Thanks @srinify for your response! 

I am not sure if the official MongoDB Python driver called `pymongo` is PEP 249 compliant. But I have been thinking that to make a new Superset connector work for MongoDB will have a slight different set of requirements, or even some schema-based mechanisms to bridge MongoDB with RDBs (instead). I have seen some academic papers discussing the nuances regarding this latter point. So I will need to really work out the concept behind such a connector first before I do any serious coding though. 
--

--
I have seen people attempted to come up with something based on the same idea with vastly different approaches. This appear to be something to be discussed  / thought through really thoroughly. Stripe's archived project called `MoSQL` is one such more promising examples of others' previous attempts: https://github.com/stripe-archive/mosql, though that is written in Ruby. I think I could probably borrow some ideas for the PR concept from them. 


--
",,,,,,,,
13929,OPEN,Internal Server Error occur in Treemap with multi group by filed,#bug,2021-04-02 06:08:47 +0000 UTC,poleft,Opened,,"
### Expected results

TreeMap visualization with 3 level

### Actual results

Internal Server Error

#### Screenshots



#### How to reproduce the bug

1. Go to 'Chart'
2. Click on 'TreeMap'
3. select labels more than 2 in groupBy field",,,,,,,,,,,,,,
13928,OPEN,Load Chart out of Dashboard,,2021-04-02 07:30:02 +0000 UTC,CaptainHoangLe,Opened,,"**Is your feature request related to a problem? Please describe.**
I am using superset to show my dashboard on mobile devices but my users want to a display the same as mobile app view not mobile web view . I created a field in table dashboard to check type display of dashboard is ""mobile_layout"" or ""web_layout"",
after that , i created a MenuRight of dashboard like below: 
![image](https://user-images.githubusercontent.com/53038465/113379781-a198d700-93a4-11eb-979f-7014f15c1872.png)

All of things i want is load chart filter_box on this right menu. I am having in trouble with how to load and save chart for each dashboad.
Please any one give me a solution .Thanks for your help <3 
",,,,,,,,,,,,,,
13925,OPEN,Unnecessary horizontal scrollbar in filter,#bug; bug:cosmetic; viz:dashboard:filterbox,2021-04-02 00:04:44 +0000 UTC,kristw,Opened,,"There is an unnecessary horizontal scrollbar in the filter in a superset dashboard

### Expected results

No scrollbar

### Actual results

visible horizontal scrollbar when it is not necessary.

#### Screenshots

![Pasted_Image_4_1_21__5_01_PM](https://user-images.githubusercontent.com/1659771/113365746-fb998e00-930b-11eb-8848-29d2efbefcdf.png)

#### How to reproduce the bug

1. Create dashboard
2. Create wide filterbox.

### Environment

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,,,,,,,,,,,,
13924,OPEN,[chart]Big number display tooltip with incorrect time granularity in dashboard,#bug; good first issue; viz:chart-number; viz:dashboard:ui,2021-04-02 00:04:21 +0000 UTC,kristw,In progress,,"## Expected results

A big number chart is added to a dashboard with ""time grain"" override. 
The tooltip of the big number chart in the dashboard should show the data point with respect to the selected ""time grain"" in dashboard.
For example,
* When select `week`, the tooltip should show `start date -- end date`, e.g. 2021-04-19 -- 2021-04-23.
* When select `day`, the tooltip should show `date`, e.g. 2021-04-19.

## Actual results

When looking at the chart on explore page, it works correctly.

![Pasted_Image_4_1_21__4_46_PM](https://user-images.githubusercontent.com/1659771/113365017-d3109480-9309-11eb-825c-10e6d3feef07.png)

However, the same chart in the dashboard behave unexpectedly.
* When select `week`, the tooltip show `month`, e.g. Mar 2020
* When select `day`, the tooltip shows `month`, e.g. Mar 2020.

![Pasted_Image_4_1_21__4_44_PM](https://user-images.githubusercontent.com/1659771/113364968-bbd1a700-9309-11eb-9159-7d6ff7b905e5.png)

#### How to reproduce the bug

1. Create a big number chart.
2. Put it in a dashboard with time grain override.
3. Set time grain.
4. Check tooltip.

### Environment

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Only the first chart's tooltip is wrong, all the other charts in the same dashboard are correct. ",,,kristw,"
--
I inspected the react component with chrome debugger and the first chart received ""P1M"" while the second chart received ""P1W"".

![Pasted_Image_4_1_21__4_53_PM](https://user-images.githubusercontent.com/1659771/113365356-d7897d00-930a-11eb-94b3-6a12af4c745e.png)

--
",junlincc,"
--
@kristw thanks for the details~ are you planning to fix it 

cc @EBoisseauSierra 
--
",,,,,,,,
13922,OPEN,[Design Proposal] Add Database Improvements,design:proposal; design:system,2021-04-01 23:45:19 +0000 UTC,Steejay,Opened,,"![Frame 284 (1)](https://user-images.githubusercontent.com/60786102/113363940-246b5480-9307-11eb-9ba1-e8213e4c0652.jpg)
_Dynamic form option (Default)_

![Frame 246](https://user-images.githubusercontent.com/60786102/113364112-888e1880-9307-11eb-919f-3c03679b59e4.jpg)
_SQLAlchemy URI option_

This mock introduces a UX update to the add database flow. The following updates have been introduced:

1. Users can select a database from a grid/list of available databases (installed db drivers)
2. A dynamic form will appear displaying all of the required fields for the specific database selected. The SQLAlchemy URI path is still available. The user may choose to connect a db using either the dynamic form _or_ the SQLAlchemy URI.
3. Once the user successfully completes the required fields and connects the db, they have the option to adjust any additional/optional fields before completing the process.


The motivation for this design includes some of the following:

1. There is no way a user knows what databases are currently supported when adding a new database until they try to connect and get an error that the driver is missing.
2. What fields are required versus optional are vague. Different databases have different requirements. Some may only require a URI while others might require additional information found in other tabs not currently marked w the required asterisk(*).

Here were some of the design goals:

1. Respect the user's time and attention
make the form as short and simple as possible

2. Minimize the number of form inputs
avoid asking other user to do unnecessary work

3. Indicate what is required and what is optional
indicate what fields must be filled out as courtesy and to prevent usability errors


",,,,,,,,,,,,,,
13920,OPEN,[Design]Import chart/dashboard UI icon is misleading,design:system; needs:design-input,2021-04-02 10:52:09 +0000 UTC,krsnik93,In progress,,"## Screenshot

![image](https://user-images.githubusercontent.com/13034472/113347164-b0dc3e00-932c-11eb-9ab9-3aa992a605a0.png)


## Description

Superset uses a ""download"" type icon for importing chart/dashboard. Problem is compounded by the absence of text next to the icon, as seen for creating a chart immediately to the left of it.

Instead of using a ""download"" icon (https://fontawesome.com/icons?d=gallery&p=2&q=download), Superset should be using either an ""upload"" icon (https://fontawesome.com/icons?d=gallery&p=2&q=upload) or an ""import"" icon (https://fontawesome.com/icons?d=gallery&p=2&q=import).

Adding a text label should be considered as well.
",,,junlincc,"
--

<img width=""1448"" alt=""Screen Shot 2021-04-01 at 4 15 19 PM"" src=""https://user-images.githubusercontent.com/67837651/113363453-bbcfa800-9305-11eb-8566-b1be377abfd3.png"">

not only the icon is confusing. it's not aligned with 2 buttons 

cc @rusackas @steejay 
--
",Steejay,"
--
Thanks for this feedback @krsnik93. Agreed, the icon should at least have a tooltip to help w understanding the action. We found some examples that interchanged some of the common icons associated with the dualities: export/import and download/upload. While selecting this icon we wanted to make sure that its opposite action (export) had an icon that was the inverse so that the relationship between the 2 was clear from purely a visual standpoint. With that being said we should take a further look into this to help clarify the action. Thank you @krsnik93 !

<img width=""181"" alt=""Screen Shot 2021-04-01 at 5 19 06 PM"" src=""https://user-images.githubusercontent.com/60786102/113366553-6055e800-930e-11eb-9f22-7ad580cae0a7.png"">

--
",krsnik93,"
--
I see your point with wanting to have icons paired. However, just from these 2 screenshots, icons should at least be switched, so from:

- download icon representing import button (my screenshot)
- upload button representing export button (your screenshot)

to:

- download icon representing export button
- upload button representing import button
--
",,,,,,
13914,OPEN,[Question]CLI commands import_* and export_*,bug; dashboard:export; dashboard:import; question,2021-04-01 20:53:53 +0000 UTC,krsnik93,In progress,,"I am trying to develop a procedure for moving charts, dashboards and other resources from one instance to another.

I started with the simplest attempt: export dashboards from one superset instance and import them back in.

Running `superset export-dashboards` creates a file called `dashboard_export_YYYYMMDDTHHMMSS`. The file is a .zip archive, which is difficult to know because the extension is missing. The `YYYYMMDDTHHMMSS` part of it is redundant and should be dropped. Upon unzipping the file, the contents are directories `charts`, `dashboards`, `databases`, `datasets` and a file called `metadata.yaml`. So even though the command is called `export-dashboards`, it actually exports other resource types as well. Not sure it's needed, certainly is unclear.

Importing back a dashboard with:
 `superset import-dashboards --path dashboard_export_20210401T140536/dashboards/USA_Births_Names.yaml`
gives:
```
File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/superset/cli.py"", line 318, in import_dashboards
    ImportDashboardsCommand(contents).run()
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/superset/dashboards/commands/importers/dispatcher.py"", line 69, in run
    raise exc
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/superset/dashboards/commands/importers/dispatcher.py"", line 58, in run
    command.run()
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/superset/dashboards/commands/importers/v0.py"", line 325, in run
    self.validate()
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/superset/dashboards/commands/importers/v0.py"", line 335, in validate
    json.loads(content)
  File ""/usr/lib64/python3.8/json/__init__.py"", line 357, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib64/python3.8/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib64/python3.8/json/decoder.py"", line 355, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
```
So it expects a .json but receives a .yaml as exported.

Then I tried a similar approach with `export-datasources` and `import-datasources`. Exporting also creates a .zip archive with the extension missing. Unzipping gives a bunch of .yaml files. Importing back with:
`superset import-datasources --path dataset_export_20210401T141602/datasets/examples/random_time_series.yaml`
gives:
```
Traceback (most recent call last):
  File ""/home/fedora/superset/venv/bin/superset"", line 11, in <module>
    load_entry_point('apache-superset==1.0.1', 'console_scripts', 'superset')()
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/flask/cli.py"", line 586, in main
    return super(FlaskGroup, self).main(*args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/decorators.py"", line 21, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/flask/cli.py"", line 426, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/decorators.py"", line 21, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/flask/cli.py"", line 426, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
TypeError: import_datasources() got an unexpected keyword argument 'sync'
```

### Expected results

Expected to be able to import/export unchanged files smoothly.

### Actual results

I might be using it wrong, but there seem to be many problems with this. File naming etc. is minor but the elementary .yaml/.json mismatch is really weird.

#### How to reproduce the bug

Described above in great detail.

### Environment

- superset version: 1.0.1
- python version: 3.8.7


### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

I have encountered a similar problem when using REST API  instead of the CLI commands:
https://github.com/apache/superset/issues/13239",,,junlincc,"
--
@betodealmeida Hi Beto, I believe this is in you domain. 
--
",betodealmeida,"
--
@krsnik93, this is not ""completely broken"", and I feel that that language is disrespectful.

The new export produces a ZIP file. The contents of the ZIP file include related objects needed to fully import what was exported. For a dashboard, that means charts, datasets and databases. This is described in https://github.com/apache/superset/issues/11167.

The reason why the filename includes the date is to help differentiate between exports. Exports can contain multiple objects (you could export 100 dashboards with all the associated charts, datasets and databases, and it would still be a single file), so we can't name the file based on the exported object.

The ZIP is an atomic bundle. You're not supposed to unzip it and import individual YAML files. You have to import the whole ZIP either in the UI or in the command line.

The reason why you can import YAML in the command line and UI is because in the original export/import we would use JSON for dashboards, and YAML for datasets.

This is the expected behavior:

1. The filename should have the `.zip` extension, so that's a bug.
2. If you export a ZIP file from the command line you should be able to import that unmodified file, either from the UI or from the CLI. If it doesn't work, it's a bug.
3. If you try to import an old JSON or YAML file it should work, both from the UI as well as CLI. If it doesn't work it's a bug.
--

--
@krsnik93 thanks, looks like the CLI has a bug when importing the ZIP, we need to implement the logic that's used in the API to load the contents of the file. I can fix it in the next few days and test to make sure it works. We've been using this feature only in the UI.
--

--
> I don't have a problem with differentiating exported files with a timestamp included in the filename. However, the filename does not have an actual timestamp but a placeholder `YYYYMMDDTHHMMSS`. Only after unzipping it does the resulting root directory have an actual timestamp in its filename. This is most likely another bug then.

Oh, yeah, that's a bug for sure.
--

--
Thanks for filing the issue, @krsnik93. I have a PR out fixing it.
--
",krsnik93,"
--
Sorry for not including in the description as I assumed it was not the way to use it, but I did try to import the whole archive first.

For `superset import-dashboards --path dashboard_export_YYYYMMDDTHHMMSS` (so using the archive ""untouched""):
```
Traceback (most recent call last):
  File ""/home/fedora/superset/venv/bin/superset"", line 11, in <module>
    load_entry_point('apache-superset==1.0.1', 'console_scripts', 'superset')()
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/flask/cli.py"", line 586, in main
    return super(FlaskGroup, self).main(*args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/decorators.py"", line 21, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/flask/cli.py"", line 426, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/decorators.py"", line 21, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/flask/cli.py"", line 426, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/superset/cli.py"", line 316, in import_dashboards
    contents = {path: open(path).read()}
  File ""/usr/lib64/python3.8/codecs.py"", line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x97 in position 14: invalid start byte
```
And for `superset import-datasources --path dataset_export_YYYYMMDDTHHMMSS`:

```
Traceback (most recent call last):
  File ""/home/fedora/superset/venv/bin/superset"", line 11, in <module>
    load_entry_point('apache-superset==1.0.1', 'console_scripts', 'superset')()
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/flask/cli.py"", line 586, in main
    return super(FlaskGroup, self).main(*args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/decorators.py"", line 21, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/flask/cli.py"", line 426, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/decorators.py"", line 21, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/flask/cli.py"", line 426, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""/home/fedora/superset/venv/lib64/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
TypeError: import_datasources() got an unexpected keyword argument 'sync'
```
--

--
> The reason why the filename includes the date is to help differentiate between exports. Exports can contain multiple objects (you could export 100 dashboards with all the associated charts, datasets and databases, and it would still be a single file), so we can't name the file based on the exported object.

I don't have a problem with differentiating exported files with a timestamp included in the filename. However, the filename does not have an actual timestamp but a placeholder `YYYYMMDDTHHMMSS`. Only after unzipping it does the resulting root directory have an actual timestamp in its filename. This is most likely a bug as it makes differentiating impossible.
--
",,,,,,
13913,OPEN,"BUG: Filterbox; when ""allow multiple selections"" AND ""required"" are enabled; then ""required"" doesn't work",#bug,2021-04-08 17:22:03 +0000 UTC,VladislavJansky,In progress,,"As the title explains:

when ""allow multiple selections"" AND ""required"" are enabled in the filter box settings (filter configuration), then ""required"" doesn't work.

",,,amitmiran137,"
--
FilterBox is deprecated and you should start evaluating DASHBOARD_NATIVE_FILTERS by toggling the ff, it is close to maturity and will be released in the 1.2 version 

If you start providing feedback about missing abilities in the native filters it is more likely you'll get a solution to your requests
--
",VladislavJansky,"
--
Thank you for your reply.
Are there any other handy charts hidden in the dark realm of the config file?

Kifflom brother.
--
",,,,,,,,
13912,OPEN,[Alert]Increase the resolution of alert email screenshot,global:alert,2021-04-02 01:01:03 +0000 UTC,fancyfreeman,In progress,,"## Description
The screenshot in the alert email  is very fuzzy. The size of the screenshot image is only 800*600, can I make it clear


",,,junlincc,"
--
@fancyfreeman my understanding is you wanna push a PR to make the enhancement  YES! 
--
",fancyfreeman,"
--
I'm sorry I didn't make it clear. I'm not an experienced Python programmer. I found a temporary solution. Modify the file located in site-package/superset/utils/screenshots.py
https://github.com/apache/superset/blob/956f276e70cd167cd62f6a65585dd88830821d0c/superset/utils/screenshots.py#L206
Change the WindowSize to a larger size, maybe 3200,2400 is OK .
--
",,,,,,,,
13909,OPEN,"[Question]How to let the null value of line chart to be ""0""",question,2021-04-02 01:29:09 +0000 UTC,riceliu,Opened,,"As flowing screenshot, how can I make the null value to be zero value?

![image](https://user-images.githubusercontent.com/5362719/113251751-67bcc780-92f5-11eb-8ea3-0bd9b728fa90.png)
",,,zhaoyongjie,"
--
Hey, @riceliu 

define a adhoc metric by follow snippet
```
sum(case ham is null then 0 else ham end)
```
--
",,,,,,,,,,
13907,OPEN,Fetch dashboard not happening properly with different type of login,#bug,2021-04-05 11:09:19 +0000 UTC,madhamanchiharsha,Opened,,"1. login user who have admin rights in browser and fetch multiple dashboard's.
2. now login same user in postman with (bearer token type authentication) and now try to call same api for multiple dashboards. 

### Expected results
both web and postman should fetch same data or same number of dashboards.

### Actual results
in web all dashboards are fetched properly. whereas in postman only world bank data dashboard is fetched.


### Environment

- superset version: `1.0`
- python version: `python 3.8`

",,,dpgaspar,"
--
@madhamanchiharsha 

Can you add more details, your getting the JWT token on `api/v1/security/login` first? with provider `db`?

--
",,,,,,,,,,
13903,OPEN,[SQL Lab] Expand/Collapse Arrow is not working,good first issue; sql_lab,2021-04-01 18:47:46 +0000 UTC,jinghua-qa,Opened,,"SQL Lab - Expand/Collapse arrow is not working in left sidebar

#### How to reproduce the bug

1. Select a table from SQL Lab
2.Click expand/collapse arrow

### Expected results

On expand/collapse: should show/hide table columns

### Actual results

 Expand/Collapse arrow is not doing anything, user need to click on table name to show/hide table columns


<img width=""1622"" alt=""Screen Shot 2021-03-31 at 4 49 22 PM"" src=""https://user-images.githubusercontent.com/81597121/113237806-6095ab00-925c-11eb-9039-7b6ec1d884cc.png"">

### Environment

superset version:  with (Preset Cloud Beta 3/31/2021 - Test on Development)

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

 

- [x]  I have reproduced the issue with at least the latest released version of superset.
- [x]  I have checked the issue tracker for the same issue and I haven't found one similar.


",,,junlincc,"
--
thank you for filing your first issue! @jinghua-qa 
to whomever is gonna tackle this `good first issue`, please check all area listed in 
https://github.com/apache/superset/pull/12920

cc @amitmiran137 
--
",,,,,,,,,,
13902,OPEN,[listview] not able to show properly after removing search term,#bug; global:listview; good first issue,2021-04-01 18:43:59 +0000 UTC,rosemarie-chiu,Opened,," ### Repo steps:
1. Go to Dashboards list view
2. Enter search term (that has zero result) and hit search
3. Instead of clicking X to remove the search term, delete string with delete button

### Expected results
Dashboard list should show again as if no search term was entered

### Actual results
Stuck on empty dashboards list. The only way out is go to other page then come back or refresh


https://user-images.githubusercontent.com/52086618/113227553-ca578a00-9247-11eb-98bd-96baf2188366.mov

",,,junlincc,"
--
Thank you for filing the first issue  @rosemarie-chiu 
I believe it never worked as expected.  tagged `good first issue `
--
",,,,,,,,,,
13900,OPEN,Connecting to external Redis,,2021-03-31 22:35:15 +0000 UTC,raefezzeldin,Opened,,"I have deployed an external redis app on the same name space: but it keeps giving me the following error

ERROR/MainProcess] consumer: Cannot connect to redis://redis-headless:6379/0: Authentication required..",,,,,,,,,,,,,,
13898,OPEN,Allow for bulk select for exporting databases,,2021-03-31 22:05:17 +0000 UTC,hughhhh,Opened,,"**Is your feature request related to a problem? Please describe.**
Just like datasets has a bulk delete, it would be cool to have a bulk selection when exporting databases.
",,,,,,,,,,,,,,
13889,OPEN,[SQL Lab] Icons on left control panel are hidden when table name is long,bash!; bug:cosmetic; sql_lab:control:ui,2021-03-31 17:57:18 +0000 UTC,zuzana-vej,Opened,,"## Screenshot

Issue - can't see close from preview.
![Screen Shot 2021-03-31 at 10 51 36 AM](https://user-images.githubusercontent.com/61221714/113188801-5ac5a880-920f-11eb-946b-81e9dc4562be.png)


Correct - table with shorter name
![Screen Shot 2021-03-31 at 10 51 45 AM](https://user-images.githubusercontent.com/61221714/113188815-5ef1c600-920f-11eb-86bf-9b4c0b5048a9.png)



## Description

If table name is long, some of the controls are hidden - in the example from the screenshot, user can't see the ""close from preview"" icon.
",,,,,,,,,,,,,,
13881,OPEN,[SQL Lab] Resizing browser window cuts off result without adding scroller,bug:cosmetic; sql_lab:control:ui,2021-03-31 18:00:18 +0000 UTC,zuzana-vej,Opened,,"## Screenshot

<img width=""441"" alt=""Screen Shot 2021-03-30 at 9 09 10 PM"" src=""https://user-images.githubusercontent.com/61221714/113089315-43010c80-919c-11eb-8ded-5d07eac3da36.png"">


![result-cropped](https://user-images.githubusercontent.com/61221714/113089255-2369e400-919c-11eb-8d82-3becc4626a5e.gif)


## Description

At the bottom of the SQL Lab result section, you can see there is one more record which isn't visible, and user has no ability to scroll to see it since there is no scroll bar on the side.

How to repro this:
1. run query in SQL Lab
2. resize Chrome window
3. see the issue

",,,junlincc,"
--
@eschutho 
--
",zuzana,"
--
Update: user is able to reproduce this even without resizing their browser window (e.g they see 14 rows instead of 15 (but 1/4 of the last row is visible).
--
",,,,,,,,
13879,OPEN,[csv] allow csv cell with separate line,enhancement:request,2021-03-31 01:41:54 +0000 UTC,graceguo-supercat,Opened,,"**Is your feature request related to a problem? Please describe.**
We have a lot of users who want to upload csv format data into Superset. But currently Superset's csv parser only accept _comma separated_ content.

**Describe the solution you'd like**
Right now modern tools (like Excel etc) can parse more complicated content. According to CSV standard https://en.wikipedia.org/wiki/Comma-separated_values:
`More sophisticated CSV implementations permit them, often by requiring "" (double quote) characters around values that contain reserved characters (such as commas, double quotes, or less commonly, newlines).`

Could we improve CSV support, which allows line break in the cell (use double quote around cell values)?
",,,junlincc,"
--
Thanks for the request! 
@yousoph This seems to be helpful and relatively small effort. I think we should include this in roadmap? 
--
",,,,,,,,,,
13867,OPEN,[todo]update apache/superset docs - install steps in dockerhub,need:validation,2021-03-31 07:23:45 +0000 UTC,preetdeepkumar,Opened,,"Team,

While following the steps as mentioned in https://hub.docker.com/r/apache/superset -> ""Initialize a local Superset Instance"", when I opened Superset via http://localhost:8080/login/, after login, I saw a lot of error messages pop up in UI.
![Capture](https://user-images.githubusercontent.com/10060103/112970573-f56cab80-916b-11eb-9a6f-f752d1e9f4a7.PNG)

After few tries when I changed the sequence by running ""Step 4 Setup roles"" before ""Step 3 Load Examples"", then everything worked as expected.
![Capture_After](https://user-images.githubusercontent.com/10060103/112970817-3cf33780-916c-11eb-835b-4029128d053f.PNG)

It will be good if you can swap Step 3 and Step 4 so that Load Examples is the last step.
",,,junlincc,"
--
thanks for pointing it out! 

@srinify could we validate the suggestions and update the doc accordingly? thanks!
--
",,,,,,,,,,
13866,OPEN,NotImplementedError for Opendistro,#bug,2021-04-01 19:01:53 +0000 UTC,Jimmy-Newtron,Opened,,"I want to create a chart **Big Number with Trendline** that must use a custom datetime column (different from @timestamp) field from an opendistro dataset

![image](https://user-images.githubusercontent.com/24460646/112965075-369f9400-9149-11eb-8aaf-cdaa92e2fc8d.png)
![image](https://user-images.githubusercontent.com/24460646/112964554-ba0cb580-9148-11eb-99f9-5fcbf288d558.png)
![image](https://user-images.githubusercontent.com/24460646/112965112-4028fc00-9149-11eb-9b24-d281b3cf14f8.png)

Using the **timestamp** field it gets the error while with the **firstEventEpochSession** it works correctly
![image](https://user-images.githubusercontent.com/24460646/112965016-28517800-9149-11eb-91d5-92267f513bc1.png)

### Expected results

Chart is able to render the data

### Actual results & screenshots

![image](https://user-images.githubusercontent.com/24460646/112962496-ba0bb600-9146-11eb-9fe5-5afa3ea444fb.png)

#### How to reproduce the bug

1. Create a dataset from an opendistro index (with a custom datetime field)
2. Create a chart of type **Big Number with Trendline**
3. Select the custom datetime field into the time section
4. Run the query & see error

### Environment

- helm chart (latest)

- superset version: `superset version`
```
root@superset-67bcb48445-hgvw9:/app# superset version
Loaded your LOCAL configuration at [/app/pythonpath/superset_config.py]
logging was configured successfully
INFO:superset.utils.logging_configurator:logging was configured successfully
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Superset 0.999.0dev
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
```

- python version: `python --version`
```
root@superset-67bcb48445-hgvw9:/app# python
Python 3.7.9 (default, Feb  9 2021, 08:33:04) 
[GCC 8.3.0] on linux
```

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Superset source code

https://github.com/apache/superset/blob/9fa52b50edaa2e42b7d064139ddb41d329aa1344/superset/db_engine_specs/base.py#L502

https://github.com/apache/superset/blob/9fa52b50edaa2e42b7d064139ddb41d329aa1344/superset/db_engine_specs/elasticsearch.py#L67",,,,,,,,,,,,,,
13865,OPEN,Can't sync column from source on complex queries,#bug,2021-03-30 07:07:20 +0000 UTC,ValentinC-BR,Opened,,"I can't use ""sync column from source"" on virtual datasets whose query doesn't start with SELECT.

### Expected results

Whatever my query is, I should be able to sync column from source for my virtual datasets.

### Actual results

When my query starts with SELECT, there's no problem.
When my query start with ""WITH"" statements : 
* I can create a virtual dataset using the ""EXPLORE"" feature in SQL Lab
* I can't update it and sync columns from source : I get the ""ONLY SELECT STATEMENTS ARE ALLOWED"" error

I'm forced to re-run the query in SQL Lab, and save is over the previous virtual dataset 

What's more, contrary to the previous version of superset (0.x.x), the time columns are not detected, but this seems to be a different issue.

#### Screenshots

![image](https://user-images.githubusercontent.com/79460908/112947293-e7506800-9136-11eb-87ac-900a30d68fc4.png)

#### How to reproduce the bug

1. Run a query in SQL Lab starting with a ""WITH"" statement
2. Click on Explore and create a virtual dataset
3. Go to dataset, update the query (you can add/remove/modify a column, anything works, even a mere filter)
4. Save
5. Click on 'Sync Columns from source' (it should get your new column list)
6. See error

### Environment

(please complete the following information):

- superset version: 1.0.1
- python version: 3.7.9
- node.js version: doesn't apply, I run on Kubernetes, using gunicorn as server
- source : Athena

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Nothing to report
",,,,,,,,,,,,,,
13864,OPEN,sqlalchemy-trino ERROR: Unexpected database format catalog/hive/,#bug,2021-03-30 16:00:12 +0000 UTC,b1600,In progress,,"Hello, I'm trying to create database connection from Superset to Trino.

What I've done:
1. Run ""echo ""sqlalchemy-trino"" >> ./docker/requirements-local.txt"" from my superset root folder to add trino database driver to superset.
2. Modify ""docker-compose-non-dev.yml"" to load docker/docker-bootstrap.sh as mentioned by @dungdm93 in https://github.com/apache/superset/issues/13640
3. Run ""docker-compose -f docker-compose-non-dev.yml up"" to start superset docker
4. Login to superset
5. And then I tried to add database connection to Trino using the following  connection string ""trino://my_username:my_password@trino_coordinator_ip:8090/catalog/hive/""
6. Click ""Test Connection"" button.

Btw, previously I've created external tables in hive that reference to delta table manifest. And Trino is used to access those hive table.

### Expected results

Able to add database connection to Trino and able to query Trino from superset.

### Actual results

I got the following error message ""ERROR: Unexpected database format catalog/hive/""

#### Screenshots

#### How to reproduce the bug

1. Login to superset UI
2. Click database tab, and then click +Database button
2. Fill in the trino connection string 
3. Click ""test connection"" button
4. See error pop-up on bottom right page

### Environment

(please complete the following information):

- superset version: docker version: latest
- python version: 
Python 2.7.5
Flask 1.1.2
Werkzeug 1.0.1


### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [] I have reproduced the issue with at least the latest released version of superset.
- [] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.

pip freeze output of worker:
aiohttp==3.7.2
alembic==1.4.3
amqp==2.6.1
'# Editable install with no version control (apache-superset==0.999.0.dev0)
-e /app
apispec==3.3.2
async-timeout==3.0.1
attrs==20.2.0
Babel==2.8.0
backoff==1.10.0
billiard==3.6.3.0
bleach==3.2.1
boto3==1.16.10
botocore==1.19.10
Brotli==1.0.9
cached-property==1.5.2
cachelib==0.1.1
celery==4.4.7
certifi==2020.6.20
cffi==1.14.3
chardet==3.0.4
click==7.1.2
colorama==0.4.4
contextlib2==0.6.0.post1
convertdate==2.3.0
cron-descriptor==1.2.24
croniter==0.3.36
cryptography==3.2.1
decorator==4.4.2
defusedxml==0.6.0
Deprecated==1.2.11
dnspython==2.0.0
email-validator==1.1.1
et-xmlfile==1.0.1
Flask==1.1.2
Flask-AppBuilder==3.2.1
Flask-Babel==1.0.0
Flask-Caching==1.9.0
Flask-Compress==1.8.0
Flask-Cors==3.0.9
Flask-JWT-Extended==3.24.1
Flask-Login==0.4.1
Flask-Migrate==2.5.3
Flask-OpenID==1.2.5
Flask-SQLAlchemy==2.4.4
flask-talisman==0.7.0
Flask-WTF==0.14.3
future==0.18.2
geographiclib==1.50
geopy==2.0.0
gunicorn==20.0.4
holidays==0.10.3
humanize==3.1.0
idna==2.10
ijson==3.1.2.post0
importlib-metadata==2.1.1
isodate==0.6.0
itsdangerous==1.1.0
jdcal==1.4.1
Jinja2==2.11.3
jmespath==0.10.0
jsonlines==1.2.0
jsonschema==3.2.0
kombu==4.6.11
korean-lunar-calendar==0.2.1
linear-tsv==1.1.0
Mako==1.1.3
Markdown==3.3.3
MarkupSafe==1.1.1
marshmallow==3.9.0
marshmallow-enum==1.5.1
marshmallow-sqlalchemy==0.23.1
msgpack==1.0.0
multidict==5.0.0
mysqlclient==1.4.2.post1
natsort==7.0.1
numpy==1.19.4
openpyxl==3.0.5
packaging==20.4
pandas==1.2.2
parsedatetime==2.6
pathlib2==2.3.5
pgsanity==0.2.9
Pillow==7.2.0
polyline==1.4.0
prison==0.1.3
psycopg2-binary==2.8.5
py==1.9.0
pyarrow==3.0.0
pycparser==2.20
pydruid==0.6.1
PyGithub==1.54.1
PyHive==0.6.3
PyJWT==1.7.1
PyMeeus==0.3.7
pymssql==2.1.5
pyparsing==2.4.7
pyrsistent==0.16.1
python-dateutil==2.8.1
python-dotenv==0.15.0
python-editor==1.0.4
python-geohash==0.8.5
python3-openid==3.2.0
pytz==2020.4
PyYAML==5.4.1
redis==3.5.3
requests==2.24.0
retry==0.9.2
rfc3986==1.4.0
s3transfer==0.3.3
sasl==0.2.1
selenium==3.141.0
simplejson==3.17.2
six==1.15.0
slackclient==2.5.0
SQLAlchemy==1.3.20
sqlalchemy-trino==0.2.0
SQLAlchemy-Utils==0.36.8
sqlparse==0.3.0
tableschema==1.20.0
tabulator==1.52.5
thrift==0.13.0
thrift-sasl==0.4.2
trino==0.305.0
typing-extensions==3.7.4.3
unicodecsv==0.14.1
urllib3==1.25.11
vine==1.3.0
webencodings==0.5.1
Werkzeug==1.0.1
wrapt==1.12.1
WTForms==2.3.3
WTForms-JSON==0.3.3
xlrd==1.2.0
yarl==1.6.2
zipp==3.4.0",,,b1600,"
--
I was just able to connect to Trino.

I've read from https://medium.com/airbnb-engineering/supercharging-apache-superset-b1a2393278bd and tried the following connection string ""presto://{trino_coordinator_ip}:{trino_coordinator_port}/hive"".

Btw I've also added ""pyhive"" and ""sqlalchemy-trino"" in my ""requirements-local.txt""
Could someone please tell me which database driver my connection string is referring to? Is it pyhive? or sqlalchemy-trino?

Sorry if my question sounded silly.
But I couldn't find connection string samples that start with ""presto://"" in https://superset.apache.org/docs/databases/trino/ and https://superset.apache.org/docs/databases/presto
--
",,,,,,,,,,
13857,OPEN,I'd love search in Superset to be fuzzier,enhancement:request; viz:dashboard:list,2021-04-02 22:39:39 +0000 UTC,srinify,Opened,,"**Is your feature request related to a problem? Please describe.**
I wanted to search for pie charts and I typed in ""pie "" (note the extra space). This returned 0 results. ""Pie Ch"" also returned 0 results. If I shorten back to just ""pie"" then it worked fine.

<img width=""1493"" alt=""Screen Shot 2021-03-29 at 5 11 16 PM"" src=""https://user-images.githubusercontent.com/801507/112900694-cf82d080-90b1-11eb-889e-b2ba579360c0.png"">

<img width=""1495"" alt=""Screen Shot 2021-03-29 at 5 10 46 PM"" src=""https://user-images.githubusercontent.com/801507/112900699-d0b3fd80-90b1-11eb-9919-cd1d8c24e331.png"">


**Describe the solution you'd like**
I wish search was fuzzier. ",,,junlincc,"
--
I have a similar request for the south data table in Explore. There's currently no consistency in all the filter bars in the product. but we should use the left data pane search as the standard, with relevancy ranking, debounce and search all objects etc. 
--
",,,,,,,,,,
13856,OPEN,Allow enable/disable Alerts and Reports frontend by user,global:alert,2021-04-05 17:23:55 +0000 UTC,graceguo-supercat,Opened,,"**Is your feature request related to a problem? Please describe.**
Superset `Alerts and Reports` is a good feature. We want to enable it in airbnb but we have performance, security and stability concerns, so at this moment we just want to open it to a few selected users before make it available to thousands of users.

Superset offers `GET_FEATURE_FLAGS_FUNC` to enable/disable features by selected user. So here are config that i tried to enable `Alerts and Reports` for selected users (**Note** I have set other necessary `CELERYBEAT_SCHEDULE` and SMTP related config):

1.  `ALERT_REPORTS = False` for all users, but enable selected users by `GET_FEATURE_FLAGS_FUNC`. Result: No users can see `Alerts & Reports` menu.
2.  `ALERT_REPORTS = True` for all users, but disable all other users in `GET_FEATURE_FLAGS_FUNC`, only allow selected users pass. Result: feature is enabled for selected users, but other users will still see `Alerts & Reports` option from the dropdown. But when they click on it, they got 404 error.

**Describe the solution you'd like**
I am not sure this is a feature request or a bug fix for use `ALERT_REPORTS ` flag. Could we have a feature flag just used to enable front-end (show `Alerts and Reports` option and related records)?

cc @nytai @dpgaspar @zuzana-vej 

",,,nytai,"
--
The issue here is that the alerts/reports menu item is controlled via a permission and the route existence is based on the feature flag, so this creates an issue when enabling this for only certain users.

- If the feature flag is conditionally set, the permission does not exist and the menu item will no be there. 
- If the permission is set for all users, but the feature flag is conditionally set, all users will have the menu option but the route will 404 for some. 

This issue is not isolated to this feature, it is an issue for all routes that are conditionally enabled based on feature flag. 

One solution is to generate the permission for all users, but hide the menu item in the frontend. The users will still have direct access to the feature via the url, but the link will be hidden. I'm wondering if @dpgaspar has any other ideas as this issue is related to FAB permissions. 


--
",dpgaspar,"
--
Like @nytai said, at the app bootstrap, the routes get registered on Flask. Routes that get registered depending on a feature flag won't get unregistered if the feature flag mutates during a request, for example depending on the user that made the request.

The menu REST endpoint result depends on the registered menus and the user's permission to them, this depends dynamically on the user that made the request and his permissions.

A possible way, don't know if it feasible on Airbnb:
By default Alerts & Reports permissions are added Admin and Alpha roles, further roles can be created like AlphaNoAlertReports and be given to those users. This can be done programatically, has well has the user registration to these roles



 


--
",graceguo,"
--
Hi @dpgaspar In airbnb we didn't really use much of Superset `roles`. In airbnb we had a few thousands of users in hundreds of teams/organizations, and we also had different data tools. Our data access is controlled in company level not in Superset. So we do not want Superset to manage extra `roles`, and we set everyone is `Alpha` or `Admin` role. 
--
",,,,,,
13850,OPEN,NULL; NULL does not get shown when grouping by a column in line chart,#bug; viz:chart-line,2021-03-29 23:15:23 +0000 UTC,bkyryliuk,Opened,,"Opening a clone of the https://github.com/apache/superset/issues/2110 per @junlincc  request.
More context can be found in that issue.

### Expected results

null, null values are shown on the line chart rather than omitted

### Actual results

![image](https://user-images.githubusercontent.com/5727938/112874335-ac3b2f80-9077-11eb-908d-d55e65ff6f07.png)


#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. create table
```
create table null_bug_report as 
SELECT substr(strftime('%Y', year),0,4) || ""0"" as decade, country_name, ""dim"" as dim, count(*) as count_per_year from wb_health_population where country_name in ('Ukraine', 'Germany') group by 1,2 
union select ""1960"", NULL, NULL, 100
```

2. add it as a datasource
3. define datetime virtual column

![image](https://user-images.githubusercontent.com/5727938/112874449-d1c83900-9077-11eb-9621-70ba66f172de.png)
4. create line chart over time with no datetime filter that is grouped by `country_name` and `dim`
5. 
![image](https://user-images.githubusercontent.com/5727938/112874716-1eac0f80-9078-11eb-908d-c262801da8d9.png)
6. notice that null, null value is not shown on the line chart


### Environment

latest master as of March 29 2021

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,,,,,,,,,,,,
13849,OPEN,[Data]ability to review dataset schema and bulk 'register' datasets,data:dataset; enhancement:request,2021-03-29 16:52:46 +0000 UTC,junlincc,Opened,,"Feature Request: I wish I could easily see all of the tables in a schema for a given database (and ideally be able to register like 10 or 20 datasets at a time). Exploring tables and finding ideas for analysis, charts, and dashboards is very challenging right now. The workflow we push people to is to add individual datasets, which isnt conducive to exploration or discovery.
Persona Affected: Data Analysts ore Explorer.  
Really anyone looking to play with data loaded by someone else and theres no documentation for this data.

@srinify @betodealmeida @yousoph ",,,,,,,,,,,,,,
13848,OPEN,Support custom SQL filters on calculated columns,viz:explore:control; viz:explore:filter,2021-03-29 16:17:07 +0000 UTC,etr2460,Opened,,"**Is your feature request related to a problem? Please describe.**
In Explore you can create custom SQL filters to apply to the query. However, referencing calculated columns from this filter doesn't work.

**Describe the solution you'd like**
Support filtering by calculated columns by mapping the calculated column name to it's SQL query before running the query

**Describe alternatives you've considered**
Alternatively, if supporting this isn't possible:
- add warnings that you cant use derived column names in the Custom SQL
- stop it from auto-populating derived fields when switching from Simple to Custom SQL

cc @junlincc ",,,,,,,,,,,,,,
13847,OPEN,[table]0 values instead on Null in table charts,#bug; viz:chart-table,2021-03-31 08:13:34 +0000 UTC,Sipondo,In progress,,"Upgraded Superset main branch, but now my Date columns in tables show the unix epoch time for Null instead of just N/A.
Values are correct (N/A) in the SQL Editor.

### Expected results

Null values

### Actual results

0 values (Unix epoch time 1970-01-01)

#### Screenshots

![image](https://user-images.githubusercontent.com/6222286/112863545-41a9e400-90b7-11eb-97c5-35508a7647d8.png)


#### How to reproduce the bug

1. Have a dataset with a date field that can be null
2. Make a table chart with the dataset
3. See issue

### Environment

(please complete the following information):

- superset version: Current main branch (as of writing)
- python version: 3.8

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x ] I have reproduced the issue with at least the latest released version of superset.
- [x ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Discussed on Slack: https://apache-superset.slack.com/archives/C016B3LG5B4/p1617025314056300
",,,junlincc,"
--
@Sipondo thanks for reporting, when was the last time it works/displays properly? 

--
",Sipondo,"
--
We pulled the most recent master branch last Friday. Our deployment before that was from the end of January.
--
",,,,,,,,
13846,OPEN,Adding calculated field to dataset : changing this dataset is forbidden,#bug,2021-03-29 12:42:43 +0000 UTC,helo-ch,Opened,,"SuperSet users want to add calculated fields to their datasets, but they run into an error :
`Changing this dataset is forbidden`

Users have the following roles :
* Gamma
* tables_modify : [can tables on Superset, can save on Datasource, can get on Datasource, can datasources on Superset, menu access on Tables, can fetch datasource metadata on Superset, can read on Dataset, can write on Dataset]
* DB_ACCESS : [database access on [their TimeScale DB], datasource access on [the dataset they want to edit]]
* sql_lab

Is there a missing permission for them to be able to edit their own datasets? They have read and write permission on it, and are the ones creating the datasets in the first place.

I looked into the database properties but could not find any setting there either. Moreover with admin role I am able to edit datasets so I guess what's missing here is a user role, I just can't find which one.

### Environment

- superset version: `1.0.1`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.


",,,,,,,,,,,,,,
13845,OPEN,javascript data interceptor,,2021-03-29 10:33:52 +0000 UTC,Riskatri,Opened,,"Hi, can I use the javascript data interceptor to get array values? Is there anyone here who knows an example / how to use it??
I want to display data from some values in a table that I have using the javascript tooltip generator, but the results I get are like this
![test](https://user-images.githubusercontent.com/59104917/112822220-ddd0ec80-90b1-11eb-9a72-e33bcf18977b.png)

even thought in the table  that I have, I have 2 data in the same location. I want to show all that data.
and this is the data from the query results

![image](https://user-images.githubusercontent.com/59104917/112822660-6ea7c800-90b2-11eb-9060-2b7046f4a7d1.png)

and this is my javascript tooltip generator 

![image](https://user-images.githubusercontent.com/59104917/112824589-ebd43c80-90b4-11eb-86e9-4d897e38d9a1.png)
",,,,,,,,,,,,,,
13844,OPEN,npm run build/dev question,#bug,2021-03-29 08:50:05 +0000 UTC,tlkzzz,Opened,,"version:1.0.1 master    /superset-frontend
CentOS Linux release 7.2.1511 (Core)
node: v14.5.0

execute npm run build   errors  

all are typescript syntax errors in the code

I hope you can retrieve the code and recompile it to see if there are the same compilation errors  thanks 



![112446431-25dad100-8d8b-11eb-85cf-0ca3449f9579](https://user-images.githubusercontent.com/10111244/112811192-63529d80-90ae-11eb-9a22-a4bb63b44ad0.png)
",,,,,,,,,,,,,,
13842,OPEN,[SQL Lab] Database and Table dropdowns disconnected,#bug,2021-03-29 06:28:40 +0000 UTC,yousoph,Opened,,"### Expected results
In SQL Lab, the db and table dropdown options should appear attached to the dropdown 

### Actual results
The db and table dropdown options are showing up disconnected/detached from the dropdown

#### Screenshots

![image](https://user-images.githubusercontent.com/10627051/112795144-f04d1680-901c-11eb-8888-70bc33c2cd5d.png)
![image](https://user-images.githubusercontent.com/10627051/112795173-fa6f1500-901c-11eb-8d75-9cf01cf7204b.png)


#### How to reproduce the bug

1. Go to SQL Lab
2. Click on the database dropdown 
3. Select a schema
4. Click on the table dropdown 

### Environment

- superset version: `master`


### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.",,,,,,,,,,,,,,
13838,OPEN,Move to monaco-editor,,2021-03-27 13:10:10 +0000 UTC,rumbin,Opened,,"### Current situation
There are several open issues that are related to the code editor, e.g. 
#12429
#9990

Switching to [monaco-editor](https://github.com/microsoft/monaco-editor) has been discussed several times and was mentioned as a post-1.0.0 task: #10254, #10267, #12423.

### Suggested change
Switch to monaco-editor, the editor behind VS Code.

Personally, I have worked with websites using this editor (dbt Cloud, dataform) and feel that it works very well.

### Alternative
Stick with the current editor or consider others.

",,,,,,,,,,,,,,
13835,OPEN,[listview]inconsistent filter dropdown component,bug:cosmetic; needs:design-input,2021-03-29 17:33:38 +0000 UTC,junlincc,In progress,,"
https://user-images.githubusercontent.com/67837651/112711829-8dc31180-8e88-11eb-81b7-5d434e8f212a.mov

Not sure if the design of listview filter dropdown is intentional.. 1)it's definitely not consistent with other dropdown cosmetic in the product 2)looks pretty broken 3) filterable items should be spaced out..... 

@steejay ",,,Steejay,"
--
@junlincc the designs should be consistent between all crud views. thanks for the recording. What filterable items are you referring to re spacing? The items in the dropdown or the spacing between filters in the bar?
--
",junlincc,"
--
the spacing between filters in the bar @Steejay  but it's your call! just my suggestion :) 
--
",,,,,,,,
13834,OPEN,[menu]combine upload CSV/Excel in menu,needs:design-input,2021-03-29 17:08:44 +0000 UTC,junlincc,Opened,,"
<img width=""592"" alt=""Screen Shot 2021-03-26 at 11 07 10 PM"" src=""https://user-images.githubusercontent.com/67837651/112711751-08d7f800-8e88-11eb-9f46-a5623529b3ea.png"">
""Current Menu - ""Upload Csv"" and ""Upload Excel"" looks clunky. I never used them before - but think there are identical format. Probably those one possible to merge into one form  ""Upload CSV/Excel"" and specify format inside the form by radio button. ""

cc @steejay ",,,Steejay,"
--
thanks @junlincc. we'll take a look into this. there are some differences between both forms but I agree that they could be consolidated better.
--
",,,,,,,,,,
13832,OPEN,[Explore]set default explore viz in user profile,viz:explore:ux,2021-03-27 05:25:36 +0000 UTC,junlincc,Opened,,"It would be great to allow setting this per user profile! (another reason to add meta storage for user configs)

_Originally posted by @ktmud in https://github.com/apache/superset/issues/13610#issuecomment-808568754_",,,,,,,,,,,,,,
13831,OPEN,[explore] Certified metric icons are various sizes on Metric control,bug:cosmetic; bug:regression; viz:explore:metrics,2021-03-27 04:11:22 +0000 UTC,zuzana-vej,Opened,,"## Screenshot

<img width=""362"" alt=""Screen Shot 2021-03-26 at 2 05 55 PM"" src=""https://user-images.githubusercontent.com/61221714/112709504-7891b700-8e77-11eb-984a-3458332ea4ca.png"">

## Description
The 1st metric on the metric control has the certification icon as a little dot - circled in red on the image. The other metric later in the list has a normal sized icon.

Both icons should be the same size (the 1st icon should be same size as the later metric)

",,,zuzana,"
--
Similar issue in the past (but on search left tab) https://github.com/apache/superset/pull/12690 
--
",,,,,,,,,,
13829,OPEN,[SQL Lab] UnicodeDecodeError when querying blob data and DB has asynchronous query execution enabled,global:async-query; sql_lab,2021-03-27 03:51:42 +0000 UTC,cabo40,Opened,,"When a DB has asynchronous query execution enabled the json dumper doesn't honor the default parser because `encoding` is not `None`
https://github.com/apache/superset/blob/18ff4841862cd53c92614805ae6db188e3379850/superset/views/core.py#L2214
Steps to reproduce on vanilla installation:
1. Set sample DB to async execution in Data / Databases
2. Query in SQL Lab the following: 
```sql
select decode('DEADBEEF', 'hex');
```
Result:
![image](https://user-images.githubusercontent.com/1605852/112706151-efbf4f00-8e67-11eb-9451-0bdee3ddc096.png)",,,junlincc,"
--
cc @robdiciuccio 
--
",,,,,,,,,,
13823,OPEN,[explore] Tweaks to sorting in metric search,enhancement:request; good first issue; viz:explore:metrics,2021-03-27 01:22:07 +0000 UTC,zuzana-vej,Opened,,"**Is your feature request related to a problem? Please describe.**
Currently when users searches metric in Chart Explore Metric dropdown, the search first displays the ""ending"" string of the metric, even if there is an exact match. For example if you search for `san_francisco` and your metrics include `population_in_san_francisco` `san_francisco` and `san_francisco_weather` your first result would be `population_in_san_francisco`

The proposal is to at least put the exact match first. See below more on ideal experience.

**Describe the solution you'd like**

Ideal experience?
1. exact match
2. starting string matches
3. anything else (string in middle or end)

From above example the expected result would be
`san_francisco`
`san_francisco_weather`
`population_in_san_francisco`",,,junlincc,"
--
@zuzana-vej thanks for suggesting! it makes perfect sense.  It is a good-first-issue, but in case no one picks it up, we should be able to squeeze it in when enhancing the metric component. 


--
",,,,,,,,,,
13821,OPEN,[explore] Metric tooltip is necessary for long metric names,enhancement:request; needs:design-input; viz:explore:metrics,2021-03-30 21:11:17 +0000 UTC,zuzana-vej,Opened,,"## Description
For long metric names, the tooltip can't be displayed, because the (i) which on hover displays the tooltip is out of range. The tooltip doesn't display the metric name (just the description)

For long metrics, this is hard to use - because users don't know what metric to select if they all start with the same substring.

Users need a way to see the full metric name on hover for metrics which are cut off in the Metrics Dropdown on chart explore page.

## Screenshots
![Screen Shot 2021-03-26 at 12 48 34 PM](https://user-images.githubusercontent.com/61221714/112686063-a7396e80-8e32-11eb-8d34-11639bfe2d4b.png)

",,,junlincc,"
--

@mihir174 can we make the enlarge icon louder? for now as a workaround.. i understand this is not gonna solve the problem....
--
",ktmud,"
--
I think we can just make the dropdown full-width with this antd option: `dropdownMatchSelectWidth`



--
",mihir174,"
--
Some other solutions in addition to full width dropdown:
![Slice 4](https://user-images.githubusercontent.com/64227069/113056657-eb917b00-9160-11eb-9ad8-92969340f62e.png)
A - The hover area is the whole menu item
B - The (i) persists and we truncate just a few characters earlier
C - Full width dropdown (@ktmud)

I like A since users it won't demand much user precision to view the entire metric name. C might create awkward looking lists with a lot of white space if there's a large variance between metric character lengths. 

I also think that the resize icon shifts the buttons to a pretty awkward alignment. Resize cursors on the edge should suffice to indicate that the popover can be enlarged. Also unsure if there's a need for vertical resizing.
![Slice 3](https://user-images.githubusercontent.com/64227069/113056670-f0eec580-9160-11eb-9130-0f4c21d16c08.png)

--
",,,,,,
13820,OPEN,[explore] Metric tooltip (i) is not visible for long metric names,bug:cosmetic; viz:explore:metrics,2021-03-26 20:01:02 +0000 UTC,zuzana-vej,Opened,,"## Description
For long metric names, the tooltip isn't displayed, because the (i) which on hover displays the tooltip is out of range. This means user has no way to see the metric description.

User needs some way to still see the metric description - e.f the tooltip (i) should be displayed in the visible section of the screen,

## Screenshots
No (i) visible:
![Screen Shot 2021-03-26 at 12 48 34 PM](https://user-images.githubusercontent.com/61221714/112686063-a7396e80-8e32-11eb-8d34-11639bfe2d4b.png)

Some long metrics name don't have the (i) visible
![Screen Shot 2021-03-26 at 12 49 10 PM](https://user-images.githubusercontent.com/61221714/112686458-35155980-8e33-11eb-915f-f7b46175dac8.png)

",,,,,,,,,,,,,,
13819,OPEN,Big Number viz doesn't allow you to change the time series formatting like you can with the Big Number.,good first issue,2021-03-31 18:32:52 +0000 UTC,rbalsick-saatva,Opened,,"I can change the Big Number format (e.g. to $) but I can't change the time series date to a different format (e.g. 10/01/2021).

Can someone add this format option to Superset?",,,Alxander64,"
--
I'm also interested in this, it would be helpful to just display a max(timestamp) as the current date and time. Only time option right now is to show a duration.

This is my first time looking at the Superset codebase so I'm unfamiliar with it. Where would the existing formatting options be found?
--
",,,,,,,,,,
13817,OPEN,[SQL lab]Strange horizontal scrolling on SqlLab table,bug:cosmetic; sql_lab,2021-04-05 18:04:19 +0000 UTC,michael-s-molina,Opened,,"Look at this video:

https://user-images.githubusercontent.com/70410625/112677607-bb886600-8e48-11eb-98fc-1025ef640926.mov

At the beginning of the video, you can see that the last column requires horizontal scrolling but none is available. The user cannot see all the data. This is happening because the horizontal scroll is being applied to the page and not to the table like Explore. When the tabs require more space, then the horizontal scroll appears on the page, but as a result, the buttons disappear to the right.

Probably related with https://github.com/apache/superset/issues/13789

### Expected results

Horizontal scrolling is applicable to the table and not the page. Similar to Explore.

#### How to reproduce the bug

1. Go to SqlLab
2. Select many tables
3. Watch the scroll behavior

@junlincc 

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,junlincc,"
--
Thanks for reporting! 
@AAfghahi @eschutho could we push a fix for this issue? 
--
",eschutho,"
--
Thanks @junlincc, yes, well take a look at this and the other one. @yousoph 
--
",yousoph,"
--
@michael-s-molina are you still seeing the issue with not being able to see the last column? I'm still seeing that the save button is getting pushed off to the side when there are a lot of preview tabs open, but can't repro the column getting cut off anymore. 
--
",michael,"
--
@yousoph The issue with the last column has been resolved on `master`. Only the buttons problem remaining.
--
",,,,
13814,OPEN,[Report] Can't send a Email Report for a Pie Chart,global:report,2021-03-29 17:01:11 +0000 UTC,younsb,In progress,,"VERSION : 1.0.1

![image](https://user-images.githubusercontent.com/81430164/112659648-deb91280-8e54-11eb-8832-8369083d6e40.png)

I try to schedule an attachement with RAW DATA  report using  ""Manage Email Reports for Charts"" for different type of visualizations.

- Pie chart :-1:  didn't work (see traceback 1)
- Word Cloud :-1: didn't work (see traceback 2)
- table :+1:  worked
- Big Number :+1: worked 

I tried also for Pie chart an attachement with ""Email Format: vizualisation"" and didnt work ; I get traceback 3 

### Expected results

4 mails received with raw data for every chart 

### Actual results

2 mails received with table raw data  and Big number raw data 

#### Screenshots
##### Picture 1

![image](https://user-images.githubusercontent.com/81430164/112662984-9865b280-8e58-11eb-957c-ad65df11a7a4.png)

##### Picture 2 : traceback 1

![image](https://user-images.githubusercontent.com/81430164/112663204-d4007c80-8e58-11eb-9a79-082d48ed9afa.png)

#####  Traceback 2

INFO:werkzeug:127.0.0.1 - - [25/Mar/2021 17:36:23] ""GET /sliceemailscheduleview/list/ HTTP/1.1"" 200 -
infos viz pie
INFO:superset.views.utils:infos viz pie
'pie'
Traceback (most recent call last):
  File ""/home/ybarie/Documents/superset1.0/superset1-dev/lib/python3.7/site-packages/superset/views/base.py"", line 181, in wraps
    return f(self, *args, **kwargs)
  File ""/home/ybarie/Documents/superset1.0/superset1-dev/lib/python3.7/site-packages/superset/utils/log.py"", line 164, in wrapper
    value = f(*args, **kwargs)
  File ""/home/ybarie/Documents/superset1.0/superset1-dev/lib/python3.7/site-packages/superset/utils/cache.py"", line 172, in wrapper
    response = f(*args, **kwargs)
  File ""/home/ybarie/Documents/superset1.0/superset1-dev/lib/python3.7/site-packages/superset/views/utils.py"", line 451, in wrapper
    check_perms(*args, **kwargs)
  File ""/home/ybarie/Documents/superset1.0/superset1-dev/lib/python3.7/site-packages/superset/views/utils.py"", line 520, in check_datasource_perms
    force=False,
  File ""/home/ybarie/Documents/superset1.0/superset1-dev/lib/python3.7/site-packages/superset/views/utils.py"", line 125, in get_viz
    viz_obj = viz.viz_types[viz_type](
KeyError: 'Cloud word'

#####  Traceback 3

![image](https://user-images.githubusercontent.com/81430164/112665327-3bb7c700-8e5b-11eb-862f-63787ebbf648.png)

#### How to reproduce the bug

1. Go to 'Chart Email schedules'
2. Add a pie chart (error for any pie chart) in 'Schedule Email Reports for Charts'
3. check Send Test Email and add a mail
4. error in the logs (traceback 1)

### Environment


- superset version: `1.0.1`
- python version: `Python 3.7.6`
- node.js version: `v14.16.0`


### Additional context

I am using geckodriver. the ""Schedule Email Reports for Charts"" is working for all charts except the pie chart and word cloud in both cases (Raw data, vizualisation) didn't work  
",,,younsb,"
--
@junlincc, can you explain to me why for you it's not a bug ? 
--
",junlincc,"
--
hey @younsb sorry i was validating the issue.. Have you tried the new reporting feature that's behind a feature flag? We will not likely to maintain the old one. LMK. 
https://preset.io/blog/2021-01-18-superset-1-0/
https://www.youtube.com/watch?v=p2gpzQcxsXs&t=209s 
https://superset.apache.org/docs/installation/alerts-reports
--
",,,,,,,,
13812,OPEN,Charts with spaces in metric names generate Unexpected Errors,#bug,2021-03-31 08:29:14 +0000 UTC,filippociceri,In progress,,"After updating to the latest version of superset (master), several BigNumber charts stop working. Note I am using BigQuery as a datasource.

### Expected results
Charts displayed as in previous versions.

### Actual results
Charts are no longer displayed. An error is shown instead, containing the name of the metric (with spaces). The following error is shown in superset logs when I try to access the chart:
```
superset_1  | INFO:superset.viz:Cache key: 5bcc722ea51102014c910e2e3b8eae85
superset_1  | 'Active users'
superset_1  | Traceback (most recent call last):
superset_1  |   File ""/app/superset/views/base.py"", line 183, in wraps
superset_1  |     return f(self, *args, **kwargs)
superset_1  |   File ""/app/superset/utils/log.py"", line 217, in wrapper
superset_1  |     value = f(*args, **kwargs)
superset_1  |   File ""/app/superset/utils/cache.py"", line 153, in wrapper
superset_1  |     return f(*args, **kwargs)
superset_1  |   File ""/app/superset/views/utils.py"", line 446, in wrapper
superset_1  |     return f(*args, **kwargs)
superset_1  |   File ""/app/superset/views/core.py"", line 618, in explore_json
superset_1  |     return self.generate_json(viz_obj, response_type)
superset_1  |   File ""/app/superset/views/core.py"", line 455, in generate_json
superset_1  |     payload = viz_obj.get_payload()
superset_1  |   File ""/app/superset/viz.py"", line 518, in get_payload
superset_1  |     payload[""data""] = self.get_data(df)
superset_1  |   File ""/app/superset/viz.py"", line 1255, in get_data
superset_1  |     aggfunc=np.min,  # looking for any (only) value, preserving `None`
superset_1  |   File ""/usr/local/lib/python3.7/site-packages/pandas/core/frame.py"", line 7038, in pivot_table
superset_1  |     observed=observed,
superset_1  |   File ""/usr/local/lib/python3.7/site-packages/pandas/core/reshape/pivot.py"", line 89, in pivot_table
superset_1  |     raise KeyError(i)
superset_1  | KeyError: 'Active users'
superset_1  | ERROR:superset.views.base:'Active users'
superset_1  | Traceback (most recent call last):
superset_1  |   File ""/app/superset/views/base.py"", line 183, in wraps
superset_1  |     return f(self, *args, **kwargs)
superset_1  |   File ""/app/superset/utils/log.py"", line 217, in wrapper
superset_1  |     value = f(*args, **kwargs)
superset_1  |   File ""/app/superset/utils/cache.py"", line 153, in wrapper
superset_1  |     return f(*args, **kwargs)
superset_1  |   File ""/app/superset/views/utils.py"", line 446, in wrapper
superset_1  |     return f(*args, **kwargs)
superset_1  |   File ""/app/superset/views/core.py"", line 618, in explore_json
superset_1  |     return self.generate_json(viz_obj, response_type)
superset_1  |   File ""/app/superset/views/core.py"", line 455, in generate_json
superset_1  |     payload = viz_obj.get_payload()
superset_1  |   File ""/app/superset/viz.py"", line 518, in get_payload
superset_1  |     payload[""data""] = self.get_data(df)
superset_1  |   File ""/app/superset/viz.py"", line 1255, in get_data
superset_1  |     aggfunc=np.min,  # looking for any (only) value, preserving `None`
superset_1  |   File ""/usr/local/lib/python3.7/site-packages/pandas/core/frame.py"", line 7038, in pivot_table
superset_1  |     observed=observed,
superset_1  |   File ""/usr/local/lib/python3.7/site-packages/pandas/core/reshape/pivot.py"", line 89, in pivot_table
superset_1  |     raise KeyError(i)
superset_1  | KeyError: 'Active users'
```

Modifying the metric's name by removing the spaces - `ActiveUsers`, effectively using camel case instead, displays the chart as expected. Inspecting the query, I can see that the label for my metric is mutated in the former case, while it stays the same in the latter.


#### How to reproduce the bug

1. Create a BigNumber chart using a BigQuery table as a datasource
2. Add a metric with a space in the name (for example, `A B`)
3. Click the Run button to generate a preview
4. See error

### Environment

(please complete the following information):

- superset version: `1.0.0`
- python version: `3.7.9`
- node.js version: `10.24.0`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,filippociceri,"
--
I forgot to mention that there are a couple of customizations in place, so the stacktrace line numbers may not match.
--

--
I took some more time to analyze this point. I think this is caused by the following lines changed in #13434 

https://github.com/apache/superset/blob/d54cc6d82aa86444dff0eaff072ee9e8e9ea915f/superset/connectors/sqla/models.py#L521-L524

https://github.com/apache/superset/blob/d54cc6d82aa86444dff0eaff072ee9e8e9ea915f/superset/connectors/sqla/models.py#L1097

where the labels_expected seems to be referencing mutated column names instead of the original/actually expected ones.

Was this change intended?

--
",,,,,,,,,,
13809,OPEN,Loading examples fails on Amazon Linux 2 - too many SQL variables,#bug,2021-03-26 12:21:41 +0000 UTC,jonasjancarik,In progress,,"When installing Superset from scratch on Amazon Linux 2 (essentially the default for Amazon EC2), `superset load_examples` will fail due to too many SQL variables being used in a SQLite query. This probably also affects Fedora and other RHEL-derivative Linux distributions. 

### Expected results

Example datasets to install correctly with `superset load_examples`.

### Actual results

```Traceback (most recent call last):
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1277, in _execute_context
    cursor, statement, parameters, context
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/sqlalchemy/engine/default.py"", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: too many SQL variables

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/jonasjancarik/venv/bin/superset"", line 11, in <module>
    load_entry_point('apache-superset==1.0.1', 'console_scripts', 'superset')()
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/flask/cli.py"", line 586, in main
    return super(FlaskGroup, self).main(*args, **kwargs)
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/click/decorators.py"", line 21, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/flask/cli.py"", line 426, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/superset/cli.py"", line 187, in load_examples
    load_examples_run(load_test_data, only_metadata, force)
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/superset/cli.py"", line 128, in load_examples_run
    examples.load_world_bank_health_n_pop(only_metadata, force)
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/superset/examples/world_bank.py"", line 77, in load_world_bank_health_n_pop
    index=False,
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/pandas/core/generic.py"", line 2615, in to_sql
    method=method,
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/pandas/io/sql.py"", line 598, in to_sql
    method=method,
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/pandas/io/sql.py"", line 1406, in to_sql
    raise err
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/pandas/io/sql.py"", line 1398, in to_sql
    table.insert(chunksize, method=method)
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/pandas/io/sql.py"", line 830, in insert
    exec_insert(conn, keys, chunk_iter)
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/pandas/io/sql.py"", line 758, in _execute_insert_multi
    conn.execute(self.table.insert(data))
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1011, in execute
    return meth(self, multiparams, params)
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/sqlalchemy/sql/elements.py"", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1130, in _execute_clauseelement
    distilled_params,
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/sqlalchemy/util/compat.py"", line 182, in raise_
    raise exception
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1277, in _execute_context
    cursor, statement, parameters, context
  File ""/home/jonasjancarik/venv/lib64/python3.7/site-packages/sqlalchemy/engine/default.py"", line 608, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) too many SQL variables
```

#### How to reproduce the bug

1. Install Superset from scratch following the docs on Amazon Linux 2 (e.g. on a fresh Amazon EC2 instance)
2. Run `superset load_examples`
4. See the error `sqlite3.OperationalError: too many SQL variables`

### Environment

(please complete the following information):

- superset version: `1.0.1`
- python version: `3.7.9`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

This is most likely due to the fact that SQLite on Amazon Linux 2 (and other RHEL-derivative Linux distributions) is compiled with the default SQLITE_MAX_VARIABLE_NUMBER option, which limits the number of variables in queries to 999 - see https://bugzilla.redhat.com/show_bug.cgi?id=1798134 and https://github.com/optuna/optuna/issues/1457#issuecomment-653587345. This limit is much lower than the limit used in other distros (250000).",,,jonasjancarik,"
--
The workaround I found is to lower the size of data batches (chunks), e.g. instead of 500 use just 1:

`sed -i 's/chunksize=500/chunksize=1/g' $(find ~/venv/lib/python3.7/site-packages/superset/examples/. -maxdepth 1 -type f)`

And replace `CHUNKSIZE = 512` with `CHUNKSIZE = 1` in `~/venv-superset/lib/python3.7/site-packages/superset/datasets/commands/importers/v1/utils.py`

Of course this makes the import slower.
--
",,,,,,,,,,
13807,OPEN,[chart]queries with order by have no effect on bar chart,bug; viz:chart-bar,2021-03-27 05:32:00 +0000 UTC,Riskatri,Opened,,"I'm using a bar chart, because I want my chart to be ordered by month so I added orders by month in my query but the result doesn't have any effect.

this is my result 
![image](https://user-images.githubusercontent.com/59104917/112610882-c2ff3d80-8e4f-11eb-83c3-9384fe3ca7a5.png)

and this is result of sql lab

![image](https://user-images.githubusercontent.com/59104917/112611071-f215af00-8e4f-11eb-93a5-8357adba93a5.png)

",,,junlincc,"
--
yup, order by doesn't work in nvd3 bar chart...not sure that's a historical issue or regression(unlikely)..
We won't be fixing it at this point since Echarts Bar will be available soon, but a quick fix PR is still welcome. 
--
",,,,,,,,,,
13804,OPEN,[Idea] Improve user system to better integrate with SSO or different login methods,,2021-03-26 07:14:28 +0000 UTC,juneauwang,Opened,,"While integrating Superset and Azure SSO, I find there is an annoying point after integration that: If you were using LDAP and put your SAMAccountName as username in Superset while Azure doesn't provide such thing, you might need to clean all existing users and start user registration from scratch.

It will be better if Superset check both username and emails in ab_user and link the existing account when user is trying to login if one is matched so that superset can better work with different login methods. 

It will be even great so that superset can provide multi-login methods so that user can choose which methods will be suitable. Don't know if flask appbuilder allowing this. 

",,,,,,,,,,,,,,
13796,OPEN,Calendar Heatmap only shows maximum values,bug:cosmetic; viz:chart-heatmap,2021-03-26 00:17:43 +0000 UTC,paulvanharen,Opened,,"I have configured a calendar heatmap and loaded some data. When hovering over the cells in the heatmap, the pop-up shows the correct data. However, the color coding scheme of the heatmap appears not to work.
* the cells in the legend all have the same color, i.e. the color associated with the maximum value
* the cells in the calendar heatmap also show the same color corresponding to the maximum value

#### Expected reults

Different colors in the legend and heatmap

#### Actual results

All cells have the same color

#### Screenshots
![Screenshot_20210325_142047](https://user-images.githubusercontent.com/2847593/112479638-86f7aa00-8d75-11eb-849e-04196d8602b9.png)


#### How to reproduce the bug

Configure a Calendar heatmap, and load some random data. The cells have values varying form 264 till 570.

### Environment
pip list

>Package                Version
>---------------------- -----------
>aiohttp                3.7.4.post0
alembic                1.5.8
amqp                   2.6.1
apache-superset        1.0.1
apispec                3.3.2
async-timeout          3.0.1
attrs                  20.3.0
Babel                  2.9.0
backoff                1.10.0
billiard               3.6.3.0
bleach                 3.3.0
Brotli                 1.0.9
cachelib               0.1.1
celery                 4.4.7
certifi                2020.12.5
cffi                   1.14.5
chardet                4.0.0
click                  7.1.2
colorama               0.4.4
contextlib2            0.6.0.post1
convertdate            2.3.2
cron-descriptor        1.2.24
croniter               1.0.9
cryptography           3.4.6
decorator              4.4.2
defusedxml             0.7.1
dnspython              2.1.0
email-validator        1.1.2
Flask                  1.1.2
Flask-AppBuilder       3.2.1
Flask-Babel            1.0.0
Flask-Caching          1.10.1
Flask-Compress         1.9.0
Flask-JWT-Extended     3.25.1
Flask-Login            0.4.1
Flask-Migrate          2.7.0
Flask-OpenID           1.2.5
Flask-SQLAlchemy       2.5.1
flask-talisman         0.7.0
Flask-WTF              0.14.3
geographiclib          1.50
geopy                  2.1.0
gevent                 21.1.2
greenlet               1.0.0
gunicorn               20.0.4
holidays               0.10.3
humanize               3.3.0
idna                   2.10
isodate                0.6.0
itsdangerous           1.1.0
Jinja2                 2.11.3
jsonschema             3.2.0
kombu                  4.6.11
korean-lunar-calendar  0.2.1
Mako                   1.1.4
Markdown               3.3.4
MarkupSafe             1.1.1
marshmallow            3.10.0
marshmallow-enum       1.5.1
marshmallow-sqlalchemy 0.23.1
msgpack                1.0.2
multidict              5.1.0
natsort                7.1.1
numpy                  1.20.1
packaging              20.9
pandas                 1.1.5
parsedatetime          2.6
pathlib2               2.3.5
pgsanity               0.2.9
Pillow                 8.1.2
pip                    21.0.1
pkg-resources          0.0.0
polyline               1.4.0
prison                 0.1.3
py                     1.10.0
pyarrow                1.0.1
pycparser              2.20
pydruid                0.6.2
PyJWT                  1.7.1
PyMeeus                0.5.11
pyparsing              2.4.7
pyrsistent             0.17.3
python-dateutil        2.8.1
python-dotenv          0.15.0
python-editor          1.0.4
python-geohash         0.8.5
python3-openid         3.2.0
pytz                   2021.1
PyYAML                 5.4.1
redis                  3.5.3
requests               2.25.1
retry                  0.9.2
selenium               3.141.0
setuptools             54.2.0
simplejson             3.17.2
six                    1.15.0
slackclient            2.5.0
SQLAlchemy             1.3.23
SQLAlchemy-Utils       0.36.8
sqlparse               0.3.0
typing-extensions      3.7.4.3
urllib3                1.26.4
vine                   1.3.0
webencodings           0.5.1
Werkzeug               1.0.1
WTForms                2.3.3
WTForms-JSON           0.3.3
yarl                   1.6.3
zope.event             4.5.0
zope.interface         5.3.0

",,,junlincc,"
--
thanks for reporting! there was a PR that attempted to fix this issue. For some reasons it was closed. 
https://github.com/apache-superset/superset-ui/pull/964

@paulvanharen feel free to implement the fix referring to this PR. we will not likely to push any fixes to this chart, a new [echarts heat map](https://echarts.apache.org/examples/en/#chart-type-heatmap) will be implemented in a couple of months, as a replacement 
--
",,,,,,,,,,
13795,OPEN,Cannot get caching to work,infra:caching; question,2021-03-26 00:21:34 +0000 UTC,paulvanharen,Opened,,"I'm adding a Flask-Cache configuration to my superset_config.py file. No matter what I try, the result always remains the same:
`superset/venv/lib/python3.8/site-packages/flask_caching/__init__.py:201: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
`

Please find my config file below. I've tried the three respective configurations independently. Redis is installed and passes the ping / PONG test.

> '''
DATA_CACHE_CONFIG = {
    'CACHE_TYPE': 'redis',
    'CACHE_DEFAULT_TIMEOUT': 60 * 60 * 24, # 1 day default (in secs)
    'CACHE_KEY_PREFIX': 'superset_results',
    'CACHE_REDIS_URL': 'redis://localhost:6379/0',
}
>
>DATA_CACHE_CONFIG = {
    'CACHE_TYPE': 'SimpleCache',
    'CACHE_DEFAULT_TIMEOUT': 60 * 60 * 24, # 1 day default (in secs)
    'CACHE_THRESHOLD': 1000,
}
>
>'''
DATA_CACHE_CONFIG = {
    'CACHE_TYPE': 'FileSystemCache',
    'CACHE_DEFAULT_TIMEOUT': 60 * 60 * 24, # 1 day default (in secs)
    'CACHE_THRESHOLD': 1000,
    'CACHE_DIR': '/tmp/flask',
    'CACHE_OPTIONS': {'mode' : 770}
}

### Environment
pip list
>Package                Version
>---------------------- -----------
>aiohttp                3.7.4.post0
>alembic                1.5.8
>amqp                   2.6.1
apache-superset        1.0.1
apispec                3.3.2
async-timeout          3.0.1
attrs                  20.3.0
Babel                  2.9.0
backoff                1.10.0
billiard               3.6.3.0
bleach                 3.3.0
Brotli                 1.0.9
cachelib               0.1.1
celery                 4.4.7
certifi                2020.12.5
cffi                   1.14.5
chardet                4.0.0
click                  7.1.2
colorama               0.4.4
contextlib2            0.6.0.post1
convertdate            2.3.2
cron-descriptor        1.2.24
croniter               1.0.9
cryptography           3.4.6
decorator              4.4.2
defusedxml             0.7.1
dnspython              2.1.0
email-validator        1.1.2
Flask                  1.1.2
Flask-AppBuilder       3.2.1
Flask-Babel            1.0.0
Flask-Caching          1.10.1
Flask-Compress         1.9.0
Flask-JWT-Extended     3.25.1
Flask-Login            0.4.1
Flask-Migrate          2.7.0
Flask-OpenID           1.2.5
Flask-SQLAlchemy       2.5.1
flask-talisman         0.7.0
Flask-WTF              0.14.3
geographiclib          1.50
geopy                  2.1.0
gevent                 21.1.2
greenlet               1.0.0
gunicorn               20.0.4
holidays               0.10.3
humanize               3.3.0
idna                   2.10
isodate                0.6.0
itsdangerous           1.1.0
Jinja2                 2.11.3
jsonschema             3.2.0
kombu                  4.6.11
korean-lunar-calendar  0.2.1
Mako                   1.1.4
Markdown               3.3.4
MarkupSafe             1.1.1
marshmallow            3.10.0
marshmallow-enum       1.5.1
marshmallow-sqlalchemy 0.23.1
msgpack                1.0.2
multidict              5.1.0
natsort                7.1.1
numpy                  1.20.1
packaging              20.9
pandas                 1.1.5
parsedatetime          2.6
pathlib2               2.3.5
pgsanity               0.2.9
Pillow                 8.1.2
pip                    21.0.1
pkg-resources          0.0.0
polyline               1.4.0
prison                 0.1.3
py                     1.10.0
pyarrow                1.0.1
pycparser              2.20
pydruid                0.6.2
PyJWT                  1.7.1
PyMeeus                0.5.11
pyparsing              2.4.7
pyrsistent             0.17.3
python-dateutil        2.8.1
python-dotenv          0.15.0
python-editor          1.0.4
python-geohash         0.8.5
python3-openid         3.2.0
pytz                   2021.1
PyYAML                 5.4.1
redis                  3.5.3
requests               2.25.1
retry                  0.9.2
selenium               3.141.0
setuptools             54.2.0
simplejson             3.17.2
six                    1.15.0
slackclient            2.5.0
SQLAlchemy             1.3.23
SQLAlchemy-Utils       0.36.8
sqlparse               0.3.0
typing-extensions      3.7.4.3
urllib3                1.26.4
vine                   1.3.0
webencodings           0.5.1
Werkzeug               1.0.1
WTForms                2.3.3
WTForms-JSON           0.3.3
yarl                   1.6.3
zope.event             4.5.0
zope.interface         5.3.0
",,,junlincc,"
--
@nytai might be able to help 
--
",,,,,,,,,,
13793,OPEN,[Request] Sort functionality in Pivot Table,enhancement:request; viz:chart-pivot,2021-03-29 09:28:37 +0000 UTC,lctdulac,In progress,,"Hi SuperSet community.
The Sort functionality in Pivot table seem to be lacking. I think this would be an interesting feature when creating pivot table with ordinal features.

An example :

<img width=""694"" alt=""Screenshot 2021-03-25 at 11 42 43"" src=""https://user-images.githubusercontent.com/46674763/112462006-dbdcf580-8d60-11eb-912f-a6197b89469b.png"">

Here it would be amazing to be able to sort both level 1 and level 2 of this table, as we would do in SQL.

In particular it would be great to be able to sort like this: 
Level 1: sorted alphabetically
Level 2: sorted with custom SQL like: Trusted > Neutral > Low > Medium > High

What do you guys think ?
",,,junlincc,"
--
@lctdulac we have a new pivot table available soon. https://github.com/apache-superset/superset-ui/pull/1023
thanks to @kgabryje 
I'm not sure we can support sorting to this extent though. 
--
",lctdulac,"
--
@junlincc thanks! the new pivot table looks great. I still think it would be amazing to be able to sort at least one level for ordinal data.
--
",,,,,,,,
13792,OPEN,[chart]Time Series Bar Chart does not sort,bug; viz:chart-bar,2021-03-27 05:34:28 +0000 UTC,lctdulac,Opened,,"Hi SuperSet community,
I encountered a bug where my Time Series Bar Chart is not sorting correctly.

### Expected results

Sort in this order : Trusted > Neutral > Low > Medium > High

<img width=""592"" alt=""Screenshot 2021-03-25 at 11 46 08"" src=""https://user-images.githubusercontent.com/46674763/112461276-1003e680-8d60-11eb-9db4-4c938b02a6c4.png"">

### Actual results

Sorted like : Low > Medium > High > Trusted > Neutral

<img width=""659"" alt=""Screenshot 2021-03-25 at 11 46 13"" src=""https://user-images.githubusercontent.com/46674763/112461418-2f9b0f00-8d60-11eb-92b4-1988348c3fe9.png"">

### Environment

Superset Version: 1.0.1

### Additional context

Other charts seem to be sorted correctly with same logic (Table chart) :

![image](https://user-images.githubusercontent.com/46674763/112461728-87397a80-8d60-11eb-9ea2-772389bfb1fe.png)

![image](https://user-images.githubusercontent.com/46674763/112461757-8c96c500-8d60-11eb-9a53-ec9ab556829c.png)


Thanks for your hard work.
",,,junlincc,"
--
same as https://github.com/apache/superset/issues/13807 
--
",,,,,,,,,,
13791,OPEN,superset-frontend]# npm run dev-server ERR,question,2021-03-26 00:47:01 +0000 UTC,tlkzzz,In progress,," 
npm run dev-server     ERR

version:1.0.1   master

![QQ20210325131841](https://user-images.githubusercontent.com/10111244/112446431-25dad100-8d8b-11eb-85cf-0ca3449f9579.png)


",,,cemremengu,"
--
This is probably related to #13744
--

--
Not sure about this since I can run fine but since superset does not officially support windows builds, any undefined behavior is possible
--
",ouydon,"
--
I have modified the file  webpack.config.js
![image](https://user-images.githubusercontent.com/1456271/112464642-b838a000-8d9e-11eb-945a-3237e2aafdca.png)
but npm run dev-server ERR
![image](https://user-images.githubusercontent.com/1456271/112465030-3137f780-8d9f-11eb-8b4f-cf0b2e8321b0.png)
![image](https://user-images.githubusercontent.com/1456271/112465058-3a28c900-8d9f-11eb-8651-7c6980337d6c.png)

windows  10
node        v14.15.0 
superset  master

 thanks 
--
",tlkzzz,"
--
> Not sure about this since I can run fine but since superset does not officially support windows builds, any undefined behavior is possible

My system version is CentOS, which is the same as windows, My first picture is CentOS
CentOS Linux release 7.2.1511 (Core)
node: v14.5.0


--
",,,,,,
13776,OPEN,Does superset supports dynamic chart loading wrt different entities in the table?,question,2021-03-26 00:39:23 +0000 UTC,jyotidhiman0610,Opened,,"I have a use case like below:
Input Data:

```
entity          test_run                some_values        
A                    x                         10                       ===> Chart 1 for entity A
A                    y                         20                       ===> Chart 2 for entity A
A                    z                         45                       ===> Chart 3 for entity A
B                    x                         56                       ===> Chart 1 for entity B 
B                    z                         89                       ===> Chart 2 for entity B
```

Dashboard should have
- 3 charts for entity A
- 2 charts for entity B

If a new entity comes in data, it should have n charts according to values in ""test_run""
The number of distinct values in ""test_run"" are known and charts can be defined for each value of ""test_run"", like chart for test_run ""x"" will always be a pie chart, etc.

Any way to it dynamically?

Kindly let me know if more details are needed.",,,zufolo441,"
--
In my opinion, you have to write some web service that cycles the query result and generates an array of json of pie chart and passes it in URL to superset.The charts could be rendered in an array of <iframe/>. Here is an example for a pie chart:
https://localhost:8088/superset/explore/?form_data={""viz_type"":""pie"";""datasource"":""113__table"";""slice_id"":512;""url_params"":{};""time_range_endpoints"":[""inclusive"";""exclusive""];""granularity_sqla"":""data_nascita"";""time_range"":""No+filter"";""groupby"":[""nome""];""metric"":""count"";""adhoc_filters"":[{""clause"":""WHERE"";""comparator"":""M"";""expressionType"":""SIMPLE"";""filterOptionName"":""filter_0qoax0rgoqyr_7dul8e6vj8j"";""isExtra"":false;""isNew"":false;""operator"":""=="";""sqlExpression"":null;""subject"":""sesso""}];""row_limit"":100;""sort_by_metric"":true;""color_scheme"":""supersetColors"";""legendType"":""scroll"";""legendOrientation"":""top"";""label_type"":""key_percent"";""number_format"":"">;.2~f"";""show_labels"":true;""labels_outside"":true;""outerRadius"":70;""donut"":true;""innerRadius"":30;""extra_form_data"":{""custom_form_data"":{};""override_form_data"":{};""append_form_data"":{}}}
--
",,,,,,,,,,
13774,OPEN,Share button should also be controlled via roles,enhancement:request; security:RBAC,2021-03-26 07:25:12 +0000 UTC,saurabnigam,Opened,,"**Is your feature request related to a problem? Please describe.**
Share button such as dashboard or share chart should be optional 

**Describe the solution you'd like**
A role for showing the share button would be sufficient

**Describe alternatives you've considered**
Adding CSS class specific for share button so custom CSS could be written to remove the element if not required

**Additional context**
We made a dashboard and want to share it externally. Problem is that there is a share button in charts as well which is not required and there is no way of removing it
",,,junlincc,"
--
some efforts have been made towards this goal. 
[[SIP-51] Dashboard Level Access](https://github.com/apache/superset/issues/10408)
https://github.com/apache/superset/pull/13108

cc @amitmiran137 


--
",amitmiran137,"
--
All other option on chart menu(explore /export and etc') are controlled via a permission that been been given to a role.

I think that share chart should also behave the same in the form of ' share chart permission that can be linked to any role.


Wdyt?
--

--
@saurabnigam if you'll create a PR with the new permission for share chart I'll support you 
--
",,,,,,,,
13766,OPEN,Excel upload menu option is missing from master,needs:design-input; question; resolved,2021-03-26 07:31:11 +0000 UTC,cemremengu,In progress,,"Is excel upload a (undocumented) feature flag? For some reason it is visible in latest docker distribution but not in master.

Checked but couldn't spot any switches in code

### Expected results

![image](https://user-images.githubusercontent.com/1297759/112272809-b337fb80-8c8d-11eb-9b91-e1f17a516a09.png)

### Actual results

![image](https://user-images.githubusercontent.com/1297759/112272907-ca76e900-8c8d-11eb-8a16-0574a8a41f29.png)

",,,junlincc,"
--
Please checkout, https://github.com/apache/superset/pull/11736 . both should be supported :)  

@mihir174 @steejay we should make it more explicit though 
--
",cemremengu,"
--
Reopening since I don't know your workflow, feel free to close.

As a side note, I think the naming changes in PR #11736 should also be applied to excel upload code since it can be a bit confusing. Created a PR to get started #13805 

![image](https://user-images.githubusercontent.com/1297759/112594832-90871d80-8e1a-11eb-8989-40fa801bfa65.png)

--
",,,,,,,,
13760,OPEN,How do I change style tooltip javascript generator?,question,2021-03-26 01:15:15 +0000 UTC,Riskatri,Opened,,"I want to change font family when I am using tooltip javascript generator, but idk how to change it. I am trying to write code like this :

![image](https://user-images.githubusercontent.com/59104917/112243661-8b8e6680-8c80-11eb-9933-8445d0825a4d.png)


where should I put the styles ? 
I try in div like 'div style=""font-family: Arial;""' but not work ",,,,,,,,,,,,,,
13755,OPEN,"Add free from ""tags"" field in the dataset + column model",enhancement:request; viz:explore:dataset,2021-03-26 17:49:19 +0000 UTC,cccs-jc,Opened,,"**Is your feature request related to a problem? Please describe.**
Superset can render dates using a data time pattern configured per column.

We would like to render other business types like IP, email etc. For example rendering a link to other web apps or formatting the data which is stored differently then how it is rendered. We are implementing a custom viz to do this. However we need a means to attach metadata information to the columns in the dataset model.

**Describe the solution you'd like**
We would like to be able to ""tag"" columns with additional information. Perhaps something similar to the ""extra"" field found on the dataset but on a column basis and also available on the client side.

**Describe alternatives you've considered**
At the moment we are using the type field of the column and setting it to IPv4, Domain, Email. However this field is probably not the best choice.


<img width=""99"" alt=""column-metadata"" src=""https://user-images.githubusercontent.com/56140112/112218891-afa76480-8bfa-11eb-941f-fc671ad45e2a.png"">
",,,junlincc,"
--
@cccs-jc the request you have is similar to in-place adding column description on semantic layer? 
we do have plan to implement it, but would love additional help from you :) 
--
",,,,,,,,,,
13754,OPEN,Native select filter does not support arbitrary values like the FilterBox did,#bug; viz:dashboard:native-filter,2021-03-25 19:51:41 +0000 UTC,cccs-jc,Opened,,"<img width=""316"" alt=""native-filter"" src=""https://user-images.githubusercontent.com/56140112/112216875-66561580-8bf8-11eb-9a71-a0b2adcd2a41.png"">

Unable to enter arbitrary values in the new select native filter. This was possible in the FilterBox. You could select from dropdown or enter new values.",,,,,,,,,,,,,,
13741,OPEN,Date Alignment Time Series Chart,enhancement:request; viz:chart-timeseries,2021-03-26 17:55:38 +0000 UTC,daniel10012,Opened,,"

![image](https://user-images.githubusercontent.com/10360991/112086280-4d9b2080-8b62-11eb-9596-fee512442099.png)

![image](https://user-images.githubusercontent.com/10360991/112086332-61df1d80-8b62-11eb-9db0-a1545bdf4895.png)

@junlincc 
",,,junlincc,"
--
Echarts actually have a very intuitive out-of-the-box solution to display x ticks. we removed it in favor of the customized date format. Opening a PR to bring it back and set it dynamic by default. but it will still have overlaps when user changes the date format....a more throughout change need to come from the Echarts team, which we have requested to. 

@daniel10012 @eugeniamz 
--
",,,,,,,,,,
13733,OPEN,Unable to create charts using REST api,REST-api; question,2021-04-05 12:44:48 +0000 UTC,timwil,Opened,,"Hello

I am using apache superset in my platform that I am building at the moment. One of the features of this platform is to create basic visualizations on a given dataset. To do this I am working with the REST api of superset (as documented here: https://superset.apache.org/docs/rest-api). Currently I am trying to create charts using this api but I have stumbled upon a problem. 

The problem is that upon calling the api, a response is send back with status code 500 and the message ""fatal error""
![afbeelding](https://user-images.githubusercontent.com/74621922/112051603-6f8ca700-8b52-11eb-921f-f06cf0e41b78.png)

I have provided a code snippet that makes the POST request to create a chart. At first the access token is retrieved (which seems to work). This token gets added to the headers of the actually request. The payload is hardcoded for now just to see if the chart is actually created. I should note that it is possible to manually login in superset and using swagger/v1, to create a chart using the payload used in the given code snippet.

code snippet:

...

@router.post(""/chart/"")
async def create_chart(datasource_id: int, viz_type: str, client: AsyncClient = Depends(superset_connection.get_superset_client)):
    dataset = await get_dataset(datasource_id, client)

    login = {
        ""password"": ""admin"",
        ""provider"": ""db"",
        ""refresh"": True,
        ""username"": ""admin""
    }

    headers = {
        'Content-Type': 'application/json'
    }
    response = await client.post(url='/security/login', headers = headers, json = login)

    token = response.json()['access_token']

    headers = {
        'accept': 'application/json',
        'Authorization': 'Bearer ' + token,
        'Content-Type': 'application/json'
    }

    # payload for POST request
    payload = {
        ""cache_timeout"": 0,
        ""dashboards"": [],
        ""datasource_id"": 9,
        ""datasource_name"": ""ob201"",
        ""datasource_type"": ""table"",
        ""description"": ""lorem ipsum"",
        ""owners"": [],
        ""params"": """",
        ""slice_name"": ""test_slice"",
        ""viz_type"": ""table""
    }

    response = await client.post(url=""/chart"", json = payload, headers = headers)

    # verify if request has succeeded
    if(response.status_code != 200):
        raise HTTPException(status_code=response.status_code, detail=response.json())

    return response
...

Stack trace:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/flask_appbuilder/api/__init__.py"", line 84, in wraps
    return f(self, *args, **kwargs)
  File ""/app/superset/views/base_api.py"", line 80, in wraps
    duration, response = time_function(f, self, *args, **kwargs)
  File ""/app/superset/utils/core.py"", line 1400, in time_function
    response = func(*args, **kwargs)
  File ""/app/superset/utils/log.py"", line 217, in wrapper
    value = f(*args, **kwargs)
  File ""/app/superset/charts/api.py"", line 274, in post
    new_model = CreateChartCommand(g.user, item).run()
  File ""/app/superset/charts/commands/create.py"", line 46, in run
    chart = ChartDAO.create(self._properties)
  File ""/app/superset/dao/base.py"", line 104, in create
    db.session.add(model)
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/scoping.py"", line 163, in do
    return getattr(self.registry(), name)(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py"", line 2019, in add
    self._save_or_update_state(state)
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/session.py"", line 2036, in _save_or_update_state
    ""save-update"", state, halt_on=self._contains_state
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/mapper.py"", line 3104, in cascade_iterator
    halt_on,
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/relationships.py"", line 1937, in cascade_iterator
    tuples = state.manager[self.key].impl.get_all_pending(state, dict_)
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/attributes.py"", line 1148, in get_all_pending
    for c in current
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/attributes.py"", line 1148, in <listcomp>
    for c in current
  File ""/usr/local/lib/python3.7/site-packages/werkzeug/local.py"", line 347, in __getattr__
    return getattr(self._get_current_object(), name)
AttributeError: 'AnonymousUserMixin' object has no attribute '_sa_instance_state'
10.99.49.186 - - [22/Mar/2021:19:45:49 +0000] ""POST /api/v1/chart/ HTTP/1.1"" 500 26 ""-"" ""python-httpx/0.17.1""

I tried to search to related problems but I can't seem to find something similar. What are the possible causes of this? I was thinking that I did something wrong in configuring superset but I can't seem to put my finger on it.",,,nytai,"
--
@dpgaspar does the auth token correctly set the current user? 
--
",inteon,"
--
+1
I'm also encountering this problem.
@junlincc Not sure this is a question: @nytai Is this ""Fatal error"" due to a configuration error or should this issue be labeled as ""bug""?
--
",dpgaspar,"
--
@timwil 

The auth token should correctly set the user, using the default configuration to invoke the API programatically you have to send the CSRF token also. `AnonymousUserMixin` error, seems that chart POST is allowed by an anonymous user, do you have `PUBLIC_ROLE_LIKE` set?
  
--
",,,,,,
13731,OPEN,Visualization Type : deck.gl Multiple Layers - Query filters are not executing,v0.36; viz:chart-deck.gl,2021-03-24 10:55:11 +0000 UTC,itsshahisthapirjade,Opened,,"I am not able to use Query->Filters while using the visualization type deck.gl Multiple Layers. 
To be more specific the filtera are not getting applied after adding the values too.
Superset version 0.36.0
![superset](https://user-images.githubusercontent.com/51056679/112017581-c050cf80-8b53-11eb-96a3-0c2e8c2980b8.JPG)
",,,,,,,,,,,,,,
13727,OPEN,[Dashboard]100 charts limits in Dashboard Edit mode; chart list,enhancement:request; viz:dashboard:list,2021-03-23 07:19:01 +0000 UTC,ValentinC-BR,Opened,,"The chart list seems to display ~100 charts.
When the users looks for a chart by typing keywords, the search is only performed on these 100 charts.

As a results, many charts cannot be found.

### Expected results

The search should be performed on the complete list of charts a user owns.

The limitation (100, or whatever the number is) should be applied on the search results, so that the user can find the desired charts

### Actual results

The search is performed on the filtered list.

#### Screenshots

/

#### How to reproduce the bug

1. Create 101 charts (not sure of the number)
2. Create a dashboard
3. Look for your 101th chart by using the search box
4. Error : your chart can't be found

### Environment


- superset version: 1.0.1
- python version: 3.7.9
- node.js version: doesn't apply, I run on Kubernetes, using gunicorn as server
- source : Athena

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

/
",,,junlincc,"
--
Thanks for reporting?

@nytai are you aware that the limit of 100 items was added? Was it intentional?  
--

--
@nytai thanks for checking! 

@ValentinC-BR I labeled it as feature request. It's not in our 2021-2022 roadmap but it would be really nice to have it implemented. I'm not sure how it will affect performance, but let us know if you would like to contribute and make the change. 


--
",nytai,"
--
I am not aware of this limitation. The search should be against the backend and the entire table should be searched. 
--

--
Ohh, @ValentinC-BR might be referring to the ""add chart"" widget in the dashboard view. The search might be happening locally there 
--

--
Yup, looks like for that widget we just fetch the first 200 results and the search is just local to those results. 
--
",ValentinC,"
--
OK thanks.

Quite surprised it's a feature request, I thought it was a bug as it's not clearly stated that the search is performed on the charts already displayed, but anyway.

I'm an end-user so I won't be able to contribute more ! Let me know if this can come sooner than 2022. There are workarounds, and not all users are affected, this is just a (quite big in some cases) confort improvement ;-)
--
",,,,,,
13721,OPEN,cx_Oracle installation malfunctioning in docker vm,install:docker,2021-03-31 04:58:16 +0000 UTC,gnoopy,Opened,,"To install a few python packages in the docker vm, I followed instructions as guided in the page https://superset.apache.org/docs/databases/dockeradddrivers 
However, cx_Oracle installation keep failed whenever I restart the superset docker container. 
Is there any recommendation to install jdbc driver for oracle ?


### Expected results
docker vm should start successfully with cx_Oracle

### Actual results

superset_worker_beat     | ERROR: Could not find a version that satisfies the requirement cx_Oracle
superset_worker_beat     | ERROR: No matching distribution found for cx_Oracle
superset_worker          | WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fb3ee561550>: Failed to establish a new connection: [Errno 111] Connection refused')': /simple/cx-oracle/
superset_worker          | ERROR: Could not find a version that satisfies the requirement cx_Oracle
superset_worker          | ERROR: No matching distribution found for cx_Oracle

#### Screenshots

![image](https://user-images.githubusercontent.com/11220690/111868604-3de2d700-89be-11eb-8e7f-0f54bfac366a.png)


#### How to reproduce the bug
1. modify ./docker/requirements-local.txt as follows
>> cx_Oracle
sqlalchemy-teradata
hdbcli
sqlalchemy-hana
pybigquery
gsheetsdb


2. docker-compose build --force-rm
3. docker-compose up

### Environment

(please complete the following information):

- superset version:  master branch lastest version on 20 March. 2021
- python version:  n/a
- node.js version: n/a

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context


",,,,,,,,,,,,,,
13717,OPEN,Group by Top-N on time-series charts,enhancement:request; viz:chart-bar; viz:chart-timeseries,2021-03-19 22:15:05 +0000 UTC,zuzana-vej,Opened,,"**Is your feature request related to a problem? Please describe.**
Visually, higher cardinality (6+) dimensions are difficult to parse and pose a challenge for data consumers in attempting to interpret what metrics increased and/or decreased for a time-series line chart or bar chart with many series. 
For data/chart producers, high-cardinality dimensions make it a challenge to display a legend (as it consumes too much real estate and ceases to be useful as an interactive visual exploration tool).  See bar chart below.
Moreover, when setting a series limit the total value of the metric for the time period is affected and reflects only the top x series limited to - leading to confusion for users about what is an accurate total. 

**Describe the solution you'd like**
Time-series Superset charts (line & bar) would include an ability to group a dimension by top-N values (where N is set by the chart creator), of this dimension (this is most meaningful when ordered by metric for the biggest buckets of the metric cut by this dimension). The remaining dimension values would be aggregated in a single Other grouping. This could be complicated by several dimension cuts to group by, but would be a modification and enhancement of the series limit feature. This type of feature is also a standard in BI tools such as Tableau.

Example:
![Screen Shot 2021-03-19 at 3 11 13 PM](https://user-images.githubusercontent.com/61221714/111847158-7fdc2200-88c5-11eb-90dc-2fa94b457b72.png)

",,,zuzana,"
--
cc: @junlincc @ktmud @graceguo-supercat 
--
",,,,,,,,,,
13715,OPEN,Allow hotkey Ctrl + Enter in SaveDatasetModal,nice-to-have,2021-03-19 20:47:46 +0000 UTC,hughhhh,Opened,,"## Description

When users are trying to create a new dataset or overwrite a dataset, we should allow them to save via Ctrl + Enter
",,,,,,,,,,,,,,
13711,OPEN,[explore] CSV Export permission is not consistent between Dashboard and Explore window,#bug,2021-03-19 17:44:25 +0000 UTC,duynguyenhoang,Opened,,"### Expected results

CSV Export permission must be consistent. When user doesn't have permission ""Can CSV on Superset"", he/she won't be able to Export CSV.
There is no `Export CSV` menu on Dashboard and on Explore page in this case.

### Actual results

""Can CSV on Superset"" is not assigned to user.
From Dashboard, user is unable to Export CSV but in Chart user can.

#### Screenshots
There is no Export CSV menu in Dashboard screen
![Screenshot_20210320_002214](https://user-images.githubusercontent.com/7106179/111819587-184cc380-8913-11eb-8726-a29752b96a48.png)

There is CSV button and we can Export CSV
![Screenshot_20210320_002357](https://user-images.githubusercontent.com/7106179/111819585-171b9680-8913-11eb-9234-bb5c57885836.png)

#### How to reproduce the bug

1. A user with normal role, can access Chart/Dashboard and Explore page
2. But he/she doesn't have ""Can CSV on Superset"" permission
2. Go to any Dashboard, there is no ""Export CSV"" menu in any chart Context menu
3. View chart in Explore
4. On the right top pannel, there is ""Export to CSV format"" => Which is wrong.

### Environment

- superset version: Master, SHA: 2b0b3f2f
- python version: Python 3.7.9
- node.js version: v14.16.0

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,,,,,,,,,,,,
13709,OPEN,Cannot load example data when setting up local environment for development,#bug,2021-04-08 04:00:21 +0000 UTC,JinJis,In progress,,"A clear and concise description of what the bug is.

### Expected results

When following the local development setup [guide](https://github.com/apache/superset/blob/master/CONTRIBUTING.md#os-dependencies), example data should be loaded successfully.

### Actual results

All previous steps successfully addressed, but `superset load_examples` step throws below error message.
```
(superset3.7)   superset git:(master) superset load_examples

logging was configured successfully
INFO:superset.utils.logging_configurator:logging was configured successfully
/Users/jinj/.virtualenvs/superset3.7/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
  ""Flask-Caching: CACHE_TYPE is set to null, ""
Loading examples metadata and related data into examples
Traceback (most recent call last):
  File ""/Users/jinj/.virtualenvs/superset3.7/bin/superset"", line 33, in <module>
    sys.exit(load_entry_point('apache-superset', 'console_scripts', 'superset')())
  File ""/Users/jinj/.virtualenvs/superset3.7/lib/python3.7/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/Users/jinj/.virtualenvs/superset3.7/lib/python3.7/site-packages/flask/cli.py"", line 586, in main
    return super(FlaskGroup, self).main(*args, **kwargs)
  File ""/Users/jinj/.virtualenvs/superset3.7/lib/python3.7/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/Users/jinj/.virtualenvs/superset3.7/lib/python3.7/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/Users/jinj/.virtualenvs/superset3.7/lib/python3.7/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/Users/jinj/.virtualenvs/superset3.7/lib/python3.7/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/Users/jinj/.virtualenvs/superset3.7/lib/python3.7/site-packages/click/decorators.py"", line 21, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/Users/jinj/.virtualenvs/superset3.7/lib/python3.7/site-packages/flask/cli.py"", line 426, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""/Users/jinj/.virtualenvs/superset3.7/lib/python3.7/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/Users/jinj/repositories/superset/superset/cli.py"", line 198, in load_examples
    load_examples_run(load_test_data, load_big_data, only_metadata, force)
  File ""/Users/jinj/repositories/superset/superset/cli.py"", line 123, in load_examples_run
    from superset import examples
  File ""/Users/jinj/repositories/superset/superset/examples/__init__.py"", line 19, in <module>
    from .birth_names import load_birth_names
  File ""/Users/jinj/repositories/superset/superset/examples/birth_names.py"", line 47, in <module>
    ""Admin user does not exist. ""
superset.exceptions.NoDataException: Admin user does not exist. Please, check if test users are properly loaded (`superset load_test_users`).
```

* `superset load_test_users` did not fix this bug
* Admin user already created by `superset fab create-admin`

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Forked the latest repository
2. Follow the [setup guide](https://github.com/apache/superset/blob/master/CONTRIBUTING.md#setup-local-environment-for-development)
3. `superset load_examples`
4. See error

### Environment

(please complete the following information):

- superset version: `Superset 0.999.0dev`
- python version: `Python 3.7.2`
- node.js version: `v14.13.1`
- MacOS: `Big Sur 11.2.1 (20D74)`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,JinJis,"
--
NVM, quickly looked through the error message today and solved this by creating admin whose username is `admin`.

But still, it may be quite confusing to newcomers because the guide does not warn us to create admin's username as `admin`.


--
",grace,"
--
`superset load_examples` should find a user who is an `admin role` instead of looking for a user whose username is `admin`.
--
",,,,,,,,
13708,OPEN,Superset UI is doesn't escape / quote the search query and currently is not working with special characters,#bug,2021-03-19 16:42:44 +0000 UTC,bkyryliuk,Opened,,"A clear and concise description of what the bug is.

### Expected results
Superset UI e.g. should escape query search parameter for superset objects like charts, dashboards etc
Expected api call:
/api/v1/report/related/dashboard?q=(filter:%27[query]%27,page_size:2000)
![image](https://user-images.githubusercontent.com/5727938/111812538-75099900-8895-11eb-99b6-395070cf4134.png)

Actual API call:
https://superset.pp.dropbox.com/api/v1/report/related/dashboard?q=(filter:[query],page_size:2000)

![image](https://user-images.githubusercontent.com/5727938/111812598-83f04b80-8895-11eb-9633-0d06ecba6582.png)


#### How to reproduce the bug

1. open superset search 
2. search for `[query]`
### Environment

https://github.com/airbnb/superset-fork/tree/release--2021-03-10


Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.

cc @dpgaspar , @villebro 
",,,nytai,"
--
This seems like a rison encoding as all calls should be going through `rison.encode`, we may have to fork and make updates to that lib
--
",,,,,,,,,,
13702,OPEN,Time filtering - Not working on calculated fields,#bug; data:connect:athena; data:connect:clickhouse; viz:explore:timepick,2021-03-27 06:37:51 +0000 UTC,ValentinC-BR,Opened,,"When using a calculated column as the TIME COLUMN, the preset filters (actually all the filters in the time section) do not work.
The calculated field (date or datetime) is compared to a STRING, causing the SQL request not to work.

### Expected results

The calculated column should be compared to a DATE or DATETIME value when the user applies a filter

### Actual results

The calculated column is compared to a STRING value

### Screenshots

TABLE CONFIGURATION

![image](https://user-images.githubusercontent.com/79460908/111745842-7767f500-888d-11eb-845f-52901a09d9a6.png)

CHART CONFIGURATION

![image](https://user-images.githubusercontent.com/79460908/111745953-a1211c00-888d-11eb-8be7-de9e167b2457.png)

GENERATED QUERY

![Capture decran 2021-03-19 a 08 33 19](https://user-images.githubusercontent.com/79460908/111748238-8ef4ad00-8890-11eb-9ad5-2c8a812a6fb3.png)

#### How to reproduce the bug

1. Create a calculated column, with a date or datetime format
2. Select it as THE time column
3. Chose any time filter (last month for instance)
4. RUN query


### Environment

(please complete the following information):

- superset version: 1.0.1
- python version: 3.7.9
- node.js version: doesn't apply, I run on Kubernetes, using gunicorn as server
- source : Athena


### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.


### Additional context

_None_
",,,ValentinC,"
--
I made a mistake while taking the screenshots (the ' character shouldn't be present). Sorry !

The error is present anyway, it is not related to it.
--

--
I get the following error : `Value cannot be cast to date: 2021-02-19 00:00:00`

Casting it as TIMESTAMP work, however.
--
",zhaoyongjie,"
--
It seems that Athena is not implicitly converted from string to datetime-like. This problem may exist in Athena / Clickhouse / Kylin2. 

@ValentinC-BR 
could you try this SQL using SQLLab, and change the where clause to like this:
```
....
WHERE date_add('day', 10, subscription_date) >= cast('2021-02-19 00:00:00' as date)
AND  date_add('day', 10, subscription_date) < cast('2021-03-19 00:00:00' as date)
... 
```
Can it query? 
--

--
ok. thanks for this debug.  We will improve the corresponding SQL generation logic.

@junlincc 
--
",junlincc,"
--
@ValentinC-BR feel free to push a fix if it's urgent.  thanks for reporting 
--
",,,,,,
13701,OPEN,FlinkSQL,,2021-03-19 07:54:51 +0000 UTC,huahua9427,Opened,,Requesting  FlinkSQL source...,,,zhaoyongjie,"
--
@huahua9427 , Please complete the issue template.

issue. 
--
",,,,,,,,,,
13700,OPEN,The CSRF session token is missing when embed superset to frontend web,,2021-03-19 07:52:55 +0000 UTC,Riskatri,Opened,,"Hi, i am using apache superset version 1.0.0 and i want to embed dashboard to my frontend web using iframe but iam getting error 

![image](https://user-images.githubusercontent.com/59104917/111744779-54096e00-88be-11eb-90ed-a3c00f22c0ca.png)

what i do : 
- run superset run -p 8088 --with-threads --reload --debugger
- put url to iframe like this 

**<iframe width=""100%""
    height=""800""
    seamless
    frameBorder=""0""
    scrolling=""no"" src=""http://127.0.0.1:8088/superset/dashboard/11""></iframe>**",,,zhaoyongjie,"
--
linked to #13697 
--
",,,,,,,,,,
13699,OPEN,VERSIONED_EXPORT overwrite all elements during import,,2021-03-19 06:59:54 +0000 UTC,PowerPlop,Opened,,"AS IS
----

When using versioned export, you can specify whether you want to overwrite the imported element. (e.g. dashboard,chart,dataset,database) However this option is only applied at the level of import (e.g. only the dashboard is overwritten but an updated chart is not overwrittten). When updating a dashboard often a combination of datasets, charts and dashboards is touched (e.g. new field, field in chart, more space in dashboard for additional field), so maybe an overwrite option per level would be useful.
Currently you have to make 3 exports of only the modified elements (thus filter charts/dataset before export) and reimport them again one by one.
 understand the reasoning behind not wanting to impact charts in other dashboard. However, you could argue that you only re-use charts over multiple dashboards if you really want to make sure the visualize the same data. If you then would choose the change the layout in one dashboard, I assume you know that it will impact the other dashboard as well.


Solution
-----------

* Allow to select which level to overwrite when importing (e.g. during dashboard import, overwrite charts as well). Can be combined with notification when chart is linked to a non-imported dashboard.
* Allow the metdata.yaml to specify multiple levels e.g. dashboard,slice,dataset",,,,,,,,,,,,,,
13697,OPEN,How to setup Superset and setup embeded charts,install:config; question,2021-04-06 01:19:14 +0000 UTC,Riskatri,In progress,,"Hi iam using superset version 1.0.0 but when i am trying to run with superset run -p 8088 --with-threads --reload --debugger i have an error 

![image](https://user-images.githubusercontent.com/59104917/111727268-22cc7600-889d-11eb-8e04-94e28aa5c977.png)

can someone help me? ",,,zhaoyongjie,"
--
Hi @Riskatri , did you change the `sueprset/config.py` file? 
--

--
@Riskatri 
A quick solution is to set `WTF_CSRF_ENABLED = False` in config.py, or any overwrite config file.
In the long term, you need to set a list for exempt CSRF protection.

https://github.com/apache/superset/blob/375ded92ef43e8c4735a4d9e2e094c67258f3f60/superset/config.py#L168-L174
--
",Riskatri,"
--
@zhaoyongjie this issue has been solve. thankyou. but did you know about embed superset with iframe? i am trying to embed but i have an error like this 

![image](https://user-images.githubusercontent.com/59104917/111741247-beb7ab00-88b8-11eb-9ccb-2110bab2aa5c.png)

i follow to change the config like this https://github.com/apache/superset/issues/8830 . but didn't work for me. 
--

--
@zhaoyongjie i've tried it, but when I logged in to the page there was no response and went back to login 

![image](https://user-images.githubusercontent.com/59104917/111755103-52927280-88cb-11eb-9bb1-0cb6c8961b3e.png)

I inspect console , and find this 

![image](https://user-images.githubusercontent.com/59104917/111759413-22999e00-88d0-11eb-8f6a-7221397850d4.png)

--

--
@anphamvn follow the step here :
-	Set ROLE_PUBLIC as public and PUBLIC_ROLE_LIKE = Gamma in file config.py
-	Add datasource access in public role
-	Re-launch superset with superset init
-	If have an error ' the csrf token is missing' , set WTF_CSRF_ENABLED = False in config.py 

--

--
@anphamvn
maybe, you are wrong. just uncomment it in the AUTH_ROLE_PUBLIC section, don't rewrite it and for PUBLIC_ROLE_LIKE. You can find it on line 265
_

> PUBLIC_ROLE_LIKE: Optional [str] = 'Gamma'

_. 
change it there, don't rewrite it.
and always re-launch again after change the file config.py.
if still no successful, try to add PUBLIC_ROLE_LIKE_GAMMA = True in file config.py and re-launch again. 
--
",anphamvn,"
--
I had the same problem when embedding the superset dashboard (using iframe) in my web, I have tried many ways but with no luck hix
Can someone help me to solve it?


--

--
> @anphamvn follow the step here :
> 
> * Set ROLE_PUBLIC as public and PUBLIC_ROLE_LIKE = Gamma in file config.py
> * Add datasource access in public role
> * Re-launch superset with superset init
> * If have an error ' the csrf token is missing' , set WTF_CSRF_ENABLED = False in config.py

Thanks @Riskatri 
i followed your instructions but still no luck :)

1. Set ROLE_PUBLIC as public and PUBLIC_ROLE_LIKE = Gamma in file config.py
AUTH_ROLE_PUBLIC = 'Public'
PUBLIC_ROLE_LIKE = 'Gamma'
![Capture_1](https://user-images.githubusercontent.com/38240236/113580982-ad102a80-9650-11eb-9d02-a038c6f6b7eb.PNG)
2. Add datasource access in public role 
i set 'All Datasource Access on all_datasource_access
![Capture_2](https://user-images.githubusercontent.com/38240236/113580986-ae415780-9650-11eb-996e-da1c4702b419.PNG)
 3. Re-launch superset with superset init
 Yes, sure :)
here log file
`INFO:superset.security.manager:Fetching a set of all perms to lookup which ones are missing
Creating missing datasource permissions.
INFO:superset.security.manager:Creating missing datasource permissions.
Creating missing database permissions.
INFO:superset.security.manager:Creating missing database permissions.
Creating missing metrics permissions
INFO:superset.security.manager:Creating missing metrics permissions
Cleaning faulty perms
INFO:superset.security.manager:Cleaning faulty perms`
![Error](https://user-images.githubusercontent.com/38240236/113580987-ae415780-9650-11eb-8125-d3631f45b848.PNG)
==> after that the error 'The CSRF missing ...' still occurred

4 I try to set  WTF_CSRF_ENABLED = False and  when I logged in to the page there was no response and went back to login





--
",,,,,,
13693,OPEN,"Can not create chart when ""Series limit"" set",data:connect:druid,2021-03-18 15:59:46 +0000 UTC,iercan,In progress,,"I'm trying to create line chart (same thing happen for all charts). Looks like superset can not create query properly and puts NULL for all series values. 

Here is how I configure chart 

![image](https://user-images.githubusercontent.com/3406152/111642341-100b6000-880f-11eb-9ccb-18c3dcee69fd.png)

Here is how superset generate query

```
SELECT ""duration"" AS ""duration"",
       COUNT(*) AS ""count""
FROM ""druid"".""mydatasource""
WHERE ""__time"" >= '2021-03-11 14:24:38.000000'
  AND ""__time"" < '2021-03-18 14:24:38.000000'
GROUP BY ""duration""
ORDER BY ""count"" DESC
LIMIT 5;

SELECT ""duration"" AS ""duration"",
       TIME_FLOOR(""__time"", 'PT5M') AS ""__timestamp"",
       COUNT(*) AS ""count""
FROM ""druid"".""mydatasource""
WHERE ""__time"" >= '2021-03-11 14:24:38.000000'
  AND ""__time"" < '2021-03-18 14:24:38.000000'
  AND (""duration"" = NULL
       OR ""duration"" = NULL
       OR ""duration"" = NULL
       OR ""duration"" = NULL
       OR ""duration"" = NULL)
GROUP BY ""duration"",
         TIME_FLOOR(""__time"", 'PT5M')
LIMIT 10000;
```

And this is actual result of first select query on druid 

![image](https://user-images.githubusercontent.com/3406152/111642269-fc5ff980-880e-11eb-9633-058d009d6b87.png)




### Expected results

Chart should be generated for selected series

### Actual results

No data return

### Environment

- superset version: 1.0.1
- python version: 3.7
- druid: 0.20.1

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,junlincc,"
--
are you one latest master or 1.0.1? @iercan 
--
",iercan,"
--
@junlincc 1.0.1
--
",,,,,,,,
13690,OPEN,"the ""type"" field in the ""table_columns"" table was too short",need:more-info,2021-03-27 18:52:22 +0000 UTC,bm8836900,In progress,,"There is an enum in my table. When creating a data set, the creation failed because the ""type"" field in the ""table_columns"" table was too short

DataBasemysql 5.7.33

Superset Error Message(MySQLdb._exceptions.DataError) (1406, ""Data too long for column 'type' at row 1"")
[SQL: INSERT INTO table_columns (uuid, created_on, changed_on, column_name, verbose_name, is_active, type, groupby, filterable, description, table_id, is_dttm, expression, python_date_format, created_by_fk, changed_by_fk) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)]
[parameters: (b'#\xe8J\xd1\xa9\xeaOg\xaaI\x12\x1c\x7fR\xfd\x82', datetime.datetime(2021, 3, 18, 17, 30, 17, 894672), datetime.datetime(2021, 3, 18, 17, 30, 17, 894672), 'Continent', None, 1, ""ENUM('ASIA','EUROPE','NORTH AMERICA','AFRICA','OCEANIA','ANTARCTICA','SOUTH AMERICA')"", 1, 1, None, 6, 0, None, None, 1, 1)]

Superset Source Code:
![image](https://user-images.githubusercontent.com/3680776/111616689-29b3a400-881d-11eb-84bc-c2c5fe96893a.png)
",,,bm8836900,"
--
superset version1.0.1
mysql databsetest
mysql table namecountry
mysqltestcountryContinentenumsupersetsupersettabletypetypesuperset
field nameContinent
field data typeENUM('ASIA','EUROPE','NORTH AMERICA','AFRICA','OCEANIA','ANTARCTICA','SOUTH AMERICA')

--
",,,,,,,,,,
13689,OPEN,Can't access database in SQL lab,data:connect:trino,2021-03-18 18:48:03 +0000 UTC,liamnv,Opened,,"A clear and concise description of what the bug is.

I'm create a role to access a database schema, assign this role to a user, but user can query this table in `CREATEA TABLE AS

### Expected results

User can run this query

### Actual results

Superset return you need access to the following tables: xxx, all_database_access or all_datasource_access

#### Screenshots

<img width=""1497"" alt=""Screen Shot 2021-03-18 at 15 55 14"" src=""https://user-images.githubusercontent.com/8578629/111611882-7f814f80-880f-11eb-8801-7b7d052a451e.png"">
<img width=""1173"" alt=""Screen Shot 2021-03-18 at 15 55 20"" src=""https://user-images.githubusercontent.com/8578629/111611899-84de9a00-880f-11eb-839a-663d0e6315f0.png"">


#### How to reproduce the bug

1. Connect Superset to Trino
2. Add database A & B from Trino to Superset
3. Create Role `test`, assign permission `schema acccess con A.xxx`, assign to user X
4. Run query similar to screenshot

### Environment


- superset version: 1.0.1
- python version: 3.8

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ x] I have reproduced the issue with at least the latest released version of superset.
- [ x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,,,,,,,,,,,,
13686,OPEN,[Table] [Pivot Table] Change column names in visualizations,enhancement:request; viz:explore:control,2021-03-19 05:52:56 +0000 UTC,ValentinC-BR,Opened,,"## Context & Current state

The column names (for display purposes only) can be changed at table level.

However, depending on the filters applied to the chart, or even on the audience or user job, the column will not have the same meaning.

Having the same name for every visualization is a bit limiting, and/or can be misleading in a few cases.

## Feature Request

In tables and pivot tables, the column names could be configurable, so that each chart could display custom column names.

## UI proposition

Similar to the metrics, clicking on the columns used in ""group by"" section could open a little panel, allowing the user to change the name (via a pen logo in the top-right corner for instance).

## Additional notes

This new panel could also be the opportunity to change number formatting at the chart level (see [this issue](https://github.com/apache/superset/issues/13311) or [this issue](https://github.com/apache/superset/issues/6924) as examples of what users need).",,,junlincc,"
--
""In addition to the color schemes, which seem to be the most persistent customization feature, every chart presenting numbers should at a minimum have the option to customize the number format. "" we are working on it ;) 
cc @yongjie

@kgabryje is in-place changing column name on click feasible in react-pivottable? 
--
",zhaoyongjie,"
--
Hi @ValentinC-BR, It's a common request in some production-ready BI software. I am working on refactor metric to unlock these actions.

--
",,,,,,,,
13685,OPEN,[Chart] Tooltip should show total in Stacked charts (bar; area; ...),enhancement:request; viz:chart-bar,2021-03-18 22:45:26 +0000 UTC,ValentinC-BR,Opened,,"## Context

The current tooltip only show the value of the bar we're hovering on.
It is often (almost always ?) interesting to know the total value, and when there are many bars, displaying it at the top of the bars is not a viable option.

## Feature Request

Show total in tootltip :
* Propose it as an option in the UI would be even better
* Leave the possibility to the user to add % in the tooltip would also be great, but quite a different feature.

What do you think of it ?",,,,,,,,,,,,,,
13674,OPEN,Clarify Superset Version Number in the UI,enhancement:request,2021-03-18 00:37:50 +0000 UTC,srinify,Opened,,"## Screenshot

<img width=""214"" alt=""Screen Shot 2021-03-17 at 2 23 08 PM"" src=""https://user-images.githubusercontent.com/801507/111518504-82efda80-872c-11eb-9fe0-bfb2fd23c5e3.png"">


## Description

I've been seeing Superset users, at least once a week, get tripped by the fact that the Version number displayed in the Superset UI is always `0.999.0dev` if people are using the `latest` tag or using bleeding edge master from Github.

Here's a recent user that was confused, wondering why they aren't on Superset v1.1: https://apache-superset.slack.com/archives/C0170U650CQ/p1616004990065000

## Potential Solutions

**Ideal User Experience:**
It Just Works (TM). Wherever possible, show the real version (using the git sha as the 'data source') for this. No doubt this will require extra work, but this would enhance the first day / first week user experience!

**Less than ideal but more helpful**
Keep the git sha but remove the version number from the UI",,,,,,,,,,,,,,
13667,OPEN,Cannot get Alerts/Reports options in the Superset Window,global:report; question,2021-03-18 00:38:18 +0000 UTC,debapratimc,Opened,,"Hey Folks,
I want to configure Alerts/Reports in my superset 1.0.1 (that I'm running on an EC2 instance - virtual environment). My `superset_config.py` file contains the following lines only.
```
ENABLE_ALERTS =True
ENABLE_SCHEDULED_EMAIL_REPORTS = True
FEATURE_FLAGS = {
    ""ALERT_REPORTS"": True,
    ""THUMBNAILS"": True
}
```
Now, I add this file to my pythonpath using `export PYTHONPATH=""$PWD/superset_config.py""`
And then run `superset init` , and then launch superset on a port
However I still cannot see ""Alerts"" in the Settings drop down menu, as shown below.
<img width=""1789"" alt=""Screenshot 2021-03-17 at 7 12 06 PM"" src=""https://user-images.githubusercontent.com/45069802/111483607-f1e42800-875a-11eb-863d-9b4125835ce6.png"">

What am I doing wrong?",,,,,,,,,,,,,,
13666,OPEN,Failed at generating thumbnail Instance <User at 0x7fbb4b1b8220> is not bound to a Session,#bug; viz:thumbnail,2021-03-18 22:40:46 +0000 UTC,Lemmynjash,Opened,,"Hi Guys
Just configured celery but when I run it, it gives me this error


```
2021-03-17 13:38:23,830: ERROR/ForkPoolWorker-3] Failed at generating thumbnail Instance <User at 0x7fbb4b1b8220> is not bound to a Session; attribute refresh operation cannot proceed (Background on this error at: http://sqlalche.me/e/13/bhk3)
Failed at generating thumbnail Instance <User at 0x7fbb4b295fa0> is not bound to a Session; attribute refresh operation cannot proceed (Background on this error at: http://sqlalche.me/e/13/bhk3)
```


I do have a user on superset database by the username as **admin**

This is my config.py


```
FEATURE_FLAGS = { ""THUMBNAILS"" : True, ""LISTVIEWS_DEFAULT_CARD_VIEW"" : False, ""ALERT_REPORTS"": True}
CACHE_CONFIG: CacheConfig = {
    'CACHE_TYPE': 'redis',
    'CACHE_DEFAULT_TIMEOUT': 24*60*60, # 1 day
    'CACHE_KEY_PREFIX': 'superset_',
    'CACHE_REDIS_URL': 'redis://localhost:6379/3'
}
DATA_CACHE_CONFIG: CacheConfig = {
    'CACHE_TYPE': 'redis',
    'CACHE_DEFAULT_TIMEOUT': 24*60*60, # 1 day
    'CACHE_KEY_PREFIX': 'data_',
    'CACHE_REDIS_URL': 'redis://localhost:6379/3'
}
THUMBNAIL_SELENIUM_USER = ""admin""
THUMBNAIL_CACHE_CONFIG: CacheConfig = {
    'CACHE_TYPE': 'redis',
    'CACHE_DEFAULT_TIMEOUT': 24*60*60,
    'CACHE_KEY_PREFIX': 'thumbnail_',
    'CACHE_NO_NULL_WARNING': True,
    'CACHE_REDIS_URL': 'redis://localhost:6379/3'
}
# Used for thumbnails and other api: Time in seconds before selenium
# times out after trying to locate an element on the page and wait
# for that element to load for an alert screenshot.
SCREENSHOT_LOCATE_WAIT = 100
SCREENSHOT_LOAD_WAIT = 600
class CeleryConfig:  # pylint: disable=too-few-public-methods
    # BROKER_URL = ""sqla+sqlite:///celerydb.sqlite""
    BROKER_URL = ""redis://localhost:6379/4""
    CELERY_IMPORTS = (""superset.sql_lab"", ""superset.tasks"", ""superset.tasks.thumbnails"")
    CELERY_RESULT_BACKEND = ""redis://localhost:6379/4""
    CELERYD_LOG_LEVEL = ""DEBUG""
    CELERYD_PREFETCH_MULTIPLIER = 10
    CELERY_ACKS_LATE = True
    CELERY_ANNOTATIONS = {
        'sql_lab.get_sql_results': {
            'rate_limit': '100/s',
        },
        'email_reports.send': {
            'rate_limit': '1/s',
            'time_limit': 600,
            'soft_time_limit': 600,
            'ignore_result': True,
        },
    }
    CELERYBEAT_SCHEDULE = {
        'reports.scheduler': {
            'task': 'reports.scheduler',
            'schedule': crontab(minute='*', hour='*'),
        },
        'reports.prune_log': {
            'task': 'reports.prune_log',
            'schedule': crontab(minute=0, hour=0),
        },
        'cache-warmup-hourly': {
            'task': 'cache-warmup',
            'schedule': crontab(minute='*/30', hour='*'),
            'kwargs': {
                'strategy_name': 'top_n_dashboards',
                'top_n': 10,
                'since': '7 days ago',
            },
       },
    }
CELERY_CONFIG = CeleryConfig  # pylint: disable=invalid-name
```

 What am I missing!!
Kindly assist?",,,,,,,,,,,,,,
13661,OPEN,[sql-lab]Pyarrow to pandas fails when timestamp is out of bounds,sql_lab,2021-04-08 17:33:56 +0000 UTC,cabo40,In progress,,"When querying from a database with a timestamp column with [us] granularity and missing data as zero dates the query cannot be completed

### Expected results

The query result with full precision of dates

### Actual results

`pyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds timestamp`

#### How to reproduce the bug

Tested with a DB2 database, select a timestamp from 1900-01-01 in sql-lab

### Environment

(please complete the following information):

- superset version: `0.38.0`
- python version: `3.7.9`
- node.js version: `16.14.0`
",,,cabo40,"
--
Hi @junlincc . I am very very sorry for not answering sooner. 

I tested with a fresh installation and could reproduce by querying in sql lab on the `examples` database:
```sql
SELECT TIMESTAMP '01-01-1000';
```
result:
![image](https://user-images.githubusercontent.com/1605852/112705667-71fa4400-8e65-11eb-93cc-9386b1757396.png)

--
",junlincc,"
--
Thanks for the additional details and contributing a fix PR, we will get it reviewed asap! 
@cabo40 
--
",,,,,,,,
13659,OPEN,How does the user only query specific tables in SQLLAB,security:RBAC; v0.38,2021-03-18 22:47:31 +0000 UTC,chen-ABC,Opened,,"I added it in the Gamma role
![image](https://user-images.githubusercontent.com/30097790/111410199-419f0100-8713-11eb-8243-f90c63194113.png)
![image](https://user-images.githubusercontent.com/30097790/111410476-c853de00-8713-11eb-98c4-5fbcb6bb2c9e.png)

the user has gamma and sqllab role
but sqllab throw error:
The data source and schema could not be obtained
How does the user  only query specific tables in SQLLAB

### Environment

(please complete the following information):

- superset version: `0.38`
- python version: `3.7.9`
- node.js version: `12.18`

",,,,,,,,,,,,,,
13653,OPEN,Big Number Chart should have an option to display chart title in dashboard,enhancement:request,2021-03-18 22:48:08 +0000 UTC,cccs-jc,Opened,,"**Is your feature request related to a problem? Please describe.**
Most of the time it's okay to have a chart title in the dashboard. However for the Big Number chart this makes less sense.

**Describe the solution you'd like**
It would be great to have a option to remove the chart title in the dashboard. Perhaps this should be an option for any chart placed inside the dashboard.

**Describe alternatives you've considered**
We can circumvent this issue by naming the Big Number chart '   ' (use spaces to name the chart). But that's obviously a hack!

",,,,,,,,,,,,,,
13651,OPEN,[Community] Help us test Superset's experience with different databases,community:initiative; preset-io,2021-03-17 13:27:22 +0000 UTC,srinify,Opened,,"Through SQLAlchemy and Python DB-API 2.0, Superset can query potentially hundreds of SQL-speaking databases. The community documentation maintains an incomplete list here: https://superset.apache.org/docs/databases/dockeradddrivers

Testing all of these databases with each minor or major release is a lot of work that can't be carried out by a single team or organization! In addition, navigating the nuances of each database is best done by people actively using the database for their own needs. This is an ideal candidate for a community effort  

**As a first step, I'm looking to build a list of Database Champions who want to help with documenting, answering questions about the specific database(s), or even testing / ensuring that Superset's integration with the database(s) is maintained!**.

Some common databases that I know at least a few community members use in conjunction with Superset:

- MySQL
- Postgres
- PrestoDB
- Trino
- Hive
- Druid
- Pinot
- Clickhouse
- BigQuery
- Redshift

If you want to help out with testing or be available as a resource for others in the community, **please comment in this thread with the databases you want to champion for**. We'll also be using the #contributing channel in the [Superset Community Slack](https://join.slack.com/t/apache-superset/shared_invite/zt-l5f5e0av-fyYu8tlfdqbMdz_sPLwUqQ) to discuss improving support for database connections. 

Thank you to Sophie @betodealmeida and etc. for helping kickstart this initiative!",,,garden,"
--
I can champion Druid to start. I can further pick up Postgres, Redshift, and PrestoDB if needed down the line as well, but would prefer to distribute the knowledge more widely if possible. 
--
",aakashnand,"
--
@srinify as discussed in this slack thread, I am still stuck at getting started trino with the superset. I will try it again and once I get access I would like to help with trino db.
https://apache-superset.slack.com/archives/C015WAZL0KH/p1614923108050900

--
",zach,"
--
Im happy to own Redshift. 
--
",,,,,,
13649,OPEN,Direct CSV export using explore using filters,#bug,2021-03-16 16:42:42 +0000 UTC,durchgedreht,Opened,,"I have a chart that has some baked in filters (e.g. ""last day""). I' like to just overwrite that one filter option and data should be exported in CSV using the ""explore_json"" option. So URL looks like:

```https://some.domain/superset/explore_json/?csv=true&form_data=XYZ```

I know the slice_id and want to load this directly by addressing it. That works fine:

 ```json
{
	""slice_id"":123
}'
```
Of course the JSON is URLencoded and Url looks like:
```
https://some.domain/superset/explore_json/?csv=true&form_data={%22slice_id%22:123}
```

Also handing over the complete config in the form_data param works (here pretty printed and unencoded for simplicity):

```json
{
  ""queryFields"": {
    ""groupby"": ""groupby"",
    ""metrics"": ""metrics""
  },
  ""datasource"": ""111__table"",
  ""viz_type"": ""table"",
  ""slice_id"": 123,
  ""url_params"": {},
  ""time_range_endpoints"": [
    ""inclusive"",
    ""exclusive""
  ],
  ""granularity_sqla"": ""mydate"",
  ""time_grain_sqla"": ""P1D"",
  ""time_range"": ""Last+day"",
  ""groupby"": [
    ""aaa"",
    ""bbb""
  ],
  ""metrics"": [
    ""m1"",
    ""m2""

  ],
  ""all_columns"": [],
  ""percent_metrics"": [],
  ""order_by_cols"": [],
  ""row_limit"": ""1000"",
  ""include_time"": true,
  ""order_desc"": true,
  ""adhoc_filters"": [],
  ""table_timestamp_format"": ""%Y-%m-%d"",
  ""color_pn"": true,
  ""show_cell_bars"": true
}
```

So I wanted to overwrite the date filter:

```json
{
  ""slice_id"":123,
  ""time_range"":""Last 2 days""
}
```
With URL:
```
https://some.domain/superset/explore_json/?csv=true&form_data=%7B%22slice_id%22%3A123%2C%22time_range%22%3A%22Last%202%20days%22%7D
```

But I get an error the table does not exist (obviously it tries to read it from the JSON and is not using the saved parameters). Is this really intended and does not take the 'convention over configuration' principle? In a big slice with many fields this can get pretty messy. Also some fields can be left out (""show_cell_bars""), others [that also don't need to be defined in the UI] seem to be mandatory (""time_table_format"").

Error:
```json
{
    ""errors"": [
        {
            ""message"": ""The datasource associated with this chart no longer exists"",
            ""error_type"": ""FAILED_FETCHING_DATASOURCE_INFO_ERROR"",
            ""level"": ""error"",
            ""extra"": null
        }
    ]
}
```




",,,,,,,,,,,,,,
13648,OPEN,[SQL Lab] Colors on SQL Lab tabs - Very small UI enhancement -,design:sqllab; enhancement:request; good first issue; sql_lab:control:ui,2021-03-17 05:29:22 +0000 UTC,zuzana-vej,Opened,,"Currently the tabs on SQL Lab queries show green circle when a query is running as well as when a query has successfully finished. 

The green circles for ""query running"" and ""query completed"" could be made more distinct - different color, or having the ""query running"" circle be hollow, and the ""query completed"" one filled in. This would enable users, who might go to SQL Lab and trigger few queries and then want to view results, visually see which of their queries are still running vs. finished.

Screenshot current state:
One query is running one has finished. User sees both as just a green circle.
![image](https://user-images.githubusercontent.com/61221714/111344909-08df2900-863a-11eb-943f-4928154c4aa4.png)

",,,yousoph,"
--
#11123 covers this! :) 
--
",,,,,,,,,,
13646,OPEN,Add a vertical header control to the dashboard,,2021-03-16 14:57:53 +0000 UTC,cccs-jc,Opened,,"**Is your feature request related to a problem? Please describe.**
Be able to put headers on the left/right vertically. It helps saving space on really crammed dashboards.

**Describe the solution you'd like**
A new control in the dashboard UI to create vertical headers.

**Describe alternatives you've considered**
We have a work around which involves putting html and css code in the markdown slice. However this solution is not very intuitive to use.

**Additional context**
This is what a dashboard with horizontal header and a custom markdown displaying vertical text. But a better solution would be to have a native vertical header.

![image](https://user-images.githubusercontent.com/56140112/111330071-04b90880-8646-11eb-88a3-90e734e3c792.png)
",,,,,,,,,,,,,,
13644,OPEN,Added chart property to display chart description by default,,2021-03-16 14:30:14 +0000 UTC,jvani,Opened,,"**Is your feature request related to a problem? Please describe.**
Our audience is not necessarily familiar with Superset (i.e., may not know to toggle chart descriptions on/off), but we must convey supplementary information for our audience to best understand our charts.

**Describe the solution you'd like**
An added checkbox/boolean to the ""Edit Chart Properties"" to either display the chart description by default or not.

**Describe alternatives you've considered**
* Adding text as part of our dashboards. However, the text is not explicitly associated with chart and cannot be customized with CSS (as part of the chart).

**Additional context**
n/a",,,,,,,,,,,,,,
13641,OPEN,Formula for subheader in Big Number,,2021-03-16 08:38:53 +0000 UTC,Riskatri,Opened,,"i want to make a KPI like this 
![image](https://user-images.githubusercontent.com/59104917/111279320-28dc1000-866d-11eb-8d1d-f1bc33b6b1fa.png)
with visualization type 'Big Number With Trendline'. can supersets use a formula like MAX or MIN in the subheader to specify a value that is not a number?",,,,,,,,,,,,,,
13639,OPEN,Database Disk Image Is Malformed: Query Record was not Recorded as Expected,question,2021-03-16 10:07:17 +0000 UTC,linkparabole,Opened,,"Every time I try running a query in SQLLab I receive the following error:


` sqlite3.DatabaseError: database disk image is malformed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/pcuser/venv/lib/python3.8/site-packages/superset/views/core.py"", line 2508, in sql_json_exec
    session.flush()
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2536, in flush
    self._flush(objects)
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2678, in _flush
    transaction.rollback(_capture_exception=True)
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py"", line 68, in __exit__
    compat.raise_(
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 182, in raise_
    raise exception
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 2638, in _flush
    flush_context.execute()
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py"", line 422, in execute
    rec.execute(self)
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py"", line 586, in execute
    persistence.save_obj(
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py"", line 239, in save_obj
    _emit_insert_statements(
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py"", line 1135, in _emit_insert_statements
    result = cached_connections[connection].execute(
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1011, in execute
    return meth(self, multiparams, params)
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/sql/elements.py"", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1124, in _execute_clauseelement
    ret = self._execute_context(
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1316, in _execute_context
    self._handle_dbapi_exception(
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1510, in _handle_dbapi_exception
    util.raise_(
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 182, in raise_
    raise exception
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1276, in _execute_context
    self.dialect.do_execute(
  File ""/home/pcuser/venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.DatabaseError: (sqlite3.DatabaseError) database disk image is malformed
[SQL: INSERT INTO ""query"" (extra_json, client_id, database_id, tmp_table_name, tmp_schema_name, user_id, status, tab_name, sql_editor_id, schema, sql, select_sql, executed_sql, ""limit"", select_as_cta, select_as_cta_used, ctas_method, progress, rows, error_message, results_key, start_time, start_running_time, end_time, end_result_backend_time, tracking_url, changed_on) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('{}', 'hecOT_KU5', 2, '', 'dbo', '1', <QueryStatus.RUNNING: 'running'>, 'Query dbo.Query dbo.admin ds-Signals/Full - General ( METER exclusive)-
![Screen Shot 2021-03-15 at 11 09 21 PM](https://user-images.githubusercontent.com/79966817/111264004-0c47c580-85e4-11eb-93b6-9b0bba753f84.png)
 03/10/2021 07:57:17', '4jVFSPyE-t', 'dbo', ""SELECT optimized.name, optimized.signalname, origin.signalname as signame2, optimized.optval, optimized.DateCreatedUTC, optimized.WeekDay, optimized. ... (1985 characters truncated) ... ed.WeekDay=origin.weekday2\r\nWhere optimized.testerr = origin.test \r\n\r\nAND optimized.DateCreatedUTC > DATEADD(dd, -7, GetDate())\r\n\r\n\r\n\t\r"", None, None, None, 0, 0, 'TABLE', 0, None, None, None, None, None, None, None, '2021-03-16 05:56:20.511112')]
(Background on this error at: http://sqlalche.me/e/13/4xp6)`



###Actual Results

Even when I try a simple SELECT * FROM ____ I get a ""Query Record was not recorded as expected"" with an instant timeout. 

The issue just started happening last week. Oddly enough the current dashboard seems to be pulling data from the datasource fine, SQLLab however cannot. 

Worth mentioning that SQL Lab recognizes the tables in my database, just cannot pull any of it. 

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Go to SQL LAB
2. Type in Query 
3. Run Query

### Environment

(please complete the following information):

- superset version: `1.0.1`
- python version: `3.8`
- node.js version: `10.19.0`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stack traces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,dpgaspar,"
--
@linkparabole this looks like a problem with SQLite, can you query it outside of Superset?
--
",,,,,,,,,,
13635,OPEN,Chart Explorer: table visualization does not check for empty COLUMNS control,#bug,2021-03-15 21:29:26 +0000 UTC,cccs-jc,Opened,,"In the Chart explorer the table visualization lets you choose aggregate or raw mode. In raw mode the COLUMNS control should report an error when no columns are chosen. For example something like this.

![image](https://user-images.githubusercontent.com/56140112/111223415-81e76d80-85b3-11eb-81f4-a63b5ed88bae.png)

#### How to reproduce the bug

1. Go to Chart explorer UI
2. Choose Table visualization
3. Choose QUERY MODE = RAW RECORDS
4. RUN query

However the table visualization does not check that the COLUMNS control is empty and lets you execute the query with no selection. This results in an ""Unexpected Error, Error: Empty query?""

It would be better if the COLUMNS control checked for the empty state and reported the issue like depicted above.
",,,,,,,,,,,,,,
13633,OPEN,[Explore]add SQL expression validator to metric popover and Edit dataset modal,enhancement:committed; viz:explore:dataset; viz:explore:metrics,2021-03-15 19:42:40 +0000 UTC,junlincc,In progress,,"Currently user can create saved from edit dataset modal, and ad hoc metrics from control panel popover using SQL expression. There's no way to validate the expression before saving. Invalid ones cause error when click RUN QUERY. 

Proposed change: Alarm user the syntax error on save metric(in both popover and edit dataset modal), instead of on RUN QUERY 

https://user-images.githubusercontent.com/67837651/111210889-083b8980-858b-11eb-8a61-e085d95ad980.mov

https://user-images.githubusercontent.com/67837651/111210943-1689a580-858b-11eb-8b4a-cfbab55a2a06.mov

",,,junlincc,"
--
@kgabryje if it takes less than 1 day to complete, let's tackle it. otherwise, @zhaoyongjie include this functionality in your redesign? 
--
",,,,,,,,,,
13631,OPEN,Superset 0.38; 0.37 (and potentially others) are broken with sqlachemy 1.4.0 release,#bug,2021-03-26 12:23:29 +0000 UTC,withernet,In progress,,"I was installing superset manually and was unable to get the installation to work properly; I had to downgrade to `0.37.2` and downgrade to `sqlalchemy==1.3.23`

### Expected results

Upon installation of `apache-superset=0.38.1` I expect `superset db upgrade` to work properly

### Actual results

The following error is thrown:

```
$ superset db upgrade
Traceback (most recent call last):
  File ""/etc/superset/bin/superset"", line 5, in <module>
    from superset.cli import superset
  File ""/etc/superset/lib/python3.6/site-packages/superset/__init__.py"", line 21, in <module>
    from superset.app import create_app
  File ""/etc/superset/lib/python3.6/site-packages/superset/app.py"", line 24, in <module>
    from flask_appbuilder import expose, IndexView
  File ""/etc/superset/lib/python3.6/site-packages/flask_appbuilder/__init__.py"", line 5, in <module>
    from .api import ModelRestApi  # noqa: F401
  File ""/etc/superset/lib/python3.6/site-packages/flask_appbuilder/api/__init__.py"", line 21, in <module>
    from .convert import Model2SchemaConverter
  File ""/etc/superset/lib/python3.6/site-packages/flask_appbuilder/api/convert.py"", line 4, in <module>
    from flask_appbuilder.models.sqla.interface import SQLAInterface
  File ""/etc/superset/lib/python3.6/site-packages/flask_appbuilder/models/sqla/interface.py"", line 16, in <module>
    from sqlalchemy_utils.types.uuid import UUIDType
  File ""/etc/superset/lib/python3.6/site-packages/sqlalchemy_utils/__init__.py"", line 1, in <module>
    from .aggregates import aggregated  # noqa
  File ""/etc/superset/lib/python3.6/site-packages/sqlalchemy_utils/aggregates.py"", line 372, in <module>
    from .functions.orm import get_column_key
  File ""/etc/superset/lib/python3.6/site-packages/sqlalchemy_utils/functions/__init__.py"", line 1, in <module>
    from .database import (  # noqa
  File ""/etc/superset/lib/python3.6/site-packages/sqlalchemy_utils/functions/database.py"", line 11, in <module>
    from .orm import quote
  File ""/etc/superset/lib/python3.6/site-packages/sqlalchemy_utils/functions/orm.py"", line 14, in <module>
    from sqlalchemy.orm.query import _ColumnEntity
ImportError: cannot import name '_ColumnEntity'
```

`pip list`:
```
$ pip list
Package                Version
---------------------- -----------
aiohttp                3.7.4.post0
alembic                1.5.7
amqp                   2.6.1
apache-superset        0.38.1
apispec                3.3.2
async-timeout          3.0.1
attrs                  20.3.0
Babel                  2.9.0
backoff                1.10.0
billiard               3.6.3.0
bleach                 3.3.0
Brotli                 1.0.9
cachelib               0.1.1
celery                 4.4.7
cffi                   1.14.5
chardet                4.0.0
click                  7.1.2
colorama               0.4.4
contextlib2            0.6.0.post1
croniter               1.0.8
cryptography           3.4.6
dataclasses            0.6
decorator              4.4.2
defusedxml             0.7.1
dnspython              2.1.0
email-validator        1.1.2
Flask                  1.1.2
Flask-AppBuilder       3.2.0
Flask-Babel            1.0.0
Flask-Caching          1.10.0
Flask-Compress         1.9.0
Flask-JWT-Extended     3.25.1
Flask-Login            0.4.1
Flask-Migrate          2.7.0
Flask-OpenID           1.2.5
Flask-SQLAlchemy       2.4.4
flask-talisman         0.7.0
Flask-WTF              0.14.3
future                 0.18.2
geographiclib          1.50
geopy                  2.1.0
greenlet               1.0.0
gunicorn               20.0.4
humanize               3.2.0
idna                   3.1
idna-ssl               1.1.0
importlib-metadata     3.7.3
isodate                0.6.0
itsdangerous           1.1.0
Jinja2                 2.11.3
jsonschema             3.2.0
kombu                  4.6.11
Mako                   1.1.4
Markdown               3.3.4
MarkupSafe             1.1.1
marshmallow            3.10.0
marshmallow-enum       1.5.1
marshmallow-sqlalchemy 0.23.1
msgpack                1.0.2
multidict              5.1.0
mysqlclient            2.0.3
natsort                7.1.1
numpy                  1.19.5
packaging              20.9
pandas                 1.1.5
parsedatetime          2.6
pathlib2               2.3.5
pip                    21.0.1
pkg-resources          0.0.0
polyline               1.4.0
prison                 0.1.3
py                     1.10.0
pyarrow                1.0.1
pycparser              2.20
PyJWT                  1.7.1
pyparsing              2.4.7
pyrsistent             0.17.3
python-dateutil        2.8.1
python-dotenv          0.15.0
python-editor          1.0.4
python-geohash         0.8.5
python3-openid         3.2.0
pytz                   2021.1
PyYAML                 5.4.1
retry                  0.9.2
selenium               3.141.0
setuptools             54.1.2
simplejson             3.17.2
six                    1.15.0
slackclient            2.5.0
SQLAlchemy             1.4.0
SQLAlchemy-Utils       0.36.8
sqlparse               0.3.0
typing-extensions      3.7.4.3
urllib3                1.26.4
vine                   1.3.0
webencodings           0.5.1
Werkzeug               1.0.1
wheel                  0.36.2
WTForms                2.3.3
WTForms-JSON           0.3.3
yarl                   1.6.3
zipp                   3.4.1
```

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug
It looks like currently superset does not support `sqlalchemy==1.4.0` (it was released less than an hour ago). To reproduce install manually (which default installs `sqlalchemy==1.4.0`

### Environment

(please complete the following information):

- superset version: `0.38.1`
- python version: `Python 3.6.9`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

I was able to get superset to work by downgrading:

```
$ pip install apache-superset==0.37.2
$ pip install sqlalchemy==1.3.23
$ superset db upgrade
logging was configured successfully
INFO:superset.utils.logging_configurator:logging was configured successfully
/etc/superset/lib/python3.6/site-packages/flask_caching/__init__.py:202: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
  ""Flask-Caching: CACHE_TYPE is set to null, ""
No PIL installation found
INFO:superset.utils.screenshots:No PIL installation found
WARNI [alembic.env] SQLite Database support for metadata databases will         be removed in a future version of Superset.
INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade  -> 4e6a06bad7a8, Init
...
```
",,,dpgaspar,"
--
Thank you for reporting.

This seems like an incompatibility between sqlalchemy-utils and the new sqlalchemy. Looking into it.

Has a note, pinning all your dependencies avoids this kind of problems, you can use superset's repo pinned dependencies: https://github.com/apache/superset/blob/master/requirements/base.txt

--

--
@withernet yes, look for `requirements.txt` at root level.

Looks like flask-sqlalchemy has issues with sqlalchemy 1.4.0: https://github.com/pallets/flask-sqlalchemy/issues/910

I'll pin Flask-AppBuilder bellow sqlalchemy 1.4.0 has a quick fix


--

--
@jonasjancarik I was able to reproduce, seems that flask-sqlalchemy has launched 2.5.0 that supports sqlalchemy > 1.4.0 already. I'll remove the pinned dependency on Flask-AppBuilder.

Note that removing Flask-AppBuilder SQLAlchemy pin under 1.4.0 will not totally resolve the issue, since `sqlalchemy-utils` is still incompatible with SQLAlchemy 1.4.0 and no new release is out yet.

So for all apache-superset versions equal and under 1.0.1, you need to `pip install ""SQLAlchemy<1.4.0""`.
Better yet:
>> pinning all your dependencies avoids this kind of problems, you can use superset's repo pinned dependencies: https://github.com/apache/superset/blob/master/requirements/base.txt

New apache-superset 1.1.0 pins SQLAlchemy bellow 1.4.0 and solves this issue


--
",withernet,"
--
Hi @dpgaspar, thanks for the info. It doesn't look like that file is in `0.37.2` tag? Is that something recently changed?
--
",jonasjancarik,"
--
Just to flag this again (and for future reference/others having the same issue), `pip install apache-superset` now displays `ERROR: flask-appbuilder 3.2.1 has requirement SQLAlchemy<1.4.0, but you'll have sqlalchemy 1.4.3 which is incompatible.` `pip install ""SQLAlchemy<1.4.0""` fixes that
--
",,,,,,
13623,OPEN,'Explore' button on SQL Lab view disabled when connected to Apache Pinot as a database,#bug,2021-03-15 14:25:02 +0000 UTC,ayush1705,Opened,,"I connected superset to pinot successfully and was able to build SQL lab queries only to find out that Superset does not support Exploring of SQL Lab virtual data as a chart if the connected database is Apache Pinot. (The Explore button is disabled)

I am using Superset version 1.0.1

#### Screenshot
![image](https://user-images.githubusercontent.com/17423001/111168535-86dafb80-8578-11eb-87ea-3e5058e4d4cb.png)

",,,,,,,,,,,,,,
13616,OPEN,Support for OR expressions in the adhoc_filters chart explorer control,,2021-03-14 12:43:57 +0000 UTC,cccs-jc,Opened,,"The adhoc_filters in the chart explorer only support AND expressions is there plans to add support for OR expressions?

For example there is no easy way to query for various city suffix using an OR expression, you have to revert to custom SQL to accomplish this.

![city-suffix](https://user-images.githubusercontent.com/56140112/111068975-48bed880-84a1-11eb-805d-366d777cfc50.png)

",,,,,,,,,,,,,,
13615,OPEN,"[Explore]""?"" datatype icon for float is very confusing",bash!; bug:cosmetic; viz:explore:datapanel,2021-03-14 05:45:07 +0000 UTC,junlincc,Opened,,"<img width=""986"" alt=""Screen Shot 2021-03-13 at 9 37 51 PM"" src=""https://user-images.githubusercontent.com/67837651/111058584-7cb7e080-8444-11eb-9b84-b91f4c5f2692.png"">

We recently added ""?"" icon to all the columns with data type that are not being able to detected. 
now ? is showing on all the double precision columns. Double precision should have its own icon... now it's super confusing. 
<img width=""570"" alt=""Screen Shot 2021-03-13 at 9 41 54 PM"" src=""https://user-images.githubusercontent.com/67837651/111058696-f2bc4780-8444-11eb-91cd-038cc4c90cef.png"">


@zhaoyongjie @villebro",,,,,,,,,,,,,,
13612,OPEN,[chart]KeyError in Bar Chart if value is null,viz:chart-bar,2021-03-14 05:17:38 +0000 UTC,zufolo441,Opened,,"Starting from version 1.0.1 Bar chart graph stopped working if one  metric value is null.
I have a metric so defined: 
AVG(indicatore) FILTER (WHERE codlivterr = '15' and selezionato = 1) AS ""Italia selezionata"" 
if the result is null I obtain this error:
KeyError: ""['Italia selezionata'] not in index""
I had to modify in: 
coalesce(AVG(indicatore) FILTER (WHERE codlivterr = '15' and selezionato = 1),0) AS ""Italia selezionata"" 

Up to v. 1.0 null values were accepted.

DB is PostgreSQL.
",,,,,,,,,,,,,,
13609,OPEN,The SQL is incorrect using the time type field,#bug; data:connect:clickhouse; viz:explore:timepick,2021-03-13 10:19:27 +0000 UTC,chen-ABC,Opened,,"
![image](https://user-images.githubusercontent.com/30097790/111022080-853ff500-840b-11eb-9d53-0fc0861f1747.png)


```
SELECT toStartOfMinute(toDateTime(toDateTime((timestamp/1000)))) AS timestamp,
       COUNT(*) AS count
FROM pupuolap.component_dependency_records_distributed
WHERE timestamp >= 1614960000000
  AND timestamp < 1615564800000
GROUP BY toStartOfMinute(toDateTime(toDateTime((timestamp/1000))))
ORDER BY count DESC
LIMIT 10000;
```
![image](https://user-images.githubusercontent.com/30097790/111021831-05fdf180-840a-11eb-8cb2-506f9ad581be.png)

The alias name should not match the original field name.

### Environment

(please complete the following information):

- superset version: `0.38.0`
- python version: `3.7.9`
- node.js version: `v12.18.3`
",,,zhaoyongjie,"
--
hi @chen-ABC 
Why is `date picker` generated long int in `where clause`? could you show me the full screenshot on the explore page?
Could you test this dataset in Superset 1.0?
--

--
From the CK error log seems that you use incompatible data type calculation. please double check table schema in clickhouse
--
",chen,"
--
@zhaoyongjie   
Because my time field is a timestamp,
![image](https://user-images.githubusercontent.com/30097790/111023493-567a4c80-8414-11eb-8d50-953201c147ae.png)

--
",,,,,,,,
13604,OPEN,Unexpected error 'version' when trying to connect to Druid Datasource,data:connect:druid,2021-03-14 05:19:12 +0000 UTC,ankitmani2004,Opened,,"A clear and concise description of what the bug is.

I have Druid cluster and Superset running in Kube. Druid has 2 datasources. I have used amancevice superset image. My dockerfile is given below:

FROM amancevice/superset:latest
USER root
COPY config_frm_amancevice.py /usr/local/lib/python3.8/site-packages/superset/config.py
    RUN pip install pydruid
    RUN pip install ""apache-superset[druid]""
    USER superset


config_frm_amancevice.py file updating DRUID_IS_ACTIVE to True
### Expected results

to connect the datasource in Druid

### Actual results

Getting Error in UI:
Unexpected error
'version'

detailed log:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/superset/viz.py"", line 540, in get_df_payload
    df = self.get_df(query_obj)
  File ""/usr/local/lib/python3.8/site-packages/superset/viz.py"", line 270, in get_df
    self.results = self.datasource.query(query_obj)
  File ""/usr/local/lib/python3.8/site-packages/superset/connectors/druid/models.py"", line 1390, in query
    query_str = self.get_query_str(client=client, query_obj=query_obj, phase=2)
  File ""/usr/local/lib/python3.8/site-packages/superset/connectors/druid/models.py"", line 968, in get_query_str
    return self.run_query(client=client, phase=phase, **query_obj)
  File ""/usr/local/lib/python3.8/site-packages/superset/connectors/druid/models.py"", line 1165, in run_query
    self.cluster.get_druid_version()
  File ""/usr/local/lib/python3.8/site-packages/superset/connectors/druid/models.py"", line 193, in get_druid_version
    return json.loads(requests.get(endpoint, auth=auth).text)[""version""]
KeyError: 'version'


#### How to reproduce the bug

1. Go to 'Superset UI'
2. Click on 'Data' then on ""Druid Clusters""
3. Give Druid broker host and port name
4. Click on 'Data' then on ""Druid Datasources""
5. Give the datasource name and SAVE
6. click on Datasource name
7. See error

### Environment

(please complete the following information):

- superset version: 1.0.1
- python version: 3.8

",,,,,,,,,,,,,,
13600,OPEN,[chart]multiple charts broken on maximizing,bash!; good first issue; viz:dashboard:max,2021-03-31 16:23:21 +0000 UTC,vivekdixit2510,In progress,,"## Screenshot
Without Maximize 
![max1](https://user-images.githubusercontent.com/80536487/110966903-ca1c4b00-837b-11eb-98ef-8017e1df7067.PNG)
With Maximize 
![max2](https://user-images.githubusercontent.com/80536487/110966937-d56f7680-837b-11eb-8a17-6b4d4f3adc39.PNG)

Without Maximize (bar graph)

![max3](https://user-images.githubusercontent.com/80536487/110967599-942b9680-837c-11eb-9774-86095b20b390.PNG)

With Maximize (scroll till bottom)
![max4](https://user-images.githubusercontent.com/80536487/110967648-a4437600-837c-11eb-94d4-1a913fd2415c.PNG)



[drag & drop image(s) here!]

## Description

Pie Charts and Bar Graphs have broken on maximizing. 

## Design input
Pie Chart: It should not disappear from the screen.
Bar Graph: It should fit on the page. 
@slayerjain",,,vivekdixit2510,"
--
Alternative, Is there a way to disable the maximize option in superset? 
--
",junlincc,"
--
thanks for reporting! this is a known issue. if you already have a solution to fix either or both charts, we will make sure your PR is reviewed.  @vivekdixit2510 
--

--
i dont think so, unfortunately. @slayerjain 
--

--
world-map is also broken

related https://github.com/apache/superset/issues/13297
--
",slayerjain,"
--
@junlincc is it possible to disable the maximise button in dashboards for the time being?
--
",,,,,,
13599,OPEN,Setup Auto-Deploy Superset Documentation,doc:developer,2021-03-23 00:28:30 +0000 UTC,srinify,Opened,,"Since we've moved over to DocZ for the Apache site (superset.apache.org), we've had to manually build and git push assets to the `superset-site` folder (https://github.com/apache/superset-site).

@nytai mentioned that we could setup auto deploy so this step would be eliminated. Creating this issue to keep track of this work / request!  ",,,nytai,"
--
Opened https://issues.apache.org/jira/browse/INFRA-21607 as the fist step
--
",,,,,,,,,,
13594,OPEN,Paired t-test Table can't render,#bug,2021-03-15 01:01:23 +0000 UTC,code-snail,Opened,,"A clear and concise description of what the bug is.

### Expected results

It should show table values like p-valuelift.

### Actual results

show empty table.

#### Screenshots
![image](https://user-images.githubusercontent.com/65579137/110917902-6d0d9e80-8355-11eb-8907-637564d4db9c.png)

![image](https://user-images.githubusercontent.com/65579137/110917870-60894600-8355-11eb-86ab-5874555d46de.png)


#### How to reproduce the bug

1. create Paired t-test Table chart
2. select metrics and group fields
3. click run

### Environment

(please complete the following information):

- superset version: 0.17.12
- python version: Python 3.7.9
- node.js version: v14.0
### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context


",,,,,,,,,,,,,,
13591,OPEN,[New feature for testing]Dashboard native filter phase 2(Full feature parity & Enhancement),viz:dashboard:native-filter,2021-04-02 21:31:09 +0000 UTC,junlincc,In progress,,"We have been iterating on Dashboard native filer behind the feature flag after announcing phase 1 testing https://github.com/apache/superset/issues/12148

All bugs that are identified have been fixed, lots of new features have been added since then, based on request and feedback from the community. We are heavily testing this week and will make all features fully available in days. 

Many thanks to all the contributors(phase 2) 
Project lead: @villebro 
@simcha90 @agatapst @zhaoyongjie @amitmiran137 @suddjian

<img width=""1892"" alt=""Screen Shot 2021-03-11 at 9 44 42 PM"" src=""https://user-images.githubusercontent.com/67837651/110897832-06d03f80-82b3-11eb-8d4b-03e563b5bb73.png"">

1. Range filter!! 
User now can set up a range filter by selecting a numeric column. filter detects min and max values as DEFAULT VALUE. Set filter range by sliding.  
<img width=""405"" alt=""Screen Shot 2021-03-11 at 9 18 19 PM"" src=""https://user-images.githubusercontent.com/67837651/110895963-80fec500-82af-11eb-9583-1a8b1225cdc7.png"">

2. Native time column select- Support multiple time columns!! 
<img width=""1885"" alt=""Screen Shot 2021-03-11 at 9 08 03 PM"" src=""https://user-images.githubusercontent.com/67837651/110896105-c3280680-82af-11eb-8e4b-e72809573463.png"">

3. Native time grain filter!!! 
<img width=""398"" alt=""Screen Shot 2021-03-11 at 9 22 19 PM"" src=""https://user-images.githubusercontent.com/67837651/110896187-ef438780-82af-11eb-81ee-6192203c0a4f.png"">

 4. Native new time filter - support LAST, PREVIOUS, CUSTOM, free text and ADVANCED syntax!! 
<img width=""777"" alt=""Screen Shot 2021-03-11 at 9 23 45 PM"" src=""https://user-images.githubusercontent.com/67837651/110896580-c079e100-82b0-11eb-9265-9f2a8df126f8.png"">

5. Sort value and multiple select by default
<img width=""963"" alt=""Screen Shot 2021-03-11 at 9 47 11 PM"" src=""https://user-images.githubusercontent.com/67837651/110898042-60386e80-82b3-11eb-9000-b08eb9bada67.png"">

To test, set ""DASHBOARD_NATIVE_FILTERS"": True in config.py 

Please comment in the thread if you find any issues. We will address reported issues(bug) ASAP! 
New feature requests will be considered, and implemented for next iteration in the future. ",,,junlincc,"
--
Request for range filter: 
Display(materialize) actual range value on filter for Range filter without hovering
<img width=""1178"" alt=""Screen Shot 2021-03-12 at 9 42 29 AM"" src=""https://user-images.githubusercontent.com/67837651/110977709-52b4d000-8317-11eb-909b-f786bc6d3e2b.png"">

<img width=""1119"" alt=""Screen Shot 2021-03-12 at 9 48 15 AM"" src=""https://user-images.githubusercontent.com/67837651/110978325-17ff6780-8318-11eb-9b24-fbc3081736f8.png"">


CC @simcha90 @mihir174 
--

--
Request for range filter:

Instead of showing ""no result"" in the loading state, can we use the infinity loading logo?

https://user-images.githubusercontent.com/67837651/110978057-c2c35600-8317-11eb-8b96-6691f5a7853d.mov

CC @simcha90 @mihir174 
--

--
<img width=""1773"" alt=""Screen Shot 2021-03-12 at 1 46 00 PM"" src=""https://user-images.githubusercontent.com/67837651/111001855-683af180-8339-11eb-9a91-cfe185193b2d.png"">
<img width=""1165"" alt=""Screen Shot 2021-03-12 at 10 28 59 AM"" src=""https://user-images.githubusercontent.com/67837651/111002042-95879f80-8339-11eb-8e00-176dfc6439a7.png"">


Request:
Prompt user to change filter name if is duplicate. the same one just keep loading.. 
--

--
<img width=""1200"" alt=""Screen Shot 2021-03-12 at 9 49 57 AM"" src=""https://user-images.githubusercontent.com/67837651/111002072-a59f7f00-8339-11eb-9c30-90ae7e7c39bc.png"">

Request: makes field collapsible  
--

--
@graceguo-supercat @zuzana-vej @ktmud 
As promised, dashboard filter has reached feature parity and beyond, tested internally. we identify quite a few non-blocker issues above^^^^. While we address those remaining issues, I wonder if we can start filter migration as discussed last year? 
--

--
@PowerPlop 

> will the viz 'filter_box' be deprecated in the future or will it stay? I would say that the filter box viz is preferred for very simple filters.

It's still in discussion when we are gonna deprecate it, but yes, very likely we will. what do you mean by _very simple filters_? 

> Is there already an option to save a dashboard with preselected native filters (as with filter box)?
good point. not yet, but we will add it before deprecating the FilterBox. 
--

--
https://user-images.githubusercontent.com/67837651/113214642-9fad0780-922e-11eb-8d8e-179739f0c1d6.mov

need better error message for non compatible time grain filter. 
--
",mihir174,"
--
UX Issue:

There are 2 'Apply' buttons in the flow of modifying a Time Range native filter - 
1. In the filter bar
2. In the Time Range filter popover


The one in the popover (shown in screenshot) is misleading because it doesn't actually apply the filter, and is clicked first.
<img width=""592"" alt=""Screen Shot 2021-03-12 at 3 04 39 PM"" src=""https://user-images.githubusercontent.com/64227069/111007783-79d5c680-8344-11eb-8c68-097cc4f91c34.png"">

This button could be changed to 'Save' or 'Update'
--

--
These buttons don't appear to be clickable because the cursor doesn't change to a pointer when hovering over them, and there is no hover state

<img width=""246"" alt=""Screen Shot 2021-03-12 at 3 09 41 PM"" src=""https://user-images.githubusercontent.com/64227069/111008015-0b453880-8345-11eb-9820-7efeb4bbc48c.png"">

--

--
Question: should the available list of datasources here be limited to those referenced by the charts on this dashboard?

<img width=""977"" alt=""Screen Shot 2021-03-12 at 3 12 32 PM"" src=""https://user-images.githubusercontent.com/64227069/111008188-6119e080-8345-11eb-93ee-f4d1c4a6f6d4.png"">

--

--
After x-ing out of the warning message that appears when you click 'Cancel' with unsaved changes, the main Save and Cancel buttons disappear and the main modal X doesn't work until another edit is made
https://user-images.githubusercontent.com/64227069/111008347-c1a91d80-8345-11eb-9d4a-fff52834f0c2.mov


--

--
We can get rid of 'Time Column' as a filter type and instead add a field under the 'Time Range' filter type to select a time column.

Time Column is currently redundant as a filter type, and the system currently chooses a potentially undesirable default time column for Time Range when there is more than 1 time column
--
",suddjian,"
--
Non-blocking suggestion: when the filters sidebar is empty, instead of a pencil icon it should have a plus

<img width=""265"" alt=""Screen Shot 2021-03-12 at 3 21 51 PM"" src=""https://user-images.githubusercontent.com/1858430/111009071-76900a00-8347-11eb-9118-cc4bb79a3e4b.png"">

--

--
Non-blocking suggestion: the Parent Filter dropdown should allow you to enter a name that does not exist to create a new filter. (similar to how in Explore you can enter a name and create a dashboard when saving the chart)
--

--
Suggestion: layout change. The organizing factor being data on the left, behavior on the right.

<img width=""949"" alt=""Screen Shot 2021-03-12 at 3 30 45 PM"" src=""https://user-images.githubusercontent.com/1858430/111009588-e5219780-8348-11eb-8cf4-1d40a757866e.png"">

--

--
All my filters have ""Apply changes instantly"" enabled, but the ""Apply"" button becomes clickable anyway (does nothing)

<img width=""261"" alt=""Screen Shot 2021-03-12 at 3 44 46 PM"" src=""https://user-images.githubusercontent.com/1858430/111009956-e2737200-8349-11eb-891b-c36b42e94854.png"">

--

--
Non-blocking suggestion: There is no indication on hover that these icons are interactive. They should use `cursor: pointer` and should probably have a popover explaining what they do.

<img width=""253"" alt=""Screen Shot 2021-03-12 at 3 48 39 PM"" src=""https://user-images.githubusercontent.com/1858430/111010178-91b04900-834a-11eb-8ebd-1064493e5cec.png"">

--

--
Clicking on the label does not activate the radio button

<img width=""318"" alt=""Screen Shot 2021-03-12 at 4 02 19 PM"" src=""https://user-images.githubusercontent.com/1858430/111010775-56af1500-834c-11eb-810f-90051566d7d2.png"">

--

--
Non-blocking suggestion: The Filter Type input should only offer filter types that are compatible with one of the dashboard's datasets
--
",daniel10012,"
--
@junlincc 
all datasets don't show in filter dropdown
![image](https://user-images.githubusercontent.com/10360991/111009383-7997f400-8361-11eb-8fb1-e05b2d536501.png)

--

--
Cascading filter indicator needs to be different than the regular filter indicator
![image](https://user-images.githubusercontent.com/10360991/111009695-5e79b400-8362-11eb-95e2-a3062eae853b.png)

--

--
Parent and child switch randomly 
![image](https://user-images.githubusercontent.com/10360991/111009872-c16b4b00-8362-11eb-9349-b72d6e3956c4.png)



--

--
Remove parent filter when doing a time range
![image](https://user-images.githubusercontent.com/10360991/111010025-47879180-8363-11eb-8645-a7ff11e379a2.png)

--
",PowerPlop,"
--
@junlincc  will the viz 'filter_box' be deprecated in the future or will it stay? I would say that the filter box viz is preferred for very simple filters.

Is there already an option to save a dashboard with preselected native filters (as with filter box)? 
--

--
@junlincc  Some elaboration on 'simple filter:

The native filter groups all filters in one generic sidebar which might be useful if you have a lot of filters that apply to all slices. 
If however you have only a small set of filters (or just one), the side bar layout is in my opinion unneccesary and less user friendly. Currently we use for example one filter box with 2 entries at the top of page.
Or we use multiple filter boxes (different tabs) that each filter only a specific set of slices. The position of the filterbox makes it clear to the user which slices will be filtered.
When using the sidebar it is unclear which filter will apply to which slice (only visible after you applied it).
Other BI tools such as Tableau keep the dasbhoard filter as a dashboard component that has to be positioned somewhere on the dashboard instead of external sidebar.

--

--
@villebro If there is still a separate filter viz, that you can ignore my comments. 
Adding an option to disable the native dashboard filter sidebar on a dashboard might make sense, if you have cross filters in the dashboard. Or how do you plan to sync both filter options?
--
",villebro,"
--
> @junlincc Some elaboration on 'simple filter:
> 
> The native filter groups all filters in one generic sidebar which might be useful if you have a lot of filters that apply to all slices.
> If however you have only a small set of filters (or just one), the side bar layout is in my opinion unneccesary and less user friendly. Currently we use for example one filter box with 2 entries at the top of page.
> Or we use multiple filter boxes (different tabs) that each filter only a specific set of slices. The position of the filterbox makes it clear to the user which slices will be filtered.
> When using the sidebar it is unclear which filter will apply to which slice (only visible after you applied it).
> Other BI tools such as Tableau keep the dasbhoard filter as a dashboard component that has to be positioned somewhere on the dashboard instead of external sidebar.

FYI we've retained the possibility of adding native filters as regular chart components, in which case they work much like filter boxes (in this case they become charts with cross filters rather than native filters). We still need to refine the UX, but check this PR to see how this feature is progressing (in the example the native select filter is added to the dashboard): #13625
--

--
@PowerPlop native filters (the one on the left) will be the preferred way to add filters to a dashboard, as the majority of users want to target all charts in a dashboard. However, for the use case you are facing it will be possible to use filter plugins that support cross filtering mode as chart elements (e.g. range and select support it), in which case they are cross filtering charts. If a dashboard doesn't have any native filters, the filter tab is collapsed by default.
--
"
13590,OPEN,[sql_lab]refresh metadata when user input non-select statement,bug; sql_lab,2021-03-12 03:36:19 +0000 UTC,junlincc,Opened,,"<img width=""1666"" alt=""Screen Shot 2021-03-11 at 7 08 14 PM"" src=""https://user-images.githubusercontent.com/67837651/110886293-45f39600-829d-11eb-81c4-5d35d9e26bf4.png"">

I dropped a table named ""calcs"" in SQL editor and it ran successfully. what I expected was 
1) ""calcs"" is removed from the dropdown list in ""SEE TABLE SCHEMA"" 
2) ""calcs"" table metadata preview is removed from the left panel.  Refreshing metadata should also be applied in any other DML, e.g. ALTER 

cc @eschutho @betodealmeida @mistercrunch ",,,,,,,,,,,,,,
13589,OPEN,[SQL Lab] Double click to select cell bleeds into adjacent cell,bug; sql_lab,2021-03-12 08:27:26 +0000 UTC,zuzana-vej,Opened,,"Description:
When user wants to copy individual cell contents from the results of a query in SQL Lab, they often double click the cell to select the value. Often (not always) it selects that value plus part of the next column too -  bleeding into the adjacent cell and selecting a mix of both. Selecting by clicking and dragging is equally unreliable.


### Expected results
Selection spans more than the selected cell.

### Actual results
Selects content of that column + part of next column. 

Workaround - only option is Export it to csv.

#### Screenshots
video 1:
![issue-002](https://user-images.githubusercontent.com/61221714/110886235-31af9900-829d-11eb-9707-ddd73688f3f3.gif)
video 2:
![issue-001](https://user-images.githubusercontent.com/61221714/110886242-33795c80-829d-11eb-818f-8c6761c31e60.gif)


#### How to reproduce the bug
1. Run a query in SQL Lab.
2. Try to select a cell's contents by double clicking the cell. 
3. In case you can't repro the bug, try to run a larger query (e.g. more columns)

### Environment

(please complete the following information):

- superset version: `master`
- Browser: Chrome

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,zuzana,"
--
FYI @yousoph 
Unsure if this is regression or not (haven't noticed this in the past). Pretty minor bug, but the workaround (csv download) kind of sucks.
--
",yousoph,"
--
Weird! It seems like triple clicking is giving the expected double click behavior right now... will look into it. 
--
",,,,,,,,
13582,OPEN,Issues in editing dashboard properties,#bug,2021-03-11 17:36:10 +0000 UTC,singh-ab,Opened,,"Cannot edit the properties for all the dashboard  which have been migrated from 0.38.0 to 1.0.1 

### Expected results

Edit the dashboard properties for existing dashboards

### Actual results

404 BAD Request.

#### Screenshots

![image](https://user-images.githubusercontent.com/70583041/110827620-8e5b8580-8296-11eb-9a6b-2b2d9834954c.png)


#### How to reproduce the bug

1. Go to any dashboard created before the upgrade
2. Click on ""edit dashboard properties"" & try to edit the name or click advanced to change other options
3. Click on ""Save""
4. Nothing happens on the UI , the background error ( attached as screenshot )

### Environment

(please complete the following information):

- superset version: `1.0.1`
- mysql version: `5.7.12`
- alembic version: `c878781977c6`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

An interesting observation which works as a fix ( not the correct way , since that doesn't explain the issue ) 

Follow until step-2 on ""How to reproduce the bug"" section  , after that : 

1. Expand the ""Advanced"" section.
2. Remove the ""import_time"" & ""remote_id"" keys 
3. Clicking on save should work .

Example : 

doesn't work : 
```
{
    ""timed_refresh_immune_slices"": [],
    ""expanded_slices"": {},
    ""refresh_frequency"": 0,
    ""default_filters"": ""{}"",
    ""color_scheme"": null,
    ""import_time"": 1551707490,
    ""remote_id"": 24,
}

```

works:
```
{
    ""timed_refresh_immune_slices"": [],
    ""expanded_slices"": {},
    ""refresh_frequency"": 0,
    ""default_filters"": ""{}"",
    ""color_scheme"": null
}
```",,,,,,,,,,,,,,
13580,OPEN,Display actual value (in tooltip) for 100% stacked bar charts,enhancement:committed; viz:chart-bar,2021-03-31 05:24:29 +0000 UTC,ValentinC-BR,Opened,,"### Feature Request 

The 100% stacked bar chart (bar chart > Contribution) only display the percentage in the tooltip when you hover on the bars.

Displaying (by default or not) the actual value of the bar would be a great enhancement.

### Example

Let's say I have : 
* 5 female & 5 male viewers on Monday (50/50) 
* 15 female & 5 male viewers on Tuesday (75/25)

Currently, the 100% staked bar chart only display ""50%"" and ""75%"" if I hover on the ""female viewers"" bars

What I'd like (in the tooltip) : 
* ""5 (50%)"" 
OR
*  ""50% (5)"" 
OR
*  ""[Name of the metric] : 5, Contribution : 50%""",,,junlincc,"
--
@ValentinC-BR Thanks for suggesting! it makes sense. we will include it in the new Echarts bar. 
--
",,,,,,,,,,
13579,OPEN,Wrong color for bar charts,bash!; good first issue,2021-03-31 05:08:11 +0000 UTC,ValentinC-BR,Opened,,"## Screenshot

![Prsentation sans titre](https://user-images.githubusercontent.com/79460908/110811066-a75c3a80-8286-11eb-80f4-0d3e8cffcc78.png)


## Description

I've set custom colors in the Dashboard properties.
Those colors seems to be pretty well taken into account but bar charts kind of convert them into ""pastel colors"" (see screenshot).

By the way, the pie chart colors are not exactly the good ones (#f97f44 instead of #ff7f44 in this example) but the difference is not visible, contrary to bar charts.  

## Environment

superset version: 1.0.1
browser : chrome 89.0",,,,,,,,,,,,,,
13572,OPEN,"ClickHouse some ENUM types doesn't stores in ""datasources"" - psycopg2.errors.StringDataRightTruncation: value too long for type character varying(32)",#bug; data:connect:clickhouse,2021-04-09 08:02:10 +0000 UTC,Slach,In progress,,"I setup superset 1.0.1 + master version clickhouse driver from git https://github.com/xzkostyan/clickhouse-sqlalchemy
I successfully added ""database"" and try to add new ""dataset"", and press ""Add""
![image](https://user-images.githubusercontent.com/105560/110770835-9fe36400-827b-11eb-80b9-fa09dbd7fdca.png)

### Expected results
Successful added dataset


### Actual results

![image](https://user-images.githubusercontent.com/105560/110770875-ad98e980-827b-11eb-8b53-99586fd1c4f3.png)


### Additional context
Stacktrace
```
superset_app            | [SQL: INSERT INTO table_columns (uuid, created_on, changed_on, column_name, verbose_name, is_active, type, groupby, filterable, description, table_id, is_dttm, expression, python_date_format, created_by_fk, changed_by_fk) VALUES (%(uuid)s, %(created_on)s, %(changed_on)s, %(column_name)s, %(verbose_name)s, %(is_active)s, %(type)s, %(groupby)s, %(filterable)s, %(description)s, %(table_id)s, %(is_dttm)s, %(expression)s, %(python_date_format)s, %(created_by_fk)s, %(changed_by_fk)s) RETURNING table_columns.id]
superset_app            | [parameters: {'uuid': UUID('0c461639-f66d-46b2-ae56-e994bee37d2d'), 'created_on': datetime.datetime(2021, 3, 11, 9, 40, 12, 26645), 'changed_on': datetime.datetime(2021, 3, 11, 9, 40, 12, 26678), 'column_name': 'event_type', 'verbose_name': None, 'is_active': True, 'type': ""ENUM8('COMMITCOMMENTEVENT' = 1, 'CREATEEVENT' = 2, 'DELETEEVENT' = 3, 'FORKEVENT' = 4, 'GOLLUMEVENT' = 5, 'ISSUECOMMENTEVENT' = 6, 'ISSUESEVENT' = 7, ... (184 characters truncated) ...  'GISTEVENT' = 16, 'FOLLOWEVENT' = 17, 'DOWNLOADEVENT' = 18, 'PULLREQUESTREVIEWEVENT' = 19, 'FORKAPPLYEVENT' = 20, 'EVENT' = 21, 'TEAMADDEVENT' = 22)"", 'groupby': True, 'filterable': True, 'description': None, 'table_id': 17, 'is_dttm': False, 'expression': None, 'python_date_format': None, 'created_by_fk': 1, 'changed_by_fk': 1}]
superset_app            | (Background on this error at: http://sqlalche.me/e/13/9h9h)
superset_app            | Traceback (most recent call last):
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1277, in _execute_context
superset_app            |     cursor, statement, parameters, context
superset_app            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute
superset_app            |     cursor.execute(statement, parameters)
superset_app            | psycopg2.errors.StringDataRightTruncation: value too long for type character varying(32)
```

How exactly should converts ENUM8 type from ClickHouse to superset?",,,hodgesrm,"
--
Hi @Slach !  This looks a little weird.  psycopg2 is the PostgreSQL driver.  Are you running Superset in Docker with PostgreSQL as the backend? 

(Superset folks, @Slach is working with me on ClickHouse driver support.)
--

--
Here's a detailed reproduction of this issue.  It does not come up in SQLite when I use a virtual env and dev setup.  It seems to be related to the VARCHAR(36) column size in PostgreSQL. 

1. Update your [virtual] environment to install master version of clickhouse-sqlalchemy from github: `pip install git+https://github.com/xzkostyan/clickhouse-sqlalchemy`
2. Go into Databases based and set up a database with the following SQLAlchemy URL: `clickhouse+native://demo:demo@github.demo.trial.altinity.cloud/default?secure=true`
3. Save the database and go to Datasets panel. 
4. Select a new dataset as shown in the image in the bug description.  Schema is 'default' and the table name is 'github_events'. 
5. Press Save to see the error.  

ClickHouse Enum types are potentially very large because they list all values.  It's hard to give an upper bound for ClickHouse types but if you are storing ClickHouse native types names in VARCHAR I would allocate at least 500 characters.  

Another option obviously would be to convert to standard SQL types within Superset, which I understand is possible to do. 
--
",Slach,"
--
@hodgesrm when we try to add ""dataset"" then superset itself try to store selected table from Datasource fields names and types into internal PostgreSQL \ MySQL \ sqlite table with name `table_columns`
--
",,,,,,,,
13571,OPEN,Dashboards not loading in Mozilla Firefox Private Window,browser:firefox,2021-04-09 07:58:20 +0000 UTC,rvabhijith,Opened,,"Why the superset dashboards are not loading in mozilla firefox private window
PFA
![Screenshot 2021-03-11 at 2 55 51 PM](https://user-images.githubusercontent.com/39004901/110765773-87267e80-827a-11eb-82a6-fd72d562f08c.png)

",,,cleman95,"
--
Hello, 
I have the same issue with superset version 1.0.1. I can't access any databases or dashboards in private window on Firefox but i can in normal windows and in Chrome (normal and private windows).


I get a log in the browser: ""DOMException: The operation is insecure.""
in the sql lab i can't access my databases i got ""error while fetching database list"" ""error while fetching schema list""
--
",,,,,,,,,,
13565,OPEN,Error docker-compose-non-dev.yml with AUTH_TYPE = AUTH_DB,install:docker,2021-03-31 04:58:36 +0000 UTC,GGPay,Opened,,"Running docker-compose-non-dev.yml  with additional requirements-local.txt

blinker==1.4 
Flask-Mail==0.9.1 

Got an error on a screenshot below  after fill in registration form and click button ""save""

![image](https://user-images.githubusercontent.com/17413180/110739389-b1365d00-81f6-11eb-9c50-e6a518398a5e.png)

### Expected results

Get an email with activation link

#### Screenshots
![image](https://user-images.githubusercontent.com/17413180/110739206-54d33d80-81f6-11eb-8ba7-b555a1931b68.png)


If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. docker-compose-non-dev.yml 
2. requirements-local.txt with blinker==1.4  and Flask-Mail==0.9.1 
3. set up email's credential in superset_config.py 
4. docker-compose-non-dev.yml up
5. try register new account 
4. See error

### Environment

- superset version: superset-image apache/superset:latest


### Additional context

Don't see that error in docker-compose.yml


Probably issue with volumes.

When run pip install Flask-Mail in superset_app - got an error on a screenshot below.

![image](https://user-images.githubusercontent.com/17413180/110740092-0fb00b00-81f8-11eb-9d15-047cb1af2b51.png)




",,,,,,,,,,,,,,
13562,OPEN,[pivot table] Query with ROW LIMIT,bug; test:case; viz:chart-pivot,2021-03-11 18:27:26 +0000 UTC,graceguo-supercat,Opened,,"#### How to reproduce the bug

1. Create a PivotTable chart,
2. Add ROW LIMIT = 200
3. Check generated query by click `VIEW QUERY`

### Actual results
```
SELECT ""dim_destination_geo"" AS ""dim_destination_geo"",
       date_trunc('week', CAST(""night_in_latest_year"" AS TIMESTAMP)) AS ""night_in_latest_year"",
       ...
FROM ""tmp"".""forward_occupancy_forecast_superset_table""
WHERE ""night_in_latest_year"" >= '2021-03-11 00:00:00.000000'
  AND ""night_in_latest_year"" < '2021-06-15 00:00:00.000000'
ORDER BY ""Nights Backlog"" DESC
LIMIT 200;
```

Instead of filtering out the top K rows in the superset view, it filters out the top K rows in the input data; this results in the pivot table having lots of missing cells.


### Expected results
Superset should generate a sub-query with limit like this:
```
SELECT ""dim_destination_geo"" AS ""dim_destination_geo"",
       date_trunc('week', CAST(""night_in_latest_year"" AS TIMESTAMP)) AS ""night_in_latest_year"",
       ...
FROM ""tmp"".""forward_occupancy_forecast_superset_table""
WHERE ""night_in_latest_year"" >= '2021-03-11 00:00:00.000000'
  AND ""night_in_latest_year"" < '2021-06-15 00:00:00.000000'
       AND (dim_destination_geo IN
              (SELECT dim_destination_geo
               FROM
                 (SELECT dim_destination_geo,
                         sum(current_forward_nights_occupied)
                  FROM tmp.forward_occupancy_forecast_superset_table
                  WHERE is_most_recent_view = True
                  GROUP BY 1
                  order by 2 DESC
                  LIMIT 200))))
ORDER BY ""Nights Backlog"" DESC
LIMIT 50000;
```




### Environment

latest master branch

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
cc @junlincc @zuzana-vej ",,,junlincc,"
--
Hi @graceguo-supercat, can you attach a screenshot with all the control select. I assume you set filters and groud-by as well? 

Couple PRs touched Pivot table lately. may need some help debugging. sorry about the regression. 
https://github.com/apache-superset/superset-ui/pull/954
https://github.com/apache/superset/pull/13057 (more likely?) 

cc @villebro 
--
",villebro,"
--
I don't believe this is a regression - this has most likely always been the case. I agree that the proposal is the correct way to filter the data. I believe we should be able to do this by leveraging the `timeseries_limit` (although I'd really like to refactor this so that the feature isn't related to timeseries): https://github.com/apache/superset/blob/5fca19da565cae1e70a0dff66007828dbe9fc8ed/superset/connectors/sqla/models.py#L1177-L1224
--
",graceguo,"
--
@junlincc this is controls list:

<img width=""320"" alt=""Screen Shot 2021-03-11 at 10 17 33 AM"" src=""https://user-images.githubusercontent.com/27990562/110835896-3dc33880-8254-11eb-8a74-8c491418bf0c.png"">

--
",,,,,,
13560,OPEN,[SQL Lab --> Explore] Better Error message when virtual dataset name exists,enhancement:request; sql_lab,2021-03-11 02:32:16 +0000 UTC,zuzana-vej,Opened,,"SQL Lab now displays a popup window to let user name their virtual datasource as a part of SQL Lab --> Explore flow. YAY! :) 
When use adds a virtual dataset name which already exists, there is a small error at the bottom right of the page which just says ""An Error occurred saving dataset"". This is a bit confusing / doesn't help user to address the issue.
<img width=""288"" alt=""Screen Shot 2021-03-10 at 1 37 58 PM"" src=""https://user-images.githubusercontent.com/61221714/110701429-026a3080-81a6-11eb-95f8-96c5a423cbcb.png"">

When user clicks on ""Explore"" again, same popup comes up, and user is able to paste the already existing name and able to get to Explore, though the name of the virtual dataset is untitled. 

Proposed user experience:
1. Ideally, the error message would state the reason ""This name already exists, please select a unique name."" OR give user option to override that virtual dataset.
2. If user clicks 2nd time, similar experience should probably happen as 1st time (as long as use knows how to fix the name) or they should have option to proceed with ""untitled query"" or other random name.",,,junlincc,"
--
@yousoph ^^. 
--
",,,,,,,,,,
13552,OPEN,deck.gl and WKT polygons,enhancement:request,2021-03-31 07:44:47 +0000 UTC,fprivitera,Opened,,"**Is your feature request related to a problem? Please describe.**
there is a missing opportunity of visualize  a lot of poligons. Most of OpenGIS databases handle WKT polygons, not geojsons or polylines.

for instance: https://trino.io/docs/311/functions/geospatial.html

**Describe the solution you'd like**
add WKT as new type of ""lines encoding""

<img width=""547"" alt=""Screenshot 2021-03-10 at 17 11 17"" src=""https://user-images.githubusercontent.com/635723/110660634-086f0a00-81c4-11eb-962d-1a1f794a69e7.png"">
",,,mjj203,"
--
I was just about to submit an issue for this exact issue. WKT point, polygon, and linestring should all be supported for any of the deckGL spatial visualizations.
--
",junlincc,"
--
tracking related issue https://github.com/apache/superset/issues/13573 here
--
",,,,,,,,
13551,OPEN,Support input validators in dashboard native filter,enhancement:request; need:followup,2021-03-27 06:41:22 +0000 UTC,cccs-jc,Opened,,"We use Superset in the context of Security information and event management (SIEM). Our data often include IP values which users want to filter by adhoc IP ranges (using CIDR notation).

For example when the user inputs a filter value of `192.168.0.0/24` this filter is converted into a range query on the server side. We leverage superset's SQL_QUERY_MUTATOR to convert the IP range filter from an SQL statement like

`WHERE IP IN '192.168.0.0/24'`
to
((IP >= 3232235520) AND (IP<= 3232235775))

The IP column is number for performance reasons.

This works quite well however there is no validation of the text provided by the user. Ideally we would like to validate the input in the client a bit like is done in the explorer UI.

It's possible to validate inputs in the chart explore UI since the viz populates the control panel and is free to customize the input control. However there is no way to specify a validator in the dashboard native filter.

There could be a registry of validator functions that the dashboard native filter would allow you to choose from when creating native filters.

Is this a feature being developed? Are there alternatives?",,,junlincc,"
--
@cccs-jc sorry, I need more context to understand the use case. could you share video or gif to demonstrate your work flow? 
--

--
Hi @cccs-jc thank you for providing such detailed description of your cases. I agree this is currently a limitation, but more as an edge case. We probably can include the enhancement in 2022 roadmap when we refractor and redesign the ad hoc filter component.  I will revisit this issue next month and let you know. If you already have a solution, feel free to open a PR. we will make sure it gets reviewed. 
--

--
@villebro let's allocate half a day to enable @cccs-jc 's team in Q2. Thanks for expressing the interest in contributing high quality code to Superset 
--
",cccs,"
--
### Use Case

We use Superset as a Security information and event management (SIEM). Network security logs are frequently used in our analysis.
Users search security logs using IP values (e.g.: 192.168.0.7) or for entire networks using CIDR notation (e.g.: 192.168.0.0/24). The CIDR is a well know networking concept expressing a range of IPs 192.168.0.0 to 192.168.0.255 in this case.

**Users search for an IP on a particular column.**

https://user-images.githubusercontent.com/56140112/110991620-dd5ef480-8342-11eb-88bc-cca8aaf01047.mp4

**Users search for a range of IPs on a particular column.**


https://user-images.githubusercontent.com/56140112/110991763-0bdccf80-8343-11eb-9526-8d968554afa1.mp4


**Users search for IP or range of IPs on either the source or destination columns.**


https://user-images.githubusercontent.com/56140112/110991799-14350a80-8343-11eb-89e2-913fab7b8fe8.mp4


**For performance reasons our IP values are physically stored as INTEGER in RDBMS. This is what a typical network IP flow table might look like**

![table](https://user-images.githubusercontent.com/56140112/110991881-2f077f00-8343-11eb-9d58-f9258f2c31ac.png)



However, users think of IPs in terms dot notation or CIDR notation, not in terms of numbers. We thus provide a conversion mechanism to hide this implementation detail from our users. Results are thus presented to the user in tables like this one.


![result-table](https://user-images.githubusercontent.com/56140112/110991943-40508b80-8343-11eb-8461-36a7e71c4fde.png)





### How we achieve this with Superset

#### Rendering

In superset weve modelled the flow table by typing the IP_SRC and IP_DST columns as IPV4. We use this typing information when rendering the values on the client side (in our custom visualization). Our rendering also generates a hyperlink to a page giving further information about the given IP.

#### Query Mutator
We leverage superset's SQL_QUERY_MUTATOR to convert the IP strings provided by the user into the corresponding number value for the RDBMS. 
A query criterion of **SRC_IP = 2.2.2.2** is mutated into **SRC_IP = 33686018** 
To query both SRC_IP or DST_IP we have introduced a filter column named IP. This column does not exist in the RDBMS. Its only used as a filter (like Lookers bind_filters feature https://docs.looker.com/reference/view-params/explore_source).
A query criterion of **IP IN '2.2.2.2'** is mutated to **(  (""SRC_IP"" = 33686018) OR  (""DST_IP"" = 33686018) )**


#### Superset Model
We use the column data type to render IPV4 columns in dot notation.

![result-table](https://user-images.githubusercontent.com/56140112/110992013-59593c80-8343-11eb-80dc-cc3df2ba7c96.png)

We mark the IP (filter column) as not a dimension. Because the IP column is not a dimension it is not available option in the COLUMNS box of the Chart explorer. This is good because its not actually a column its just a filter.
![pick-columns-to-display](https://user-images.githubusercontent.com/56140112/110992059-69711c00-8343-11eb-9b10-9a44e5e5b4f8.png)

However, the IP filter column is available in the ad-hoc filter. Which is great.

![use-ipfilter-in-adhoc-filter](https://user-images.githubusercontent.com/56140112/110992099-755cde00-8343-11eb-9161-ac52aeb18e73.png)


The SQL_QUERY_MUTATOR not only converts IP strings to numbers but can also handle more complex scenarios like querying on either the SRC_IP or DST_IP columns.
For example, given the configuration above, superset would produce this query
**FROM FLOW WHERE IP IN '2.2.2.2'**
Which our SQL_QUERY_MUTATOR transforms into
**FROM FLOW WHERE (  (""SRC_IP"" = 33686018) OR  (""DST_IP"" = 33686018) )**
Before it is sent to the RDBMS.

On the client side we have a custom visualization which renders columns of type IPV4 in dot notation and generates hyperlinks.

![results-rendered-as-dot-notation](https://user-images.githubusercontent.com/56140112/110992137-84dc2700-8343-11eb-9a64-ac4ee4383c29.png)


#### IP Filter integration in a Dashboard
The IP filter column works the same inside Dashboards. Since IP is a column it can be used in a filter box or in the new native filter support. The IP filter column can also handle CIDR notation (IP range query).


![ip-filter-works-in-dashboard](https://user-images.githubusercontent.com/56140112/110992224-a0dfc880-8343-11eb-945a-a4bacd92c4ad.png)


**FROM FLOW
WHERE IP IN '2.2.2.0/24'**
Is mutated into a range query on either SRC_IP or DST_IP columns
**FROM FLOW
WHERE
  (  (""SRC_IP"" >= 33686016 AND ""SRC_IP"" <= 33686271) OR
     (""DST_IP"" >= 33686016 AND ""DST_IP"" <= 33686271)  )**



### Gaps and improvements
Our current implementation is working well but the user experience needs some improvement.

#### Inconsistent SQL operators

The IP filter column is really a filtering function. It accepts one or more IPs and generates a query which applies a filter on either SRC_IP or DST_IP columns. Only the IN and = operator really make sense for the IP filter column. However, there is no means to control which operators to show the user.


![only-valid-operators](https://user-images.githubusercontent.com/56140112/110992295-b9e87980-8343-11eb-9411-41a9cbabb0b1.png)


The same argument can be made for port number. String operators LIKE, IS NOT NULL, IS NULL are not applicable for the port number column.


![port-number-column](https://user-images.githubusercontent.com/56140112/110992348-ca98ef80-8343-11eb-9cfd-6187cc446044.png)


**It would be desirable for the ad-hoc filter UI and for the dashboard filters to only present to the user the applicable operators. The dataset model could be extended to support a list of applicable operators per column.** 


#### No input validation

We would like to be able to validate the values entered for the IP filter. It should be of the form 0.0.0.0 or 0.0.0.0/32.






![invalid-input](https://user-images.githubusercontent.com/56140112/110992394-dd132900-8343-11eb-8efe-044eb7948a8a.png)



There are validators in superset to validate numerical and empty values. It would be great if we could create new ones for IPv4, IPv6 etc. Examples in other business lines might be credit card number, social insurance number, validating wildcard search expressions etc. 
The same argument applies to the port number column. It should only be possible to enter a number value and in this specific case a value between 0 and 65536 (2^32).
The dataset model could be extended to support a list of applicable input validators for a given column.



**The ad-hoc filter UI and the dashboard filters could use this information and apply the appropriate validation based on the chosen column.**












--

--
@junlincc as you see above being able to validate inputs and configure available operators on a column/filter basis would be greatly beneficial. Any recommendations, are there any efforts towards these features. Do you have any questions?
--

--
I would like to make progress on this earlier than 2022. My team and I are willing to contribute to Superset however we need guidance on how best to provide hooks into Superset for customization. We don't want to hack support for our features directly into the code. But given proper hooks we can implement our features ourselves. @villebro has offered to help us create a custom filter for the dashboard. Hopefully we could apply some of that knowledge to the adhoc filter. In fact since this post we were able to modify the adhoc filter to validate inputs but only for our own visualization. Ideally we would like to be able to override the validation of the adhoc filter globally.
--
",,,,,,,,
13544,OPEN,[event flow] Error in rendering the chart,#bug; bash!; good first issue; viz:chart-event,2021-03-31 04:44:34 +0000 UTC,kamalkeshavani-aiinside,Opened,,"When moving the cursor to chart area, it results in render error.

### Expected results

User can move cursor to chart area.

### Actual results

The chart is loaded when query is executed, but results in render error when cursor moves to chart area.

#### Screenshots

Loaded chart:
![image](https://user-images.githubusercontent.com/74634977/110594721-85988000-81c0-11eb-8386-c758ab905d9b.png)

Error:
![image](https://user-images.githubusercontent.com/74634977/110594674-7580a080-81c0-11eb-84b0-cfb1295fd578.png)


#### How to reproduce the bug

1. Create an Event Flow chart from 'flights' table from examples.
2. Run the query
3. Move the cursor to chart area.

Error in UI:
```  
    in i
    in div
    in i
    in div
    in o
    in withBoundingRects()
    in div
    in t
    in div
    in div
    in t
    in div
    in t
    in div
    in t
    in div
    in t
    in div
    in t
    in div
    in t
    in h
    in n
    in div
    in w
    in A
    in N
    in div
    in div
    in Styled(div)
    in d
    in ee
    in x
    in div
    in div
    in t
    in div
    in Styled(div)
    in Er
    in div
    in div
    in Styled(div)
    in ho
    in x
    in T
    in m
    in Unknown
    in s
    in yo
```
Console Logs:
```
172.17.0.1 - - [10/Mar/2021:07:53:16 +0000] ""GET /api/v1/chart/0 HTTP/1.1"" 404 24 ""http://localhost:8080/superset/explore/?form_data=%7B%22viz_type%22%3A%22event_flow%22%2C%22datasource%22%3A%2210__table%22%2C%22url_params%22%3A%7B%7D%2C%22time_range_endpoints%22%3A%5B%22inclusive%22%2C%22exclusive%22%5D%2C%22granularity_sqla%22%3A%22ds%22%2C%22time_range%22%3A%22No+filter%22%2C%22entity%22%3A%22FLIGHT_NUMBER%22%2C%22all_columns_x%22%3A%22AIRLINE_DELAY%22%2C%22row_limit%22%3A10000%2C%22order_by_entity%22%3Atrue%2C%22min_leaf_node_event_count%22%3A1%2C%22adhoc_filters%22%3A%5B%5D%2C%22all_columns%22%3A%5B%5D%2C%22extra_form_data%22%3A%7B%22custom_form_data%22%3A%7B%7D%2C%22override_form_data%22%3A%7B%7D%2C%22append_form_data%22%3A%7B%7D%7D%7D"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.192 Safari/537.36""

172.17.0.1 - - [10/Mar/2021:07:53:16 +0000] ""POST /superset/log/?explode=events HTTP/1.1"" 200 1 ""http://localhost:8080/superset/explore/?form_data=%7B%22viz_type%22%3A%22event_flow%22%2C%22datasource%22%3A%2210__table%22%2C%22url_params%22%3A%7B%7D%2C%22time_range_endpoints%22%3A%5B%22inclusive%22%2C%22exclusive%22%5D%2C%22granularity_sqla%22%3A%22ds%22%2C%22time_range%22%3A%22No+filter%22%2C%22entity%22%3A%22FLIGHT_NUMBER%22%2C%22all_columns_x%22%3A%22AIRLINE_DELAY%22%2C%22row_limit%22%3A10000%2C%22order_by_entity%22%3Atrue%2C%22min_leaf_node_event_count%22%3A1%2C%22adhoc_filters%22%3A%5B%5D%2C%22all_columns%22%3A%5B%5D%2C%22extra_form_data%22%3A%7B%22custom_form_data%22%3A%7B%7D%2C%22override_form_data%22%3A%7B%7D%2C%22append_form_data%22%3A%7B%7D%7D%7D"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.192 Safari/537.36""
```

### Environment

(please complete the following information):

- superset version: `v1.0.1`
- python version: `v3.7.9`
Using official docker image without any changes to config.

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Also can reproduce the error in master.
",,,junlincc,"
--
@kamalkeshavani-aiinside thanks for reporting. I also saw this error in latest master couple days ago. 

--

--
@daniel10012 thanks for confirming. unfortunately we will have to wait for a couple of sprints.....
--
",daniel10012,"
--
@junlincc the SQL query generated seems wrong and has to be fixed in the event flow chart

![image](https://user-images.githubusercontent.com/10360991/110849402-29476600-827d-11eb-9364-f20e697531b4.png)

![image](https://user-images.githubusercontent.com/10360991/110849600-38c6af00-827d-11eb-9d08-9e6f495185d7.png)

--
",,,,,,,,
13543,OPEN,[Time-series Table] Multiple cosmetic issues in Time-series Table,bash!; good first issue; viz:chart-timeseries,2021-03-31 04:44:44 +0000 UTC,kamalkeshavani-aiinside,Opened,,"## Screenshots

1. Tooltip
![image](https://user-images.githubusercontent.com/74634977/110591424-6697ef00-81bc-11eb-9454-d679a1e2e48d.png)

2. Half cut bottom axis value
![image](https://user-images.githubusercontent.com/74634977/110591705-becef100-81bc-11eb-992f-c64a6b5864e5.png)

3. Missing bottom axis
![image](https://user-images.githubusercontent.com/74634977/110592033-1ec59780-81bd-11eb-8c18-7b9329dcc252.png)


## Description

1. Tooltip can be in a position, where the value is not visible.
2. The bottom half of bottom axis value is hidden.
3. In some cases, the bottom axis and value is missing.

Checked in v1.0.1 and master.

## Design input
",,,junlincc,"
--
thanks for reporting, confirm 1 & 2. should be an easy fix. can't repro 3.... 
--
",kamalkeshavani,"
--
@junlincc I used 'cleaned_sales_data' table in examples db with below config to reproduce all 3 issues. You can try at your end.
![image](https://user-images.githubusercontent.com/74634977/110731377-89321280-8265-11eb-97dc-b1bd67d40541.png)

--
",,,,,,,,
13542,OPEN,Safari: 'explore-container' div has more height than 'app' div on Dataset Explore page,browser:safari; good first issue,2021-03-31 05:27:12 +0000 UTC,KrupaVadher,Opened,,"## Description

Safari Version: 14.0.3 (16610.4.3.1.4)
Superset Commit hash: c9a755f25

As seen in the below video, after changing any chart type or running an invalid query, the height of 'explore-container' div gets larger than 'app' div. Hence, the screen becomes ineffective if we want to run another query since the controls like 'Run' and 'Save' gets hidden.

https://user-images.githubusercontent.com/57817186/110589449-4835e300-812b-11eb-86b5-2225f70b3291.mp4

Tested with Google Chrome and Firefox where it works fine. 

#### Expected Screen:

Google Chrome:

<img width=""1434"" alt=""Preset_io_Dataset_expected"" src=""https://user-images.githubusercontent.com/57817186/110590155-43256380-812c-11eb-994c-810a02e48c9d.png"">
 

## Design input
[describe any input/collaboration you'd like from designers, and
tag accordingly. For design review, add the
label `design:review`. If this includes a design proposal,
include the label `design:suggest`]
",,,junlincc,"
--
Thanks for reporting @KrupaVadher 
--
",,,,,,,,,,
13541,OPEN,Redis connectivity failing on including special characters in the password.,data:connect:redis,2021-04-07 18:43:20 +0000 UTC,skini-galaxia,Opened,,"On deploying superset on K8s (with Terraform), connectivity with Redis fails on including special characters in the password.
### Expected results
Superset should connect to Redis on inclusion of special characters in the password, or allowed special characters should be documented.

### Actual results
```
ERROR:superset.views.base:invalid password
Traceback (most recent call last):
  File ""/app/superset/views/base.py"", line 183, in wraps
    return f(self, *args, **kwargs)
  File ""/app/superset/utils/log.py"", line 211, in wrapper
    value = f(*args, **kwargs)
  File ""/app/superset/utils/cache.py"", line 153, in wrapper
    return f(*args, **kwargs)
  File ""/app/superset/views/utils.py"", line 446, in wrapper
    return f(*args, **kwargs)
  File ""/app/superset/views/core.py"", line 618, in explore_json
    return self.generate_json(viz_obj, response_type)
  File ""/app/superset/views/core.py"", line 455, in generate_json
    payload = viz_obj.get_payload()
  File ""/app/superset/viz.py"", line 443, in get_payload
    payload = self.get_df_payload(query_obj)
  File ""/app/superset/viz.py"", line 493, in get_df_payload
    cache_value = cache_manager.data_cache.get(cache_key)
  File ""/usr/local/lib/python3.7/site-packages/flask_caching/__init__.py"", line 248, in get
    return self.cache.get(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/flask_caching/backends/rediscache.py"", line 114, in get
    self._read_clients.get(self._get_prefix() + key)
  File ""/usr/local/lib/python3.7/site-packages/redis/client.py"", line 1606, in get
    return self.execute_command('GET', name)
  File ""/usr/local/lib/python3.7/site-packages/redis/client.py"", line 898, in execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
  File ""/usr/local/lib/python3.7/site-packages/redis/connection.py"", line 1192, in get_connection
    connection.connect()
  File ""/usr/local/lib/python3.7/site-packages/redis/connection.py"", line 567, in connect
    self.on_connect()
  File ""/usr/local/lib/python3.7/site-packages/redis/connection.py"", line 643, in on_connect
    auth_response = self.read_response()
  File ""/usr/local/lib/python3.7/site-packages/redis/connection.py"", line 739, in read_response
    response = self._parser.read_response()
  File ""/usr/local/lib/python3.7/site-packages/redis/connection.py"", line 340, in read_response
    raise error
redis.exceptions.AuthenticationError: invalid password

```

### Version Info. (Chart.yaml)
```
apiVersion: v1
appVersion: ""1.0""
description: Apache Superset is a modern, enterprise-ready business intelligence web application
name: superset
maintainers:
  - name: Chuan-Yen Chiang
    email: cychiang0823@gmail.com
    url: https://github.com/cychiang
version: 0.1.0

```
",,,caleb15,"
--
I believe https://github.com/apache/superset/issues/13995 is the cause of your issue. The password needs to be passed into superset, but superset does not have the configuration necessary yet.
--
",,,,,,,,,,
13539,OPEN,Helm chart not adhering to additionalRequirements for drivers,#bug,2021-03-09 23:06:29 +0000 UTC,deworkman,Opened,,"Hi, I need a superset cluster that can hit pinotdb, but why does adding pinotdb to helm charts values file additionalRequirements not install the driver and how do I do so please? Thanks!

### Helm chart does not appear to adhere to additionalRequirements section.

Tried with user-supplied values file.
```
helm upgrade --install --namespace superset --values values-dev.yaml superset . --debug
USER-SUPPLIED VALUES:
additionalRequirements:
- psycopg2==2.8.5
- redis==3.2.1
- sqlalchemy-trino
- pinotdb==0.3.3
- pyhive==0.6.3
- ibm_db_sa
```

Tried without user-supplied values file.
```
helm upgrade --install --namespace superset superset . --debug
COMPUTED VALUES:
additionalRequirements:
- psycopg2==2.8.5
- redis==3.2.1
- sqlalchemy-trino
- pinotdb==0.3.3
- pyhive==0.6.3
- ibm_db_sa
```
### Expected results

Expected `pinotdb` and `ibm_db_sa` to be pip installed

### Actual results

No pip install

```
[root@10-222-77-102 xxxxx]# kubectl get pods -n superset
NAME                              READY   STATUS      RESTARTS   AGE
superset-7bb7c98bbc-7pjpk         1/1     Running     0          99s
superset-7bb7c98bbc-982fc         1/1     Running     0          99s
superset-init-db-2pjxs            0/1     Completed   0          98s
superset-postgresql-0             1/1     Running     0          98s
superset-redis-master-0           1/1     Running     0          98s
superset-worker-6b5947658-247p4   1/1     Running     0          99s
superset-worker-6b5947658-lfdfs   1/1     Running     0          99s
[root@10-222-77-102 xxxxx]# kubectl exec -it pod/superset-7bb7c98bbc-7pjpk -n superset -- pip list | grep pinotdb
[root@10-222-77-102 xxxxx]# kubectl exec -it pod/superset-7bb7c98bbc-7pjpk -n superset -- pip list | grep ibm_db_sa
[root@10-222-77-102 xxxxx]#
```

Also the GUI errors. Attempting to add Database via the GUI like this:
pinot+http://[pinot-broker]:30099/query/sql?server=http://[pinot-controller]:31000/
yields:
ERROR: Could not load database driver: pinot+http

### Additional context

No logs print when the error occurs.

Original logs look fine
```
$ kubectl logs pod/superset-7bb7c98bbc-2bbch -n superset -f
: not found /app/pythonpath/superset_bootstrap.sh:
: not found /app/pythonpath/superset_bootstrap.sh:
ERROR: Invalid requirement: ''
: not found /app/pythonpath/superset_bootstrap.sh:
[2021-03-09 22:59:24 +0000] [12] [INFO] Starting gunicorn 20.0.4
[2021-03-09 22:59:24 +0000] [12] [INFO] Listening at: http://0.0.0.0:8088 (12)
[2021-03-09 22:59:24 +0000] [12] [INFO] Using worker: gthread
[2021-03-09 22:59:24 +0000] [15] [INFO] Booting worker with pid: 15
logging was configured successfully
INFO:superset.utils.logging_configurator:logging was configured successfully
Loaded your LOCAL configuration at [/app/pythonpath/superset_config.py]
```",,,,,,,,,,,,,,
13536,OPEN,[Dashboard]Show relevant datasets ONLY in native filter dataset select,assigned:nielsen; enhancement:committed; viz:dashboard:native-filter,2021-03-30 18:40:29 +0000 UTC,junlincc,In progress,,"Dashboard native filter enhancement 

Must have -In Datasource field, user should only see the list of datasets that are relevant (being used on the charts in dashboard) in select dropdown.

Should have - add sort to value list to reach feature parity 
Nice to have - If all charts use the same dataset, preselect the one for user. 

To do: 
1. change label ""Datasource"" to ""Dataset""
2. set Allow multiple selections by default 



https://user-images.githubusercontent.com/67837651/110543448-2f9fdb80-80df-11eb-9f0d-8d7028cd4134.mov

",,,junlincc,"
--
@simcha90 Hey Simcha, we would really appreciate if you can help adding a couple of above features. 
Priority : Sort, default for multi select, narrow down relevant option list. 

cc @amitmiran137 @villebro 
--
",,,,,,,,,,
13533,OPEN,Add new role permission for the ENABLE_JAVASCRIPT_CONTROLS option,enhancement:request; need:followup,2021-03-11 18:14:25 +0000 UTC,Benji81,In progress,,"Hello,

ENABLE_JAVASCRIPT_CONTROLS is actually a global configuration flag. It would be great if we can give access to that feature to some users/groups through the roles permissions.

Thus we could have trusted users with this great functionality and it would be disabled for others.
",,,junlincc,"
--
hi @Benji81, thanks for suggesting. I'm not familiar with this FF. Is it for the deck.gl chart editing? 
And can you elaborate you pain points, and provide details how adding permission help solve them? thanks 
--
",Benji81,"
--
Yes @junlincc it is for deck.gl . I do not know if other charts allow embedded JS.

**My pain points:**

The ENABLE_JAVASCRIPT_CONTROLS option says that it is disabled by default because of XSS possibility if a bad user includes malicious JS in a chart.

In my usage, I have some trusted users and some untrusted users. My trusted users are ""power users"" that need to add some JS in their charts for tooltip on map for example. They ask me if I can enable this feature.

What I do not want is to also enable that feature for untrusted users because theoretically, with XSS, they could write a JS to stole session/cookie of any user that would display a malicious chart. It would be very dangerous if this session/cookies are those of a power user with extended permission or an admin.

Add this as a permissions should solve this by adding this feature to a ""role"" for power user and not adding them to the  ""standard user"" role
--
",,,,,,,,
13532,OPEN,Question: Is there any way to increase the query timeout for superset windows 10 setup ?,question,2021-03-10 05:57:00 +0000 UTC,tilak-nanavati-opshub,Opened,,"While using superset for creating dashboards and some useful charts we are getting the following error after 1 minute.

**Were having trouble loading this visualization. Queries are set to timeout after 60 seconds.
This may be triggered by:
Issue 1000 - The dataset is too large to query.
Issue 1001 - The database is under an unusual load.**

Any alternative solution or any option to increase this time out?

Note: Currently, we are using Ubuntu tool available on Microsoft store to deploy and run Superset server. So not able to directly access and update config.py file.

![image](https://user-images.githubusercontent.com/77785326/110507790-5d5c4300-8126-11eb-8467-92edc3a84d34.png)
",,,,,,,,,,,,,,
13531,OPEN,Unable to pass boolean values to connect_args of database connections,data:connect:mysql,2021-03-10 06:00:08 +0000 UTC,cccs-jc,Opened,,"Unable to pass boolean values to database connections. When configuring mysql+mysqlconnector I'm able to do the following in the python interpreter

```
import sqlalchemy as db
engine = db.create_engine('mysql+mysqlconnector://<user>:<pass>@<host>', connect_args={
'auth_plugin':'mysql_clear_password', 
'ssl_verify_cert':  False , 
'ssl_ca': '/etc/ssl/certs/Entrust_Root_Certification_Authority.pem'})
con = engine.connect()
```
Note I'm passing a certificate just to appease the mysqlconnector, strangely even if ssl_verify_cert is turned of it wants a cert.

However when I try to do the same thing via the database connect of superset it refuses the boolean value
  
```
{
    ""metadata_params"": {},
    ""engine_params"": {
          ""connect_args"": {
		    ""auth_plugin"":""mysql_clear_password"", 
		    ""ssl_verify_cert"": False, 
		    ""ssl_ca"": ""/etc/ssl/certs/Entrust_Root_Certification_Authority.pem""
		  }
     }
}
```

If I change the value to be a string either ""False"" or ""false"" then the connector rejects the configuration saying ssl_verify_cert should be a boolean.


### Expected results

Able to pass boolean configurations to underlying connection library. I looks like there needs to be interpretation of the values to try and convert them to boolean flags. This is actually done explicitly for the allows_virtual_table_explore value.
",,,,,,,,,,,,,,
13528,OPEN,Not able to download large CSV file,data:csv; need:followup,2021-03-10 06:52:22 +0000 UTC,SyamprasadNakka,In progress,,"A clear and concise description of what the bug is.

We are trying to download 300 MB to 500 MB size file from SQL_LAB and Chart and the database connection is Oracle, but we are getting page crash error. 
We are able to download max of 90 MB CSV file only.

### Expected results

It should be able to download huge files of sizes > 500 MB also.

### Actual results

We are able to download max of 90 MB CSV file only.

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

### Environment

(please complete the following information):

- superset version: `0.36`
- python version: `3.7`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,junlincc,"
--
@SyamprasadNakka thanks for reporting. what error are you getting and can you attach a screenshot? 
I don't think there's a size limit in download. 

--
",SyamprasadNakka,"
--
> 
> 
> @SyamprasadNakka thanks for reporting. what error are you getting and can you attach a screenshot?
> I don't think there's a size limit in download.

![image](https://user-images.githubusercontent.com/35252020/110588399-fb89f080-819a-11eb-9360-68fa990b814e.png)
Hello  junlincc, 
It says ""the site can't be reached"", after refresh able to see the page. Please find attached the snapshot for reference.

Regards,
Syam
--
",,,,,,,,
13525,OPEN,How do i customize url from apache/superset/welcome to my new domain name,enhancement:request,2021-03-10 06:07:02 +0000 UTC,esambu,Opened,,"0


1
I need to change the term superset in the url for apache superset hosted in my server. Tried to change the superset source code and couldn't achieve the result.

Eg : http://name.domain.com/superset/welcome has to be changed as http://name.domain.com/new-name/welcome",,,,,,,,,,,,,,
13511,OPEN,Error on importing a dashboard from JSON File over https.,dashboard:import; global:error,2021-03-08 22:11:53 +0000 UTC,skini-galaxia,Opened,,"I am seeing an error in the Chrome console on importing a dashboard on Superset (image attached).
### Expected results
Dashboard import should work properly over https by default.
what you expected to happen.

#### Screenshots
![IMG-20210308-WA0050](https://user-images.githubusercontent.com/76421853/110290543-e5502900-8010-11eb-95cd-a682362426c7.jpg)
",,,yousoph,"
--
Hi @skini-galaxia, have you tried this with VERSIONED_EXPORT = True ? That's the updated import and export, please update here if you're still seeing the issue there. Thanks! 
--
",,,,,,,,,,
13510,OPEN,Question: option to remove /superset from the URLs,question,2021-03-08 22:39:54 +0000 UTC,chaitanya-coditation,Opened,,"As of now, there is no option to remove **/superset** from the URLs. Option to remove this should be provided somehow. Thanks",,,junlincc,"
--
@dpgaspar  Hey Daniel, I'm curious to know as well if you happen to know. 

--
",,,,,,,,,,
13503,OPEN,API /chart/{pk}/cache_screenshot/,question; viz:chart-api,2021-03-08 22:19:15 +0000 UTC,davidfabri,Opened,,"I'm unable to call a screenshot using the Chart REST API screenshot endpoint. 

### Expected results

response API 

### Actual results
Postman give this message:

<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2 Final//EN"">
<title>404 Not Found</title>
<h1>Not Found</h1>
<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try
	again.</p>

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug
to check API I use Postman
1. GET http://localhost:8088/api/v1/chart/142/cache_screenshot/
2. insert the authorization token 
3. send GET request

### Environment

Ubuntu 20.04
docker version
Client:
 Version:           19.03.8
 API version:       1.40
 Go version:        go1.13.8
 Git commit:        afacb8b7f0
 Built:             Fri Dec 18 12:15:19 2020
 OS/Arch:           linux/amd64
 Experimental:      false
Server:
 Engine:
  Version:          19.03.8
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.13.8
  Git commit:       afacb8b7f0
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.3.3-0ubuntu2.2
 runc:
  Version:          spec: 1.0.1-dev
 docker-init:
  Version:          0.18.0


- superset version: About Version: 0.999.0dev
- python version: `python 3.7.9`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,,,,,,,,,,,,
13492,OPEN,Menu items have mouseover/highlighting display error,bash!; bug:cosmetic; good first issue,2021-03-08 22:23:57 +0000 UTC,rusackas,Opened,,"## Screenshot

![image](https://user-images.githubusercontent.com/812905/110177168-62816100-7db9-11eb-800c-58093ecc1d40.png)
![image](https://user-images.githubusercontent.com/812905/110177182-68774200-7db9-11eb-9d93-74013f7f5da3.png)
![image](https://user-images.githubusercontent.com/812905/110177196-6e6d2300-7db9-11eb-9135-0564c331c2de.png)

## Description

Menu items in superset show a faint blue bar that's very tight to the text. The intent was to have the full item ""bar"" be that color, from left to right, with a little padding above/below.

## Design input
[describe any input/collaboration you'd like from designers, and
tag accordingly. For design review, add the
label `design:review`. If this includes a design proposal,
include the label `design:suggest`]
",,,,,,,,,,,,,,
13491,OPEN,Proposal for improved interoperability with Teradata Vantage,data:connect:teradata; enhancement:request,2021-03-09 19:41:36 +0000 UTC,tdEmpl064,In progress,,"### Motivation

In attempting to use Apache Superset with Teradata, we observed the following:

1. **[Topic 1]** The connection instructions provided at https://superset.apache.org/docs/databases/teradata can be updated to the latest and most efficient way to connect to an Advanced SQL Engine (formerly Teradata Database) of a [Teradata Vantage](https://docs.teradata.com/r/3AkrVQlhjJMha4KRVJmm1w/root) system.
2. **[Topic 2]** Most critically, with the current Superset codebase (as of apache-superset v.1.0.1) an error occurs when attempting to perform a simple SELECT query in the SQL Lab; see Figure 1 below. Namely, observe in Figure 1 the ""Syntax Error: expected something between the word '<table_Name>' and the 'LIMIT' keyword"". The root cause for this error is the improper translation to Teradata SQL of the 'LIMIT' keyword in the Superset back-end. Teradata SQL uses the 'TOP' or 'SAMPLE' keywords instead of 'LIMIT'. Observe that issue #11405 appears to be closely related to this issue.

Figure 1
![Figure_1](https://user-images.githubusercontent.com/60898895/110174175-973ee980-7db4-11eb-8309-0b4c38dc4973.jpg)

The current SIP is a proposal to update Superset connectivity to Teradata Vantage systems, and to correct the way the 'LIMIT' keyword in the Superset back-end SQL is interpreted by Teradata SQL. It is likely that the latter fix might also provide a fix for issue #11405, too.

### Proposed Changes

#### For **Topic 1**, the following is proposed:

We recommend updating the current (as of March 5, 2021) Superset Teradata connection webpage at https://superset.apache.org/docs/databases/teradata so that
* The new [teradatasqlalchemy](https://pypi.org/project/teradatasqlalchemy) package is used instead of the dated [teradata-sqlalchemy](https://pypi.org/project/sqlalchemy-teradata) project.
* The more efficient [teradatasql](https://pypi.org/project/teradatasql) driver is used instead of ODBC for Teradata Vantage connections.

This proposal simplifies Teradata connections, by rendering as obsolete the previously necessary steps to download the Teradata ODBC drivers and to specify corresponding environment variables. The current proposal only requires installation of the teradatasqlalchemy package, as teradatasql is a dependency of teradatasqlalchemy and is satisfied upon installation of teradatasqlalchemy. Alternatively, if users have already installed the teradataml package (that is, the Teradata client package for Python) on their client computer, then no further action is needed.

In all, the proposal is summarized into the single action of replacing the existing text on the page https://superset.apache.org/docs/databases/teradata with the following text:

> Teradata
> 
> The recommended connector library is [teradatasqlalchemy](https://pypi.org/project/teradatasqlalchemy).
> The connection string for Teradata looks like this:
> `teradatasql://{user}:{password}@{host}`

A sample connection string to a Teradata Vantage system by using teradatasqlalchemy with the teradatasql driver is shown in the Figure 2 below.

Figure 2
![Figure_2](https://user-images.githubusercontent.com/60898895/110174452-fc92da80-7db4-11eb-841a-bc2e94bfbb54.jpg)

#### For **Topic 2**, the following is proposed:

My colleague David Chan identified and implemented what needs to be done in the existing Superset code, so that the 'LIMIT' keyword is correctly identified and translated to Teradata SQL, as follows:

* Identify whether the parsing SQL is Teradata SQL.
* When the 'LIMIT' keyword is encountered, translate appropriately the query sent to the Teradata Vantage server by using the 'TOP' or 'SAMPLE' keywords in the Teradata SQL query.

The proposed solution is **non-destructive**, in that it introduces new code that is executed only upon identifying a connection as a Teradata connection. The corresponding fix is provided in the modified versions of the files ""db_engine_specs/base.py"", ""sql_lab.py"", and ""sql_parse.py"", which are included in the file ""SIP_Teradata.zip"" at the bottom of the present SIP. The listing of differences between the original files and the proposed fix files is detailed in the text file ""codeDifferences.txt"", which is also included in the file ""SIP_Teradata.zip"" at the bottom of the present SIP.

By using the fix files in place of the original ones, the issue identified above in Figure 1 is resolved as illustrated in the following Figure 3.

Figure 3
![Figure_3](https://user-images.githubusercontent.com/60898895/110174586-2c41e280-7db5-11eb-8c23-a9ffa1226686.jpg)

### New or Changed Public Interfaces

The present proposal requires no new or changed public interfaces.

### New dependencies

Only for Teradata connections, the present proposal requires installation of

* the [teradatasqlalchemy](https://pypi.org/project/teradatasqlalchemy) connector, and
* the [teradatasql](https://pypi.org/project/teradatasql) driver

packages to connect to a Teradata Vantage system Advanced SQL Engine Database.
These packages are actively maintained and their licenses are Teradata proprietary. The teradatasql and teradatasqlalchemy license files are attached below at the bottom of this posting. Upon installing these packages on a computer, these license files can be also found in the corresponding package directory under the main Python installation directory.

### Migration Plan and Compatibility

No migration is necessary. The proposed code changes are an extension and non-destructive to the existing code, thus not affecting existing compatibility.

### Rejected Alternatives

No alternative approaches have been considered or rejected.

### Attachments

[SIP_Teradata.zip](https://github.com/apache/superset/files/6093455/SIP_Teradata.zip)
[LICENSE_teradatasql.txt](https://github.com/apache/superset/files/6110960/LICENSE_teradatasql.txt)
[LICENSE_teradatasqlalchemy.txt](https://github.com/apache/superset/files/6110962/LICENSE_teradatasqlalchemy.txt)
",,,cwiebe18,"
--
Thanks very much Alexander. I see and understand the fixes you are proposing, and believe they provide a meaningful improvement to the Superset community in that our Teradata customers will now have the option to use Superset with Teradata.
@mccushjack @marcchernoff
--

--
Thanks very much Alexander. I see and understand the fixes you are proposing, and believe they provide a meaningful improvement to the Superset community in that our Teradata customers will now have the option to use Superset with Teradata.

--
",betodealmeida,"
--
Great to see the SQLAlchemy driver updated! I think we might not need a SIP for that, it's a clear win and not controversial, though it might require a new major release. Can the new driver co-exist with the old one?

> **[Topic 2]** Most critically, with the current Superset codebase (as of apache-superset v.1.0.1) an error occurs when attempting to perform a simple SELECT query in the SQL Lab; see Figure 1 below. Namely, observe in Figure 1 the ""Syntax Error: expected something between the word '<table_Name>' and the 'LIMIT' keyword"". The root cause for this error is the improper translation to Teradata SQL of the 'LIMIT' keyword in the Superset back-end. Teradata SQL uses the 'TOP' or 'SAMPLE' keywords instead of 'LIMIT'. Observe that issue #11405 appears to be closely related to this issue.

This is an easy fix:

```python
class TeradataEngineSpec(BaseEngineSpec):
    """"""Dialect for Teradata DB.""""""

    limit_method = LimitMethod.FETCH_MANY
    ...
```

This error happens because in SQL Lab we try to limit the query to prevent fetching too much data. There are 3 strategies for this:

1. `WRAP_SQL`: if the dropdown limit is 1000, wrap the SQL Lab query and apply a limit to the outer select:

```sql
SELECT * FROM (
    $SQL_QUERY
) LIMIT 1000;
```

This method is great because it doesn't require parsing the SQL query, and is mostly foolproof. Not all databases support this, though.

2. `FORCE_LIMIT`: parse the SQL Lab query and insert a LIMIT or replace the existing LIMIT if present. This method is more error-prone, since it requires parsing the original query. And as you saw, not all databases support the `LIMIT` clause.

3. `FETCH_MANY`: do not modify the SQL query, but fetch only a limited number of rows. With this method, the SQL Lab query is run as is, and we fetch a number of rows equal to the limit dropdown. This is the least efficient method, since the query might produce 1 million rows for us to fetch only 1000. Firebird uses this.
--

--
Also, fell free to create a draft PR with the proposed changes, it's easier to discuss it on the PR.
--

--
> These packages are actively maintained and their licenses are Teradata proprietary.

I'm not sure if we can list it as an optional dependency (like we do for [other databases](https://github.com/apache/superset/blob/master/setup.py#L114-L151)) if the license is not compatible with ASF.
--

--
Since `FETCH_MANY` is inefficient, another way to solve the problem of databases like Teradata and Firebird that use a syntax other than `LIMIT` is to implement the `apply_limit_to_sql` method in the DB engine spec. The base method (in `superset/db_engine_specs/base.py`) is:

```python
    @classmethod
    def apply_limit_to_sql(cls, sql: str, limit: int, database: ""Database"") -> str:
        """"""
        Alters the SQL statement to apply a LIMIT clause

        :param sql: SQL query
        :param limit: Maximum number of rows to be returned by the query
        :param database: Database instance
        :return: SQL query with limit clause
        """"""
        # TODO: Fix circular import caused by importing Database
        if cls.limit_method == LimitMethod.WRAP_SQL:
            sql = sql.strip(""\t\n ;"")
            qry = (
                select(""*"")
                .select_from(TextAsFrom(text(sql), [""*""]).alias(""inner_qry""))
                .limit(limit)
            )
            return database.compile_sqla_query(qry)

        if cls.limit_method == LimitMethod.FORCE_LIMIT:
            parsed_query = sql_parse.ParsedQuery(sql)
            sql = parsed_query.set_or_update_query_limit(limit)

        return sql
```

You could implement a custom method in `superset/db_engine_specs/teradata.py` that applies the correct syntax for Teradata.

This approach would be better than the one you have modifying `superset/db_engine_specs/base.py`, `superset/sql_lab.py`, and `superset/sql_parse.py`, since all the changes needed to support Teradata would be self-contained in `db_engine_specs/teradata.py`.
--

--
@mccushjack awesome, thanks! Let me take a look at the PR.
--
",mccushjack,"
--
@betodealmeida Thanks for your comments. I created a pull request as suggested. We did you look briefly at the custom class ""TD_TOP"" we go unhung up on the apply_limit and update_limt with TOP in the raw sql text. Let us know if that is the approach that can get approved. here is the link to my pull request https://github.com/apache/superset/pull/13519
--
",tdEmpl064,"
--
Edited the original post to provide more clarity regarding the teradatasql (and teradatasqlalchemy) license files. I also explicitly attached these license files in the ""Attachments"" section at the bottom of the post.
--
",,,,
13490,OPEN,"[chart]""Unexpected error"" in Dashboard thumbnails (Superset 1.0.1)",viz:explore:error,2021-04-09 11:47:38 +0000 UTC,mrshu,In progress,,"Although the thumbnails have (seemingly) been set up properly (as outlined in https://github.com/apache/superset/issues/12867#issuecomment-771770878), the thumbnail image still shows an error, saying ""Unexpected error"".

This is difficult to debug, as the logs do not show anythings suspicious. I have also tried to increase the values of `SCREENSHOT_LOCATE_WAIT` and `SCREENSHOT_LOAD_WAIT` to `120` and `600`, respectively, but it did not seem to help.

### Expected results

The screenshot/thumbnail of a dashboard. Something like the following:

![Screenshot from 2021-03-05 12-58-16](https://user-images.githubusercontent.com/461491/110174490-7ac89e80-7e00-11eb-870e-1c27fbc3124d.png)

### Actual results (+ screenshot)

The screenshot/thumbnail contains just the error pill:

![8964f231-5cef-44a5-a2ac-96f3bcd91f01](https://user-images.githubusercontent.com/461491/110171548-2b806f00-7dfc-11eb-8cfb-1b9b22db6a0c.png)

### Environment

- superset version: 1.0.1
- python version: 3.8

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.",,,junlincc,"
--
#12867 (comment) seems unrelated to this issue, they were for the home screen configuration. can you expand the ""unexpected error"" and provide more details? is it only happening on pie chart? 
--

--
I am not sure how to help you reproduce.......but i can confirm that Pie chart is working properly. 
whats your db connection?
click see more in the top right conner?  

--

--
@villebro @zhaoyongjie looks like echarts and nvd3 have different permissions? 
--
",mrshu,"
--
Thanks @junlincc!

Would you mind providing a bit more details as to how to do that? As this is being opened in the headless Chrome, I am not certain how to go about reproducing it.

Thanks!
--

--
Thanks for your reply @junlincc.

As I mentioned in the previous message, I am not certain how to reach the top-right corner in the headless Chrome where this scrrenshot is from -- when I open the actual URL which is being scrrenshoted (essentially `/superset/dashboard/4/?standalone=true`) the chart works without issues. This particular chart uses the Athena connection.

However, when trying the very same URL in a private window (without Superset's session cookies), I was able to reproduce the issue (i.e. I got the same ""Unexpected error"" as in the screenshot above). As such, it seems this is a permissions issue in the end. Sorry for not starting with that -- it would be great if the error message could be a bit more descriptive in this regard.

That said, I am still not sure how to resolve this issue and would very much appreciate any further pointers. The `THUMBNAIL_SELENIUM_USER` variable is set to the `admin` user in our config and hence I was under the impression permissions should not be an issue. The `admin` use is also the owner of this dashboard -- I am not exactly sure what further permissions could they need.

I would be very grateful if you could provide some more pointers.

Thanks!
--

--
Thanks @villebro, even on the Admin role though? That's rather surprising to be honest.
--

--
@villebro Upon further reflection it does seem that the the Admin role, which the `admin` user has assigned. I am therefore not sure where I'd add this permission -- I'd be very grateful for any pointers.

Thanks!

![Screenshot from 2021-04-09 13-39-21](https://user-images.githubusercontent.com/461491/114174531-07530900-9939-11eb-947d-f23769b195ee.png)

--

--
Thanks @villebro -- will try that out.

I just don't seem to be able to wrap my head around `THUMBNAIL_SELENIUM_USER` and what is it good for then, if the `Public` role needs the ""read on Chart"" permission.
--
",ankargren,"
--
I have the same issue, but with another type of plot. I'm posting here as my guess is that they are related to the same problem, but if you'd prefer that I open up a new issue just let me know!

To reproduce the problem:

- Create a chart using the wb_health_population example data
- Create a time-series chart as follows and save to a new dashboard:
![bild](https://user-images.githubusercontent.com/14996051/113988202-81a75e80-984f-11eb-8abc-9d8e6e82120b.png)
- Create a line chart with the exact same specifications and save to the same dashboard
- Expected output (logged in, as admin in my case) is:
![bild](https://user-images.githubusercontent.com/14996051/113988403-bb786500-984f-11eb-9f70-6a9276d4efdc.png)
- Now opening up a fresh browser and visiting the dashboard without logging in, I get:
![bild](https://user-images.githubusercontent.com/14996051/113988642-f4183e80-984f-11eb-839f-6d8d721ce29e.png)

My expectation here would be that both should be visible. Toggling ""See more"" gives no additional information as the box is empty. The public role I use is configured as follows:
![bild](https://user-images.githubusercontent.com/14996051/113988895-3477bc80-9850-11eb-89ef-43e26f6e90a0.png)



--

--
@villebro This solved the issue and example I posted, so thanks a lot!
--
",villebro,"
--
Reproduced - investigating
--

--
@mrshu I believe you need to add ""can read on Chart"", after that it should work as expected.
--

--
You need to add it to the Public role.
--
",,,,
13486,OPEN,[dashboard]Create color mappings for quick update of dashboard colors,enhancement:committed; need:followup; viz:dashboard:colors,2021-03-10 09:28:25 +0000 UTC,opus-42,Opened,,"**Is your feature request related to a problem? Please describe.**
Configuring dashboard with custom colors is quite painful and requires manually updates. Moreover, these labels usually refers to known context and may repeat over different board for visual consistency.

Current way of creating label_colors is updating the dashboard JSON Metadata :
```JSON
{
    ""label_colors"": {
        ""Girls"": ""#FF69B4"",
        ""Boys"": ""#ADD8E6""
    }
}
```

**Describe the solution you'd like**

A solution would be define general **color mappings** which would basically be a list of key-value items and could be add through UI to the dashboards:
* One menu for creating color mappings
* One bouton in dashboard config to apply the dashboard 

Exemple of a color mappings:
```JSON
{
    ""US"": ""#000000"",
    ""FR"": ""#000000"",
}
```

In this proposal the dashboard wouldn't be directly linked to the color mappings.
Color mappings would simply updated dashboard metadata once, that is to say future changes in the color mappings won't apply to the dashboard.

**Describe alternatives you've considered**
An alternative that is already used in my company is to use the dashboard REST API and define colors mapping and update via Custom CLI / Python Scripts. This alternative would requires to Open-Source our CLI and Python Superset Client.

There would be a gain to directly integrate this in Superset.

I am ready to develop that feature if there is a boarder community need for it. ",,,junlincc,"
--
@opus-42 Thank you so much for filing the request and offering contribution! This issue has been on our radar and we recently kick off the discussion. @geido is the engineer assigned to this project and can lead the implementation. 

can I schedule a meeting with you both later this week? many thanks! 
--

--
@opus-42 maybe you can drop-in our weekly office hour to share. 

@srinify @garden-of-delete Do we publish upcoming Superset office hours calendar somewhere? 
--
",opus,"
--
@junlincc. Yes I would be available for a meeting and eager to discuss it. Let me know for organization details and how I can send contact information (I'd prefer not to publish personal / pro email here).  
--

--
@junlincc I have just sent you an email on your preset-io for direct contact / followup to setup this meeting with @geido. Thanks  
--
",,,,,,,,
13481,OPEN,KeyError when using uppercase Metrics with Athena (in Superset 1.0.1),data:connect:athena; good first issue; viz:explore:metrics,2021-03-15 21:27:45 +0000 UTC,mrshu,In progress,,"After updating to Superset 1.0.1 we started running into an issue with the Athena adapter, where Metrics with uppercase name would stop working with the following error:

```
Can't resolve label reference for ORDER BY / GROUP BY / DISTINCT etc. Textual SQL expression 'New_metric' should be explicitly declared as text('New_metric')
```

Strangely enough, we found out that this issue does not persist when the name of metric gets lowercased. It therefore seems like the following issues/PRs may be related:

- https://github.com/apache/superset/issues/5308
- https://github.com/apache/superset/pull/4994

### Expected results

The computation of the defined metric.

### Actual results

An error:

```
Can't resolve label reference for ORDER BY / GROUP BY / DISTINCT etc. Textual SQL expression 'New_metric' should be explicitly declared as text('New_metric')
```

The stacktrace is as follows:

```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/compiler.py"", line 835, in visit_textual_label_reference
    col = with_cols[element.element]
KeyError: 'New_metric'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/superset/viz.py"", line 540, in get_df_payload
    df = self.get_df(query_obj)
  File ""/usr/local/lib/python3.8/site-packages/superset/viz.py"", line 270, in get_df
    self.results = self.datasource.query(query_obj)
  File ""/usr/local/lib/python3.8/site-packages/superset/connectors/sqla/models.py"", line 1295, in query
    query_str_ext = self.get_query_str_extended(query_obj)
  File ""/usr/local/lib/python3.8/site-packages/superset/connectors/sqla/models.py"", line 767, in get_query_str_extended
    sql = self.database.compile_sqla_query(sqlaq.sqla_query)
  File ""/usr/local/lib/python3.8/site-packages/superset/models/core.py"", line 409, in compile_sqla_query
    sql = str(qry.compile(engine, compile_kwargs={""literal_binds"": True}))
  File ""<string>"", line 1, in <lambda>
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py"", line 481, in compile
    return self._compiler(dialect, bind=bind, **kw)
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py"", line 487, in _compiler
    return dialect.statement_compiler(dialect, self, **kw)
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/compiler.py"", line 592, in __init__
    Compiled.__init__(self, dialect, statement, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/compiler.py"", line 322, in __init__
    self.string = self.process(self.statement, **compile_kwargs)
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/compiler.py"", line 352, in process
    return obj._compiler_dispatch(self, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/visitors.py"", line 96, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/compiler.py"", line 2201, in visit_select
    text = self._compose_select_body(
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/compiler.py"", line 2314, in _compose_select_body
    text += self.order_by_clause(select, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/compiler.py"", line 2373, in order_by_clause
    order_by = select._order_by_clause._compiler_dispatch(self, **kw)
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/visitors.py"", line 96, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/compiler.py"", line 1040, in visit_clauselist
    text = sep.join(
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/compiler.py"", line 1040, in <genexpr>
    text = sep.join(
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/compiler.py"", line 1043, in <genexpr>
    c._compiler_dispatch(self, **kw) for c in clauselist.clauses
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/visitors.py"", line 96, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/compiler.py"", line 1277, in visit_unary
    return self._generate_generic_unary_modifier(
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/compiler.py"", line 1424, in _generate_generic_unary_modifier
    return unary.element._compiler_dispatch(self, **kw) + opstring
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/visitors.py"", line 96, in _compiler_dispatch
    return meth(self, **kw)
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/compiler.py"", line 837, in visit_textual_label_reference
    elements._no_text_coercion(
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py"", line 4757, in _no_text_coercion
    util.raise_(
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 182, in raise_
    raise exception
sqlalchemy.exc.CompileError: Can't resolve label reference for ORDER BY / GROUP BY / DISTINCT etc. Textual SQL expression 'New_metric' should be explicitly declared as text('New_metric')
```


#### Screenshots
Here is what the chart rendering looks like:
![Screenshot from 2021-03-05 12-42-50](https://user-images.githubusercontent.com/461491/110111252-52b14f00-7db0-11eb-932a-4fbd21ba4ff2.png)

And this is what one can find in the Dataset editor (Metrics tab):
![Screenshot from 2021-03-05 12-43-50](https://user-images.githubusercontent.com/461491/110111341-75dbfe80-7db0-11eb-9253-ca98d1868ffb.png)


#### How to reproduce the bug

1. Go to an Athena-based Dataset in Superset (1.0.1
2. Add a new metric called `New_metric`
3. Create a  (say) Bar Chart and choose `New_metric` as the metric to be used in it
4. See the error after clicking ""Run query""

### Environment

- superset version: 1.0.1
- python version: 3.8

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,junlincc,"
--
@mrshu thanks for reporting! we may not get to any data-specific issues soon. If you have a solution in-place, feel free to ope n a PR. We will make sure it get reviewed. 
--
",mrshu,"
--
Thank you @junlincc!

Sadly, the only solution that worked for us was manually renaming the Metric in question. 

I find this strange, as the `_mutate_label` function should take care of this (i.e. it basically does the same thing):

https://github.com/apache/superset/blob/c91c45574be2c413484c62dd38f06a93166ef246/superset/db_engine_specs/athena.py#L58-L66

If you could provide any pointers, I would be happy to put a PR together!
--
",didva,"
--
Hey guys. 
Faced with the same issue and after some investigation found that it seems to be resolved in master branch as part of https://github.com/apache/superset/pull/13059 
@mrshu please give it a try. 
--
",,,,,,
13478,OPEN,[Request] Pin dashboards on top of the dashboard list view,enhancement:request; needs:design-input,2021-03-31 04:29:28 +0000 UTC,lctdulac,In progress,,"Hi SuperSet community,
As our use of SuperSet is growing and we have more and more dashboards in the system, it is becoming harder to see clearly what are **the most important dashboards.**
I propose a pin feature in the dashboard list, to highlight most important dashboards first.

![Screenshot 2021-03-05 at 12 10 09](https://user-images.githubusercontent.com/46674763/110108116-1380ff00-7dac-11eb-9501-09dfe2d6c2c4.png)

What do you guys think? Would that be something you would be interested as well ?",,,junlincc,"
--
Hi @lctdulac, thanks for proposing. Does ""star"" not serve this purpose? Starred items naturally get on the top of the list. Let us know~ 
--

--
@lctdulac I think the request is valid. To achieve your goal is only one more click away from staring the item though. I see how Pin can be useful when there are hundreds of Starred dashboard in thousands of dashboard in the list. :) deferring the question to our product designer. I will drop in design office hour next week to see if we can come up with some solutions. @steejay @mihir174 
--
",lctdulac,"
--
Hi @junlincc thanks for replying! Star does not automatically ping the dashboard on top. The user still need to filter based on stars. If an organization has say 100 dashboards, but one ''main'' one that should be used by everyone, it would make sense to be able to ''ping'' it on top of the dashboard list. What do you think ?
--

--
Cool, thanks all!
--
",,,,,,,,
13476,OPEN,select * with the Chart Data API,viz:chart-api,2021-04-09 10:28:22 +0000 UTC,suedschwede,In progress,,"I want to use the chart data api without defining the column names
In the schema definition columns is not mandatory

```
 columns = fields.List(
        fields.String(),
        description=""Columns which to select in the query."",
        allow_none=True,
    )
```
I want to use following query object 
```
{
  ""datasource"": {
      ""id"": 3,
      ""type"": ""table""
  },
  ""queries"": [
      {
          ""row_limit"": 10,
          ""row_offset"": 1
      }
  ],
  ""result_format"": ""json"",
  ""result_type"": ""results""
}
```

I made a quick fix on my system
superset/common/query_context.py

```
def get_single_payload(
   .....
   if len(query_obj.columns) < 1:
          query_obj.columns = [o.column_name for o in self.datasource.columns]
   ......
```

",,,suedschwede,"
--
![image](https://user-images.githubusercontent.com/25843519/110100215-74a3d500-7da2-11eb-9c38-63e97fe9c9ff.png)
Why did you change it to a question ?
--

--
Yes that is what I want,
--

--
We want that users can define their own RESP APIs in superset

In your example I would need 2 rest calls to ge a result ( that is to complicated)

The Rest API should be as simple as possble - I don't understand why it is a problem to return all columns

--

--
At the moment there is no other possibility

The endpoint ""explore_json"" doesn't support paging
/explore_json/?form_data=%7B""slice_id""%3A201%7D&results=true  

I would prefer a data endpoint to saved queries

--

--
We want to give our customer the possibility to define their own REST APIs to fetch data

The API call should be as simple as possible
e.g.: https://hostname/superset?id=3&offset=0&limit=10

We can transform the REST Call on our API Gateway  to
```
{
  ""datasource"": {
      ""id"": 3,
      ""type"": ""table""
  },
  ""queries"": [
      {
          ""row_limit"": 10,
          ""row_offset"": 1
      }
  ],
  ""result_format"": ""json"",
  ""result_type"": ""results""
}
```
 
I don't want to enumerate every coumn in the Rest Call. 




--
",dpgaspar,"
--
I see, my mistake.

@villebro can you offer some help here?
--
",villebro,"
--
@suedschwede do I understand you correctly that you want to find a convenient method to do a `select *` without explicitly naming the columns you wish to query?
--

--
While the endpoint is pretty versatile, it's not really designed for `select * from table` type queries. If you're using the chart data endpoint programmatically, you can first fire off a query to get all columns in the table like so:
```json
{
   ""datasource"": {
      ""id"": 1,
      ""type"": ""table""
   },
   ""queries"": [
      {
         ""result_type"": ""columns""
      }
   ]
}
```
This will render the following payload:
```json
{
  ""result"": [
    {
      ""data"": [
        {
          ""column_name"": ""source"",
          ""verbose_name"": null,
          ""dtype"": 1
        },
        {
          ""column_name"": ""target"",
          ""verbose_name"": null,
          ""dtype"": 1
        },
        {
          ""column_name"": ""value"",
          ""verbose_name"": null,
          ""dtype"": 0
        }
      ]
    }
  ]
}
```
--

--
The endpoint is primarily intended for charts to be able to request very specific data from tables. We could potentially introduce new functionality (a new result type for example), but given that this endpoint is already fairly complex, I'm not sure if this is something that this endpoint should do vs calling one of the endpoints SQL Lab uses.
--

--
Can you explain what your use case is in detail? I'd like to help out, but I'm still not entirely sure what the use case is and whether or not this endpoint is the right solution to your problem.
--

--
@suedschwede I think we can solve this by calling the chart data api with the `SAMPLES` datatype. I'll post an example shortly and can add offset support if that's currently not supported.
--
",,,,,,
13472,OPEN,"[Explore][Line Viz] ""Sort by"" not applied when series limit is not set",bash!; bug; viz:explore:sort,2021-03-09 03:14:22 +0000 UTC,ktmud,Opened,,"## Actual Results

In Line chart, there is a ""Sort by"" field, but it is only applied when series limit is set. I.e. it has not affect on the generated query if series limit is empty or 0.

<img src=""https://user-images.githubusercontent.com/335541/110074610-57d1b800-7d36-11eb-8c3c-bf72a513bf16.png"" width=""600"">

This works somewhat differently than the ""Sort by"" field that [recently added](https://github.com/apache/superset/pull/13057) to other multi-series charts. E.g. Area chart:

<img src=""https://user-images.githubusercontent.com/335541/110073332-10e2c300-7d34-11eb-9bdc-dc13861ba375.png"" width=""400"">

For those, ""Sort by"" will also apply when series limit is not set. In combination with row limit, the query results may be different:

<img src=""https://user-images.githubusercontent.com/335541/110075861-5a351180-7d38-11eb-8dbb-286fb8227fc2.png"" width=""600"">

## Expected Results

Line chart should also respect ""Sort by"" when series limit is not set.
",,,junlincc,"
--
Thanks for reporting and sorry about the series of new bugs recently. please give us a couple of weeks to get to those. 

cc @rusackas @villebro @maloun96 


--
",,,,,,,,,,
13453,OPEN,Add multiple results view into the SQL Lab.,need:followup; needs:design-input; sql_lab,2021-03-29 23:32:53 +0000 UTC,clear-m,Opened,,"**Is your feature request related to a problem? Please describe.**
In the SQL Lab (in the Superset 1.0.1), we have the ability to see the results of only the last statement that produces result.

**Describe the solution you'd like**
Maybe there should be multiple results views when users decide to fetch data from DB with different SELECT statements?


**Additional context**
![VirtualBox_debian_10_8_04_03_2021_13_12_34](https://user-images.githubusercontent.com/20908177/109948415-5f1fa400-7ceb-11eb-80f6-42eb636ba6a4.png)

",,,junlincc,"
--
thanks for suggesting. do you mean fetching results from multiple statements in the same editor, and compare the results below? 
cc @yousoph 


--

--
This request did come up a multiple times. With semicolons, user can write multiple queries in the editor, which is great.. user can view one of the query results by highlighting the statement, also great. but in order to compare, user have to switch tabs...

ability to compare result tables that has only few rows of data make sense, and I assume that's your case. what about large result tables that barely fit in the view? @clear-m 
--

--
ah! thanks for mocking the idea. How many tables do you think we should support viewing side by side? i wouldn't recommend more than 2. tbh.

bringing @steejay to the conversation 
--
",clear,"
--
> thanks for suggesting. do you mean fetching results from multiple statements in the same editor, and compare the results below?
> cc @yousoph

@junlincc Thanks for the answer! Yes, I mean that in the case of multiple statements (in SQL Lab) that produce result data (e.g. multiple SELECTs), those rows of data can be visualized in separate result tables (for each SELECT statement for example). Now (Superset v1.0.1) there is only one result view that contains rows from the last statement executed.
Well, it seems very natural and convenient to add multiple result tables (in the SQL Lab UI) that the user can interact with (because this is not a common result table (with concatenated rows for each query), but different result tables for each statement with data ...
Thanks a lot in advance, viewing multiple results is a feature I am missing and I hope other users found it helpful too.
--

--
> what about large result tables that barely fit in the view?

@junlincc Of course, there is some problem with rendering a large dataset that the user has to manipulate: if the user really needs a large dataset to view, then there is already a virtualized view there, but to be able to render even more data (actually an unlimited number of rows) in the table (more than  ~ 5MB of data) we need to dig deeper into virtualization (one of the good examples of how this can be done is here https://www.ag-grid.com/javascript-grid/server-side-model/ ).
And my guess is that in this particular case (large tables of results) a scrollbar (the one that already used in table previews) can be used. Even more scrollbars can be used: one for the results view, limiting the rows in one result view, and one for all result views (for example, like in table previews, see image below).
![VirtualBox_debian_10_8_10_03_2021_12_10_31](https://user-images.githubusercontent.com/20908177/110607959-d26a5f80-819c-11eb-927d-6651f18ed106.png)

--

--
> And my guess is that in this particular case (large tables of results) a scrollbar (the one that already used in table previews) can be used. Even more scrollbars can be used: one for the results view, limiting the rows in one result view, and one for all result views.

I will add UI diagrams to clarify the idea this weekend 
--

--
Sorry this is what I meant, hope this helps.

![wireframe](https://user-images.githubusercontent.com/20908177/112277646-02345f80-8c93-11eb-9034-79826d842414.png)

--
",rumbin,"
--
How about showing the multiple results tables in tabs, allowing to group them side by side by dragging the tabs?
The tables should be labeled on their tabs, e.g. by using the first N characters of their queries.
So if the user puts a comment in front of a single statement, we would display the beginning of this comment, stripping the comment chars (--), or so.
--

--
When talking about having multiple statements in a single script, I think we should also consider having distinct keyboard shortcuts for running _all_ statements at once, or running solely the one where the cursor currently is positioned, starting from the previous and spanning to the following semicolon.
I'd opt for Ctrl+Shift+Enter and Ctrl+Enter, respectively.
--
",Steejay,"
--
@clear-m thanks for the suggestion and the mock up your created. I'd love to learn more about this idea! Can you tell me more the last time you wanted to see multiple results views in the same context? What tasks were you trying to accomplish? How would seeing multiple views assuage some of the challenges you were experiencing at this time? How are you currently solving this problem without having multiple views?
--
",,,,
13451,OPEN,Filter box value does not update annotation layer on line chart [v1.0 and v1.0.1],enhancement:committed; viz:dashboard:native-filter; viz:explore:annotation,2021-03-09 04:50:24 +0000 UTC,quadh,Opened,,"Created a line chart with annotation layers (time series - line charts) and added a filter box (with the appropriate filter mappings), however, changes to the filter box dropdown value are not applied to the chart's annotation layers at all. The filtered column is set to 'Filterable' in the dataset configuration and is present in the chart and annotation layer sub-charts as well.

Issue occurs on Superset v1.0 and v1.0.1.

Are there any configuration options which I missed? Is this a bug?",,,junlincc,"
--
related to https://github.com/apache/superset/issues/10817 
Thanks for filing the issue. This is not a bug, we have been looking for opportunity to implement the enhancement, but now it make more sense to add it to native-filter. 

cc @villebro @rusackas  (bringing it to your attention, not a priority yet) 
--
",,,,,,,,,,
13449,OPEN,[Explore] Long columns and metrics with descriptions wrap differently in Dataset panel,P2; bug:cosmetic; viz:explore:dnd,2021-03-09 03:27:58 +0000 UTC,ktmud,Opened,,"## Screenshot

<img src=""https://user-images.githubusercontent.com/335541/109906304-19b09780-7c55-11eb-8fd5-3e29fb3a5744.png"" width=""400"">

## Description

The ""Info"" icon of a very long metric will not wrap to a new line, but it will for a very long column.
 

",,,junlincc,"
--
@zhaoyongjie let's get it addressed by the end of next week. 
--
",zhaoyongjie,"
--
@junlincc If you need me to fix it, feel free to assign it to me
--
",,,,,,,,
13447,OPEN,'Recents' section shows a deleted dashboard on the welcome screen,global:homepage,2021-03-09 04:59:48 +0000 UTC,KrupaVadher,Opened,,"After deleting a dashboard from Superset welcome screen's Dashboard section, 'Recents' section still shows that dashboard which leads to an on-click error page.

NOTE: 'Recents' section shows the deleted dashboard until the screen is refreshed.

### Expected results

After deleting a dashboard from Welcome screen's Dashboard section, that dashboard details should also be removed from the 'Recents' section.

### Actual results

After deleting a dashboard from Welcome screen's Dashboard section, 'Recents' sections shows the details about that deleted dashboard until the screen is refreshed.

#### Screenshots

<img width=""1417"" alt=""Screen Shot 2021-03-03 at 4 22 38 PM"" src=""https://user-images.githubusercontent.com/57817186/109894787-f54bbf80-7c42-11eb-8e9e-fae12a73f0b5.png"">


On clicking the deleted dashboard from the 'Recents' section leads to the below error-screen.

<img width=""834"" alt=""Screen Shot 2021-03-03 at 4 52 24 PM"" src=""https://user-images.githubusercontent.com/57817186/109894855-0eed0700-7c43-11eb-9480-b1592bfff18b.png"">


#### How to reproduce the bug

1. Go to Superset's Welcome screen
2. Scroll to the 'Dashboard' section
2. Delete an already existing dashboard from the 'Dashboard' section which is also present in 'Recents' section
3. If no dashboard is present, add a new dashboard by clicking on '+ Dashboard' button in 'Dashboard' section and delete it
4. Scroll up to the 'Recents' section
5. Click on the recently deleted dashboard from the 'Recents' section
6. You should see an error page in a new tab
7. Close the error-page
8. Navigate to the previous tab of Superset Welcome screen
9. Refresh the page
10. The deleted dashboard should disappear from the 'Recents' section 

### Environment

(please complete the following information):

- superset version: commit#  `c9a755f25`
- python version: `Python 2.7.16`
- node.js version: `v13.11.0`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,junlincc,"
--
confirmed this is an issue, thanks for reporting!  

We should do an audit to make sure deleting dashboards and charts from different entry points in the product actually update the Recent cards. low priority, not immediate need, should address it when we have the new design for ""more recent cards""

cc @rusackas @pkdotson (Phillip, please keep track of this issue. Thanks!) 



--
",,,,,,,,,,
13446,OPEN,[chart]option to not plot added metrics in chart; only display values in tooltip on hover,enhancement:committed; needs:design-input; viz:explore:dnd,2021-03-18 20:35:20 +0000 UTC,daniel10012,In progress,,"For example in Barcharts, I am looking to be able to see a metric by hovering on the barchart, but without it being graphed

![image](https://user-images.githubusercontent.com/10360991/109882818-735e9500-7c48-11eb-81d5-59f064582b55.png)

Here I would like the Count not to appear in the bars but only in tooltip


",,,daniel10012,"
--
@junlincc 
--

--
@junlincc this just adds/removes the bar value at the top but not the fact that it is graphed ?
--
",junlincc,"
--
@daniel10012 try going to Customize tab and uncheck the Bar value box? 

<img width=""804"" alt=""Screen Shot 2021-03-08 at 9 04 15 PM"" src=""https://user-images.githubusercontent.com/67837651/110421189-db9ae580-8051-11eb-8cd2-af6c6f0707c3.png"">

--

--
hm I don't understand what you mean by in the graph we do need to fix all the overlapping on the top! @daniel10012 
--

--
ooook i got it now. It is very common in BI tools. 
<img width=""552"" alt=""Screen Shot 2021-03-09 at 10 48 18 AM"" src=""https://user-images.githubusercontent.com/67837651/110522442-c19aea80-80c5-11eb-8b4d-b786c1a59dcb.png"">

How should we best approach it from UI?

In Tableau, user simply drag measures into the tooltip field, which is separate from rows(x) and columns(y)
In Superset, if we start introducing different dropping fields(tooltip, size and label etc), our current layout will be changed a lot. But i dont think we should only limit the customization to tooltip. 
<img width=""173"" alt=""Screen Shot 2021-03-09 at 10 52 28 AM"" src=""https://user-images.githubusercontent.com/67837651/110522254-8ef0f200-80c5-11eb-9ce0-47d90d4070cf.png"">

cc @kgabryje @zhaoyongjie let's brainstorm 

@mihir174 @steejay would love to get your thoughts on it, happy to provide more details to you. 

--

--
@ValentinC-BR right, that will would in Customize tab, assuming we are keeping the current tab layout. 
--
",villebro,"
--
@junlincc I think the idea is to add metrics that only show up on the tooltip, not on the chart (this is available e.g. in Tableau). This is fairly simple to implement (good starter task).
--
",ValentinC,"
--
This would be a great enhancement !

This could be done in the UI via an additional section ""tooltip"" with a ""metrics"" input for instance.
--
",mihir174,"
--
We could add an additional section for tooltip metadata like this - 

<img width=""288"" alt=""Line Chart - Data (Default)"" src=""https://user-images.githubusercontent.com/64227069/111694049-9a8f9780-87ee-11eb-94e0-7a2c45cf469f.png"">

But I think we should think about chart customization more holistically, and identify other additional input fields we might want to have eg. label, color etc like Tableau
--
",,
13442,OPEN,[SIP-60]Proposal to extend i18n support to charts,i18n:ui; sip,2021-04-09 07:22:50 +0000 UTC,thelightbird,Opened,,"## [SIP] Proposal to extend i18n support to charts

### Motivation
We would like to adopt Superset to replace our in-house BI tool. However, it turns out that a key feature is partly missing for us: internationalization.
Indeed, even though it is already partially implemented, we would like to push Supersets internationalization even further.

We see internationalization as follows in Superset:

1. **Superset UI Translation:** text displayed in the frontend that is not entered by the user.  
 Already implemented and it is possible to add languages, after activation of the feature in the settings file. Given that translations are packed into the frontend, a re-deployment is required for any change.

2. **Translation of the text that the user can fill in the charts** (e.g. chart title, axis names, and metric names).  
 Not implemented.

3. **Localization (l10n):** Format the data according to the selected language: number, date, calendar, currency, ...  
Example: the days of the week and month on the time axes of the charts.  
 Not implemented.  

In this SIP, we aim to address part 2. about chart text translation.

We would like to be able to register multiple translations (one per language) for each text element that a chart may include. This way, when a user would switch the language in the frontend, the change would not be limited to Supersets UI only. Instead, the newly selected language would be applied to the whole dashboard, then including for each chart: the chart title, the chart axis labels, and the metric labels.

## Proposed Change

In this SIP, the internationalization implementation that we offer relies on the language selection feature, which is already available in Superset. The proposed change consists of replacing each text with a JSON containing the translations, as described in the section New or Changed Public Interfaces below.
When there is one or no language defined, the behavior would remain the same as it is right now. But once that 2 or more languages would be defined, the multi-language behavior would be enabled.

Specifically, in the frontend, the change involves replacing all text input components with a new React component that would let the user input one translation per language via an edit button opening a modal (cf screenshot 3). When the modal is not opened, this component would only display the text matching the currently selected language.


### Screenshot Before - Chart edition page
![1 before](https://user-images.githubusercontent.com/2736098/109843978-f6351f00-7c4b-11eb-9a82-e756aa189a6c.png)

### Screenshot After - Chart edition page
![2 after](https://user-images.githubusercontent.com/2736098/109843994-fa613c80-7c4b-11eb-869e-a2711eeb601c.png)
![3 edit translation](https://user-images.githubusercontent.com/2736098/109843996-faf9d300-7c4b-11eb-8354-d6842f102523.png)


### New or Changed Public Interfaces

For each text field on the chart page, another i18n form field containing a JSON with all the translations would be added. This way, it would still be backward compatible if the user would decide later to deactivate the i18n feature from the backend configuration file.

First, we offer changes specifically for the translation of the title of a chart, named identically `slice_name` as a field of the Slice database model and as a form field validated by a schema.

The entire set of translations would be returned each time a GET request would be performed for a chart. To save the new i18n fields, the backend would have to be modified. Depending on the number of languages defined in the config file, the frontend would have to choose from which field the text would need to be displayed (example: il would be `slice_name` if less than 2 languages, else `slice_name_i18n` with the key of the currently selected language).

In the frontend, to avoid developing 2 distinct components (one to display, and another to edit translations), we can reuse the same component to display and optionally edit the translations.
When less than 2 languages would be defined in the superset config, the component would act as a simple text field as it is right now. But when more than 1 language would be defined, it would display the text matching the currently selected language and have an edit button to change all translations.



Interface of the frontend React component:
```
TranslatableTextField.propTypes = {
  value: PropTypes.string, // Example: ""Title""
  setValue: PropTypes.func,
  i18nValue: PropTypes.string, // Example: { ""en"": ""Title"", ""fr"": ""Titre"" }
  setI18nValue: PropTypes.func,
  enableEditMode: PropTypes.bool, // Show/hide edit button
}
```

Example of use:
```
<TranslatableTextField
  value={chart.slice_name}
  setValue={updateTitle()}
  i18nValue={chart.slice_name_i18n}
  setI18nValue={updateTitleTranslations()} />
```

The parameter `i18nValue` would be used only when more than one language would be defined.
The parameter `value` would be used even if more than one language would be defined in case we want to disable multi-language support later.

To display the text, the react component would try to display the text associated with the currently selected language. If it is null or undefined, a generic message or a dash could be displayed instead.

Example of the changes on the GET chart response with the name of the chart if we would have 2 or more languages defined:
- pre-implementation:
```
slice_name: ""Title""
```
- post-implementation:
```
slice_name: ""Title"",
slice_name_i18n: ""{ ""en"": ""Title"", ""fr"": ""Titre"" }""
```

For the other text parts of a chart, like its axis labels or its metric name, which would need to be translated too, and which are currently all stored in the `params` field of the Slice object database model, it may require to store their matching translated text parts inside this `params` as well.
We have thought about the required changes for those other text parts, but we would like to have the SIP initially reviewed on the idea and feasibility of incorporating the proposed changes for the translation of the chart title first because it is easier to reason about. We could offer more detailed changes about those other chart text fields in this SIP later.


### Migration Plan and Compatibility

#### Required changes
 - frontend: edit all views which display text in chart fields, to wrap the displayed text with the new React component that would only display the translation associated with the currently selected language
- backend: add an i18n field for each text field of the chart, add some code to transform i18n fields on the fly if they are empty.

#### Compatibility enabling and disabling multi-language

In the backend, when a GET request would be performed while more than one language would be defined, the backend would transform the i18n fields to fill each empty language with the untranslated key.
Example:
In database: 
```
slice_name: ""Title"",
slice_name_i18n: null
```
Response of the GET:
```
slice_name: ""Title"",
slice_name_i18n: ""{ ""en"": ""Title"", ""es"": ""Title"", ""fr"": ""Title"" }""
```

In the frontend, if more than one language would be defined in the backend configuration file, when creating a chart, the untranslated text field (for eg. `slice_name`) would be filled by the React component with the translation matching the selected language.

### Rejected alternatives
Currently, the only way we have found to have charts in multiple languages is by duplicating charts and dashboards but this does not scale.


Related issue: i18n: translation of dashboards and charts #13030
",,,kamalkeshavani,"
--
Really like this proposal!!

One suggestion for frontend:
There is a case when the user just wants to enable UI translations and don't want to write translations for every chart's title, axis, metrics, etc. 

> To display the text, the react component would try to display the text associated with the currently selected language. If it is null or undefined, a generic message or a dash could be displayed instead.

So there should be an option/control(either in UI or in config), which allows the user to display(re-use) the text associated with default(1st) language for other languages if not defined. Allowing user to control this behavior from UI(instead of config), means user can have different behavior for different charts allowing more control to user. This will also inspire user to try it out in a few charts, and then write more translations later if needed.
--
",Papipo,"
--
I definitely need this! I don't have any feedback, a basic version should probably suffice.
--
",darekw,"
--
+1 
--
",junlincc,"
--
@thelightbird @Papipo and @darekw 

Thanks for the detailed proposal. The requested enhancements make sense and we would definitely like to make using Superset easier for global users. 

Since the inception of the Superset, UI localization efforts have been mainly driven by and came from the community. The work of translating to different languages is also done relatively independently. `Translation of the text that the user can fill in the charts ` is slightly different, since we will need to introduce a few new controls that impact all users in Explore. To keep the impact minimal to non-international users, the feature will be likely behind a Feature Flag. The complexity of adding translation to different chart components like, title, axis, metrics and filter are also different.  

- I assume users only want to see the same language when using the product, so a global control for all UI, chart, and data(later down the road)may work, instead of adding one control to each component. 
- Also, we need to think about an entry point where user configure which languages can be selected. we don't have a optimized profile setting page yet. 

To push this forward, we will need your help to better define the minimum project scope, timeline, as well as contributing code collaboratively. 
We are in the process of redesigning the control panel, adding Drag and Drop, Dynamic Control and some other cool features. I'm not sure how different projects develop in parallel affect each other. But we want to make it work for all users! :) 



--
",,,,
13435,OPEN,Issues about sharing chart,need:more-info,2021-03-11 02:38:01 +0000 UTC,guoguoguilai,In progress,,"Sharing charts with iframe is awesome. I want to make a static site like [this](https://about.gitlab.com/handbook/engineering/development/performance-indicators/#review-to-merge-time-rtmt), and we take Kramdown as the format that supports iframe grammar. And with the update of data, the chart is also updated. 
I found two ways to share charts in Superset. One is share the chart directly, and the data will be updated automaticlly. But it shows all the charts of the dashboard. The other one is ""explore chart"" with iframe. But in this case, the chart will no longer update since the url is fixed in static site.

How can I make a automaticlly updated chart with a fixed chart url?",,,junlincc,"
--
To confirm, the request is - to share individual updated chart in iframe from the dashboard?  @guoguoguilai
bringing in our iframe expert @rusackas 
--
",guoguoguilai,"
--
> To confirm, the request is - to share individual updated chart in iframe from the dashboard? @guoguoguilai
> bringing in our iframe expert @rusackas

Yes, this is what I want
--
",,,,,,,,
13430,OPEN,New alerts: Grace period does not work as expected,#bug; global:report,2021-03-03 06:06:03 +0000 UTC,iercan,Opened,,"I experienced new alert module grace period does not work as expected. I want to set an alert without grace period with following settings
```
cron: 59 * * * *
grace period: 0
alert condition: value > 0
```

### Expected results

Alert should trigger every run since grace period lesser than cron period.

### Actual results

Alert does not trigger consecutively. We should get alert at 00:59 but instead it claims grace period just ended. 

![image](https://user-images.githubusercontent.com/3406152/109750395-6c566900-7bed-11eb-8c6f-42c7858c0953.png)

#### How to reproduce the bug

1. Create an alert which grace period lesser than check period

### Environment

Installed superset via apache/superset docker image. 

- superset version: 1.0.1
- python version: 3.7

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,,,,,,,,,,,,
13428,OPEN,[Dashboard]native filter to support freeForm values,enhancement:request; viz:dashboard:native-filter,2021-03-03 02:38:39 +0000 UTC,junlincc,Opened,,"Posting on behalf of users 

Feedback 1: AFAIK Superset prefetchs all the distinct values of filters by running a separate query for each filter. 
I'm looking for a way to implement an instant filter (filter as you type).Is there a way to achieve that? My dataset contains at least 2 millions of records so, I think prefeching all different values for each filter is not the best way for UX AND performance. My first guess is to use the data API but i'm not sure if this is the correct path to follow. Any suggestion? 


Feedback 2: I've been using the new Dashboard native filters. I like them very much. One feature that is no longer available is entering freeForm values like it was possible with FilterBox. Is there plans to support free from values in native filters. Perhaps a configuration option? In our use cases it's not possible to provide a list of all possible values in the UI. So having the ability to enter arbitrary values is crucial.


Proposed solution
- configuration to 'uncheck' autocomplete filter, similar to the setting in Explore. 
- Allow user to type instant filter value



",,,,,,,,,,,,,,
13427,OPEN,[pivot] Order of metrics is incorrect when using 'Combine Metrics',bash!; good first issue; viz:chart-pivot,2021-03-31 04:31:50 +0000 UTC,kamalkeshavani-aiinside,Opened,,"In pivot table the order of metrics in the chart should be same as the order seen in control panel on left. But when using 'Combine Metrics' option, this does not work.

### Expected results

Order of metrics can be changed by user.

### Actual results

Order of metrics can not be changed by user.

#### Screenshots
From examples table birth_names:

Order as expected:
![image](https://user-images.githubusercontent.com/74634977/109743105-81de8980-7c13-11eb-94b1-147585e75636.png)

Order not as expected with 'Combine Metrics':
![image](https://user-images.githubusercontent.com/74634977/109742986-4f349100-7c13-11eb-904b-2c891c531dff.png)


#### How to reproduce the bug

1. Create a pivot table with multiple metrics.
2. Change the order of metrics in control panel by drag-&-drop
3. Enable 'Combine Metrics' option from 'Pivot Options'.
4. Change the order of metrics

### Environment

(please complete the following information):

- superset version: `v1.0.1`
- python version: `3.7`
- node.js version: `v14.15.5`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,junlincc,"
--
Thanks for reporting! @kamalkeshavani-aiinside confirmed this issue is also on master 

<img width=""928"" alt=""Screen Shot 2021-03-02 at 7 47 36 PM"" src=""https://user-images.githubusercontent.com/67837651/109749850-45b61500-7b90-11eb-8d73-6c83cc384cee.png"">

--
",,,,,,,,,,
13419,OPEN,[TODO] Clean up `controlPanels/sections` and `controls` in superset-frontend,good first issue; techdebt,2021-03-31 04:37:48 +0000 UTC,ktmud,Opened,,"Part of these files are moved to `@superset-ui/chart-controls` but still incomplete.

- [superset-frontend/src/explore/controlPanels/sections.jsx](https://github.com/apache/superset/blob/2a587f6cb1e598ef2d90ff3e60e97094ceec2ca1/superset-frontend/src/explore/controlPanels/sections.jsx) -> [packages/superset-ui-chart-controls/src/sections.tsx](https://github.com/apache-superset/superset-ui/blob/59283de7b0c9110c5dea64ac0ff4ef8d4554456d/packages/superset-ui-chart-controls/src/sections.tsx)
- [superset-frontend/src/explore/controls.jsx](https://github.com/apache/superset/blob/3c62069bbb0d07ff0194fbb159c2e04fd98fafc2/superset-frontend/src/explore/controls.jsx#L281) -> [packages/superset-ui-chart-controls/src/shared-controls/index.tsx](https://github.com/apache-superset/superset-ui/blob/bd4d592299ff31c9f036feb93f93b529eb66fb8e/packages/superset-ui-chart-controls/src/shared-controls/index.tsx)

Diverging changes in these almost identical files have caused [confusion](https://github.com/apache/superset/discussions/13404) and bugs. We should clean up the version in `superset-frontend` and port any [subsequent changes](https://github.com/apache/superset/commits/master/superset-frontend/src/explore/controlPanels/sections.jsx) after the file was initially copied over to `sueprst-ui/chart-controls`.

",,,,,,,,,,,,,,
13416,OPEN,Jinja template not working in Dashboard's markdown,v0.37.2; viz:dashboard:markdown,2021-03-10 08:17:29 +0000 UTC,mingzeng21,In progress,,"In a dashboard view, I am adding a markdown, and then editing some jinja method, such as, {{ current_username() }}, {{ current_user_id() }}. But it does not work as I expected.

### Expected results

I expected to see the result of the used jinja method, such as the current username.

### Actual results

The actual results is plain string of the jinja method put in the markdown, such as {{ current_username() }}.

#### Screenshots

![image](https://user-images.githubusercontent.com/41520399/109649410-d68fef00-7b96-11eb-9d37-4fdded54ebab.png)


#### How to reproduce the bug

1. Go to 'Dashboard'
2. Click on 'EDIT DASHBOARD' then plug in a markdown
3. Edit markdown, type {{ current_username() }} 
4. Preview the markdown or click outside markdown, see result

### Environment

- superset version: 0.37.2
- python version: 3.7.4
- node.js version: 15.7.0

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

",,,junlincc,"
--
Hi @mingzeng21 I'm not sure if what you described is expected behavior. Did it work before? 
--
",mingzeng21,"
--
> @mingzeng21

Dear @junlincc , it used to work in markup viz, but not dashboard's markdown. Since markup was removed, I guess dashboard's markdown is the replacement.
--
",asdi744,"
--
I am facing the same issue, trying to get the current user id/name in SQL LAB in Superset v1.0.0

none of the following methods work:

`current_user_id()`
`current_username()`
`cache_key_wrapper()`

<img width=""851"" alt=""Screenshot 2021-03-04 at 15 29 44"" src=""https://user-images.githubusercontent.com/4865462/109950669-99467100-7cfe-11eb-8870-aa39ce30e230.png"">
--
",zufolo441,"
--
for SQLLab, you have to set to True the parameter ENABLE_TEMPLATE_PROCESSING in config.py.
For Markdown, I already asked this functionality some time ago. 
--
",,,,
13405,OPEN,big number with trendline,need:followup; need:validation,2021-03-02 16:56:03 +0000 UTC,Riskatri,In progress,,"why chart big number with trendline in apache superset minus one year from actual data and always get timestamp at 31 december 17:00:00 when i choose time grain 'year' ?? can the timestamp match with actual data from database? 

for example:
in my case, i have a period like this: 
![image](https://user-images.githubusercontent.com/59104917/109597153-40ce7280-7b4a-11eb-94cf-8a9bf3f813d3.png)

but in chart big number with trendline, the value of timestamp like this:
![image](https://user-images.githubusercontent.com/59104917/109597306-8c811c00-7b4a-11eb-951d-cea051c24fe2.png)

My expectation was that big num with trendline would show  same year from database",,,junlincc,"
--
@riahk 
checked the sample dataset and chart, things look right from my end. could you please provide more details and confirm year 2014 is not in your dataset?

https://user-images.githubusercontent.com/67837651/109604590-a124cc00-7ad8-11eb-9bb9-07c529af6db6.mov


--

--
which db are you connecting to? 
--
",Riskatri,"
--
yes, 2014 is not my dataset, only 2015.
iam using apache superset version 1.0.0
--

--
i am connecting to postgresql. can you selected the time grain to year at your example? because if time grain is a day or month is look right.
--
",,,,,,,,
13403,OPEN,[CRUD] Allow history back on CRUD views,enhancement:committed; global:listview; good first issue,2021-03-02 06:02:14 +0000 UTC,ktmud,Opened,,"**Is your feature request related to a problem? Please describe.**

It seems the CRUD views (Charts and Dashboards) remember search filters in URL, but it does not allow using back and forward buttons to return to previous filters. This causes frustration when I searched something, didn't find I want, then clicked on the browser's ""Back"" button to return to previous list or to start a new search

**Describe the solution you'd like**

- CRUD page filters should update location history with `history.push` instead of `history.replace`.
- Note the first page view of `/chart/list` or `/dashboard/list` redirect you to a new URL with all the empty filters in the URL, this redirection should still use `history.replace`.

",,,,,,,,,,,,,,
13400,OPEN,[Explore]Chart Gridlines,enhancement:request; good first issue; viz:chart-others,2021-03-02 06:01:58 +0000 UTC,JosephTheuri,Opened,,"## Screenshot

[drag & drop image(s) here!]
![image](https://user-images.githubusercontent.com/32618888/109572955-2ae79000-7aff-11eb-91f7-7ed99d9dcd5e.png)


## Description

Gridlines can sometimes make charts look less presentable and as such it would be great to have more control over them.

## Design input
 `design:suggest`: 
 - Include gridline (on/off) toggle for both y-axis and x-axis under the customize tab
",,,junlincc,"
--
Agreed. We should allow users to hide at least the vertical gridlines. 
--
",,,,,,,,,,
13399,OPEN,Front End Shapefile upload,question; viz:chart-others,2021-03-31 14:36:49 +0000 UTC,JosephTheuri,Opened,,"I couldn't get to ""superset-frontend/src/visualizations/CountryMap/countries"" to add a new shape file as indicated in the documentation. Searching for the folder also didn't help

As the documentation is being updated it would be great to allow addition of shape files from the front_end

",,,zhaoyongjie,"
--
Hey @JosephTheuri ,
The map files at this folder: 
https://github.com/apache-superset/superset-ui/tree/master/plugins/legacy-plugin-chart-country-map/src/countries

--
",,,,,,,,,,
13398,OPEN,ID / Name label option for Filterbox,enhancement:committed; viz:dashboard:native-filter,2021-03-30 18:45:15 +0000 UTC,durchgedreht,Opened,,"**Is your feature request related to a problem? Please describe.**
In many DBs there's an ID / Name pattern. So the *id* is integer and *name* is String for human readable representation.
When using Filterbox you have to decide to use id field or String field, but ideally you'd like to use the id field for the technical filter and use the name for labelling only.

example table:
<table>
<tr>
<th>id</th>
<th>name</th>
<th>revenue</th>
</tr>
<tr>
<td>1</td>
<td>sports</td>
<td>1000</td>
</tr>
<tr>
<td>2</td>
<td>cars</td>
<td>3000</td>
</tr>
<tr>
<td>3</td>
<td>entertainment</td>
<td>20000</td>
</tr>
<table>


Benefits would be:

- User still sees a list of Strings to include (as if he selected *name*)
- *preselecxt_fiter* option could inject INTEGER values from the outside. This is especially useful from an integration perspective as normally APIs and applications that link directly into superset and want to preselect a filter will work on the ID rather than on the name 
- imagine the name is changed -this would break the 'contract' between the url parameters and the DB values. The argument, it's based on the data is very weak as names are considered dimension data and thus any data tool should rely on the ID field.  

**Describe the solution you'd like**
It would be great to have an option 'verbose column' (or similar) in the filterbox's parameters. That column selection is optional, but would take care about the  ID -> string mapping.  

<img width=""390"" alt=""Screenshot 2021-03-01 at 23 51 10"" src=""https://user-images.githubusercontent.com/4490840/109569700-1ef0d380-7ae9-11eb-9d3c-8f5135225a87.png"">

In the UI filter box a user would just see values like 'sports', 'cars', 'entertainment', but it could be controlled by preselect_filter using IDs 1, 2, 3.
<img width=""326"" alt=""Screenshot 2021-03-02 at 00 02 26"" src=""https://user-images.githubusercontent.com/4490840/109570645-a9860280-7aea-11eb-82ae-d0111c117858.png"">


**Describe alternatives you've considered**
If the 'verbose' field could be entered using a dedicated SQL expression potentially controlled/secured by some JINJA templating, that would maby more technical, but also more flexible.

",,,junlincc,"
--
@durchgedreht thanks for suggesting. we included this item in 2021Q1 roadmap(tentative), but will mostly likely to implement the enhancement on the dashboard native filter, which is currently behind a feature flag. 
--
",,,,,,,,,,
13396,OPEN,Adding metric to Druid chart errors with Uncaught TypeError: it is undefined,data:connect:druid; need:validation; viz:explore:metrics,2021-03-02 16:36:57 +0000 UTC,razzius,Opened,,"When creating a chart based on a Druid datasource, clicking ""Add metric"" gives an error which crashes the frontend.

### Expected results

Clicking ""Add metric"" shows the metric picker popup.

### Actual results

App crashes.

#### Screenshots

<img width=""1552"" alt=""image"" src=""https://user-images.githubusercontent.com/2244895/109567897-b9cabd00-7aab-11eb-8bcc-0cdd774ab3e7.png"">

#### How to reproduce the bug

1. Hover over the + and select ""Chart"" to add a new chart
2. Choose a dataset backed by druid
3. Click ""CREATE NEW CHART""
4. Click the + to the right of ""METRICS"" to add a new metric

### Environment

- superset version: `1.0.1`
- python version: `3.8.5`
- node.js version: `14.11.0`

### Checklist

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

React logs this message in the console:
```
The above error occurred in the <AdhocMetricEditPopover> component:
    in AdhocMetricEditPopover (created by AdhocMetricPopoverTrigger)
```

Stacktrace:

```
Uncaught TypeError: it is undefined
    exports filter.js:6
    render AdhocMetricEditPopover.jsx:323
    finishClassComponent react-dom.development.js:17185
    updateClassComponent react-dom.development.js:17135
    ...
```

It seems like the erroring code is:

```js
// superset-frontend/src/explore/components/controls/MetricControl/AdhocMetricEditPopover.jsx:302

    const aggregateSelectProps = {
      placeholder: t('%s aggregates(s)', AGGREGATES_OPTIONS.length),
      value: adhocMetric.aggregate || adhocMetric.inferSqlExpressionAggregate(),
      onChange: this.onAggregateChange,
      allowClear: true,
      autoFocus: !!columnValue,
      showSearch: true,
    };

    // ...

    if (this.props.datasourceType === 'druid') {
      aggregateSelectProps.options = aggregateSelectProps.options.filter(
        aggregate => aggregate !== 'AVG',
      );
    }
```

`aggregateSelectProps` doesn't have `options` defined, so calling `aggregateSelectProps.options.filter()` triggers the error",,,davyWong3,"
--
We met the same problem in version 1.0.0 & 1.0.1.
--

--
> thanks for reporting. Is it happening to all charts?
> 
> @zuzana-vej is it affecting your users as well?
> 
> cc @nikolagigic waiting for validation, no action needed yet. 

Only datasources imported via druid connectors are affected (**all charts**). Datasets imported via SQLALCHEMY work well.
--
",junlincc,"
--
thanks for reporting. Is it happening to all charts? 

@zuzana-vej is it affecting your users as well? 

cc @nikolagigic waiting for validation, no action needed yet. 
--
",zhaoyongjie,"
--
@razzius Hi, there.
Druid datasource will be discarded in Superset, please use SQL query instead of Druid Native query.

--
",zuzana,"
--
@junlincc yes, also for Druid datasource, user gets JS error when clicking + to add new metric. 
--
",,,,
13394,OPEN,OCI runtime create failed: this version of runc doesn't work on cgroups v2: unknown,question,2021-03-02 06:44:26 +0000 UTC,zhengyiii,Opened,,"```
docker build --target superset-node -t superset-node:1.0 .
OCI runtime create failed: this version of runc doesn't work on cgroups v2: unknown
```
Any idea what is it? how to fix it?",,,,,,,,,,,,,,
13392,OPEN,Error loading Alerts & reports page after saving CRON schedule with step values,bug; global:report,2021-03-02 06:55:01 +0000 UTC,eohulse,Opened,,"Some CRON schedules when saved in the database cannot be read back. For example, when scheduling a report to be sent every day at 20:00, 20:05, and 20:10 (CRON 0-10/5 20 * * *), one can save this to the database. However when the Alerts & reports page is reloaded the page does not load any report, and an error appear in the log indicating 10/5 cannot be handled.

### Expected results
Since I generated the CRON using the UI, superset should be able to read them back from the database.

### Actual results
No reports can be loaded or created anymore. The whole alert/reporting functionality seemed to be broken because o f the error.

#### Screenshots
![Screenshot from 2021-03-01 22-15-22](https://user-images.githubusercontent.com/7784632/109561906-0cbd6800-7ade-11eb-9f98-20be1506c28d.png)


#### How to reproduce the bug
0. Set up a docker enabling reports/alerts using postgres. (Followed this [template](https://github.com/apache/superset/blob/6b565707f03a0c592a85deb112c1d1d67903fc96/docs/src/pages/docs/installation/email_reports.mdx) 
1. Go to Settings > Alerts & reports > Reports
2. Click on '+ REPORT'
3. Set up a report to be sent, but use the following CRON: 0-10/5 20 * * *
4. Save the report
5. Go to the superset home page
6. Go back to the Alerts & reports page, the new report that was recently created is not listed (empty list)

### Environment

(please complete the following information):

- superset version: `1.01`
- python version: `3.7.9`",,,,,,,,,,,,,,
13390,OPEN,DOCS: git checkout latest doesn't include docker-compose-non-dev.yml,#bug,2021-03-01 21:14:04 +0000 UTC,shapiroj18,Opened,,"I was installing superset for the first time and followed the instructions to `git checkout latest`. However, doing this gave me a checkout to `c9a755f25`. However, this checkout did not include `docker-compose-non-dev.yml`, so I had to re-checkout to master in order to start the services.

### Expected results
`docker-compose-non-dev.yml` on all lasest checkouts, or docs changed to just use the master branch.

### Actual results
```ERROR: .FileNotFoundError: [Errno 2] No such file or directory: './docker-compose-non-dev.yml'```

### Environment

Mac OS Terminal

### Screenshot
<img width=""959"" alt=""Screen Shot 2021-03-01 at 3 24 40 PM"" src=""https://user-images.githubusercontent.com/55108156/109554660-7f105700-7aa2-11eb-9556-68666bb48e16.png"">

",,,nytai,"
--
This is because the docs were published before there was a new release. The upcoming release should include this change, for now you can either use master or use the dev docker-compose workflow. 
--
",,,,,,,,,,
13386,OPEN,[SQL Lab] Copy partition to clipboard doesn't work in left SQL Lab panel,bug:newfeature; sql_lab:control:ui,2021-03-30 21:43:49 +0000 UTC,zuzana-vej,Opened,,"The action ""Copy partition to clipboard"" on the left side of SQL Lab doesn't work.

### Expected results

record is in clipboard

### Actual results

nothing (old value remains, even though bottom of the page says ""copied to clipboard!""

#### Screenshots

![Screen Shot 2021-03-01 at 8 51 50 AM](https://user-images.githubusercontent.com/61221714/109530453-95e78700-7a6b-11eb-9719-0629ceda5c58.png)
![image (7)](https://user-images.githubusercontent.com/61221714/109530460-9aac3b00-7a6b-11eb-8c62-c66548a950c3.png)


#### How to reproduce the bug

1. Go to 'SQL Lab'
2. In left panel select schema and table
3. Click down arrow to see details of the table
4. Click on ""copy"" next to latest partition info
5. See error

### Environment

(please complete the following information):

- superset version: `master`
### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,zuzana,"
--
cc @yousoph
--
",etr2460,"
--
Any updates here yet?
--
",,,,,,,,
13384,OPEN,Continuous point color binning option for deck.gl scatterplot,enhancement:request; viz:chart-deck.gl,2021-03-02 07:08:09 +0000 UTC,stanbiryukov,Opened,,"Continuous binning for points would be useful in addition to the default categorical binning. For example, a scatter of lat/lon and weather station temperatures using a continuous color scheme results in unordered colors, bins, and negative temperatures are ordered last. Further, continuous variables, without manual rounding, are each mapped to correspond to a unique legend category.",,,,,,,,,,,,,,
13373,OPEN,Chart Export CSV output is blank,#bug; chart:export; good first issue,2021-03-03 13:49:34 +0000 UTC,rvabhijith,Opened,,"Export CSV option in charts handled by the new /chart/data api returns a blank csv file 

Dataset : Virtual dataset
Using Jinja template in query for getting filter values

Reason identified : filter values passed in the json payload (form_data) is not getting applied in the actual query executed.
Query getting executed with blank values hence no result",,,,,,,,,,,,,,
13372,OPEN,Could not load database driver elasticsearch+http,,2021-04-04 19:56:27 +0000 UTC,davidsheard,In progress,,"Hi,
I have spent all day trying to connect Superset to Elasticsearch
I added a Database and tested the connection using this string:
elasticsearch+http://elastic:elastic@localhost:9200/
  I can connect to Kibana using elastic:elastic as I set this password. I also removed basic security on elasticsearch and ran:
 elasticsearch+http://localhost:9200/ - with same error.

I get this error: 
""ERROR: Could not load database driver: elasticsearch+http""
I even removed the training forward slash.

I am running a local instance of Superset and Elasticsearch on my Mac.
I am using Docker and Superset is running fine:
docker-compose -f docker-compose.yml up. (I even tried docker-compose  up)
superset_node            | No type errors found
superset_node            | Version: typescript 4.0.3
superset_node            | Time: 119888ms
superset_node            | <s> [webpack.Progress] 100% 
superset_node            | 
superset_node            |    10612 modules


I installed:
pip install elasticsearch-dbapi
I tried elasticsearch 7.6.2 and then installed 7.10.1 which is running fine.
pip install elasticsearch==7.10.1

I tired with basic security via X-Pack and without.
I have no idea what to try next?? If I can't find a solution I'll go back to Kiabna. I am playing to see if it's worth adding to our visualisation tools at our company but so far not looking good.


",,,davidsheard,"
--
Extra information:
ip freeze | grep elasticsearch
elasticsearch==7.10.1
elasticsearch-dbapi==0.2.0

--
",BFergerson,"
--
I've only just started looking into how to use superset and the first issue I ran into has to do with the missing `docker-compose-non-dev.yml` file as mentioned in https://superset.apache.org/docs/installation/installing-superset-using-docker-compose. I've got superset running by instead just running `docker-compose up` in the `docker` directory.

This is the issue I'm at now. Superset looks really nice but the documentation seems to be a bit erratic for those getting started.
I'll update this if I figure it out.
--

--
Looks like I was just dealing with old documentation. I was able to setup elasticsearch after following https://superset.apache.org/docs/databases/dockeradddrivers.
--
",nytai,"
--
@BFergerson If you checkout `master` the `docker-compose-non-dev.yml` should exist. The documentation was published before an official release, hence the file is missing from the `latest` tag. 
--
",,,,,,
13369,OPEN,filter box with an option to 'group by' the data,assigned:nielsen; enhancement:request; viz:dashboard:indicator; viz:dashboard:native-filter,2021-03-31 15:10:41 +0000 UTC,hyouinKyouma,Opened,,"Hi all,
Filterbox is a great feature to filter data but there are features that can make it better.
Here is my used case:
So lets say that I have a 100 products of different colors, red = 40, blue=20, green=20, , yellow= 20.

I am using 'Table' visualization in dashboard with metric = count

Now when no filter is applied it will show me all the 500 products in the table, and I can select any color from the drop down, lets say that I selected yellow so it will show me the count of 100 for yellow.

But here is the problem I can't have a group by option in the filter box to count of products in the 'Table' for different colors. so I can never see count for all the products for different colors. One option here is to have group by filter applied on the 'table'. but it soon starts to get complicated when working with more than single group by options.

Wouldn't it be nice to have a checkbox beside the filter box drop down to specify the group by category for the data?

Here I have added the pics for giving the rough idea of my proposition,


![githubq3](https://user-images.githubusercontent.com/37413995/111031113-eaa3de00-842b-11eb-8b45-09892d51ac29.png)

In current implementation following steps had to be followed:
1.. add the color category to group by.
2.. select all the colors in the dashboard in the filterbox

![githubQ6](https://user-images.githubusercontent.com/37413995/111031117-ed9ece80-842b-11eb-8764-c12e85d1df08.png)

desired implementation:
1.. have a checkbox to dyanamically group by based on the category.
2.. default state should be all.

",,,yashsaxena,"
--
Hey @amitmiran137 looking forward to see this feature in superset !!! :)
--
",amitmiran137,"
--
We will start working on this next week and let you know 
--
",,,,,,,,
13366,OPEN,SQL Lab - Selecting a table on the left panel doesn't expand the table info by default,#bug; sql_lab,2021-02-28 06:25:47 +0000 UTC,yousoph,Opened,,"In SQL Lab, when selecting a table on the left panel, the table should expand by default to show the columns 

### Expected results
![image](https://user-images.githubusercontent.com/10627051/109369765-8e996100-7852-11eb-83fd-aea17bc84fb4.png)

### Actual results
When selecting a table, the info in the left panel is collapsed by default
![image](https://user-images.githubusercontent.com/10627051/109369803-c43e4a00-7852-11eb-9378-6dbdb4266501.png)

#### How to reproduce the bug

1. Go to SQL Lab
2. Select a database and schema 
3. Click on a table to select it 

### Environment

- superset version: `master`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.",,,,,,,,,,,,,,
13356,OPEN,Add more control for new welcome/home screen in v1.0.x,enhancement:request; viz:home:ux,2021-02-28 06:28:47 +0000 UTC,kamalkeshavani-aiinside,Opened,,"**Is your feature request related to a problem? Please describe.**
The new Welcome/Home screen of Superset v1.0.x includes Recents, Dashboards, Saved Queries & Charts sections.
![image](https://user-images.githubusercontent.com/74634977/109264692-b8e62100-7848-11eb-9e7e-c85bbfbcd54b.png)

For many users in our case, they only use Superset to look at dashboards. So these users don't know about Charts/Saved Queries, as we have even removed those menu items from UI.

**Describe the solution you'd like**
We would like to control the sections visible to users on Welcome/Home screen based on role permissions, like we can control the Menu items in UI for specific roles using `menu access on ....` permission.

**Describe alternatives you've considered**
If this feature can take time, then I suggest to atleast make the minimize action of any Section on Home screen by user as sticky along with [#13303](https://github.com/apache/superset/issues/13303) 
Or maybe both can be implemented.

**Additional context**
Add any other context or screenshots about the feature request here.
",,,,,,,,,,,,,,
13348,OPEN,unable to run superset on Nginx . Any workable source code to run superset on server?,,2021-02-25 21:36:10 +0000 UTC,elaimte,Opened,,"## Screenshot

[drag & drop image(s) here!]

## Description

[describe the issue here!]

## Design input
[describe any input/collaboration you'd like from designers, and
tag accordingly. For design review, add the
label `design:review`. If this includes a design proposal,
include the label `design:suggest`]
",,,,,,,,,,,,,,
13345,OPEN,"cannot delete user ""Associated data exists; please delete them first""",#bug,2021-02-25 20:12:35 +0000 UTC,maddyobrienjones,Opened,,"When trying to delete a user, the error message ""Associated data exists, please delete them first"" appears. All charts and dashboards created by that account have been deleted. I have also set the user as inactive and deleted its role setting. I am able to edit the user but not delete it. 

I found the following similar issue which was marked as stale with no response: https://github.com/apache/superset/issues/8752 

### Expected results

Successful deletion of a user

### Actual results

""Associated data exists, please delete them first"" error message

Logs when navigating to user list and trying to delete user (domain has been replaced with deploymentlink.com):
10.4.15.23 - - [25/Feb/2021:19:48:28 +0000] ""GET /users/list/ HTTP/1.1"" 200 26741 ""deploymentlink.com/superset/welcome"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36""
10.4.15.23 - - [25/Feb/2021:19:48:34 +0000] ""POST /users/delete/3 HTTP/1.1"" 302 299 ""deploymentlink.com/users/list"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36""
10.4.15.23 - - [25/Feb/2021:19:48:34 +0000] ""GET /users/list/ HTTP/1.1"" 200 26786 ""-"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36""


#### Screenshots

![Screen Shot 2021-02-25 at 2 42 56 PM](https://user-images.githubusercontent.com/35735580/109208087-e616ce80-7777-11eb-8b28-4ff3920c773b.png)

#### How to reproduce the bug

1. Go to 'List Users'
2. Navigate to user row
3. Click on 'Delete' icon
4. See error

### Environment

(please complete the following information):

- superset version: `superset version`: Superset 0.999.0dev
- python version: `python --version`: 3.7.9
- node.js version: `node -v`: couldn't find

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

none",,,,,,,,,,,,,,
13341,OPEN,[Dashboard] Support Filterbox validators,enhancement:committed; viz:chart-filterbox,2021-03-03 14:12:52 +0000 UTC,cccs-jc,Opened,,"**Describe the solution you'd like**

Ability to specify a validators inside a FilterBox.

We use Superset in the context of Security information and event management (SIEM). Our data often include IP values which users want to filter by adhoc IP ranges (using CIDR notation).

For example when the user inputs a filter value of 192.168.0.0/24 this filter is converted into a range query on the server side. We leverage jinja templating to convert the IP range into an SQL statement like 

`((IP_COLUMN >= lowIP) AND (IP_COLUMN <= highIP))`

This works quite well however there is no validation of the text provided by the user. Ideally we would like to validate the input in the client a bit like is done in the explorer UI.

It's possible to validate inputs in the explore UI since the viz populates the control panel and is free to customize the input control. However there is no way to specify a validator in a FilterBox.

Is this a feature being developed? Are there alternatives?",,,junlincc,"
--
@cccs-jc thanks for suggesting. we got this request quite often. the reason we are holding off on this is that FilterBox will be replaced by dashboard native filter https://github.com/apache/superset/issues/12148 as it rolls out. please follow step to enable the new feature and let us know if it doesn't work for you case. if not(likely), we are committed to make enhancement on the new solution. 
--
",cccs,"
--
@junlincc it does not work in the native filters either. I'll will write a comment in the issue you mentioned. Thanks.
--
",,,,,,,,
13339,OPEN,Keycloak OAuth2 invalid user 'logging',#bug,2021-04-09 08:05:33 +0000 UTC,dusatvoj,In progress,,"Can't login via OAuth2 LDAP with some issues.
1. Error with `client_id=None`
2. Error `ERROR:flask_appbuilder.security.views:Error returning OAuth user info: name 'logging' is not defined`

First issue is docs-based, because with config:
```
'consumer_key':'SOME_CLIENT_ID',
'consumer_secret':'SOME_SECRET'
```
says Keycloak this: `... type=LOGIN_ERROR, realmId=SOME_REALM, clientId=None, userId=null, ...`, so Superset don't care about `client_id`.
When I've duplicated `consumer_key` as `client_id` it started working up to the second problem.

Second problem is Error written above. Keycloak says: 
![image](https://user-images.githubusercontent.com/11560794/109177050-b243a500-7787-11eb-97af-b6d0a4cfcabe.png)
and `ERROR:flask_appbuilder.security.views:Error returning OAuth user info: name 'logging' is not defined`. IDK what to do :/

### Expected results

Successful OAuth2 login

### Actual results

Lying docs, can't easily setup OAuth2

#### How to reproduce the bug

1. Try to setup OAuth2 with keycloak provider

### Environment

- superset version: `1.0.1`
- python version: `3.8.7`
- node.js version: `v12.18.4`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

_superset_config.py_
```
...
#---------------------------------------------------------
# OAuth Config
#---------------------------------------------------------
from custom_sso_security_manager import CustomSsoSecurityManager
CUSTOM_SECURITY_MANAGER = CustomSsoSecurityManager

AUTH_TYPE = AUTH_OAUTH
OAUTH_PROVIDERS = [
    {   'name':'SSO',
        'token_key':'access_token', # Name of the token in the response of access_token_url
        'icon':'fa-address-card',   # Icon for the provider
        'remote_app': {
            'consumer_key':'SOME_CLIENT_ID',  # Client Id (Identify Superset application)
            'client_id':'SOME_CLIENT_ID',  # Client Id (Identify Superset application)
            'consumer_secret':'SOME_CLIENT_SECRET', # Secret for this Client Id (Identify Superset application)
            'client_secret':'SOME_CLIENT_SECRET', # Secret for this Client Id (Identify Superset application)
            'request_token_params':{
                'scope': 'email profile'               # Scope for the Authorization
            },
            'access_token_method':'POST',    # HTTP Method to call access_token_url
            'access_token_params':{        # Additional parameters for calls to access_token_url
                'client_id':'SOME_CLIENT_ID'
            },
            'access_token_headers':{    # Additional headers for calls to access_token_url
                'Authorization': 'Basic Base64EncodedClientIdAndSecret'
            },
            'base_url':'https://<KEYCLOAK_URL>/auth/realms/<REALM>',
            'access_token_url':'https://<KEYCLOAK_URL>/auth/realms/<REALM>/protocol/openid-connect/token',
            'authorize_url':'https://<KEYCLOAK_URL>/auth/realms/<REALM>/protocol/openid-connect/auth'
        }
    }
]
...
```

_custom_sso_security_manager.py_
```
from superset.security import SupersetSecurityManager

class CustomSsoSecurityManager(SupersetSecurityManager):

    def oauth_user_info(self, provider, response=None):
        logging.debug(""Oauth2 provider: {0}."".format(provider))
        if provider == 'SSO':
            # As example, this line request a GET to base_url + '/' + userDetails with Bearer  Authentication,
    # and expects that authorization server checks the token, and response with user details
            me = self.appbuilder.sm.oauth_remotes[provider].get('userDetails').data
            logging.debug(""user_data: {0}"".format(me))
            return { 'name' : me['name'], 'email' : me['email'], 'id' : me['user_name'], 'username' : me['user_name'], 'first_name':'', 'last_name':''}
    ...
```

It's configured exactly as in [docs](https://superset.apache.org/docs/installation/configuring-superset#custom-oauth2-configuration) _(except the client ID that's not working as in docs)_",,,junlincc,"
--
@dusatvoj 
https://github.com/apache/superset/issues/13806 
may help, lmk! 


--

--
Well, Superset is an open source project, you are always welcome to update our docs and contribute to the project, if you see something doesn't make sense to you. 

>SuperSet docs are shitty as usual.

saying ^ is quite disrespectful to all the people who have work on the product and the docs. 

--
",dusatvoj,"
--
Looks good, thank you @junlincc . I will try it
--

--
SuperSet docs are shitty as usual.
There's solution even for SS `v1.0.x` https://stackoverflow.com/questions/54010314/using-keycloakopenid-connect-with-apache-superset
I'm leaving this issue open. Hope somebody will fix it in the docs or integrate to SS core
--
",,,,,,,,
13328,OPEN,value of result timestamp in chart big number with trendline doesnt match with database,,2021-02-26 00:49:19 +0000 UTC,Riskatri,Opened,,"- i am using apache superset version 1.0.1
- i create chart big number with trendline
i have database like this 
![image](https://user-images.githubusercontent.com/59104917/109099810-48f76e00-7756-11eb-9a61-2f0a7448a874.png)

but when i am using chart big number with trendline, value of periode doesnt match with my data 
![image](https://user-images.githubusercontent.com/59104917/109099991-94aa1780-7756-11eb-80d7-8007a97d046c.png)

value of periode or timestamp in chart minus one year from my database, but i want the timestamp value match with my data in database",,,,,,,,,,,,,,
13326,OPEN,AWS EKS SSL Cert pass through,question,2021-03-30 18:38:51 +0000 UTC,priteshwatch,In progress,,"I was able to setup using the helm service. However, when I tried to use the ssl cert via AWS Cert manager, things dont seemt to work. I am seeing a redirect from https -> http. I would like to only have https exposed. But it seems like superset is redirectly from https to http, not realizing that the https is managed outside. 

service.beta.kubernetes.io/aws-load-balancer-type: nlb
service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-1:XXXX:certificate/YYYYYY
service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ""ELBSecurityPolicy-TLS-1-2-2017-01""
external-dns.alpha.kubernetes.io/hostname: superset.domain.com

Is there a way to keep superset on strictly https?
",,,priteshwatch,"
--
I have the following settings in the config

ENABLE_PROXY_FIX = True
PREFERRED_URL_SCHEME = 'https'
FORWARDED_ALLOW_IPS = '*'

There is a 302 redirect from https to http 

--
",,,,,,,,,,
13321,OPEN,Error in FilterBox SQL Query using elasticsearch-dbapi 0.1.2/0.2.0 on Elasticsearch 7.x.x,data:connect:elasticsearch; need:reproduce; viz:chart-filterbox,2021-02-24 17:35:52 +0000 UTC,Polignao,Opened,,"As the self explanatory title says, I'm getting this error when trying to use elasticsearch as database for a filterbox. 

**Superset versions I tested this on: 0.990-dev, 1.0.1**

Superset is installed using Docker Desktop on Windows 10 19042.804
_https://hub.docker.com/r/amancevice/superset/ **for 1.0.1**
https://hub.docker.com/r/apache/incubator-superset for **0.990-dev**_

Elasticsearch is in Docker too, following guide here: 
_https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html_

_WARNING:elasticsearch:POST http://host.docker.internal:9200/_sql/ [status:400 request:0.015s]
DEBUG:elasticsearch:> {""query"":""SELECT \""street.keyword\"" AS \""street.keyword\""\nFROM \""italy-addresses\""\nWHERE ((lower(city.keyword) like '%roma%'))\nGROUP BY \""street.keyword\""\nLIMIT 1000""}
DEBUG:elasticsearch:< {""error"":{""root_cause"":[{""type"":""verification_exception"",""reason"":""Found 1 problem\nline 3:9: Unknown function [lower], did you mean [POWER]?""}],""type"":""verification_exception"",""reason"":""Found 1 problem\nline 3:9: Unknown function [lower], did you mean [POWER]?""},""status"":400}_

I don't know if it's only me getting this or if I'm just missing something.

In the Elasticsearch's official documentation it is possible to find the equivalent for the function lower(<string>) as LCASE:
_https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-functions-string.html#sql-functions-string-lcase_

Steps to reproduce:

- Create a filterbox from an elasticsearch dataset
- Create a dashboard with the filterbox in it
- Type something in the filterbox and wait for error to occur

It seems noone is getting this error so I think I might be doing something wrong. 
Since I don't know who generates the query with the lower() function I explored the code that manipulates SQL in Superset, Elasticsearch db engine and elasticsearch-dbapi, but had no luck.

Any advice is welcome!

Thanks!",,,,,,,,,,,,,,
13316,OPEN,Sort map (deck.gl Scatterplot) legend in Apache Superset,enhancement:request; viz:chart-geo,2021-02-24 17:37:11 +0000 UTC,mona-mk,Opened,,"Map legend needs to be sorted alphabetically or even better with a custom given order (same applies with numerical legends). e.g. in example image below, I want the order A, B, C, D for the legend.

<img width=""636"" alt=""Screenshot 2021-02-24 at 13 47 15"" src=""https://user-images.githubusercontent.com/59080347/108996367-f4e18080-76a6-11eb-814f-6033ca12b75c.png"">
",,,,,,,,,,,,,,
13315,OPEN,"Unable to Use Jinja Context for ""url_param""",bug; jinja; sql_lab,2021-04-07 03:26:09 +0000 UTC,ranggarppb,Opened,,"A clear and concise description of what the bug is.

### Expected results

It gives 'rangga' as the parameter of username in url

### Actual results

It returns None

#### Screenshots

![image](https://user-images.githubusercontent.com/44335152/108995403-a694a780-76cf-11eb-9ac5-69e87f27dfea.png)

#### How to reproduce the bug

0. Activate your jinja context in superset/docker/pythonpath_dev/superset_config.py by adding ""ENABLE_TEMPLATE_PROCESSING"" : True in dictionary of FEATURE_FLAGS
![image](https://user-images.githubusercontent.com/44335152/108995785-29b5fd80-76d0-11eb-90d1-649de0704a90.png)

1. Go to sqllab
2. Edit your url and add '?username' argument
3. Reload the page
4. Write the query using jinja context of url_param and run the query

### Environment

(please complete the following information):

- superset version: 1.0.1 (docker version)
- python version: 3.7.9

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.",,,robdiciuccio,"
--
Confirmed
--
",maltoze,"
--
The bug was caused by this PR: https://github.com/apache/superset/pull/8256

https://github.com/apache/superset/pull/8256/files#diff-adb44dfd0f0ed41256a2b5aa78da261268394042c37c2adbff36775f628ead5aL223
--
",,,,,,,,
13313,OPEN,Event flow chart failed and report sql error after upgraded to superset v1.0.1,need:followup; viz:chart-event,2021-02-24 17:34:25 +0000 UTC,Wang-Yong2018,Opened,,"A clear and concise description of what the bug is.

### Expected results
what you expected to happen.
Like superset0.3X version, the eventflow chart could show chart based on entity column and event column based on the datetime columns.
For example,  the examples dataset wb_health_population can show eventflow. 
The input (
datasource = wb_health_population
chart type = event flow
timecolumn = year
entity= country_code
event= region
The screen capture is below:
![image](https://user-images.githubusercontent.com/36652035/108979000-2f5a1600-76c5-11eb-9dd9-d74ab0a0db95.png)

The output
the X axis is time gap , the Y axis is the grouped country_code
![image](https://user-images.githubusercontent.com/36652035/108979356-88c24500-76c5-11eb-8de6-965890ab2f5d.png)

### Actual results

what actually happens.
After I upgraded to superset v1.0.1, the eventflow coould not work and report sql error info as below

#### Screenshots

If applicable, add screenshots to help explain your problem.
![image](https://user-images.githubusercontent.com/36652035/108979563-bf985b00-76c5-11eb-971d-ad95f42a81b1.png)

#### How to reproduce the bug

1. Go to 'charts'
2. Click on '+chart' to goto ""Create new chart"" page
3. In the ""Create new chart"" page, click ""choose a dataset"",
4. Scroll down to 'wb_health_population'
4. At visualize type, click ""table"" and switch to ""Event Flow"" by scrow down
5. click ""create new chart"" and wait page loading
6. In the loaded page, click ""time range"" and select ""no filter"" in timerange select and click apply
7. At entity column, scroll down and select ""country_code"" 
8. at event column, scrool down and select ""region"" column 
9. client ""run""
10. Error log is below
```
PostgreSQL Error
column ""wb_health_population.region"" must appear in the GROUP BY clause or be used in an aggregate function LINE 1: SELECT region AS region, 
```

### Environment
Docker Environment amancevice/superset :1.0.1

(please complete the following information):

- superset version: `superset version`
```
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Superset 1.0.1
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
```
- python version: `python --version`
`Python 3.8.7`

- node.js version: `node -v`
`bash: node: command not found`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
```
superset_1  | WARNING:superset.connectors.sqla.models:Query SELECT region AS region,
superset_1  |        country_code AS country_code,
superset_1  |        year AS __timestamp
superset_1  | FROM wb_health_population
superset_1  | GROUP BY year
superset_1  | ORDER BY country_code ASC
superset_1  | LIMIT 10000 on schema None failed
superset_1  | Traceback (most recent call last):
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/superset/connectors/sqla/models.py"", line 1321, in query
superset_1  |     df = self.database.get_df(sql, self.schema, mutator)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/superset/models/core.py"", line 390, in get_df
superset_1  |     self.db_engine_spec.execute(cursor, sqls[-1])
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/superset/db_engine_specs/base.py"", line 909, in execute
superset_1  |     cursor.execute(query)
superset_1  | psycopg2.errors.GroupingError: column ""wb_health_population.region"" must appear in the GROUP BY clause or be used in an aggregate function
superset_1  | LINE 1: SELECT region AS region,
superset_1  |                ^
superset_1  |
```

- [X ] I have reproduced the issue with at least the latest released version of superset.
- [X ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context
Add any other context about the problem here.
After compared with previously superset v0.34 or v0.36, and the chart query by click ""view query"" in chart page. I noticed the issue was generated by wrong sql of the chart in V1.0.1

In superset v0.34 or v0.36, the correct sql is below:
```
SELECT region AS region,
       country_code AS country_code,
       DATE(year) AS __timestamp
FROM wb_health_population
ORDER BY country_code ASC
LIMIT 50000
```
As eventflow chart only need trunc time column to selected time grain. But in superset v1.0.1, the eventflow chart sql is just group by the time column to selected time grain. Let's the reason missing columns. The wrong sql is below:
```
SELECT region AS region,
       country_code AS country_code,
       year AS __timestamp
FROM wb_health_population
GROUP BY year
ORDER BY country_code ASC
LIMIT 50000;
```

If the event flow chart can call similar sql generation function as v0.3x, the problem might be solved.

Looking foward anyone could support on it. Or advice me which part of code is related to event chart sql generated, I can fixed and fixed the bug. ",,,,,,,,,,,,,,
13312,OPEN,Line chart series input doesnt work | Clickhouse,#bug; v0.34,2021-03-08 09:55:45 +0000 UTC,mikekytyzov,In progress,,"A clear and concise description of what the bug is.

I have a metric called Total users. I want to make a line chart with this metric by day, but display only 5 top results per day. 

So, i select `series` input to 5, but it doesnt work, all results still displayed. Screen is attached

### Expected results

Series is applied by each time granula. 

### Actual results

Series input doesnt make any affect 

#### Screenshots

![Screenshot 2021-02-24 at 11 01 30](https://user-images.githubusercontent.com/21098110/108967021-04a19a80-7690-11eb-858f-018737de9fe6.png)

#### How to reproduce the bug

1. Try create line chart with a lot of data
2. Select series to 5
3. See that series doesnt make any affect

### Environment

(please complete the following information):

- superset version: `0.34.1`
- python version: `3.6.12`


may be related to https://github.com/apache/superset/issues/11426 

Adding sorting option doesnt not have affect. Also area chart works with the same bug. ",,,anilvpatel21,"
--
@mikekytyzov In the above case, Sub Query is not generated when applying series limit in clickhouse db. It might be because Window function is not supported in the clickhouse.(Guess)  

As a work around, you may apply actual query & create datasource with `Limit By` functionality. And then apply dummy query on the top of the datasource using visualisation (viz. of line chart). 
Example: `SELECT colnames FROM tbl (clickhouse db) WHERE time_received >= toDateTime('{{ from_dttm }}')
AND time_received < toDateTime('{{ to_dttm }}') LIMIT 5 BY time_received`  -- Here time_recieved is the time column. Using this as a datasource, you will achieve it. 

@junlincc Its not working in v1.0.1 also. 


--
",mikekytyzov,"
--
@anilvpatel21 okay, i see. thanks for response. 

btw ch already have window functions, but as experimental feature https://clickhouse.tech/docs/en/sql-reference/window-functions/ 

will you add a support of it ?
--
",,,,,,,,
13311,OPEN,Add output formatting to the customize section of every visualization,enhancement:committed,2021-03-18 07:46:42 +0000 UTC,vrenvren,Opened,,"The visualizations provided by the current version (1.0.1) of Superset could be a more refined, coherent set of tools. Now every visualization has its own style and options. Some basic functionalities should be available for every visualization

In addition to the color schemes, which seem to be the most persistent customization feature, every chart presenting numbers should at a minimum have the option to customize the number format. Controlling the number format at a data source level in some charts and enabling it to be changed in others is odd; it should be enabled in every visualization.

Concretely, providing the ""Number format"" field, as in pivot table, is what I am hoping to see.
",,,poleft,"
--
Agree! Looking forward to the upgrade !
--
",ValentinC,"
--
Agree ! I think Number formatting is an important area of improvement. It would definitely improve end-users' daily use of Superset.

Users should be able to chose the number format for each metric / numeric dimension displayed in a chart
--
",,,,,,,,
13309,OPEN,[Explore]mysterious blank chart page in Explore,bug; viz:explore:error,2021-02-24 05:36:53 +0000 UTC,junlincc,Opened,,"After clicking ""View chart in Explore"", on one of the charts in dashboard, got a blank Explore page with only main menu showing, and below error. Only affecting one specific chart and the chart on dashboard shows perfectly... 
Search the chart from chart list and open, also see a blank page. 

<img width=""948"" alt=""Screen Shot 2021-02-23 at 9 11 00 PM"" src=""https://user-images.githubusercontent.com/67837651/108952763-f8c9c000-761e-11eb-844b-4ed1b259c605.png"">
<img width=""2044"" alt=""Screen Shot 2021-02-23 at 9 06 20 PM"" src=""https://user-images.githubusercontent.com/67837651/108952962-3af30180-761f-11eb-8a26-f10f7eaf8bd6.png"">

",,,,,,,,,,,,,,
13308,OPEN,CSS dashboard not working after download a dashboard as an image,,2021-02-24 03:04:08 +0000 UTC,Riskatri,Opened,,"i add a css template and edit css on dashboard for hide a scrollbar in filter box, its work like this 
![filter data](https://user-images.githubusercontent.com/59104917/108941061-ff8c1d80-7686-11eb-883f-d25156cb7f0b.PNG)
but when i download dashboard as an image, scrollbar still exist 
![scorllbar](https://user-images.githubusercontent.com/59104917/108941381-948f1680-7687-11eb-9d71-dde3fcebde69.PNG)

",,,,,,,,,,,,,,
13303,OPEN,[home page] make the tab/pills sticky,enhancement:committed; viz:home:ux,2021-02-28 06:28:22 +0000 UTC,mistercrunch,Opened,,"<img width=""520"" alt=""Screen Shot 2021-02-23 at 10 26 10 AM"" src=""https://user-images.githubusercontent.com/487433/108889803-a1e1cd80-75c1-11eb-903c-74ef4b0af593.png"">

I take the time to set my favorite dashboards and charts, but the home page won't remember that I prefer ""Favorites"" over ""Mine"". Let's make that sticky. Let's also create a nice simple abstraction for things to be sticky. Maybe that becomes a `sticky` React prop for our Tab/Pills component in our design system (?)",,,nytai,"
--
As in save the preference to local storage or in metadata db? 
--

--
I would be nice to create a framework where this is saved in the metadata db instead of local storage 
--

--
Let's just work with a good abstraction that should allow us to switch over to a different storage mechanism later on. As the UI gets more complex and we create more of these sticky options, it would be a shame for all of that to be lost once someone upgrades to a new computer which is fairly common in corporate settings. 
--
",junlincc,"
--
my first reaction is local storage, that's how we set in Explore, but after thinking through, it might make more sense to save it in metadata db. @nytai 
--
",ktmud,"
--
+1 on good abstraction. Another benefit of saving things like this to db is cross-platform/browser consistency.
--
",,,,,,
13295,OPEN,apache superset datetime format,,2021-02-23 08:17:35 +0000 UTC,Riskatri,In progress,,"hi, i use apache superset version 1.0.1,
i have data period as datetime in my database and then i create a pie chart but the periode value like this
![Capture](https://user-images.githubusercontent.com/59104917/108809763-25101d00-75dc-11eb-89ff-f4b3479cbf1d.PNG)

the periode value in database like this 
![periode](https://user-images.githubusercontent.com/59104917/108809828-45d87280-75dc-11eb-802b-3ed0c93fabf5.PNG)

",,,villebro,"
--
Thanks for reporting this @Riskatri . We've recently added better column typing metadata to the chart requests, so we should be able to add proper date formatting to the labels now. @junlincc I propose trying to tackle this soon to set a precedent for how charts should handle temporal columns in labels.
--

--
@Riskatri please be mindful that this is an open source project, hence we have limited resources to respond to community feature requests. In the meantime I recommend creating a calculated column in which you format the dates as strings and then group by them instead of the original temporal column.
--
",Riskatri,"
--
thankyou @villebro but i need the visualization data right now. any solution to solve this problem? i have tried to change datetime format to string format. its work for label pie chart, but it cannot be filtered using filter box time range
--

--
thankyou @villebro , your recommend work for me 
![date](https://user-images.githubusercontent.com/59104917/108817271-33fdcc00-75ea-11eb-8c65-f55fdec2daa8.PNG)

--
",,,,,,,,
13293,OPEN,pie chart date format,bug:cosmetic; viz:chart-pie,2021-02-26 00:33:49 +0000 UTC,Riskatri,In progress,,"hi, i make a pie chart using apache superset and have a periode data but the data getting value 1491829101010 not a date format. if i want to change the value to date format as my data in database. what should i do?

",,,junlincc,"
--
Hi @Riskatri please fill out the issue template with details, screenshot, reproduce steps, and your current version. if you are not on the latest v1.0.1, please do check it out and lmk. 
--
",Riskatri,"
--
> Hi @Riskatri please fill out the issue template with details, screenshot, reproduce steps, and your current version. if you are not on the latest v1.0.1, please do check it out and lmk.

hi, i use apache superset version 1.0.1, 
i have data period as datetime in my database and then i create a pie chart but the periode value like this 
![Capture](https://user-images.githubusercontent.com/59104917/108808516-5f2bef80-75d9-11eb-9260-dbcd8446435e.PNG)

period value in my database like this '2019-02-23'

--
",0101Unkown0101,"
--
Ummm how did you get my email and who are you????

On Mon, Feb 22, 2021, 11:19 PM Riska Tri Handayani <notifications@github.com>
wrote:

> Hi @Riskatri <https://github.com/Riskatri> please fill out the issue
> template with details, screenshot, reproduce steps, and your current
> version. if you are not on the latest v1.0.1, please do check it out and
> lmk.
>
> hi, i use apache superset version 1.0.1,
> i have data period as datetime in my database and then i create a pie
> chart but the periode value like this
> [image: Capture]
> <https://user-images.githubusercontent.com/59104917/108808516-5f2bef80-75d9-11eb-9260-dbcd8446435e.PNG>
>
> period value in my database like this '2019-02-23'
>
> 
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/apache/superset/issues/13293#issuecomment-783939626>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ASDBY6W5IMLRGQLLKI3YX4LTANCGVANCNFSM4YBUDKAA>
> .
>

--
",eugeniamz,"
--
@junlincc  I verified this issue
--
",,,,
13290,OPEN,Dashboard fails to load after adding chart,need:more-info; v0.37,2021-02-23 01:20:15 +0000 UTC,prb219,Opened,,"Hi,

I had a dashboard with few charts that were working as expected. Then I went ahead and added a line chart (which had empty result) into the dashboard which crashed my dashboard and I see the below error. Please note that this issue exists only in one of the dashboard, all other dashboards loads fine. 


### Expected results

expected to load the dashboard
### Actual results

Sorry, something went wrong
500 - Internal Server Error
Stacktrace

        Traceback (most recent call last):
  File ""/appvenv/lib/python3.6/site-packages/flask/app.py"", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File ""/appvenv/lib/python3.6/site-packages/flask/app.py"", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/appvenv/lib/python3.6/site-packages/flask/app.py"", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/appvenv/lib/python3.6/site-packages/flask/_compat.py"", line 39, in reraise
    raise value
  File ""/appvenv/lib/python3.6/site-packages/flask/app.py"", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/appvenv/lib/python3.6/site-packages/flask/app.py"", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/appvenv/lib/python3.6/site-packages/flask_appbuilder/security/decorators.py"", line 109, in wraps
    return f(self, *args, **kwargs)
  File ""/appvenv/lib/python3.6/site-packages/superset/views/core.py"", line 1616, in dashboard
    for datasource, slices in datasources.items()
  File ""/appvenv/lib/python3.6/site-packages/superset/views/core.py"", line 1616, in <dictcomp>
    for datasource, slices in datasources.items()
  File ""/appvenv/lib/python3.6/site-packages/superset/connectors/base/models.py"", line 285, in data_for_slices
    metric_names.add(utils.get_metric_name(metric))
TypeError: unhashable type: 'dict'

#### How to reproduce the bug

- Adding a line chart with no result to dashboard. however, this behavior is not consitent

### Environment

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ Y] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ Y] I have reproduced the issue with at least the latest released version of superset.
- [ Y] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Superset version: 0.37.0
",,,junlincc,"
--
Hi @prb219 what do you mean by added `a line chart (which had empty result)` ? This should not be an issue in latest 1.0.1, you want to check it out? 
--
",,,,,,,,,,
13287,OPEN,publish superset helm chart to https://artifacthub.io/,deploy:helm; enhancement:request,2021-02-23 01:20:39 +0000 UTC,amitmiran137,Opened,,"**Is your feature request related to a problem? Please describe.**
As a kubernetes based deployment provider, I would like to use the helm chart provided officially by superset instead copying source files to my deployment solution


**Describe the solution you'd like**
start publishing helm artifact to https://artifacthub.io/  on every change to the chart
manage versioning by bumbing the chart version

**Describe alternatives you've considered**
copying chart files and building the chart using helm 

**Additional context**
There are GitHub actions that support this:
option 1: https://github.com/helm/chart-releaser-action


#### existing helm chart(not official)
![image](https://user-images.githubusercontent.com/47772523/108751407-d5354580-754a-11eb-8b14-1bca31a303db.png)

Any other solution proposals will be welcomed: @Yann-J @craig-rueda @nytai @willbarrett @vnourdin ",,,,,,,,,,,,,,
13285,OPEN,Superset Clickhouse driver,data:connect:clickhouse; doc:user,2021-03-18 08:13:59 +0000 UTC,suedschwede,In progress,,"At the moment there are two superset drivers ""clickhouse-sqlalchemy"",""sqlalchemy-clickhouse""

There are problems with both drivers and that leads to a lot of questions

**sqlalchemy-clickhouse**

- Table with 128 Columns  - Datasets sync of column does not work correctly - it seems there is a maximum columns you can use
- If the default user has a password you must use ""infi.clickhouse_orm==1.0.4""
- using a dictionary you get ""Python int too large to convert to C long""

**clickhouse-sqlalchemy**

- Sync of columns you get wrong data types e.g.: ""NULLABLE(STRING)"" instead of ""STRING""


For me the only stable driver is ""clickhouse-sqlalchemy"" with the native interface

The documentation should recommend ""clickhouse-sqlalchemy"" instead of ""sqlalchemy-clickhouse""
https://superset.apache.org/docs/databases/installing-database-drivers

It would be nice if the columns sync works correctly with ""clickhouse-sqlalchemy""
",,,junlincc,"
--
@srinify ^ 
--
",hodgesrm,"
--
Hello @suedschwede, I'm testing this case and need some advice on the problem you are seeing.  To check datatypes I made a small table and added a row to enable queries. 
```
CREATE TABLE superset_test (
    `basic_string` String,
    `nullable_string` Nullable(String),
    `lowcard_string` LowCardinality(String),
    `basic_datetime` DateTime,
    `nullable_datetime` Nullable(DateTime)
) ENGINE = TinyLog;

INSERT INTO superset_test VALUES('basic', 'nullable', 'lowcard', now(), now());
```
I can see the effect of syncing columns in the Edit Dataset view.  The data types that return are STRING, NULLABLE(STRING), LOWCARDINALITY(STRING), DATETIME, NULLABLE(DATETIME).  It looks as if the SQLAlchemy driver is getting these out of system.columns.   A couple of questions:

1. Is the expected behavior to see just STRING and DATETIME?  Or would you expect SQL types like VARCHAR? 
2. Are there cases where these vendor-specific datatypes cause failures?
3. Are there other places where these types appear and cause problems?

It looks as Sqllite returns standard SQL types by comparison. 

Thanks!  I appended a copy of the Edit Dataset view mentioned above. 

Cheers, Robert
![ClickHouse_Superset_Column_Sync](https://user-images.githubusercontent.com/2666453/109085919-7513e000-76bf-11eb-9c26-fd9207deb02a.png)


--

--
Adding to the original request by @suedschwede, I strongly recommend switching to clickhouse-sqlalchemy as the standard driver for ClickHouse. The reasons for this switch are as follows. 

1. clickhouse-sqlalchemy is actively maintained (last commit 18 Feb 2021, under active development).
2. Releases are pushed regularly to pypi.org (last push 14 Dec 2020).
3. It has what appears to be a fairly complete SQLAlchemy implementation. 
4. It supports both HTTP and native TCP wire protocols (vs. HTTP only)
5. It has a good security including support for TLS, SNI, and configurable certificate verification (vs. no encryption support)
6. It has a good suite of tests that check SQLAlchemy interfaces. 

To the extent that there are compatibility issues like the one discussed here, we (Altinity) can get them fixed fairly quickly.  We currently recommend clickhouse-sqlalchemy and clickhouse-driver to our users and have made fixes to them in the past, most recently in December.

What are the next steps to making this switch?
--
",suedschwede,"
--
If you try to make a date/time filter to NULLABLE(DATE) you get an error in superset
![image](https://user-images.githubusercontent.com/25843519/109122105-67ef0380-7748-11eb-8b0a-aca07ed1a89e.png)

The expected behavior is to see just STRING and DATETIME

NULLABLE(STRING) -> STRING
NULLABLE(DATE) -> DATE
NULLABLE(DATETIME) -> DATETIME
--
",villebro,"
--
We're currently working on improving type inference (https://github.com/apache/superset/pull/13294). We'll make it top priority to enable this for Clickhouse as soon as possible.
--
",HeinzMayer,"
--
I have created a temporary fix on my environment
[clickhouse.py](https://github.com/HeinzMayer/superset/blob/1.0.MIC/superset/db_engine_specs/clickhouse.py)

It seems that the master branch does not work with clickhouse-sqlalchemy==0.1.6
[Discussion](https://github.com/apache/superset/discussions/10700)
--
",,
13278,OPEN,Pivot Table - Wrong Sort Order,#bug,2021-02-23 07:14:14 +0000 UTC,ValentinC-BR,Opened,,"When I create a pivot table with a dimension in ""group by"" section and a numeric metric, the rows are not sorted correctly.

### Expected results

Rows should be sorted from the biggest number to the lowest one

### Actual results

Row are sorted as if the metric is a strinc (90 > 9.0 > 80 > 8.1 > 70 etc.)

#### Screenshots

![image](https://user-images.githubusercontent.com/79460908/108719836-59260800-7520-11eb-8a1e-9b8c97ee605d.png)


#### How to reproduce the bug

1. Go to 'Chart'
2. Click on 'Create' / Pivot Table
3. Select a dimension in ""group by""
4. Create a metric (sum(x) for instance)
5. Sort the metric column by clicking on the top right logo if needed

### Environment

(please complete the following information):

- superset version: 1.0.1
- python version: 3.7.9
- node.js version: doesn't apply, I run on Kubernetes, using gunicorn as server
- source : Athena

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

/
",,,,,,,,,,,,,,
13275,OPEN,[Sql_lab]Bug on all SQL Lab queries,need:validation; sql_lab,2021-02-26 14:11:30 +0000 UTC,rmgpinto,Opened,,"I'm running superset 1.0.1 on kubernetes.
All SQL Lab queries return:
```
Database error
'dict' object has no attribute 'set'
```


Logs 
```
Triggering query_id: 296
INFO:superset.views.core:Triggering query_id: 296
Query 296: Running query on a Celery worker
INFO:superset.views.core:Query 296: Running query on a Celery worker
```


```
Query 296: Polling the cursor for progress
[2021-02-19 15:35:54,702: INFO/ForkPoolWorker-1] Query 296: Polling the cursor for progress
Query 296: Polling the cursor for progress
[2021-02-19 15:35:55,831: INFO/ForkPoolWorker-1] Query 296: Polling the cursor for progress
Query 296: Polling the cursor for progress
[2021-02-19 15:35:57,870: INFO/ForkPoolWorker-1] Query 296: Polling the cursor for progress
Query 296: Polling the cursor for progress
[2021-02-19 15:35:59,129: INFO/ForkPoolWorker-1] Query 296: Polling the cursor for progress
Query 296: Polling the cursor for progress
[2021-02-19 15:36:01,206: INFO/ForkPoolWorker-1] Query 296: Polling the cursor for progress
Query 296: Polling the cursor for progress
[2021-02-19 15:36:03,227: INFO/ForkPoolWorker-1] Query 296: Polling the cursor for progress
Query 296 progress: 39.0 / 2244.0 splits
[2021-02-19 15:36:04,248: INFO/ForkPoolWorker-1] Query 296 progress: 39.0 / 2244.0 splits
Query 296: Polling the cursor for progress
[2021-02-19 15:36:05,253: INFO/ForkPoolWorker-1] Query 296: Polling the cursor for progress
Query 296 progress: 2163.0 / 2244.0 splits
[2021-02-19 15:36:06,315: INFO/ForkPoolWorker-1] Query 296 progress: 2163.0 / 2244.0 splits
Query 296: Polling the cursor for progress
[2021-02-19 15:36:07,320: INFO/ForkPoolWorker-1] Query 296: Polling the cursor for progress
Query 296: Storing results in results backend, key: 2fc2200e-2259-461c-9769-6f714192e35c
[2021-02-19 15:36:07,465: INFO/ForkPoolWorker-1] Query 296: Storing results in results backend, key: 2fc2200e-2259-461c-9769-6f714192e35c
```

### Expected results

The query results to return.

### Actual results

No results and an error message is returned.

### Environment

(please complete the following information):

- superset version: `superset version` 1.0.1
- python version: `python --version` 3.8
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ x] I have reproduced the issue with at least the latest released version of superset.
- [ x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,junlincc,"
--
@betodealmeida could you take a look? thanks! 
--
",jonathanmorais,"
--
I caught the same error, can someone take a look? 

`ERROR/ForkPoolWorker-1] Task sql_lab.get_sql_results[xxxxxx-efe2-4445-bdf1-dd587c8fcc1d] raised unexpected: KeyError('query_id')`

Within the same context, I use Athena's database, and in the Worker logs I noticed the KeyError expection: query_id, which in v35 did not exist.
--
",,,,,,,,
13272,OPEN,superset db upgrade not working when upgrading from v 0.35.2 to 1.0.1,#bug,2021-03-10 07:33:25 +0000 UTC,asdi744,In progress,,"I am trying to upgrade `superset` from version 0.35.2 to 1.0.1

### Expected results

`superset db upgrade` command should migrate my default db compatible to v 1.0.1

### Actual results

`superset db upgrade` failed with the following error

sqlalchemy.exc.IntegrityError: (psycopg2.errors.NotNullViolation) column ""table_name"" contains null values

[SQL: ALTER TABLE tables ALTER COLUMN table_name SET NOT NULL]

#### Screenshots

<img width=""1068"" alt=""Screenshot 2021-02-22 at 16 11 29"" src=""https://user-images.githubusercontent.com/4865462/108700589-a1492880-7528-11eb-830f-5400f69a2463.png"">

#### How to reproduce the bug

1. pip install apache-superset
2. superset db upgrade

### Environment

(please complete the following information):

- superset version: `1.0.1`
- python version: `3.7.3`
- flask version: `1.1.2`
- Werkzeug version `1.0.1`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.",,,asdi744,"
--
Dear community, please help.
--
",,,,,,,,,,
13269,OPEN,[chart] Allow to change sorting of Grouped/Stacked bars,enhancement:committed; need:followup; viz:chart-bar,2021-03-01 05:41:50 +0000 UTC,kamalkeshavani-aiinside,Opened,,"**Is your feature request related to a problem? Please describe.**
In Time-series bar chart, when user creates groups of bars(grouped/stacked), the order of bars in a group/stack is based on the value of metric(descending).
In case of Bar chart, the order is based on 'breakdown' dimension value(ascending).
There is an option of selecting 'Sort by', but that is just to calculate which bars will be part of chart when series limit is applied.

**Describe the solution you'd like**
Here, user should be able to sort the bars based on the value of 'Group by' dimension(for time-series bar chart) or by metric value(for bar chart).
For example in below chart(sample flights data), user can sort the bars in each group/stack alphabetically(by airline company) to show 'AA' in left most or right most.
![image](https://user-images.githubusercontent.com/74634977/108677384-ef602b00-752c-11eb-8bdf-a3deaf6ad7c0.png)
![image](https://user-images.githubusercontent.com/74634977/108677672-57af0c80-752d-11eb-9a74-fd655df63816.png)

In case of bar chart, the sorting of bars is alphabetical(ascending) value of 'breakdown' dimension, it should be possiblle to change this behavior as well.
![image](https://user-images.githubusercontent.com/74634977/108678379-5df1b880-752e-11eb-83bc-9a81859873a0.png)

**Describe alternatives you've considered**
A more complex feature can be to sort the bars based on some other metric(as already available for series limit).

",,,,,,,,,,,,,,
13267,OPEN,calculated columns creation using datasetput api,#bug,2021-04-02 04:55:13 +0000 UTC,madhamanchiharsha,In progress,,"HI,
I am automating our dashboard creation using apis. We have to create a calculated columns in our datasets. I have done the same from UI and added a sample image here.In order to create the calculated columns, I am using /api/v1/dataset/{pk} api. However these are the problems I am facing:
 When I try this in swagger: validation fails for override_columns (I have tried false/False/FALSE/0).
 When I try this is in postman. I get the following error saying ""Datasource already exists"".

### Expected results

we need to get response as 201 for creating calculated columns in dataset api

### Actual results

In swagger validation failing for override columns field.

#### Screenshots
![image](https://user-images.githubusercontent.com/14903753/108668971-240abd00-7502-11eb-8180-b284ea3d4207.png)


#### How to reproduce the bug



### Environment

(please complete the following information):

- superset version: 1.0
- python version: 3.8
",,,villebro,"
--
@dpgaspar can you take a look at this?
--
",dpgaspar,"
--
@madhamanchiharsha Can you please add few examples of the POST payload and response
--
",madhamanchiharsha,"
--
headerPart:
http://127.0.0.1:8088/api/v1/dataset/72?override_columns=False
bodyPart:
{
  ""database_id"": 1,
  ""schema"": ""null"",
  ""table_name"": ""examples"",
  ""columns"": [
    {
      ""column_name"": ""example1"",
      ""description"": null,
      ""expression"": ""split_part(\""IterationPath\"", '\\', 3)"",
      ""filterable"": true,
      ""groupby"": true,
      ""python_date_format"": null,
      ""verbose_name"": ""example1""
    },
    {
      ""column_name"": ""example2"",
      ""description"": null,
      ""expression"": ""split_part(\""IterationPath\"", '\\', 4)"",
      ""filterable"": true,
      ""groupby"": true,
      ""python_date_format"": null,
      ""verbose_name"": ""example2""
    },
    {
      ""column_name"": ""example3"",
      ""description"": null,
      ""expression"": ""split_part(\""AreaPath\"", '\\', 2)"",
      ""filterable"": true,
      ""groupby"": true,
      ""python_date_format"": null,
      ""verbose_name"": ""example3""
    }
  ]
}
--
",,,,,,
13263,OPEN,[Explore][Time Picker] Tooltip should have a top and bottom border,,2021-02-22 01:18:22 +0000 UTC,ktmud,Opened,,"## Description

Currently the time picker help text tooltip does not have a top and bottom border, resulting this weird look:

![Snip20210221_23](https://user-images.githubusercontent.com/335541/108645569-23254d00-7468-11eb-82bd-b5c8d2c7badd.png)

We should add a padding to the tooltip content wrapper (the one with black background) and let the content itself scroll instead.


cc @zhaoyongjie 
",,,,,,,,,,,,,,
13257,OPEN,[Global] Revisit open menu on hover,revisit:design-sys; viz:explore:ui,2021-03-17 01:31:13 +0000 UTC,ktmud,In progress,,"We DON't have to change the current behavior, just creating an issue in case this comment got slip through the cracks.

---

@rusackas I'm finding a case where hovering to show menu might have become an inconvenience:

https://user-images.githubusercontent.com/335541/108138715-6439ed80-7073-11eb-8233-e56ef5a99322.mp4

When saving chart, I wanted to click on the ""Save"" button, but if I move the mouse too fast, it will trigger open the hover menu and when I moved back to the position of the ""Save"" button and click, I would accidentally click on the menu and lost all my changes to the chart---which is quite annoying.

I'm wondering if we can:

1. Increase the size of the run button area---it could use a little bit more margin around the buttons.
2. Move SQL lab to the second menu item so the hover menu doesn't overlap with the buttons.
3. Add a delay to open the menu on hover?

to avoid users accidentally clicking on the hovered menu item. We can do one or all of these if they make sense to you.


cc @junlincc @mihir174

_Originally posted by @ktmud in https://github.com/apache/superset/issues/12025#issuecomment-780207617_",,,junlincc,"
--
@ktmud
All make sense, 2 for sure. 
what i was thinking is to  move run button to the bottom, move save to top right corner, once we simplify and reorganize the controls?

<img width=""1591"" alt=""108141052-da405380-7077-11eb-9586-2e24e57bd0d9"" src=""https://user-images.githubusercontent.com/67837651/108633525-7c21c080-7429-11eb-8c0b-3d7e6b6c5ee4.png"">

--

--
""When creating/editing a chart, the save button lines up underneath a drop down to navigate to SQL Lab. The hover area that expands the drop down is pretty large. If a user (me) wanted to save their work on the chart (I did), they might inadvertently trigger the hover to expand the drop down and click through to the SQL editor losing their work on the chart (doh! it happened to me).
It's obviously not a huge deal. Maybe the hover could only be over the text instead of the area or the save button could be moved or a warning could allow the user to cancel navigating away from a chart with unsaved changes."" 

cc @steejay we have an open ticket to track this issue. ^
--
",rusackas,"
--
In the long term, I'm a fan of @junlincc 's proposal... just moving to SIP-34's layout.

In the meantime, there are probably other places one might overshoot with the mouse and run into similar issues.

Option 2 is reasonable enough, but I'm not sure it will solve the problem in other areas. 

Option 3 is intriguing to me... we could do it with JS, or we might be able to do it with a little CSS like [this little demo](https://codepen.io/rusackas/pen/ZEBXKVb) I just threw together.
--
",ktmud,"
--
@rusackas CSS delay looks cool. I did some testing and felt 250~300ms is the most comfortable.
--
",,,,,,
13252,OPEN,[Explore]show actual column name in tooltips on hover,bug:cosmetic; good first issue; viz:explore:datapanel,2021-02-20 01:15:32 +0000 UTC,junlincc,Opened,,"Current, in Explore left data panel Columns section, user see 
    1) actual column name that without a label e.g.` job_intr_dataengn`(column name, no lable)
or 2) label name of columns e.g. `yt_codingtuts360 label`(label name of `yt_codingtuts360`)
from the view. 
when user hover on the text, exact same information
change: display actual column name in tooltip, when there's a label 
<img width=""278"" alt=""Screen Shot 2021-02-19 at 5 13 13 PM"" src=""https://user-images.githubusercontent.com/67837651/108577451-ff320200-72d5-11eb-871a-1f08ee6df1b6.png"">

<img width=""293"" alt=""Screen Shot 2021-02-19 at 5 08 39 PM"" src=""https://user-images.githubusercontent.com/67837651/108577236-2805c780-72d5-11eb-8377-5a157f0122a1.png"">
",,,,,,,,,,,,,,
13251,OPEN,[Explore]severe input delay in Edit dataset modal,bug:regression; viz:explore:dataset,2021-02-20 01:05:14 +0000 UTC,junlincc,Opened,,"see video, getting 3s second delay after typing. can we have a standard across the entire product? 

https://user-images.githubusercontent.com/67837651/108576881-022bf300-72d4-11eb-94d2-1c6b9e6e1c9c.mov

",,,,,,,,,,,,,,
13249,OPEN,[Explore][Control] Allow selecting ad-hoc metrics in filters,bug:regression; viz:explore:filter,2021-02-22 09:53:13 +0000 UTC,ktmud,In progress,,"When adhoc filters were first added ( #4909 ), it can select from selected adhoc metrics. Part of the code is also still there, but the feature somehow stopped working from certain point.

Could be related to #12095 ",,,ktmud,"
--
I discovered this while working on #13221 

Will take a look once more refactoring is done.
--

--
Two feature requests I've been thinking on for a while:

1. **Adhoc Column:** similar to adhoc metrics, users should also be able to create adhoc calculated columns (dimensions)
2. **Adhoc metric/column in other controls:** users should be able to select already created adhoc metrics and columns in other controls (e.g. sort by and filters).
--
",junlincc,"
--
> Will take a look once more refactoring is done.

happy to take it on while we are refractor the filter component in a few weeks. @ktmud 
@zuzana-vej can you provide a complete list of expected behavior in ad hoc filter and metric popover, + new features suggestion (i.e, in-place saving metric)?

--

--
@villebro @ktmud hey both, please take a look the draft 1 year roadmap for Explore and send me comments, thoughts before I finalize? I believe we captured most of the significant items requested by the community. 
shared a gsheet link with you, please comment directly on it.  


<img width=""1509"" alt=""Screen Shot 2021-02-21 at 10 23 33 AM"" src=""https://user-images.githubusercontent.com/67837651/108634560-f4d74b80-742e-11eb-9846-3c53613af3df.png"">
<img width=""1484"" alt=""Screen Shot 2021-02-21 at 10 23 51 AM"" src=""https://user-images.githubusercontent.com/67837651/108634562-f6a10f00-742e-11eb-920a-be697978b8ee.png"">



--

--
Thank you all for suggesting new features. 
committed to both. 
--
",villebro,"
--
> Two feature requests I've been thinking on for a while:
> 
> 
> 
> 1. **Adhoc Column:** similar to adhoc metrics, users should also be able to create adhoc calculated columns (dimensions)
> 
> 2. **Adhoc metric/column in other controls:** users should be able to select already created adhoc metrics and columns in other controls (e.g. sort by and filters).

Big +1 for adhoc columns!
--
",zhaoyongjie,"
--
> Two feature requests I've been thinking on for a while:
> 
>     1. **Adhoc Column:** similar to adhoc metrics, users should also be able to create adhoc calculated columns (dimensions)
> 
>     2. **Adhoc metric/column in other controls:** users should be able to select already created adhoc metrics and columns in other controls (e.g. sort by and filters).

I'm trying to redesign the semantic layer, it might help. these are good requests.
--
",,,,
13248,OPEN,[explore] Show detailed data type tooltip when hovering type icon,enhancement:committed; good first issue,2021-02-20 01:17:15 +0000 UTC,mistercrunch,Opened,,"Minor improvement: it would be nice to show the type as in `VARCHAR(50)` when hovering the icon in the metadata panel (`ABC` icon in the screenshot bellow)

<img width=""372"" alt=""Screen Shot 2021-02-19 at 3 57 38 PM"" src=""https://user-images.githubusercontent.com/487433/108574083-364ee600-72cb-11eb-8bdd-d0e5ef993349.png"">
",,,junlincc,"
--
@mistercrunch 
one more related #13252 
--
",,,,,,,,,,
13247,OPEN,[explore] metadata panel should show numeric icon for `REAL` data type,enhancement:committed; viz:explore:datapanel,2021-02-20 01:19:04 +0000 UTC,mistercrunch,Opened,,"
<img width=""704"" alt=""Screen Shot 2021-02-19 at 3 53 08 PM"" src=""https://user-images.githubusercontent.com/487433/108573976-e3752e80-72ca-11eb-9d7d-a9b81dc67236.png"">
",,,junlincc,"
--
@mistercrunch this one goes under a bigger epic, will take it in house 
https://github.com/apache/superset/issues/12961
--
",,,,,,,,,,
13243,OPEN,Can't open Settings -> List Users when using OAuth with Google for an Admin user,bug:regression; global:user-list,2021-03-09 08:42:24 +0000 UTC,pingusix,Opened,,"A clear and concise description of what the bug is.

### Expected results

User list displays and users can be edited When settings -> List Users is selected

### Actual results

Get an access denied popup and return to dashboard


#### How to reproduce the bug

Enable AUTH_TYPE=AUTH_OAUTH in config.py

Snippet of config.py: 
 
AUTH_TYPE = AUTH_OAUTH
AUTH_USER_REGISTRATION = True
AUTH_USER_REGISTRATION_ROLE = ""Admin""
PUBLIC_ROLE_LIKE_GAMMA = True
OAUTH_PROVIDERS = [
    {
        ""name"": ""google"",
        ""icon"": ""fa-google"",
        ""token_key"": ""access_token"",
        ""remote_app"": {
            ""client_id"": ""xxx"",
            ""client_secret"": ""xxx"",
            ""api_base_url"": ""https://www.googleapis.com/oauth2/v2/"",
            ""client_kwargs"": {""scope"": ""email profile""},
            ""request_token_url"": None,
            ""access_token_url"": ""https://accounts.google.com/o/oauth2/token"",
            ""authorize_url"": ""https://accounts.google.com/o/oauth2/auth"",
        }
    }
]

Register a new user using Google as a provider. 
Go to settings-> List Users

Also changed auth type back to AUTH_DB and logged in as admin. I then deleted permissions for new google-based admin user and added them back in so they matched the currently authorised admin. Logged out, changed config.py back to use oauth and logged in as google-based admin user. Problem remains (also created  a new auth_db based user with same permissions and worked correctly)


### Environment

(please complete the following information):

Ubuntu 18.04.3
Superset 1.0.1
Python 3.7.5
Node 8.10.0

",,,rrittsteiger,"
--
We are also struggling with this problem.
--
",,,,,,,,,,
13242,OPEN,"Make possible to disable the annoying ""This dashboard is currently force refreshing"" message",enhancement:request; revisit:design-sys; viz:dashboard:ui,2021-03-31 07:43:32 +0000 UTC,ktecho,In progress,,"**Is your feature request related to a problem? Please describe.**

With Superset 1.0, it seems a new toast has been added for when you have auto-refresh in a dashboard:

""This dashboard is currently force refreshing; the next force refresh will be in 10 seconds.""

This is super-annoying because it distracts your attention from the changing dashboard. I think the ""infinite / superset"" animated symbol in each graph is enough. Besides, if you go to another tab in your browser, sometimes you can see more than one toast stacked.

**Describe the solution you'd like**

If you think it's a good idea to have the toast displaying each 10 seconds (maybe once at first would be fine...), please make it configurable so I can disable it from the UI or directly in the json config.

**Describe alternatives you've considered**

One alternative would be displaying the message just once when you open a dashboard.
",,,junlincc,"
--
> if you go to another tab in your browser, sometimes you can see more than one toast stacked.

@ktecho Thanks for reporting. I sometimes find the toast massage taking away the focus and annoying too(as the PM). agree that one loading-infinity icon should be sufficient to indicate refresh activity. 
@steejay wdyt?

https://user-images.githubusercontent.com/67837651/108571307-c178ad80-72c4-11eb-8714-c20fd7963500.mov


--

--
we had a discussion about adding a status bar to show some basic info and dashboard interaction setting in the dashboard view, which could also include 'last refresh'.  cc designer @mihir174 

@ktecho can we agree that ""last updated"" indicator is still necessary for dashboard consumer, but not necessary in a form of the toast msg? 
--
",Steejay,"
--
hmm interesting. i think displaying the toast every 10 seconds is excessive. its like an alarm clock you cant turn off in the morning. how do you turn on auto refresh? is it through the Superset UI? If so, we can add some information in the location of that control via the use of help text (in which we could take away the toast completely for this use case). Therefore when the user turns on auto refresh theyll understand the result of this action in context. 

Another potential alternative would be to include a ""last updated"" indicator somewhere on the dashboard. 
--

--
@ktecho agreed. it would be good to consider the users who might experience the dashboard too. for the dashboard creator the auto refresh cadence might be well understood but for someone who only observes the dashboard, they might not have context on how often the dashboard is refreshing unless they're counting the intervals. are we willing to accept this potential ambiguity? if so the loading symbol might be enough as is. if not a ""last updated"" indicator could add context while still staying relatively subtle. 
--
",ktecho,"
--
Yes, I like @Steejay suggestion about including a ""last updated"" indicator. But even without that, you'll see that the dashboard is auto-refreshing because the widgets refreshes with the animated infinite symbol.
--

--
Yeah, totally agree.
--

--
@junlincc Yeah, I think that would make it.
--
",,,,,,
13240,OPEN,Resizing row and column vs chart behaves differently in Dashboard Edit Mode,bug:regression; viz:dashboard:editmode,2021-02-19 23:52:05 +0000 UTC,zuzana-vej,Opened,,"When a user is trying to resize charts on dashboards, they behave differently based on if it is a row or a column, or just a chart. 

Chart can be resized by going to the bottom right corner, in both directions (width or height).

If you add a row or a column on the dashboard, those can be resized (in one of the directions) by going to the size of the object.

In the two cases, the mouse hover will look differently (hard to see in the video recording). Users can't always figure out how to resize chart inside a row or column and assume the functionality is broken.

See the two behaviors:
![resizing_row_vs_chart](https://user-images.githubusercontent.com/61221714/108543252-af354a00-7299-11eb-8e92-4f08bc406f5b.gif)


**Describe the solution you'd like**
Consider making the resizing experience consistent.

",,,zuzana,"
--
Added the bug label since we confirmed with few users that in the past the experience was consistent across resizing chart and row / column. 
--

--
Users can always adapt over time :). They found it confusing / thought there was a high pri bug, but if this is part of the library there might not be a lot we can do easily. We can all keep an eye on this and see if there are other user reports. 
--
",junlincc,"
--
Expected behavior could have been changed by us bumping the drag and drop library by 9 major versions recently. 
is the new behavior problematic? or do you think users can slowly adapt? 
@zuzana-vej 


--
",,,,,,,,
13239,OPEN,Exporting dashboard(s) with REST API creates .json instead of .zip or .yaml,dashboard:export; doc:user,2021-04-01 19:53:39 +0000 UTC,krsnik93,In progress,,"I am trying to export and then import back a dashboard by using Superset's REST API. The documentation (https://superset.apache.org/docs/rest-api) for endpoint `/dashboard/export` states: ""Exports multiple Dashboards and downloads them as YAML files."" However, the resulting file is seemingly always .json (regardless of number of dashboards exported). Furthermore, when trying to import back into Superset, endpoint `/dashboard/import` seems to require a .zip file.

So:
1. Documentation states .yaml file will be created
2. Exporting actually creates .json file
3. Import endpoint expects .zip file


### Expected results

Expect to be able to export and then import back an unchanged dashboard file.

### Actual results

```
Feb 19 16:27:14 superset gunicorn[15773]: ERROR:root:__init__() got an unexpected keyword argument 'passwords'
Feb 19 16:27:14 superset gunicorn[15773]: Traceback (most recent call last):
Feb 19 16:27:14 superset gunicorn[15773]:   File ""/usr/local/lib64/python3.8/site-packages/flask_appbuilder/api/__init__.py"", line 84, in wraps
Feb 19 16:27:14 superset gunicorn[15773]:     return f(self, *args, **kwargs)
Feb 19 16:27:14 superset gunicorn[15773]:   File ""/usr/local/lib/python3.8/site-packages/superset/views/base_api.py"", line 79, in wraps
Feb 19 16:27:14 superset gunicorn[15773]:     duration, response = time_function(f, self, *args, **kwargs)
Feb 19 16:27:14 superset gunicorn[15773]:   File ""/usr/local/lib/python3.8/site-packages/superset/utils/core.py"", line 1286, in time_function
Feb 19 16:27:14 superset gunicorn[15773]:     response = func(*args, **kwargs)
Feb 19 16:27:14 superset gunicorn[15773]:   File ""/usr/local/lib/python3.8/site-packages/superset/dashboards/api.py"", line 722, in import_
Feb 19 16:27:14 superset gunicorn[15773]:     command.run()
Feb 19 16:27:14 superset gunicorn[15773]:   File ""/usr/local/lib/python3.8/site-packages/superset/dashboards/commands/importers/dispatcher.py"", line 57, in run
Feb 19 16:27:14 superset gunicorn[15773]:     command = version(self.contents, *self.args, **self.kwargs)
Feb 19 16:27:14 superset gunicorn[15773]: TypeError: __init__() got an unexpected keyword argument 'passwords'
```
This error happens in `v0.ImportDashboardsCommand` in `https://github.com/apache/superset/blob/master/superset/dashboards/commands/importers/dispatcher.py`. The request should never reach that command as the `v1` version is the one that gets executed first. However, version `v1` fails because `metadata.yaml` is not in the uploaded .zip file as per `https://github.com/apache/superset/blob/7bef5ab4d23faf571c8cbf2e55a623dac0ee7f64/superset/commands/importers/v1/utils.py`.

#### Screenshots



#### How to reproduce the bug

1. Create dashboard
2. Export dashboard with ` curl  -v --cookie ""session=<your_cookie>"" --output dashboard.zip http://<your_host>/api/v1/dashboard/export/?q=[<your_dashboard_id>]`
3. do `cat dashboard.zip` to check that a .json file was created instead
4. Uploading the file back to `/dashboard/import` as it was created results in a ""Not a zip file"" type of error
5. Zipping the resulting .json from step 2. results in the error described in the previous section as not all needed files are in the archive

### Environment

(please complete the following information):

- superset version: 1.0.0
- python version: 3.8.6
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,willbarrett,"
--
Looping in @betodealmeida and @dpgaspar on this one
--
",junlincc,"
--
@srinify Srini, can you help make sure the doc states the expected behavior? I believe it should yaml though. 

--
",betodealmeida,"
--
@KrishMunot you need to enable the `VERSIONED_EXPORT` feature flag to have the dashboard exported as a ZIP file.

As for the error you get, the v0 importer was fixed to take `**kwargs` in its arguments, so it shouldn't happen anymore.
--
",KrishMunot,"
--
Hi @betodealmeida are you sure you tagged the right dev? 
--
",krsnik93,"
--
Thanks @betodealmeida, just tested this and was able to import back in.

I don't understand why it would depend on `VERSIONED_EXPORT` though. I would say expected behavior is for `VERSIONED_EXPORT` either not to affect this or to be enabled by default. Worst case would be to at least mention in the API docs for import/export that it requires this flag, but this is a dirty workaround as (once again) this flag and what it does should not affect import/export functionality.
--
",,
13238,OPEN,Bulk deleting dashboards doesn't instantly reflect in the list,bug:regression; global:bulk-delete,2021-02-20 01:33:34 +0000 UTC,mistercrunch,Opened,,"Bulk deleting works as expected on the backend, except that after the user confirmation, it doesn't remove the dashboards from the list view and doesn't get out of bulk delete mode.",,,willbarrett,"
--
Adding this as a ticket for our next sprint.
--
",junlincc,"
--
@mistercrunch tested both chart list and dashboard list. both worked properly in master. what version are you on? 

https://user-images.githubusercontent.com/67837651/108578759-94ce9100-72d8-11eb-8e3c-30a6e88f7c72.mov




--
",,,,,,,,
13237,OPEN,Importing chart over API fails if the chart was built on top of a virtual dataset,bug; chart:import,2021-02-19 23:17:59 +0000 UTC,krsnik93,Opened,,"Importing chart over API fails if the chart was built on top of a virtual dataset. I exported the chart with the `/chart/export` endpoint and then tried POSTing the resulting .zip to ` /chart/import`.

### Expected results

Expect chart to get uploaded successfully.

### Actual results

```
Feb 19 15:17:45 superset gunicorn[10599]:   File ""/usr/local/lib/python3.8/site-packages/superset/models/slice.py"", line 103, in cls_model
Feb 19 15:17:45 superset gunicorn[10599]:     return ConnectorRegistry.sources[self.datasource_type]
Feb 19 15:17:45 superset gunicorn[10599]: KeyError: 'view'
Feb 19 15:17:45 superset gunicorn[10599]: During handling of the above exception, another exception occurred:
Feb 19 15:17:45 superset gunicorn[10599]: Traceback (most recent call last):
Feb 19 15:17:45 superset gunicorn[10599]:   File ""/usr/local/lib/python3.8/site-packages/superset/charts/api.py"", line 993, in import_
Feb 19 15:17:45 superset gunicorn[10599]:     command.run()
Feb 19 15:17:45 superset gunicorn[10599]:   File ""/usr/local/lib/python3.8/site-packages/superset/charts/commands/importers/dispatcher.py"", line 66, in run
Feb 19 15:17:45 superset gunicorn[10599]:     raise exc
Feb 19 15:17:45 superset gunicorn[10599]:   File ""/usr/local/lib/python3.8/site-packages/superset/charts/commands/importers/dispatcher.py"", line 55, in run
Feb 19 15:17:45 superset gunicorn[10599]:     command.run()
Feb 19 15:17:45 superset gunicorn[10599]:   File ""/usr/local/lib/python3.8/site-packages/superset/commands/importers/v1/__init__.py"", line 71, in run
Feb 19 15:17:45 superset gunicorn[10599]:     raise self.import_error()
Feb 19 15:17:45 superset gunicorn[10599]: superset.charts.commands.exceptions.ChartImportError: Import chart failed for an unknown reason
```

#### How to reproduce the bug

1. Create a virtual dataset in SQL Lab
2. Create a chart on top of that dataset
3. Export chart via REST API `/chart/export` endpoint with:
`curl -v --cookie ""session=<your_cookie>"" --output <filename>.zip http://<your_host>/api/v1/chart/export/?q=[<your_chart_id>]`
4. Try to import resulting .zip file with:
`curl 'http://<your_host>/api/v1/chart/import/' \
     -X POST \
    -H 'Cookie: session=<your_cookie>'  \
    -H ""X-CSRF-Token: <your_csrf_token>"" \
    -F formData=@<filename>.zip \
    -F overwrite=true \
    --compressed \
    --insecure`

### Environment

(please complete the following information):

- superset version: 1.0.0
- python version: 3.8.7
- node.js version: v12.20.1

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,junlincc,"
--
@dpgaspar could you please take a look? thanks Daniel! 
--
",,,,,,,,,,
13234,OPEN,Add Opsgenie Integration to Alerts Modal,enhancement:request; global:alert,2021-02-23 17:21:30 +0000 UTC,KentonParton,In progress,,"**Is your feature request related to a problem? Please describe.**
Superset currently doesn't support Opsgenie as an alert integration which many companies use.

**Describe the solution you'd like**
I would like to add Opsgenie as a notification method to the alert modal using the [Opsgenie API integration](https://docs.opsgenie.com/docs/alert-api#create-alert). The easiest implementation will be to add Opsgenie to the list of notification methods and allow users to create a custom payload as shown in the Opsgenie API docs.

Alternatively, UI components can be created for the different payload fields such as priority, responders, tags etc.

When Opsgenie is selected as a notification method, ""Message Content"" should be greyed out.

**Describe alternatives you've considered**
Currently, alerts can be dispatched to Opsgenie via Slack; however, this creates a dependency on Slack and is not ideal.

**Additional context**
<img width=""1430"" alt=""Screen Shot 2021-02-19 at 2 17 17 PM"" src=""https://user-images.githubusercontent.com/20202312/108503555-3ea02480-72bd-11eb-891f-0fa38c6385eb.png"">

",,,willbarrett,"
--
Hey @KentonParton - I think we'd like to eventually move to a plugin architecture that would allow additional notification types to be easily added by the community. Is that an effort you'd be interested in participating in?
--

--
@KentonParton yes, that would be fine for now. Go for it!
--
",KentonParton,"
--
Hey @willbarrett thanks for the comment. While I'd love to try the plugin approach, I can't say I am hugely familiar with Superset. Would it be okay to keep the scope to adding the Opsgenie implementation then have a follow-up PR trying the plugin route?
--
",,,,,,,,
13232,OPEN,Retreiving data chartd/datasets/dashboards based on,chart:export; dashboard:export; question,2021-03-29 10:36:53 +0000 UTC,supermarin1,In progress,,"Is there the way to export data charts/dashboards based on with REST API v. 1.0.0?

Tried /chart/export/, /database/export/, /dashboard/export/ and have no luck. Or I understand wrong meaning of export?

Please, advise.",,,junlincc,"
--
@dpgaspar ^ Daniel could you help answer? thanks! 
--
",willbarrett,"
--
@betodealmeida would you mind weighing in?
--
",supermarin1,"
--
please, anybody?
--

--
@GGPay thanks

I can get all info about dashboard/chart/dataset but I'm interesting in possibility to get/export data dashboard/chart/dataset based on. 

I tried all API calls and did not find any way to retrieve data at least from DB. Is this possible? 
--

--
I've got .zip file with .yaml files which are schemas, not values.

May be smth need to be configured in additional way? 
--

--
And I can't get data from DB through the API?
--

--
Yes! That what I was looking for. Thanks @GGPay @dpgaspar 
--

--
The next question:

I have schema and information for ordering from `/api/v1/chart/export?q=[chat_ds]` (please note order columns is empty)
`  order_by_cols: [],
  order_desc: true`
  
  Forming the json for `api/v1/chart/data` POST request ` ""order_desc"": true ` doesn't provide any ordering. Changing to `false` doesn't make any impact. 
  
  How can I get the proper ordering?
 
--

--
Thera are no test case covering my case :-( And this is more about `/api/v1/chart/export` endpoint. 

I'm able to get all info from `api/v1/chart/export?q=[chart_id]` to form post request to `/api/v1/chart/data` **except** ordering columns.

There is no chart settings as for sorting
![image](https://user-images.githubusercontent.com/24755510/112824371-1f9e6a80-9093-11eb-9024-050579a58d90.png)
but there is sorting in SQL query (chart -> view query)
![image](https://user-images.githubusercontent.com/24755510/112824530-507e9f80-9093-11eb-9a09-9a8edab1672d.png)

As the result - I can't get the same result with API as I can see on UI.

May be there is the way we can get that query? 

--
",GGPay,"
--
I've used **dashboard** with q parameters  `{
  ""order_column"": ""changed_on_delta_humanized"",
  ""order_direction"": ""desc"",
  ""page"": 0,
  ""page_size"": 25
}`

Got response 
![image](https://user-images.githubusercontent.com/17413180/111582648-2078f780-8789-11eb-8455-9746d1188e03.png)


And then make a request - where id in parentheses is ID of dashboard

http://127.0.0.1:8088/api/v1/dashboard/export/?q=!(3)

Wouldn't say it's proper solution but that the way how i made it work.




--

--
As i said before - open url in a browser where 142 - is chart ID

Don't forget authenticate.

http://your_superset_host/api/v1/chart/export/?q=!(142)

3 - Dashboard ID

http://your_superset_host/api/v1/dashboard/export/?q=!(3)

And get yaml zip file

--

--
I don't really understand what are you trying to achieve? Dashboard/Charts/Queries export are export of .yamls schemas. Don't understand why you need export values - just write SQL to database.

Take a look chart/data  or reverse frontend request to find what you're looking.

--

--
I'm not really famililar with `api/v1/chart/data` endpoint but have look tests - it's really useful

`superset/tests/charts/api_tests.py`
--
",dpgaspar,"
--
@supermarin1 like @GGPay take a looks at `api/v1/chart/data`
--
",,
13226,OPEN,[Explore]set default aggregate based on selected column type,enhancement:request; viz:explore:metrics,2021-02-19 00:43:42 +0000 UTC,mistercrunch,Opened,,"We could save 2 clicks by assuming people want to `SUM` numeric columns and `COUNT(DISTINCT {col})` strings.

<img width=""364"" alt=""Screen Shot 2021-02-18 at 2 51 30 PM"" src=""https://user-images.githubusercontent.com/487433/108432218-e1429f80-71f8-11eb-9f23-5413e47aff4d.png"">
",,,junlincc,"
--
@mistercrunch This popover is gonna function quite differently in d&d,  we will take it into consideration while redesigning. 

--

--
set dynamic aggregate function default based on column types
numeric and contains `_id_ _key_  _code_ ` -> COUNT(DISTINCT)
other numeric -> `SUM`
strings -> `COUNT(DISTINCT)`
TEXT -> SUM(LENGTH) (?)

@zhaoyongjie @villebro 
--
",,,,,,,,,,
13207,OPEN,[Alerts & Reports] Can't switch 'Message content' from a Chart to a Dashboard,#bug; global:alert,2021-02-18 22:25:31 +0000 UTC,vnourdin,Opened,,"When I configure an Alert or a Report from the new `ALERT_REPORTS` interface, I can change the chart that is sent (""Message content"") but if I replace the chart by a dashboard, when saving, the alert/report switch back to the old chart.

### Expected results
I can modify an alert/report and switch from any chart to any dashboard and vice versa.

### Actual results
From what I've tested, the current state is:
* from a dashboard to another one :heavy_check_mark:
* from a dashboard to a chart :heavy_check_mark:
* from a chart to another one :heavy_check_mark: 
* from a chart to a dashboard :heavy_multiplication_x: 

#### Screenshots
![chart-to-dashboard](https://user-images.githubusercontent.com/11643685/108359740-ab012200-71f0-11eb-9356-293979b4b7af.gif)

#### How to reproduce the bug
1. Go to 'Settings'  'Alerts & Reports'
2. Click on '+ Alert' to create a new one
3. Fill it and choose a 'Chart' as content
4. Save
5. Edit your alert, change content to a 'Dashboard'
6. Save
7. Edit your alert again, the content should be back to the old 'Chart'

### Environment
- superset version: `1.0.1`
- python version: `3.7.9`
- node.js version: doesn't apply, I run on Kubernetes, using gunicorn as server",,,willbarrett,"
--
Thank you for the bug report @vnourdin - we'll look into it!
--
",,,,,,,,,,
13206,OPEN,[todo]Ability to re-rerun CI failed jobs,enhancement:committed; to-do,2021-02-19 04:44:31 +0000 UTC,michael-s-molina,Opened,,"Today when we need to re-run CI jobs we need to force a push or open/close a PR to trigger CI execution. It would be great if we could just re-run failed jobs.

As @villebro suggested, we can implement something like `/rerun failed` to accomplish that.

This functionally would improve CI processing time and speed up PR reviews.

@villebro @rusackas @junlincc",,,junlincc,"
--
@michael-s-molina thanks for suggesting! anything that improves developer experience and process efficiency make sense to be prioritized. 
@ktmud Since Michael doesn't have the permission right, could you help? 
--
",rusackas,"
--
The https://github.com/apache-superset/superset-github-bot could probably be leveraged here too, to perform some actions with emoji-driven processes. For example:

Restart failed CI jobs:


Merge when CI passes


Cancel CI Jobs

--
",,,,,,,,
13202,OPEN,[Alerts & Reports]Allow 'Raw data' report for chart in new 'Alerts & Reports' UI.,global:alert,2021-03-02 00:36:06 +0000 UTC,kamalkeshavani-aiinside,Opened,,"**Is your feature request related to a problem? Please describe.**
The old UI for 'Chart Email Schedules' has an option to send Visualization or Raw data in the email/slack. This option of sending 'Raw data'(csv) is not available in new UI.
Due to issue of too many repeat notifications using old UI, I cannot use old UI as well for now.

**Describe the solution you'd like**
When the user selects 'Message Content' as '**Chart**', another dropdown should allow the user to select the 'Message format' as 'Visualization' or 'Raw data'(with default of visualization).

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**

New UI:
![image](https://user-images.githubusercontent.com/74634977/108328195-a7c95e80-720f-11eb-9482-8815089af74e.png)

Old UI:
![image](https://user-images.githubusercontent.com/74634977/108328091-87999f80-720f-11eb-8baa-a3ee357c7204.png)",,,willbarrett,"
--
Hi @kamalkeshavani-aiinside thanks for the report! I've seen another request for CSV report types, so this is something we'll be looking into.
--

--
Hi @kamalkeshavani-aiinside - thanks for clarifying the issue. We'll prioritize adding ""Raw Data"" export to the new reports, but it is unlikely to make v1.1 at this point given that the branch has been cut. However, knowing that this is functionality missing from the new Alerts & Reports that was in the old, I'll put it on the top of the stack for enhancements to this system for my team at Preset. I'm sorry that I can't provide an ETD at this point.
--
",kamalkeshavani,"
--
@willbarrett Just wanted to point that csv raw data reporting feature is lost(in a way) in Superset v1.0.x, since the old 'Chart Email Schedule' cron job has some issues.
So although I understand that PDF reporting(and maybe another option) is also under consideration, I would request to at least make csv reporting usable again in v1.1
--
",,,,,,,,
13193,OPEN,[Superset SPA] Change frontend code that depends on the url,SPA; enhancement:committed; to-do,2021-02-26 05:58:21 +0000 UTC,suddjian,Opened,,"Once Superset is a Single Page App, we should try to change code that uses `window.location` to use the tools in `react-router` instead.

`window.location` is referenced 71 times in 45 files in the Superset repo. Most of these can probably change to use the location from react-router, but some might be trickier.",,,,,,,,,,,,,,
13192,OPEN,Edit Dataset Table Selector Overlap,bug:cosmetic; viz:explore:dataset,2021-02-18 22:30:18 +0000 UTC,yousoph,Opened,,"A clear and concise description of what the bug is.

### Expected results
Dropdown selector to select a table should not be overlapped so user can scroll and select their table. 

### Actual results
The table is overlapped by the button toolbar. 

#### Screenshots
<img width=""890"" alt=""Explore_-_bart_lines"" src=""https://user-images.githubusercontent.com/10627051/108280712-6e232580-7133-11eb-8ff2-e454d3af4166.png"">

#### How to reproduce the bug

1. From Explore, click ""Edit Dataset""
2. Click the lock button to make changes
3. Select a Database and Schema 
4. Try to select a table

### Environment

(please complete the following information):

- superset version: `master`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.",,,,,,,,,,,,,,
13187,OPEN,Add Google Sheets dataset bad error msg,bug:cosmetic,2021-02-17 23:30:47 +0000 UTC,mistercrunch,Opened,,"Add Google Sheets dataset bad error msg, bad formatting

From the logs, ""Got an error Column must be constructed with a non-blank name or assign a non-blank .name before adding to a Table"" should be surfaced to the user
```
DEBUG:urllib3.connectionpool:https://docs.google.com:443 ""GET /spreadsheets/d/1VsvYexiZo8CXxIGGtjYYtHFWvPjws1jUDDLOZ9Whr78/gviz/tq?gid=0&tq=SELECT%20%2A%20LIMIT%200 HTTP/1.1"" 200 None
WARNING:superset.datasets.dao:Got an error Column must be constructed with a non-blank name or assign a non-blank .name before adding to a Table. validating table: https://docs.google.com/spreadsheets/d/1VsvYexiZo8CXxIGGtjYYtHFWvPjws1jUDDLOZ9Whr78/edit?usp=sharing
```

<img width=""753"" alt=""Screen Shot 2021-02-17 at 12 18 42 PM"" src=""https://user-images.githubusercontent.com/487433/108262849-568b7300-711a-11eb-91ad-8560797a7441.png"">
",,,,,,,,,,,,,,
13183,OPEN,Postgres connection,dashboard:import; data:connect:postgres,2021-02-18 22:34:51 +0000 UTC,wildsonc,Opened,,"I'm trying to link the superset to postgres, but when I try to import a dashboard it crashes. I also can't save charts.

#### Screenshots

![image](https://user-images.githubusercontent.com/72408418/108253301-644ff100-7138-11eb-9b24-5107813dde3d.png)
![image](https://user-images.githubusercontent.com/72408418/108253425-86497380-7138-11eb-9048-c46e97e5f4c9.png)

#### How to reproduce the bug

1. Go to Import Dashboard
2. Click on Upload
3. See error.

### Environment

- superset version: 1.0.1
- python version: Python 3.8.5
- postgres version: psql (PostgreSQL) 12.6 (Ubuntu 12.6-0ubuntu0.20.04.1)
- Instaled from Scratch

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ x] I have reproduced the issue with at least the latest released version of superset.
- [ x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Log

ERROR:superset.app:Exception on /superset/import_dashboards [POST]
Traceback (most recent call last):
  File ""/root/superset/lib/python3.8/site-packages/flask/app.py"", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File ""/root/superset/lib/python3.8/site-packages/flask/app.py"", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/root/superset/lib/python3.8/site-packages/flask/app.py"", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/root/superset/lib/python3.8/site-packages/flask/_compat.py"", line 39, in reraise
    raise value
  File ""/root/superset/lib/python3.8/site-packages/flask/app.py"", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/root/superset/lib/python3.8/site-packages/flask/app.py"", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/root/superset/lib/python3.8/site-packages/flask_appbuilder/security/decorators.py"", line 109, in wraps
    return f(self, *args, **kwargs)
  File ""/root/superset/lib/python3.8/site-packages/superset/utils/log.py"", line 164, in wrapper
    value = f(*args, **kwargs)
  File ""/root/superset/lib/python3.8/site-packages/superset/views/core.py"", line 660, in import_dashboards
    databases = db.session.query(Database).all()
  File ""/root/superset/lib/python3.8/site-packages/sqlalchemy/orm/query.py"", line 3373, in all
    return list(self)
  File ""/root/superset/lib/python3.8/site-packages/sqlalchemy/orm/query.py"", line 3535, in __iter__
    return self._execute_and_instances(context)
  File ""/root/superset/lib/python3.8/site-packages/sqlalchemy/orm/query.py"", line 3556, in _execute_and_instances
    conn = self._get_bind_args(
  File ""/root/superset/lib/python3.8/site-packages/sqlalchemy/orm/query.py"", line 3571, in _get_bind_args
    return fn(
  File ""/root/superset/lib/python3.8/site-packages/sqlalchemy/orm/query.py"", line 3550, in _connection_from_session
    conn = self.session.connection(**kw)
  File ""/root/superset/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 1142, in connection
    return self._connection_for_bind(
  File ""/root/superset/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 1150, in _connection_for_bind
    return self.transaction._connection_for_bind(
  File ""/root/superset/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 409, in _connection_for_bind
    self._assert_active()
  File ""/root/superset/lib/python3.8/site-packages/sqlalchemy/orm/session.py"", line 289, in _assert_active
    raise sa_exc.InvalidRequestError(
sqlalchemy.exc.InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: 'ascii' codec can't encode character '\xe1' in position 7: ordinal not in range(128) (Background on this error at: http://sqlalche.me/e/13/7s2a)
INFO:werkzeug:170.83.134.254 - - [17/Feb/2021 18:50:15] ""POST /superset/import_dashboards HTTP/1.1"" 500 -
INFO:werkzeug:170.83.134.254 - - [17/Feb/2021 18:54:59] ""GET /superset/import_dashboards HTTP/1.1"" 200 -
",,,willbarrett,"
--
@betodealmeida you're the most familiar with import - thoughts?
--
",,,,,,,,,,
13177,OPEN,Support sending report in PDF & CSV,enhancement:request; global:report,2021-02-18 22:35:19 +0000 UTC,DRavikanth,In progress,,"
In the latest version of Superset we introduced Alerts & Reports new functionality for scheduled email of reports. However currently we are sending only a snapshot of the dashboard using the webdriver. Ask is to support sending the screenshot of the dashboard embedded in either CSV/PDF file. Most of the BI tools in the market support this functionality. It would be good to have this functionality supported in Superset as well. 
",,,willbarrett,"
--
Thanks for the suggestion @DRavikanth - I'm going to look into it.
--
",DRavikanth,"
--
Thanks for taking a look at it @willbarrett. Please review the requirement and let me know when this can be targeted for.
--
",,,,,,,,
13172,OPEN,Couldn't able to add reports,global:report; question,2021-03-31 06:39:01 +0000 UTC,wargod797,In progress,,"![image](https://user-images.githubusercontent.com/43702120/108200899-1c839680-7145-11eb-9841-96015ba52d24.png)

After following doc from below link setup email config 
https://superset.apache.org/docs/installation/email-reports

ENABLE_ALERTS = True
ENABLE_ALERTS = True

email schedule working fine 
Add button is gray couldn't able add any alerts or reports 

i'm using superset 1.0.0 Jan 21st Release
firefox-geckodriver = 0.29

OS: Ubuntu 20.04 LTS Linux Sub System



please help",,,dpgaspar,"
--
@wargod797 please fill out the issue bug form, specially interested on knowing you superset version or commit SHA.
Thank you

--
",wargod797,"
--
@dpgaspar  I'm using superset 1.0.0 Jan 21 2021 Release facing same issue on 1.0.1 Feb 6 th Release also, filled the bug form as requested
--
",,,,,,,,
13162,OPEN,[sqllab] While querying from sqllab editor to database table; characters written Japanese and Chinese are getting converted to '???',enhancement:request; i18n:chinese; i18n:japanese,2021-02-17 01:04:25 +0000 UTC,puranikamey,In progress,,"A clear and concise description of what the bug is.
While performing CURD operations using sqllab editor, Chinese and Japanese chars are getting converted to '???'.  For ex. While inserting value in to the table, insert query runs successfully but data inserted in the table getting converted to  '?????'. Also, if you refresh page, values in the sql editor (Chinese and Japanese chars) are getting converted to '???'. 

### Environment

- superset version: 0.36.0
- python version: 3.6.9
- node.js version: 6.11.5 <7.0.0 || >= 8.9.0

### Expected results
On Sqllab editor, Chinese and Japanese chars should not be getting converted to '???'.

### Actual results
Should be able to perform CURD operations using Chinese and Japanese language. 

#### Screenshots

As you can see data inserted  in the table is ??????

![image](https://user-images.githubusercontent.com/23127814/108135531-4629b880-7086-11eb-8578-0108f0b3adaf.png)

**Refresh page**
![image](https://user-images.githubusercontent.com/23127814/108141383-9bb79280-7091-11eb-911e-e658f0170af5.png)




#### How to reproduce the bug

1. Go to /superset/sqllab
2. Select database(connection)
3. Select schema(connection)
4. Create new table. (Mysql: create table multilang (name varchar(80) null); Snowflake: create table multilang (name nvarchar);
5. Run following query : insert into multilang values('');
6. Insert query will run successfully, however data inserted will be '??????'
7. Refresh the page; insert into multilang values(''); will get converted to insert into multilang values('???????');


### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,puranikamey,"
--
Just FYI: We are using charset=utf-8 in connection string. Also, We are able to perform all the above scenerios via connecting through snowflake UI etc, so this seems to be superset UI issue. 
--
",,,,,,,,,,
13158,OPEN,[Dashboard]Deleting an existing tab immediately after having created a new tab with title does not work,bash!; bug; viz:dashboard:tab,2021-02-16 20:38:02 +0000 UTC,geido,In progress,,"  Hi @geido thanks for PR it looks good changes, I just found some buggy behavior, can you look please:

https://user-images.githubusercontent.com/56388545/107982543-e91dfc00-6fcc-11eb-99d9-3986b30f00ad.mov

_Originally posted by @simcha90 in https://github.com/apache/superset/issues/12773#issuecomment-779390572_",,,geido,"
--
Specific steps to reproduce:

1. Create a new tab and give it a title
2. Create another tab and give it a title
3. Now without moving the focus to it,  click on the ""X"" icon of the **previous** tab to delete it
4. Observe the issue. The Delete modal will remain stuck


https://user-images.githubusercontent.com/60598000/108106681-f7d0e580-7096-11eb-8d6e-7b35f5f82ed8.mp4



--
",,,,,,,,,,
13155,OPEN,[explore] BigQuery DatabaseError surfaced as UnknownError with bad formatting,good first issue; viz:explore:error,2021-02-17 01:07:18 +0000 UTC,mistercrunch,Opened,,"BigQuery error is hard to read.

### Expected results

In Explore, when creating a bad expression (say `DATE_TRUNC(column_that_dont_exist, DAY)`) in BigQuery, the DatabaseError is shown as a UnknownError. In SQL Lab, DatabaseErrors are surfaced properly and make sure to use a monospace font so that the formatting is preserved. For most database, the formatting doesn't matter much, but for BigQuery it's important to preserve it.

#### Screenshots

In explore (where the issue shows)
<img width=""1098"" alt=""Screen Shot 2021-02-16 at 8 02 26 AM"" src=""https://user-images.githubusercontent.com/487433/108088556-74c77500-702d-11eb-8034-dd9f46d8eed6.png"">

In SQL Lab, where it's better formatted
<img width=""1307"" alt=""Screen Shot 2021-02-16 at 8 04 41 AM"" src=""https://user-images.githubusercontent.com/487433/108088726-a6404080-702d-11eb-9421-ca0ca276faeb.png"">
",,,,,,,,,,,,,,
13151,OPEN,Issue in sql Editor on trying to run query or selecting table in SQL LAB,,2021-02-18 06:27:21 +0000 UTC,Newgin-sam,Opened,,"### Screenshot
![screenshot](https://user-images.githubusercontent.com/60768808/108072203-99066e80-708c-11eb-88b4-1ed483d3855a.PNG)
### Description
When navigating to sql editor and trying to run a query or selecting table on left dropdown the error as above screen shot
I am able to create datasets create charts and display on dashboards but i colud'nt run any query in sql editor as the error comes

### Error

Database error
Instance <Query at 0x1b8f2ec1e50> is not bound to a Session; attribute refresh operation cannot proceed (Background on this error at: http://sqlalche.me/e/13/bhk3)",,,yousoph,"
--
#13041 fixes this 
--
",suzjstgithub,"
--
https://github.com/apache/superset/issues/12824

ENV WIN10pyhon venv 3.7.1  superset 1.0.1
same bug
Database error
Instance <Query at 0x243d3302ac8> is not bound to a Session; attribute refresh operation cannot proceed (Background on this error at: http://sqlalche.me/e/13/bhk3)

fixbug
Lib\site-packages\superset\utils\core.py 
![image](https://user-images.githubusercontent.com/62527347/108314748-60ce6f80-71f5-11eb-92e8-336c837bd853.png)

--
",,,,,,,,
13146,OPEN,Line chart: Option to display values next to the data points,,2021-02-16 08:19:28 +0000 UTC,leocape,Opened,,"**Is your feature request related to a problem? Please describe.**
When looking at a line chart, it's difficult to see what the precise values of the data points are - currently we need to gauge their approximate value by eyeballing the x and y axis, which is straining and not accurate. Especially for quick glances / reporting in meetings.

**Describe the solution you'd like**
To have the values displayed next to the data points themselves, so that we can see the precise values without having to approximate them by eyeballing the axes. This should be optional, and can be shown/hid in the Customize menu

**Describe alternatives you've considered**
This is really the only approach

**Additional context**
&nbsp;
As is:
&nbsp;
![image](https://user-images.githubusercontent.com/67806134/108034365-15259580-703e-11eb-9647-db6fa4f158e8.png)

&nbsp;
To be:
![image](https://user-images.githubusercontent.com/67806134/108034647-7b121d00-703e-11eb-8f29-32eced1ad7b8.png)

&nbsp;
Example of how Looker does it:
&nbsp;
![image](https://user-images.githubusercontent.com/67806134/108034896-e1973b00-703e-11eb-8c8d-37cf79ce9a68.png)
&nbsp;
Where the option to show / hide might be in the UI:
&nbsp;
![image](https://user-images.githubusercontent.com/67806134/108035493-aea17700-703f-11eb-8d0e-f1d3f73c3a14.png)

",,,,,,,,,,,,,,
13125,OPEN,[Dataset][Time filter] Allow setting a global or dataset-level default time range,data:database:config; data:dataset:edit; enhancement:committed; viz:explore:timepick,2021-03-17 17:50:13 +0000 UTC,ktmud,In progress,,"**Is your feature request related to a problem? Please describe.**

Currently new charts are created with the default time range filter value set to [""Last week""](https://github.com/apache-superset/superset-ui/blob/46f0a174fed7b7c518d078b0eb16b129183c3b9d/packages/superset-ui-chart-controls/src/shared-controls/index.tsx#L319). This may make sense for some businesses, but often cause confusion when users didn't notice this filter is being implicitly applied.  Experienced users may even have learned the first thing they do when creating a new chart is to reset this to `No filter` (or whatever is the best for their dataset).

**Describe the solution you'd like**

It would be ideal if we can provide a global, as well as database and dataset-level settings on the default Time Range filter when creating a new chart. For dataset-level configuration, we may even allow it to be configurable for each datetime column.

We can start simple with global level configuration that each business can adjust for their own needs. In the future, in the spirit of doing less implicit things, we should probably also consider setting the default global config to ""No filter"".

Or we can focus on implementing the dataset level config, and set it as ""Last week"" for all existing datasets.

**Describe alternatives you've considered**

Simply changing the default to ""No filter"" may not be desired because time filters have impact on the performance of db queries and setting ""No filter"" as global default may cause undesired long running queries, especially when a chart start running new queries right after creation (e.g. table chart).

**Additional context**

Ref: https://github.com/apache/superset/pull/13109#issuecomment-778928053",,,junlincc,"
--
I agree with this change - setting time range filter at the dataset-level. the reason we set filter to default ""Last week"" is to prevent users from running expensive explorational query. Not sure about changing default to""No filter"" though, shouldn't we allow user to set default? or similar to ad-hoc filter, store value in local storage as user's own default? 
--

--
https://github.com/apache/superset/issues/12542 related
--

--
> We could also just focus on Dataset level settings and make the default time range filter a required field when creating a new dataset.

@ktmud 
Will asking new users to setup default time range filter be too confusing at the first place? since the user need is coming from those who have been forced to switch range from ""Last week"" to ""No Filter"". Why not store users' own ""default"" at local storage? 
--

--
what about setting [max date -7, max date) of any dataset by default? @ktmud 
--

--
engineering wise, shouldn't be too hard. and if min and max dates in dataset can be detected, do we still need any configuration? 
--
",ktmud,"
--
@junlincc the idea IS to allow users to set default. I was just proposing we don't set the initial default. Once the global setting for default time filter is added, Superset may ship with the global config as ""No filter"" because it is the most neutral choice. It also forces organizations to pick a default value that makes the most sense to them.

We could also just focus on Dataset level settings and maybe make the default time range filter a required field when creating a new dataset.

(Updated the issue description to reflect these clarifications.)
--

--
I think remembering users' last selected time filter MAY work, but it doesn't solve the problem of the initial default ""last week"" not making sense to certain datasets (i.e. those who don't update every week). We still need a dataset-level default.

We can drop the idea of making it a required field as I imagine it'd be quite hard to implement a good UX for it---especially considering most virtual datasets are created from SQL Lab. Whether the global initial default is ""Last week"" or ""No filter"" is debatable, but I guess it doesn't matter much as long as there is a way to change it in Superset configs.
--

--
That's a great idea, but I'm not sure how easy it is implement, though--both engineering and design-wise. We need a clean design to communicate this default value to users clearly, and it needs to be performant (which means there has to be some kind of cache, you can't just query what is the min/max date on the fly).
--

--
@junlincc It's not easy engineering wise.
--
",villebro,"
--
I like the idea of being able to define a global default time range in `superset_config.py` (this would be the one e.g. the native filter defaults to), and then being able to override it per dataset.
--
",,,,,,
13124,OPEN,Dashboard Email Reports Selector displays ID instead of Name,global:report,2021-02-17 14:46:47 +0000 UTC,jawabuu,In progress,,"## Screenshot

![image](https://user-images.githubusercontent.com/49921621/107914239-0b832b80-6f73-11eb-947a-4d3befb06165.png)


## Description

When creating a Dashboard Email report the selector displays the ID instead of the name.
For Example `Dashboard <1>` instead of `World Bank's Data`
I have observed this in version `1.0.0` and `1.0.1`
Chart Email reports work just fine.


## Design input
Display Dashboard Name.
@junlincc ",,,ktmud,"
--
There is an open PR for this: https://github.com/apache/superset/pull/11776

Although you can now also try out the new Alert & Report manager by turning on the [`ALERT_REPORTS`](https://github.com/apache/superset/blob/50fa10054fd5d2c28553c4ffd754e12bdedef5da/RELEASING/release-notes-1-0/README.md#feature-flags) feature flag.
--

--
Have you turned off `ENABLE_REACT_CRUD_VIEWS` by any chance?

The new alerts and reports kind of depend on it: https://github.com/apache/superset/blob/52ba82fb8b9f1ce5519f1a13f6417a0ce486e7df/superset/views/alerts.py#L80


@lilykuang @nytai do you think whether it makes sense to force turning on `ENABLE_REACT_CRUD_VIEWS` when  `ALERT_REPORTS` is enabled?
--
",jawabuu,"
--
Hey @ktmud 
Thank you
I have the following set up
```
FEATURE_FLAGS = {
    ""KV_STORE"": True,
    ""SHARE_QUERIES_VIA_KV_STORE"": True,
    ""ENABLE_TEMPLATE_PROCESSING"": True,
    ""ENABLE_REACT_CRUD_VIEWS"": False,
    ""ROW_LEVEL_SECURITY"": True,
    ""ALERT_REPORTS"": True,
    ""DASHBOARD_NATIVE_FILTERS"": True,
    ""THUMBNAILS"": True,
  }
```
However, when I visit `Settings>Alerts & Reports` I get a 404

![image](https://user-images.githubusercontent.com/49921621/107926359-a9ccbc80-6f86-11eb-9ed0-1c0a10260b6d.png)

--

--
@ktmud I had turned it off. Let me turn it on and retry.
--

--
@ktmud 
I am now able to view the reports and have set up a test report to be sent every minute.
I have celery workers and beat running as well as cache configured.
![image](https://user-images.githubusercontent.com/49921621/107951380-4d2ec900-6fa9-11eb-927a-33555244958b.png)
I have not received any so far, is there any further configuration I need to do?

--

--
Managed to find these from github search

```
from celery.schedules import crontab
CELERYBEAT_SCHEDULE = {
          'email_reports.schedule_hourly': {
              'task': 'email_reports.schedule_hourly',
              'schedule': crontab(minute='*/59'),
          },
          'alerts.schedule_check': {
            'task': 'alerts.schedule_check',
            'schedule': crontab(minute='*/4'')
          },
          'reports.scheduler': {
            'task': 'reports.scheduler',
            'schedule': crontab(minute='*/5'),
          },
          'reports.prune_log': {
              'task': 'reports.prune_log',
              'schedule': crontab(minute=0, hour=0),
          },
      }
```
--
",nytai,"
--
@ktmud We could throw a warning if `ALERT_REPORTS` is enabled, but `ENABLE_REACT_CRUD_VIEWS` is not. I think that fully deprecating the `ENABLE_REACT_CRUD_VIEWS` feature flag makes more sense, I don't think there's any strong case for continuing to support the old FAB views at this point. 
--
",dpgaspar,"
--
@nytai agree, planning on removing the MVC CRUD `ModelView` code also. But we can deprecate the flag now
--
",,,,
13115,OPEN,2 TIme Column Filter for Superset Apache,question,2021-02-15 04:09:27 +0000 UTC,alvinlabiano,Opened,,"Is there are way to create a widget with Superset Apache where the SQL Query is based on 2 time columns?

Example Query:

""SELECT 
   title
  ,pipeline_id
  ,alc_production_status
  ,campaign_start_date
  ,campaign_end_date
  ,website_name
  ,performance_status_name
  ,sum(spend) AS spend
FROM superset_native_dashboard.v_content_list_with_facebook_ads_v1
WHERE ((campaign_start_date BETWEEN date '2020-12-27' AND date '2020-12-28') OR
     (campaign_end_date   BETWEEN date '2020-12-27' AND date '2020-12-28'))
GROUP BY
   title
  ,pipeline_id
  ,alc_production_status
  ,campaign_start_date
  ,campaign_end_date
  ,website_name
  ,performance_status_name",,,,,,,,,,,,,,
13112,OPEN,[Explore] Validation Error indicator need some spacing adjustment,bug:cosmetic; good first issue; viz:explore:control,2021-02-15 03:27:58 +0000 UTC,ktmud,Opened,,"## Screenshot

<img width=""778"" alt=""spacing-for-frontend-errors"" src=""https://user-images.githubusercontent.com/335541/107871282-f021fe80-6e54-11eb-8476-745bf9847c1f.png"">


## Description

There needs to be some space between the run buttons and the validation error indicator. (You see this indicator when there are frontend validation errors, which can happen when you create a new bar chart.)
",,,,,,,,,,,,,,
13110,OPEN,Limit Decimals in Surburst Chart while hovering #superset,,2021-02-14 06:30:03 +0000 UTC,Kamakshi12,Opened,,"## Screenshot
![image](https://user-images.githubusercontent.com/78851183/107870121-e7184800-6ebb-11eb-855b-2cef232e84e5.png)

## Description
I would like to limit the decimals to 2 places on the surburst chart while hovering on it. Kindly suggest ways to accomplish the same.

",,,,,,,,,,,,,,
13107,OPEN,[Dashboard][Edit Mode] Add a Refresh Chart List link,enhancement:request,2021-02-15 16:56:55 +0000 UTC,ktmud,Opened,,"**Is your feature request related to a problem? Please describe.**

After creating a chart, it's not very convenient to add a chart to the desired position I want. But if I'm already in Dashboard Edit mode (in another browser window), it would be nice it I can just refresh the chart list and starting adding the new chart:

<img width=""464"" alt=""refresh-chart-list"" src=""https://user-images.githubusercontent.com/335541/107859773-97b91580-6df0-11eb-92a0-4350b154db3f.png"">

**Describe the solution you'd like**

Add a ""Refresh"" button or link to the chart list

**Describe alternatives you've considered**

If the list can automatically refresh, that would also be great. This could be achieved via [WebWorker or localStorage](https://stackoverflow.com/questions/28230845/communication-between-tabs-or-windows).

**Additional context**

There are other proposals on making adding charts to dashboard easier: https://github.com/apache/superset/issues/13012

But this fix should be the simplest.",,,,,,,,,,,,,,
13106,OPEN,World map error after upgrade to 1.0.1,question; viz:chart-maplayer,2021-02-23 13:04:39 +0000 UTC,vialcollet,In progress,,"I updated from 0.38 to 1.0.1.
I performed db upgrade as expected.
Then when trying to display a world map chart (directly or from dashboard) it is not loading and this message is displayed:
```
Admin user does not exist
```
My only admin user has `bastien` as a username.

### Expected results

I am expecting the world map to be displayed eventhough there is no admin user with `admin` as a username.

### Actual results

Following message is displayed:

```
Unexpected error
Admin user does not exist. Please, check if test users are properly loaded (`superset load_test_users`).
```

#### Screenshots

![image](https://user-images.githubusercontent.com/4971086/107847885-802d5d00-6def-11eb-953c-a871e9f3d908.png)


#### How to reproduce the bug

1. Install superset
2. Create an admin with a specific username other than `admin`
3. Log on the UI
4. Create a World map graph

### Environment

(please complete the following information):

- superset version: 1.0.1
- python version: 3.8.7
- node.js version: 12

### Checklist

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x ] I have reproduced the issue with at least the latest released version of superset.
- [x ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

I have workaround the issue by adding an admin user with `admin` as a username.
",,,vialcollet,"
--
It's worth mentioning that this was working as expected in 0.38
--
",TheRum,"
--
I'm also facing the same issue after upgrading to Superset 1.0.1.
--
",,,,,,,,
13103,OPEN,Dashboard / Charts not loading (first time after login; every login) in superset #supersetissues #superset,,2021-02-13 06:25:47 +0000 UTC,Kamakshi12,Opened,,"While opening charts/dashboard after login for the first time am getting the following error in superset. However after sometime in the same login when load it again, I am getting the dashboards / charts properly.
![Screenshot_2021-02-13 Screenshot](https://user-images.githubusercontent.com/78851183/107843534-ec10c500-6df1-11eb-8484-00b5723792ee.png)

P.S.: This happens at the first instance of every login. 

Kindly let me know a solution.",,,,,,,,,,,,,,
13101,OPEN,[list]hide all sample dashboard and chart,enhancement:request; viz:dashboard:list,2021-02-13 01:40:36 +0000 UTC,junlincc,Opened,,"As a Superset user, I would like to have the option to hide all sample dashboards from the list/card view. 
currently in Superset there's no content organization ability, also no way to restore deleted objects. Sample dashboards and charts are helpful for first-time users, and can be a good reference from time to time. but they can get really annoying when mixing with users' own content in the list as they grow.....

Proposed change: 1) ability to hide all samples - short term. 2) folder - long term 
<img width=""1825"" alt=""Screen Shot 2021-02-12 at 5 39 38 PM"" src=""https://user-images.githubusercontent.com/67837651/107838138-4d388a00-6d59-11eb-8e7f-372740de4a9c.png"">
",,,,,,,,,,,,,,
13089,OPEN,It is not possible to edit a database if it is not accessible,#bug,2021-02-12 10:04:30 +0000 UTC,iercan,In progress,,"We have a database connection that is not accessible anymore. I want to remove it from sql lab view but since database is not accessible I'm getting error in the picture.  

### Expected results

Database should be saved successfully.  

### Actual results
Giving below error and doesn't save
![image](https://user-images.githubusercontent.com/3406152/107741456-404a7500-6d1e-11eb-88bf-451494630839.png)


#### How to reproduce the bug

1. Add any database 
2. Make database inaccessible 
3. Try to edit database sqllab settings (i.e. disable sql lab expose)

### Environment

(please complete the following information):

- superset version: 1.0.1
- python version: 3.7

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,dpgaspar,"
--
Every DB connection you create is shown on SQLLab (if the user has permissions for it). On this case I think you actually want to remove the connection (delete the record from the db connection list)

--

--
Yes, I understand now. By design when an update is done, superset fetches the existing schemas from the db and updates the necessary security permissions. This behaviour can probably be improved if it's a problem to not be able to edit for example SQLLab database properties.

Can you explain in more detail, your use case for editing SQLLab properties for a database that is not accessible?


--
",iercan,"
--
No I don't want to remove database in case I may use it in future. Also this is not just about sqllab, database is not editable in anyway if it is inaccessible. I just gave sqllab situation  as an example.  
--

--
I just wanted to disable it from sqllab for more clear view. I think there is no need to fail because of a connection issue for every update action while there is a 'test connection' button there. 
--
",,,,,,,,
13085,OPEN,[table]save sort order of table column when saving/overwriting chart,enhancement:committed; viz:chart-table,2021-02-12 02:55:40 +0000 UTC,junlincc,Opened,,"Currently in table chart, there is no way to save sort order of a column when saving the chart. 

Feature request: save column sort-by when saving(over-writing) a table chart. 

https://user-images.githubusercontent.com/67837651/107725141-63334580-6c9a-11eb-8d72-3d353dba1292.mov

",,,,,,,,,,,,,,
13081,OPEN,Possibility to plot Line of best fit in a Line Chart?,enhancement:request; good first issue; need:followup; viz:chart-line,2021-02-16 07:08:38 +0000 UTC,klazaj,Opened,,"Is there a way to plot Line of best fit (simple linear regression) into a Line chart, with times series data? Does this option exist in Superset, or should it be coded manually into a SQL Lab before applying it to a chart? ",,,junlincc,"
--
good question.... @villebro does annotation support? if not, can we extend this feature to support? 
--
",villebro,"
--
Hi, we don't currently have support for this, but it should be fairly straight forward to implement for the new chart types (=the ones using the new chart data API). If someone wants to work on this I can give directions on how to get started.
--
",,,,,,,,
13080,OPEN,HELM CHART: After successful deployment and login; internal query with SQL Alchemy fails,#bug,2021-02-18 21:41:09 +0000 UTC,joehoeller,In progress,,"Here is the error that is returned after logging in from GUI:

```

Sorry, something went wrong
500 - Internal Server Error
Stacktrace
        Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1277, in _execute_context
    cursor, statement, parameters, context
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedTable: relation ""user_attribute"" does not exist
LINE 2: FROM user_attribute 
             ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python3.7/site-packages/flask/_compat.py"", line 39, in reraise
    raise value
  File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python3.7/site-packages/flask/app.py"", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/app/superset/views/core.py"", line 2545, in welcome
    .filter_by(user_id=g.user.get_id())
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/query.py"", line 3496, in scalar
    ret = self.one()
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/query.py"", line 3463, in one
    ret = self.one_or_none()
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/query.py"", line 3432, in one_or_none
    ret = list(self)
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/query.py"", line 3508, in __iter__
    return self._execute_and_instances(context)
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/query.py"", line 3533, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1011, in execute
    return meth(self, multiparams, params)
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py"", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1130, in _execute_clauseelement
    distilled_params,
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1317, in _execute_context
    e, statement, parameters, cursor, context
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1511, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/util/compat.py"", line 182, in raise_
    raise exception
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1277, in _execute_context
    cursor, statement, parameters, context
  File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation ""user_attribute"" does not exist
LINE 2: FROM user_attribute 
             ^

[SQL: SELECT user_attribute.welcome_dashboard_id AS user_attribute_welcome_dashboard_id 
FROM user_attribute 
WHERE user_attribute.user_id = %(user_id_1)s]
[parameters: {'user_id_1': '1'}]
(Background on this error at: http://sqlalche.me/e/13/f405)

```",,,joehoeller,"
--
@willbarrett plz assist (?)
--

--
There was no migration, this is literally on startup.

On Thu, Feb 18, 2021 at 3:28 PM Will Barrett <notifications@github.com>
wrote:

> Hi @joehoeller <https://github.com/joehoeller> - please make sure that
> your database migrations have completed successfully. If there is an issue
> with the migrations, please post the backtrace here. Additionally, please
> fill out the Bug Report issue template so we can better assist you.
>
> 
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/apache/superset/issues/13080#issuecomment-781646823>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABHVQHCJQSWDLRVZ2WA2NBLS7WBBNANCNFSM4XPJQN5A>
> .
>

--
",willbarrett,"
--
Hi @joehoeller - please make sure that your database migrations have completed successfully. If there is an issue with the migrations, please post the backtrace here. Additionally, please fill out the Bug Report issue template so we can better assist you.
--
",nytai,"
--
There is a k8s job that runs migrations and creates the admin user, you'll want to find the logs for that job as it likely failed or was skipped. It should be suffixed with `-init-db`

The definition is here: https://github.com/apache/superset/blob/b34c86382525811fd707a9c133b9051d5c46a393/helm/superset/templates/init-job.yaml#L17

The config is here:
https://github.com/apache/superset/blob/b34c86382525811fd707a9c133b9051d5c46a393/helm/superset/values.yaml#L192
--
",,,,,,
13077,OPEN,Import dataset YAML in GUI,dataset:import; enhancement:request,2021-03-31 04:52:06 +0000 UTC,omnidepp,Opened,,"**Is your feature request related to a problem? Please describe.**
I can happily export/download complex dataset-configurations through the GUI as a YAML files:
![Bildschirmfoto 2021-02-11 um 16 50 57](https://user-images.githubusercontent.com/14217074/107661280-6ffb6d80-6c89-11eb-9184-3a2397e415d2.png)

 However, I cannot import these configuration in a similar manner. 

**Describe the solution you'd like**
I would like to upload/import dataset yaml files through the GUI.

**Describe alternatives you've considered**
I'm using a docker setup, so using the commandline tool requires significantly more effort like additional volumes etc.
",,,,,,,,,,,,,,
13073,OPEN,International boundaries on mapbox-based visualizations: Legal issues,,2021-02-11 08:20:13 +0000 UTC,pradhangn,Opened,,"### Problem:
Each country has legal regulations on what boundaries you can display on a map (eg. India, China). Due to disputes there are differing boundary definitions in each jurisdiction. Currently Superset shows a default mapbox country boundary which would violate Indian and Chinese and possibly other countries laws.

This means companies or users who make dashboards or share screenshots of mapbox visualizations from superset may be subject to legal backlash from Governments.

### Solution:

Mapbox offers a solution through their world views feature:
Blog: https://blog.mapbox.com/dynamically-change-disputed-national-borders-14c820cae923
Code example: https://docs.mapbox.com/mapbox-gl-js/example/toggle-worldviews/

We can use this mapbox feature in 2 ways:

1. A system setting to control the world-view option via superset config file: simple change
2. Add ability to change worldview in each visualization that uses mapbox: larger scope

### Alternatives
We don't have an alternative that will follow the law of all jurisdictions

",,,,,,,,,,,,,,
13068,OPEN,Example Dashboards: Tabbed Dashboard Charts Don't Show Up,bug:regression; need:validation,2021-02-25 23:57:33 +0000 UTC,yousoph,Opened,,"### Expected results
Dashboard should load charts properly or we should remove the tabbed dashboard from the example dashboards 

### Actual results
![image](https://user-images.githubusercontent.com/10627051/107592047-cb155d80-6bc0-11eb-97d5-6b65a8a75a1e.png)
Charts aren't working in the Tabbed example dashboard 

#### How to reproduce the bug

1. Click on Dashboards
2. Click on the Tabbed Dashboard 

### Environment

(please complete the following information):

- superset version: 'master'
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.",,,junlincc,"
--
I encountered this message twice before. 
1. with native dashboard filter FF tuned on, it went away after nuking db
2. changing properties 

I will look into it. thanks for reporting! @yousoph 
--

--
confirmed that this sample dashboard has been broken since/in v1.0 
and the dashboard seems got removed as a whole(does not exist anymore)...

<img width=""1760"" alt=""Screen Shot 2021-02-10 at 6 40 43 PM"" src=""https://user-images.githubusercontent.com/67837651/107597495-a674b200-6bcf-11eb-89ab-0b647bdf033c.png"">

--
",,,,,,,,,,
13066,OPEN,SQLLab Query Results not appearing if we leave tabs open for a long time,#bug; sql_lab:editor,2021-02-11 02:31:22 +0000 UTC,pabrahamusa,Opened,,"SuperSet SQLLab query search will not display any results , even if there are results. This happens when we leave the browser open for a long time say 1-2 hours. Or we revisit the SqlLab (Chrome) after ~2 hours. 

As a workaround I have to close the tab and reopen the tab, Copy the SQL query across then run the query again a fresh.

### Expected results

I expect the SQL results to appear in SQLLab no matter how long I leave my browser open 

### Actual results

No Results for SQL search 

#### Screenshots

If applicable, add screenshots to help explain your problem.

![image](https://user-images.githubusercontent.com/47830507/107581754-855b9380-6bc6-11eb-9b43-81047ae3e85e.png)


#### How to reproduce the bug

1. Go to SQLLab and open three tabs 
2. Go to each tab and execute some query, verify the results are appearing.
3. Now leave the browser for 1 to 2 hours. If Chrome close the browser tab and open again after 2 hours
    the same tabs will be reloaded from cache
4. Go to one of the tab , click Run . The SQL will be executed but results wont be displayed.

### Environment

(please complete the following information):

- superset version: 1.0.1 (build from master branch as of Feb 9 2021)
- python version: Python 3.7.9
- node.js version: `node -v`


### Additional context

As a workaround I have to Copy the SQL query from the broken tab, close the tab and reopen the tab, paste the SQL query across then run the query again a fresh. The main problem is sometimes we get confused that there are no results for a query then later only we realize this is a SuperSet UI issue.
",,,,,,,,,,,,,,
13065,OPEN,Can't edit SQL expression of metric/calculated columns in datasets,#bug; sql_lab:editor,2021-02-11 02:31:48 +0000 UTC,betodealmeida,Opened,,"When editing a dataset it's impossible to edit the SQL expression of metrics/calculated columns

### Expected results

I should be able to edit.

### Actual results

I can't edit, the input field for SQL expression doesn't show up.

#### Screenshots

N/A

#### How to reproduce the bug

1. Edit a dataset
2. Try to change the SQL expression of a metric or calculated column

### Environment

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

N/A
",,,yousoph,"
--
I think this is an issue with the UI not being clear rather than a bug. 

Clicking the SQL expression allows you to edit it for metrics: 
![image](https://user-images.githubusercontent.com/10627051/107586573-0e69cf00-6bb5-11eb-9905-7e3c6ae924d9.png)

And calculated columns: 
![image](https://user-images.githubusercontent.com/10627051/107586826-8fc16180-6bb5-11eb-9fd2-bf298cf1fad8.png)

--
",,,,,,,,,,
13062,OPEN,"database names with ""$"" in name converted to ""%24"" in connection string",data:connect:mysql; question,2021-02-11 02:34:43 +0000 UTC,beautah,Opened,,"I have a several mySQL databases with the naming convention: `<user>$<device>`. When querying the connection seems to fail often, seemingly with different behavior for different methods called. It was apparent from the error message that the database name was being mutated to `<user>%24<device>`. Its a simple enough fix to use the following mutator function in the superset_config.py to fix this:

```
def DB_CONNECTION_MUTATOR(uri, params, username, security_manager, source):  
    uri.database = uri.database.replace('%24', '$')  
    return uri, params
```
But given ""$"" is a valid character in a mySQL database's name it seems to me that this should work ""out of the box"" so to speak. Again, easy fix but seems like a bug. Excellent choice to make the DB_CONNECTION_MUTATOR available in the config, not sure if that's borrowed from another library, but its a slick way to get the job done.

### Expected results

I expect the connection string to keep the name of the database correct, even with a ""$"". Or conversely convert the url encoding back to the original before passing it to sqlalchemy's create_engine

### Actual results

It's obviously converting to a url safe string, I'm guessing urllib.parse? Any ""$"" gets converted to ""%24""

#### Screenshots

None... the error had something to do with invalid credentials, but in reality the database it was looking for doesn't exist

#### How to reproduce the bug

Create a database with a ""$"" in the name of the database, mySQL in particular. Then try to connect in SQLab

### Environment

- superset version: `1.0`
- python version: `3.8`
- node.js version: `unknown`

### Checklist

- [x ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x ] I have reproduced the issue with at least the latest released version of superset.
- [x ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

This is no biggie, easily solved with a provided methodology (mutate the connection), but given its a valid use case I wanted to make the devs aware. Sorry no logs, they have since been deleted after I fixed and I'd rather not revert just to get the exact error message/traceback.
",,,,,,,,,,,,,,
13060,OPEN,when user doesn't have permission to access resource; http code 400 is returned,global:error,2021-02-11 02:35:41 +0000 UTC,duynguyenhoang,Opened,,"Currently when user doesn't have permission to access resource, for example save a chart, the http code 400 is returned.

### Expected results

http code 403 is returned.

### Actual results

http code 400 is returned.

### Environment

- superset version:  master
- python version: 3.8
- node.js version: v12.19.0

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

",,,,,,,,,,,,,,
13048,OPEN,Add search and pagination to Time Series Table Chart,enhancement:request; need:followup,2021-02-20 07:15:14 +0000 UTC,lctdulac,In progress,,"Hi SuperSet community!

There is no filter box nor page-like option for Time-Series tables. 

<img width=""792"" alt=""Screenshot 2021-02-10 at 10 37 12"" src=""https://user-images.githubusercontent.com/46674763/107492439-bb7c2180-6b8c-11eb-991e-a684b6478897.png"">


It'd be great if it was possible to do it as in the Table view (below) :

<img width=""1429"" alt=""Screenshot 2021-02-10 at 10 37 28"" src=""https://user-images.githubusercontent.com/46674763/107492451-bf0fa880-6b8c-11eb-84da-c2cd6f69bc07.png"">


What do you guys think ?

Thanks,
Lance
",,,junlincc,"
--
changing title to `add search and pagination to Time Series Table Chart`

Thanks for suggesting! @lctdulac can you let us know your use cases? why/how do you find adding search and pagination helpful in time series table chart? 
--

--
@lctdulac thanks for the additional information! It really makes sense from the product standpoint. I wish we can get to it soon Feel free to implement the proposed change though, we are happy to review. 
--
",lctdulac,"
--
Thanks @junlincc for the answer!
My use case is that I am monitoring 200+ time-series which is a bit hard to do in a single page because :
* The header columns disappear when scrolling down
* There is no search bar that can help navigating (like in the table view) without filtering the whole dashboard
* There is no pagination which helps to understand the number of time series there is in the chart.

All in all this is just cosmetics but I think this chart could benefit a lot from these small changes.
Hopefully that makes sense.
--
",,,,,,,,
13045,OPEN,[timezone]add timezone setting to chart and advanced analytics,enhancement:committed; global:timezone; need:followup,2021-02-16 06:45:49 +0000 UTC,amuluowin,In progress,,"**Is your feature request related to a problem? Please describe.**
1.I have some statistics that require time zone conditions, but the time conditions use the service time zone by default
2.When I switch the time granularity in the dashboard, I cannot reset `Comparison Period Lag`

**Describe the solution you'd like**
1.Add timezone option to `Filter Box` and `Charts`.
2.The `Big Number with` Trendline option `Comparison Period Lag`
",,,junlincc,"
--
timezone setting is highly requested lately and is something we are committed to implement in Q2-Q3. @amuluowin please help us by providing detailed use cases. thanks! 
--
",amuluowin,"
--
OK
--
",,,,,,,,
13043,OPEN,Filter Box incorrect,need:followup,2021-02-11 10:24:11 +0000 UTC,Beknazar19,In progress,,"Hello everybody.
I have a problem with the filter when I went from 0.38 to 1.0.0

ORA-00936: missing expression
This may be triggered by:
Issue 1002 - The database returned an unexpected error. 
![image](https://user-images.githubusercontent.com/45395471/107468027-70184200-6b91-11eb-8704-07e27745c826.png)

thanks...",,,junlincc,"
--
@Beknazar19 thanks for reporting, could you give clear steps to reproduce? 
--
",Beknazar19,"
--
I created filters in 0.38 as usual, everything worked.
Then I imported the dashboard to 1.0.0.
Diagrams were imported correctly, but filters with errors.
--

--
an error occurs after importing the dashboard
--

--
We use one host server, which have two superset release 0.38 with port 8080
and 1.0.0 with port 8088.
May be that's why error occurred?

, 10 . 2021 ., 16:13 Ville Brofeldt <notifications@github.com>:

> Are you getting errors when importing dashboards, or is this happening
> after you upgraded your superset installation from 0.38 to 1.0.0?
>
> 
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/apache/superset/issues/13043#issuecomment-776599545>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AK2K4D4SA2HF3LCJGNTJTVTS6JL23ANCNFSM4XMJDRLA>
> .
>

--

--
> Can you provide details on what the exact ""Oracle Error"" is (either by clicking on ""See more"" or in server logs)?

it shows: 
ORA-00936: missing expression
This may be triggered by:
Issue 1002 - The database returned an unexpected error. 

I think the error is not related to the import of the dashboard.
When I create a new filter in version 1.0.0, an error appears.
--
",villebro,"
--
Are you getting errors when importing dashboards, or is this happening after you upgraded your superset installation from 0.38 to 1.0.0?
--

--
@Beknazar19 a little more context would be needed to help debug this. Can you provide details on what the exact ""Oracle Error"" is (either by clicking on ""See more"" or in server logs)? Also, exporting and importing across different versions is not guaranteed to work; it may work, but in many cases it won't. To be able to import dashboards into your 1.0.0 version of Superset, you need to make sure the export has been generated using a Superset instance on the same version. 
--
",,,,,,
13040,OPEN,[Explore] Time grain should match X-axis,bash!; enhancement:committed; viz:explore:ux,2021-02-10 06:36:12 +0000 UTC,zuzana-vej,Opened,,"**Is your feature request related to a problem? Please describe.**
Currently when user selects monthly or weekly time grain in Explore, the X axis on the chart still displays all the days (if there is space for it - depending on time period overall selected). There is no way to change this currently in the Customize tab.

**Describe the solution you'd like**
If monthly time grain is selected, the X axis on the chart should just display months (January, February etc.) based on what formatting user selects in the Customize tab.
If weekly time grain is selected, the X axis on the chart should just display weeks. (e.g. date of the first day (Monday) in each week)

**Describe alternatives you've considered**
other option is to let user configure this in the ""customize tab"" 

**Additional context**
See two examples of the problem. 

My time grain is weekly, but I see all the days bellow, even though chart values are populated weekly.

![Screen Shot 2021-02-09 at 12 14 42 PM](https://user-images.githubusercontent.com/61221714/107423171-e3a64a80-6ad0-11eb-8079-98a71bf2f989.png)

My time grain is monthly but I see again all the days on X axis.
![Screen Shot 2021-02-09 at 12 14 55 PM](https://user-images.githubusercontent.com/61221714/107423216-ed2fb280-6ad0-11eb-83cc-0a226c095d5d.png)
",,,junlincc,"
--
@zuzana-vej 
thanks for the request. Yes, x-axis is not picking up the time grain consistently in all the time series chart, not deal breaker but  it could get pretty annoying. 
Our plan is to implement this enhancement in Echarts only, along with advanced analytics(rolling window etc.) to reach full feature parity and deprecate NVD3 time series charts. We will add to roadmap when it gets closer. 

cc: @villebro @maloun96 let's aim for mid Q2 
--
",,,,,,,,,,
13036,OPEN,Wrap text results output for long textual outputs in SQL results tab,sql_lab:editor,2021-02-09 19:44:49 +0000 UTC,ashishtadose,In progress,,"Current any long textual output is displayed as a single line (with a scroll bar).

This behavior makes it cumbersome to read text outputs for frequently used SQL queries such as SHOW CREATE TABLE or EXPLAIN PLAN which contains text output. 
  
<img width=""1599"" alt=""Screenshot 2021-02-10 at 12 25 43 AM"" src=""https://user-images.githubusercontent.com/1897846/107414101-8e515400-6b37-11eb-9295-5e270eb8e977.png"">
",,,ashishtadose,"
--
Is there any config to enforce text wrapping for superset text result output?

Thanks in advance. 
--
",,,,,,,,,,
13030,OPEN,i18n: translation of dashboards and charts,i18n:general,2021-02-09 22:00:46 +0000 UTC,thelightbird,In progress,,"I am currently investigating the use of Apache SuperSet at work.

**Is your feature request related to a problem? Please describe.**
An important feature for us is to be able to translate dashboards and charts in different languages.

**Describe the solution you'd like**
I've seen that SuperSet has an internationalization feature, but it seems that it is currently only used to translate the dashboard UI, not the custom text that users type to create new dashboards or charts.

:arrow_right: Are you planning to add this feature?
:arrow_right: Do you have any recommendations on how to implement such a feature? ",,,junlincc,"
--
<img width=""762"" alt=""Screen Shot 2021-02-09 at 11 59 22 AM"" src=""https://user-images.githubusercontent.com/67837651/107421016-595ce700-6ace-11eb-9c12-96517ddb3696.png"">
@thelightbird custom text works for me, or maybe I didn't understand your request correctly? 
--

--
got it! unfortunately this is not in our roadmap in the near future. feel free to provide a SIP(Superset improvement proposal) and detailed plan to push this forward though!  @thelightbird 
--
",thelightbird,"
--
I meant to say: for a given dashboard or chart, we would like to be able to specify text translations for each language without having to duplicate the whole dashboard or chart. For example: create the chart in the first language, then save it, change the language, and then edit the same chart and it will edit the translations of that selected language without losing previous language translations.

The goal is to have multiple translations (one per language) to avoid the duplication of the dashboard or chart components.
This way, for the same chart, changing the current language would change the text of the title, axis labels, etc.
--
",,,,,,,,
13025,OPEN,Pe-commit hooks using deprecated code,question,2021-03-31 06:41:05 +0000 UTC,TColl,Opened,,"Since `isort` v5.0, `seed-isort` is no longer needed and the source code [has been archived](https://github.com/asottile-archive/seed-isort-config). 

By bumping the pre-commit hooks to newer versions we can remove the seed-isort hook entirely and stay with code that is actively supported.

### Expected results

Pre-commit hooks should be bumped to newer versions to avoid relying on unmaintained code.

### Actual results

Pre-commit still using `seed-isort` and `isort v4.3.2.1`

#### Screenshots

NA

#### How to reproduce the bug

N/A

### Environment

(please complete the following information):

- superset version: `v1.0`
- python version: `3.8.5`
- node.js version: `8.15.1`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

N/A
",,,,,,,,,,,,,,
13023,OPEN,Chart name looks as its id number when trying to edit an alert or report,global:report,2021-02-09 20:29:38 +0000 UTC,iercan,Opened,,"When I try to edit an alert or report I realized selected chart name seems as id number. 

### Expected results
I should see selected chart name. 
![image](https://user-images.githubusercontent.com/3406152/107335208-b3f34480-6ac8-11eb-982f-7b177070cea0.png)

### Actual results
Chart name looks as chart id when trying to edit. 
![image](https://user-images.githubusercontent.com/3406152/107335148-a0e07480-6ac8-11eb-8b22-aa122477b6ca.png)


#### How to reproduce the bug

1. Create an alert/report. Choose a chart for message content
2. Save it and refresh page. 
3. Click edit to new saved alert/report

### Environment

- superset version: 1.0.1
- python version: 3.7
- mysql: 5.32.7
### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,,,,,,,,,,,,
13018,OPEN,[SQL Lab] Restyle Existing Display Limit Warning,sql_lab,2021-02-09 01:13:19 +0000 UTC,yousoph,Opened,,"## Screenshot
![Clipboard_2021-25-01_at_11 13 23_PM](https://user-images.githubusercontent.com/10627051/107300020-860cf200-6a2d-11eb-81fd-2a8f2dbdd09b.png)

## Description
How to Repro: 
* Set DISPLAY_MAX_ROW to a low number 
* Run a query with the LIMIT dropdown set to a number higher than DISPLAY_MAX_ROW 
The orange LIMIT indicator that appears wasn't styled with the new toolbar and appears twice. 

Remove the current indicator and add the ! indicator and message based on mocks. 
Admin view: https://www.figma.com/file/5db2MpaKNnC1c7MK9ysdeV/SQL-Limit-Indicator?node-id=90%3A210 
Non-admin view: https://www.figma.com/file/5db2MpaKNnC1c7MK9ysdeV/SQL-Limit-Indicator?node-id=90%3A352",,,,,,,,,,,,,,
13013,OPEN,[SIP-58] Proposal to move superset-ui to this repo,sip,2021-03-03 13:17:43 +0000 UTC,MatanBobi,In progress,,"## [SIP] Proposal to move superset-ui to this repo

### Motivation

At the moment, working with `superset-ui` in order to develop plugins or packages to superset comes with a very bad developer experience. 
In case you want to edit or add a plugin, you need to manually link it using `npm link`, this causes a higher barrier for new developers to the project. Working on multiple packages in `superset-ui` will need a developer to manually link all the packages.
It's important to note that not all packages and plugins will move back to this repo so we'll still need to use `npm link` from time to time.
Using `lerna` in our new monorepo will also ensure that a `superset-frontend` version will also have the latest versions of all the plugins and packages.
This will also let us align the tech stack between `superset-frontend` and the packages and plugins used in it.

### Proposed Change

The whole `superset-ui` repo will be transferred to `superset-frontend` and `superset-ui`'s repo will be archived.
The stack will be the same as the one used in `superset-ui`, using `lerna` and `yarn workspaces`  to manage the monorepo.
The release flow will also be from this repo using github actions as `superset-ui` works.
`superest-frontend` will become a package within the new monorepo with `""private"": true` flag, this means that no package will be deployed to npm for that package and the docker image build process will remain as is.
The whole monorepo will have one storybook, it will include all the plugins. 
Testing the plugins and packages will remain as it is in `superset-ui`, using jest. Support for RTL will be added (since it's used in `superset-frontend` and not in `superset-ui`.

Regarding storybook, we can take two approaches:
- We can keep the `superset-ui-demo` package where we define all the stories.
- We can create a main storybook folder and write each story in it's own plugin. The main storybook will collect all stories from every plugin/package.
We should rethink later on if we want to rename the scope from `@superset-ui` to `@superset` or keep it that way.

New folders structure (WIP):
![image](https://user-images.githubusercontent.com/12711091/107249688-426db480-6a3c-11eb-8b76-ea4ae7da5fea.png)

### New or Changed Public Interfaces

None. We will use the same package names.

### New dependencies

`lerna` - To manage the monorepo.
This is already being used in `superset-ui` so I believe licensing isn't an issue here.

### Migration Plan and Compatibility

No database migrations.

### Rejected Alternatives

The current solution where these packages that are tightly related sit under different repositories create a bad developer experience.

### Open issues
At the moment, `superset-ui`'s lerna configuration sets the same version for all packages, this means that the `superset-frontend` version will be aligned with the plugins and packages versions.
There's also the option to set the versioning as independent but we need to define what's the behavior we'd like to see here :)

### Todo List:
- [ ] Move the relevant plugins and packages from superset-ui (the ones that doesn't have licensing issues)
- [ ] Align all tooling between core ui and plugins/packages (ts, babel, jest configs etc.)
- [ ] Verify build and publish process works with commands
- [ ] Integrate build and publish process to github actions
- [ ] Storybook locally should work
- [ ] All workflows (Chromatic, storybook) preview deployments should work.

CC: @amitmiran137 @ktmud @rusackas 
",,,ktmud,"
--
Adding some historical context to make sure we made a conscious decision after considering all pros and cons:

1. The initial discussion on moving packages out of the main repo: https://github.com/apache/superset/issues/5667#issuecomment-415511861
2. My question about the developer experience on working across multiple repos: https://github.com/apache/superset/pull/8638#issuecomment-587734238
3. A couple of historical PRs to address the pain raised from `npm link` and diverging build scripts, etc: #8638 #9326
4. When we merge `superset-ui-plugins` to `superset-ui`: https://github.com/apache-superset/superset-ui/pull/333
5. When we merge all core superset-ui packages: https://github.com/apache-superset/superset-ui/pull/768

Note in the original proposal to move superset-ui to npm packages, we did consider keeping packages under one monorepo, but [the decision at the time](https://github.com/apache/superset/issues/5667#issuecomment-423614852) was that pros outweighs cons with separate repos.

A question to be answered in this SIP: **What has changed?** The biggest argument I can see is that after migrating to separate repos, and starting to use TypeScript, we realized a couple of new facts:

1. Maintaining diverging build configs between repos is not easy.
2. `npm link` is often problematic, especially with TypeScript.
3. The two PRs + waiting for npm release workflow severely hinders developer velocity and it turned out to be larger problem than we thought it to be as number of Superset contributors grow.
4. There may be other solutions to the cons or the cons were not that important than we original thought.

Ultimately, moving to either direction (monorepo vs separate rpos) is a big tradeoff. By moving to monorepo, we will immediately lose these current benefits (unless more work is done on making sure relevant tools are properly configured ):

1. PR preview Storybook via Zeit and Chromatic UI (it's more difficult to request new Github Apps under the `apache` org, which hosts all other Apache projects, than in `apache-superset`, which most Superset PMC members have direct admin access).
2. Reduced build time from smaller codebase and not having to share the GitHub Action runner pool with other Apache projects (Can be mitigated by conditionally triggered workflows)
3. Automatic NPM release (need to configure NPM_SECRET and the required Workflow)
4. 100% test coverage guarantee for core packages (need to update codecov config)


--

--
> I believe the issue we have now is the opposite one.. superset-ui doesn't have tests almost at all. The coverage configs are all on 0.

This is not true. `superset-ui` core packages (`@superset-ui/core` and `@superset-ui/chart-controls`) do enforce [100% test coverage](https://github.com/apache-superset/superset-ui/blob/a4ecdb3d5a7791e864703318d6da9154bb842762/codecov.yml#L8) at least on non TSX/JSX files (the goal for TSX/JSX  is 50%).
--
",kristw,"
--
When we moved to the separated repos, the goal was also to raise code quality bar, increase development velocity, and produce components that can be natively embedded in other applications (therefore the `npm` packages).
However, as @ktmud mentioned, it has been 2 years since then and many things were changed.

----

**History-1:** The quality of the code in `apache/(incubator-)superset` was not great. Some parts were well tested. Some not. Lints were enabled only for certain parts. `superset-ui` was created to be stricter: requiring 100% test coverage and TypeScript. We rewrote many core parts including massive clean up of all charts, then replaced legacy code in the main repo with these newer components. 

**Current:**   Main repo has more TypeScript + lints and overall code quality has improved.

----

**History-2:** CI jobs in `apache/(incubator-)superset` took long time to run and was not stable (flaky tests and `master` often had issues at that time). 

**Current:**   CI and `master` are more stable these days.  The jobs still take time to run. Probably can mitigate by skipping the unnecessary ones. 

----

**History-3:** Better tooling and flexibility. We experimented and adopted bots, conventional commits, PR previews (Netlify then later Zeit/Vercel), semi-automatic `npm` packages release via github actions. These are difficult to experiment and configure on repos under `apache` org which we don't have admin rights.

**Current:**   We still don't have admin rights on `apache/superset` and some apps are not allowed.

----

**History-4:** There were needs to embed charts in other apps. Plugins system and publishing the packages on `npm` was for this.
**Current:**   Embeddable chart is less of a focus now. There are some internal apps using the core package to call Superset APIs, but I believe they are fairly stable.

----

**History-5:** Some legacy chart plugins were added by community and caused maintenance problems for committers over time. The plugins system and publishing the packages on `npm` hoped to decentralize this a bit. Each org/person can have repo of their custom plugins with repo structure forked from `superset-ui-plugins`. 

**Current:**   Can still have boilerplate of standalone plugin repo for 3rd party who wants to create plugin, but it will still have issues: keep build config in sync and `npm link`. If the main developers are no longer dogfooding this approach, it will likely be broken after a while. 

Or we could change the process for creating custom plugins to (1) fork `apache/superset` (2) dev on that and (3) only publish the flagged plugins. However, the forked repo can be heavy and need to keep all parts to be able to `git rebase` from upstream.

-----

Overall, it seems things have changed in favorable direction. 

I agree with the observations (@ktmud 's 1-4 above) that the workflow has caused new issues with more contributors joining the project. The key questions to be addressed before merging back are also good and I want to add a bit more on top of that.

| Topic | Things to consider before proceeding |
| ----- | ----- |
| **Repo tools:** PR preview Storybook via Zeit and Chromatic UI (it's more difficult to request new Github Apps under the apache org, which hosts all other Apache projects, than in apache-superset, which most Superset PMC members have direct admin access). | Do some feasibility study. Check with Apache admin to know what are allowed. Are there workaround and are we OK losing some if not allowed? Better avoid surprises. |
| **Reduced build time** from smaller codebase and not having to share the GitHub Action runner pool with other Apache projects | Verify that conditionally triggered workflows can help and make this task a requirement before the repo merge, not after. Make sure we won't save time from `npm link` (fix cost) only to spend more time staring at CI instead (recurring). |
| **Revise automatic NPM release** | Should be able to set `NPM_SECRET` via JIRA to Apache admins. Have to reconsider publishing flow. Currently, releases are triggered by committers committing a PR that bumps version in `package.json` and CI took care of the rest. Direct `git push` to `master` is not allowed in `apache/superset`. |
| **Figure out `npm` packages release cadence**: How often will the plugins be published? | Currently it is being forced because the main app refer to plugins via package and version in `package.json`. (1) If the `npm` publishing is still a requirement, you still need 2-3 PRs in `apache/superset`. One PR up to the point where plugin is completed. Potentially another PR to bump version and force `npm` publishing. Another PR is to update package version in `superset-frontend` after new packages have been published. (2) The 2-3 PRs process sounds tedious. After moving to the same repo, one might be interested in pointing to plugin `src` directly and bypass the npm publishing at all. This could leave the `npm` packages outdated and someone has to publish at one point, when will that be? This is not a problem for the main app, but downstream apps or 3rd party chart plugin dev may be using outdated `@superset-ui/xxx`.   |
| **Maintain quality bar** e.g. 100% test coverage guarantee for core packages | Sounds straightforward (update codecov config) |
| **3rd party chart plugins**: How would someone develop chart plugins that are only useful for his/her org (without dropping code in `apache/superset`)? | Worth spend sometimes thinking how to provide flexibility while avoid accumulating maintenance cost for `apache/superset`. Please refer to the comment above about History-5. |

To sum up, thank you for writing the SIP and thinking more about this problem. I think this could work and would welcome overall improvement to developer experience. Just want to make sure important issues are brought to attention and some thoughts are put into them before voting and executing.
--
",rusackas,"
--
A few additional points building on all the helpful background/issues from @ktmud and @kristw:

* SIP-57, in favor of Semantic Versioning, is still in the VOTE process. This should be considered here. The current NPM publishing scripts bump ALL touched packages in parallel. We may need to pay more attention to semantically bumping the version numbers of _individual_ plugins, or establishing some process to make sure that ALL plugins are bumped to the highest/safest level (i.e. a breaking change on one package means they all get a major number bump).
* I really prefer the idea of using the `src` path for packages, rather than having to jump through the hoops of publishing/bumping packages internally in Superset. Removal of this overhead is the main motivation for the homecoming, in my opinion. We should indeed address the cadence of when we publish new plugin versions. My off-the-cuff suggestion would be to make it part of the Superset OSS release procedure, so they're in tighter lock-step. We _could_ even consider aligning version numbers between Superset and the internal packages.
* Regarding the viz plugins, we would still need to keep some of them external, on `superset-ui`. One motivation to externalize them was around the licensing of dependencies being incompatible with Apache. I believe this is likely to remain the case, so only the ""safe"" ones can come home. We'll need to support `npm link`, Storybook, release actions, etc on `superset-ui` to support those remaining packages.
--
",villebro,"
--
I think it would be nice if the `superset-ui` npm packages would be synced with the official release versions; when a new Superset version is released, the all `superset-ui` packages would be released with the same version on npm. If we stay true to semantic versioning (the SIP has now passed), people would know that if their external applications is using e.g. v. 1.1.0 of `superset-ui`, it will be compatible with a backend running 1.5.0.
--

--
Thanks so much for the comprehensive post and keeping this SIP alive!

> > I think it would be nice if the `superset-ui` npm packages would be synced with the official release versions; when a new Superset version is released, the all `superset-ui` packages would be released with the same version on npm. If we stay true to semantic versioning (the SIP has now passed), people would know that if their external applications is using e.g. v. 1.1.0 of `superset-ui`, it will be compatible with a backend running 1.5.0.
> 
> @villebro I do agree that it can be nice, but I'm not sure it is really helpful. This means that whenever you want to release a patch to a plugin you'll also bump all the packages and the superset-ui core module, sounds redundant to me but I'm not familiar with the whole release mechanism so I might be missing some stuff.

One consequence of moving `superset-ui` to the `apache/superset` repo is we're going to be effectively changing the ownership of the code to the Apache Software Foundation (ASF). That in itself isn't a problem, but the ASF is very strict about the protocol surrounding releases and publishing code from an ASF repo. In practice we won't be able to do non-ASF sanctioned releases of the `superset-ui` packages without going through the full ASF voting process, which requires 72 h voting time and sign-off from a minimum of 3 PMC members (there's some lenience wrt to `latest` builds from `master`). In practice this means that anything we release to npm will need to go through the normal voting process. There's no problem with doing different product releases under the Superset umbrella, even following different version numbers (=we could do `superset-ui` releases separately from `apache/superset`), but they'll still need to go through the regular voting flow before being published.
--
",MatanBobi,"
--
> 
> * SIP-57, in favor of Semantic Versioning, is still in the VOTE process. This should be considered here. The current NPM publishing scripts bump ALL touched packages in parallel. We may need to pay more attention to semantically bumping the version numbers of _individual_ plugins, or establishing some process to make sure that ALL plugins are bumped to the highest/safest level (i.e. a breaking change on one package means they all get a major number bump).
> * I really prefer the idea of using the `src` path for packages, rather than having to jump through the hoops of publishing/bumping packages internally in Superset. Removal of this overhead is the main motivation for the homecoming, in my opinion. We should indeed address the cadence of when we publish new plugin versions. My off-the-cuff suggestion would be to make it part of the Superset OSS release procedure, so they're in tighter lock-step. We _could_ even consider aligning version numbers between Superset and the internal packages.
> * Regarding the viz plugins, we would still need to keep some of them external, on `superset-ui`. One motivation to externalize them was around the licensing of dependencies being incompatible with Apache. I believe this is likely to remain the case, so only the ""safe"" ones can come home. We'll need to support `npm link`, Storybook, release actions, etc on `superset-ui` to support those remaining packages.

Thanks everyone for all the helpful inputs!

@rusackas 
1. IMO we should use lerna independent versioning, I don't think that a breaking change in one plugin should cause a major version bump in another version.
2. If we decide to align the version numbers we'll get a problem with semantic versioning and major version bumps you raised on the first point wouldn't we?
3. That's a good point. Is there a list of the plugins/packages we can move based on the license? Since I couldn't find any licensing on the plugins, just on the whole `superset-ui` repo.

> I think it would be nice if the `superset-ui` npm packages would be synced with the official release versions; when a new Superset version is released, the all `superset-ui` packages would be released with the same version on npm. If we stay true to semantic versioning (the SIP has now passed), people would know that if their external applications is using e.g. v. 1.1.0 of `superset-ui`, it will be compatible with a backend running 1.5.0.

@villebro I do agree that it can be nice, but I'm not sure it is really helpful. This means that whenever you want to release a patch to a plugin you'll also bump all the packages and the superset-ui core module, sounds redundant to me but I'm not familiar with the whole release mechanism so I might be missing some stuff.


@kristw thanks for the helpful inputs!

> **History-1:** The quality of the code in `apache/(incubator-)superset` was not great. Some parts were well tested. Some not. Lints were enabled only for certain parts. `superset-ui` was created to be stricter: requiring 100% test coverage and TypeScript. We rewrote many core parts including massive clean up of all charts, then replaced legacy code in the main repo with these newer components.
> 
> **Current:**  Main repo has more TypeScript + lints and overall code quality has improved.


I believe the issue we have now is the opposite one.. `superset-ui` doesn't have tests almost at all. The coverage configs are all on 0.

> History-3: Better tooling and flexibility. We experimented and adopted bots, conventional commits, PR previews (Netlify then later Zeit/Vercel), semi-automatic npm packages release via github actions. These are difficult to experiment and configure on repos under apache org which we don't have admin rights.
> 
>  Current:  We still don't have admin rights on apache/superset and some apps are not allowed.

That's an important one, the question is, is that a blocker?

> History-5: Some legacy chart plugins were added by community and caused maintenance problems for committers over time. The plugins system and publishing the packages on npm hoped to decentralize this a bit. Each org/person can have repo of their custom plugins with repo structure forked from superset-ui-plugins.
> 
>  Current:  Can still have boilerplate of standalone plugin repo for 3rd party who wants to create plugin, but it will still have issues: keep build config in sync and npm link. If the main developers are no longer dogfooding this approach, it will likely be broken after a while.
> 

I think we should still maintain that approach, having developers create PR's on the main repo might be a serious bottleneck.
As long as there are still plugins left outside on `superset-ui` due to licensing I guess we'll keep maintaining it, but even after, we need to maintain a doc page explaining how to do it and also not to break that mechanism.

Regrading the open issues:
- Repo tools: PR preview Storybook via Zeit and Chromatic UI - Who can check with Apache admins what are we allowed to do?
- Reduced build time from smaller codebase and not having to share the GitHub Action runner pool with other Apache projects - I'll work on this one as part of this SIP.
- Revise automatic NPM release - We need to talk about this one since IMO we need to define a way to publish plugins which is unrelated to the main repo. Also, since the FE will be a part of the lerna monorepo, we should know that a new version will be released once it has changes (though we might not rebuild the docker image of-course).
- Figure out npm packages release cadence: How often will the plugins be published? - IMO this should be automatic even if we decide to use the `src` folder instead of the npm package. Every change will be released using lerna and github actions.
- Maintain quality bar e.g. 100% test coverage guarantee for core packages - As I stated before, the current status of `superset-ui` isn't as good as we thought it is..
- 3rd party chart plugins: How would someone develop chart plugins that are only useful for his/her org (without dropping code in apache/superset)? - This doesn't sound like something which should be coupled to this SIP but I do agree that we need to think about it. At the moment, one already needs to edit code in superset to add plugins. We might need to figure out a way to extend the plugins without editing any code.

Thanks everyone for the valuable inputs! I'm starting to work on this one but I think we should setup some milestones we want to see. I'll try to update the PR and SIP with specific milestones.
--

--
> > I believe the issue we have now is the opposite one.. superset-ui doesn't have tests almost at all. The coverage configs are all on 0.
> 
> This is not true. `superset-ui` core packages (`@superset-ui/core` and `@superset-ui/chart-controls`) do enforce [100% test coverage](https://github.com/apache-superset/superset-ui/blob/a4ecdb3d5a7791e864703318d6da9154bb842762/codecov.yml#L8) at least on non TSX/JSX files (the goal for TSX/JSX is 50%).

That's my bad, sorry. I was looking at [this](https://github.com/apache-superset/superset-ui/blob/a4ecdb3d5a7791e864703318d6da9154bb842762/jest.config.js#L15) inside the jest configs.
--
",,
13012,OPEN,[explore] add chart to a selected tab on save to dashboard,enhancement:request; needs:design-input; viz:explore:savemodal,2021-02-11 21:27:33 +0000 UTC,junlincc,Opened,,"""It would be great if I can choose which tab to add a chart to dashboard to rather than dropped in the first page.""
 And nice-to-have choose add to top or add to bottom.

<img width=""601"" alt=""Screen Shot 2021-02-08 at 11 06 21 AM"" src=""https://user-images.githubusercontent.com/67837651/107269307-45e24b00-69fe-11eb-95a4-1eaec53445c3.png"">

",,,srinify,"
--
Love this and strongly agree! Having worked on at least 10, multi-tab dashboards .. being forced to hunt down the tab the chart dropped into and drag it into the tab I actually care about is an annoying pattern!
--
",mihir174,"
--
Yup I think this is a great idea. I also figured if we do the bonus of adding to the bottom or top of a tab, we could provide the same option for adding to a dashboard in general

<img width=""1408"" alt=""Slice 2 (2)"" src=""https://user-images.githubusercontent.com/64227069/107700821-e3db4d00-6c6c-11eb-9c9e-4a30ada66adb.png"">

--
",,,,,,,,
13009,OPEN,Incorrect SELECT * statement previewing tables with timestamp partitions,#bug; sql_lab,2021-02-11 19:31:31 +0000 UTC,kekwan,In progress,,"When previewing a table in SQL Lab, a SELECT * statement is generated based on table data and table metadata if it contains partitions. When generating the SELECT * statement on a table with partitions, the table data, 'TIMESTAMP' in my case is not casted in the WHERE block. 

### Expected results

Previewing tables with data partitions should work. The WHERE block of the generated SELECT * statement should talk into account casting of data types, particularly TIMESTAMP.

### Actual results

`select_star` method calls `where_latest_partition` to generate WHERE block when partition exists. `where_latest_partition` is failing to CAST data types needed for WHERE block causing syntax error when it runs the select * statement.

SQL Statement generated by `get_table_metadata` calling `select_star`. Columns `datetimepartition` and `etl_ts` should be casted to TIMESTAMP, not VARCHAR.

```
SELECT ""event_type"" AS ""event_type"",
       ""trace_id"" AS ""trace_id"",
       ""tenant"" AS ""tenant"",
       ""core_tenant"" AS ""core_tenant"",
       ""event_timestamp"" AS ""event_timestamp"",
       ""table_id"" AS ""table_id"",
       ""dtr_workspace_id"" AS ""dtr_workspace_id"",
       ""connectors"" AS ""connectors"",
       ""table_type"" AS ""table_type"",
       ""row_count"" AS ""row_count"",
       ""field_count"" AS ""field_count"",
       ""partition_count"" AS ""partition_count"",
       ""data_size_bytes"" AS ""data_size_bytes"",
       ""datetimepartition"" AS ""datetimepartition"",
       ""etl_ts"" AS ""etl_ts"",
       ""insert_try_number"" AS ""insert_try_number""
FROM ""events"".""data_tables""
WHERE ""datetimepartition"" = '2021-02-08 16:00:00.000'
  AND ""etl_ts"" = '2021-02-08 16:30:00.000'
  AND ""insert_try_number"" = 1
LIMIT 100
```
Syntax error from Presto due to the incorrect SQL statement
`Presto error: Cannot apply operator: timestamp(3) = varchar(23)`

#### Screenshots
![image](https://user-images.githubusercontent.com/19199254/107262094-fc413280-69f4-11eb-8816-af2de056338e.png)


#### How to reproduce the bug

1. Go to SQL Lab
2. Preview Trino/Presto table with partitions and TIMESTAMP as a partition column.

### Environment


- superset version: `0.37.2`
- python version: `python 3.6`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,tritonrc,"
--
Hi, I ran into this same issue yesterday and after digging around the code I landed on a fix.  In my case, the fields were coming back marked as `Time` type.  Looking at https://github.com/apache/superset/blob/master/superset/db_engine_specs/presto.py#L576

I insert two lines to cover the `Time` type at line 579:

```python
if tt == utils.TemporalType.TIME:
            return f""""""from_iso8601_timestamp('{dttm.isoformat(timespec=""microseconds"")}')""""""  # pylint: disable=line-too-long
``` 

As part of digging into this, I ran across the recently landed PR https://github.com/apache/superset/pull/12861 which might help here as well.
--
",kekwan,"
--
Hey @tritonrc. I also fixed the issue but had a slightly different fix for my scenario.

Issue 1 was our Presto `TIMESTAMP` columns were getting marked as `OTHER` (not `TIME` either) due to Trino/Presto's new timestamp precision type. We followed this https://github.com/apache/superset/issues/11138#issuecomment-702475356, to omit timestamp precision and it correctly identified our `TIMESTAMP` columns.

Even this didn't fix our issue with previewing tables however. I believe Presto/Trino doesn't do automatic casting in `WHERE` statements so when the SELECT * is generated for preview tables, it needs to explicitly cast the partition value to 
`TIMESTAMP`. This is a valid query in Presto (notice the explicit casting of the partition value)

```
SELECT ""event_type"" AS ""event_type"",
       ""trace_id"" AS ""trace_id"",
       ""tenant"" AS ""tenant"",
       ""core_tenant"" AS ""core_tenant"",
       ""event_timestamp"" AS ""event_timestamp"",
       ""table_id"" AS ""table_id"",
       ""dtr_workspace_id"" AS ""dtr_workspace_id"",
       ""connectors"" AS ""connectors"",
       ""table_type"" AS ""table_type"",
       ""row_count"" AS ""row_count"",
       ""field_count"" AS ""field_count"",
       ""partition_count"" AS ""partition_count"",
       ""data_size_bytes"" AS ""data_size_bytes"",
       ""datetimepartition"" AS ""datetimepartition"",
       ""etl_ts"" AS ""etl_ts"",
       ""insert_try_number"" AS ""insert_try_number""
FROM ""events"".""data_tables""
WHERE ""datetimepartition"" = TIMESTAMP '2021-02-08 16:00:00.000'
  AND ""etl_ts"" = TIMESTAMP '2021-02-08 16:30:00.000'
  AND ""insert_try_number"" = 1
LIMIT 100
```

In order to do this, I implemented a TypeDecorator class which overrode existing functionality of the SQLA timestamp type and instantiated that class here when the column type is timestamp https://github.com/apache/superset/blob/master/superset/db_engine_specs/presto.py#L917
--

--
@tooptoop4 Yup. I filed a PR https://github.com/apache/superset/pull/13082
--
",tooptoop4,"
--
@kekwan can u share the code for the fix?
--
",,,,,,
13003,OPEN,Big Query Impersonate User to have Access to DataSource,#bug,2021-02-08 16:19:44 +0000 UTC,codegeekjk,Opened,,"Hello All,

We have big query as one of the datasource in superset, currently access to dataset is based on the google-group.
From superset we want to impersonate user so only the authorised users should be able to access the datasets.

Currently the user impersonate is available for  Hive and Presto, how can I do that for BigQuery ?
Anywork arounds avaliable so we can get this working ? ",,,,,,,,,,,,,,
13001,OPEN,[SQL Lab] Update button labels,sql_lab:control:ui,2021-02-08 15:38:15 +0000 UTC,Steejay,Opened,,"## Screenshot

<img width=""1364"" alt=""Screen Shot 2021-02-08 at 6 46 07 AM"" src=""https://user-images.githubusercontent.com/60786102/107235242-53d1a500-69d9-11eb-85ae-15862c2e2a71.png"">


## Description
Update SQL Lab result button labels to be more clear and predictable. Currently, the ""CSV"" and ""Clipboard"" button labels do not indicate the action. Instead, use a verb+noun format. 

Change ""CSV"" to ""DOWNLOAD CSV""
Change ""CLIPBOARD"" to ""COPY TO CLIPBOARD""


cc @yousoph @mihir174 
",,,,,,,,,,,,,,
12988,OPEN,[Explore]ability to search by column in datatable,enhancement:request; viz:explore:viewdata,2021-02-09 01:12:50 +0000 UTC,junlincc,In progress,,"I want to look for a specific column and its sample data in a large dataset with 300+ columns. I can only search by the row values but not by column name, which is a major limitation of the search functionality...

<img width=""1764"" alt=""Screen Shot 2021-02-06 at 6 01 47 PM"" src=""https://user-images.githubusercontent.com/67837651/107134371-5d0e2500-68a6-11eb-9868-1cca4ec64506.png"">
",,,kgabryje,"
--
What behaviour do we expect here? When a user types a search query, should the columns that don't match disappear? How do we want to tell if user wants to filter rows or columns? 
--
",junlincc,"
--
Expected behavior - only showing the matched columns in a table. 

but this is a low-priority.  [[Explore]user can not type filter values when above 10k option limit](https://github.com/apache/superset/issues/13017)  is higher @kgabryje 
--
",,,,,,,,
12987,OPEN,Trouble adding Argentina geojson to country map legacy plugin,need:more-info; viz:chart-maplayer,2021-02-24 10:33:05 +0000 UTC,aguspina,In progress,,"I connected superset with a local modify version of the superset-ui plugin legacy-country map but I got this viz:

<img width=""783"" alt=""Screen Shot 2021-02-06 at 16 34 26"" src=""https://user-images.githubusercontent.com/4105740/107128049-6cd33c80-6899-11eb-8d2e-eda969ef041f.png"">

I'm the the right geojson generated from the file downloaded from https://www.diva-gis.org/gdata with de ISO column updated to follow de ISO 3166-2 standard.

If someone help me solve this, I will make the corresponding PR!

",,,aguspina,"
--
Here is the geojson file https://gist.github.com/aguspina/570fe8c52bb9628f38618ad9b037f4e7
--

--
Of course :)

I added this line to the countries.js

```python
import argentina from 'file-loader!./countries/argentina.geojson';
```

The argentina.geojson file into the countries folder and added argentina to the choices in the controlPanel.ts file
```typescript
label: t('Country Name'),
              default: 'France',
              choices: [
                'Argentina',
```

And I had to change the package json in superset-frontend to ponint to my local fork of superset-ui plugins
--

--
Hi @djok thanks for your help. I think the geojson you shared to me is malformed, it is duplicated and truncated.
--
",junlincc,"
--
@aguspina can you provide detailed steps to reproduce this issue? thanks! 
--

--
@aguspina thanks for the details.  @mayurnewase do you mind taking a look this issue? 
--
",mayurnewase,"
--
I think the file is in a different format, I found this pr fixes Bulgaria map, which suffers from a similar issue.
--
",djok,"
--
Please test this geoJson https://gist.github.com/djok/b53cbbb4f76ecaa8499fdafbc1cac040

I used mapshaper.org to simplify and fix overlapping
--
",bdiouih,"
--
> Please test this geoJson https://gist.github.com/djok/b53cbbb4f76ecaa8499fdafbc1cac040
> 
> I used mapshaper.org to simplify and fix overlapping

I have exactly the similar issue with Afghanistan geojson file.
How can this be solved exactly  :  how can it be simplified and overlapping fixed?
--
",,
12979,OPEN,[SQL Lab] Split run button doesn't transition smoothly between disabled and enabled states,sql_lab:editor,2021-02-06 23:20:57 +0000 UTC,yousoph,In progress,,"## Screenshot
https://user-images.githubusercontent.com/10627051/107099586-ebab7500-67c6-11eb-94a0-dd87f325fcc8.mp4

## Description
When transitioning between a disabled and enabled state, the right side of the split run button in SQL Lab changes color/state a bit later than the left side. 

How to repro: 
* In SQL Lab, select a db where you have CTAS and/or CVAS enabled 
* Delete all the text in your SQL Lab tab to see the disabled state. Start typing to see the enabled state ",,,yousoph,"
--
@AAfghahi - I noticed this with the new disabled state that you added 
--
",AAfghahi,"
--
oh yeah this looks bad, I will look through and work on it. Maybe I can just set a timeout or something. 
--
",,,,,,,,
12971,OPEN,Add support for deleting columns/metrics in the Dataset API,enhancement:request,2021-02-06 06:48:25 +0000 UTC,etr2460,Opened,,"**Is your feature request related to a problem? Please describe.**
As titled, the new API doesn't support this yet. This is needed because the old api for creating/updating/deleting columns and metrics isn't very performant.

**Additional context**
See discussion here: https://github.com/apache/superset/discussions/12819

cc: @dpgaspar ",,,,,,,,,,,,,,
12964,OPEN,[Chart]Option to set thresholds and Color coding in Big numbers,enhancement:committed; good first issue; need:followup; viz:chart-number,2021-02-16 09:28:39 +0000 UTC,maheshbelavadi,In progress,,"**Is your feature request related to a problem? Please describe.**
I use Big numbers in my summary dashboard to show current status such as average production outcome, etc. If the number is less than a threshold, want to represent it in different color (red, green, amber). Currently there is not option to set thresholds in any of the charts.  

**Describe the solution you'd like**
Typically I would want to set thresholds levels for the BIG number chart, if the actual number is below the specified threshold, the Big chart background must be red or a different color opted by the user. 

**Describe alternatives you've considered**
Currently there is no alternative. I have added the the threshold text under SUBHEADER 

**Additional context**
Current this is how it is displayed.
![image](https://user-images.githubusercontent.com/13337408/107023721-084aab80-67cd-11eb-8fd6-bda67e26719c.png)

Expect a option to set thresholds and display it as below 
![image](https://user-images.githubusercontent.com/13337408/107024209-bd7d6380-67cd-11eb-8ee7-63d3ba3642f6.png)
![image](https://user-images.githubusercontent.com/13337408/107024556-3381ca80-67ce-11eb-8c74-5e942f9425a4.png)
![image](https://user-images.githubusercontent.com/13337408/107024651-51e7c600-67ce-11eb-8ebd-953a2c25a789.png)
",,,junlincc,"
--
Thanks @maheshbelavadi for proposing changes! Definitely a valid request, high impact - low effort. There are a couple asks around Big Number charts, we can get to it safely in Q2-Q3. If you already have a solution in place, feel free to drop a PR. 
--

--
@maheshbelavadi sorry for the late reply! please reach out to me in Slack, I can point you to the right direction. 
@ktmud @maloun96 would you mind giving a tour in the codebase? many thanks! 
--
",maheshbelavadi,"
--
@junlincc will be glad to contribute. I am bit new to superset UI. Is there any quick reference material which I can refer to. Just on where to start from, I will figure out my way if I can get some handle to start with. 
--
",ktmud,"
--
@maheshbelavadi You can start reading CONTRIBUTING.md , especially the section on [viz plugins](https://github.com/apache/superset/blob/master/CONTRIBUTING.md).

Currently you'd need to edit files in the superset-ui repo and wait for them to be published as npm packages in order to make the changes available to the superset repo.
--
",,,,,,
12963,OPEN,[Explore] Chart should strip ending semicolon for virtual datasource queries,bash!; enhancement:request; good first issue; viz:explore:others,2021-03-25 22:34:57 +0000 UTC,ktmud,Opened,,"<img width=""921"" alt=""semicolon"" src=""https://user-images.githubusercontent.com/335541/107020466-bf0c4480-6757-11eb-93a4-0c065aa97bca.png"">

When a virtual datasource has ending semicolon in it's SQL query,

The generated queries for Explore will produce an error:
<img width=""540"" alt=""error"" src=""https://user-images.githubusercontent.com/335541/107020521-cf242400-6757-11eb-8dd9-e7cf46a86787.png"">

Because it didn't strip the semicolon from the original query:

<img width=""486"" alt=""query"" src=""https://user-images.githubusercontent.com/335541/107020639-f549c400-6757-11eb-8405-07af26e8ce46.png"">


The query engine should be smart enough to know semicolon is not needed.",,,junlincc,"
--
@ktmud would like to have this fixed. how often do you users bump into this error?  
--
",villebro,"
--
I run into this quite often actually. This fix should be very simple, low risk and have good overall effect on UX.
--
",pkdotson,"
--
PR is out: https://github.com/apache/superset/pull/13801
--
",,,,,,
12961,OPEN,[Explore]improve the ability to infer datatype in data panel,enhancement:committed; viz:explore:datapanel,2021-02-16 20:36:20 +0000 UTC,junlincc,In progress,,"**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]
Most of the columns showed in data panel have an icon to indicate their data types. 
e.g. ABC - VARCHAR
However, columns like `FLOAT` and `DOUBLE PRECISION`, also columns with NULL in some db are missing icons. 

**Describe the solution you'd like**
A clear and concise description of what you want to happen.
Goal: to have a clear indication of what the data type of each column is
improve the ability to infer datatype in data panel, 
add icon for FLOAT and DOUBLE PRECISION, 
set default icon, maybe ?  for unclear columns 

The actual implementation might be complicated, but this is a necessary step for 1) drag and drop  2) dynamic control from a UX perspective 

<img width=""217"" alt=""Screen Shot 2021-02-04 at 5 23 40 PM"" src=""https://user-images.githubusercontent.com/67837651/106985563-8ac86200-671e-11eb-885c-c465fa364076.png"">
<img width=""370"" alt=""Screen Shot 2021-02-04 at 5 21 28 PM"" src=""https://user-images.githubusercontent.com/67837651/106985568-8ef47f80-671e-11eb-8ebe-9bb10467edd1.png"">
",,,junlincc,"
--
@nikolagigic ^ as part of https://github.com/apache-superset/superset-roadmap/issues/152 
--

--
https://github.com/apache/superset/issues/13159 related 
--
",,,,,,,,,,
12959,OPEN,[Dashboard] Removing empty top level tabs crashes the dashboard builder,#bug; viz:dashboard:editmode,2021-02-05 01:29:49 +0000 UTC,ktmud,Opened,,"### Expected results

When removing empty top-level tab, the page should not crash

### Actual results


https://user-images.githubusercontent.com/335541/106976486-57c9a280-670d-11eb-8f97-09c692d796f9.mp4

There are a couple of bugs or inconveniences related to adding tabs in dashboard builder:

1. Adding top level tab does not have proper drop indicator
2. Collapsing the top level tab makes the whole page crush if you have deleted the all tabs in it
3. Deleting the last tab collapses the tab container to a weird-looking state. We should probably either redesign it or just disabling deleting the last tab

#### How to reproduce the bug

1. Try add a top-level tab

### Environment

Latest master

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,,,,,,,,,,,,
12958,OPEN,"Add ""Main Datetime Column"" in Edit Dataset modal",good first issue; viz:explore:dataset,2021-02-05 18:35:11 +0000 UTC,yousoph,In progress,,"**Is your feature request related to a problem? Please describe.**
Main Datetime Column exists in the Legacy Datasource Editor but not in the new editor 
![image (8)](https://user-images.githubusercontent.com/10627051/106968181-352f8d80-66fd-11eb-9d9e-f125a6d1bba7.png)

**Describe the solution you'd like**
Add the field in the new Edit Dataset modal, maybe under the Settings tab Advanced section? ",,,yousoph,"
--
fyi @ktmud @etr2460 
--
",,,,,,,,,,
12957,OPEN,Serial Number and row count in SQL Lab query results,enhancement:request; needs:design-input; sql_lab,2021-02-12 00:30:52 +0000 UTC,kamalkeshavani-aiinside,Opened,,"**Is your feature request related to a problem? Please describe.**
Almost all popular SQL editors provide serial number(row number) and the total row count of results in the query results. 

**Describe the solution you'd like**
For row number, add a column to left-side with row number of query results.
- These numbers will not be used when user creates a table from query results clicking 'Explore', or downloads csv or copies results to clipboard.
- User cannot sort the results by the serial numbers.

For row count, the explore view already shows result row count in UI. We can show in similar way in SQL Lab as well.

some samples:
![image](https://user-images.githubusercontent.com/74634977/106974080-6eb4d700-6797-11eb-918c-74c7dde46de9.png)

![image](https://user-images.githubusercontent.com/74634977/106974116-7d9b8980-6797-11eb-969c-d87bfcc17976.png)


**Additional context**
This feature will bring Superset SQL editor a bit more closer to other popular editors, and make the users more comfortable using SQL lab more frequently.",,,kamalkeshavani,"
--
Updated the feature request to show row count of query results as well.
--

--
Hey @Steejay, here is a use case where I felt that serial numbers could have helped me recently.

- I was looking at a query result grouped by 2 columns, date and name(also ordered by date and name).
- You could say it is more of a habit from other SQL editors, where I can simply use serial number to find out how many rows with the first name are present in the results.
- In SQL lab, I just had to add row_number() in SELECT clause to get the same data.
 
I can't really say if a lot of people use the serial numbers the same way, but I am guessing that many of them have some usecase where they find it useful. And if I my guess is correct then I think this feature can help them feel more comfortable doing data exploration in SQL lab when starting to use Superset.
--
",junlincc,"
--
+1 on having total row count of results in SQL lab, should be a simple enhancement. I believe is in design phase 
https://github.com/apache/superset/issues/12436#issuecomment-771073296

Having serial number in the csv download file from Explore view is nice to have as well. 


--
",Steejay,"
--
Hey @kamalkeshavani-aiinside thanks for the feedback and feature requests! As @junlincc mentioned we're adding a row count indicator in SQL Lab for the results. Feedback is welcome on the designs posted in #12436. 

Re serial number. this is a great suggestion and would be a great result table enhancement. Im interested to learn more about this. Can you tell me about the last time you wanted to see serial numbers and why? What task were you trying to accomplish and how would serial numbers help you?
--
",,,,,,
12954,OPEN,Error Upgrading Bar Charts with SQL Functions in Series to Superset 1.0.0,need:more-info; need:validation,2021-02-08 18:34:32 +0000 UTC,akud,In progress,,"I am migrating from superset version 0.34.0rc2 to 1.0.0. In the old version we use the ""Bar Chart"" visualization with a sql function in the ""Series"" box successfully. In the new version, this chart is broken. For example, we have a chart that renders successfully in the old version has a ""Series"" column of `datepart(year,shipment_month)`. This chart fails in the new version. The data source is in redshift.

### Expected results

I expected the bar chart to render correctly.

### Actual results

I saw an error message stating `Columns missing in datasource: ['datepart(year,shipment_month)']`

#### Screenshots

Configuration from the old version:
![Screenshot from 2021-02-04 11-58-09](https://user-images.githubusercontent.com/509910/106948612-c5130e80-66e0-11eb-9589-82a3846fad04.png)

Successfully rendered chart in the old version:

![Screenshot from 2021-02-04 12-04-59](https://user-images.githubusercontent.com/509910/106949216-85005b80-66e1-11eb-994f-d7f10dec99e8.png)

Configuration in the new version:

![Screenshot from 2021-02-04 11-58-46](https://user-images.githubusercontent.com/509910/106949358-ad885580-66e1-11eb-97fd-8c2aa5a03319.png)

Error message in the new version:

![Screenshot from 2021-02-04 09-51-22](https://user-images.githubusercontent.com/509910/106949404-bf69f880-66e1-11eb-9b81-259813cc420e.png)


#### How to reproduce the bug

1. View a Redshift dataset with a datetime column in Explore mode
2. Choose ""Bar Chart"" visualization
3. Add ""datepart(year, <datetimecolumn>)"" as a ""Series"" entry
4. Click the ""Run"" button
5. See the error

### Environment

(please complete the following information):

- superset version: `superset version`
```
Loaded your LOCAL configuration at [/home/superset/superset_config.py]
logging was configured successfully
INFO:superset.utils.logging_configurator:logging was configured successfully
/usr/local/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
  ""Flask-Caching: CACHE_TYPE is set to null, ""
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Superset 1.0.0
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
```
- python version: `python --version`

```
Python 3.7.9
```
- node.js version: `node -v`

```
v10.23.2
```
### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
Request log:
```
[2021-02-04 20:29:35 +0000] [63] [DEBUG] POST /superset/explore_json/
DEBUG:parsedatetime:eval - with context - False, False
Cache key: f540b9108c6c05f255250f9c259ec9a9
INFO:superset.viz:Cache key: f540b9108c6c05f255250f9c259ec9a9
127.0.0.1 - - [04/Feb/2021:20:29:35 +0000] ""POST /superset/explore_json/ HTTP/1.1"" 400 981 ""http://localhost:8088/superset/explore/?form_data=%7B%22datasource%22%3A%221__table%22%2C%22viz_type%22%3A%22dist_bar%22%2C%22url_params%22%3A%7B%7D%2C%22time_range_endpoints%22%3A%5B%22inclusive%22%2C%22exclusive%22%5D%2C%22granularity_sqla%22%3A%22created_on%22%2C%22time_range%22%3A%22Last+week%22%2C%22metrics%22%3A%5B%22count%22%5D%2C%22adhoc_filters%22%3A%5B%5D%2C%22groupby%22%3A%5B%22datepart%28year%2C+created_on%29%22%5D%2C%22columns%22%3A%5B%5D%2C%22row_limit%22%3A10000%2C%22color_scheme%22%3A%22supersetColors%22%2C%22label_colors%22%3A%7B%7D%2C%22show_legend%22%3Atrue%2C%22y_axis_format%22%3A%22SMART_NUMBER%22%2C%22bottom_margin%22%3A%22auto%22%2C%22x_ticks_layout%22%3A%22auto%22%7D"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36""
```
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

We are upgrading to superset 1.0.0 by cloning our taking a snapshot of the metadata db and spinning up a new superset instance on top of it. The failing chart is a copy of a chart that was working in the old version.
",,,junlincc,"
--
@akud 
what is the data type of that column? can you also provide the sql query that generated in the old version? thanks
--
",akud,"
--
Hi @junlincc the column is a DATE column. Here is an obfuscated version of the query produced by the old version of superset:

```
SELECT version AS version,
                  (datepart(year, shipment_month)) AS ""datepart(year,shipment_month)"",
                  product_group AS product_group,
                  sum(net_shipped_amount_usd) AS ""sum(net_shipped_amount_usd)""
FROM
  (... Data Source custom SQL ...)
GROUP BY version, (datepart(year, shipment_month)), product_group
ORDER BY ""sum(net_shipped_amount_usd)"" DESC
LIMIT 1000;
```

For anyone stumbling across this, I was able to solve the problem in the UI by editing the datasource and adding a ""Derived Column"" with the sql function, then using that in the chart Series.


--
",,,,,,,,
12952,OPEN,"[explore]allow users to search ""type""""schema""'connection""""creator"" in Change dataset",enhancement:committed; good first issue; viz:explore:dataset,2021-04-09 03:58:00 +0000 UTC,junlincc,In progress,,"I connected to multiple different db for testing purposes. I wanted to test datasets that are from a presto db connection, but had a hard time looking for it. there is no way to search by ""type"", ""schema"",""connection"" and ""creator"" currently. 

proposed change - allow user to search by in columns. 

https://user-images.githubusercontent.com/67837651/106941138-4796d080-66d7-11eb-8201-0436cf54c58b.mov

",,,ankh6,"
--
Hey @junlincc !
I'd like to work on this issue if possible. Could you clarify these points:

- Does this task also involve frontend work ?
- Which modules are involved ?
- Should we implement as a @classmethod in a base class so that we can search irregardless of the database?

--

--
@junlincc  any news :) ?
--
",junlincc,"
--
@ankh6 thank you for picking up the task! 

@ktmud can you help answer these technical questions ^^ 
--

--
https://github.com/apache/superset/tree/master/superset-frontend/src/datasource please see link @ankh6 
--
",ktmud,"
--
Feel free to dig into the code. You'll know the answer when you see how current component is implemented. @ankh6
--
",zhaoyongjie,"
--
> Hey @junlincc !
> I'd like to work on this issue if possible. Could you clarify these points:
> 
>     * Does this task also involve frontend work ?
> 
>     * Which modules are involved ?
> 
>     * Should we implement as a @classmethod in a base class so that we can search irregardless of the database?

Hi, there.
It is a frontend work, please explore the codebase here.

https://github.com/apache/superset/blob/a3b41e2bac554a097015cf73e950ca1fc5e43b6e/superset-frontend/src/datasource/ChangeDatasourceModal.tsx#L99
--
",,,,
12951,OPEN,Overriding font-size for subheader-line in Big Number Chart on a dashboard causes page crash,question,2021-02-05 09:38:13 +0000 UTC,klazaj,Opened,,"I tried to override the `subheader-line` class for Big Number chart, by adding this CSS template in 'Edit CSS' option in the dashboard, like so:

```
.subheader-line {
    font-size: 150%!important;
}
```
However, when I click ""Save Changes"", the page becomes unresponsive. Why is that? How can I override the CSS of Subheader? The font-size becomes too small when I write more than a few words. It's too small even when I choose `Huge` for Font-size option in Chart Options. 

Adding an example for clarity. This is what the Big Number looks like with 'Huge' font-size for the Subheader:
![image](https://user-images.githubusercontent.com/22456202/107016257-04ebfb80-679e-11eb-8064-96d3f20bb2d6.png)
As you can see, it's not really readable. Overriding it improves the readability:
![image](https://user-images.githubusercontent.com/22456202/107016386-2cdb5f00-679e-11eb-8eda-fb589a2dea90.png)
 ",,,,,,,,,,,,,,
12942,OPEN,Generating Reports,global:report,2021-02-12 14:25:05 +0000 UTC,JMGGarcia,In progress,,"(First of all, congratulations on the v1 release of Superset, it looks very nice and includes a lot of nice new features!) 

On the old version of Superset, I had an issue generating reports on a server (#11552). I was trying out the new version of superset, using v1.0.1rc2 and the default docker configs to try and generate reports (first locally, and then I would try on a server). I define the report to run every 6 minutes, and use one of the default dashboards (covid), and set the report to be sent by email to me. On the config file, I put `ENABLE_SCHEDULED_EMAIL_REPORTS = True` and the SMTP configurations. I tried with the debug mode on and off. 

The problem is that nothing seems to run, with errors or not. There are no logs, neither in the UI nor in the terminal. 

### Expected results

Logs related to the generation of reports. 

### Actual results

No logs are generated, nothing seems to happen. 

#### How to reproduce the bug

1. Go to Alert & Report page
2. Define a report
3. Wait for logs to be generated

### Environment

(please complete the following information):

- superset version: `v1.0.1rc2`
- Using default docker-compose with configs defined to generate reports

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.",,,shemExelate,"
--
I'm having the same issue, running latest from `/docker`
my config:

**docker-compose.yaml**
added:
```yaml
  superset-beat:
    image: *superset-image
    container_name: superset_beat
    command: [""/app/docker/docker-bootstrap.sh"", ""beat""]
    env_file: docker/.env
    restart: unless-stopped
    depends_on: *superset-depends-on
    user: ""root""
    volumes: *superset-volumes
```

**docker-bootstrap.sh**
modified to:
```sh
if [[ ""${1}"" == ""worker"" ]]; then
  echo ""Starting Celery worker...""
  celery worker --app=superset.tasks.celery_app:app --pool=prefork -O fair -c 4 -l DEBUG
elif [[ ""${1}"" == ""beat"" ]]; then
  echo ""Starting Celery beat...""
  celery beat --app=superset.tasks.celery_app:app -l DEBUG
elif [[ ""${1}"" == ""app"" ]]; then
  echo ""Starting web app...""
  flask run -p 8088 --with-threads --reload --debugger --host=0.0.0.0
fi
```

**superset_config.py**
modified to:
```py

RESULTS_BACKEND = RedisCache(
    host=REDIS_HOST, port=REDIS_PORT)

FEATURE_FLAGS = {
    ""ALERT_REPORTS"": True,
}

class CeleryConfig(object):
    BROKER_URL = f""redis://{REDIS_HOST}:{REDIS_PORT}/{REDIS_CELERY_DB}""
    CELERY_IMPORTS = (
        ""superset.sql_lab"",
        ""superset.tasks"",
    )
    CELERY_RESULT_BACKEND = f""redis://{REDIS_HOST}:{REDIS_PORT}/{REDIS_RESULTS_DB}""
    CELERY_ANNOTATIONS = {
        ""tasks.add"": {
            ""rate_limit"": ""10/s"",
        },
        ""sql_lab.get_sql_results"": {
            ""rate_limit"": ""100/s"",
        },
        ""email_reports.send"": {
            ""rate_limit"": ""1/s"",
            ""time_limit"": 120,
            ""soft_time_limit"": 150,
            ""ignore_result"": True,
        },
    }
    CELERYBEAT_SCHEDULE = {
        ""email_reports.schedule_hourly"": {
            ""task"": ""email_reports.schedule_hourly"",
            ""schedule"": crontab(minute=1, hour=""*""),
        },
    }
    CELERY_TASK_PROTOCOL = 1

CACHE_CONFIG = {
    ""CACHE_TYPE"": ""redis"",
    ""CACHE_DEFAULT_TIMEOUT"": 60 * 60 * 24, # 1 day default (in secs)
    ""CACHE_KEY_PREFIX"": ""superset_results"",
    ""CACHE_REDIS_URL"": f""redis://{REDIS_HOST}:{REDIS_PORT}/{REDIS_RESULTS_DB}"",
}

CELERY_CONFIG = CeleryConfig
SQLLAB_CTAS_NO_LIMIT = True

ENABLE_SCHEDULED_EMAIL_REPORTS = True
EMAIL_NOTIFICATIONS = True

SMTP_HOST = ""smtp.gmail.com""
SMTP_STARTTLS = True
SMTP_SSL = True
SMTP_USER = ""df.team.test@gmail.com""
SMTP_PORT = 465
SMTP_PASSWORD = os.environ.get(""SMTP_PASSWORD"")
SMTP_MAIL_FROM = ""df.team.test@gmail.com""
```
--
",vnourdin,"
--
@shemExelate the config for reports scheduling is missing from your `CeleryConfig`, you must add the content of [this PR](https://github.com/apache/superset/pull/12999/files).
This feature is not yet documented properly, we are working on it!
--

--
Are you creating your reports from the old menu or the new ""Alerts & Reports"" one?
The celery beat is only used with the new implementation I believe.
--

--
Then I don't know what's wrong :thinking: I got those logs:
```
 [2021-02-12 13:51:22,565: DEBUG/MainProcess] beat: Ticking with max interval->5.00 minutes
 [2021-02-12 13:51:22,572: DEBUG/MainProcess] beat: Waking up in 37.42 seconds.
 [2021-02-12 13:52:00,011: DEBUG/MainProcess] beat: Synchronizing schedule...
 [2021-02-12 13:52:00,057: INFO/MainProcess] Scheduler: Sending due task reports.scheduler (reports.scheduler)
 [2021-02-12 13:52:00,092: DEBUG/MainProcess] reports.scheduler sent. id->656ea944-efa3-4acb-a171-90394e58546d
 [2021-02-12 13:52:00,093: DEBUG/MainProcess] beat: Waking up in 59.90 seconds.
```
The only main missing config I see is `WEBDRIVER_BASEURL` but you have no error...
--
",JMGGarcia,"
--
@vnourdin Thank you, but with the content of that PR, I'm still getting the same results (using basically the settings @shemExelate posted previously). The beat container only logs 

```
superset_beat            | [2021-02-12 11:45:15,424: DEBUG/MainProcess] beat: Ticking with max interval->5.00 minutes
superset_beat            | [2021-02-12 11:45:15,425: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.
superset_beat            | [2021-02-12 11:50:15,525: DEBUG/MainProcess] beat: Synchronizing schedule...
superset_beat            | [2021-02-12 11:50:15,531: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.
superset_beat            | [2021-02-12 11:55:15,631: DEBUG/MainProcess] beat: Synchronizing schedule...
superset_beat            | [2021-02-12 11:55:15,633: DEBUG/MainProcess] beat: Waking up in 5.00 minutes.
``` 
--

--
I'm using the new one (although I tested and I'm able to run the old one). 
--
",,,,,,
12941,OPEN,Unable to Login after docker container connect to Postgres Container,authentication:login; need:validation,2021-02-12 09:39:48 +0000 UTC,PullGituser,In progress,,"A clear and concise description of what the bug is.
I have configured superset docker container and then change the URI in conifig.py to point to postgres. I am able to run the 'superset db upgrade' and could see tables are getting created under the database. Also able to create a new user in the postgres DB. But unable to login to the application using 'admin:admin'. 
### Expected results

what you expected to happen.
Should be able to login to application with dafult admin username and can add the postgres database in screen.
### Actual results
Invalid User name/password error
what actually happens.

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

### Environment

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,junlincc,"
--
this is strange, i run the same steps on a daily basis but never encountered this issue. 
@dpgaspar @srinify do you guys know? thanks 
--
",PullGituser,"
--
I am using superset as a different container and postgres as a different container. (NOT USING Docker Compose). After updating the sqlalchemy connection string in config.py file to point to postgres container, I am not able to Login to application. Let me know if you need any additional details.

Note - Tables are and user entry details are present in Postgres DB...
--
",anilkulkarni87,"
--
I get the same error. I am trying this on a Windows WSL2 Ubuntu. I am using docker-compose up after cloning the repo.
--
",dpgaspar,"
--
@PullGituser,

make sure that when you update the config you reinstall superset `pip install -e .` or `python setup.py install` and then follow the init steps: `superset db upgrade`, `superset init` etc. What procedure did you use to create the user on Postgres?
--
",,,,
12940,OPEN,Nested Fields,enhancement:request,2021-02-07 10:19:18 +0000 UTC,saultawil,In progress,,"I would like to know if there is any plans to support nested fields like some other tools that now support this like elasticsearch/kibana.

I use Superset against a Spark SQL HiveServer2 compatible JDBC query engine that reads data from Hive tables that support nested data but in Superset I cannot selected the nested fields independently.

I spend a lot of time flattening fields for new projects. 
Also, for users, referencing the fields with dot notation would be much more intuitive.

A lot of modern data arrives in json with nested struct fields and is stored on an object store like S3. Since there are more and more data lakes or lakehouses front-ended with query engines like Presto and Spark with this type of nested data, users/developers want a simple way of querying this data with minimal change. So it would be very strategic and beneficial to Superset to support such a capability.",,,junlincc,"
--
@saultawil thanks for posting this enhancement request. sorry that we couldn't answer your questing in the meetup on Wednesday. I don't think we currently has the ability of running independent query on nested data, there might be work around that I don't know though. 

Tableau offers something similar to your request - after user connect to a JSON file, it prompts users to select the schema levels. Is this what you are looking for?

![renditionDownload](https://user-images.githubusercontent.com/67837651/107132691-0dbff880-6896-11eb-84bf-1f60cc32b7ad.jpeg)
![renditionDownload (1)](https://user-images.githubusercontent.com/67837651/107132694-10bae900-6896-11eb-9baa-441f40c0bf85.jpeg)

@villebro @zhaoyongjie 

--
",saultawil,"
--
Thanks for the response Junlin...I suppose Tableau may offer access to nested fields but the above example is more complex because it involves arrays which can be repeatable sets of nested data. I am looking for support for something much simpler - simple struct fields where one struct field is made of multiple nested fields that occur exactly once. An example that I have seen in a similar tool called Kibana (see below image attached) - I did not set up this data source but I do not think it required any special input from the user regarding levels. The tool simply recognizes the nested fields and treats them like separate fields that can be accessed with dot notation not matter the level:

![image](https://user-images.githubusercontent.com/60597083/107143410-934ab380-693d-11eb-854c-3cb5e6f45e22.png)


--
",,,,,,,,
12939,OPEN,How can I bypass unnecessary sign-in page when using SSO with OAUTH2?,authentication:login:sso; question,2021-02-25 10:38:35 +0000 UTC,saultawil,Opened,,"I set up SSO login for Superset successfully by setting up the config file to use OAUTH2 login and integrating it with OKTA SSO service. However the redirect uri http://[site]:8088/oauth-authorized/okta that needs to be configured for Flask Application Builder to work redirects to a Superset sign-in page where the user needs to choose a provider and click sign-in. This is an unnecessary step since there is only one provider and the user already logged into OKTA SSO provider and should not need to sign-in or login again.

Is there any way to bypass this redundant sign-in page which my users find confusing and sometimes get stuck on. If there is no way to bypass it can someone tell me how I can customize the login page to instruct users how to use it?",,,,,,,,,,,,,,
12938,OPEN,"Error response 400 ""unknown field"" when setting filters_immune_slices in Dashboard metadata",#bug,2021-02-04 10:14:55 +0000 UTC,krsnik93,Opened,,"Trying to add `filter_immune_slices` to prevent chart from being affected by a `filter_box`. After clicking save in the Dashboard edit modal, there is no visual feedback, but after inspecting Network tab in Developer Tools I can see error response:
`{message: {json_metadata: [{filter_immune_slices: [""Unknown field.""]}]}}`

### Expected results

Change gets applied to the dashboard.

### Actual results

No visual feedback after clicking save, HTTP response code 400 with ""unknown field"" for ""filter_immune_slices"".

#### Screenshots

![image](https://user-images.githubusercontent.com/13034472/106877508-bd4b6e00-66d0-11eb-823f-902bc835f44d.png)


#### How to reproduce the bug

Create a dashboard. Add a filter box to it. Add a chart with column filtered by the filter_box. Try to edit dashboard json metadata by adding the following key-value:
`""filter_immune_slices"": [slice_id]` 
where slice_id is the id of the chart with column filtered by the filter_box. 

### Environment

(please complete the following information):

- superset version: `superset version`: Superset 1.0.0
- python version: `python --version`: Python 3.8.7
- node.js version: `node -v`: Node not installed.

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,,,,,,,,,,,,
12929,OPEN,Presto admin commands(set role; grant perms) not supported on Superset,data:connect:presto; v0.37.2,2021-02-04 05:41:30 +0000 UTC,ramyarajasekaran,Opened,,"### Environment
- superset version: `0.37.2` I upgraded to 1.0.0 and still saw this issue.
- python version: `3.6`
- node.js version: `12`

**Is your feature request related to a problem? Please describe.**
Superset does not support Presto's administrative commands such as SET ROLE, GRANT PERMS ON ROLE, CREATE ROLE, etc. These commands work on Presto CLI. Are there plans to support this feature in future releases?

**Describe the solution you'd like**
Running administrative commands should result in altering a user's data permissions for a user's session on Superset. This should be handled through SQLAlchemy and PyHive. 

**Describe alternatives you've considered**
Potentially using Presto CLI to run administrative commands and grant admins another role for querying purposes since admin role cannot be assumed on Superset.",,,,,,,,,,,,,,
12916,OPEN,How can I upload a CSV file with non-UTF8 character set,data:csv; enhancement:request,2021-02-04 01:16:16 +0000 UTC,ethanhuang1009,In progress,,"A clear and concise description of what the bug is.

### Expected results
upload success

### Actual results
upload failed and I faced an error without any solutions

#### Screenshots
<img width=""661"" alt=""_20210203181452"" src=""https://user-images.githubusercontent.com/65382362/106733116-70be4f00-664c-11eb-93e8-7e2b1e844f0c.png"">

**Unable to upload CSV file ""11.csv"" to table ""11"" in database ""test"". Error message: 'utf-8' codec can't decode byte 0xd4 in position 0: invalid continuation byte**


#### How to reproduce the bug

1. Go to 'upload a csv'
2. Click on 'select a file.'
3. Scroll down to '....'
4. See an error

### Environment

(please complete the following information):

- superset version: `1.0.0`
- python version: `python3.8`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional [context]

Add any other context about the problem here.
",,,zhaoyongjie,"
--
@ethanhuang1009 
Please check the CSV file enconding.
1. Do not use CSV file exported from Excel on Windows plantform, Excel exported CSV file encoding is `utf8-with-bom`. 
2. The Superset only supports utf8 encoding CSV file import, so please carefully check the file encoding format.

This is a rough encoding issue,  in the future we can use some solution guess that file encoding.

======================
In Chinese
,  CSV.
1.  Excel CSV,  UTF8-bom .
2.  UTF8 ,  CP936(GB2312) . 
, . UTF8 .  SublimeText .
--
",ethanhuang1009,"
--
@zhaoyongjie  Thanks you very much. I change the file encoding format from **utf8-with-bom** to utf8, then I can upload the file successfully. And I hope Superset can find and give us a better solution for the non-utf8 character set files in the near future.
--
",,,,,,,,
12915,OPEN,Unable to import Postgresql table,need:validation,2021-02-04 05:46:28 +0000 UTC,aminhakimsazali,Opened,,"A clear and concise description of what the bug is.

### Expected results

Import table from Postgres Database

### Actual results

I created a PostgreSQL database using AWS RDS and then import the database into Superset. I've checked the connection and it prompted OK. 

![image](https://user-images.githubusercontent.com/32535528/106720516-8a589a00-663e-11eb-8831-1c29ae02940d.png)

The table 'test' is also detected. 

![image](https://user-images.githubusercontent.com/32535528/106720812-e15e6f00-663e-11eb-8467-77dee4b2cc26.png)

The issue is, I cant import the test table. It prompted an error as follows

![image](https://user-images.githubusercontent.com/32535528/106721026-166ac180-663f-11eb-8b48-b4a21a21c2cd.png)

Based on the pgAdmin structure, I think I've filled up the correct schema and table name. 

PgAdmin: 

![image](https://user-images.githubusercontent.com/32535528/106721431-a0b32580-663f-11eb-9a97-fa9ac576a1ea.png)

Superset import table: 

![image](https://user-images.githubusercontent.com/32535528/106721644-e374fd80-663f-11eb-895d-31b94b9f476e.png)


",,,junlincc,"
--
looks like you are on a pretty old version, have you tried using v1.0? @aminhakimsazali i just connected to PostgresSQL today, and connection works fine 
--
",,,,,,,,,,
12912,OPEN,Tooltip in multilayer map,viz:chart-maplayer,2021-02-03 06:38:03 +0000 UTC,Geethanjali11,Opened,,"## Screenshot

[drag & drop image(s) here!]
![image](https://user-images.githubusercontent.com/78130557/106706634-fd014f80-6615-11eb-8c50-898786c61c3b.png)


## Description

I have three layers in this map. Scatterplot layer has customised tooltip as displayed. But the tooltip is displayed only when I hover over a point outside polygon layer. Ideally, I want the tooltip over every point irrespective of the layer over it. Any idea how to resolve this?


",,,,,,,,,,,,,,
12911,OPEN,Multilayer map - Feature to toggle between different layers,enhancement:request; viz:chart-deck.gl,2021-02-05 00:22:50 +0000 UTC,Geethanjali11,Opened,,"
## Description

Is there a feature to toggle between different layers in a deck.gl multiple layer map once it is published in a dashboard?
For example, if a map has 3 layers (scatterplot, line map, arc map) , I want to enable only scatterplot. A toggle button to change the layers.

",,,junlincc,"
--
@Geethanjali11 thanks for suggesting. 
from product standpoint this is almost a  _must have_, however, I don't think we can get to it anytime soon. 
community contribution is always welcome! 
--
",,,,,,,,,,
12910,OPEN,Giving access based on Dashboards instead of Datasources.,security:access:dashboard,2021-02-04 07:33:47 +0000 UTC,Renganayaki1,In progress,,"As of now, we can give access based on data sources. If we give permission to one dataset, all the dashboard created using that dataset can be viewed. But what if we need to restrict some of the dashboards here. 

We should need the access permission based on Dashboads. 

Some of the related Issues, SIP and pull requests i have seen. I have attached all the link below.

1) https://github.com/apache/superset/issues/5483
2) https://github.com/apache/superset/issues/10408
3) https://github.com/apache/superset/pull/12407

So, what is the current status on this? Is the ""Access based on Dashboards"" option implemeted? 

If so, What is the steps to achieve that and from where I can find the option?

Someone please conclude this. Thanks.
",,,amitmiran137,"
--
Hi there.
It is always good to know that others can't wait for that feature ;)

we are working towards the **first** milestone of implementation in which dashboard access can be enforced bases on roles assigned to ( est. in 2-4 weeks) 
but you'll still need to manage the data access in order to allow charts on your dashboard to fetch data

the **second** milestone is to enable data access to all of the charts within a dashboard just by having a dashboard access
this step is a bit more complex and we still don't have estimates for that but for sure this is something on the short-term roadmap.

hope this concludes everything

Amit





--
",pl77,"
--
@amitmiran137 This sounds complex.  If a person has access to the dashboard it would seem that they pretty much already have access to the data source, do they not?  I guess, from my side, if I wanted to prevent users from accessing various columns or records within a table/database I'd just generate a separate materialized view or other temporary table with the permissible data and give the users full access to that instead.  Otherwise it feels like I'll be playing an endless game of whack-a-mole trying to plug various holes to prevent users from tweaking their AJAX calls and grabbing the whole illicit data set. 

Anyway, I'll be watching this closely as it might minimize some of my data-post-processing efforts if I can set stronger permissions on data subsets.  
--
",Renganayaki1,"
--
Thanks @amitmiran137 for the conclusion. Please let us know once this requirement added.
Rens. 
--
",,,,,,
12908,OPEN,Ability to give Row Level Security to users directly instead of roles.,security:row-level-security,2021-02-04 07:34:51 +0000 UTC,Renganayaki1,Opened,,"We have Row Level Security option under the ""Setting"". We can add tables on which the RLS to be applied, we can select filter type and can give WHERE Clause. But in option ""Roles"", we can add only roles. To do that, we need to create users first, then to create roles, assign it to the users and then add it there.. 
But, in some situation, we will be having 100s of users under different departments. Inside one department, we could need some restrictions in the Row Level. 
To do so, we have to create, that much roles and assign those to the users correctly and then need to be added in Row Level Security Section. Very tedious process!!
So, what is the short way to assign Row Level Security directly to users, instead of adding Roles ?

![image](https://user-images.githubusercontent.com/77659418/106697618-1fd73800-6605-11eb-9e53-5a1a303eda8c.png)


",,,,,,,,,,,,,,
12907,OPEN,[table] Total Row for Table Charts,enhancement:request; needs:design-input; viz:chart-table,2021-02-09 13:00:50 +0000 UTC,rwspielman,In progress,,"**Is your feature request related to a problem? Please describe.**
Table charts don't include an option for 'Total' when utilizing group by. There **is** the option to do this in pivot tables, however, the 'All' record just floats around and isn't really useful to quickly glance at from the user's POV.

**Describe the solution you'd like**
A toggle option on table charts that lets the user include a total row. Ideally, the total row will stick to the bottom of the table and be stylized differently than the rest of the table.

**Describe alternatives you've considered**
We tried using the pivot table with the option for a total row - but because it is named 'All' if we sort by the groupby in alphabetical, it will just float in the midst of all the data.

",,,Steejay,"
--
@junlincc what does 'All' show in this context? Doesn't seem like it is displaying the sum of numbers in the result being displayed. 

![Screen Shot 2021-02-04 at 1 50 46 PM](https://user-images.githubusercontent.com/60786102/106959778-fd6e1900-66ef-11eb-8efd-f3a6927397c4.png)

cc @mihir174 

--
",junlincc,"
--
<img width=""1402"" alt=""Screen Shot 2021-02-04 at 2 06 18 PM"" src=""https://user-images.githubusercontent.com/67837651/106961384-5939a180-66f2-11eb-9c0e-d167aaaf9511.png"">

@rwspielman Thanks for requesting. this has been on our radar. 

@Steejay I think what Rob meant was adding a row at the bottom of the table to sum numbers in each column. something like this in tableau. 
![image](https://user-images.githubusercontent.com/67837651/106961919-2348ed00-66f3-11eb-8bf7-c4c88498f332.png)

the challenge for Design here is one -> when there's pagination(large amount of rows), how do we define which is the ""last row"" . does user need to flip to the last page to view total? or can we truncate from middle, display only one page, when user switch to the Sum mode. 
two, where to add this switch? 

@Steejay @mihir174 




--
",rwspielman,"
--
Junlin (@junlincc), confirming that your second screenshot is the end result I'm looking for. We are coming from excel reporting mostly so the parallel in excel looks like this: 
<img width=""266"" alt=""Screen Shot 2021-02-05 at 1 04 40 AM"" src=""https://user-images.githubusercontent.com/20442310/107000837-2c1adc80-674e-11eb-8fe8-70810811b6e9.png"">

To opine on your second design consideration of where to add the switch, I think that for the case of `Query Mode: Aggregate` it should be placed in the `Group By` section of the control panel. From a user standpoint, it would be like unioning an additional row where we are grouping by `True` (i.e. `SELECT ""Grand Total"" as <groupby>, <metrics> FROM <table> GROUP BY True ...`)

Additionally, our use case is for table viz `Query Mode: Aggregate`. For this case, it should carry over the metrics defined in the query section of the control panel and not require the user to define an aggregating function.


--
",villebro,"
--
@rwspielman now that it has been migrated the table chart to the new chart data endpoint (see #10270), it is finally possible to add a total row to the table chart. The reason this wasn't possible before is because legacy charts were really only able to do single query requests, making it difficult to get both row level data and aggregates for the whole dataset. With the new architecture, it's possible to issue multiple queries to the backend for a single chart rendering. In practice this feature would require issuing a second query to the analytical database, where the groupings have been removed, and only the metrics are present.

@amitmiran137 I know your team has been thinking about working on this functionality. Have you considered how pagination would be taken into account when displaying results? I personally think think
- The total should display the value for all metrics without grouping: if a metric is `AVG(x)`, the total row would display the average for the whole column.
- In addition to showing the aggregate metric values, it would also show total row count.
- The total row would be visible all the time, showing results for ALL rows, not only those visible in the current pagination view.

--
",korpa,"
--
I would have 2 wishes if you start to work on this:

* Make total lines position configurable: _top_ or _bottom_
* Make total columns position configurable: _beginning_ or _end_

This could be very handy if you have many lines or many rows
--
",,
12900,OPEN,Hide tool Bar for SQLLab Tab broken 1.0,bug:regression; sql_lab,2021-02-03 00:32:52 +0000 UTC,pabrahamusa,Opened,,"Hide tool Bar not working for SQL lab tabs properly. It was working with the previous 0.38 version

### Expected results

When you have multiple tabs , Hide Tool Bar should appear for all 'Expanded' Tabs

### Actual results

When you have multiple tabs, Hide tool Bar will only work for first tab. All remaining tab will still display Expand Tool Bar. which is incorrect . 

#### Screenshots

In this image you can see that it is not displaying 'Hide tool Bar' . Instead it is showing 'Expand Tool Bar'..!
![image](https://user-images.githubusercontent.com/47830507/106669226-58c9cb00-6579-11eb-8285-52ec5b620cf3.png)


#### How to reproduce the bug

1. Go to 'SQLLab'
2. Create couple of New Tabs by clicking + 
3. Go to one of the Tab and Click on the three dots on the left side of tab head and choose ""hide Tool Bar' from the drop down.
4. Go to a different tab try to do the same. 
5. You can see 'Hide Tool Bar' is not displaying for all other tabs.

### Environment

(please complete the following information):

- superset version: 1.0 (Docker image build from master v1.0.0)
```
root@sb-superset-666fdfc5db-c6l7l:/app# superset version
Loaded your LOCAL configuration at [/app/pythonpath/superset_config.py]
logging was configured successfully
INFO:superset.utils.logging_configurator:logging was configured successfully
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Superset 0.999.0dev
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
```
- python version: 
```
root@sb-superset-666fdfc5db-c6l7l:/app# python --version
Python 3.7.9
```
- node.js version: `node -v`
```
root@sb-superset-666fdfc5db-c6l7l:/app# node -v
bash: node: command not found
```

",,,junlincc,"
--
tested on master by creating 10 tabs, clicked all Hide tool bar on all tabs, seems working properly.
however, please do checkout master yet as we discovered some severe regression. please expect fixes in patch release @pabrahamusa 

https://user-images.githubusercontent.com/67837651/106670707-4001eb00-6562-11eb-8fd1-72e391dfe2cb.mov


--
",yousoph,"
--
Thanks for reporting, I'm seeing the issue on master (hide toolbar, switch to a tab with an expanded toolbar, click to try to hide the toolbar in the new tab) 
--
",,,,,,,,
12899,OPEN,Consolidate SQL components,#bug; sql_lab,2021-02-02 22:26:07 +0000 UTC,betodealmeida,Opened,,"We currently have a few places where users can write SQL:

- SQL Editor
- When editing a virtual dataset
- When defining a custom metric
- (Any other places?)

Each one uses a different component for the user to type SQL. Ideally, we should use a single component, making the experience consistent and having features available everywhere (for example, autocompleting function names is only available in SQL Editor).

### Expected results

Consistent UX and UI.

### Actual results

- NO autocomplete when editing a virtual dataset's query
- No autocomplete for *function names* when defining a custom metric via SQL

#### Screenshots

N/A

#### How to reproduce the bug

N/A

### Environment

(please complete the following information):

- superset version: master
- python version: N/A
- node.js version: N/A

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

N/A",,,,,,,,,,,,,,
12897,OPEN,"[Superset SPA] merge various apps into a single entry (using lazy loading for each ""app"")",enhancement:committed,2021-02-02 18:49:20 +0000 UTC,nytai,In progress,,"Part of project 
https://github.com/apache/superset/projects/10",,,nytai,"
--
Actually, this is likely blocked by needing to normalize our store. Each app, aside from mounting a component, also sets up redux with an initial state. 
--
",,,,,,,,,,
12896,OPEN,"[Superset SPA] Set up lazy loading of the various ""apps""",enhancement:committed,2021-02-02 18:49:41 +0000 UTC,nytai,Opened,,"Part of superset SPA frontend project 

https://github.com/apache/superset/projects/10",,,,,,,,,,,,,,
12895,OPEN,"unhealthy ""app"" and ""worker"" docker containers",install:docker,2021-03-19 19:13:15 +0000 UTC,shemExelate,Opened,,"after pulling latest and running `docker-compose up` inside `/docker` folder
`docker ps -a` prints 
```
CONTAINER ID   IMAGE                        COMMAND                  CREATED          STATUS                      PORTS                              NAMES
0974b8deb211   apache/superset:latest-dev   ""/usr/bin/docker-ent""   15 minutes ago   Up 15 minutes (unhealthy)   8080/tcp, 0.0.0.0:8088->8088/tcp   superset_app
998556009d30   apache/superset:latest-dev   ""/usr/bin/docker-ent""   15 minutes ago   Up 15 minutes (unhealthy)   8080/tcp                           superset_worker
8a3b7fc9dd0e   node:12                      ""docker-entrypoint.s""   15 minutes ago   Up 15 minutes                                                  superset_node
e16fc861fbf8   apache/superset:latest-dev   ""/usr/bin/docker-ent""   15 minutes ago   Exited (0) 6 minutes ago                                       superset_init
50c8ddf6536b   apache/superset:latest-dev   ""/usr/bin/docker-ent""   15 minutes ago   Exited (1) 15 minutes ago                                      superset_tests_worker
b98e572ece92   postgres:10                  ""docker-entrypoint.s""   15 minutes ago   Up 15 minutes               127.0.0.1:5432->5432/tcp           superset_db
963658d2a1a8   redis:3.2                    ""docker-entrypoint.s""   15 minutes ago   Up 15 minutes               127.0.0.1:6379->6379/tcp           superset_cache
```

### Expected results

they should be healthy :)

### Actual results

unhealthy, affects debugging 

#### How to reproduce the bug

1. git clone https://github.com/apache/superset.git
2. cd superset/docker
3. docker-compose up
4. wait for init container to finish (also unhealthy)

### Environment

dockerized

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,ansidev,"
--
Ref Link: https://github.com/amancevice/docker-superset/issues/51#issuecomment-759349896
This problem address: https://github.com/apache/superset/blob/fa072cd74e5320b4b8c0c234835d2d4fb677da9e/Dockerfile#L116
--

--
> This may be due to a misconfigured health check for workers (celery beat and worker)

I 've updated my comment. The problem came from `Dockerfile`.
--

--
> right, that healthcheck will work for the webapp contianer but not the worker containers since no server is running

So the problem is we are using same Docker image for both `app` and `worker`. IMO, We should use separate Docker image for worker with different HEALTHCHECK command
--

--
> something like this should work:
> https://stackoverflow.com/a/62364527/13977852

Yes, I can do that. But we need a solution for everyone, not only me.
--
",nytai,"
--
This may be due to a misconfigured health check for workers (celery beat and worker)
--

--
right, that healthcheck will work for the webapp contianer but not the worker containers since no server is running 
--

--
You can override the healthcheck command using docker-compose 
--

--
something like this should work:
https://stackoverflow.com/a/62364527/13977852
--

--
When you get it working, please open a PR :) 
--
",,,,,,,,
12888,OPEN,how to add custom style to mapbox?,question; viz:chart-maplayer,2021-02-23 02:23:21 +0000 UTC,Kingflyinger,In progress,,"I have searched the way to add custom style to mapbox and found nothing.
but the six existing styles  is too boring, so anyone knows how  to add custom style to mapbox?
",,,Kingflyinger,"
--
is there anyone knows the way? I`m a new learner,but I do feel there is an easy way to add a custom style to the chart.please help me out 
--
",candy4290,"
--
I have the save question, how to fix it?help
--
",,,,,,,,
12881,OPEN,can't connect clickhouse db using superset occur ERROR: Could not load database driver: clickhouse,data:connect:clickhouse; question,2021-02-12 18:51:52 +0000 UTC,calipeng,In progress,,"URI String is : clickhouse://gpsec:xxx@xxx:8858/sec

docker-compose logs -f -t superset

uperset_app             | 2021-02-02T03:23:36.323066230Z 172.17.0.233 - - [02/Feb/2021 03:23:36] ""GET /api/v1/database/_info?q=(keys:!(permissions)) HTTP/1.1"" 200 -
superset_app             | 2021-02-02T03:23:36.323076390Z INFO:werkzeug:172.17.0.233 - - [02/Feb/2021 03:23:36] ""GET /api/v1/database/_info?q=(keys:!(permissions)) HTTP/1.1"" 200 -
superset_app             | 2021-02-02T03:23:36.377123020Z 172.17.0.233 - - [02/Feb/2021 03:23:36] ""GET /static/assets/images/favicon.png HTTP/1.1"" 200 -
superset_app             | 2021-02-02T03:23:36.377136655Z INFO:werkzeug:172.17.0.233 - - [02/Feb/2021 03:23:36] ""GET /static/assets/images/favicon.png HTTP/1.1"" 200 -
superset_app             | 2021-02-02T03:23:48.713990050Z DEBUG:superset.models.core:Database.get_sqla_engine(). Masked URL: clickhouse://gpsec:XXXXXXXXXX@10.200.xx.xx:8858/sec
superset_app             | 2021-02-02T03:23:48.716697915Z DEBUG:superset.stats_logger:[stats_logger] (incr) DatabaseRestApi.test_connection.error
superset_app             | 2021-02-02T03:23:48.716723303Z DEBUG:superset.stats_logger:[stats_logger] (timing) DatabaseRestApi.test_connection.time | 7.412515580654144
superset_app             | 2021-02-02T03:23:48.717956405Z 172.17.0.233 - - [02/Feb/2021 03:23:48] ""POST /api/v1/database/test_connection HTTP/1.1"" 422 -
superset_app             | 2021-02-02T03:23:48.717973565Z INFO:werkzeug:172.17.0.233 - - [02/Feb/2021 03:23:48] ""POST /api/v1/database/test_connection HTTP/1.1"" 422 -

what you expected to happen.

### Actual results

what actually happens.

#### Screenshots

If applicable, add screenshots to help explain your problem.
![image](https://user-images.githubusercontent.com/60840837/106553464-6751bb80-6554-11eb-96b8-33b58d7973a7.png)

#### How to reproduce the bug

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

### Environment

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,calipeng,"
--
dba@noslt-db7:~/incubator-superset$ ./venv/bin/superset --version
Python 3.7.3
Flask 1.1.2
Werkzeug 1.0.1
--

--
ClickHouse server version 20.9.3.45 (official build).
--
",raphaelauv,"
--
same for me , maybe **sqlalchemy-clickhouse** is not part of the docker image of dev

using the docker-compose of the repo for superset

```shell
superset/superset$ docker-compose up -d
```

 and this clickhouse-server :

```shell
docker run --rm -e CLICKHOUSE_DB=my_database -e CLICKHOUSE_USER=username -e CLICKHOUSE_PASSWORD=password -p 9000:9000/tcp yandex/clickhouse-server:21.1.2
```

with this SQLALCHEMY URI :

```
clickhouse://username:password@localhost:9000/my_database
```
--

--
Alright ! thank @nytai 

by adding sqlalchemy-clickhouse inside superset/.docker/requirements-local.txt

but I still have error connecting to clickhouse, even when I add it to the docker-compose.yml of the repo 

```yml
clickhouse_db:
  image: yandex/clickhouse-server:20.8
  environment:
    CLICKHOUSE_DB: my_database
    CLICKHOUSE_USER: username
    CLICKHOUSE_PASSWORD: password
  ports:
    - 9000:9000
    - 8123:8123
```

URI : 
```
clickhouse://username:password@clickhouse_db:9000/my_database
```

```java
DEBUG:superset.models.core:Database.get_sqla_engine(). Masked URL: clickhouse://username:XXXXXXXXXX@clickhouse_db:9000/my_database
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): clickhouse_db:9000
DEBUG:superset.stats_logger:[stats_logger] (incr) DatabaseRestApi.test_connection.error
DEBUG:superset.stats_logger:[stats_logger] (timing) DatabaseRestApi.test_connection.time | 11.717678004060872 
172.19.0.1 - - [02/Feb/2021 21:33:04] ""POST /api/v1/database/test_connection HTTP/1.1"" 422 -
INFO:werkzeug:172.19.0.1 - - [02/Feb/2021 21:33:04] ""POST /api/v1/database/test_connection HTTP/1.1"" 422 -
```

Any idee ? thank again
--

--
With dbeaver it's working : jdbc:clickhouse://localhost:8123/my_database

![Screenshot from 2021-02-02 23-21-21](https://user-images.githubusercontent.com/10202690/106670495-6ac56100-65ad-11eb-8890-9204dcb77c0a.png)


but with superset theses URI :
```
clickhouse://username:password@localhost:8123/my_database
```
or
```
clickhouse://username:password@clickhouse_db:8123/my_database
```
don't work

--

--
@suedschwede thank for your help

I changed the clickhouse docker-compose configuration to :
```yml
clickhouse_db:
  image: yandex/clickhouse-server:20.8
  environment:
     CLICKHOUSE_DB: my_database
  ports:
    - 9000:9000
    - 8123:8123
```

I can connect from dbeaver without setting a user and password.

And this URI work in superset

> clickhouse://clickhouse_db:8123/my_database

![Screenshot from 2021-02-10 10-33-39](https://user-images.githubusercontent.com/10202690/107491475-7a374200-6b8b-11eb-8ec5-a93eefe00c80.png)


@hodgesrm not providing the port also work when there is no defined user and password 
> clickhouse://clickhouse_db/my_database


-----


I will give a try to pip install infi.clickhouse_orm==1.0.4
--

--
@anilvpatel21 I don't understand your question and also what is the relation with superset :confused: 

--
",nytai,"
--
There's info for how to add python db drivers in the docker docs

https://github.com/apache/superset/blob/676e0bb28211ec8e588abb8898924771d3dd6f32/docker/README.md#local-packages
--

--
Looks like you got past the missing driver error though. I suspect your connection string is wrong, are you sure it's not port `9001`? You could try to connect to your db outside of the superset context to make sure a connection is possible 
--
",hodgesrm,"
--
Try leaving off the port.  The ClickHouse SQLAlchemy driver uses native tcp connectivity and should connect to port 9000.  If you leave it off the driver will do the right thing. 
--
",suedschwede,"
--
If you use a password for the default user in clickhouse, you need a different infi.clickhouse_orm version

pip uninstall infi.clickhouse_orm
pip install infi.clickhouse_orm==1.0.4



--
",anilvpatel21,"
--
Is there any way to connect with multiple hosts, as a clickhouse driver support alt_hosts as a parameters. Any help?
https://clickhouse-driver.readthedocs.io/en/latest/
--

--
alt_hosts list of alternative hosts for connection. Example: `clickhouse://username:password@host:port/database?alt_hosts=host1:port1,host2:port2`

It will assure that if the host is failing it will connect to the alternate host for the high availability. 

As in Apache superset the parameters are not acceptable, may be it's a problem of sqlachemy-clickhouse drive. 

Also I tried clickhouse-sqlachemy driver which depends on clickhouse-driver and the driver gets loaded and connection is established using TCP protocol, but somehow did not received acknowledgement. It also a problem of the driver here. 

 How to assure high availability of clickhouse database while connecting using superset?

8123 worked because it runs on the http protocol.
--
"
12860,OPEN,Presto incorrect order of column type mappings,#bug,2021-02-01 13:47:30 +0000 UTC,rmgpinto,Opened,,"Presto has incorrect order of column type mappings.
`timestamp` is overriden with `time` due to these lines:

```
(re.compile(r""^time.*"", re.IGNORECASE), types.Time()),
(re.compile(r""^timestamp.*"", re.IGNORECASE), types.TIMESTAMP())
```

causing incorrect query statement to be passed from superset to presto.

### Expected results

Query statements to have correct column type/cast.

### Actual results

The query is incorrect.

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,,,,,,,,,,,,
12855,OPEN,Facing problem while configuring OAuth,install:config,2021-02-01 10:17:16 +0000 UTC,shikhnu,Opened,,"Hi,

While configuring oAuth, I am encountering some issues. I am using superset with the help of docker. 
System: Linux (ubuntu 18.04)
File location: /home/myuserx/incubator-superset/docker/pythonpath_dev/superset_config.py

Below is the code used in superset_config.py
```
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
# This file is included in the final Docker image and SHOULD be overridden when
# deploying the image to prod. Settings configured here are intended for use in local
# development environments. Also note that superset_config_docker.py is imported
# as a final step as a means to override ""defaults"" configured here
#

import logging
import os
from flask_appbuilder.security.manager import (
    AUTH_OID,
    AUTH_REMOTE_USER,
    AUTH_DB,
    AUTH_LDAP,
    AUTH_OAUTH
)
#from flask_appbuilder.security.manager import AUTH_OID, AUTH_REMOTE_USER, AUTH_DB, AUTH_LDAP, AUTH_OAUTH
basedir = os.path.abspath(os.path.dirname(__file__))
ROW_LIMIT = 5000
SUPERSET_WORKERS = 4
SECRET_KEY = 'a long and random secret key'
SQLALCHEMY_DATABASE_URI ='postgresql://superset:superset@db:5432/superset'
CSRF_ENABLED = True
AUTH_TYPE = AUTH_OAUTH
AUTH_USER_REGISTRATION = True
AUTH_USER_REGISTRATION_ROLE = ""Public""
OAUTH_PROVIDERS = [
        {
            'name':'google', 'icon':'fa-google', 'token_key':'access_token',
        'remote_app': {
            'client_id':'GOOGLE KEY',
            'client_secret':'GOOGLE SECRET',
            'api_base_url':'https://www.googleapis.com/oauth2/v2/',
            'client_kwargs':{
              'scope': 'email profile'
            },
            'request_token_url':None,
            'access_token_url':'https://accounts.google.com/o/oauth2/token',
            'authorize_url':'https://accounts.google.com/o/oauth2/auth'}
    }
 ]

```
Below is the error that I am encountering.
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/flask/_compat.py"", line 39, in reraise
    raise value
  File ""/usr/local/lib/python3.7/site-packages/flask/cli.py"", line 184, in find_app_by_string
    app = call_factory(script_info, attr, args)
  File ""/usr/local/lib/python3.7/site-packages/flask/cli.py"", line 119, in call_factory
    return app_factory()
  File ""/app/superset/app.py"", line 69, in create_app
    raise ex
  File ""/app/superset/app.py"", line 62, in create_app
    app_initializer.init_app()
  File ""/app/superset/app.py"", line 551, in init_app
    self.init_app_in_ctx()
  File ""/app/superset/app.py"", line 520, in init_app_in_ctx
    self.configure_fab()
  File ""/app/superset/app.py"", line 591, in configure_fab
    appbuilder.init_app(self.flask_app, db.session)
  File ""/usr/local/lib/python3.7/site-packages/flask_appbuilder/base.py"", line 202, in init_app
    self.sm = self.security_manager_class(self)
  File ""/usr/local/lib/python3.7/site-packages/flask_appbuilder/security/sqla/manager.py"", line 51, in __init__
    super(SecurityManager, self).__init__(appbuilder)
  File ""/usr/local/lib/python3.7/site-packages/flask_appbuilder/security/manager.py"", line 250, in __init__
    from authlib.integrations.flask_client import OAuth
ModuleNotFoundError: No module named 'authlib'

```

I have also attached a screenshot of the error. I would appreciate it if anyone can help me out in resolving the issue.

Regards.
![Error_SS](https://user-images.githubusercontent.com/56694179/106431157-d8309f00-6492-11eb-83fd-80b540f0e33f.png)
",,,dpgaspar,"
--
Authentication is supported by FAB and to use OAuth you have to install authlib here's the documentation: https://flask-appbuilder.readthedocs.io/en/latest/security.html#authentication-oauth

--
",,,,,,,,,,
12850,OPEN,[database connection]Bar Chart with Presto does not add ORDER BY when using limit rows,P1; bash!; data:connect:athena; data:connect:presto; viz:chart-bar,2021-02-10 06:42:46 +0000 UTC,eugeniamz,Opened,,"When creating a bar chart with the source database Presto / Athena, the SQL does not include ORDER BY with LIMIT rows. Not having ORDER BY with LIMIT does not warrant that the right data is shown. 

``` 
SELECT ""country"" AS ""country"",
       sum(""new_cases"") AS ""SUM(new_cases)""
FROM ""default"".""cases_ts""
GROUP BY ""country""
LIMIT 100
```
<img width=""1010"" alt=""Screen Shot 2021-01-31 at 11 09 45 AM"" src=""https://user-images.githubusercontent.com/58375897/106390129-e6a88900-63b4-11eb-9886-841b55194114.png"">

I tried Bar Chart with Vertica and the ORDER BY shows. 

I changed the chart to Pie with Presto Datasource and the ORDER BY Also shows 
```
SELECT ""country"" AS ""country"",
       sum(""new_cases"") AS ""SUM(new_cases)""
FROM ""default"".""cases_ts""
GROUP BY ""country""
ORDER BY ""SUM(new_cases)"" DESC
LIMIT 100
```
<img width=""1101"" alt=""Screen Shot 2021-01-31 at 11 07 58 AM"" src=""https://user-images.githubusercontent.com/58375897/106390100-c4af0680-63b4-11eb-852b-a8fb6a659833.png"">


#### How to reproduce the bug

1. Go to - Chart and select a datasource that is from Presto query engine and chart type ```bar chart```. Review query and query won't have the order by 
2. Change the chart type to ```Pie```, review the query and it will include the ORDER BY 


### Environment
Superset 1.0 
Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ X] I have reproduced the issue with at least the latest released version of superset.
- [ X] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,KentonParton,"
--
@eugeniamz thank you for adding this item.

It appears to be impacting Redshift, Postgres, and MySql in addition to Presto/Athena. There may be more databases.

To keep charts consistent, should we add a Sort Descending toggle as we have on the Time-series Bar Chart?
--

--
@daniel10012 @zhaoyongjie  I have checked in LINT and LIVE environment and not seeing order by in the query as shown below. I am running release 1.0.0. Are there any recommendations to resolve this?

**Redshift/Postgres**
<img width=""890"" alt=""Screen Shot 2021-02-01 at 11 29 12 AM"" src=""https://user-images.githubusercontent.com/20202312/106440003-3bf5a080-6470-11eb-85d7-f9e7d4665fa4.png"">

**Athena**
![image](https://user-images.githubusercontent.com/20202312/106440224-824aff80-6470-11eb-8aab-32c46ce9d35a.png)

--

--
Thanks for verifying @zhaoyongjie 
--
",junlincc,"
--
Thank you both for reporting with details. We will look into it.
@zuzana-vej I know you guys use Presto, is it also affecting you? 

https://github.com/apache/superset/pull/12661 should be able to solve the sort descending issue, I will double check when can it get to next minor release. @KentonParton @eugeniamz 

--
",daniel10012,"
--
@KentonParton I have tested it on Postgres, Redshift, and MySQL and the Order By appears for bar charts and pie charts for me:
![image](https://user-images.githubusercontent.com/10360991/106406472-5cd5db80-6407-11eb-93be-969fbeaa3f6e.png)

@eugeniamz @junlincc 
--
",zhaoyongjie,"
--
Hi @eugeniamz   
I have tested it on Presto(0.235-cb21100), the query with `order by` and `limit` clause. It seems to work

![image](https://user-images.githubusercontent.com/2016594/106423324-2e094500-649b-11eb-8532-f1641804e249.png)

![image](https://user-images.githubusercontent.com/2016594/106423240-06b27800-649b-11eb-9685-80fbf1e04588.png)



--

--
@KentonParton @eugeniamz 
I reproduced this bug in 1.0, but the latest master code works fine. 

this PR fixed this bug 
https://github.com/apache/superset/commit/f2b802978d99b0cf329520c120b2decd5953ddd0

<img width=""1306"" alt=""image"" src=""https://user-images.githubusercontent.com/2016594/106463492-742dcb00-64d2-11eb-99fc-832e51f01bac.png"">

![image](https://user-images.githubusercontent.com/2016594/106463552-8d367c00-64d2-11eb-9935-942e418fa9a7.png)


--
",bryanck,"
--
FYI, this also impacts the word cloud chart, and the line charts that use group by. See:
https://github.com/apache/superset/pull/12674

--
",,
12847,OPEN,[contributor wanted]Tree diagram Echarts integration,enhancement:request; viz:echarts,2021-03-03 19:14:22 +0000 UTC,junlincc,In progress,,"**Motivation**
Superset offers a wide range of visualizations to serve different data structures and tell story users to convey through data. Looking at the 46+ chart types currently available in Superset viz selection, most of them(time series, composition, and distribution charts) focus on data analytics. The robust ability to turn the immense amount of enterprise data into actionable business insights has been an advantage of Superset in the BI market.
One area that we would like to improve is to provide more viz types as a tool for story telling, project management and visualizing relationships. We currently offer Sankey Diagram, Force-directed Graph, Chord Diagram that are partially serving these purposes. Still, they are far away from enough to make Superset stand out in the BI landscape.

**Proposed Solution**
Therefore, as part of Echarts viz enhancement project, we would like to introduce more relational diagrams and flow charts to Superset. To name a few that are on our radar, **Event tree diagram, Funnel charts, Process flow diagram, Social network, etc.** As Echarts offers most of the above as out-of-the-box solutions, we anticipate the experiment will be relatively smooth, but the entire process may take a couple of quarters without helps and contributions from the bigger community.

To kick off this open-project, we are going to start with integrating Echarts **Tree diagram**.
A tree diagram is particularly useful in
1)recording and analyzing possible actions in events and
2)management planning that depicts the hierarchy of tasks and subtasks needed to complete an objective.

**Calling for help from the community to signup for project(tree diagram) ownership and contribute  
Superset data viz team will be offering all necessary support in engineering, product and design, and more.**

Related: https://github.com/apache/superset/issues/7286 
Roadmap item:https://github.com/apache-superset/superset-roadmap/issues/150

![0fd46e74-f527-440a-8382-25268f370e58](https://user-images.githubusercontent.com/67837651/106368472-8f47e180-62fe-11eb-834b-67471a95d51e.png)
![decision-tree-diagram-kI4sJiM7lTW-thumb](https://user-images.githubusercontent.com/67837651/106368476-9111a500-62fe-11eb-916d-41458efad69e.png)


Echarts Tree diagram
<img width=""1817"" alt=""Screen Shot 2021-01-30 at 12 20 10 PM"" src=""https://user-images.githubusercontent.com/67837651/106368482-953dc280-62fe-11eb-9c2d-023c7e8744e6.png"">

",,,anushkrishnav,"
--
I would like it work on the project with some guidance , sounds very interesting.

--

--
Is there a IRC or slack channel where I can seek guidance or talk to mentors, I am interested to work with Apache at GSOC 21 I wanted to understand the codebase and how I can be helpful.
--

--
Sounds great @villebro , if you can share some prior examples and things we need to prior to starting this that would be useful, we can get ready so that once it kicks off we can jump right in and start working on it. I am currently learning about Event tree diagrams, Funnel charts, Process flow diagrams.
--

--
Looking great, I guess having a bit of Typescript knowledge will be useful
@villebro Is there a slack channel I can join to talk more about it or seek guidance 
--

--
I am in, Will join shortly
--
",srinify,"
--
I will also raise my hand to help! @villebro @junlincc 
--
",junlincc,"
--
@anushkrishnav @srinify thank you so much both for your interest! @villebro needs to lay some groundwork first to make this process easier for you guys. We will reach out and kick off the project by the end of next week. 

@austinpray are you in Superset slack? 

@mistercrunch so glad to see that we have more and more contributors from the community. 
--

--
@mayurnewase hey Mayur thanks for volunteering AGAIN!  cc @villebro  ^ 
since @anushkrishnav is also interested in contributing, why don't you two work along side? we need help in both tree diagram and funnel chart. 
Please join #echarts-volunteer in slack to coordinate. many thanks to you both! 
--

--
We ran a performance comparison with same dataset in NVD3 and Echarts line chart on the dashboard. 
Applying time grain filter to compare
going from time-grain = year (9 rows) -> quarter (33 rows) ->  week (2.5k rows) -> no time grain (10k rows)
At 2.5k rows, NVD3 takes much longer to render 
At 10k rows, stuck entirely... 

By proofing Echarts migration's benefits from ALL angles(functionality, performance, UX and extensibility), we hope to attract more people from the community to contribute and expedite this project. 

![grain2](https://user-images.githubusercontent.com/67837651/109858150-527a4d80-7c10-11eb-82f3-4f6b1071e3ce.gif)

Thanks for all the hard work of the superset-echarts team for the improvement @villebro @mayurnewase 


--
",kamalkeshavani,"
--
I would like to contribute but not sure what skills are required for it. I am comfortable with Python, so let me know if I can be of any help.
--
",villebro,"
--
Thanks for volunteering @anushkrishnav and @kamalkeshavani-aiinside, this is awesome!   We're currently cleaning up some typing issues that changed when upgrading from ECharts 4.x to 5.x (FYI @maloun96 who is looking into this right now). Once we have those cleaned up it should be easier to start work on these new plugins. @mayurnewase is currently working on the graph viz, (see https://github.com/apache-superset/superset-ui/pull/918) so we'll probably prioritize getting that one finished first + address any issues that we surface during that migration work to make sure subsequent ECharts ports are as easy to implement as possible. We'll keep you posted!
--

--
@anushkrishnav while we work on this, please check out the examples on the ECharts homepage, there's loads of good examples there to draw inspiration from. Also feel free to check out what the current ECharts implementations look like: https://github.com/apache-superset/superset-ui/tree/master/plugins/plugin-chart-echarts 
--
",mayurnewase,"
--
Is there any progress on this?
If not I would like to work on it.
--
"
12843,OPEN,Option to select limited values of filter,need:more-info; need:screenshot,2021-01-31 18:04:32 +0000 UTC,cnuonline,Opened,,"*Please make sure you are familiar with the SIP process documented*
(here)[https://github.com/apache/superset/issues/5602]

## [SIP] Proposal for having Radio button or dropdown or limit the user to select filter values

### There are few usecases where the data sliced (pre-prepared) should not have multiple selection of filter values.

Description of the problem to be solved.

### Proposed Change

Dashboard filters should have option to select  the number of filter values associated to it.


",,,junlincc,"
--
> having Radio button or dropdown or limit the user to select filter values

@cnuonline  it seems to me this is a proposal of UI change in filterbox, could you provide a wireframe? thanks! 

--
",,,,,,,,,,
12842,OPEN,Update CTAS Schema from text field to a dropdown,,2021-02-03 17:08:53 +0000 UTC,yousoph,Opened,,"## Description
Current CTAS Schema field is a text field but users need to input the name of an existing schema for CTAS/CVAS to work as expected. 
Instead of requiring the user to input the schema name, we should update the text field to a dropdown list of existing schemas
(This is a follow up to Issue #12470) 

## Screenshot
![Add Modal - SQL Lab Settings @1456 (1)](https://user-images.githubusercontent.com/10627051/106341138-07f06480-6251-11eb-961b-9c5bdf86dac4.jpg)

https://www.figma.com/file/NbuTOMaUdv2GCBoDyihdLw/Database-CRUD---P0?node-id=479%3A39226",,,mistercrunch,"
--
I think it's a jinja-templated field and allows something like `sandbox-{{username}}`, let's make sure to cover that use case and make it clear the help-text underneat!
--

--
BTW this field is not used very often and typically only by admins, so it's probably not a big deal to have them type the name, still nice-to-have.
--
",Steejay,"
--
@mistercrunch just to clarify, users can actually _create_ new schemas in this field not just select existing schemas?
--
",,,,,,,,
12834,OPEN,Page routing does not respect next parameter,viz:explore:others,2021-01-31 18:30:55 +0000 UTC,mayurnewase,Opened,,"When I enter chart url directly without logging in,ui redirect to login page keeping chart url in 'next' parameter.
After logging in it redirects to homepage ignoring that url.
### Expected results
Should go to url which was used before logging in.

#### How to reproduce the bug
log in
open any chart,copy its url from address bar
log out
enter that url in address bar
log in,you would see homepage",,,,,,,,,,,,,,
12830,OPEN,Grand Total For Tabular report,,2021-01-29 12:24:03 +0000 UTC,nirajtambat,Opened,,"Hello
Need some help on below issue.
**Tabular Report -**
Please advise how do we have Grand Total for tabular report in both ways column wise and row wise.
how can we have sign for numbers in tabular report . like for prices column we want to show $ sign.
**Pivot table -**
whenever we rename the metrics pivot tables gives error. e.g. sum(qty) we can not give label for this metics. please advise. also sorting is not working on piovt table.
**World Map -** 
World map does not work for city names. and we can have more than one metircs there.
**Other issue -**
how to show text messages on dashboard, I want to show some information around 20/30 lines as information we don't see header or markup is useful here. please advise.
how to show Hierarchical or Tree structure data in superset. don't see any viz like this.

Please advise.
",,,,,,,,,,,,,,
12827,OPEN,Docker for testing Superset Failed in local,#bug,2021-03-16 18:28:08 +0000 UTC,Tana8M,Opened,,"A clear and concise description of what the bug is Docker

### Expected results
-I want to test superset dashboard.


### Actual results
-When I login page , that haven't anything in localhost:8088 (http://localhost:8088/superset/welcome)


#### Screenshots

![Screen Shot 2564-01-29 at 15 50 25](https://user-images.githubusercontent.com/53141452/106253601-4898af80-624a-11eb-959a-2f93dd668802.png)


",,,singh,"
--
 did you run `superset-init` ? it takes a while for all the assets to load 

Would make it easier if you can share some logs .
--
",SuzGori,"
--
same issue.

superset-init done.

but same page.

woops.

superset-node is don't finished...
--

--
@yerrochdi 
test_worker is not important for me.
this is ""test"".
exit code 0 and work is important for me.
--
",acvander,"
--
I experienced the same issue earlier. I followed the instructions in #9880 and increased the memory allocation in Docker to 8GB. 

The `superset-node` service does take a long time to complete installing the node modules. For me it took at least 20 minutes. You can follow the logs using ```docker-compose logs superset-node```. 

My logs looked like this while it was installing the modules
`
npm WARN using --force I sure hope you know what you are doing.
/usr/local/bin/webpack-cli -> /usr/local/lib/node_modules/webpack-cli/bin/cli.js
/usr/local/bin/webpack -> /usr/local/lib/node_modules/webpack/bin/webpack.js
 webpack-cli@4.4.0
 webpack@5.19.0
added 131 packages from 161 contributors in 20.531s
npm WARN using --force I sure hope you know what you are doing.
`
--
",yerrochdi,"
--
same issue.
- superset_init not running 
superset_init            | ######################################################################
superset_init            | 
superset_init            | 
superset_init            | Init Step 4/4 [Complete] -- Loading examples
superset_init            | 
superset_init            | 
superset_init            | ######################################################################
superset_init            | 
superset_init exited with code 0

- superset_tests_worker not runing 
""Flask-Caching: CACHE_TYPE is set to null, ""
ERROR:flask_appbuilder.security.sqla.manager:DB Creation and initialization failed: (psycopg2.OperationalError) FATAL:  database ""test"" does not exist

thank you for help

--
",gorcurek,"
--
did you guys follow https://superset.apache.org/docs/installation/installing-superset-using-docker-compose ? I have just tried it and got no error, all is running well on http://localhost:8088

I also tried: **$ docker run -p 8080:8080 -d apache/incubator-superset** which started superset container.
I then got into the container with **$ docker exec -it <container_id> /bin/bash**, ran **superset db upgrade** and **superset init**, created an admin account with **superset fab create-admin** and all is working in that part as well.
--
",vineetgupta02,"
--
I followed this link and was able to get superset up and running with just one docker image 
https://hub.docker.com/r/apache/superset 

hope this helps.
--
"
12826,OPEN,[explore][ux]prompt user to save edited chart before leaving the page,enhancement:committed; good first issue; viz:explore:ux,2021-02-16 06:47:40 +0000 UTC,junlincc,Opened,,"UX enhancement: after editing a chart, users leave from Explore to other product area without knowing their changes have been saved or not.(current behavior - not saved).
proposed change: if a chart is edited, before users leave Explore page, they should receive a message to confirm either to save or to abort changes. 

https://user-images.githubusercontent.com/67837651/106237595-fa38d180-61b3-11eb-8d6c-4a505d142769.mov

",,,,,,,,,,,,,,
12820,OPEN,Use python code as plugin,viz:dynamic-plugins,2021-02-11 14:39:46 +0000 UTC,GuillaumeTh,In progress,,"I would like to know if it is possible to use python code as plugin instead of TypeScript code.

As an example, I would like to perform a pearson correlation. I already have this code on my hand but I would like to use it with superset interface. Is it possible to do that ? If yes do you have a documentation about that ?

Thanks

Guillaume",,,ktmud,"
--
Currently you can add [Pandas post processing](https://github.com/apache/superset/blob/4d329071a1651994574c3ad1f26e32b56d9ff810/superset/utils/pandas_postprocessing.py#L0-L1) for certain charts (those using the new API endpoint), but not all of them.

Although for your specific case, Pearson correlation is also very easy to calculate [in JavaScript](https://gist.github.com/matt-west/6500993). For reference, we do have a [Paired t-test chart](https://superset-ui.superset.vercel.app/?path=/story/legacy-chart-plugins-legacy-plugin-chart-paired-t-test--basic) maybe you can extend from.


--
",villebro,"
--
@GuillaumeTh I have actually created a viz that's able to run Python code and render just like regular charts, but it's not quite ready for public consumption yet. In the meantime, would you be able to shar what your preferred workflow would look like? I assume you would like to be able to write regular python code, much like in a notebook, and then render a matplotlib or similar chart in the chart window?
--
",GuillaumeTh,"
--
@ktmud Thanks I will check that. In our case Pearson is a small example. We have more complex stats already in python. @villebro currently I have my own Flask server and I get infos from html dropdown. After that a python code run pearson correlation on selected data and display table and graphs in the HTML Flask page. Is it fit in your current workflow ?
--

--
Hi @villebro I am interesting to test your python support and give you feed back if you want. This python support is important for us and could be a game changer.
--
",aliJabra,"
--
Going down the same route, I found these [SQL based functions](https://www.red-gate.com/simple-talk/blogs/statistics-sql-pearsons-correlation/) useful for 2d series, but I'm looking forward to @villebro's python-bridge because nothing beats pandas.corr() s simplicity in this case. 
--
",,,,
12814,OPEN,[import]Importing a dashboard via API fails with error 500,bug; dashboard:import,2021-02-01 08:46:37 +0000 UTC,ktecho,In progress,,"While importing an exported dashboard (.json file) from an older version of Superset (0.36.0) through the API of a Superset 1.0.0 (using built in Swager), it errors out with Error 500 and this trace:

```
File is not a zip file
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/flask_appbuilder/api/__init__.py"", line 84, in wraps
    return f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/superset/views/base_api.py"", line 79, in wraps
    duration, response = time_function(f, self, *args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/superset/utils/core.py"", line 1286, in time_function
    response = func(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/superset/dashboards/api.py"", line 705, in import_
    with ZipFile(upload) as bundle:
  File ""/usr/lib/python3.8/zipfile.py"", line 1269, in __init__
    self._RealGetContents()
  File ""/usr/lib/python3.8/zipfile.py"", line 1336, in _RealGetContents
    raise BadZipFile(""File is not a zip file"")
zipfile.BadZipFile: File is not a zip file
127.0.0.1 - - [28/Jan/2021 14:37:07] ""POST /api/v1/dashboard/import/ HTTP/1.1"" 500 -
```

From the documentation, it seems that it should just accept the .json file and not need a .zip, but just in case the file should be .zipped, I made a .zip file and now the error changes:

```
Expecting value: line 1 column 1 (char 0)
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/flask_appbuilder/api/__init__.py"", line 84, in wraps
    return f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/superset/views/base_api.py"", line 79, in wraps
    duration, response = time_function(f, self, *args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/superset/utils/core.py"", line 1286, in time_function
    response = func(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/superset/dashboards/api.py"", line 712, in import_
    json.loads(request.form[""passwords""])
  File ""/usr/lib/python3.8/json/__init__.py"", line 357, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python3.8/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python3.8/json/decoder.py"", line 355, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
127.0.0.1 - - [28/Jan/2021 14:38:06] ""POST /api/v1/dashboard/import/ HTTP/1.1"" 500 -
```

### Expected results

My dashboard imported without problems.

### Actual results

At first, trying to import .json file, the error is: `zipfile.BadZipFile: File is not a zip file`
After zipping the exported dashboard, the error is: `json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)`

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Go to an older Superset (0.36.0)
2. Navigate to the `Dashboards` menu, then click over one of your dashboards then `Actions / Export`. Save the .json file.
3. Go to Superset 1.0.0 in a different machine
4. Navigate to the function `/dashboard/import/` in http://localhost:8180/swagger/v1
5. Choose the previously exported dashboard for the `formData` parameter and click Execute
6. You get the aforementioned bugs.

### Environment

- superset version: 1.0.0
- python version: 3.8.5
- node.js version: none

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,ktecho,"
--
Now the entire description is ready.
--

--
I've been checking the export dashboard in Superset 1.0.0 and it doesn't export a zip file.

So I've tested an empty dashboard export in 1.0.0 and import in the same installation (1.0.0) and that doesn't work (error 500 described in the description of the issue).
--

--
While looking at this, I found the Superset CLI, and it seems the dashboard stuff is not coherent:

```
root@docker_host:/# superset | grep dashboard
  export-dashboards         Export dashboards to JSON
  import-dashboards         Import dashboards from ZIP file
```

If the export is in JSON format (like in previous versions or when using the UI), the import should be JSON too, right?
--

--
But using the command line, in the end works ok for JSON files this way:

`superset import-dashboards -p nyc.json`

So the API is broken (asks for a zipfile), but the CLI just have the description wrong:

`import-dashboards         Import dashboards from ZIP file`

should be:

`import-dashboards         Import dashboards from JSON file` 
--

--
Also, from the error messages during the process, I infered that you need `Pillow` module installed. Just in case you want to put it in the documentation somewhere.

I don't know why you need an image manipulation module to import a .json file into the database, but that's another story :)
--
",junlincc,"
--
@betodealmeida Beto, mind helping? thanks! 
--
",,,,,,,,
12809,OPEN,[alert]Alerts are not working; importing security_manager returns NoneType object,bug; global:alert,2021-01-28 19:28:44 +0000 UTC,VMois,Opened,,"After enabling ""Alerts"" and configuring one, I cannot see any triggers and email notifications being sent to me. I am sure that SQL query should return rows to trigger alert and, also, `Alert Log` is not showing any triggers. 

Using Celery for email notifications. Maybe, I am missing some Celery configs in `superset_config.py`?

### Expected results

I want to be able to trigger alerts.

### Actual results

Email notifications are working, it means something wrong with alerts. After checking the Docker logs, I found a possible issue, there is an error in `models/alerts.py` while importing `security_manager`. Not sure what causes this error. In addition, within the above error there is another error:

```
sqlalchemy.exc.InvalidRequestError: Table 'alert_owner' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.
```
More logs below.

#### Screenshots (logs)

Docker logs:

```
class Alert(Model):
  File ""/usr/local/lib/python3.6/site-packages/superset/models/alerts.py"", line 60, in Alert
    owners = relationship(security_manager.user_model, secondary=alert_owner)
  File ""/usr/local/lib/python3.6/site-packages/werkzeug/local.py"", line 347, in __getattr__
    return getattr(self._get_current_object(), name)
AttributeError: 'NoneType' object has no attribute 'user_model'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/bin/celery"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.6/site-packages/celery/__main__.py"", line 16, in main
    _main()
Loaded your LOCAL configuration at [/opt/superset/superset_config.py]
  File ""/usr/local/lib/python3.6/site-packages/celery/bin/celery.py"", line 322, in main
    cmd.execute_from_commandline(argv)
  File ""/usr/local/lib/python3.6/site-packages/celery/bin/celery.py"", line 499, in execute_from_commandline
    super(CeleryCommand, self).execute_from_commandline(argv)))
  File ""/usr/local/lib/python3.6/site-packages/celery/bin/base.py"", line 289, in execute_from_commandline
    argv = self.setup_app_from_commandline(argv)
  File ""/usr/local/lib/python3.6/site-packages/celery/bin/base.py"", line 509, in setup_app_from_commandline
    self.app = self.find_app(app)
  File ""/usr/local/lib/python3.6/site-packages/celery/bin/base.py"", line 531, in find_app
    return find_app(app, symbol_by_name=self.symbol_by_name)
  File ""/usr/local/lib/python3.6/site-packages/celery/app/utils.py"", line 376, in find_app
    sym = imp(app)
  File ""/usr/local/lib/python3.6/site-packages/celery/utils/imports.py"", line 111, in import_from_cwd
    return imp(module, package=package)
  File ""/usr/local/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 941, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/usr/local/lib/python3.6/site-packages/superset/sql_lab.py"", line 44, in <module>
    from superset.db_engine_specs import BaseEngineSpec
  File ""/usr/local/lib/python3.6/site-packages/superset/db_engine_specs/__init__.py"", line 36, in <module>
    from superset.db_engine_specs.base import BaseEngineSpec
  File ""/usr/local/lib/python3.6/site-packages/superset/db_engine_specs/base.py"", line 54, in <module>
    from superset.models.sql_lab import Query
  File ""/usr/local/lib/python3.6/site-packages/superset/models/__init__.py"", line 17, in <module>
    from . import (
  File ""/usr/local/lib/python3.6/site-packages/superset/models/alerts.py"", line 43, in <module>
    Column(""alert_id"", Integer, ForeignKey(""alerts.id"")),
  File ""<string>"", line 2, in __new__
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/util/deprecations.py"", line 139, in warned
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/sql/schema.py"", line 542, in __new__
    ""existing Table object."" % key
sqlalchemy.exc.InvalidRequestError: Table 'alert_owner' is already defined for this MetaData instance.  Specify 'extend_existing=True' to redefine options and columns on an existing Table object.
```

#### How to reproduce the bug

No idea.

### Environment

Docker image: python:3.6-jessie

- superset version: `0.37.2`
- python version: `3.6.9`
- node.js version: `not installed`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

No additional context
",,,,,,,,,,,,,,
12808,OPEN,[docs]API docs not clear about /dashboard/import,doc:user,2021-01-28 19:29:28 +0000 UTC,ktecho,In progress,,"In the [documentation page about the API](https://superset.apache.org/docs/rest-api), there should probably be an entry about the import of dashboards: /api/v1/dashboard/import

That url is working, so I think it's lacking in the docs. It would be good to know the parameters that I need to pass to import dashboards programmatically.",,,dpgaspar,"
--
@ktecho thank you for reporting,

We will fix it, meanwhile you can access online API docs by accessing your superset on `api/v1/swagger/v1`

--
",ktecho,"
--
For me it seems the online API docs are at just `/swagger/v1`. But thanks! That will help :)
--
",,,,,,,,
12807,OPEN,Celery Flower: How to check the Redis DB info when is the next trigger of a particular job.,question,2021-01-28 19:29:39 +0000 UTC,GangadharGuska,Opened,,"How to check the Redis DB Information of task when is the next trigger time, Clerey Flower shows the success and failed information. Is there possible way to get the information Superset API or CELERY componets

",,,,,,,,,,,,,,
12805,OPEN,[chart]Apache Superset Deck.GL Multiple layer map bug,bug:regression; viz:chart-deck.gl,2021-02-03 23:38:29 +0000 UTC,Geethanjali11,Opened,,"

### Expected results
Multi layer map with  coordinates near Houston.
[Multiple layer bug.zip](https://github.com/apache/superset/files/5885653/Multiple.layer.bug.zip)
### Actual results
Multi layer map with coordinates fixed on san Francisco sample data and is inflexible with any dataset change
what actually happens. 0 rows are being fetched.
#### Screenshots
![image](https://user-images.githubusercontent.com/78130557/106106817-19ac0c00-616c-11eb-9c64-60d8051fe1ad.png)
#### How to reproduce the bug

1.Upload the attached csv file . Open add new chart . Go to 'deck.gl multiple layers'
2. Click on 'scatter plot'

### Environment
- superset version: `Version: 0.999.0dev`
### Additional context
Please find attached the dataset used.
",,,VSZM,"
--
Also why is it not possible to customize the individual layers? 
--
",,,,,,,,,,
12802,OPEN,[explore]metrics list order should not change after editing metrics,bug; good first issue; viz:explore:dataset,2021-02-01 11:57:08 +0000 UTC,junlincc,Opened,,"Currently, after user editing a metric in Edit dataset modal in Explore, the edited metric jump to the bottom of the metric list. 
it create a few issues: 
1) when the metric list is long, by dropping to the bottom, user might take a while to find it or think the most recent edited metic is lost. 
2) every time users edit a single metric, they see a different order of metric list showing in the panel, which can be confusing

Proposed short-term change: the metric list should be arranged in the order of creation time

https://user-images.githubusercontent.com/67837651/106104741-aff21a80-60f7-11eb-9f5d-4d874b1bd152.mov


",,,anushkrishnav,"
--
If it hasn't been solved already , I can take up on it, is there any example I can refer to ?
--
",,,,,,,,,,
12801,OPEN,[explore]users should not be allowed to save metric without sql expression,enhancement:committed; viz:explore:dataset,2021-01-28 10:17:39 +0000 UTC,junlincc,Opened,,"currently, user can create a new metric in Edit dataset modal in Explore without sql expression, or without validating the new metric. 

the new metrics becomes available in metric popover dropdown for user.  once it is selected and run query, users receive a non human readable or non actionable error message, which leads to a bad user experience.. It also would become problematic once drag and drop query control is available. 

Proposed solution(shortterm): the modal should not allow user to save a metric without a VALID SQL expression. 
Long term - will require redesigning this modal as a whole.... 

https://user-images.githubusercontent.com/67837651/106103648-1f670a80-60f6-11eb-9637-8b67553c53df.mov


",,,zhaoyongjie,"
--
@junlincc 
The existing architecture of Superset has no way to verify SQL snippet(metric/derived column), so we can only guarantee that the metric expression is not empty.
--
",,,,,,,,,,
12790,OPEN,Unable login to the Superse when enabled CORS,question,2021-02-13 18:19:48 +0000 UTC,weihaonan,In progress,,"sueprsetcorsqiankunentryhttp://127.0.0.1:5000/login/?REMOTE_USER=adminsupersethttp://127.0.0.1:5000 
",,,zhaoyongjie,"
--
Hi @weihaonan 
what is `qiankun`

In chinese
. .
 qiankun ?

--
",junlincc,"
--
@zhaoyongjie thanks Yongjie for interacting with users in chinese. with your help i think Superset is ready to serve more users across the Pacific. 

@weihaonan and yes please fill out the issue template in English or Chinese as much as you can, so that we can help! thanks! 
--

--
not sure how we could help you. but you need to use SSO on Superset, can consider our [cloud offering](https://preset.io/)
--
",weihaonan,"
--
@zhaoyongjie Qiankun is an implementation of Micro Frontends, which based on single-spa. It aims to make it easier and painless to build a production-ready microfront-end architecture system.
 i want to lead superset into qiankun as a sub application
--
",amitmiran137,"
--
There are quite a few which are using superset embedded in an iframe within another frontend application with SSO in a standalone mode

If you configure it correctly all should work for you 
--
",,,,
12788,OPEN,[chart]old force-directed graph not re-render properly & echarts migration,enhancement:committed; viz:chart-forced; viz:echarts,2021-01-27 07:13:02 +0000 UTC,junlincc,Opened,,"the legacy force-directed graph fail to re-render when data table is open or resizing control panel. 

![force](https://user-images.githubusercontent.com/67837651/105955685-853a9000-602b-11eb-87a9-7ca880fd262e.gif)

As part of the Echarts migration plan, we are not planning to fix this legacy chart. Luckily, we will have new Echarts force-directed graph as replacement and enhancement soon 
please checkout and test https://github.com/apache-superset/superset-ui/pull/918 

thanks to contributor: @mayurnewase 
",,,,,,,,,,,,,,
12786,OPEN,[dashboard]Monitor Query on SuperSet Dashboard,enhancement:request; viz:dashboard:perf,2021-02-10 07:08:21 +0000 UTC,Asturias-sam,Opened,,"Hello Team,

Is there any way to get the execution time of the queries on dashboard, so it will be helpful for performance analysis.
From SQLLAB we can get the execution time but for dashboards is it possible see the execution time for queries ? ",,,junlincc,"
--
@Asturias-sam could you provide a use case? thanks 
--

--
If your goal is to improve dashboard performance, I suggest that to checkout and enable async query first, docs can be found [here ](https://superset.apache.org/docs/installation/async-queries-celery)  If you feel like testing the feature, feel free to enable the GLOBAL_ASYNC_QUERIES feature flag. See the CONTRIBUTING.md documentation for details: https://github.com/apache/superset/blob/master/CONTRIBUTING.md#async-chart-queries

I'm still trying to understand the business value of this requested feature. I assume you wanna use it as an assessment indicator against other BI tools? 
--
",Asturias,"
--
@junlincc 
We have dashboard having various charts, we want to monitor Superset performance  i.e  latency between Superset and Our data sources in terms of query execution shouldn't be much. 

--

--
We want to manage the Dashboard Performance  in Superset so it will help us to alert our team about slow performance and we  can take the necessary steps to speed up our datasources.
--
",,,,,,,,
12784,OPEN,[explore]specify Inclusive start; exclusive end in Timepicker Advanced input,enhancement:committed; needs:design-input; viz:explore:timepick,2021-01-28 01:41:51 +0000 UTC,junlincc,In progress,,"The actual time range that generated by new time picker in Explore sets Start date/time - inclusive and End date/time  - exclusive in all time range setting mode(e.g. last week => 2021-01-20  col < 2021-01-27, previous year => 2020-01-01  col < 2021-01-01). 
This has been the behavior in Superset since [SIP15](https://github.com/apache/superset/issues/6360) which ensures time Intervals consistency. Based on the amount of research conducted and user feedback, we agree that this should be a standard default behavior across all verticals in Superset for the long term.  

Feature request: In the time picker popover Advanced range type, we should specify that user's date string input in Start will be interpreted as inclusive, vs. date string in End will be interpreted as exclusive.  Design input is needed. 

<img width=""586"" alt=""Screen Shot 2021-01-26 at 6 22 06 PM"" src=""https://user-images.githubusercontent.com/67837651/105934835-c7e97180-6005-11eb-8ebb-1ba98a1922f8.png"">
Custom as well
<img width=""587"" alt=""Screen Shot 2021-01-26 at 6 42 30 PM"" src=""https://user-images.githubusercontent.com/67837651/105935189-607ff180-6006-11eb-85ab-47d835787428.png"">



Related projects are considering - 
1. Allow user to set time interval preference at system level 
2. Allow user to set time zone at system level



cc @mihir174 @zhaoyongjie @john-bodley 

",,,john,"
--
@junlincc I was thinking something of the form (BTW I really like the materialization of the actual interval per the `` and `<` symbols):

<img width=""588"" alt=""Screen Shot 2021-01-27 at 3 43 45 PM"" src=""https://user-images.githubusercontent.com/4567245/105935239-7a790d00-60b6-11eb-8e80-18a2653c5213.png"">

The time interval preferences and time zone topics should definitely be discussed via a SIP. 

The later is quite nuanced as there may be numerous times zones in consideration, i.e., i) the reference time zone of the data (e.g. what actual period a date represents), ii) the time zone of the Superset server (which is likely being used for all the server-side timestamps and the materialization of ""now""), and iii) the time zone of the client. 

Ideally having (i), (ii), and (iii) use the safe reference is ideal and it ensures consistency throughout. It would be quite problematic if a chart was rendered based on the client time zone as two people in two different time zones would be seeing different renderings of the same chart.

--
",junlincc,"
--
Thank you, John for your input! Agreed, both topics deserve a SIP to iron out all the considerations, possible solutions and their pros and cons. I consider time zone setting is a very important piece in Superset's international product strategy. I will do some research and put together a SIP soon. 
@john-bodley 
--

--
i'm not sure regular/pure business users can easily understand the meaning of Start (inclusive) and End (exclusive). tooltips to explain what they actually mean could be helpful. Having both makes sense @mihir174 
--
",mihir174,"
--
Hey, here's how tooltips can be set up to clarify the inclusion/exclusion of start and end dates after details are ironed out in the SIP(s).
![Slice 2](https://user-images.githubusercontent.com/64227069/106074857-72729a80-60c1-11eb-92a8-dfaece027825.png)

--
",ktmud,"
--
@mihir174 I think John's proposal is cleaner and easier to comprehend. Users don't have to hover and it'd be more direct to see the difference between these two fields. What do you think of adding both the tooltips and the label change?

Btw, should we also replace ""start date"" with ""start time"" just to be consistent?
--
",,,,
12769,OPEN,Unable to change Time Range: Access is Denied,data:connect:impala; need:validation,2021-03-18 15:18:02 +0000 UTC,gorcurek,In progress,,"While trying to create a chart for my dataset on the new **1.0** version of superset, for no apparent reason, I cannot change Time Range because I am getting an Access Denied error.

The **DATETIME** column in table is a TIMESTAMP format column of **Impala**.

I am able to query the table without any problems.

The table itself is an Impala table, connection provided by Impyla driver.

#### Screenshots

![why](https://user-images.githubusercontent.com/62286967/105857360-813a4e00-5fea-11eb-9426-88968f93614b.png)

### Environment

- superset version: 1.0
- python version: 3.7.4
",,,junlincc,"
--
what's your permission setting in Superset? @gorcurek 
--

--
time picker added a new api which requires running `superset init` to grant permission. lmk if that works @georgeke 
--
",gorcurek,"
--
Hi,

I have **Admin** role, as seen here:

![my-profile](https://user-images.githubusercontent.com/62286967/106138792-e027d080-616c-11eb-953d-4f79ca0120b5.png)

And these are the permissions for **Admin** role:

![admin-role](https://user-images.githubusercontent.com/62286967/106138902-051c4380-616d-11eb-8a59-77a1f6e964ec.png)

The other role I have, **Public**, has No permissions.


--
",mbiberhofer,"
--
Just FYI, we had the same issue and running `superset init` fixed it. Thanks for the advice!
--
",,,,,,
12768,OPEN,Redirect happens whenever user changes language,bug:regression; good first issue,2021-03-05 22:14:47 +0000 UTC,ayanginet,Opened,,"When the user presses on change language a redirect to `/users/list` happens. I think, in terms of UX, it should remain on the same page as before changing the language.

### Expected results

Remain on the same page, but translated into a different language.

### Actual results

Redirects to `users/list`.

#### Screenshots

![gif-gif](https://user-images.githubusercontent.com/39133100/105840998-be580d80-5ff5-11eb-9427-c849a3e55f07.gif)


#### How to reproduce the bug

1. Enable languages
2. Open any page
3. Change the language

### Environment

(please complete the following information):

- superset version: `0.999.0dev`
- python version: `3.7.4`
- node.js version: `v14.11.0`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,,,,,,,,,,,,
12767,OPEN,[Chart List] Viz Type filter search ranking,global:listview,2021-01-26 18:33:26 +0000 UTC,ktmud,Opened,,"When searching ""table"", this is the current filtered viz type options:

<img width=""276"" alt=""viz type search"" src=""https://user-images.githubusercontent.com/335541/105839753-d04e9780-5f86-11eb-8519-666502667d8f.png"">

It make sense to rank the options alphabetically when users enter any search keyword, but after they do, the search results should be either ranked by relevance + popularity. It doesn't make much sense to rank the least used ""Paired t-test table"" to the top.


cc @nytai 
",,,,,,,,,,,,,,
12763,OPEN,Superset impala connection error cdp-3,data:connect:impala,2021-01-26 18:34:06 +0000 UTC,pankajshanbhag,Opened,,"With impyla 0.14 , while connecting to impala through superset getting below error : 

impala://host:port/default

Unexpected error connect() got an unexpected keyword argument 'username'

{
""metadata_params"" : {},
""engine_params"" : {
""connect_args"" : {
""auth_mechanism"" : ""GSSAPI"",
""kerberos_service_name"" : ""impala""
}
}
}

* Enabled the impersonate user

",,,,,,,,,,,,,,
12756,OPEN,[explore]Improve Experience of adding chart to NEW dashboard from Explore,revisit:design-sys; viz:explore:control,2021-01-28 01:10:40 +0000 UTC,zuzana-vej,Opened,,"**Is your feature request related to a problem? Please describe.**

The current experience of adding chart from Explore to a NEW dashboard is confusing for some users. 

![Screen Shot 2021-01-25 at 7 38 55 PM](https://user-images.githubusercontent.com/61221714/105797622-33b7d580-5f45-11eb-9f86-e47dea56fbd7.png)


In the textbox, it says ""Select a dashboard OR create a new one"" but it's not apparent to users that all they need to do is type a new dashboard name, press enter, and then the ""Save & go to dashboard"" button will be enabled. User thought this was a bug. 

**Describe the solution you'd like**
In the past, there used to be a ""Create new dashboard option"" from the dropdown and it opened up a dialog box to type a dashboard name - also slightly clunky. Maybe if the text said something like ""Type new dashboard name here or select from existing"". There just needs to be some signal that it's type-able.

**Describe alternatives you've considered**
Few options:
1. easiest: Better text in the text field - for example _""Type new dashboard name here or select from existing""_
2. button create new dashboard, which opens a popup where user fills new dashboar details (apparently past solution)
3. tooltip, letting user know to ""Type new dashboard name and click enter""
",,,junlincc,"
--
> it's not apparent to users that all they need to do is type a new dashboard name, press enter, and then the ""Save & go to dashboard"" button will be enabled.

suffering from the same confusion here  we may have to go with the easiest solution for now. tag `design-sys-revisit`


@mihir174
--
",ktmud,"
--
Related #10355 
--
",mihir174,"
--
Here's a common pattern for saving to a new instance:
<img width=""1972"" alt=""Slice 1 (6)"" src=""https://user-images.githubusercontent.com/64227069/106075490-92568e00-60c2-11eb-87c0-c7db4bd800db.png"">

--
",,,,,,
12753,OPEN,[explore]chart missing dataset open error msg,revisit:design-sys; viz:explore:dataset,2021-01-26 05:56:57 +0000 UTC,junlincc,In progress,,"We want to make sure PRs that introduce UI changes from the community are always welcome without follow strict guideline when they are first created. Design support will follow after these PRs are open if we can't provide Product and Design support upfront. tagging `design-sys-revisit `to track and prevent system level design debt. 

![image](https://user-images.githubusercontent.com/67837651/105782352-34db0980-5f29-11eb-84b6-c4d9df56f8fe.png)

https://github.com/apache/superset/pull/12705

'@junlincc @mihir174 feel free to create another issue to track followups if you want to update the design or copy.'
'A possible followup would be to add ""Delete chart"" shortcut CTA to make it easier for chart owners to delete outdated charts.'
",,,junlincc,"
--

@ktmud @mistercrunch FYI ^ above is a temporary solution for balancing velocity and design quality, and for keeping OSS product development process transparent and open in the community. 
--
",mistercrunch,"
--
@junlincc I like it. 

Seems like the ""CHANGE DATASET"" button should be styled in orange/warning too to avoid blue-on-orange
--
",,,,,,,,
12751,OPEN,Update Tooltip Help Text in Edit Database > SQL Lab Settings,,2021-01-25 23:13:19 +0000 UTC,yousoph,Opened,,"## Screenshot

![image](https://user-images.githubusercontent.com/10627051/105771002-cc822d00-5f14-11eb-8c6c-7ab62e91cd54.png)

## Description
Tooltip / help text updates to the SQL Lab Settings on the Edit Database modal to be more descriptive

Updates: 

Allow in SQL Lab: 
Expose this DB in SQL Lab -> Allow this database to be queried in SQL Lab

Allow CTAS: 
Allow CREATE TABLE AS option in SQL Lab -> Allow creation of new tables based on queries

Allow CVAS: 
Allow CREATE VIEW AS option in SQL Lab -> Allow creation of new views based on queries

Allow DML: 
Allow users to run non-SELECT statements (UPDATE, DELETE, CREATE, ...) -> Allow manipulation of the database using non-SELECT statements such as UPDATE, DELETE, CREATE, etc.  

Allow Multi Schema Metadata Fetch: 
Allow SQL Lab to fetch a list of all tables and all views across all database schemas. For large data warehouse with thousands of tables, this can be expensive and put strain on the system. -> no change ",,,,,,,,,,,,,,
12744,OPEN,[explore] Firefox - Click on collapse causes additional space in the top,browser:firefox; viz:explore:ui,2021-01-25 21:56:41 +0000 UTC,adam-stasiak,Opened,,"## Screenshot


https://user-images.githubusercontent.com/25153919/105762598-ea6f8200-5f54-11eb-9b53-1d0b3420e76b.mov


## Description
Open chart Are you an ethnic minority in your city?
Collapse columns and metrics 

84.0.2 Firefox 
3fb183349f2f5710bc0816670a2d52c50fa4d2e6 master 

## Design input

",,,,,,,,,,,,,,
12741,OPEN,[explore] Click on collapse causes jump of search metrics edit field,bug:cosmetic; viz:explore:ui,2021-01-25 19:37:02 +0000 UTC,adam-stasiak,Opened,,"## Screenshot

https://user-images.githubusercontent.com/25153919/105755433-914f2080-5f4b-11eb-98a2-a56bcbff78fc.mov

## Description

I can see that when I click on Columns collapse then Search metric & Columns glitches.

Go to chart Are you an ethnic minority in your city?
Click on columns collapse


## Design input
",,,,,,,,,,,,,,
12735,OPEN,Filter not clean when remove value in annother,enhancement:committed,2021-01-25 21:55:39 +0000 UTC,Leandro-GPIN,Opened,,"Filter not clean when remove value in annother 

### Expected results

![espected](https://user-images.githubusercontent.com/19553149/105711893-e82b0a80-5ef7-11eb-9515-8d76ac614830.png)

### Actual results

![current](https://user-images.githubusercontent.com/19553149/105711905-ec572800-5ef7-11eb-8852-9ba8d0691ff9.png)

#### Screenshots

![filter-problem](https://user-images.githubusercontent.com/19553149/105711258-10fed000-5ef7-11eb-904a-1908d3bb6f6e.gif)

#### How to reproduce the bug

1. Select East Asia & Pacific in region filter
2. Select North Americ in region filter
3. Select United States in country filter
4. Remove North Americ in region filter
5. See error.

### Environment

(please complete the following information):

- superset version: `37 docker version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,junlincc,"
--
I believe you are expecting these two FilterBox work as cascading filters with workaround solution? We probably won't invest further in this solution as we recently roll out our new Dashboard native filter which does support this functionality.  It is currently behind a feature flag, and very close to be done.  to test - > https://github.com/apache/superset/issues/12148 @Leandro-GPIN 
--
",,,,,,,,,,
12734,OPEN,Some annotation lines are not visible when axis Y start to zero,cant-reproduce,2021-01-25 23:48:07 +0000 UTC,Leandro-GPIN,Opened,,"When selected axis Y to start to zero, some annotation lines are not visible. 

### Actual results

![annotation-problem](https://user-images.githubusercontent.com/19553149/105707379-9aab9f00-5ef1-11eb-8bca-9f7f2dedb064.png)

#### Screenshots

![config-axis_y](https://user-images.githubusercontent.com/19553149/105707405-a8612480-5ef1-11eb-9c4c-4475385dea36.png)

![annotation-config-problem](https://user-images.githubusercontent.com/19553149/105708314-07736900-5ef3-11eb-9d86-029032144ae4.png)

#### How to reproduce the bug

1. Set axis Y to start to zero
2. Add annotation in graph line
3. See error

### Environment

(please complete the following information):

- superset version: 37 docker
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ x ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ x ] I have reproduced the issue with at least the latest released version of superset.
- [ x ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,junlincc,"
--
Thanks for reporting, but im not able to reproduce,  can you provide more information? @Leandro-GPIN 
--
",,,,,,,,,,
12732,OPEN,"get_viz viz_obj = viz.viz_types[viz_type](KeyError: 'yh_color_table'"")",need:more-info,2021-01-25 22:14:10 +0000 UTC,lcyh,Opened,,"A clear and concise description of what the bug is.

I customized a component yh_color_tablebut  it was wrong,I don't know what's wrong
error: ""'yh_color_table'""
stacktrace: ""Traceback (most recent call last):  File ""/data/app/superset-web/superset/views/base.py"", line 135, in wraps    return f(self, *args, **kwargs)  File ""/data/app/superset-web/superset/utils/decorators.py"", line 63, in wrapper    check_perms(*args, **kwargs)  File ""/data/app/superset-web/superset/views/core.py"", line 125, in check_datasource_perms    force=False,  File ""/data/app/superset-web/superset/views/utils.py"", line 120, in get_viz    viz_obj = viz.viz_types[viz_type](KeyError: 'yh_color_table'"")

Ping gave me some suggestions on how to create a custom component

### Environment
 superset --version                                                      
Python 3.7.8
Flask 1.1.2
Werkzeug 0.16.0
",,,junlincc,"
--
do you mind updating the title to be more human readable? did you build it using dynamically imported viz plugins? @lcyh 
--
",,,,,,,,,,
12729,OPEN,[SIP] Support star schemas,viz:data:join,2021-02-25 20:29:06 +0000 UTC,wernerdaehn,In progress,,"## [SIP] Support star schemas

### Motivation

While I am absolutely fine with the decision to base a diagram on a single table/view only, most databases cannot cope with that. A typical view would join 100 tables and have 5000 columns. 
Take sales orders as example: For a proper self service BI you would join order header, order line item, three times the customer mater table for soldto-shipto-billto, the material master table, tons of tiny lookup tables with e.g. statusid, statustext etc.
If the joined table has a foreign key defined and the column is not-null, then the database optimizer can ignore that join in case none of the columns are used. Only a few databases are that advanced.
Further more, for many visual clues like slice/dice and filters a distinct list of attributes is needed. What is faster? Finding the three order states in the state text table or a select on the text table itself?

Hence I would suggest to support star schema data models. A chart is based on a fact table. The list of columns shown are all columns of the fact table plus all columns of all tables this fact table has a FK relationship with.

### Proposed Change

1. In addition to tables there are virtual objects FACT_TABLE and DIMENSION_TABLE. These simply point to physical tables. Note: A fact table in one data model can be a dimension table for another. Example: ORDER is a fact table with the measure ORDER_AMOUNT but is a referenced dimension table for the BILLINGS fact.
2. There is a relationship between facts and dimensions. Only trivial PK based equal conditions are allowed.
3. Charts can use tables or FACT_TABLEs.
",,,zhaoyongjie,"
--
@wernerdaehn 
Thank you for your comments. I think stars or snowflake model is OLAP Dataware house concept, for more modern BI prefer to use single-table model, like PBI/Tableau.

What do you think?
--
",wernerdaehn,"
--
@zhaoyongjie From a technical side, if all databases would support views with a virtual unlimited number of columns and do a perfect job from the SQL optimizer point of view, then yes. But the opposite is the case. Only Oracle And SAP Hana (Calculation Views) are even close to that and far from perfect.
From a logical point of view, Superset wants to sum up measures and group by attributes. That is a dimensional concept. 
Further more, if the tool knows what a fact table is and what a dimension, we have much more options later. For example you might have an ORDER and DELIVERY table, both have a shared dimension CUSTOMER. In a single-table model the customer table with its 500 columns must be defined multiple times whereas with a shared dimension just once. And in the dashboard, when you want to view the order amount, shipped amount and order backlog, a shared dimension allows to filter all measures at once. You know what is shared and how.
Also for the users the selection of columns is much easier when they are grouped. The customer master dimension obviously provides all columns that are customer related, the material all material related fields. In a single table model you would have 300 columns from the one and 300 columns from the other. And if customer can be filtered on sold-to, ship-to and bill-to basis, you would have the 300 customer fields three times.
And finally, while today the semantic layer of Superset is very thin, it will grow. And then specifying the same information multiple times get even more of a burden.

So no, this has nothing to do with OLAP, it is related to being a business intelligence tool.

I see that as something fairly straight forward to implement and with lots of potential initially and later.
--

--
@villebro A generic join feature I would advise against. Things like a theta join, snowflake-like joins with intermediate tables etc. These can all be built in the database using views. So why implementing these things a second time? I would really support only the most trivial case with a single fact in the middle and directly linked dimension tables with PKs. 
This gives you the biggest bang for your bucks.

As with the other proposals, let me know if I should create a comprehensive write-up. I would be willing to invest the time, but only if it is worth it.
--

--
Understood. My fear is just that when you implement that, you start with the SQL join clause in mind. I have seen multiple products fail because the join clause is extremely powerful and frankly, not suited for graphical tools. Use the SQL Editor of Superset for those. Anyway, you got my point, that is the important part.

Regarding the second paragraph, while your statement is certainly correct, it is not related to the star schema feature as such. If all needs to be in a single table, then this single table has equally many columns and must not be overwhelming as well.

Please let me know if I can do any work in that regards, in case it makes sense.
--
",villebro,"
--
Thanks for the comprehensive explanation @wernerdaehn ! Ping @amitmiran137 , here's a good motivation for adding support for join semantics.
--

--
@wernerdaehn I was speaking in general terms, i.e. supporting joining tables beyond the current table/virtual table functionality.

One thing that sticks out: if we have 100 FKs and thousands of columns, this can quickly become very burdensome, both for the application but even for the end user. So this needs to be properly scoped to make sure it improves the user experience rather than overwhelm it.
--
",,,,,,
12727,OPEN,Add a top level data navigation to the dashboard screen,design:suggest,2021-03-30 18:29:19 +0000 UTC,wernerdaehn,Opened,,"## [SIP] Add a top level data navigation to the dashboard screen

### Motivation
The user does have a dashboard showing data based on the FLIGHTS table. One chart shows the delays, one the number of flights, on the sum of flight time, the plan model used.
It would be natural to navigate within the data: How does the dashboard look when limiting to the current year? Flights that departed during business hours? Impact on holidays?
By providing a top navigation on the dashboard also ([see here for charts](https://github.com/apache/superset/issues/12725)) this can be achieved nicely.

### Proposed Change

1. Add a top navigation area to the dashboard. Everything that is filtered here impacts the filters applied within each chart.
2. In the dashboard filters between charts can have relationships. Chart1 might be based on TABLE1 and chart2 on TABLE2 but TABLE1.START_DATE should be TABLE2.CREATE_DATE. This way with a single top level selector both filters, although on different tables, can be selected at once.


Please let me know if you are interested and I would spend some time to work out the details. ",,,,,,,,,,,,,,
12725,OPEN,[SIP] Add a top level data navigation to each chart,,2021-01-25 08:08:23 +0000 UTC,wernerdaehn,Opened,,"## [SIP] Add a top level data navigation to each chart

### Motivation
Slice and dice is done by the chart consumer, not only(!) the chart developer.

I have been working for Business Objects and SAP and I am in the Business Intelligence Market for more than 20 years. I feel that the filter concept is too technical, the navigation within the data not suited for self service BI.

Imagine the FLIGHTS data in a real world scenario. You create a chart with the flight delays and this chart has certain configurations, e.g. group by AIRLINE, flight date is the time component. You might add filters as well, e.g. last year and delay >10 minutes.

If the created chart has a drop down box for each (useful) attribute above it, the user can easily move within the data. One drop down box is called ORIGIN_AIRPORT and shows the list of all airport codes to select from. Another are STATE, COUNTRY, DISTANCE,... all by which analyzing the data makes sense. ""Show me the delays for one airport"", ""Should I pick SFO or San Jose?"", ""Does the time of the day play a role?""

The main problem of such navigation bar is that a normal table/view has thousands of columns the user might want to navigate within the data. This is when hierarchies come into play. In above example the COUNTRY, STATE, ORIGIN_AIRPORT are related to each other. It does not make sense to filter on a specific airport and then offer to filter the country also. Instead of rendering each attribute individually the attributes are organized in hierarchies:
COUNTRY->STATE->AIRPORT
Then the Location-of-Departure hierarchy dropdown is opened, at the first level all countries are shown. The user can select one country or multiple (checkboxes!) or expand one country to see its states etc.

### Proposed Change

1. On top of each chart is a horizontal, scroll-able list of columns the user might want to navigate within. This list contains attributes and measures.
2. Depending on the type of element different selectors are shown. For attributes like names a checkbox is rendered to select/deselect. For attributes that have an order like a date/time a from-to range selector is shown. Long/Lat has a rect shape selector,...
3. In the table definition hierarchies can be defined to group attributes together. Such hierarchies can either be
    * based on columns: GEOGRAPHY hierarchy is from COUNTRY to STATE to AIRPORT
    * based on parent-child relationships: PRODUCT_GROUP, PARENT_PRODUCT_GROUP
    * can contain alternative hierarchies also: GEOGRAPHY has an alternative using LONG/LAT columns
    * can be generated: TIMESTAMP has YEAR->MONTH->DAY->HOUR and as alternatives: WEEK, WEEKDAY, WORKDAY, QUARTER,...
4. It would be nice if the drop down list is sorted by relevance. For example you might have filtered on a regional airport and the plane type selector lets you chose a 747? Nothing is more demotivating as constantly getting a no-data chart.
5. Similarly showing the impact of a filter. If each filter value hints how much it will modify the diagram, e.g. airport1 has lots of delays, airport2 none that would help the user also.

Please let me know if you are interested and I would spend some time to work out the details. ",,,,,,,,,,,,,,
12724,OPEN,[SIP] Propose visualizations based on data,enhancement:request; viz:explore:control; viz:explore:dataset; viz:explore:ux,2021-01-25 11:01:16 +0000 UTC,wernerdaehn,In progress,,"## [SIP] Propose visualizations based on data

### Motivation

I have been working for Business Objects and SAP and I am in the Business Intelligence Market for more than 20 years. One thing that is still not satisfying is how the charting options are chosen.
Over the time the number of available charts and their variants will increase more and more and selecting from the long list is cumbersome. Also not everybody knows all visualization options for every case.
But given that superset has a semantic layer, you can preselect the visualizations.

Example: 2 Attributes & 2 Measures? Very likely a Pie Chart will not be the proper visualization.

There is an entire academic theory about different axis types (Nominalscale, Ordinalscale, Intervalscale, Ratioscale) for example. In case you are interested we can work on the details.

### Proposed Change

1. Collect more metadata about attributes: Number of distinct values, what axis type it can be used for,...
2. Define the aggregation type of a measure and if it is semi-additive
3. For each charting option and variant specify a rank how useful it is based on the number of attributes, number of measures, axis type of the attribute, measure type.
4. Order the charting options based on an overall rank

Please let me know if you are interested and I would spend some time to work out the details.",,,junlincc,"
--
Thanks for suggesting! @wernerdaehn 

> 1. Collect more metadata about attributes: Number of distinct values, what axis type it can be used for,...

It is aligned with our long term product roadmap. in fact, when we implemented new time picker in Superset, we thought about allowing user to query the earliest(min) and latest(max) time available in the timestamp dimension. couldn't get to it by v1.0 because of potential performance issues and our time constraints. collecting more metadata of dataset is something we wanna do once we get to refactoring the major control fields like metrics, filter etc. 

> 2. Define the aggregation type of a measure and if it is semi-additive

something we will consider. it probably will require us to 'thickening' our semantic layer in Superset and steepen the learning curve of Superset. 

> 3 & 4. 

both are features available in Tableau. I agree they provides nice user experience and enables non tech users to create visualization intuitively. we would love to get to both someday.

https://user-images.githubusercontent.com/67837651/105690448-dd00bc00-5eb0-11eb-8145-8aea2a934399.mov



--

--
@wernerdaehn if you would like contribute any above items to Superset in any ways, we would love to work with you! 
--
",wernerdaehn,"
--
@junlincc Thanks for the feedback. Just for the records, what Tableau does is just the very beginning!
See here for how wide the topic can get: https://datavizproject.com/
--

--
Any suggestion of what I can do for you in that regards? Else I will try to come up with something to discuss but would love to get your guidance.
--
",ktmud,"
--
Thanks for bringing up this topic! This definitely is an interesting area of work and has a lot of potential for Superset.

What you described is often called automated chart specification, or automated Exploratory Data Analysis (EDA), which is also quite big among DataViZ academics: https://github.com/mstaniak/autoEDA-resources

It would be tremendously valuable if we could somehow integrate the latest research findings to an open source/commercial BI software.

This SIP is a good starting point, which seems to have identified a couple of items we can already do. Id recommend keep researching on this topic and start digging into the Superset codebase/architecture to form a more concrete action plan. We should at least be able to answer:

1. What is possible and what is not, and
2. What is the MVP
3. Which API we need to change or add?
4. What other areas of work we need to tackle first before working on this? E.g. SIP-34 column stats looks like a must.


Some other useful links:
- https://github.com/antvis/AVA
- https://exploratory.io/
- https://www.usedive.com

--
",,,,,,
12723,OPEN,database test does not existand image 404,install:docker; resolved,2021-03-24 02:15:58 +0000 UTC,billy98,In progress,,"I used docker-compose to deploy the superset, which is version v1.0.0,
Test database does not exist during initialization
Unable to open it after login.




```
superset_app             | INFO:superset.utils.logging_configurator:logging was configured successfully
superset_app             | INFO:superset.utils.logging_configurator:logging was configured successfully
superset_worker          | INFO:superset.utils.logging_configurator:logging was configured successfully
superset_tests_worker    | INFO:superset.utils.logging_configurator:logging was configured successfully
superset_db              | 2021-01-25 02:51:53.376 UTC [31] FATAL:  database ""test"" does not exist
superset_tests_worker    | /usr/local/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
superset_tests_worker    |   ""Flask-Caching: CACHE_TYPE is set to null, ""
superset_tests_worker    | ERROR:flask_appbuilder.security.sqla.manager:DB Creation and initialization failed: (psycopg2.OperationalError) FATAL:  database ""test"" does not exist
superset_tests_worker    | 
superset_tests_worker    | (Background on this error at: http://sqlalche.me/e/13/e3q8)
superset_tests_worker    | Loaded your LOCAL configuration at [/app/docker/pythonpath_dev/superset_config.py]
superset_tests_worker exited with code 1
superset_init            | INFO:superset.utils.logging_configurator:logging was configured successfully
superset_init            | /usr/local/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
superset_init            |   ""Flask-Caching: CACHE_TYPE is set to null, ""
superset_init            | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
superset_init            | INFO  [alembic.runtime.migration] Will assume transactional DDL.
superset_init            | Loaded your LOCAL configuration at [/app/docker/pythonpath_dev/superset_config.py]
superset_worker          | /usr/local/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
superset_worker          |   ""Flask-Caching: CACHE_TYPE is set to null, ""
superset_worker          | /usr/local/lib/python3.7/site-packages/celery/platforms.py:801: RuntimeWarning: You're running the worker with superuser privileges: this is
superset_worker          | absolutely not recommended!

......

superset_init            | INFO:superset.models.helpers:Updating table_columns yt_other
superset_init            | INFO:superset.models.helpers:Updating table_columns gender
superset_init            | INFO:superset.datasets.commands.importers.v1.utils:Loading data inside the import transaction
Exception in thread Thread-22:
Traceback (most recent call last):
  File ""site-packages/urllib3/connectionpool.py"", line 384, in _make_request
  File ""<string>"", line 2, in raise_from
  File ""site-packages/urllib3/connectionpool.py"", line 380, in _make_request
  File ""http/client.py"", line 1331, in getresponse
  File ""http/client.py"", line 297, in begin
  File ""http/client.py"", line 258, in _read_status
  File ""socket.py"", line 586, in readinto
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""site-packages/requests/adapters.py"", line 449, in send
  File ""site-packages/urllib3/connectionpool.py"", line 638, in urlopen
  File ""site-packages/urllib3/util/retry.py"", line 368, in increment
  File ""site-packages/urllib3/packages/six.py"", line 686, in reraise
  File ""site-packages/urllib3/connectionpool.py"", line 600, in urlopen
  File ""site-packages/urllib3/connectionpool.py"", line 386, in _make_request
  File ""site-packages/urllib3/connectionpool.py"", line 306, in _raise_timeout
urllib3.exceptions.ReadTimeoutError: UnixHTTPConnectionPool(host='localhost', port=None): Read timed out. (read timeout=60)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""threading.py"", line 916, in _bootstrap_inner
  File ""threading.py"", line 864, in run
  File ""compose/cli/log_printer.py"", line 214, in watch_events
  File ""compose/project.py"", line 493, in yield_loop
  File ""compose/project.py"", line 461, in build_container_event
  File ""compose/container.py"", line 48, in from_id
  File ""site-packages/docker/utils/decorators.py"", line 19, in wrapped
  File ""site-packages/docker/api/container.py"", line 755, in inspect_container
  File ""site-packages/docker/utils/decorators.py"", line 46, in inner
  File ""site-packages/docker/api/client.py"", line 230, in _get
  File ""site-packages/requests/sessions.py"", line 537, in get
  File ""site-packages/requests/sessions.py"", line 524, in request
  File ""site-packages/requests/sessions.py"", line 637, in send
  File ""site-packages/requests/adapters.py"", line 529, in send
requests.exceptions.ReadTimeout: UnixHTTPConnectionPool(host='localhost', port=None): Read timed out. (read timeout=60)

superset_init            | /app/docker/docker-init.sh: line 79:    36 Killed                  superset load_examples
superset_db              | 2021-01-25 02:56:12.771 UTC [39] LOG:  unexpected EOF on client connection with an open transaction
superset_app             | /usr/local/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
superset_app             |   ""Flask-Caching: CACHE_TYPE is set to null, ""
superset_app             | 192.168.20.46 - - [25/Jan/2021 02:56:12] ""GET / HTTP/1.1"" 302 -

.....
superset_app             | 192.168.20.46 - - [25/Jan/2021 02:57:00] ""GET /superset/welcome HTTP/1.1"" 200 -
superset_app             | INFO:werkzeug:192.168.20.46 - - [25/Jan/2021 02:57:00] ""GET /superset/welcome HTTP/1.1"" 200 -
superset_app             | 192.168.20.46 - - [25/Jan/2021 02:57:00] ""GET /static/assets/images/loading.gif HTTP/1.1"" 404 -
superset_app             | INFO:werkzeug:192.168.20.46 - - [25/Jan/2021 02:57:00] ""GET /static/assets/images/loading.gif HTTP/1.1"" 404 -
superset_app             | DEBUG:superset.stats_logger:[stats_logger] (incr) welcome
superset_app             | 192.168.20.46 - - [25/Jan/2021 02:57:01] ""GET /superset/welcome HTTP/1.1"" 200 -
superset_app             | INFO:werkzeug:192.168.20.46 - - [25/Jan/2021 02:57:01] ""GET /superset/welcome HTTP/1.1"" 200 -
superset_app             | 192.168.20.46 - - [25/Jan/2021 02:57:01] ""GET /static/assets/images/loading.gif HTTP/1.1"" 404 -
superset_app             | INFO:werkzeug:192.168.20.46 - - [25/Jan/2021 02:57:01] ""GET /static/assets/images/loading.gif HTTP/1.1"" 404 -

```
![image](https://user-images.githubusercontent.com/13307862/105660304-af922d80-5f05-11eb-8d0d-06d9c17392e5.png)
",,,dpgaspar,"
--
seems like a internet connection problem:

```
superset_init            | INFO:superset.datasets.commands.importers.v1.utils:Loading data inside the import transaction
Exception in thread Thread-22:
Traceback (most recent call last):
  File ""site-packages/urllib3/connectionpool.py"", line 384, in _make_request
  File ""<string>"", line 2, in raise_from
  File ""site-packages/urllib3/connectionpool.py"", line 380, in _make_request
  File ""http/client.py"", line 1331, in getresponse
  File ""http/client.py"", line 297, in begin
  File ""http/client.py"", line 258, in _read_status
  File ""socket.py"", line 586, in readinto
socket.timeout: timed out
```

--
",jesperbagge,"
--
I have the same issue when running docker-compose up from a Mac running latest Docker Desktop. There's no evidence of network errors in the startup logs, but I have flask_appbuilder errors. And the end result is the same - a blank page with broken image links and buttons.

```
db_1                     | CREATE DATABASE
db_1                     |
db_1                     |
db_1                     | /usr/local/bin/docker-entrypoint.sh: ignoring /docker-entrypoint-initdb.d/*
db_1                     |
db_1                     | 2021-01-25 18:27:34.546 UTC [48] LOG:  received fast shutdown request
db_1                     | waiting for server to shut down....2021-01-25 18:27:34.547 UTC [48] LOG:  aborting any active transactions
db_1                     | 2021-01-25 18:27:34.549 UTC [48] LOG:  worker process: logical replication launcher (PID 55) exited with exit code 1
db_1                     | 2021-01-25 18:27:34.551 UTC [50] LOG:  shutting down
db_1                     | 2021-01-25 18:27:34.563 UTC [48] LOG:  database system is shut down
db_1                     |  done
db_1                     | server stopped
db_1                     |
db_1                     | PostgreSQL init process complete; ready for start up.
db_1                     |
db_1                     | 2021-01-25 18:27:34.659 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
db_1                     | 2021-01-25 18:27:34.659 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
db_1                     | 2021-01-25 18:27:34.661 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
db_1                     | 2021-01-25 18:27:34.672 UTC [76] LOG:  database system was shut down at 2021-01-25 18:27:34 UTC
db_1                     | 2021-01-25 18:27:34.677 UTC [1] LOG:  database system is ready to accept connections
superset_node            | npm WARN using --force I sure hope you know what you are doing.
superset_app             |  * Serving Flask app ""superset.app:create_app()"" (lazy loading)
superset_app             |  * Environment: development
superset_app             |  * Debug mode: on
superset_app             |  * Running on http://0.0.0.0:8088/ (Press CTRL+C to quit)
superset_app             |  * Restarting with stat
superset_tests_worker    | INFO:superset.utils.logging_configurator:logging was configured successfully
db_1                     | 2021-01-25 18:27:38.783 UTC [83] FATAL:  database ""test"" does not exist
superset_tests_worker    | /usr/local/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
superset_tests_worker    |   ""Flask-Caching: CACHE_TYPE is set to null, ""
superset_tests_worker    | ERROR:flask_appbuilder.security.sqla.manager:DB Creation and initialization failed: (psycopg2.OperationalError) FATAL:  database ""test"" does not exist
superset_tests_worker    |
superset_tests_worker    | (Background on this error at: http://sqlalche.me/e/13/e3q8)
```
--
",billy98,"
--
> seems like a internet connection problem:
> 
> ```
> superset_init            | INFO:superset.datasets.commands.importers.v1.utils:Loading data inside the import transaction
> Exception in thread Thread-22:
> Traceback (most recent call last):
>   File ""site-packages/urllib3/connectionpool.py"", line 384, in _make_request
>   File ""<string>"", line 2, in raise_from
>   File ""site-packages/urllib3/connectionpool.py"", line 380, in _make_request
>   File ""http/client.py"", line 1331, in getresponse
>   File ""http/client.py"", line 297, in begin
>   File ""http/client.py"", line 258, in _read_status
>   File ""socket.py"", line 586, in readinto
> socket.timeout: timed out
> ```

I can guarantee that my network is good
--
",lihongjie224,"
--
Yeah, I have the same issue too. How to fix it?
--
",deddu,"
--
```
docker-compose ps
 
        Name                       Command                   State                     Ports              
----------------------------------------------------------------------------------------------------------
superset_app            /usr/bin/docker-entrypoint ...   Up (unhealthy)   8080/tcp, 0.0.0.0:8088->8088/tcp
superset_cache          docker-entrypoint.sh redis ...   Up               127.0.0.1:6379->6379/tcp        
superset_db             docker-entrypoint.sh postgres    Up               127.0.0.1:5432->5432/tcp        
superset_init           /usr/bin/docker-entrypoint ...   Up (unhealthy)   8080/tcp                        
superset_node           docker-entrypoint.sh /app/ ...   Up                                               
superset_tests_worker   /usr/bin/docker-entrypoint ...   Exit 1                                           
superset_worker         /usr/bin/docker-entrypoint ...   Up (unhealthy)   8080/tcp                        
```

I encountered this error following the tutorial too. (`git clone`,`git checkout latest`, `docker-compose up`) on mac os. 
Let me say that I do not know much about this project, but this connection error comes from the test_worker, and I don't know if the tests_worker is really necessary in the demo.

The tests_worker fails because is looking for a local test postgres database in the host network. see https://github.com/apache/superset/blob/master/docker-compose.yml#L93

``` yml
  superset-tests-worker:
    image: *superset-image
    container_name: superset_tests_worker
    command: [""/app/docker/docker-bootstrap.sh"", ""worker""]
    env_file: docker/.env
    environment:
      DATABASE_HOST: localhost
      DATABASE_DB: test
      REDIS_CELERY_DB: 2
      REDIS_RESULTS_DB: 3
      REDIS_HOST: localhost
    network_mode: host
    depends_on: *superset-depends-on
    user: ""root""
    volumes: *superset-volumes
```
--

--
I think the problem is in the `superset_node` container, this seems a more likely kind of cause..
```
superset_node            | npm WARN tarball tarball data for typescript@4.0.3 (sha512-tEu6DGxGgRJPb/mVPIZ48e69xCn2yRmCgYmDugAVwmJ6o+0u1RI18eO7E7WBTLYLaEVVOhwQmcdhQHweux/WPg==) seems to be corrupted. Trying one more time.
superset_node            | npm WARN tarball tarball data for echarts@5.0.0 (sha512-6SDcJbLVOcfQyjPg+spNU1+JVrkU1B9fzUa5tpbP/mMNUPyigCOJwcEIQAJSbp9jt5UP3EXvQR0vtYXIo9AjyA==) seems to be corrupted. Trying one more time.
superset_node            | npm WARN tarball tarball data for datamaps@0.5.9 (sha512-GUXpO713URNzaExVUgBtqA5fr2UuxUG/fVitI04zEFHVL2FHSjd672alHq8E16oQqRNzF0m1bmx8WlTnDrGSqQ==) seems to be corrupted. Trying one more time.
superset_node            | npm WARN tar ENOENT: no such file or directory, open '/app/superset-frontend/node_modules/.staging/datamaps-264ee032/dist/datamaps.cod.js'
superset_node            | npm WARN tar ENOENT: no such file or directory, futime
...
superset_node            | npm ERR! code EINTEGRITY
superset_node            | npm ERR! Verification failed while extracting typescript@4.0.3:
superset_node            | npm ERR! Verification failed while extracting typescript@4.0.3:
superset_node            | npm ERR! sha512-tEu6DGxGgRJPb/mVPIZ48e69xCn2yRmCgYmDugAVwmJ6o+0u1RI18eO7E7WBTLYLaEVVOhwQmcdhQHweux/WPg== integrity checksum failed when using sha512: wanted sha512-tEu6DGxGgRJPb/mVPIZ48e69xCn2yRmCgYmDugAVwmJ6o+0u1RI18eO7E7WBTLYLaEVVOhwQmcdhQHweux/WPg== but got sha512-vEl8EzFuCX9GwYtHD4t+Vzco2tSgbKRwJnz4n4ux61l9C4ITzH87+gGRu4IoHIOM9SS2kYOS9BxKz3uFBfn/8g==. (7387597 bytes)
superset_node            |
superset_node            | npm ERR! A complete log of this run can be found in:
superset_node            | npm ERR!     /root/.npm/_logs/2021-01-26T05_21_11_814Z-debug.log
superset_node exited with code 1
```
--

--
I just tried again on the latest master. Mac os, 9gb ram in my docker setup. Not fixed afaict. still blank page after the login. 

I could run the frontend manually in the host, after the superset-node container crashed,  by cd'ing into the frontend folder, installing the deps via `npm i` and then `npm run dev`. 
--
",gee,"
--
Try increasing the RAM to 7-8gb and restarting docker, fixed it for me 
--
"
12714,OPEN,[dashboard]preselect underlying dataset for user in native filter modal Datasource field,enhancement:request; viz:dashboard:native-filter,2021-01-24 00:08:25 +0000 UTC,junlincc,Opened,,"Not sure how feasible it is...... 
When users have long list of physical and virtual datasets, it takes a while to scroll and look for a specific dataset from dropdown menu. 
it will be great that filter modal preselects the underlying dataset used by all charts or most charts on dashboard, for the users.

nice to have, low priority, but worth looking into solutions. 

@agatapst ",,,,,,,,,,,,,,
12713,OPEN,[dashboard]allow multiple default filter value select in native filter,enhancement:committed; viz:dashboard:native-filter,2021-01-23 23:53:59 +0000 UTC,junlincc,Opened,,"Filterbox allows user to set default filter value by typing in the default value in Explore, set value shows in the filter box dropdown select. - not intuitive at all. 

Native filter makes one big  step forward by allowing user to select default filter value from a dropdown list in dashboard filter modal. selected default value is carried to the native filter bar and can be instantly applied to dashboard. default value persists. wonderful job!! @agatapst 

Next step, please allow users to select multiple default values. 
<img width=""1329"" alt=""Screen Shot 2021-01-23 at 3 53 42 PM"" src=""https://user-images.githubusercontent.com/67837651/105617441-2f4bbb00-5d93-11eb-8116-6b35e434a45c.png"">



",,,,,,,,,,,,,,
12707,OPEN,[explore]cosmetic improvement suggestions,bash!; needs:design-input; viz:explore:ui,2021-02-24 07:31:10 +0000 UTC,junlincc,In progress,,"
<img width=""1863"" alt=""Screen Shot 2021-01-23 at 12 31 48 AM"" src=""https://user-images.githubusercontent.com/67837651/105573577-6b3f3b80-5d13-11eb-85fd-1f084e669bc1.png"">


1. decrease the spacing between VIEW RESULTS & VIEM SAMPLES tabs and caret & section title 
2. remove the frame(including drop-shadow) around data table and chart sections
3. move pagination down to create more space for the table 

cc @mihir174 ",,,ktmud,"
--
I think all of these are already fixed by #12649 .

The only thing I still don't like about this panel is the caret and the resize bar, it seems there is a lot of waste of space that could be optimized.
--
",junlincc,"
--
> I think all of these are already fixed by #12649 .

checked again, yes - mysterious frame and drop-shadow are gone.  
yup, we will kill some space between chart and the table

<img width=""1134"" alt=""Screen Shot 2021-01-23 at 12 28 18 PM"" src=""https://user-images.githubusercontent.com/67837651/105613332-bc344b80-5d76-11eb-9b99-40f473052fb3.png"">


--
",,,,,,,,
12700,OPEN,[dashboard]native-filter cosmetic issues,assigned:polidea; bug:cosmetic; viz:dashboard:native-filter,2021-01-23 23:57:32 +0000 UTC,junlincc,In progress,,"Looks like the incompatible filters have a different select box. Let's make sure all the select field have 1)caret 2)magnifier icon for consistency 

https://user-images.githubusercontent.com/67837651/105557616-84b79780-5cc1-11eb-8e42-b6671fa89035.mov

@agatapst 

",,,junlincc,"
--
<img width=""643"" alt=""Screen Shot 2021-01-22 at 3 37 41 PM"" src=""https://user-images.githubusercontent.com/67837651/105560117-c9dec800-5cc7-11eb-94fc-8248050fa006.png"">

also the right side of dropdown in FIELD select is missing. 

 @agatapst 
--

--
<img width=""1025"" alt=""Screen Shot 2021-01-22 at 3 39 54 PM"" src=""https://user-images.githubusercontent.com/67837651/105560288-34900380-5cc8-11eb-8f9f-4bcc324a630b.png"">

let's stack `FILTER NAME` under `FIELD`, make it only one column without increasing the height of modal by narrowing and standardizing the spacing between fields. 

Rename ""Datasource"" to ""Dataset"" 

all these changes can go into one PR. @agatapst  thank you for taking care of native filter! 
--

--

<img width=""1348"" alt=""Screen Shot 2021-01-23 at 3 55 18 PM"" src=""https://user-images.githubusercontent.com/67837651/105617558-ae40f380-5d93-11eb-8638-ca6c7509798c.png"">


line break when select value is truncated. ideally the cursor should be in the same line 
--
",,,,,,,,,,
12691,OPEN,MySQL Connection Fails If MySQL User Password Includes Special Characters,#bug,2021-01-22 18:18:52 +0000 UTC,mariotalavera,Opened,,"On **SQL Lab / SQL Editor**, the HTML element for schema dropdown fails (to fetch) schema list if the MySQL user password contains special characters.  

**Note** - Registering a database with a user whose password contains special characters works OK.

### Expected results

MySQL User Passwords should be allowed to contain special characters.

#### Screenshots

![image](https://user-images.githubusercontent.com/1094812/105527988-68f8c500-5cb2-11eb-8e00-180ae00d317d.png)

#### How to reproduce the bug

1. Register a MySQL database with a user whose password contains a special character.  This is successful.
2. Go to **SQL Lab / SQL Editor** and select database just registered from dropdown.
3. See popup error shared above.

### Environment
 
- environment: Amazon Linux 2
- docker: Docker version 19.03.13-ce, build 4484c46 
- docker-compose: docker-compose version 1.27.4, build 40524192
- superset version: 1.0.0
- python version: Python 3.7.9

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.

```
superset_app             | DEBUG:superset.models.core:Database.get_sqla_engine(). Masked URL: mysql+pymysql://mario:XXXXXXXXXX@10.12.12.97:3306/st_utils?charset=utf8
superset_app             | DEBUG:superset.stats_logger:[stats_logger] (incr) DatabaseRestApi.schemas.error
superset_app             | DEBUG:superset.stats_logger:[stats_logger] (timing) DatabaseRestApi.schemas.time | 15.424574026837945 
superset_app             | 10.12.21.136 - - [22/Jan/2021 18:15:09] ""GET /api/v1/database/11/schemas/?q=(force:!f) HTTP/1.1"" 500 -
superset_app             | INFO:werkzeug:10.12.21.136 - - [22/Jan/2021 18:15:09] ""GET /api/v1/database/11/schemas/?q=(force:!f) HTTP/1.1"" 500 -
superset_app             | DEBUG:superset.models.core:Database.get_sqla_engine(). Masked URL: mysql+pymysql://mario:XXXXXXXXXX@10.12.12.97:3306/st_utils?charset=utf8
superset_app             | DEBUG:superset.stats_logger:[stats_logger] (incr) DatabaseRestApi.schemas.error
superset_app             | DEBUG:superset.stats_logger:[stats_logger] (timing) DatabaseRestApi.schemas.time | 15.86733793374151 
superset_app             | 10.12.21.136 - - [22/Jan/2021 18:15:09] ""GET /api/v1/database/11/schemas/?q=(force:!f) HTTP/1.1"" 500 -
superset_app             | INFO:werkzeug:10.12.21.136 - - [22/Jan/2021 18:15:09] ""GET /api/v1/database/11/schemas/?q=(force:!f) HTTP/1.1"" 500 -
```

- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.


",,,,,,,,,,,,,,
12689,OPEN,[explore] Dropdowns tooltip icon jumping when mouse is on label,bash!; bug:cosmetic; viz:explore:ui,2021-01-23 02:10:32 +0000 UTC,adam-stasiak,Opened,,"## Screenshot


https://user-images.githubusercontent.com/25153919/105512024-e7645f80-5cd0-11eb-80b6-e52abcacb445.mov



## Description

Tooltip icon blinks and jump when I put mouse on these fields.

Steps to reproduce:
Add chart with multiline viz type
Go to Y Axis Left section and put mouse on dropdowns

I would expect to not see this jumping - no action when mouse is on text/field and tooltip displayed when mouse on tooltip icon.

## Design input
",,,,,,,,,,,,,,
12688,OPEN,[chart]rich tooltip text overflows outside of the parent div for line chart,good first issue; need:more-info; viz:chart-multiline,2021-01-22 15:38:29 +0000 UTC,bryanck,In progress,,"## Screenshot

![Screen Shot 2021-01-22 at 7 01 01 AM](https://user-images.githubusercontent.com/5475421/105507084-a09a4980-5c7f-11eb-95fe-9eda8b2d56b3.png)


## Description

The rich tooltip text renders in a table that is inside a parent div that has the max-width set. If the text in the table is too wide, then the text will render outside of the div.

This is being caused by this line setting the max width of the div:
https://github.com/apache-superset/superset-ui/blob/b7a2f5241b56ff78dfb3f53105383fc40b12e0e9/plugins/legacy-preset-chart-nvd3/src/utils.js#L317

You can reproduce this by creating a multiline chart, group by a field with longer strings, and then reducing the width of the chart by resizing your browser window.",,,junlincc,"
--
@bryanck hey, thanks for reporting issues lately! do you minding include Superset version? this looks like a good first issue, lmk if you would like to work on it.  :) 
--
",bryanck,"
--
This is version 1.0. I can create a PR in superset-ui. I was thinking to remove the `max-width` setting, which does resolve this.
--
",,,,,,,,
12686,OPEN,How can we set chart datetime language?,need:more-info; question,2021-01-22 15:43:22 +0000 UTC,andrelucassc,In progress,,"I've followed guidelines from  [Issue 3068](https://github.com/apache/superset/issues/3068) to change the language to ""pt_BR"" and the UI in most parts has taken the expected change, but there is one major area that this was not the case. 

Creating a linechart with a metric and a datetime component results in the date beeing in english. The date rendered in the visualizations (example Oct 15 2021) can be altered? Is it something that has to be taken into account what is used to build those visualizations in the first place?",,,andrelucassc,"
--
Example of what I mean:

![problem date chart6bc2aa76a67d6fb7192f11d9ec1cc1129191f48fa546fcab2d79f27a43b57047](https://user-images.githubusercontent.com/20599467/105501648-3e534000-5ca3-11eb-85cc-edd080b277b8.png)

Expected Outcome:
Fev 2012
--

--
Hi, @junlincc , the version was 0.38.0.
--
",junlincc,"
--
@andrelucassc Thanks for reporting, Andr. what version are you at? 
--
",,,,,,,,
12684,OPEN,Connecting to Databases/index.mdx: no built-in support for Postgres,data:connect:postgres,2021-01-22 17:01:24 +0000 UTC,frafra,Opened,,"https://github.com/apache/superset/blob/f2b802978d99b0cf329520c120b2decd5953ddd0/docs/src/pages/docs/Connecting%20to%20Databases/index.mdx#L19

https://github.com/apache/superset/blob/f2b802978d99b0cf329520c120b2decd5953ddd0/docs/src/pages/docs/Connecting%20to%20Databases/postgres.mdx#L11-L12

https://github.com/apache/superset/blob/f2b802978d99b0cf329520c120b2decd5953ddd0/requirements/docker.in#L19",,,BIT,"
--
Gy
--
",,,,,,,,,,
12683,OPEN,Upgrade from 0.38.0 to 1.0.0 fails,data:database:migration; install:docker,2021-01-30 15:54:40 +0000 UTC,grafke,In progress,,"A clear and concise description of what the bug is.

### Expected results

A successfull upgrade from 0.38.0 to 1.0.0

### Actual results.

superset db upgrade command has failed with:
```pkg_resources.ContextualVersionConflict: (PyJWT 2.0.1 (/usr/local/lib/python3.8/site-packages), Requirement.parse('PyJWT<2.0,>=1.6.4'), {'Flask-JWT-Extended'})```


After manually downgrading the pyjwt to 1.7.1 I ran the superset db upgrade again, and this time another error popped up:
```INFO  [alembic.runtime.migration] Running upgrade b56500de1855 -> af30ca79208f, Collapse alerting models into a single one
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1276, in _execute_context
    self.dialect.do_execute(
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 609, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.NotNullViolation: column ""database_id"" contains null values


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/usr/local/bin/superset"", line 8, in <module>
    sys.exit(superset())
  File ""/usr/local/lib/python3.8/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/flask/cli.py"", line 586, in main
    return super(FlaskGroup, self).main(*args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/usr/local/lib/python3.8/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/usr/local/lib/python3.8/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/usr/local/lib/python3.8/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/usr/local/lib/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/click/decorators.py"", line 21, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/flask/cli.py"", line 426, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/flask_migrate/cli.py"", line 136, in upgrade
    _upgrade(directory, revision, sql, tag, x_arg)
  File ""/usr/local/lib/python3.8/site-packages/flask_migrate/__init__.py"", line 96, in wrapped
    f(*args, **kwargs)
  File ""/usr/local/lib/python3.8/site-packages/flask_migrate/__init__.py"", line 271, in upgrade
    command.upgrade(config, revision, sql=sql, tag=tag)
  File ""/usr/local/lib/python3.8/site-packages/alembic/command.py"", line 294, in upgrade
    script.run_env()
  File ""/usr/local/lib/python3.8/site-packages/alembic/script/base.py"", line 481, in run_env
    util.load_python_file(self.dir, ""env.py"")
  File ""/usr/local/lib/python3.8/site-packages/alembic/util/pyfiles.py"", line 97, in load_python_file
    module = load_module_py(module_id, path)
  File ""/usr/local/lib/python3.8/site-packages/alembic/util/compat.py"", line 182, in load_module_py
    spec.loader.exec_module(module)
  File ""<frozen importlib._bootstrap_external>"", line 783, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/usr/local/lib/python3.8/site-packages/superset/migrations/env.py"", line 124, in <module>
    run_migrations_online()
  File ""/usr/local/lib/python3.8/site-packages/superset/migrations/env.py"", line 116, in run_migrations_online
    context.run_migrations()
  File ""<string>"", line 8, in run_migrations
  File ""/usr/local/lib/python3.8/site-packages/alembic/runtime/environment.py"", line 813, in run_migrations
    self.get_context().run_migrations(**kw)
  File ""/usr/local/lib/python3.8/site-packages/alembic/runtime/migration.py"", line 560, in run_migrations
    step.migration_fn(**kw)
  File ""/usr/local/lib/python3.8/site-packages/superset/migrations/versions/af30ca79208f_collapse_alerting_models_into_a_single_.py"", line 120, in upgrade
    op.add_column(
  File ""<string>"", line 8, in add_column
  File ""<string>"", line 3, in add_column
  File ""/usr/local/lib/python3.8/site-packages/alembic/operations/ops.py"", line 1758, in add_column
    return operations.invoke(op)
  File ""/usr/local/lib/python3.8/site-packages/alembic/operations/base.py"", line 354, in invoke
    return fn(self, operation)
  File ""/usr/local/lib/python3.8/site-packages/alembic/operations/toimpl.py"", line 134, in add_column
    operations.impl.add_column(table_name, column, schema=schema, **kw)
  File ""/usr/local/lib/python3.8/site-packages/alembic/ddl/impl.py"", line 256, in add_column
    self._exec(base.AddColumn(table_name, column, schema=schema))
  File ""/usr/local/lib/python3.8/site-packages/alembic/ddl/impl.py"", line 146, in _exec
    return conn.execute(construct, multiparams)
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1011, in execute
    return meth(self, multiparams, params)
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/sql/ddl.py"", line 72, in _execute_on_connection
    return connection._execute_ddl(self, multiparams, params)
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1068, in _execute_ddl
    ret = self._execute_context(
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1316, in _execute_context
    self._handle_dbapi_exception(
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1510, in _handle_dbapi_exception
    util.raise_(
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 182, in raise_
    raise exception
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1276, in _execute_context
    self.dialect.do_execute(
  File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 609, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.NotNullViolation) column ""database_id"" contains null values

[SQL: ALTER TABLE alerts ADD COLUMN database_id INTEGER NOT NULL]
(Background on this error at: http://sqlalche.me/e/13/gkpj)```

what actually happens.

#### How to reproduce the bug

1. Install 0.38.0 using this Dockerfile: https://github.com/amancevice/docker-superset/blob/main/Dockerfile
2. Then change the superset version to 1.0.0 (https://github.com/apache/superset/archive/${SUPERSET_VERSION}.tar.gz)
3. Install
4. Run superset db upgrade inside a docker container

### Environment

(please complete the following information):

- superset version:  1.0.0
- python version:  3.8.6
- node.js version: 12

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.


",,,grafke,"
--
Looks like it's related to https://medium.com/the-andela-way/alembic-how-to-add-a-non-nullable-field-to-a-populated-table-998554003134
--

--
I worked around this by updating a portion of the file af30ca79208f_collapse_alerting_models_into_a_single_.py:

```...
       else:  # mysql does not support server_default for text fields
        op.add_column(
            ""alerts"",
            sa.Column(""validator_config"", sa.Text(), default="""", nullable=True),
        )
        op.add_column(
            ""alerts"", sa.Column(""database_id"", sa.Integer(), default=0, nullable=True),
        )
        op.execute(""UPDATE alerts SET database_id = 1"")
        op.alter_column('alerts', 'database_id', nullable=False)

        op.add_column(""alerts"", sa.Column(""sql"", sa.Text(), default="""", nullable=True))
        op.execute(""UPDATE alerts SET sql = '' "")
        op.alter_column('alerts', 'sql', nullable=False)

        op.add_column(
            ""alerts"",
            sa.Column(
                ""validator_type"", sa.String(length=100), default="""", nullable=True
            ),
        )
        op.execute(""UPDATE alerts SET validator_type = '' "")
        op.alter_column('alerts', 'validator_type', nullable=False)
```


Judging by the comment, this section should apply to mysql only, however, my superset instance is backed by the postgres db. 
--
",lamielle,"
--
I am investigating a 0.38.0 -> 1.0.0 upgrade as well.  We are deployed into k8s using a Helm chart based on the one provided in the Superset repo.  I'm also running on Postgres.  I haven't yet tried an upgrade but plan to sometime this week.  I will see if I am able to repro the bug you're reporting here.
--
",vnourdin,"
--
We have the same issue (`column ""database_id"" contains null values`) coming from 0.37.
Deployed in K8S (EKS) + PostgreSQL (RDS) through similar Helm chart too.
--

--
The problem was coming from the `alerts` table, but I checked and there were no null `database_id` in this table, so I don't get it. :shrug: 
Our alerts were just an old test, so I truncated the table, and we achieved to finish the `init_db` :tada:
But for people already using alerts it is still a blocker !
--
",,,,,,
12679,OPEN,"[Explore] Hide ""Predictive Analytics"" when ""fbprophet"" is not installed",assigned:flexiana; bug:newfeature; enhancement:committed; good first issue,2021-02-16 06:48:34 +0000 UTC,ktmud,Opened,,"The ""Predictive Analytics"" section should be hidden for ECharts timeseries viz when the Python package ""fbprophet"" is not installed.

<img width=""955"" alt=""predictive"" src=""https://user-images.githubusercontent.com/335541/105460546-cd7b3c00-5c40-11eb-9790-8ba612f4279f.png"">


cc @villebro ",,,villebro,"
--
Thanks, I've been meaning to tackle this, will put it on my backlog
--
",junlincc,"
--
thanks ktmud for pointing it out! yes, we have plan to do it. removed @villebro  and reassigning 
--
",,,,,,,,
12677,OPEN,SQL parser incorrectly identifies SQL reserved keywords as table names,#bug,2021-03-03 10:32:02 +0000 UTC,john-bodley,Opened,,"There's a few examples we've seen where the SQL parser incorrectly identifies SQL reserved keywords as tables when parsing the query. Note I've worked with `sqlparse` in another project and I sense we probably may need to complete rewrite Superset's parser in order for accurately extracting the table names leveraging more of the core `sqlparse`'s functionality.

### Expected results

The SQL parser should identify all data warehouse tables/views with no false positives.

### Actual results

The SQL parser misidentifies (under certain circumstances) SQL reserved keywords as table names.

#### How to reproduce the bug

```python
>>> from superset.sql_parse import ParsedQuery
>>> ParsedQuery(sql_statement=""SELECT * FROM foo bar TABLESAMPLE BERNOULLI(50);"").tables
{Table(table='TABLESAMPLE', schema=None, catalog=None), Table(table='foo', schema=None, catalog=None)}
```

Note adding the `AS` keyword results in a different result:

```python
>>> from superset.sql_parse import ParsedQuery
>>> ParsedQuery(sql_statement=""SELECT * FROM foo AS bar TABLESAMPLE BERNOULLI(50);"").tables
{Table(table='foo', schema=None, catalog=None)}
```

### Environment

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,john,"
--
@bkyryliuk and @villebro et al. any thoughts on this? Has Dropbox or Preset run into this issue?
--

--
@villebro or @dpgaspar has Preset experienced this issue?
--
",dpgaspar,"
--
Not that I'm aware of. probably these keywords are not on `sqlparse`, not too familiar with it, would it make sense or solve the issue to extend those keywords?
--
",villebro,"
--
@john-bodley I'm surprised adding that `AS` there makes a difference, I would have assumed it easily infers that's an alias in the first example. I assume this is a bug on `sqlparse`, maybe we should open a ticket there? I also haven't heard of anyone having this problem on Preset.
--
",,,,,,
12676,OPEN,User can reset password without current password,#bug,2021-01-26 05:53:57 +0000 UTC,liamnv,In progress,,"A clear and concise description of what the bug is.

### Expected results

User must provide current password to reset password

### Actual results

User can reset password without of current password

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

### Environment

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,lamielle,"
--
@liamnv can you provide a few more details on the bug you're reporting here?  This could be a serious security issue.  I'm relatively new to Superset but I think providing more information per the issue template would help the Superset team triage this bug.
--
",liamnv,"
--
@lamielle I found this is an issue of Flask App Builder, I'm contributing for its here https://github.com/dpgaspar/Flask-AppBuilder/pull/1553
--
",mistercrunch,"
--
@dpgaspar ^^^
--
",,,,,,
12672,OPEN,[explore] Time Series Columns Configuration - no save button; (i) wrapped to new line,bash!; bug:cosmetic; good first issue; viz:explore:ui,2021-03-31 04:19:21 +0000 UTC,zuzana-vej,Opened,,"## Screenshot

Issue 1: No Save Button, however the configuration gets saved is user clicks outside the configuration popup. Is this expected behavior? Given all other settings have save button, it would be good for consistency to have the Save here too.
![Screen Shot 2021-01-21 at 5 25 00 PM](https://user-images.githubusercontent.com/61221714/105433267-27164300-5c0e-11eb-91e5-fb0bd1706089.png)

Issue 2:
Here the (i) wraps to a new line, consider keeping it inline (make the popup window slightly larger?)
![Screen Shot 2021-01-21 at 5 24 29 PM](https://user-images.githubusercontent.com/61221714/105433343-4614d500-5c0e-11eb-8c12-b3475012a538.png)


## Description

Described above alongside screenshot.

## Design input
Confirm if the SAVE button should be on the ""Time Series Columns"" configuration popup.
",,,junlincc,"
--
> No Save Button, however the configuration gets saved is user clicks outside the configuration popup. Is this expected behavior?

It's not , we came across this issue when we audit all charts but decided to let it go back then. agreed we should get it fixed. 

> Here the (i) wraps to a new line, consider keeping it inline (make the popup window slightly larger?)

this is happening in many chart controls. we will see if we can get it fixed(in our Explore cosmetic bash) next week for patch release

Thank you for reporting  @zuzana-vej 
--

--
we will have a bash for all the `good first issue` in mid May 

cc @amitmiran137 thanks for organizing 
--
",zuzana,"
--
Low pri bug, but wanted to ping a reminder so that it's not completely lost. cc @junlincc 
--

--
@junlincc  curious if this might be prioritized not in near future?
--
",,,,,,,,
12656,OPEN,Is Superset compatible with Web Accessibility Standards or Numeric Accessibility Standards ?,,2021-03-08 18:20:36 +0000 UTC,zsellami,Opened,,"Hello,
Several companies perform an application audit before integrating them into their catalogs.
One of the criteria for the selection is digital accessibility for people with disabilities.
Is Superset compatible with accessibility standards like those proposed by W3C 
https://www.w3.org/WAI/standards-guidelines/wcag/
https://www.w3.org/WAI/fundamentals/accessibility-intro/ ?",,,etr2460,"
--
Superset is not currently compatible with accessibility standards. This sort of compatibility is something that the product certainly needs, but isn't currently being prioritized (as far as I know). I think we'd be happy if some community members wanted to step up and own a project around it
--
",mgifford,"
--
A good start is to run the site against a browser tool like https://accessibilityinsights.io/ to check for and fix the low-hanging-fruit. It won't get you to meet WCAG 2.1 AA, but it will at least make some basic improvements that will benefit users with permanent, temporary & situational disabilities. 
--
",,,,,,,,
12650,OPEN,"bar chart: disabling ""sort bars"" doesn't reset sort order",assigned:flexiana; viz:chart-bar,2021-01-21 14:17:15 +0000 UTC,villebro,Opened,,"When enabling ""sort bars"" and disabling again, the order isn't reset to the original order.

### Expected results

The original order is restored.

### Actual results

The data is sorted by x-axis.

#### Screenshots

![bar-chart-sort](https://user-images.githubusercontent.com/33317356/105328638-ec45e800-5bd8-11eb-8652-018646c52ca5.gif)

#### How to reproduce the bug

1. Go to 'Genders by state' example chart
2. Click on 'Customize'
3. check 'sort bars'
4. uncheck 'sort bars'
4. See data ordered by x-axis

### Environment

(please complete the following information):

- superset version: `master` branch

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,,,,,,,,,,,,
12645,OPEN,[dashboard]cancel creating native filter confirmation(edge case),assigned:polidea; viz:dashboard:native-filter,2021-01-21 05:08:45 +0000 UTC,junlincc,Opened,,"Scenario:
Go to native filter
add 2 filters and save
Open edit modal again and add new filter (without filling data) and delete name of first filter
Cancel edition - confirm you want to lost your changes

_Originally posted by @adam-stasiak in https://github.com/apache/superset/issues/12554#issuecomment-764007447_
",,,,,,,,,,,,,,
12643,OPEN,[dashboard] Toggle Chart Description results in chart overflowing it's area,bug:cosmetic; good first issue; viz:dashboard:ui,2021-02-25 20:54:20 +0000 UTC,zuzana-vej,Opened,,"When a user clicks on ""Toggle chart description"", the description gets displayed bellow the chart title, and the chart shifts downwards, resulting in the bottom of the chart (the x axis and legend) overflowing out of the chart area

### Expected results
Chart doesn't overflows

### Actual results
Chart overflows

#### Screenshots
Before ""Toggle Chart description""
![Screen Shot 2021-01-20 at 7 54 13 PM](https://user-images.githubusercontent.com/61221714/105278542-95013280-5b5a-11eb-8acc-819b1ba29ed0.png)


After ""Toggle Chart description""
![Screen Shot 2021-01-20 at 7 54 06 PM](https://user-images.githubusercontent.com/61221714/105278657-d265c000-5b5a-11eb-9f56-fd93108db283.png)


#### How to reproduce the bug

1. Go to Dashboard
2. Click on '...' on a chart which has a description (can be added in chart explore edit properties)
3. Click on ""Toggle Chart description""
4. See issue

### Environment
latest master

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,zuzana,"
--
Low pri bug, but just wanted to ping here for a reminder (@junlincc).
--
",,,,,,,,,,
12640,OPEN,[explore][discussion]short & long term solutions for displaying truncated info in Explore and Dashboard,.pinned; design:explore,2021-01-21 17:54:50 +0000 UTC,graceguo-supercat,Opened,,"We had naming rules for metrics, so that many metrics have same prefix. In the adhoc filter dropdown list, it only shows same prefix, there is no way for our users to pick the right metric from dropdown list:

<img width=""1175"" alt=""Screen Shot 2021-01-20 at 5 42 54 PM"" src=""https://user-images.githubusercontent.com/27990562/105268664-77769d80-5b47-11eb-8b2a-12e1593dfd18.png"">

### Expected results

Should add hover effect or tooltip for every item in the dropdown list.

### Environment

latest master

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

cc @junlincc ",,,zhaoyongjie,"
--
Hi, @graceguo-supercat 
There is a resize icon available to zoom in and out the metric(and filter) popover.

![image](https://user-images.githubusercontent.com/2016594/105269504-067ac880-5bcf-11eb-92bf-d48f279b9f38.png)


--
",ktmud,"
--
I'm pretty sure 90% of users would not think of using that resize button when they see truncated options. 
--

--
> before we come up a perfect solution, will [title attribute](https://www.w3schools.com/tags/att_global_title.asp) be a simple solution?

+1 title attribute should be the default in most places with potential truncation. I remember my job as a junior frontend engineer 10 years ago was adding ""title"" in a lot of places.



--
",junlincc,"
--
Then the question becomes, how to teach Superset users this new pattern. feature tutorial and on boarding are absolutely areas that we have room to improve. 

As we made all Explore components resizable, including data panel, data table, charts, control panel and popovers, users should have much greater flexibility operating in Explore than ever before, so I think it might worth users time learning and adapting to this enhanced pattern. Happy to hold a training session with your users to introduce all the new features in Explore and get immediate feedback from them. 

I'm not sure adding tooltips everywhere to show truncated information is the best UIUX. If you can think of any better ideas, please let us know   
--

--
Feature tutorial and user guide are  definitely something we should start incorporating to the product. I will see what I can do in Explore. 
--

--
if we do go with the solution of adding title attribute or tooltip, that will have to happen systematically, which takes time. 
the fastest/easiest solution will be increasing the width of popover by default. I remember the longest title you gave me is about 50 character long, to be safe, should we bump it to fit 70 character by default? @mihir174 
--
",graceguo,"
--
> Then the question becomes, how to teach Superset users this new pattern. feature tutorial and on boarding are absolutely areas that we have room to improve.
> 
> As we made all Explore components resizable, including data panel, data table, charts, control panel and popovers, users should have much greater flexibility operating in Explore than ever before, so I think it might worth users time learning and adapting to this enhanced pattern. Happy to hold a training session with your users to introduce all the new features in Explore and get immediate feedback from them.
> 
> I'm not sure adding tooltips everywhere to show truncated information is the best UIUX. If you can think of any better ideas, please let us know

I agree this. There are so many different users with different usage patterns, and some users may just use it once per a few month. You can't rely on one-time education or training.

It's worth to watch a video for large feature, like how to use annotation. For small feature like dropdown list, no one will bother to ""learn"" how to use it.
--

--
before we come up a perfect and beautiful solution, will [title attribute](https://www.w3schools.com/tags/att_global_title.asp) be a simple solution?
--

--
`increasing the width of popover` is stick to a fixed width, which probably still not guarantee all the cases. 

Same issue happens in dataset panel, user can not see the full metric name. You will see that even as wide as data panel with max width, a lot of metric names still get truncated.
<img width=""581"" alt=""Screen Shot 2021-01-21 at 9 23 52 AM"" src=""https://user-images.githubusercontent.com/27990562/105387592-8eaa9f00-5bca-11eb-9460-6c3017fe16e2.png"">

(Note: a couple of days ago the data panel can show horizontal scrollbar, but this behavior is changed now.) 

cc @betodealmeida :)

--

--
Did you notice it also offers tooltip for the full name?
<img width=""763"" alt=""Screen Shot 2021-01-21 at 9 54 26 AM"" src=""https://user-images.githubusercontent.com/27990562/105391145-abe16c80-5bce-11eb-9d6f-90a37335d7a8.png"">

--
",zuzana,"
--
Expecting users to resize this every single time they open the Metrics popup is unreasonable expectation. 

On the tutorial / education piece I agree there is space for improvement. One idea users suggested in the past is have some kind of ""since you last visited"" or ""since last release"" where they can have a summary. I am personally not convinced if this would be used (I never use this feature in other products) but we could explore this option a bit.
--
",mihir174,"
--
I think there is room for this popover to be wider horizontally by default. I agree that it's unreasonable to manually expand every time. I think hovering for tooltips might get frustrating when combined with scrolling + scanning through a big list. What's a string character length that we'd want to fit into the menu without truncation?
--

--
Agreed that all cases won't ever be guaranteed. My thought is we can increase the width so that more cases are covered and fewer will need to be treated with a title attribute. 

I also wanted to bring up truncation again - will it be more useful if metrics are truncated in the middle? I've noticed that many long metrics differ towards the end of their strings. Like so -
<img width=""228"" alt=""Screen Shot 2021-01-21 at 9 49 03 AM"" src=""https://user-images.githubusercontent.com/64227069/105390440-e72f6b80-5bcd-11eb-9ab9-df8fe6f01cd5.png"">

--
"
12639,OPEN,[Cosmetic] Position for extra filter indicator is not correct,design:explore,2021-01-21 03:55:51 +0000 UTC,ktmud,In progress,,"## Screenshot

<img src=""https://user-images.githubusercontent.com/335541/105262669-0cc46280-5b45-11eb-80cb-fbe3bf4abc76.png"" width=""400"">

## Description

Shouldn't the warning icon be inside the selected filter pill?

## Steps to reproduce

1. Go to a dashboard with filters
2. Apply a filter
3. Explore a chart that has the filter applied",,,mihir174,"
--
What is the warning for?
--
",ktmud,"
--
<img src=""https://user-images.githubusercontent.com/335541/105277238-e6f48900-5b57-11eb-8042-34ecf4e438b3.png"" width=""400"">

It's when you visit a chart from a dashboard, the dashboard filter will be applied to the chart so users can see the exact same chart as they see in the dashboard. But the filter itself is not persisted in the chart, so when the chart is used in other dashboards, the filters don't cross-over.

--
",,,,,,,,
12634,OPEN,Explore control panel minor cosmetic issues,bash!; design:explore,2021-01-24 16:33:58 +0000 UTC,mihir174,Opened,,"Hey all, here are just a few tweaks to the explore control panel to make things look nicer 

I compiled all the correct icons for explore here for reference/use. You can export them as SVGs if needed!
https://www.figma.com/file/JWaGztdhZS0kS5ruG7x9tB/Control-Panel?node-id=813%3A47391

### 1. Horizontal padding around input fields 

Currently, the input fields are indented on the left
<img width=""329"" alt=""Screen Shot 2021-01-20 at 3 13 34 PM"" src=""https://user-images.githubusercontent.com/64227069/105252390-2d36f180-5b32-11eb-82e4-2607724dafc8.png"">

The input fields should be aligned with the section headers like this (with 8px padding on both the left and right) - 
<img width=""477"" alt=""Screen Shot 2021-01-20 at 3 18 09 PM"" src=""https://user-images.githubusercontent.com/64227069/105252647-b5b59200-5b32-11eb-89ee-c395b9e530c1.png"">

### 2. Caret icons

The fields section and control panel have different caret icons
<img width=""616"" alt=""Screen Shot 2021-01-20 at 3 18 55 PM"" src=""https://user-images.githubusercontent.com/64227069/105252776-f7ded380-5b32-11eb-8cfa-67ef9c2b2370.png"">

They should be the same one from the Superset design system - 
<img width=""728"" alt=""Screen Shot 2021-01-20 at 3 20 48 PM"" src=""https://user-images.githubusercontent.com/64227069/105252817-1513a200-5b33-11eb-8e7e-a76a00c2f8c7.png"">

Here's the link to the correct icon - 
https://www.figma.com/file/JWaGztdhZS0kS5ruG7x9tB/Control-Panel?node-id=813%3A47391

### 3. Alignment of icons within input fields 

The f(x) icon is vertically off-center a tiny bit
<img width=""209"" alt=""Screen Shot 2021-01-20 at 3 31 47 PM"" src=""https://user-images.githubusercontent.com/64227069/105253578-9e77a400-5b34-11eb-8bcf-14fdfc8b51cc.png"">

Here's a spec - 
<img width=""779"" alt=""Screen Shot 2021-01-20 at 3 35 28 PM"" src=""https://user-images.githubusercontent.com/64227069/105253845-2493ea80-5b35-11eb-8a7f-a17f5ed70b7a.png"">

### 4. Icons in fields/dataset panel
<img width=""95"" alt=""Screen Shot 2021-01-20 at 3 37 12 PM"" src=""https://user-images.githubusercontent.com/64227069/105253951-5efd8780-5b35-11eb-90dc-1367cee7615a.png"">
These are a bit inconsistent, the correct set is in the Figma link at the top

cc: @junlincc ",,,junlincc,"
--
let's only address the first item 1. Horizontal padding around input fields in quality bash. @villebro 
--
",,,,,,,,,,
12631,OPEN,"[discuss]Should we remove the ""+"" icon for single metric control?",.pinned; design:explore; revisit:design-sys; viz:explore:control,2021-01-28 03:24:20 +0000 UTC,ktmud,In progress,,"## Screenshot

<img src=""https://user-images.githubusercontent.com/335541/105228368-5eee8f00-5b17-11eb-81f7-7d45f7d5ba30.png"" width=""320"">

<img src=""https://user-images.githubusercontent.com/335541/105228392-64e47000-5b17-11eb-9557-026638d9eb29.png"" width=""320"">


## Description

Should we remove ""+"" icon for the the single metric control? It's not adding much value and kind of distracting. 

Maybe we can even get rid of the ""+"" altogether by always keeping a ""Add metric""/""Add filter"" CTA in the selected area:

<img src=""https://user-images.githubusercontent.com/335541/105228937-108dc000-5b18-11eb-91e5-0f786fb9c9b0.png"" width=""320"">

We already received user feedbacks that they did not know how to add a second metric.

## Design input

Just a proposal 
",,,junlincc,"
--
Thanks for proposing! Could you provide some actual quotes from users about their confusion? I would also like to meet with them if its possible. When it comes to UI/UX we wanna understand the root problem first before diving into any solutions. Also please keep in mind that we are in transition phase, getting ready to fully implement the proposed design in SIP 34. Nothing is not done and set, we welcome feedbacks from all organization. 
--

--
reasons why I have some hesitation 
- Original design is proposed by Cartel in SIP 34 with a fair amount of research went it. it has served as a design Northstar in the product
- UI change can be subjective; we haven't heard enough 'complaint' yet on this subject. Let's let it sit for a while. When we start seeing a pattern of preference, we can get back to it
- Superset design system guidelines are not fully baked yet.  Until they are ready, we try to avoid changing back and forth, which could lead to  diminishing return, especially in the area(Query control) where it is mostly visited/used. 

By knowing both of you, who have close contact with large number of users and knowledge in BI tool, having the same feedback, I'm pretty convinced already.  @ktmud @srinify 

cc @mihir174 , bringing it to your attention; no immediate action needed yet 

--

--
   i vote +1 too, just delay implementing the change for a bit....we are lacking design support severely at this moment and i don't feel confident/comfortable enough to make ad hoc decision this way. please understand guys
--

--
you have made your points loud and clear, I agree with most of them. 
I believe Explore control is the most frequently used feature in the entire product, therefore it deserves a full cycle of research-design-implementation-testing, which takes time. All I'm asking here is to allow that process to happen. among all the other design tasks, I consider this one is low priority since it's not blocking any operation, while others could. also we should leave some time for other users to provide feedback since we just roll out v1.0. We could probably come up with even better solution from the collective wisdom than simply removing the ""+"".  this should be an open discussion, not an either/or decision. 


--
",ktmud,"
--
""I cant figure out how to add a second metric"" is the exact quote. They were woking on a line chart, which do support adding multiple metrics.
--

--
Hi, @srinify and @villebro , thanks for the inputs! Most of my UX suggestions are based on my intuition, and sometimes I may have not explained myself too well. It's nice to see someone else articulate the benefits of a proposed design better than me.

I agree we don't have to implemented this right away and I never expected it to be. The whole thing is just a proposal and if 
there are strong reasons of objections or better alternatives, I'd be happy to drop it.

But I don't believe this particular proposal is blocked by design resources as the proposal is pretty clear/simple and basically just an extension of the existing design. As an open source project, a lot of discussions for Superset happens asynchronously like this on Github issues. ""Proper design review"" would be a luxury for most cases. I'd be happy to hear designers' opinion on this, but sometimes we should not have to wait, exactly because how severely constrained the design resources are.

Also comment on these two points:

> 1. Original design is proposed by Cartel in SIP 34 with a fair amount of research went it. it has served as a design Northstar in the product

IMO, SIP-34 is more like a north star for design aesthetics, rather than every detail of all the UX. It's quite natural that issues come up when a design mockup is being actually implemented. We should be more flexible here and open to changes. I could be wrong, but SIP-34 mockup doesn't even have an empty state for these controls, signaling this area was probably not thoroughly considered to begin with.

> Superset design system guidelines are not fully baked yet. Until they are ready, we try to avoid changing back and forth

IMO this is a step forward, not back. Unless we believe the ""+"" icon will have to come back at some point, it's relatively low-risk. It's true introducing new UX patterns may break user habits they just got used to, but his particular proposal is not a new pattern as users would be quite familiar with the ghost button anyway.
--

--
@junlincc Agree with most of what you said and totally understand the desire of having a stable product and making decisions based on more user feedbacks.

However, this design does block some new users from adding a second metric, at least initially, as evidenced by our user feedback. I'd consider anything that already has a negatively user feedback (even just one) to be relatively high priority... We don't have to agree with every user feedback, but we should at least consider if there are better solutions to their problems. If yes, then why not go ahead with it? If someone comes up with a better solution later, we can always iterate.

I guess what I didn't fully get is why can't we put this into the ""OK to implement, but no need to prioritize"" bucket, rather than ""hold until further design review which nobody knows when will happen"", when there is already enough consensus?
--
",srinify,"
--
@junlincc I will say, in general it took me many attempts to understand the + button. I found it very awkward that it was floating above / in the top right corner of the metric name. I actually expected something like the solution proposed here (a persistent ghost-option / shadow button to add a new metric)
--
",villebro,"
--
I personally think this proposal is an improvement for the following reasons:
- the surface area of the ""Add"" event is larger and requires less aiming (the current plus button is pretty tiny)
- the new metric shows up exactly where the user clicked in the first place (less confusion)
- the user can click on the ghost button as many times as they want until they run out of ghost buttons (=viz doesn't support more metrics) - no need to inspect the color of the plus button.
- The ghost button just feels very intuitive, almost calling on the user to press it  

So voting +1 for this.
--

--
I agree with @junlincc , let's let this go through proper design review so we don't cause unnecessary back and forth.
--
",zhaoyongjie,"
--
> I agree with @junlincc , let's let this go through proper design review so we don't cause unnecessary back and forth.

I am totally in agreement with that.
--
",mihir174,"
--
Hey everyone, this was a very interesting discussion. Here's my take:

A big ""plus"" side to the button is that the control panel has a large vertical scroll surface, so having an ""Add metric"" CTA in the selected area will only make this taller and require more user scrolling. That being said, I agree that the button is unnecessary for input fields that take only 1 input.

Given that we have received feedback from a couple of users, I think it's worth running some basic usability testing to compare different options before coming to a conclusion. If we are 100% confident that a large percentage of users will prefer the proposed design change and if we think this issue is critical enough to address immediately because a significant number of users are being blocked, I will also +1 the change (bearing in mind the added vertical real estate). 
--
"
12623,OPEN,[import]Dashboard import doesn't support multiple databases,dashboard:import,2021-01-20 18:28:19 +0000 UTC,bryanck,Opened,,"If a dashboard uses datasets from multiple databases, then during import all datasets are assigned to the database specified in the import form. This causes the import to fail when a dataset doesn't exist in the specified database. Perhaps there should be an option to ignore the database override.

### Expected results

Expected that a dashboard that uses multiple databases wouldn't fail with an error.

### Actual results

Dashboard import fails with an error.

#### Screenshots

#### How to reproduce the bug

1. Create a dashboard
2. Add charts that use different databases
3. Export the dashboard
4. Delete the dashboard, charts, and datasets
5. Import the dashboard. You must select a single database here
6. Import fails. Or if it succeeds, it creates datasets all in the same database

### Environment

(please complete the following information):

- superset version: 1.0 RC4
- python version: 3.7.9
- node.js version: 12.14.1

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [*] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [*] I have reproduced the issue with at least the latest released version of superset.
- [*] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

",,,,,,,,,,,,,,
12618,OPEN,How to enable query execution in sqllab for Anonymous user(public role) ?,question,2021-01-20 18:28:36 +0000 UTC,vikalpmullick,Opened,,"Superset version: 0.38

Superset config:
AUTH_ROLE_PUBLIC='Public'
PUBLIC_ROLE_LIKE='Admin'

After superset init, all admin permissions are copied to Public role.


In Sqllab query execution does not work for Public role

ERROR: superset.views.core:Query2: 'AnonymousUserMixin' object has no attribute 'username'

![20210120_165427.jpg](https://user-images.githubusercontent.com/68067507/105168488-3464fc00-5b40-11eb-8a89-6847ce19a86e.jpg)
Pls excuse me for image quality.

Query executes for admin user.

What need to be done to allow anonymous user to allow query execution in sqllab ?


",,,,,,,,,,,,,,
12589,OPEN,How to add group by (BigNumber),need:more-info; question,2021-01-20 06:21:01 +0000 UTC,pavimimos,Opened,,"![image](https://user-images.githubusercontent.com/74908755/104982850-79a50300-5a46-11eb-85cc-43ded4b6de5d.png)


Hi

I want to sum column A and distinct by column B and show it using a BigNumber chart",,,,,,,,,,,,,,
12588,OPEN,[chart] Y Axis Bounds in more charts (Bar Chart; Dual,enhancement:committed; viz:chart-bar,2021-02-08 00:34:37 +0000 UTC,kamalkeshavani-aiinside,Opened,,"**Is your feature request related to a problem? Please describe.**
Currently Y-axis bounds option is available only for below Time-series charts:

- Line Chart
- Time-series Bar chart
- Time-series Percent Change
- Time-series Period Pivot

**Describe the solution you'd like**
I think it should be available in below charts as well:

- Dual Line Chart
- Bar Chart
- Multiple Line Chart

**Additional context**
The option of defining Y-bounds is really helpful to set min of Y-axis at 0 in Line charts, which will otherwise start at minimum marker value by default.
Similarly maximum value setting is very helpful in bar charts when there are 2 similar charts in a dashboard.",,,junlincc,"
--
https://github.com/apache-superset/superset-ui/pull/908 
we added y bounds for Bar Chart, Dual Line Chart & Multi Line Chart
feature should be available soon. 
--
",kamalkeshavani,"
--
@junlincc This feature is added in superset-ui 17.0 release, but superset 1.0.1 includes superset-ui 16.9. So this feature will be visible in the next release of Superset, is that understanding correct?
--
",,,,,,,,
12559,OPEN,"[sample]The new ""FCC 2018 Survey"" data has bad calculated column",#bug; doc:examples,2021-01-15 22:02:21 +0000 UTC,ktmud,In progress,,"
#### Screenshots

If you ""View Samples"" or select the ""Highest Degree Earned"" column in a table chart, you will see follow error:

![Snip20210115_170](https://user-images.githubusercontent.com/335541/104773866-91207b00-572a-11eb-800d-c90926d4f724.png)

![image](https://user-images.githubusercontent.com/335541/104774024-ccbb4500-572a-11eb-8f63-5c19c5da7c6e.png)

This is because the column expression used double quotes which will be considered column name escaping in most databases:

![image](https://user-images.githubusercontent.com/335541/104774132-f96f5c80-572a-11eb-9731-64b0d5b93ca9.png)


#### How to reproduce the bug

1. Choose ""FCC 2018 Survey"" dataset
2. Go to ""View Samples"" or use the ""Highest Degree Earned"" column anywhere in a chart

### Environment

Latest master

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,betodealmeida,"
--
@ktmud was this closed on purpose?
--

--
Cool, let me fix this.
--
",ktmud,"
--
@betodealmeida oh no... didn't know how I clicked on close..
--
",,,,,,,,
12557,OPEN,Chart disappears in standalone slice,#bug; need:validation,2021-01-19 12:19:14 +0000 UTC,duynguyenhoang,In progress,,"As we implemented resizable panels in https://github.com/apache/superset/pull/12411

But this introduce issue with standalone slice, example http://localhost:8088/superset/explore/?form_data=%7B%22slice_id%22%3A%20185%7D&standalone=true

In standalone mode, `chartWidth` is set to undefined, which cause chart disappear

### Expected results

Chart should be rendered properly in standalone url

### Actual results

Chart disappears in standalone url

#### How to reproduce the bug

1. Go to standalone slice URL, example http://localhost:8088/superset/explore/?form_data=%7B%22slice_id%22%3A%20185%7D&standalone=true
4. See error, the page is empty.

### Environment

(please complete the following information):

- superset version: mater
- python version: 3.8
- node.js version: v12.19.0

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,duynguyenhoang,"
--
I found the root cause, going to create PR couple of hours later.
--
",,,,,,,,,,
12550,OPEN,Include jdbc-sqlalchemy / jaedebeapi db driver for jdbc,,2021-01-18 10:28:50 +0000 UTC,squalou,In progress,,"**Is your feature request related to a problem? Please describe.**

I use some DB that are not yet supported by superset or sqlalchemy.
BUT jdbc drivers are available.

This project seems to bridge the gap between jdbc and sqlalchemy

more specifically .... jaydebeapi does.

https://pypi.org/project/sqlalchemy-jdbcapi/

Would be a great news to have this supported


**Describe the solution you'd like**

Support jdbc connectors provided suiting jar is given.


**Describe alternatives you've considered**

for my specific case right now, see #12544 but I feel like its' even worse.

**Additional context**

I'm running Superset inside a container in which I installed
jaydebeapi ith pip
and a jre

This snippet works.

```python
import jaydebeapi
from os import environ
environ['JAVA_HOME']=""/usr/lib/jvm/java-8-openjdk-amd64/""
conn = jaydebeapi.connect(""software.amazon.timestream.jdbc.TimestreamDriver"",
         ""jdbc:timestream"",
         {'AccessKeyId':""mykey"", 'SecretAccessKey':""mysecret"", 'Region':""eu-east-1""},
         ""/etc/amazon-timestream-jdbc-1.0.0.jar"")

curs = conn.cursor()
curs.execute('select * from ""timestream-test-db"".testtable where time > ago(1d)')
data=curs.fetchall()
curs.close()
conn.close()
for d in data:
    print(d)
```


I did not manage to use the supposedly working sqlalchemy project I mentioned.

Would be incredibly great to be able to use theses connexions, no idea how",,,dpgaspar,"
--
Hi @squalou your example is at the dbapi layer, please test using SQLAlchemy with a driver that supports it's dialect:

From the docs (https://github.com/daneshpatel/sqlalchemy-jdbcapi):
```
from sqlalchemy import create_engine
engine = create_engine('jdbcapi+pgjdbc://{}:{}@{}/{}'.format(username, password, <ip:host>', <database name>))
rows = engine.connect().execute(
    ""select * from some_table LIMIT 10""
)
print([row for row in rows])
```


--
",squalou,"
--
I've read a bit more about sqlalchemy etc... and indeed, no miracle can occur : I would need a dedicated dialect to begin with :)
(I mean : jdbc driver is not enough)
--
",,,,,,,,
12546,OPEN,[Dashboard]arrow sorting label drifted off the attribute text in dashboard table,bug:cosmetic; good first issue; viz:chart-table,2021-01-17 18:19:23 +0000 UTC,amyshangshang,Opened,,"## Screenshot

attached

## Description

1. click one dashboard
2. in the dashboard page, the table on the left part has the name and count attributes. next to the name attribute, the sorting label was placed too far away from its text. 

<img width=""1402"" alt=""Screen Shot 2021-01-15 at 2 35 11 AM"" src=""https://user-images.githubusercontent.com/4502866/104715562-930f1d80-56db-11eb-8cb1-b2e21d3ceb90.png"">
<img width=""1405"" alt=""Screen Shot 2021-01-15 at 2 38 07 AM"" src=""https://user-images.githubusercontent.com/4502866/104715571-96a2a480-56db-11eb-970e-409d19822a36.png"">
",,,junlincc,"
--
good catch! thank you @amyshangshang for reporting it! 
--
",,,,,,,,,,
12544,OPEN,Driver for Amazon Timestream (panda driver exists),,2021-01-15 14:35:44 +0000 UTC,squalou,In progress,,"Amazon Timestream has recently been publicly released.
Among connectivity support, there is panda (but alos others)

https://github.com/awslabs/amazon-timestream-tools/tree/master/integrations/pandas

Would be nice to have a diver in superset.


**Describe the solution you'd like**

Support AWS Timestream connexion and dialect

**Describe alternatives you've considered**

... not many options avaialble unfortunately.  (unless superset can support jdbc drivers one day ? or does it and I didn't see it)

**Additional context**

",,,villebro,"
--
If there is a SqlAlchemy driver for Amazon Timestream it will be simple to add support for it.
--
",squalou,"
--
I'm note sure.
I mean, I had a look at this : 

https://pypi.org/project/awswrangler/

and also 

https://github.com/awslabs/aws-data-wrangler/releases


which says : 

```
Breaking changes

    sqlalchemy and psycopg2 dependencies replaced by redshift_connector and pg8000    
```

I have no idea what to do about it.

If I wanted to 'quickly' try something on my side, is there any pointer as to where I could start ?

I'll help as much as my python knowledge can help but I need a few explanations first :)
--

--
I also came accross this

https://pypi.org/project/sqlalchemy-jdbcapi/

that would allow jdbc connexion to superset easily, which would in turn open lots of DB (including aws tmestram).

Any clue about this one ? will probably add another issue / feature request.

--
",,,,,,,,
12542,OPEN,Remember last selected filter values for a datasource and use them across dashboards,enhancement:request,2021-02-19 19:28:51 +0000 UTC,durchgedreht,In progress,,"When switching dashboards the Filterbox is loaded from scratch and the user sees the preselection of the Time picker, e.g. ""Last week"":

<img width=""246"" alt=""Screenshot 2021-01-15 at 10 24 01"" src=""https://user-images.githubusercontent.com/4490840/104711442-ac3abd00-5721-11eb-8d25-771a53e4be26.png"">

We have multiple dashboards and users switch between them and they also need to analyse day, week and month. So they always have to chnage daterange over and over again. It would be great if one could configure the dashboards to take over the selection from last dashboard if available (like if it was handed over as URL parameter).

If that filter is available, apply it, if not use behaviour as is. Potentially this could be store in a cookie or session. 

The filterbox component could be marked with a checkbox 'use last filter' or similar.

",,,ktmud,"
--
You can actually persist default filters for dashboards, including the time filters.  It's just the option is a little hidden:

<img width=""917"" alt=""dashboard-properties"" src=""https://user-images.githubusercontent.com/335541/108139378-d4953e80-7074-11eb-9a1a-fa8f05f5cd21.png"">

You need to go to the edit mode,  Edit Dashboard Properties, and add a `default_filters` field of stringified JSON in the advanced settings.

--

--
<img width=""394"" alt=""share-dashboard"" src=""https://user-images.githubusercontent.com/335541/108139697-7ae14400-7075-11eb-8caa-bfdf2e9588f6.png"">


<img width=""538"" alt=""share-chart-with-filters"" src=""https://user-images.githubusercontent.com/335541/108139642-5b4a1b80-7075-11eb-9f2e-34dfe9d04925.png"">

You can also apply the filter values you like, then click on ""Share dashboard"" or ""Share chart"" to get permlinks of the dashboard with filter values remembered.
--

--
If multiple dashboards are related, then maybe you can place them in different tabs of the same dashboard, so the time filter can be reused. But I do admit there may be value in remembering last selected filters and use them across dashboards.
--
",durchgedreht,"
--
This is not what I've asked for:
This way you need to inject the filter EVERY time.

It would be great if you could keep your selection and walk along. Think about the user:

- Open Dashboard ""sold items""
- Selecting ""last week"" in filters
- **WOW ! So many sales!!! I'd like to see my revenue!!!**
- Next he switches to dashboard ""revenue""
- likely he's interested in the same timeframe

It would be cool to have an option ""use last selection if available"". That would override the default.

How can I persist the selection? How can Superset itself inject that the way you described?
--
",,,,,,,,
12533,OPEN,[code hygiene] Fully move Timeseries Table plugin to superset-ui,assigned:turing; good first issue; viz:chart-time-table,2021-01-15 08:24:14 +0000 UTC,ktmud,Opened,,"Follow up for #12532 .

We need to fully move the Timeseries Table plugin to [`@superset-ui/legacy-plugin-chart-time-table`](https://github.com/apache-superset/superset-ui/blob/84a6d592b9470883150c1085dada740b69e46fe6/plugins/legacy-plugin-chart-time-table/) and delete related files in `superset-frontend`.",,,,,,,,,,,,,,
12531,OPEN,[explore]Filter box should use specified predicated time range in the dataset,bug:regression; viz:explore:dataset,2021-02-02 16:57:45 +0000 UTC,eugeniamz,Opened,,"Filter box does a 
```select <field> from datasource```

for each column in the filter box but the queries that execute in the database add a predicated to select everything from the previous date or older

``` select <field> from datasource where <random date column> < 'date 00:00'```

This won't include the new values from the 'today' and scan all the data from the dataset. This full scan in the database can be expensive to the database.. 

if in the dataset exists the option to predefined a predicated to improve the filters loads  performance, we should use the same for the filters box values instead to add a random predicated

![image](https://user-images.githubusercontent.com/58375897/104667955-2d07a380-56a5-11eb-862b-36bd49cafc4d.png)

**Example to reproduce**
 
![Screen Shot 2021-01-14 at 8 16 21 PM](https://user-images.githubusercontent.com/58375897/104668457-1ada3500-56a6-11eb-99f5-0bb8d6922b55.png)
",,,junlincc,"
--
@eugeniamz clear, will look into it

--

--
@eugeniamz started working on this one, sorry about the delay. 
--
",nikolagigic,"
--
@eugeniamz @junlincc If I understood this correctly we want to add the predefined **Autocomplete query predicate** in case it exists to the filters query when its ran?
--
",,,,,,,,
12516,OPEN,[chart] Replace Big Number with Big Card,enhancement:committed; need:followup,2021-02-19 09:37:29 +0000 UTC,kamalkeshavani-aiinside,Opened,,"**Is your feature request related to a problem? Please describe.**
I would like to show a single metric/dimension value from db in a report. Currently the only option for that is Big Number chart type, which can work with only Numbers.

**Describe the solution you'd like**
Instead of Big Number, a new chart type like 'Big Card' or 'Big Value' should allow any data type like datetime, string, %, etc. along with some subheader text as it allows now.
And ofcourse, d3 formatting to set the Big Value formatting style.

**Describe alternatives you've considered**
An alternative could be supporting SQL query results in Markdown component of dashboard. Users can setup their formatting style in markdown, and get the values from SQL query. But this solution will need extra effort to support filtering effects.
",,,junlincc,"
--
@maloun96 
--

--
@kamalkeshavani-aiinside @inesplc can you send us the link to the library? thanks!
--
",inesplc,"
--
Also could use this feature!
Our current work around is to use a word map chart in a dataset that returns only one series, but obviously this is not its intended use.

Similar issues were raised in https://github.com/apache/superset/issues/1784, https://github.com/apache/superset/issues/2919 and https://github.com/apache/superset/issues/6771 but were never finished.
--
",kamalkeshavani,"
--
@junlincc what library are you asking about?
--
",,,,,,
12512,OPEN,[explore] switch between SIMPLE and CUSTOM SQL in ad hoc filter,bash!; bug; viz:explore:filter,2021-03-09 18:59:58 +0000 UTC,junlincc,In progress,,"![ezgif-4-cd50b8c2a1f0](https://user-images.githubusercontent.com/67837651/104552675-15211880-55ee-11eb-908f-00ba77769d1e.gif)

Expected behavior:
select filter value in the SIMPLE tab should automatically update SQL expression in the CUSTOM tab

Actual behavior: 
Users have to click the input field to update the change 

To reproduce
- create a chart in Explore
- create a filter in SIMPLE 
- switch to CUSTOM SQL without clicking SAVE or CLOSE
- switch back to SIMPLE, change the filter value
- switch to CUSTOM again see expression 



",,,junlincc,"
--
@eschutho @willbarrett we discussed about investing in automated test rather than having both manual testing and automated test, do you guys think issues like this one can be easily caught? and how long will it take to get to that point?  

cc @mistercrunch 
--

--
> we need to merge this [PR](https://github.com/apache/superset/pull/12446), before fix this bug.

will do @zhaoyongjie 
--
",zhaoyongjie,"
--
@junlincc 
we need to merge this [PR](https://github.com/apache/superset/pull/12446), before fix this bug. 
--
",eschutho,"
--
> @eschutho @willbarrett we discussed about investing in automated test rather than having both manual testing and automated test, do you guys think issues like this one can be easily caught? and how long will it take to get to that point?
> 
> cc @mistercrunch

Yes @junlincc I think once we start doing more integration tests with React Testing Library, we can focus on these types of assertions. 
--
",bkyryliuk,"
--
Is anyone working on the fix ?
--
",,,,
12501,OPEN,[SQL Lab] Split Run Button on Toolbar Missing Disabled State,sql_lab:editor,2021-02-04 18:03:03 +0000 UTC,yousoph,Opened,,"## Screenshot
![image](https://user-images.githubusercontent.com/10627051/104499666-4b30af00-5592-11eb-8d4f-be78795e3528.png)

## Description
Prereqs: Have CTAS and/or CVAS enabled on a database 
In SQL Lab, delete all text in your query. The button should be showing a disabled state of the split run button but currently shows as the wrong width with the text cut off. 
The disabled state of the split run button shouldn't have a tooltip on hover.

## Design input
Mockup of the new disabled state for the split button would be helpful here @steejay, thank you! ",,,Steejay,"
--
cc @yousoph @eschutho 

![Split button disabled](https://user-images.githubusercontent.com/60786102/105513467-32598500-5c87-11eb-9163-1742a8323ba5.png)

link to Figma:
https://www.figma.com/file/mPacKMIAXDpZMrQiGDeJMd/SQL-Toolbar-UX%2FUI---P0?node-id=1033%3A26744
--
",,,,,,,,,,
12500,OPEN,Add docs for the new time/date filters,doc:user; viz:explore:timepick,2021-01-13 20:05:31 +0000 UTC,ktmud,Opened,,"**Is your feature request related to a problem? Please describe.**

#11418 added a bunch of [custom date functions](12408) but there is no documentation on how to use them. 

**Describe the solution you'd like**

We should add a user-tutorial page in https://superset.apache.org/ on how to use the new time picker. Explaining:

1. The definition of Previous/Last periods
2. Free text shortcuts, e.g. ""now: last 7 days""
3. Custom time functions
4. Anchor for relative dates
",,,junlincc,"
--
it's currently WIP!
--
",,,,,,,,,,
12495,OPEN,[dashboard] Impossible to edit imported dashboard data,dashboard:import,2021-01-13 17:47:21 +0000 UTC,agatapst,Opened,,"When user exports and then imports a dashboard, it is impossible to change some data, i.e. name of the imported dashboard or name of the filter.

### Expected results
It should be possible to edit data of the imported dashboard, the same as it is possible to edit data of the existing dashboard (the one user exports).

### Actual results
When user changes some data, it is not presented. 
In case of changing the name in all dashboard panel, it is not possible to click ""save"" in editing modal.

#### Screenshots
Change name of the imported dashboard
![imported_dash_change_name](https://user-images.githubusercontent.com/47450693/104449615-25f66d80-559f-11eb-9209-6e9df82d9239.gif)

Change filter name of the imported dashboard
![imported_dash_filter](https://user-images.githubusercontent.com/47450693/104449754-635afb00-559f-11eb-903c-4f0aa7924dad.gif)

#### How to reproduce the bug
1. Export dashboard
2. Import this dashboard
3. Try to edit this dashboard - for example name in all dashboard panel or click dashboard name and change filter name

### Environment
Last commit:  [#407b194b15](https://github.com/apache/superset/commit/407b194b15)

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context
I have noticed, that if I remove last lines from JSON metadata, editing works fine.
![imported_dash_json](https://user-images.githubusercontent.com/47450693/104452211-f2b5dd80-55a2-11eb-9ca7-70cd04d72587.gif)

",,,junlincc,"
--
cc @betodealmeida 
--
",,,,,,,,,,
12494,OPEN,[list_view] Unify Forbidden behavior for removing and editing datasets,enhancement:request; needs:design-input,2021-01-14 21:32:00 +0000 UTC,adam-stasiak,Opened,,"In https://github.com/apache/superset/pull/12491 we are introducing popup for forbidden behavior when user tries to modify dataset. We should keep this way also for removal of datasets.

### Expected results

When I try to remove dataset - I can see popup
### Actual results

When I try to remove dataset - I see a toast

#### Screenshots
![image](https://user-images.githubusercontent.com/25153919/104451626-19bfdf80-55a2-11eb-91e1-be4fbc9f0fb2.png)
![image](https://user-images.githubusercontent.com/25153919/104451680-2a705580-55a2-11eb-9551-8d601925a91c.png)


#### How to reproduce the bug

Login as admin
add 2 users with alpha permission
As user 1 add dataset
As user 2 try to remove dataset

### Environment

(please complete the following information):

docker - commit 0ae16bfe49817cdaacb9cbfff322b327f087f901

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

",,,Steejay,"
--
hey @adam-stasiak I think we could avoid having users encountering this decision by stopping them earlier in the flow. We suggest disabling the edit and delete icon in the actions column. When a user hovers over the disabled icon, a tooltip would appear describing why this action is disabled.

delete tooltip:
""You do not have permission to delete this (object).""

edit tooltip:
""You do not have permission to edit this (object).""

Generally we should avoid modal on modal situations. If this were something like an API error then the toast might be a better solution given the existing modal. 

cc @rusackas @mihir174
--
",,,,,,,,,,
12493,OPEN,[Explore]Custom SQL COUNT_DISTINCT subtraction in metrics,assigned:preset; enhancement:committed; viz:explore:control,2021-01-13 17:57:12 +0000 UTC,bcbilgin,In progress,,"Hi, 
It's version 0.36.0.
I try to subtract two metric columns; like column1-column2. I need distinct counts here so my first number will be COUNT_DISTINCT(column1) and second one is COUNT_DISTINCT(column2).
This works:
![superset2](https://user-images.githubusercontent.com/10693748/104449690-09166600-55b0-11eb-814a-f7bad25e9d12.PNG)
This does not work:
![superset3](https://user-images.githubusercontent.com/10693748/104449730-1af80900-55b0-11eb-96fc-20859e67be50.PNG)
The error is:
> druid error: Unknown exception (org.apache.calcite.tools.ValidationException): org.apache.calcite.runtime.CalciteContextException: From line 7, column 8 to line 7, column 30: No match found for function signature COUNT_DISTINCT(<CHARACTER>)

Note that COUNT_DISTINCT() function -like COUNT() function- works for both column1 and column2.

Is this intentional? What is the difference between subtraction on counts and subtraction on distinct counts?",,,villebro,"
--
`COUNT_DISTINCT(x)` is, I believe, a relic from old times, and should really be replaced by `COUNT(DISTINCT x)`. Regular count is used for counting rows, while distinct counting is used for counting unique values in the column.
--

--
Yes, I agree, the mapping from simple to custom filter for `COUNT_DISTINCT` is weird and should be changed. @junlincc let's fix this
--
",bcbilgin,"
--
Yes, of course I need to count distinct values and I think you are right, but the reason I use `COUNT_DISTINCT(x)` instead of `COUNT(DISTINCT x)` is the following:
![superset4](https://user-images.githubusercontent.com/10693748/104453374-5d701480-55b5-11eb-9383-c2b616e1c106.png)
The default selection is `COUNT_DISTINCT`
--

--
`COUNT(DISTINCT column1) - COUNT(DISTINCT column2)` did work for me, but I still believe a fix is needed here :)
--
",junlincc,"
--
@zhaoyongjie @villebro let's include the fix it the metric refactoring project, which i wanna prioritize in Q1 before any further feature development 
--
",,,,,,
12484,OPEN,[Dashboard]Native Filter and top-level tabs,assigned:nielsen; bug:cosmetic; viz:dashboard:native-filter,2021-04-09 10:36:49 +0000 UTC,ktmud,Opened,,"#### Screenshots

![top-level-tabs](https://user-images.githubusercontent.com/335541/104416235-50591400-5528-11eb-857c-1a1d61debf02.gif)

Should the native filters be out side of all tabs, including the top-level tabs?

#### How to reproduce the bug

Test with the ""Tabbed dashboard"" example dashboard.


### Environment

Latest master
",,,junlincc,"
--
should be outside of all tabs. 
--
",,,,,,,,,,
12476,OPEN,[dashboard] including chart which is malformed (removed datasource) results in broken links,viz:dashboard:error,2021-01-13 08:23:38 +0000 UTC,adam-stasiak,Opened,,"When I open chart which is malformed because of deleted datasource I get error.

### Expected results

?? @benceorlai Could you define what is expected behavior for such case? In additional context section I described problem.

### Actual results

fatal error
#### Screenshots

<img width=""1435"" alt=""Zrzut ekranu 2021-01-12 o 23 31 12"" src=""https://user-images.githubusercontent.com/25153919/104383477-2d326280-5530-11eb-91c8-3b1d6ffff97a.png"">

#### How to reproduce the bug

Find chart with datasource X
Find dashboard with this chart
Remove datasource X
Open dashboard with this chart
See error

### Environment
docker commit e47350ef961917102c6319545e52df3961752a46

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x ] I have reproduced the issue with at least the latest released version of superset.
- [x ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

You can find similiar issue https://github.com/apache/superset/issues/12475 resolved in PR https://github.com/apache/superset/pull/12468 . But here situation is more difficult because I can imagine situation where dashboard is full of valid charts and only single one is malformed and then user cannot access this space. We should think about long term solution for this - consistent for both cases.

CC: @junlincc @kkucharc 
",,,nytai,"
--
I updated the title because this is really an issue with the dashboard itself, not the list view. The same error would happen if the user were visiting a bookmarked link to the dashboard. 

We should probably handle this the same way we're handling https://github.com/apache/superset/issues/12464 as it's just another symptom of the same problem. 
--
",,,,,,,,,,
12475,OPEN,[explore] Removing data from annotation layerdropdown causes modal dismiss,bash!; viz:explore:annotation,2021-01-23 20:10:19 +0000 UTC,adam-stasiak,Opened,,"Removing data from annotation layerdropdown causes modal dismiss


### Expected results

modal should not be dismissed

### Actual results

modal is dismissed

#### Screenshots

https://user-images.githubusercontent.com/25153919/104378913-fc9afa80-5528-11eb-924e-c6be7d6e2c4c.mov

#### How to reproduce the bug

Go to chart
Open add annotation layer area
Add annotation source
Remove annotation source

### Environment

docker 
on master I was not able to add any object to delete so I used branch and commit: f6446af9cd43a479ca5696062234cff732b51286
### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [x ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

",,,,,,,,,,,,,,
12474,OPEN,[explore] Changing chart type with active annotation layer and filter causes missing chart,assigned:flexiana; viz:chart-others,2021-01-13 09:42:57 +0000 UTC,adam-stasiak,Opened,,"When I set Annotation layer and filter and switch chart type then I got it missing.
### Expected results

proper chart should be displayed
### Actual results

chart is missing
#### Screenshots



https://user-images.githubusercontent.com/25153919/104376132-d70bf200-5524-11eb-9cf5-3bf90be33704.mov

Sorry for bad quality of video. It was long and I was not able to make this under 10MB limit.


#### How to reproduce the bug

add new line chart for random_time_series dataset
add annotation layer for this chart
add filter ""ds is not null""
Run query
Change chart type to Area chart
Run Query
Change chart type to Line chart
Run Query


### Environment

docker run
commit 5f93a14f5dd422e6f9ed468390b128776f278b03

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ x] I have reproduced the issue with at least the latest released version of superset.
- [ x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

",,,,,,,,,,,,,,
12470,OPEN,"Move CTAS Schema Field Closer to the ""Allow Create Table As"" In Edit Datasource Modal",,2021-01-25 23:16:16 +0000 UTC,yousoph,In progress,,"## Screenshot

![Clipboard 2021-07-01 at 10 44 15 AM](https://user-images.githubusercontent.com/10627051/104364487-84521c00-54cb-11eb-960c-aea2504783ae.png)

## Description

It's not obvious that the CTAS Schema field is related to the Allow Create Table As option and might be easily missed by users. The text field should be closer to the Allow CTAS option visually so users know they're related and can change the options together. ",,,yousoph,"
--
Will work with @steejay on a mock for this 
--
",Steejay,"
--
Here's a link to the prototype that addresses this pain point. The goal is to display only what the user needs to see at any given time in their flow to reduce visual clutter and information overload. 

Changes:
 Both Allow CTAS and CVAS are dependent on the 'Expose in SQL Lab' selection so they appear only when applicable
 The schema input field moved closer to its related context (CVAS and CTAS) 
 The schema input field copy is updated to address CTAS **and** CVAS

https://www.figma.com/proto/NbuTOMaUdv2GCBoDyihdLw/Database-CRUD-P0?node-id=475%3A1&scaling=min-zoom%5C

![Edit database modal](https://user-images.githubusercontent.com/60786102/105243949-9ca5e480-5b24-11eb-9aac-2a55f9f1a06d.jpg)

cc @eugeniamz 

--

--
@lyndsiWilliams @yousoph @hughhhh 

Here is an updated prototype that reflect some changes to the progressive disclosure.
https://www.figma.com/proto/NbuTOMaUdv2GCBoDyihdLw/Database-CRUD---P0?node-id=613%3A50&scaling=min-zoom

Figma spec:
https://www.figma.com/file/NbuTOMaUdv2GCBoDyihdLw/Database-CRUD---P0?node-id=479%3A39226

--

--
![Add Modal - SQL Lab Settings @1456 (2)](https://user-images.githubusercontent.com/60786102/105777977-423fc600-5f20-11eb-9659-06e36f2a4cb0.jpg)

--
",,,,,,,,
12462,OPEN,[dashboard][native-filter]make single filter select field collapsible,assigned:nielsen; enhancement:committed; good first issue; viz:dashboard:native-filter,2021-04-09 10:38:42 +0000 UTC,junlincc,Opened,,"requirement: make filter select field collapsible to allow user hide the long list of selected value of a single filter from the dashboard. 

<img width=""2043"" alt=""Screen Shot 2021-01-12 at 9 07 52 AM"" src=""https://user-images.githubusercontent.com/67837651/104347760-d5efac00-54b5-11eb-9867-70acb217f8b8.png"">


",,,,,,,,,,,,,,
12460,OPEN,[dashboard] 1 option0 instead of 1 option,bug:newfeature; viz:dashboard:native-filter,2021-01-12 17:09:45 +0000 UTC,adam-stasiak,Opened,,"You can see malformed text when you add parent filters that limits number of options to single value.
### Expected results

1 option
### Actual results

1 option0 

#### Screenshots

If applicable, add screenshots to help explain your problem.![image](https://user-images.githubusercontent.com/25153919/104343037-c430f780-54fb-11eb-867e-9f8e2cd1598f.png)

#### How to reproduce the bug
Scenario:
enable native filters
go to tabbed dashboard
add country filter with wb_health_population datasource and field country_name. Check Apply changes instantly
add region filter with the same datasource and field region. check apply changes instantly and set parent filter to country.
save
via dashbard side menu set country to Somalia
check content of region

### Environment

docker
sha 90915db60d0f330fd28a5fc2b7a6676944c4dbb5
### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ x] I have reproduced the issue with at least the latest released version of superset.
- [ x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

",,,,,,,,,,,,,,
12445,OPEN,[explore] Chart embed code modal glitch,assigned:polidea; bug:cosmetic; viz:explore:others,2021-01-18 08:58:49 +0000 UTC,adam-stasiak,Opened,,"When I change size of code block then I can make this bigger than modal

### Expected results

Modal should react to code size change.

### Actual results

Code is outside the modal

#### Screenshots


https://user-images.githubusercontent.com/25153919/104322949-d607a000-54e5-11eb-9fb1-c1b27bea1459.mov


#### How to reproduce the bug

1. Go to chart
2. Click on embed code
3. adjust size
4. See error

### Environment
commit 90915db60d0f330fd28a5fc2b7a6676944c4dbb5
docker

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

",,,junlincc,"
--
@kgabryje let's hop on this one while i try to repro the other two bugs for ya~ 
--
",,,,,,,,,,
12444,OPEN,[explore] Scrolling not working on Chart Data section with Safari,browser:safari; viz:explore:viewdata,2021-01-12 17:14:33 +0000 UTC,adam-stasiak,Opened,,"When I am using Safari then I am not albe to scroll this area.
### Expected results

it should be scrollable.
### Actual results

it is not scrollable.
#### Screenshots


https://user-images.githubusercontent.com/25153919/104316037-27129680-54dc-11eb-9ebf-f19ff21cb853.mov



#### How to reproduce the bug

1. Go to any chart
2. click on Data to expand
3. scroll data area
4. See error

### Environment


docker build
commit 0f731f27e4b41ca63689094ad57c94cc5aa75b89
- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x ] I have reproduced the issue with at least the latest released version of superset.
- [ x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context


",,,,,,,,,,,,,,
12443,OPEN,[settings] List Users-> Filter changed by can be set with empty value,,2021-01-12 17:15:16 +0000 UTC,adam-stasiak,Opened,,"When I go to List Users
And Set filter Changed By with empty value (yes it is possible to choose empty value from dropdown)
Then I click Search
### Expected results

I am not able to choose empty value from dropdown. When value is not set -> search operation should be not permitted.

### Actual results

I can set empty value and then application got an error from backend.

#### Screenshots
<img width=""1155"" alt=""Zrzut ekranu 2021-01-12 o 13 05 22"" src=""https://user-images.githubusercontent.com/25153919/104313056-c5502d80-54d7-11eb-95d5-28afbe9b2975.png"">

https://user-images.githubusercontent.com/25153919/104313080-ce40ff00-54d7-11eb-8122-64a112e1a5a1.mov




#### How to reproduce the bug

1. Go to list users
2. add filter Changed by
3. set empty value from dropdown
4. click search

### Environment

commit 0f731f27e4b41ca63689094ad57c94cc5aa75b89
### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x ] I have reproduced the issue with at least the latest released version of superset.
- [x ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

",,,,,,,,,,,,,,
12442,OPEN,[data] Filter dropdowns are not in the line with filter name,global:listview,2021-01-12 17:26:29 +0000 UTC,adam-stasiak,Opened,,"## Screenshot

![image](https://user-images.githubusercontent.com/25153919/104311694-dac45800-54d5-11eb-8c9f-aa9844771ae1.png)


## Description
Use 0f731f27e4b41ca63689094ad57c94cc5aa75b89 commit
Go to Data> Databases

You can see that Expose in SQL Lab is not straight with dropdown. It should be in the same line.
## Design input

",,,,,,,,,,,,,,
12441,OPEN,[welcome] Share Success toast for sql query is displayed too right,global:homepage,2021-01-12 17:26:43 +0000 UTC,adam-stasiak,Opened,,"## Screenshot

Go to /superset/welcome
Tap on sql query share option

https://user-images.githubusercontent.com/25153919/104311125-0e52b280-54d5-11eb-94c6-90ad7223cf83.mov

## Description

You can see that success toast is displayed too right and not fully visible
## Design input
",,,,,,,,,,,,,,
12439,OPEN,DOCUMENTATION_ICON and DOCUMENTATION_TEXT not supported,doc:user,2021-01-23 21:00:04 +0000 UTC,Asturias-sam,Opened,,"A clear and concise description of what the bug is.

### Expected results

When user sets the  DOCUMENTATION_ICON and DOCUMENTATION_TEXT it should be visible on top of the page 

### Actual results

DOCUMENTATION_URL is working fine, but not able to set DOCUMENTATION_ICON and DOCUMENTATION_TEXT, i guess this code needs to be fixed https://github.com/apache/superset/blob/6df822438121b02bdcb26108f41df0ad290703a4/superset-frontend/src/components/Menu/Menu.tsx#L247

#### Screenshots

If applicable, add screenshots to help explain your problem.


### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,,,,,,,,,,,,
12436,OPEN,[sql_lab]Display Rows Returned / Clarify when LIMIT is reached,assigned:preset; sql_lab:editor,2021-02-09 17:58:32 +0000 UTC,eschutho,In progress,,"**Is your feature request related to a problem? Please describe.**
- Users can't easily understand how many rows their query returned
- Users can't easily tell when the SQL Lab LIMIT is reached, so they don't know if they are only seeing a partial result set. This leads to users accidentally exporting partial datasets to CSVs

**Describe the solution you'd like**
Show rows returned in south pane of sql lab when they run a search

(https://www.figma.com/file/5db2MpaKNnC1c7MK9ysdeV/SQL-Limit-Indicator?node-id=90%3A71)",,,eschutho,"
--
@yousoph @steejay can you help with designs?
--

--
> @eschutho
> thanks for filing the issues, could you please follow the label guideline below, and use [sql_lab] as issue prefix, thanks!
> 
> _As you might have noticed, all labels(212 total) are now specified with prefix and color-coded by types and product area.
> By tagging the right labels, you can help us more efficiently sort, identify and prioritize issues, questions, and PRs, ultimately helping us build a better product for the community!
> 
> https://github.com/apache/incubator-superset/labels?page=5&sort=name-asc
> 
> Here are some labels applying rules we recommend the community to follow -
> select one label describing the product area (e.g. install:dependencies, viz:explore:savemodal, viz:chart-nightingale)
> if you are reporting a bug, select a label describing the specific type of bug (cosmetic, performance, regression)
> if you are using the enhancement request, use enhancement: committed to indicate you are working on this request, otherwise use enhancement: request
> if you are working on a raised issue that needs more details or other inputs, etc., use the label starting with need: (e.g., need: screenshot)
> some labels are 'reserved' for a small group of people who actively maintain the repo and direct resources. 1. assigned:_____ 2. attn:blocking and attn:rush 3. P0, P1, P2_

@junlincc I don't have access to create labels, but thanks for the heads up. Are you proposing an issue prefix in addition to the label? If so, maybe we can mention that in the issue template. That could help guide people. 
--
",junlincc,"
--
@eschutho 
thanks for filing the issues, could you please follow the label guideline below, and use [sql_lab] as issue prefix, thanks! 

_As you might have noticed, all labels(212 total) are now specified with prefix and color-coded by types and product area.
By tagging the right labels, you can help us more efficiently sort, identify and prioritize issues, questions, and PRs, ultimately helping us build a better product for the community!

https://github.com/apache/incubator-superset/labels?page=5&sort=name-asc

Here are some labels applying rules we recommend the community to follow -
select one label describing the product area (e.g. install:dependencies, viz:explore:savemodal, viz:chart-nightingale)
if you are reporting a bug, select a label describing the specific type of bug (cosmetic, performance, regression)
if you are using the enhancement request, use enhancement: committed to indicate you are working on this request, otherwise use enhancement: request
if you are working on a raised issue that needs more details or other inputs, etc., use the label starting with need: (e.g., need: screenshot)
some labels are 'reserved' for a small group of people who actively maintain the repo and direct resources. 1. assigned:_____ 2. attn:blocking and attn:rush 3. P0, P1, P2_
--

--
agree! will get to it soon! @eschutho 
--
",Steejay,"
--
wanted to share this mockup here that shows a row returned indicator with additional info upon hover. I positioned the indicator under the results tab since it has a direct relationship with the result table. 

The indicator displays w a success check icon when the row returned matches the limit dropdown or the limit specified in the query w a tooltip that adds context.

The indicator displays with a warning icon when the row returned _does not_ match the limit dropdown w a tooltip that describes why.  ie ""The number of rows displayed is limited by the flag DISPLAY_MAX_ROWS = 1000"".

![Frame 254](https://user-images.githubusercontent.com/60786102/106502182-e5409480-6478-11eb-9e07-4814ea5a9282.jpg)

![Frame 255 (1)](https://user-images.githubusercontent.com/60786102/106503021-ffc73d80-6479-11eb-8750-1dafbe2684a0.jpg)

cc @yousoph @tooptoop4 @himanshpal @ramyarajasekaran @mihir174 

--

--
@tooptoop4 yes. anytime a query runs successfully there will always be a row returned indicator. here is another exploration that exposes the copy so that it does not rely on hover and hopefully simplifies the logic a little more.

_row indicator_ shows anytime a query successfully runs
_warning_ shows anytime the result is limited from the server side (config)
_message_ shows anytime the result is limited by something from Superset  (config, limit dropdown, query)

![Frame 285](https://user-images.githubusercontent.com/60786102/106528219-45493200-649d-11eb-9c38-e184dbf51623.jpg)


cc @yousoph
--

--
Figma spec: 
https://www.figma.com/file/5db2MpaKNnC1c7MK9ysdeV/SQL-Limit-Indicator?node-id=90%3A0
--
",tooptoop4,"
--
@Steejay if less rows than limit returned does it show the number returned?
--
",yousoph,"
--
Thanks @Steejay ! I like the indicator + messages in the second exploration you shared 

cc @zuzana-vej as well, this is related to #10330 that you raised 
--

--
Let's use this issue to just add the number of rows returned, ie x rows returned (https://www.figma.com/file/5db2MpaKNnC1c7MK9ysdeV/SQL-Limit-Indicator?node-id=90%3A71) 

Related follow up tickets to add the additional indicators and messaging: 
#13018 and #10330
--
",,
12435,OPEN,[sql_lab]Create export functionality for saved queries,assigned:preset; sql_lab:saved_queries,2021-02-05 17:45:45 +0000 UTC,eschutho,In progress,,"**Is your feature request related to a problem? Please describe.**
We have the backend api for saved queries, and would like to add a button next to each query in the list so that the user can export their saved queries and download a zip file that can be imported later. 

**Describe the solution you'd like**
Add an export button on the saved queries page with a tooltip, similar to the charts list page: 
<img width=""390"" alt=""_DEV__Superset"" src=""https://user-images.githubusercontent.com/5186919/104266735-1105c700-5445-11eb-8077-f47a25d7d98f.png"">
charts example

-------------

<img width=""1787"" alt=""_DEV__Superset"" src=""https://user-images.githubusercontent.com/5186919/104266753-195e0200-5445-11eb-9bc8-136c9b7aae0c.png"">
current saved queries page

**Additional context**
The api exists at `api/v1/saved_query/export/`
",,,eschutho,"
--
I have someone to work on this. 
--
",yousoph,"
--
@lyndsiWilliams there's a feature flag for the imports and exports called VERSIONED_EXPORT that you'll need to set to True to see the exports for dashboards, charts, datasets, and databases 
--
",,,,,,,,
12434,OPEN,[sql_lab]Implement export feature for saved queries (front end only),assigned:preset; sql_lab:saved_queries,2021-01-12 06:20:27 +0000 UTC,eschutho,Opened,,"**Is your feature request related to a problem? Please describe.**
We have the backend api for saved queries, and would like to add a button next to each query in the list so that the user can export their saved queries and download a zip file that can be imported later. 

**Describe the solution you'd like**
Add an export button on the saved queries page, similar to the charts list page: 
<img width=""390"" alt=""_DEV__Superset"" src=""https://user-images.githubusercontent.com/5186919/104266735-1105c700-5445-11eb-8077-f47a25d7d98f.png"">
charts example

<img width=""1787"" alt=""_DEV__Superset"" src=""https://user-images.githubusercontent.com/5186919/104266753-195e0200-5445-11eb-9bc8-136c9b7aae0c.png"">
current saved queries page

**Additional context**
The api exists at `api/v1/saved_query/export/`
",,,,,,,,,,,,,,
12431,OPEN,[sql_lab]Add back description column to saved queries,assigned:preset; sql_lab:saved_queries,2021-01-12 16:40:42 +0000 UTC,eschutho,In progress,,"**Is your feature request related to a problem? Please describe.**
The description column was removed from the saved queries. We would like to add it back. 
Styling TBD

**Describe the solution you'd like**
Show the description of the saved query if it exists on the saved queries page.

**Describe alternatives you've considered**
Not showing the description, but it seems helpful.


**Additional context**
<img width=""1779"" alt=""Banners_and_Alerts_and__DEV__Superset"" src=""https://user-images.githubusercontent.com/5186919/104261470-77d1b300-543a-11eb-8b91-0f0f13ff29be.png"">
",,,eschutho,"
--
@yousoph @steejay can you help with designs? Thanks!
--
",Steejay,"
--
@eschutho @yousoph yes! ill make a design ticket for this and coordinate w Sophie
--
",,,,,,,,
12429,OPEN,[sql_lab]Ctrl + t shortcut in SQL Lab opens new tab in Chrome/PC,assigned:preset; sql_lab:editor,2021-04-05 20:42:52 +0000 UTC,eschutho,In progress,,"The tooltip for the new tab instructs users to use `Ctrl + t` for a new tab, but this doesn't work on a PC. On a PC `Ctrl + t` opens a new browser tab.
<img width=""585"" alt=""_DEV__Superset"" src=""https://user-images.githubusercontent.com/5186919/104260080-94202080-5437-11eb-8344-ed014ca9b5c0.png"">

Suggested solution: change the tooltip to suggest using `Ctrl +q` for PC users and update the keyboard shortcuts. 

### Expected results

The instructions in the tooltip should open a new SQL Lab tab.

### Actual results

Following the instructions on a PC opens a new Chrome tab.

#### How to reproduce the bug

1. Go to `/superset/sqllab`
2. Hover over the ""+"" icon next to the tabs for the instructions
3. Hit `Ctrl + t` in Chrome on a PC
4. A new Chrome tab opens

### Environment

- superset version: `1.0`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar. (Closed that issue for this one which is more accurate given the recent changes)
",,,eschutho,"
--
I'm going to claim this ticket for someone on my team. 
--

--
@yousoph ?
--
",hughhhh,"
--
@lyndsiwilliams is working on this now
--
",rumbin,"
--
Not sure if this is caused by the same effects, but Ctrl+X for cutting is also not working on a PC in Chrome or Firefox.
Should I open a new ticket for this or is it closely related?
--
",yousoph,"
--
Hi @rumbin , a new issue would be good for the ctrl+x shortcut. This particular one should be resolved already with this change: https://github.com/apache/superset/pull/12772 

Thanks! 
--
",,,,
12422,OPEN,[Dashboard][Native Filter] Field select in forever loading state when Datasource is not selected,assigned:polidea; viz:dashboard:native-filter,2021-01-18 17:59:53 +0000 UTC,ktmud,Opened,,"## Screenshot

In the new native filter

""FIELD"" in forever loading state when ""DATASOURCE"" is not selected:

![image](https://user-images.githubusercontent.com/335541/104243353-498dac80-5415-11eb-8261-de792cac80f0.png)

BTW, we should probably rename ""DATASOURCE"" to ""Dataset"".

Some other potential TODOs:

1. Automatically select the main datasource in the dashboard
2. Limit the datasource select to only those used in the dashboard (debatable)
",,,junlincc,"
--
man!! i'm still in Explore and you already started on the Dashboard!!! @ktmud  
--
",,,,,,,,,,
12421,OPEN,[Explore]Dataset icon should not change size when dragging,bug:cosmetic; good first issue; viz:explore:datapanel,2021-01-11 23:59:13 +0000 UTC,adam-stasiak,Opened,,"
## Screenshot
https://user-images.githubusercontent.com/25153919/104241922-a8c3df00-545e-11eb-982b-3df602181738.mov

## Description

When dragging column I can see that only dataset icon changes size. I think it should remain the same size as other icons in this area (collapse error).

## Design input

",,,,,,,,,,,,,,
12419,OPEN,Email Report for Dashboard has */10 * * * * but only sent once per hour,global:report,2021-01-12 17:34:33 +0000 UTC,norus,Opened,,"I have the ""latest"" tag running inside Docker and the email functionality is working well. I'm using the default hourly schedule which is picked up by Beat just fine:

```
    CELERYBEAT_SCHEDULE = {
        'email_reports.schedule_hourly': {
            'task': 'email_reports.schedule_hourly',
            'schedule': crontab(minute='1', hour='*'),
        },
    }
```

However, when creating this email report, I specified crontab to run every 10 minutes (see screenshot).

### Expected results

Expecting Celery worker to run the Email Report as per the crontab (*/10 * * * *) and not once per hour.

### Actual results

Email Report is only sent once per hour.
```
[2021-01-11 19:01:00,223: INFO/MainProcess] Received task: email_reports.send[685edddf-e689-46e3-af7e-f21966004d61]  ETA:[2021-01-11 19:00:00+00:00]
[2021-01-11 19:01:00,227: INFO/ForkPoolWorker-1] Task email_reports.schedule_hourly[2edcfa44-76cb-44e8-a2f0-a67707550fec] succeeded in 0.024685528995178174s: None
[2021-01-11 20:01:00,152: INFO/MainProcess] Received task: email_reports.schedule_hourly[5ccec57b-a8f7-451e-8016-28d8b6ceca70]
[2021-01-11 20:01:00,172: INFO/ForkPoolWorker-1] Task email_reports.schedule_hourly[5ccec57b-a8f7-451e-8016-28d8b6ceca70] succeeded in 0.018417919003695715s: None
```

#### Screenshots

![Screen Shot 2021-01-11 at 9 53 31 PM](https://user-images.githubusercontent.com/234848/104237110-dbb6a480-5457-11eb-93cd-395c22384533.png)

### Environment

- superset version: 0.38.0
- python version: 3.7.9
- node.js version: v12.20.1

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.",,,graceguo,"
--
I had similar problem, so i created this PR to fix it: https://github.com/apache/superset/pull/11414, which is after release 0.38. You can try it.
--
",,,,,,,,,,
12416,OPEN,[Chart] Metric columns are not in the correct order in bar chart,viz:chart-bar,2021-01-11 19:24:58 +0000 UTC,duynguyenhoang,Opened,,"When design a bar chart, I encounter problem with the order of metric columns. They are not in the correct order.

### Expected results

Metric columns are preserved correct order in the request

### Actual results

Metrics columns is sorted by alphabet

#### Screenshots

Before 
![Current Problem](https://user-images.githubusercontent.com/7106179/104222466-eb90b180-5474-11eb-9a8f-6fc4a9751aba.png)

After
![Expectation](https://user-images.githubusercontent.com/7106179/104222520-006d4500-5475-11eb-87bb-3f3608072443.png)

#### How to reproduce the bug

1. Go to explore window and design any bar chart
2. Create list of metrics, name them as any order in alphabet (Example: z_metric, a_metric, k_metric)
3. Run the visualization
4. See the problem, now the columns are: a_metric, k_metric, z_metrics which are not in correct order.

### Environment

- superset version: master branch
- python version: 3.8

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,junlincc,"
--
@duynguyenhoang thanks so much, Duy! Every issue you open comes with a fix! Appreciate your help to improve Chart and Explore lately. If you are looking into making a more significant impact on Superset, please reach out to me on slack! 
--
",,,,,,,,,,
12414,OPEN,Add user role programmatically,question,2021-01-11 19:42:09 +0000 UTC,javidov,Opened,,"Hello all,
Is it possible to create a new user role based on permissions programmatically using FAB? I mean without having to use the UI.

It would be nice if we can create the role, then create a user and assign the created role   

",,,,,,,,,,,,,,
12375,OPEN,[Explore] Saved metrics label overrides not working,assigned:polidea; bash!; viz:explore:control,2021-01-23 20:05:21 +0000 UTC,ktmud,In progress,,"#### Screenshots

<img src=""https://user-images.githubusercontent.com/335541/104081231-2d55f980-51e2-11eb-96db-e6ee8003465a.png"" width=""600"">

#### How to reproduce the bug

The new AdhocMetric control made it possible to add a custom label for saved metrics, but the added override doesn't actually affect anything. This is confusing. We should either disable ""Click to edit label"" or allow the label override to propagate to the selected pills and charts. The former is probably easier.

1. Add a saved metric
2. Click on the popover title to edit the metric label

### Environment

Latest master

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

BTW, I really think saved metrics should be the first tab (at least when the dataset has some saved metrics), because the point of having saved metrics is to have an easy access to them. We should also encourage the use of saved metrics as it: 1) helps organizations reuse the same metric definition; 2) saves the labor of reconfigure the same metric.",,,junlincc,"
--
> BTW, I really think saved metrics should be the first tab (at least when the dataset has some saved metrics), because the point of having saved metrics is to have an easy access to them. We should also encourage the use of saved metrics as it: 1) helps organizations reuse the same metric definition; 2) saves the labor of reconfigure the same metric.

can we conduct user research or at least a poll to have some data point to support this proposed change? 
--

--
> @ktmud @junlincc I like the idea of supporting custom labels for saved metrics. If you agree I can fix this.

please do! but hold off from switching tabs. @ktmud your request and reasoning makes sense, please allow research and design process to take place first. 
--
",ktmud,"
--
I've heard many times at Airbnb that it's beneficial to have shared metric definitions. Other company may have different needs, but the discussion can start from some simple reasoning.

What was the reasoning behind putting SIMPLE as the default tab? Is it because we don't want to show users an empty list of saved metrics? Then what do you think of conditionally change the default tab for new metrics based on whether there are saved metrics in the datasource or not?
--

--
> @ktmud @junlincc I like the idea of supporting custom labels for saved metrics. If you agree I can fix this.

@villebro could you hold until #10270 and https://github.com/apache-superset/superset-ui/pull/889 are merged? I just need to add more test cases and fix the CI. There are some (relatively large) refactoring related to QueryObject and formData, which may very likely conflict with whatever changes you will make in this area.
--
",villebro,"
--
@ktmud @junlincc I like the idea of supporting custom labels for saved metrics. If you agree I can fix this.
--

--
> @villebro could you hold until #10270 and https://github.com/apache-superset/superset-ui/pull/889 are merged? I just need to add more test cases and fix the CI. There are some (relatively large) refactoring related to QueryObject and formData, which may very likely conflict with whatever changes you will make in this area.

Sure thing 

--
",,,,,,
12364,OPEN,[Explore] Drag & Drop Adhoc Metric while the Popover is open messes things up,assigned:polidea; bash!; viz:explore:control,2021-01-26 07:57:05 +0000 UTC,ktmud,In progress,,"#### Screenshots

![drag-drop](https://user-images.githubusercontent.com/335541/104071967-0a661e00-51bf-11eb-979e-1231878a3015.gif)


#### How to reproduce the bug

1. Add at least two metrics
2. Click to open the popover of the second metric
3. Drag & Drop to change order
4. Save the popover. 
5. It should have updated the original metric but it updates the other metric at the original position instead.
6. Then if change the other metric, both metrics are updated.

### Environment

Latest master

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,ktmud,"
--
@junlincc I don't think this is a cosmetic issue. It's a functionally incorrect behavior.
--

--
https://user-images.githubusercontent.com/335541/105816164-ce75db80-5f68-11eb-819c-c1fa9d1b4618.mp4

This bug is still reproducible. As noted in the original issue description, it happens when you do Drag & Drop while the popover is open.
--
",junlincc,"
--
Sounds right.major refactoring work for control fields is gonna happen soon.We will address all related known issue together. But feel free to add p1 label if you consider this particular one as 1.0 release blocker @ktmud 
--

--
This behavior only happens when you have two same metrics. which means... it has been resolved by https://github.com/apache/superset/pull/12657 
--
",,,,,,,,
12343,OPEN,[Design Proposal]Capitalization guidelines for consistency,.pinned; assigned:turing; design:system,2021-01-22 12:51:51 +0000 UTC,mihir174,In progress,,"This design proposal aims to make text consistently capitalized across the product, to address issues like https://github.com/apache/superset/issues/12023

Here are the guidelines @steejay and I developed, based on design systems from Google, IBM and Atlassian. 

### Sentence case
Use sentence-case capitalization for everything in the UI (except these **). 

Sentence case is predominantly lowercase. Capitalize only the initial character of the first word, and other words that require capitalization, like:
- **Proper nouns.** Objects in the product _are not_ considered proper nouns e.g. dashboards, charts, saved queries etc. Proprietary feature names eg. SQL Lab, Preset Manager _are_ considered proper nouns
- **Acronyms** (e.g. CSS, HTML)
- When referring to **UI labels that are themselves capitalized** from sentence case (e.g. page titles - Dashboards page, Charts page, Saved queries page, etc.)
- User input that is reflected in the UI. E.g. a user-named a dashboard tab

**Sentence case vs. Title case:**
Title case: ""A Dog Takes a Walk in Paris""
Sentence case: ""A dog takes a walk in Paris""

**Why sentence case?**
- Its generally accepted as the quickest to read
- Its the easiest form to distinguish between common and proper nouns

### How to refer to UI elements
When writing about a UI element, use the same capitalization as used in the UI. 

For example, if an input field is labeled Name then you refer to this as the Name input field. Similarly, if a button has the label Save in it, then it is correct to refer to the Save button. 

Where a product page is titled Settings, you refer to this in writing as follows: 
Edit your personal information on the Settings page.

Often a product page will have the same title as the objects it contains. In this case, refer to the page as it appears in the UI, and the objects as common nouns:

- Upload a dashboard on the Dashboards page
- Go to Dashboards
- View dashboard
- View all dashboards
- Upload CSS templates on the CSS templates page
- Queries that you save will appear on the Saved queries page
- Create custom queries in SQL Lab then create dashboards


### **Exceptions to sentence case:
- Input labels, buttons and tabs are all caps
",,,mihir174,"
--
fyi @mistercrunch @zuzana-vej @graceguo-supercat 
--

--
@ktmud yes - totally agree with user inputs appearing exactly as they were inputted 
--

--
Made some minor additions + clarifications to the initial proposal @amitmiran137 @michael-s-molina @ktmud 

Added under **Sentence case**: 
""User input that is reflected in the UI. E.g. a user-named a dashboard tab""

Clarified in **Exceptions to sentence case**:
""_Input_ labels, buttons and tabs""


--
",junlincc,"
--
Awesome! Thank you both. @mihir174 @Steejay Kicking off the implementation following this guideline! 
--

--

> Thanks for the detailed proposal and the reasoning! I believe Superset will greatly benefit from more guidelines like this. Maybe we could even develop our own design system!
> 
> Do you think it makes sense to add following exception to the proposal as well?
> 
> > In general, user inputs should not change cases.

+1. no more forcing upper case in the product.. 
--
",ktmud,"
--
Thanks for the detailed proposal and the reasoning! I believe Superset will greatly benefit from more guidelines like this. Maybe we could even develop our own design system!

Do you think it makes sense to add following exception to the proposal as well?

> In general, user inputs should not change cases.
--
",michael,"
--
Guidelines added to `CONTRIBUTING.md`.
--
",,,,
12339,OPEN,"Changing the ""Table Timestamp Format"" gives incorrect results.",need:followup; viz:chart-table,2021-04-09 06:40:27 +0000 UTC,qyra,In progress,,"I created a Table chart with a time column

With the default Adaptative formatting it displays correctly:
A `2020-11-30T20:37:30.122883`
B `2020-11-30T20:37:17.179797`
C `2020-11-30T20:31:41.414866`

When I change Table Timestamp Format to `%Y-%m-%d %H-%M-%S` I would expect that the decimal portion of the seconds should be removed, but that the actual date should be identical. Instead the times shift by many hours.
A `2020-12-01 01:37:30`
B `2020-12-01 01:37:17`
C `2020-12-01 01:31:41`

All of these timestamps are off by ~5:00:00 so presumably there's some odd behaviour related to timezones.

### Environment
- superset version: `superset version`
Superset 0.38.0

- python version: `python --version`
Python 3.7.5

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.90. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",ktmud,"
--
The timestamps are always displayed as UTC times. This is by design. There are many challenges related to synching timezones between Superset, the datasources, and the client. We would need to think about this problem more thoroughly and come up with a comprehensive plan if we want to address this.
--
",qyra,"
--
It seems to me like they are not being displayed as UTC times. The query itself returns correct data and the CSV export is also correct. I am in the ET timezone so it would make sense that a 5 hour offset would come up if something was trying to convert times to a local timezone or back. (The times being displayed are actually japanese timestamps in the underlying postgres query, but that does not match the offset I am seeing)
--
",wisonwang,"
--
i see the issue too.  And  maybe i found the reason:
When create a chart with time grains, superset will use it's default db_engine grains(mysql, db2, clickhouse...), like clickhouse:
[https://github.com/apache/superset/blob/master/superset/db_engine_specs/clickhouse.py](https://github.com/apache/superset/blob/master/superset/db_engine_specs/clickhouse.py)

code: 

> class ClickHouseEngineSpec(BaseEngineSpec):  # pylint: disable=abstract-method
>     """"""Dialect for ClickHouse analytical DB.""""""
> 
>     engine = ""clickhouse""
>     engine_name = ""ClickHouse""
> 
>     time_secondary_columns = True
>     time_groupby_inline = True
> 
>     _time_grain_expressions = {
>         None: ""{col}"",
>         ""PT1M"": ""toStartOfMinute(toDateTime({col}))"",
>         ""PT5M"": ""toDateTime(intDiv(toUInt32(toDateTime({col})), 300)*300)"",
>         ""PT10M"": ""toDateTime(intDiv(toUInt32(toDateTime({col})), 600)*600)"",
>         ""PT15M"": ""toDateTime(intDiv(toUInt32(toDateTime({col})), 900)*900)"",
>         ""PT0.5H"": ""toDateTime(intDiv(toUInt32(toDateTime({col})), 1800)*1800)"",
>         ""PT1H"": ""toStartOfHour(toDateTime({col}))"",
>         ""P1D"": ""toStartOfDay(toDateTime({col}))"",
>         ""P1W"": ""toMonday(toDateTime({col}))"",
>         ""P1M"": ""toStartOfMonth(toDateTime({col}))"",
>         ""P0.25Y"": ""toStartOfQuarter(toDateTime({col}))"",
>         ""P1Y"": ""toStartOfYear(toDateTime({col}))"",
>     }

to fix this issue , you can change the code to support your  local time zone. or if you just need dt grains, you can just let zhe chart's time grains to null.
 
--
",ITCheng0712,"
--
I tried to modify the code for clickhouse.py, but the page display did not change
--
",,
12332,OPEN,SQL Lab Table Selector is very laggy when typing,bug:performance; bug:regression; sql_lab:editor,2021-02-03 17:55:30 +0000 UTC,etr2460,In progress,,"Typing in the table selector in SQL Lab recently became much more laggy. I know at one point there were perf improvements made to improve this, but they might've gotten lost in a refactor.

### Expected results
Typing in the selector to be snappy

### Actual results
3-4 seconds of latency between typing and getting results when the schema has ~260k tables

#### How to reproduce the bug

1. Have a schema with a lot of tables in it
2. Load the schema in SQL Lab
3. Search for tables in the table selector
4. See each keystroke take seconds to resolve

### Environment

(please complete the following information):

- superset version: Cut from master on Dec. 18th
",,,etr2460,"
--
cc @betodealmeida or @hughhhh who I think have been active in SQL Lab recently. Are you aware of any changes that were made to the table selector on the frontend that could've regressed the filtering speed?
--
",,,,,,,,,,
12324,OPEN,[Explore]Set default filterbox value using SQL statement,enhancement:request; viz:chart-filterbox,2021-01-07 21:18:19 +0000 UTC,stevensuting,Opened,,"**Describe the solution you'd like**
Currently Superset allows us to set static default values to the filter box:
<img width=""297"" alt=""Screenshot 2021-01-07 at 1 07 35 PM"" src=""https://user-images.githubusercontent.com/8875448/103864959-76765280-50e9-11eb-8b32-88224d511b97.png"">

What I would like is for this value to be dynamic based on some condition set via SQL.
For example, we have a column called Team with values (David's Team, Jack's Team, Tim's Team)
Team is used on a Filter Box for a dashboard.
- When David logs in, the value on the filterbox should be David's Team
- When Jack logs in, the value on the filterbox should be Jack's Team

A sample SQL for this could be:
`SELECT team_name from table where manager_name = '{{current_username()}}'`

**Describe alternatives you've considered**
A potential unclean solution that uses url parameters, like
http://localhost:8088/superset/dashboard/1/?preselect_filters={""1"": {""Team"": ""David's Team""}}
http://localhost:8088/superset/dashboard/1/?preselect_filters={""1"": {""Team"": ""Jack's Team""}}

Using Ngnix+Lua to do a URL redirect when a particular user logs in
",,,,,,,,,,,,,,
12323,OPEN,Query changes on Sql Lab do not reflect on Slices that use them,sql_lab:editor,2021-01-07 15:11:33 +0000 UTC,stevensuting,Opened,,"### How to reproduce the bug

If a query on SQLlab  is used to create a slice by:
 **_Sql Lab -> Write Query -> Save -> Explore -> Create Slice -> Saved_**
Lets say the query used is: `SELECT name from table;`
And the slice is called **_Details_Chart_**
Work great to this point.

Now, if we want to modify the query to `SELECT name, address from table;`
**_Sql Lab -> Saved Queries -> Edit Query -> Update the Query -> Save (UPDATE)_**

If we go slice to **_Details_Chart_** it will only show `name` as available column and not `name` & `address`
However, If we access Edit Dataset via the Explore Slice editor, it shows the updated SQL query ie  `SELECT name, address from table;`

### Expected results

It would be great if the update to the Sql Lab query pushed down to entities that use it. Much like how the Legacy editor already does.

### Alternatives
For now I use the following methods to solve this:
#1 
1. Go to Sources -> Tables -> Remove the SQL LAB VIEW Filter -> Hit refresh
2. Navigate to the table you want to edit and click on edit record
3. Update the SQL query under Detail -> SQL
4. Then click on the Column tab and add new column 
5. Hit save. 
6. Navigate to the chart and you will find the column available.


#2
1. Go to SQL Lab -> Saved queries -> Navigate to the query you want to edit -> Click on Pop tab link
2. Make modifications to the SQL query -> Run the query
3. Hit the Save button, then select the update option with an appropriate name.
4. The click on Explore. 
5. Rebuild the chart

### Environment

(please complete the following information):

- superset version: `0.37 to 0.38`
- python version: `3.7`


### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [Y] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [Y] I have reproduced the issue with at least the latest released version of superset.
- [Y] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,,,,,,,,,,,,
12322,OPEN,Forgot Password feature,enhancement:request,2021-01-07 14:28:04 +0000 UTC,stevensuting,Opened,,"**The Issue**
Currently if a user forgets their password they need to inform the Superset admin from outside the superset environment via email, slack. After which the admin either:
1.  Runs a CLI command
`superset fab reset-password --username name@domain.com  --password the_new_password`
OR
2. Deactivates the user from Superset UI with the need to change all mandatory fields to prevent referential integrity conflicts on the DB and then recreates the user.


**Desired Solution**
Adding a forgot password option/button on the login page that any user can access themselves to change their passwords would make it so much efficient for Admins to manage.

_Note: The only change/update password feature that exist is after the user has successfully logged in._",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.94. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",,,,,,,,,,
12314,OPEN,[Explore]Rolling Function in charts should have the option of using calendar dates for periods,enhancement:committed; viz:explore:control,2021-02-16 05:42:55 +0000 UTC,cooley-pe,Opened,,"**Is your feature request related to a problem? Please describe.**
Rolling sums (and the like) dont handle sparse dates in an intuitive way. For example if you have one data point per year for 10 years, but want a 365 day rolling sum (time grain: day, Periods: 10 is for illustration purposes, as a 365 day rolling sum would be Period: 10), it will count all 10 of those data points, even though theyre over the span of 10 years.
![Screen Shot 2021-01-05 at 3 34 30 PM](https://user-images.githubusercontent.com/64038793/103822517-25048f80-5025-11eb-84d0-f1da8b0d9886.png)
![Screen Shot 2021-01-05 at 3 33 17 PM](https://user-images.githubusercontent.com/64038793/103822521-27ff8000-5025-11eb-9e0b-820e29dbac51.png)


**Describe the solution you'd like**
There should be an option (or it should be defaulted) to use calendar dates for the rolling calculation regardless of whether they appear in the data set. [Pandas rolling() function supports](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rolling.html) this by making the date the Index.

**Describe alternatives you've considered**
Make this the default and only functionality. I can't think of a time that the current behavior would be preferable.


",,,junlincc,"
--
thanks for suggesting! 

@zhaoyongjie do we fell comfortable including this in 2021 q2? 
--
",zhaoyongjie,"
--
In my Q2-Q3 plan, @junlincc 
--
",,,,,,,,
12304,OPEN,.dockerignore file problem,install:docker,2021-01-07 06:54:12 +0000 UTC,JayGuAtGitHub,In progress,,"In my production superset website, I found that it still call the webpack debug server like 

http://localhost:9000/sockjs-node/info?t=1609923238691

After long time debug, I found that, in my project folder, /superset/static, there were debug js files in that folder like:

/superset/static/assets/dashboard.ede529be.entry.js

When run the docker build command, all these files will be copy to the docker environment via the line :

COPY superset /app/superset

I checked .gitignore file and found it contains (which .dockerignore doesn't contain) the following lines:

superset/static/assets
superset/static/version_info.json

Should it be also included in .dockerignore file?

before fix:

url: /superset/dashboard/20

the end of the html:


<!-- Bundle js menu START -->
--
 | 
 | <script src=""/static/assets/vendors-major.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/vendors-addSlice-crudViews-dashboard-explore-menu-preamble-profile-showSavedQuery-sqllab.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/vendors-addSlice-crudViews-dashboard-explore-menu-preamble-profile-sqllab.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/vendors-addSlice-crudViews-dashboard-explore-menu-profile-showSavedQuery-sqllab.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/vendors-addSlice-crudViews-dashboard-explore-menu-profile-sqllab.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/vendors-addSlice-crudViews-dashboard-explore-menu-sqllab.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/menu.ede529be.entry.js""></script>
 | 
 | <!-- Bundle js menu END -->
 | 
 | 
 | 
 | 
 | <!-- Bundle js dashboard START -->
 | 
 | <script src=""/static/assets/0.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/1.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/5.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/vendors-addSlice-crudViews-dashboard-explore-profile-showSavedQuery-sqllab.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/vendors-addSlice-crudViews-dashboard-explore-profile-sqllab.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/vendors-addSlice-crudViews-dashboard-explore-sqllab.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/7.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/vendors-addSlice-crudViews-dashboard-explore.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/vendors-crudViews-dashboard-explore-sqllab.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/9.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/10.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/vendors-dashboard-explore.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/vendors-dashboard-sqllab.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/vendors-crudViews-dashboard.ede529be.chunk.js""></script>
 | 
 | <script src=""/static/assets/dashboard.ede529be.entry.js""></script>
 | 
 | <!-- Bundle js dashboard END -->

wrong request example:

![image](https://user-images.githubusercontent.com/9403854/103761353-804c7700-5051-11eb-8c23-dfdea9f15f9b.png)




after fix 

url: the same url

end of html:


<!-- Bundle js menu START -->
--
 | 
 | <script src=""/static/assets/vendors-major.8bbbbd6861e6fcf3ab0a.p.chunk.js""></script>
 | 
 | <script src=""/static/assets/vendors-addSlice-menu-preamble-profile.8691d15f3774d396769f.p.chunk.js""></script>
 | 
 | <script src=""/static/assets/menu.34a393af25687694f53f.p.entry.js""></script>
 | 
 | <!-- Bundle js menu END -->
 | 
 | 
 | 
 | 
 | <!-- Bundle js dashboard START -->
 | 
 | <script src=""/static/assets/1.73836e34a18efb7519a2.p.chunk.js""></script>
 | 
 | <script src=""/static/assets/dashboard.ad893fc5e808044d0535.p.entry.js""></script>
 | 
 | <!-- Bundle js dashboard END -->






",,,willbarrett,"
--
@craig-rueda do you think this change should be made?
--
",nytai,"
--
If you're running from port 9000 you're likely running the webpack debug server. You should not be running the debug server in production as it is only meant for debugging (hence the calls to debug files). Instead if you run `npm run build` before building the docker container the production asset bundles will be outputted to `/superset/static/assets` and the flask web server will be serving them instead of the webpack dev server. 
--
",craig,"
--
Agree with @nytai . Can you try using the official image? (https://hub.docker.com/r/apache/incubator-superset)
--
",JayGuAtGitHub,"
--
> Agree with @nytai . Can you try using the official image? (https://hub.docker.com/r/apache/incubator-superset)

I didn't try but I'm definitely sure the official image will be OK. For some reasons I have to clone the repo and add customize functions. That's why I found this issue.
--

--
> If you're running from port 9000 you're likely running the webpack debug server. You should not be running the debug server in production as it is only meant for debugging (hence the calls to debug files). Instead if you run `npm run build` before building the docker container the production asset bundles will be outputted to `/superset/static/assets` and the flask web server will be serving them instead of the webpack dev server.

I'm running in the production envrionment (with port 8088). These steps can explain how this problem appear:

Run ""docker build -t my_customized_superset:latest ./"" to build the customized local docker image

in the Dockerfile

> FROM node:10-jessie AS superset-node
>
>......
>
>RUN cd /app/superset-frontend \
>        && npm run ${BUILD_CMD} \
>        && rm -rf node_modules

this command try to build the production js bundle

>FROM python:${PY_VER} AS lean
>
>......
>
>COPY --from=superset-node /app/superset/static/assets /app/superset/static/assets

this command try to copy all production js bundle from ""superset-node"" to ""docker environment""

> COPY superset /app/superset

this command try to copy all python code from ""packaging host machine envrionment"" to ""docker environment"", in this command, there are some debug js file in the path superset/static/assets (I didn't check why it appears, I think maybe it is generated by docker-compose up). In this situation, the debug js is packaged into producton docker environment. And the js paths were put in the end of the dashboard page html.

I think another reason which cause the problem is that I don't have a CI envrionment. This won't appear in that envrionment beacause CI envionment will never pull the debug js and will never send the files to production environment. Maybe not all project can have a standard workflow especially for those quick-online startup project. Maybe this promotion can help those people to avoid some wired bugs. ( I have to say if you don't master webpack and docker at the same time, it's really hard to find the problem. Maybe the only way it clone project again-change the code-package)






--
",,,,
12302,OPEN,[Explore] Time picker custom setting for timezone,enhancement:committed; viz:explore:timepick,2021-02-16 06:49:53 +0000 UTC,graceguo-supercat,Opened,,"- In Advanced range type, user can type in time with free text (**this is great! l like this design and implementation!!**): 

![advanced](https://user-images.githubusercontent.com/27990562/103743405-f1881d80-4fb0-11eb-8431-9ae1bbfa62a0.gif)


- But in Custom range type, user have to set time by click on hour/minute/second, free text will be overwritten when user close calendar. 
![b8psB5uD5r](https://user-images.githubusercontent.com/27990562/103743964-c9e58500-4fb1-11eb-8110-53192a9ceb51.gif)

- I think the 3 columns for hour/minute/second is a little extra. 
- The settings for Advanced vs Custom should be consistent: one place allow user type free text, the other is not, with no obvious reason or hint from UI. When i type in time in Custom range type and then get removed , i thought it was a bug.

### Environment

latest mater

@junlincc @zhaoyongjie 
",,,junlincc,"
--
Thanks for the nice comment for Advanced! @graceguo-supercat 
much more users can do in the free text input, and it reaches full feature parity with Tableau! 
if you think below would be useful for your users, pls also help test them and give us feedback   @zuzana-vej  @etr2460 
<img width=""1001"" alt=""Screen Shot 2021-01-06 at 6 44 36 AM"" src=""https://user-images.githubusercontent.com/67837651/103781065-b0146400-4fea-11eb-97a1-949470e2f38c.png"">

--

--
again, thanks for the valuable feedback, we will get this enhancement done by 1.0 release. 
The settings for Advanced vs Custom should be consistent 

@zhaoyongjie 
--

--
> is it Midnight UTC time zone?

Yes, it's UTC time zone, and we have plan to allow user to set time zone. it's a committed enhancement in roadmap @graceguo-supercat 
--

--
> Advanced date type has many new operators, like `dateadd`, `datetrunc` etc, which we don't have before. We should offer a full list of feature descriptions for users. Otherwise for those non-tech background users, they don't know how to use it or how to find the secret.

Yes! I am well aware of the need of adding tooltip of advanced syntax description, as I'm one of the non-tech background users. We will get both projects started within a couple weeks. @graceguo-supercat 
--
",graceguo,"
--
This specifications for the new Timepicker is great. I will share with airbnb users after our next release. 
--

--
Thanks @zhaoyongjie I didn't realize `enter key` is the key :)

I have a couple of other requests:

- **Midnight in Custom type**: 

is it Midnight UTC time zone? or user's local time zone? Is it the last midnight or the next midnight in the future? Please make it clear:
<img width=""591"" alt=""Screen Shot 2021-01-07 at 3 56 48 PM"" src=""https://user-images.githubusercontent.com/27990562/103958272-708c6b80-5101-11eb-8e6d-25353a6a94af.png"">


- **Available date operations in Advanced type**:
<img width=""596"" alt=""Screen Shot 2021-01-07 at 4 11 50 PM"" src=""https://user-images.githubusercontent.com/27990562/103958903-27d5b200-5103-11eb-8eb9-21a50b5ccf31.png"">
Advanced date type has many new operators, like `dateadd`, `datetrunc` etc, which we don't have before. We should offer a full list of feature descriptions for users. Otherwise for those non-tech background users, they don't know how to use it or how to find the secret. 

So i am thinking better way to expose advanced operations, for example: 
- add a big tooltip for `Config Advanced Time Range`, and list many usage samples like @junlincc shared, or
- something like Markdown component, it links out to a Markdown Cheatsheet.

cc @junlincc please give some suggestions?



--
",zhaoyongjie,"
--
Hi, @graceguo-supercat. There are some UX usage issues here.

- In the Custom Frame:
The new Datepicker(Antd Datepicker) use mouse `onBlur event` as shortcut to cancel value input. (so the Datepicker value cannot be saved in time)
If you need to confirm the change, you need to use the `enter key` or mouse click the `Ok button`. (submitted Datepicker value)

https://user-images.githubusercontent.com/2016594/103897402-777da300-512e-11eb-80a8-4c428b91f533.mp4

- For Advanced Frame vs Custom Frame consistent
`Custom Frame` is just a subset of the `Advanced Frame` representation range, Advanced Frame has more rich datetime-expression. `Datapicker` in the `Custom Frame` only accepts `YYYY-MM-DD[T]HH:mm:ss` datetime style, while `Advanced Frame` is a free-text input box.


CC: @junlincc 
--
",,,,,,
12301,OPEN,Access Embeddable charts From API via passing AccessToken,question,2021-01-06 14:34:20 +0000 UTC,BigDataArtist,Opened,,"Hi Superset,

I want to access the embeddable charts through an API via passing an access token but in return, it again asks for authentication login. 
Is there somehow any way where we can bypass that step, as we are anyways passing a valid access token in the first step.

",,,,,,,,,,,,,,
12294,OPEN,[cosmetic][explore] Filters popup on Explore - confusing UI experience,assigned:turing; bug:cosmetic; hold!; viz:explore:filter,2021-01-06 18:37:28 +0000 UTC,zuzana-vej,Opened,,"## Screenshot

![Screen Shot 2021-01-05 at 3 52 06 PM](https://user-images.githubusercontent.com/61221714/103712203-0a231400-4f6e-11eb-8bd2-c8b8779dca82.png)


![explore-where](https://user-images.githubusercontent.com/61221714/103712051-ae588b00-4f6d-11eb-99ed-6ba77dcc4cd0.gif)

## Description

The behavior of the new filter dropdown specifically for ""IN"" condition is a bit confusing:

## Design input
1. when user wants to remove an item they click on the ""check mark"" rather than a ""cross"" - this seems confusing
2. when adding new values, user needs to carefully click out within the filter box to be able to see the save button

Note: the screenshot is based on master as of 12/17/2020 so there is a possibility this has been addressed in the meanwhile.",,,junlincc,"
--
we can make that work~! 
--

--
1. @zuzana-vej this styling comes with the AntD component which I also agree that the design makes sense within context. This component is also being used in Native dashboard filter and other places in the future. the effort of changing it in Explore is relatively low, but keeping the consistency across the product will be hard. Can we simply hide the check marks for now, and revisit general issue in the near future? 

--

--
Discussed with @michael-s-molina, any changes here could introduce regression in other areas. To ensure we meet Superset 1.0 release timeline, we need to hold off on making suggested changes. We also spotted related issues in Dashboard, and will provide a general solution based on your user reactions and feedback for all in Q1.  @zuzana-vej 

replacing `p1 `tag with `hold!`
--
",michael,"
--
@junlincc We have some decisions to make. 

Let's start with item 1:

The check mark is a pattern adopted by AntD and actually it makes sense when you analyze the context. Look at the following picture:

<img width=""378"" alt=""Screen Shot 2021-01-06 at 9 24 13 AM"" src=""https://user-images.githubusercontent.com/70410625/103768269-f7a5ea80-5000-11eb-9f01-3c8b1e2acf07.png"">

This filter has value suggestions and the check mark is indicating which of these suggestions are selected. It gets a little strange when we don't have suggestions enabled (`Autocomplete Filters` disabled in dataset config) because only the selected items appear. So we have two options: leave as it is or hide the dropdown when the suggestions are disabled. Like this:

<img width=""336"" alt=""Screen Shot 2021-01-06 at 10 30 24 AM"" src=""https://user-images.githubusercontent.com/70410625/103773700-3a1ff500-500a-11eb-8b57-403c8a64bebd.png"">

To resolve item 2 what we can do is limit the height of all select items and create the necessary space to accommodate the options. Like this:

<img width=""375"" alt=""Screen Shot 2021-01-06 at 9 12 13 AM"" src=""https://user-images.githubusercontent.com/70410625/103774295-18733d80-500b-11eb-8da4-feb3b9bf5d26.png"">
<img width=""373"" alt=""Screen Shot 2021-01-06 at 9 12 37 AM"" src=""https://user-images.githubusercontent.com/70410625/103774302-1ad59780-500b-11eb-83a5-eb8d393ca252.png"">
<img width=""369"" alt=""Screen Shot 2021-01-06 at 10 54 26 AM"" src=""https://user-images.githubusercontent.com/70410625/103776002-98020c00-500d-11eb-9531-0d3569db0f79.png"">
<img width=""367"" alt=""Screen Shot 2021-01-06 at 10 48 22 AM"" src=""https://user-images.githubusercontent.com/70410625/103775458-d5b26500-500c-11eb-9b44-0b2f888c6791.png"">
<img width=""369"" alt=""Screen Shot 2021-01-06 at 10 59 08 AM"" src=""https://user-images.githubusercontent.com/70410625/103776459-3db57b00-500e-11eb-9c62-d79f2786aa24.png"">

If I had to choose between higher selects on top of buttons vs scrolled selects with buttons always visible I would choose the first one. In fact we have a bunch of selects that cover some part of the UI when they are open.

Let me know what you think 



--
",zuzana,"
--
Thanks for the details. Also I m not a designer I found that confusing and wasn't sure if it's bug or as-designed. I think in the first case as explained by @michael-s-molina the checks make complete sense. The only thing that is weird is that when you click on the check the item unselects. But this might be less confusing if the suggestion is enabled.

Are the suggestions disabled across for everyone or is this setting per dataset or setting per Superset installation? (if you know the answer if now we can look into it).

If they are always disabled hiding the check as @junlincc suggested would be little better for now I think.  Otherwise I am okay to keep it and we can see if users have other reactions or feedback.

For point 2 we can also keep what we have and see users reactions or feedback.  
--
",,,,,,
12275,OPEN,Not able to see those charts which is created by the another user; while editing the dashboard,enhancement:request; need:more-info; viz:dashboard:editmode,2021-01-06 11:14:38 +0000 UTC,umesh11111,In progress,,"A clear and concise description of what the bug is.

### Expected results
Those charts should be availale while editing the dashboard so we can add those chart which is created by another user.
what you expected to happen.

### Actual results
Getting those charts which is created by my userid.
what actually happens.

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Go to '...' Any dashboard
2. Click on '....' on edit dashboard, then click on your charts& filters
3. Scroll down to '....' try to see those charts which is created by another user.
4. See error

### Environment

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist
any help would be appreciated. Thanks
Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,zuzana,"
--
If the chart owner is someone else - you can't add that chart into the dashboard, this is by design.  Are you referring to this case (chart owner is someone else) or are you referring to a case where you are co-owner of the chart and still can't add the chart to the dashboard? 
--
",umesh11111,"
--
> If the chart owner is someone else - you can't add that chart into the dashboard, this is by design. Are you referring to this case (chart owner is someone else) or are you referring to a case where you are co-owner of the chart and still can't add the chart to the dashboard?

Thanks for your response,
I am talking about chart owner is someonelse,then we are not able to see that chart while editing the dashboard,
we want to make this as enhancement.
--
",,,,,,,,
12273,OPEN,[dashboard]Add Chart filters displayed in Applied filters section,assigned:polidea; enhancement:committed; viz:dashboard:indicator,2021-01-05 18:53:45 +0000 UTC,adam-stasiak,Opened,,"**Is your feature request related to a problem? Please describe.**
When I set filter in explore view of chart then I would expect to see these filter in Applied (Not in not Applied) filters section.

**Describe the solution you'd like**
Please add an option to display in this section filters chosen for chart from explore view.
![image](https://user-images.githubusercontent.com/25153919/103643827-9bd25780-4f55-11eb-9157-625517eae2b0.png)
![image](https://user-images.githubusercontent.com/25153919/103643853-a4c32900-4f55-11eb-871d-1ce3364d2f58.png)
<img width=""1177"" alt=""Zrzut ekranu 2021-01-5 o 12 58 57"" src=""https://user-images.githubusercontent.com/25153919/103643960-d76d2180-4f55-11eb-8767-1852991b759b.png"">
![image](https://user-images.githubusercontent.com/25153919/103644006-ec49b500-4f55-11eb-8b2e-a12bfef212a1.png)

**Describe alternatives you've considered**
-
**Additional context**
Add any other context or screenshots about the feature request here.
",,,junlincc,"
--
https://github.com/apache-superset/superset-roadmap/issues/96 We are committed to make this happen! thanks for suggesting! @adam-stasiak 


--
",,,,,,,,,,
12272,OPEN,[dashboard]Magnifer icon does not focus field for native filters,assigned:polidea; enhancement:committed; viz:dashboard:native-filter,2021-01-05 15:55:25 +0000 UTC,adam-stasiak,Opened,,"You can see that for most filters in application when you tap on icon then you are moved to edit field. For native filters no reaction.

### Expected results

I think We should be moved to edition of native filter. Also for child filters.
### Actual results
For native filters no reaction after click on magnifer.


#### Screenshots


https://user-images.githubusercontent.com/25153919/103642354-47c67380-4f53-11eb-9adc-29c85f423345.mov



#### How to reproduce the bug

1. Go to Dashboard
2. Add native filter
3. Click any component filter section
4. In filters list click on magnifer icon 

### Environment

(please complete the following information):

I launch app via docker-compose. Commit 843df379643bc046e0480a3b7eca76c0428fc977.
- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ x] I have reproduced the issue with at least the latest released version of superset.
- [x ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context


",,,junlincc,"
--
thanks for throughout testing and reporting @adam-stasiak ! we should treat it as a committed enhancement request, i did not include this in the 1.0 scope. 

@agatapst 
--
",,,,,,,,,,
12271,OPEN,D3 datetime format can not change the correct 'Hours offset',need:more-info,2021-01-05 15:46:38 +0000 UTC,qinzl1,Opened,,"
",,,,,,,,,,,,,,
12270,OPEN,Filtered chart in email,enhancement:request; global:report; question,2021-01-07 20:55:05 +0000 UTC,singh-ab,Opened,,"**Is your feature request related to a problem? Please describe.**
While superset offers the functionality of scheduling a chart as an email. We cannot send an email with a filter , to achieve this we have to save the same chart with multiple names , have the filter attached and then schedule them


**Describe the solution you'd like**
It would be great to have the functionality extended with filters on chart .
",,,zhaoyongjie,"
--
@singh-ab I am curious about the chart can not with filter, the filter is embedded in the charts, why is it not sending by email
--

--
@singh-ab 
I got what you mean. Now Superset Chart Scheduler should not be flexible enough to complete it. But you can try to write some Jinja2 macro to finish some parts of the function.(Adhoc Filter by SQL)

1. https://superset.apache.org/docs/installation/sql-templating
2. Adhoc filter by SQL
<img width=""700"" alt=""image"" src=""https://user-images.githubusercontent.com/2016594/103670731-eb943b80-4fb4-11eb-927f-08970f62b7a7.png"">

BTW, This is a good suggestion. It shouldn't be too hard to add this feature.

--
",junlincc,"
--
cc @benceorlai 
--
",singh,"
--
@zhaoyongjie with filter I mean dynamic filter . 

Example : let's say I want to schedule an email report and pass a filter . 

Is that possible ? 
--

--
@zhaoyongjie : thanks I will give this a try !
that said if anyone has examples around this feel free to share it here   
--
",,,,,,
12256,OPEN,Need to show timestamp with time zone field in a different time zone in the report,question,2021-01-11 15:00:14 +0000 UTC,gogitub,In progress,,"Hi,
We have many timestamp with time zone fields in Oracle database which is the source for our Superset (0.36 version) reports.
We need to show the data in EST or PST or some other time zones based on customer need.

Ex:
Data in Oracle database table is like '2021-01-04 21:48:56.66 +05:30'
If we want to display in EST, report should show it like : 2021-01-04T11:18:56.660000
If we want to display in PST, report should show it like : 2021-01-04T08:18:56.660000

So, is there any configuration parameter that we can modify to display the data in desired time zone (like EST/PST or some other)?

Please advise. Thanks in advance.",,,zhaoyongjie,"
--
@gogitub. Is it possible to add derived dimensions from the original date field ?
--
",gogitub,"
--
@zhaoyongjie, Thank you for your interest in answering my question.
There are a lot of such fields and many time zones are available in the data. we are not preferring any derived columns because we already have lot many reports/dashboards built with the existing columns.
Also, we need to show the data displayed in time zone per user profile or preference if there is a way.
--

--
It is observed that Superset is automatically converting the timestamp fields to EST time while displaying in the SQL Lab or Chart.
Anyone knows where exactly we can change this setting to show it in different time zone or customize it to work using a config parameter?
--

--
Not this datasource specific setting. 
Currently, any TIMESTAMP with TIME ZONE field from source DB (Oracle) is being displayed in EST time zone in our application (I think it is converting to the system time zone where it is installed)
--
",eugeniamz,"
--
Have you tried the HOURS OFFSET from Dataset / Setting?
![image](https://user-images.githubusercontent.com/58375897/104192915-7af28200-53ed-11eb-8bde-954bd045b7ab.png)

I am not sure if this is your solution, would like to know your feedback. 
--
",,,,,,
12244,OPEN,issue in windows installation,question,2021-01-27 09:26:46 +0000 UTC,kalimuthu123,In progress,,"
Traceback (most recent call last):
  File ""D:\bitool\venv\lib\site-packages\superset\views\core.py"", line 2603, in _sql_json_sync
    with utils.timeout(seconds=timeout, error_message=timeout_msg):
  File ""D:\bitool\venv\lib\site-packages\superset\utils\core.py"", line 593, in __enter__
    signal.signal(signal.SIGALRM, self.handle_timeout)
AttributeError: module 'signal' has no attribute 'SIGALRM'

the issue came around the tool after windows installation 

",,,kalimuthu123,"
--
https://github.com/apache/incubator-superset/issues/9992
--
",dpgaspar,"
--
@kalimuthu123,

This is probably related to https://stackoverflow.com/questions/52779920/why-is-signal-sigalrm-not-working-in-python-on-windows. Note that windows is not a supported platform for superset

--
",villebro,"
--
In theory it would be easy to bypass this by adding a `hasattr(signal, ""SIGALRM"")` to the if statement, but I'm not sure running Superset without timeouts is a good idea, either. I'd be curious to hear what others have done to get Superset running on Windows.
--
",Xiaojun,"
--
modify  core.py  code

def __exit__(  # pylint: disable=redefined-outer-name,unused-variable,redefined-builtin
        self, type: Any, value: Any, traceback: TracebackType
    ) -> None:
        try:
            pass
        except ValueError as ex:
            logger.warning(""timeout can't be used in the current context"")
            logger.exception(ex)
--
",,,,
12237,OPEN,Support for blazingsql,data:connect:suggest,2021-02-03 00:27:43 +0000 UTC,mctouch,Opened,,"**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

support for https://blazingsql.com/ an in GPU SQL database

https://github.com/BlazingDB


**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
",,,junlincc,"
--
cc @srinify ~
--
",mistercrunch,"
--
For Superset to work with any SQL-speaking database, it requires for that database to have a DBAPI-compatible driver as well as a SQLAlchemy dialect.

Those are well documented Python standards that aren't too difficult to implement. The dialect is generally super easy, and the DBAPI driver is kind of a must for the database to work with the Python ecosystem.
--
",srinify,"
--
@mctouch hey there, I'm happy to collaborate on this and help get this working. Can you investigate if blazingsql has a dbapi-compatible driver + a SQLAlchemy dialect?
--

--
hey @mctouch just thought I'd follow up!
--
",,,,,,
12231,OPEN,can't insert with clickhouse datasource,data:connect:clickhouse; question,2021-01-04 02:58:27 +0000 UTC,wxf163,Opened,,"clickhouse 20.3.18.10
superset docker 0.28.1

table::  create table t(id Int32, aa String)
sql insert into testdb.t values(3,'cccc')



log :
 
2020-12-31 02:30:32,478:DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): service-ck-test123008.ck-test123008-hd.test-test.testdomain.com
2020-12-31 02:30:32,491:DEBUG:urllib3.connectionpool:http://service-ck-test123008.ck-test123008-hd.test-test.testdomain.com:8123 ""POST /?database=default&user=testuser&password=123456 HTTP/1.1"" 200 None
2020-12-31 02:30:32,492:INFO:root:Running query: 
insert into testdb.t values(3,'cccc')
2020-12-31 02:30:32,492:INFO:root:insert into testdb.t values(3,'cccc')
2020-12-31 02:30:32,494:DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): service-ck-test123008.ck-test123008-hd.test-test.testdomain.com
2020-12-31 02:30:32,507:DEBUG:urllib3.connectionpool:http://service-ck-test123008.ck-test123008-hd.test-test.testdomain.com:8123 ""POST /?query_id=27d78240-4b10-11eb-b191-8a1bd80bcf85&database=default&user=testuser&password=123456  HTTP/1.1"" 500 None
2020-12-31 02:30:32,507:ERROR:root:Code: 27, e.displayText() = DB::Exception: Cannot parse input: expected ( before: FORMAT TabSeparatedWithNamesAndTypes:  at row 1 (version 20.3.18.10 (official build))
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/superset/sql_lab.py"", line 182, in execute_sql
    db_engine_spec.execute(cursor, query.executed_sql, async_=True)
  File ""/usr/local/lib/python3.6/site-packages/superset/db_engine_specs.py"", line 385, in execute
    cursor.execute(query)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy_clickhouse/connector.py"", line 210, in execute
    self._process_response(response)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy_clickhouse/connector.py"", line 341, in _process_response
    for r in response:
  File ""/usr/local/lib/python3.6/site-packages/infi/clickhouse_orm/database.py"", line 218, in select
    r = self._send(query, settings, True)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy_clickhouse/connector.py"", line 102, in _send
    raise Exception(r.text)
Exception: Code: 27, e.displayText() = DB::Exception: Cannot parse input: expected ( before: FORMAT TabSeparatedWithNamesAndTypes:  at row 1 (version 20.3.18.10 (official build))
",,,zhaoyongjie,"
--
@wxf163 
Hi there, For the Clickhouse table, you need to specify the engine type, (Mergetree etc.  [Clickhouse tutorial)](https://clickhouse.tech/docs/en/getting-started/tutorial/)

<img width=""1021"" alt=""image"" src=""https://user-images.githubusercontent.com/2016594/103496193-4aa06600-4e78-11eb-927f-8586991c4b38.png"">

I use [`clickhouse-sqlalchemy`](https://github.com/xzkostyan/clickhouse-sqlalchemy) instead of `sqlalchemy-clickhouse`, and use `native` driver connect to Clickhouse instance.

<img width=""1324"" alt=""image"" src=""https://user-images.githubusercontent.com/2016594/103496363-eaf68a80-4e78-11eb-846e-4936735c4b6e.png"">

then allow DML in SQLLab.

<img width=""1331"" alt=""image"" src=""https://user-images.githubusercontent.com/2016594/103496403-0f526700-4e79-11eb-9475-1ae906ddc132.png"">

Hope it can help you.

--
",,,,,,,,,,
12222,OPEN,Update documentation for Custom OAuth2 Configuration for superset version >= 0.37.0,bug; doc:developer; v0.37,2021-01-09 16:50:31 +0000 UTC,HUSSTECH,Opened,,"### Expected results

Prerequisite: deployment is using a [Custom OAuth2 Configuration](https://github.com/apache/incubator-superset/blob/master/docs/src/pages/docs/installation/configuring.mdx)

After upgrading to `>= 0.37.0`, following notes in [UPDATING.md#0.37.0](https://github.com/apache/incubator-superset/blob/master/UPDATING.md#0370) and [updating](https://github.com/dpgaspar/Flask-AppBuilder/blob/v3.0.1/README.rst#change-log) the OAuth configuration in FAB for the breaking change, Superset should restart and function as normal.

### Actual results

Users unable to log in with an error message ""invalid login please try again"". Logs will show the following ERROR output
```
ERROR:flask_appbuilder.security.views:Error returning OAuth user info: 'Response' object has no attribute 'data'
```
### Solution

Due to the update of FAB to 3.0.0, we can see in their [`security/manager.py`](https://github.com/dpgaspar/Flask-AppBuilder/blob/22d546012c877773595c9d3c88c0be766f02fbbe/flask_appbuilder/security/manager.py#L513) module the updated call to get the response data directly from a requests.Response object. From `.data` to `.json()`.

For most people the change is simple enough. In your `CustomSsoSecurityManager` class, update the line from,
```python
me = self.appbuilder.sm.oauth_remotes[provider].get('userinfo').data
```
to
```python
me = self.appbuilder.sm.oauth_remotes[provider].get('userinfo').json()
```
> Note: I have seen implementations where `.get('userDetails')` or `.get('userinfo')` are used. Depends on your setup.

### Additional context

Searching the repo, I think an update to the docs is just required to the following files:
- docs/src/pages/docs/installation/configuring.mdx
- docs/installation.rst
",,,Bhavik,"
--
Hello I want to work on this is
--

--
@HUSSTECH I am new to superset , Plz guide me
--
",,,,,,,,,,
12220,OPEN,jwt access_token is invalid,bug,2020-12-29 09:39:04 +0000 UTC,chen-ABC,Opened,,"A clear and concise description of what the bug is.

### Expected results

jwt aceess_token can be recognized

### Actual results

token is invalid
error:  'AnonymousUserMixin' object has no attribute 'get_user_id'


#### How to reproduce the bug
1. 
```
curl -X POST ""http://localhost:5000/api/v1/security/login"" -H  ""accept: application/json"" -H  ""Content-Type: application/json"" -d ""{\""password\"":\""admin\"",\""provider\"":\""db\"",\""refresh\"":true,\""username\"":\""admin\""}""

// response body
{
  ""access_token"": ""eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpYXQiOjE2MDkyMjc4MjksIm5iZiI6MTYwOTIyNzgyOSwianRpIjoiZjc3N2NjY2YtZDY3YS00ODE0LWI4ZTMtODFiZTQwYmE0ZTcxIiwiZXhwIjoxNjA5MjI4NzI5LCJpZGVudGl0eSI6MywiZnJlc2giOnRydWUsInR5cGUiOiJhY2Nlc3MifQ.ciBsncRTIwymPvaVekJE8aiJwfDB7U8dpW2pDH3YEgA"",
  ""refresh_token"": ""eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpYXQiOjE2MDkyMjc4MjksIm5iZiI6MTYwOTIyNzgyOSwianRpIjoiMDE5MTgwZDktNzE3Ny00NWZmLWFiMzEtNDc1MTA2MWE5NjM2IiwiZXhwIjoxNjExODE5ODI5LCJpZGVudGl0eSI6MywidHlwZSI6InJlZnJlc2gifQ.DwSqLRJJjx3uJ5PNaBlidqidQExaTWsYeSucWAJ92XM""
}
```
2. use token 
```
curl -X GET ""http://localhost:5000/api/v1/query/"" -H  ""accept: application/json"" -H  ""Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpYXQiOjE2MDkyMjc4MjksIm5iZiI6MTYwOTIyNzgyOSwianRpIjoiZjc3N2NjY2YtZDY3YS00ODE0LWI4ZTMtODFiZTQwYmE0ZTcxIiwiZXhwIjoxNjA5MjI4NzI5LCJpZGVudGl0eSI6MywiZnJlc2giOnRydWUsInR5cGUiOiJhY2Nlc3MifQ.ciBsncRTIwymPvaVekJE8aiJwfDB7U8dpW2pDH3YEgA""
```
result:
![image](https://user-images.githubusercontent.com/30097790/103269106-0a606400-49f0-11eb-9f0f-afa4452e4810.png)
![image](https://user-images.githubusercontent.com/30097790/103269117-10564500-49f0-11eb-8eea-017a14a72ee3.png)


I find that there is no JWT condition at superset/security/manager.py
```
 def can_access(self, permission_name: str, view_name: str) -> bool:
        """"""
        Return True if the user can access the FAB permission/view, False otherwise.

        Note this method adds protection from has_access failing from missing
        permission/view entries.

        :param permission_name: The FAB permission name
        :param view_name: The FAB view-menu name
        :returns: Whether the user can access the FAB permission/view
        """"""

        user = g.user
        if user.is_anonymous:
            return self.is_item_public(permission_name, view_name)
        return self._has_view_access(user, permission_name, view_name)
```

but I update the code  
```
  return self.has_access(permission_name,view_name)
```
jwt always none

![image](https://user-images.githubusercontent.com/30097790/103274371-78ab2380-49fc-11eb-8884-bca99e53cd28.png)


### Environment

(please complete the following information):

- superset version: `0.38`
- python version: `3.7.9`
- node.js version: `v12.18.3`
",,,,,,,,,,,,,,
12212,OPEN,[up coming change]Remove tests errors and warnings,assigned:turing; change:component,2021-01-02 01:37:49 +0000 UTC,michael-s-molina,Opened,,"Remove all tests errors and warnings to improve results readability.

@rusackas @junlincc ",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.89. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",,,,,,,,,,
12203,OPEN,Multiple tables join queries to produce a single chart,enhancement:request; viz:data:join,2021-02-25 20:29:32 +0000 UTC,Mhs-Aaron,Opened,,"**Is your feature request related to a problem? Please describe.**
I'm always frustrated when the business department has requirements change, the previously prepared single Table cannot be required (N tables need to be combined). In view of this situation, data can only be processed manually at present, and then feedbacks can be made to the business department.

**Describe the solution you'd like**
Whether we can consider to realize this function in the future: multiple tables can be aggregated to generate a Charts.

**Describe alternatives you've considered**
For Chart production, you can select multiple tables and add a drop-down list for association selection (left/right/full join...)..
After the selection, the filter consolidation is passed to the back end for data query.This is just my preliminary idea.If thank you, welcome to discuss and exchange.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.81. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",Mhs,"
--
@altef  I wonder if you are interested in this topic, or if you can help me with @ related personnel.Thank you
--

--
@nytai Later I went to try the way you said, but at the moment our SQLLab or only the admin can operate, so in view of the scene I said, can't let business people operating on a graph, it would greatly increase the workload of the admin staff, if SQLLab open to business people, they do not have time to learn the relevant operation is grammar, this is to let a person helpless.
--
",nytai,"
--
You should be able to achieve this via a SQL view, or by forming your query in SQL Lab and then clicking explore 
--
",,,,,,
12202,OPEN,superset db upgrade failed when trying to upgrade superset (already on use) from an old build,bug; data:connect:mysql,2021-01-27 00:21:59 +0000 UTC,zhangsikai123,Opened,,"when I try to upgrade my superset, I use superset db upgrade, I ran into the error:  
```
sqlalchemy.exc.IntegrityError: (pymysql.err.IntegrityError) (1062, ""Duplicate entry 'xxxx' for key 'uq_saved_query_uuid'"")
[SQL: ALTER TABLE saved_query ADD CONSTRAINT uq_saved_query_uuid UNIQUE (uuid)]
(Background on this error at: http://sqlalche.me/e/13/gkpj)
```
### Environment
Mysql-5.7.25

### Additional context
I went back to commit d7eb1d47:  on 96e99fb176a0_add_import_mixing_to_saved_query.py, there is a line of code, which is obviously written for acceleration:
```
# Add uuids directly using built-in SQL uuid function
add_uuids_by_dialect = {
    MySQLDialect: """"""UPDATE %s SET uuid = UNHEX(REPLACE(uuid(), ""-"", """"));"""""",
    PGDialect: """"""UPDATE %s SET uuid = uuid_in(md5(random()::text || clock_timestamp()::text)::cstring);"""""",
}

```
I tried this line of code on my table with couple rows, it gives exact same uuid for each row. After I deleted this line of code, everything goes on well.",,,,,,,,,,,,,,
12195,OPEN,Improve dataset icons in explorer view,viz:explore:ui,2021-01-02 01:36:52 +0000 UTC,eugeniamz,Opened,,"In the dataset window we have different icons for virtual dataset and physical tables 
<img width=""795"" alt=""Screen Shot 2020-12-23 at 7 21 56 AM"" src=""https://user-images.githubusercontent.com/58375897/102995980-2fd10780-44f0-11eb-9aaa-890cb3662d0a.png"">

When we create a dataset we have different icon to see if a table is a physical table or a database view 
<img width=""624"" alt=""Screen Shot 2020-12-23 at 7 22 17 AM"" src=""https://user-images.githubusercontent.com/58375897/102996032-4d9e6c80-44f0-11eb-9e8a-d5de0c6a3a3b.png"">

But the explorer window we always shows the same icon 
<img width=""702"" alt=""Screen Shot 2020-12-23 at 7 20 39 AM"" src=""https://user-images.githubusercontent.com/58375897/102996053-5bec8880-44f0-11eb-8cc6-986dd7134881.png"">

Is it possible 
1- To add the view icon in the dataset to distinguish physical tables and physical views 
2- Add the 3 different icons on the explorer view.  If one is not possible at least add a different icon for virtual and physical. 

As a data engineer, knowing the source of the data with a simple visual instead to do a search is very useful. 

",,,betodealmeida,"
--
This is something we should improve because the messaging is not clear (cc: @srinify).

These are different things: inside the database we have two types of **tables**, physical tables and (native) views. In SQL Lab these are represented by the table icon and the eye icon, respectively.

Once you're outside SQL Lab, Superset doesn't really care if the database entity is a physical table or a native view  it treats them the same, because the DB abstract these two entities through a single API: SQL.

Superset has a concept that lives atop the physical tables and native views: the dataset. The dataset can be physical or virtual. A **physical dataset** is one that points to a ""table name"" on the DB, that could be physical or a view. A **virtual dataset** is one that has SQL associated with it, pretty much like a native DB view. The main difference is that the SQL defining a virtual dataset lives in the Superset main database.

So you could have a **physical dataset** in Superset that points to a **native DB view**. Or a **virtual dataset** that is defined by SQL stored in Superset, pointing to a **physical dataset** that points to a DB native view, defined by SQL stored in the DB.

In summary, at the DB level we have 2 things: physical tables and views.
One level above, we have Superset datasets, that can be: physical datasets or virtual datasets.

This is super confusing, and we should figure out a better way of communicating this. Maybe we can use native views whenever possible, so that when a user clicks ""visualize"" in SQL Lab we run CVAS if the DB supports it? And then we provide ways of fetching the view definition stored in the DB so that the user can edit it.
--
",srinify,"
--
Somewhat related, I find myself having to hover over the icon frequently because the icons themselves don't clearly scream ""physical"" or ""virtual"" (in the way that the eye / eyeball icon actually does imply virtual).


--
",,,,,,,,
12170,OPEN,Columns list are not updated on chart editor after updating SQL Query on Edit Dataset,bug; data:dataset,2021-01-25 06:35:23 +0000 UTC,zsellami,Opened,,"
### Expected results

After updating a query on the edit Dataset by adding alias or other columns in the query of a Table Chart or any other type of chart (screen 1) I expected to have the list of these columns on the Query Option (screen 2)  

### Actual results
The list of available columns are not updated after editing the query of the datasource of the chart.

#### Screenshots
screen 1
![Screenshot_20201222_100139](https://user-images.githubusercontent.com/22955406/102870360-394d6780-443d-11eb-8d68-dc61a9da88df.png)

screen 2
![Screenshot_20201222_100236](https://user-images.githubusercontent.com/22955406/102870389-410d0c00-443d-11eb-827a-2f103e458cbf.png)


### Environment

(please complete the following information):

- superset version: `0.38`
- python version: `3.6.9`
- database : postgres
- web browser : firefox or chrome

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,eugeniamz,"
--
You are right, after SAVE the columns should be automatically updated. 
The workaround to resolve the issues is to save the new virtual dataset and then sync the columns. 
I explain it in a [video](https://youtu.be/8rKk727UM7s) here
--
",kamalkeshavani,"
--
I faced the same issue today in Superset 1.0.0. Although I can see from Euginea's video, that it is expected behavior that user needs to 'Sync' columns again after saving the changes to dataset, but I would still say the flow can be improved.

I am not sure how heavy is the task of columns sync, but I guess one way could be to always sync columns when a change in dataset(change in SQL or change to/from physical-virtual) is saved.
--
",,,,,,,,
12160,OPEN,Passing extra data to VizPlugin,viz:dynamic-plugins,2021-01-02 01:36:15 +0000 UTC,lucaxsilveira,Opened,,"I have a custom viz plugin (https://superset.apache.org/docs/installation/building-custom-viz-plugins/).
Can i pass **extra data** to **formData** from python to this new plugin? 

",,,,,,,,,,,,,,
12155,OPEN,Getting error while trying to connect mongo via drill,bug; data:connect:drill,2021-01-02 01:40:30 +0000 UTC,iercan,Opened,,"Hi. I configured drill so that I can read mongodb via superset. I'm getting error when sqllab try to get tables.

### Expected results

Send  sql requests successfully to mongo by using sql_lab

### Actual results

Superset throws below error. 

```
superset_1  | DEBUG:root:Mapping column TABLE_NAME of Drill type VARCHAR to dtype string
superset_1  | module 'sqlalchemy_drill.drilldbapi' has no attribute 'Error'
superset_1  | Traceback (most recent call last):
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1276, in _execute_context
superset_1  |     self.dialect.do_execute(
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute
superset_1  |     cursor.execute(statement, parameters)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy_drill/drilldbapi/_drilldbapi.py"", line 65, in func_wrapper
superset_1  |     return func(self, *args, **kwargs)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy_drill/drilldbapi/_drilldbapi.py"", line 164, in execute
superset_1  |     elif str(df[col_name].iloc[0]).startswith(""["") and str(df[col_name].iloc[0]).endswith(""]""):
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/pandas/core/indexing.py"", line 879, in __getitem__
superset_1  |     return self._getitem_axis(maybe_callable, axis=axis)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/pandas/core/indexing.py"", line 1496, in _getitem_axis
superset_1  |     self._validate_integer(key, axis)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/pandas/core/indexing.py"", line 1437, in _validate_integer
superset_1  |     raise IndexError(""single positional indexer is out-of-bounds"")
superset_1  | IndexError: single positional indexer is out-of-bounds
superset_1  | 
superset_1  | During handling of the above exception, another exception occurred:
superset_1  | 
superset_1  | Traceback (most recent call last):
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/superset/models/core.py"", line 537, in get_all_view_names_in_schema
superset_1  |     views = self.db_engine_spec.get_view_names(
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/superset/db_engine_specs/base.py"", line 651, in get_view_names
superset_1  |     views = inspector.get_view_names(schema)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/reflection.py"", line 325, in get_view_names
superset_1  |     return self.dialect.get_view_names(
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy_drill/base.py"", line 318, in get_view_names
superset_1  |     curs = connection.execute(""SELECT `TABLE_NAME` FROM INFORMATION_SCHEMA.views WHERE table_schema='"" + schema + ""'"")
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 2235, in execute
superset_1  |     return connection.execute(statement, *multiparams, **params)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1003, in execute
superset_1  |     return self._execute_text(object_, multiparams, params)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1172, in _execute_text
superset_1  |     ret = self._execute_context(
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1316, in _execute_context
superset_1  |     self._handle_dbapi_exception(
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1390, in _handle_dbapi_exception
superset_1  |     isinstance(e, self.dialect.dbapi.Error)
superset_1  | AttributeError: module 'sqlalchemy_drill.drilldbapi' has no attribute 'Error'
superset_1  | ERROR:superset.models.core:module 'sqlalchemy_drill.drilldbapi' has no attribute 'Error'
superset_1  | Traceback (most recent call last):
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1276, in _execute_context
superset_1  |     self.dialect.do_execute(
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute
superset_1  |     cursor.execute(statement, parameters)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy_drill/drilldbapi/_drilldbapi.py"", line 65, in func_wrapper
superset_1  |     return func(self, *args, **kwargs)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy_drill/drilldbapi/_drilldbapi.py"", line 164, in execute
superset_1  |     elif str(df[col_name].iloc[0]).startswith(""["") and str(df[col_name].iloc[0]).endswith(""]""):
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/pandas/core/indexing.py"", line 879, in __getitem__
superset_1  |     return self._getitem_axis(maybe_callable, axis=axis)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/pandas/core/indexing.py"", line 1496, in _getitem_axis
superset_1  |     self._validate_integer(key, axis)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/pandas/core/indexing.py"", line 1437, in _validate_integer
superset_1  |     raise IndexError(""single positional indexer is out-of-bounds"")
superset_1  | IndexError: single positional indexer is out-of-bounds
superset_1  | 
superset_1  | During handling of the above exception, another exception occurred:
superset_1  | 
superset_1  | Traceback (most recent call last):
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/superset/models/core.py"", line 537, in get_all_view_names_in_schema
superset_1  |     views = self.db_engine_spec.get_view_names(
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/superset/db_engine_specs/base.py"", line 651, in get_view_names
superset_1  |     views = inspector.get_view_names(schema)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/reflection.py"", line 325, in get_view_names
superset_1  |     return self.dialect.get_view_names(
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy_drill/base.py"", line 318, in get_view_names
superset_1  |     curs = connection.execute(""SELECT `TABLE_NAME` FROM INFORMATION_SCHEMA.views WHERE table_schema='"" + schema + ""'"")
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 2235, in execute
superset_1  |     return connection.execute(statement, *multiparams, **params)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1003, in execute
superset_1  |     return self._execute_text(object_, multiparams, params)
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1172, in _execute_text
superset_1  |     ret = self._execute_context(
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1316, in _execute_context
superset_1  |     self._handle_dbapi_exception(
superset_1  |   File ""/usr/local/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1390, in _handle_dbapi_exception
superset_1  |     isinstance(e, self.dialect.dbapi.Error)
superset_1  | AttributeError: module 'sqlalchemy_drill.drilldbapi' has no attribute 'Error'
```

#### How to reproduce the bug

1. Configure drill so that it can request mongodb
2. Add drill datasource to superset
3. Try to send request from sqllab

### Environment

- superset version: 0.38
- python version: 3.8.6
- drill version: 1.18
- sqlalchemy-drill version: 0.2.2.dev0

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,,,,,,,,,,,,
12152,OPEN,supetset mail report bug,bug; need:more-info,2021-01-02 01:41:02 +0000 UTC,huangxinjun,Opened,,"celery worker --app=superset.tasks.celery_app:app --pool=prefork -O fair -c 4


Message: unknown error: net::ERR_CONNECTION_REFUSED
  (Session info: headless chrome=87.0.4280.88)
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/superset/utils/celery.py"", line 50, in session_scope
    yield session
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/superset/tasks/schedules.py"", line 517, in schedule_email_report
    schedule.deliver_as_group,
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/superset/tasks/schedules.py"", line 261, in deliver_dashboard
    driver = create_webdriver(session)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/superset/tasks/schedules.py"", line 205, in create_webdriver
    get_reports_user(session)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/superset/utils/webdriver.py"", line 80, in auth
    driver, user
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/superset/utils/machine_auth.py"", line 53, in authenticate_webdriver
    driver.get(headless_url(""/login/""))
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 333, in get
    self.execute(Command.GET, {'url': url})
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py"", line 321, in execute
    self.error_handler.check_response(response)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py"", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: net::ERR_CONNECTION_REFUSED
  (Session info: headless chrome=87.0.4280.88)",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",,,,,,,,,,
12150,OPEN,CSRF Token Issue when embedding Superset in iFrame,bug,2021-03-19 06:34:50 +0000 UTC,skini-galaxia,Opened,,"On embedding Superset in an HTML iframe, a 'Bad Request' is displayed.

The fixes mentioned in https://github.com/apache/incubator-superset/issues/8382 do not seem to fix this completely.

On completing the following steps, the issue was resolved:
- Create a new hostname using sudo nano /etc/hosts (eg foo.bar.com) with the IP address of the machine (use ifconfig).
- Run the command python -m http.server 8000 in the directory containing the file with the embedded iframe.
- Change `SESSION_COOKIE_SAMESITE = None ` in superset/config.py.

Requesting the following steps to be added in the documentation for some clarity or a fix/workaround for this issue.",,,Riskatri,"
--
hi, i have same issue. can you explain solution for this issue?
--
",,,,,,,,,,
12149,OPEN,Add ErrorBoundary to page components,assigned:flexiana,2021-01-06 04:43:37 +0000 UTC,ktmud,Opened,,"Sometimes pages go completely blank when a JS error happens (#11908, #11759 , #11568 ). 

We should wrap major subcomponents with `ErrorBoundary` so users can still navigate off the error state.

@nytai has [raised the issue](https://github.com/apache/incubator-superset/issues/11759#issuecomment-730714595) before. Creating a ticket to keep track of progress.

A recommended list of boundaries to add:

1. Explore
    - [ ] Datasource Column
    - [ ] Chart Control Column
    - [ ] Chart Header
    - [x] Chart Render
    - [ ] Chart Data Preview
    - [ ] DataSource Editor Modal
2. SQL Lab
    - [ ] Datasource Sidebar
    - [ ] SQL Editor
    - [ ] Query Results Preview
3. Dashboard
    - [x] Filters
    - [ ] Main dashboard body
3. List View Pages
    - [ ] Filters
    - [ ] List Table View
    - [ ] Thumbnails View
4. Home Page
    - [ ] Recents
    - [x] Charts
    - [ ] Dashboards
    - [ ] Saved Queries",,,junlincc,"
--
related? https://github.com/apache/incubator-superset/issues/12109 
--
",,,,,,,,,,
12141,OPEN,[explore]customize previous calendar week time range,enhancement:committed; viz:explore:timepick,2021-01-02 01:48:58 +0000 UTC,junlincc,In progress,,"<img width=""1180"" alt=""Screen Shot 2020-12-18 at 8 41 56 PM"" src=""https://user-images.githubusercontent.com/67837651/102680903-bfce2480-4171-11eb-8b1a-641cffd3dd0f.png"">
<img width=""543"" alt=""Screen Shot 2020-12-18 at 8 43 45 PM"" src=""https://user-images.githubusercontent.com/67837651/102680907-c2c91500-4171-11eb-946d-1f612ca45532.png"">

also minor issue, need to set calendar week range from `Sunday to Saturday` instead of `Monday to Sunday `

latest master",,,nytai,"
--
I this this varies by locale, some start the week on Sunday and others on Monday. From [wikipedia](https://en.wikipedia.org/wiki/Week)

>While, for example, the United States, Canada, Brazil, Japan and other countries consider Sunday as the first day of the week, and while the week begins with Saturday in much of the Middle East, the international ISO 8601 standard and most of Europe has Monday as the first day of the week.

It's likely that whatever library is generating this range is following the ISO 8601 standard.
--

--
We could also try to detect the locale, but empowering the user is probably best for a BI tool. 
--
",junlincc,"
--
enhancement request: allow user to customize ""calendar week"" 
--
",zhaoyongjie,"
--
This is a problem that can be solved by introducing a config item. 

```
# Which is the first day of the week, the default set to 1 or 0
FIRST_DAY_OF_WEEK = [0-6]
```
--

--
@nytai Since the server is often set to UTC time zone, it may not be easy to detect
--
",,,,,,
12133,OPEN,[SQL Lab] Double click to edit tab title,enhancement:request; good first issue,2021-02-17 22:17:18 +0000 UTC,ktmud,Opened,,"**Is your feature request related to a problem? Please describe.**

Currently you have to click three times in order to edit the title of a SQL Lab tab:

<img src=""https://user-images.githubusercontent.com/335541/102665507-c80a6d80-4139-11eb-9779-1a5e25349a17.png"" 
 width=""400"" />

Considering the tab title is almost the most important thing you want to change on a tab, it would be nice if we can make editing it easier. 

**Describe the solution you'd like**

- Click on a focused tab enters edit mode
   <img src=""https://user-images.githubusercontent.com/335541/102665802-57b01c00-413a-11eb-8bdd-848eb6e305d0.png"" width=""400"">
- Blurring from the input saves the title

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
",,,yousoph,"
--
Thanks for the suggestion!! Will look into an improvement as part of some upcoming UI changes that we're working on :) 
--
",,,,,,,,,,
12109,OPEN,[explore]click Altered crashing the page,assigned:flexiana; bug,2021-01-02 02:19:20 +0000 UTC,junlincc,Opened,,"![crash](https://user-images.githubusercontent.com/67837651/102578936-386cac80-40b0-11eb-9c85-9c2a3e6e9c68.gif)

Issue 1 Altered button crashes the page
Issue 2 Altered is missing on the new sample charts

<img width=""2041"" alt=""Screen Shot 2020-12-17 at 9 39 48 PM"" src=""https://user-images.githubusercontent.com/67837651/102579000-67831e00-40b0-11eb-9a00-d01a470f46e7.png"">
",,,,,,,,,,,,,,
12094,OPEN,output clipped; log limit 1MiB reached error,bug,2021-03-03 11:01:51 +0000 UTC,dongshanlang,Opened,,"when I run :
`docker build  --target superset-node -t supserset-node:1.0 .`
the result would be:
[webpack.Progress] 69% building 3513/3547 modules 34 active /app/superset-frontend/node_modules/rc-picker/es/hooks/useCellClassName.js
[webpack.Progress] 69% building 3514/3547 modules 33 active /app/superset-frontend/node_modules/rc-picker/es/hooks/useCellClassName.js
[webpack.Progress] 69% bu
 [output clipped, log limit 1MiB reached]
------------
What's the problem?",,,BigRantLing,"
--
I have the same issue. Did you fix it ?
--
",,,,,,,,,,
12093,OPEN,Not able to view dashboards after integrating LDAP in superset,,2020-12-22 14:17:34 +0000 UTC,IamAnandGoudar,Opened,,"With Authentication Type as DB, I was able to login with admin/admin. I had all the permissions.
Now I have changed my superset's config.py to use ldap server,
AUTH_TYPE = AUTH_LDAP
AUTH_LDAP_SERVER = ""ldap://***.**.**.**""
AUTH_LDAP_SEARCH = ""DC=example,DC=com""
AUTH_LDAP_BIND_USER = ""ou=users,dc=example,dc=com""
AUTH_LDAP_BIND_PASSWORD = ""**********""
AUTH_LDAP_UID_FIELD = ""sAMAccountName""

After making these changes and installing pythonldap in my environment, I was able to successfully login.
But now, I dont have permission to view/create dashboards/charts or do any other thing 
<img width=""928"" alt=""Superset_LDAP"" src=""https://user-images.githubusercontent.com/59425258/102458203-e99c2400-4069-11eb-9e17-4db35bdb455f.PNG"">

",,,dpgaspar,"
--
Hi,

Can you please fill all the necessary fields for a bug report, would be really helpful also to include server logs. Thank you!
--
",,,,,,,,,,
12083,OPEN,[chart]: include group/series label in Histogram tooltip,enhancement:request; tooltip:chart; viz:chart-histogram,2021-01-02 01:55:59 +0000 UTC,junlincc,Opened,,"currently, group label is not showing in the tooltip when user hovering on a Histogram with bars split by multiple groups/series. 

Proposed enhancement: 
<img width=""1781"" alt=""Screen Shot 2020-12-16 at 1 28 08 PM"" src=""https://user-images.githubusercontent.com/67837651/102410589-9a8cbb00-3fa5-11eb-88c6-cbf4465be145.png"">


",,,,,,,,,,,,,,
12068,OPEN,[chart]:categorical color scales and color schemes is broken on treemap,enhancement:committed; viz:chart-treemap,2021-01-02 01:53:53 +0000 UTC,junlincc,Opened,,"### Expected results

With a primary Group By, each region should have a different color label. e.g. South Asia - blue, Europe & Central Asia - yellow, East Asia - green
![image (2)](https://user-images.githubusercontent.com/67837651/102283237-d9a80700-3ee6-11eb-949d-c33cebf9a8e2.png)

### Actual results

everything is green...

<img width=""1149"" alt=""Screen Shot 2020-12-15 at 2 42 46 PM"" src=""https://user-images.githubusercontent.com/67837651/102282712-dfe9b380-3ee5-11eb-854c-a94f990affc0.png"">

This is not a recent regression.

cc: @kristw (we are planning to work on it, do you mind transferring some knowledge if needed? thanks! https://github.com/apache/incubator-superset/pull/5815)",,,mistercrunch,"
--
 visualization
--
",kristw,"
--
The legacy treemap did not include a way to specify what to encode color by:
* Currently it is using the depth of the nodes in the tree.
* Prior alternative was to color node by its name, which was also not useful as each node pretty much have unique name and there will never be enough colors so the colors are reused. Then nodes with same color have nothing in common.

It would be best to modify this chart, so users can choose another field to encode color by.
--
",,,,,,,,
12066,OPEN,Unable to use Group by with multiple entries with TIME-SERIES TABLE,,2020-12-15 22:50:41 +0000 UTC,guoxin-li,Opened,,"**Issue**
This issue has been seen in superset 0.26, 0.35, and the latest version of 0.36. When queries with more than one entires of `Group by` for Time-Series Table, the following error is showing up :
 `Unexpected Error
 keys must be str, int, float, bool or None, not tuple`
 [](url)
 **How to reproduce this Error:**
 Use any available data source to create a TIME-SERIES TABLE, add more than one Group by entries and run query
  
  **Trace log**
  `Traceback (most recent call last):
File ""/opt/superset/venv/lib/python3.6/site-packages/superset/views/base.py"", line 120, in wraps
return f(self, *args, **kwargs)
File ""/opt/superset/venv/lib/python3.6/site-packages/superset/utils/decorators.py"", line 69, in wrapper
return f(*args, **kwargs)
File ""/opt/superset/venv/lib/python3.6/site-packages/superset/views/core.py"", line 1092, in explore_json
viz_obj, csv=csv, query=query, results=results, samples=samples
File ""/opt/superset/venv/lib/python3.6/site-packages/superset/views/core.py"", line 1014, in generate_json
return data_payload_response(*viz_obj.payload_json_and_has_error(payload))
File ""/opt/superset/venv/lib/python3.6/site-packages/superset/viz.py"", line 478, in payload_json_and_has_error
return self.json_dumps(payload), has_error
File ""/opt/superset/venv/lib/python3.6/site-packages/superset/viz.py"", line 470, in json_dumps
obj, default=utils.json_int_dttm_ser, ignore_nan=True, sort_keys=sort_keys
File ""/opt/superset/venv/lib64/python3.6/site-packages/simplejson/_init_.py"", line 412, in dumps
**kw).encode(obj)
File ""/opt/superset/venv/lib64/python3.6/site-packages/simplejson/encoder.py"", line 296, in encode
chunks = self.iterencode(o, _one_shot=True)
File ""/opt/superset/venv/lib64/python3.6/site-packages/simplejson/encoder.py"", line 378, in iterencode
return _iterencode(o, 0)
TypeError: keys must be str, int, float, bool or None, not tuple
 `",,,,,,,,,,,,,,
12058,OPEN,Getting 502 client not found error,bug,2020-12-15 13:51:41 +0000 UTC,thammanenitcs,Opened,,"A clear and concise description of what the bug is.
Hi Team,

I am using superset 0.36 version, have ran a query on sqlab having 13M rows and 25 columns. i am trying to download data using ""CSV"" download option. The response is giving back to ""502 client not found"" after sometime. 

When i checked pod logs, it shows ""worker is getting rebooted"" at some point of time.

### Expected results
CSV file with data of 13M rows.

what you expected to happen.
CSV file has to download.

### Environment
Kubernetes:
pod resource limits :-
resources:
  limits:
	cpu: '1'
	memory: 3000Mi
  requests:
	cpu: 100m
	memory: 400Mi

(please complete the following information):

- superset version: `superset version` 0.36.0
- python version: `python --version` 3.6
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ Y] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [Y ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,,,,,,,,,,,,
12052,OPEN,`An error occurred` when trying to visualize a group of 4-million-row data,bug,2020-12-18 09:56:34 +0000 UTC,lanyusea,In progress,,"I got a group of data about 4million stored in mysql, I'm trying to load it in superset and do the virtualization.

It works well when there is only thousands data but after I put all data in the mysql, the superset failed to do the job with error `An error occurred` in the chart page but no further information, nor in the log.

### Expected results
Superset can fetch the data though it reports problems

### Actual results
1. It says `An error occurred` with no result.
2. a large temp db is created in my mysql

#### Screenshots
![image](https://user-images.githubusercontent.com/2766729/102171538-b888eb80-3ed1-11eb-9a9b-02779761f7fe.png)

and the log file has nothing useful:
![image](https://user-images.githubusercontent.com/2766729/102172491-d6efe680-3ed3-11eb-8fa9-af3929020a81.png)

#### How to reproduce the bug

1. Add 4million rows of data
2. do the virtualization

### Environment

(please complete the following information):

- superset version: `0.37.2`
- python version: `3.6.9`
- node.js version: `didn't install`

superset is installed from Scratch, not docker


### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

I have searched and find few discussion about the data scale. the official document only mentions issue about the [loading speed](https://superset.apache.org/docs/frequently-asked-questions#how-big-can-my-datasource-be) while in #4588 people also talks about the speed. Seems nobody meets a fail issue as me.

also, to make it runable, I have extended the timeout to 3000s and several other limit settings.
![image](https://user-images.githubusercontent.com/2766729/102172766-6d240c80-3ed4-11eb-956b-c57c12ea2959.png)

The problem always happen after querying about 600s, so I'm thinking if there is still some timeout in the connection but I didn't find any other in the superset config.py.

So I'm wondering how could I know what the exact error is, and how to I solve it.

or is there any suggestion I can bypass this issue.

Thanks!",,,lanyusea,"
--
I just found the superset will resend the query request if the result doesn't go back in around 120s. So if the mysql failed to finish the query in 120s, it will run into a terrible condition until all resources used up.

anyone know how could I modify this retry behavior?
didn't find any related settings in the config file.
--
",,,,,,,,,,
12037,OPEN,Custom datetype for Dataset is not passed to group by operation,bug,2020-12-14 13:10:19 +0000 UTC,0xBADBAC0N,Opened,,"A clear and concise description of what the bug is.

### Expected results

If I use a custom temporal column (eG string type but contains %Y%m%d) the type is correctly set in the conditions (where clause) but should also be set correctly in all other locations (eG group by)
### Actual results

what actually happens:
I have set the correct settings for the dataset:
![image](https://user-images.githubusercontent.com/24916828/102083756-59b16c80-3e14-11eb-9e0f-f7daf17f6d9b.png)

and the changes where applied correct to the where clause, but not the group by clause.
![image](https://user-images.githubusercontent.com/24916828/102083788-6635c500-3e14-11eb-80d8-9c7100ff19b7.png)

This generated query would also not really work as in Hive select UNIX_TIMESTAMP('2012-05-10'); (=> NULL)
would break as the custom schema is not pushed down. With pushing down a proper schema, it would work like select UNIX_TIMESTAMP('2012-05-10', 'yyyy-MM-dd'); (=> 1336608000)

#### How to reproduce the bug

1. Create a temporal column as string and add the formatting manually.
2. Try to group by that column in Hive.
3. Click on View Query and see the raw query.
4. See error

### Environment

(please complete the following information):

- superset version: 0.38
- python version: python 3
- node.js version: n/a

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,,,,,,,,,,,,
12031,OPEN,[chart]:area chart tooltip missing highlight,tooltip:chart; viz:chart-area,2021-01-02 01:55:45 +0000 UTC,junlincc,Opened,,"area chart has the same issue as [#12030 ](https://github.com/apache/incubator-superset/issues).But there's no highlight on the selected group/area even when the tooltip can fit the view.

<img width=""2019"" alt=""Screen Shot 2020-12-13 at 8 58 56 PM"" src=""https://user-images.githubusercontent.com/67837651/102043253-c3843480-3d88-11eb-9e80-963a8017a53c.png"">

<img width=""2021"" alt=""Screen Shot 2020-12-13 at 9 22 43 PM"" src=""https://user-images.githubusercontent.com/67837651/102043473-5b821e00-3d89-11eb-92f1-1c5093d57e97.png"">
",,,,,,,,,,,,,,
12030,OPEN,[chart]: linechart unable to scroll tooltip to see highlight when SERIES LIMIT is large,tooltip:chart; viz:chart-line,2021-01-02 01:56:25 +0000 UTC,junlincc,Opened,,"
Expected behavior: 
when user hover on a specific line/group in line chart, user can see highlight of the selected group in tooltip. 
<img width=""1321"" alt=""Screen Shot 2020-12-13 at 9 05 44 PM"" src=""https://user-images.githubusercontent.com/67837651/102042605-1c52cd80-3d87-11eb-9d33-ca7747414c2c.png"">

Issue: 
when Series Limit is large, tooltip overflows and users have no way to scroll and view all

Actual result: 
user can not see the highlight of selected group/line when it is not in the view. 
<img width=""1347"" alt=""Screen Shot 2020-12-13 at 9 10 59 PM"" src=""https://user-images.githubusercontent.com/67837651/102042841-badf2e80-3d87-11eb-823e-4d31636c0676.png"">
",,,,,,,,,,,,,,
12023,OPEN,[global]menu item casing standard in Superset,P2; bug:cosmetic,2021-01-16 05:35:38 +0000 UTC,junlincc,In progress,,"I scanned through most of the menu items in Superset, and found the casing is a mess across the product...
Is there a standard for capitalizing (or not capitalizing) each word in the menu item? if not, we should have one

<img width=""155"" alt=""Screen Shot 2020-12-11 at 7 17 03 PM"" src=""https://user-images.githubusercontent.com/67837651/101971442-35704880-3be6-11eb-88c4-1bd1c70c1098.png"">
<img width=""165"" alt=""Screen Shot 2020-12-11 at 7 16 38 PM"" src=""https://user-images.githubusercontent.com/67837651/101971446-386b3900-3be6-11eb-871c-03240f40af1c.png"">
<img width=""163"" alt=""Screen Shot 2020-12-11 at 7 17 08 PM"" src=""https://user-images.githubusercontent.com/67837651/101971451-3ef9b080-3be6-11eb-908b-c2dd292e5fec.png"">
<img width=""245"" alt=""Screen Shot 2020-12-11 at 7 21 55 PM"" src=""https://user-images.githubusercontent.com/67837651/101971455-4325ce00-3be6-11eb-9301-7b2fdc0f721d.png"">
<img width=""241"" alt=""Screen Shot 2020-12-11 at 7 16 47 PM"" src=""https://user-images.githubusercontent.com/67837651/101971604-82a0ea00-3be7-11eb-98d7-efb9b1d1a6a4.png"">

_Originally posted by @junlincc in https://github.com/apache/incubator-superset/issues/11993#issuecomment-743694009_",,,junlincc,"
--
@mihir174 @steejay please come up with a standard for Superset, would like to get this done for 1.0, right now is a total mess. Thanks! 

cc @mistercrunch @rusackas 
--

--
+1 for sentence case 
this UI issue is quite noticeable and global which really should have been addressed a while ago. 
@Steejay @mihir174 the implementation is in queue, do you think we can get design input fairly soon? 
--

--
<img width=""991"" alt=""Screen Shot 2021-01-15 at 8 34 01 PM"" src=""https://user-images.githubusercontent.com/67837651/104798216-0e6ede80-5771-11eb-9d8d-4f0c41d044df.png"">

Hey @michael-s-molina , dashboard native filter is under a feature flag currently, please make sure the casing in this modal is also following the design guideline @Steejay and @mihir174 provided. thank you all! 
--
",ktmud,"
--
Airbnb convention is only capitalize the first letter. Same for GitHub. This is true for all of menu, label, placeholder, and CTA.
--
",rusackas,"
--
I'm also leaning toward sentence case rather than title case. But I won't put up too much of a fight over it, as long as we're consistent one way or another.
--
",mistercrunch,"
--
Personally I'm fine either way as long as we're consistent!
--
",Steejay,"
--
thanks for flagging. type system review and fixes already in the works. 
cc @mihir174
--

--
 needs:design-input
--
",srinify,"
--
Great idea @junlincc !! I also lean towards sentence case, but agree that consistency is king  
--
"
12005,OPEN,Boxplot - wrong logic,enhancement:request; viz:echarts,2020-12-11 18:32:34 +0000 UTC,li-ana,Opened,,"Box plot assumes that you will have only 1 observation per timestamp. In my case, this is not true, which means that the box plot will aggregate all of the observations per timestamp using the function that you select, thus it skews the results. It needs to look at raw data, instead of the aggregation. 

Query produced by the Superset box plot:
**
![image](https://user-images.githubusercontent.com/20177485/101823239-79851f80-3af8-11eb-917b-79121d8004ee.png)
**
Results of this query:
![image](https://user-images.githubusercontent.com/20177485/101823445-c2d56f00-3af8-11eb-8a27-4dafc6a98459.png)

Box plot details:
![image](https://user-images.githubusercontent.com/20177485/101823581-fca67580-3af8-11eb-90ef-8444e4dad961.png)

What it should be:
![image](https://user-images.githubusercontent.com/20177485/101823662-1e076180-3af9-11eb-9883-5e0eb787252d.png)


",,,villebro,"
--
While we don't yet support using the full raw data and then calculating the boxplot on that, we recently migrated Boxplot to ECharts and added some features in this PR: #11199 . In the below example I've created a Boxplot where the categories are continents and the distribution is calculated across countries (I'm using the average of total population, as the dataset contains data for multiple years):

![image](https://user-images.githubusercontent.com/33317356/101879303-cc62e380-3b99-11eb-9793-a82eb52575e8.png)

If you have a row id, you can use that as the the ""Distribute Across"" parameter. The plan is to add support for using the raw row data, but I probably won't have time to work on it any time soon.
--
",,,,,,,,,,
12001,OPEN,[chart]time-series bar chart x-axis range incorrect,assigned:flexiana; bug; viz:chart-timeseries; viz:echarts,2021-01-06 04:08:51 +0000 UTC,junlincc,Opened,,"<img width=""1281"" alt=""Screen Shot 2020-12-10 at 6 50 13 AM"" src=""https://user-images.githubusercontent.com/67837651/101798292-c6003900-3abf-11eb-8b23-057b196b5665.png"">

(sorry please excuse the messy legend we will address it separately )

Context:
In Echarts Times-Series, users can easily switch between different chart types using the same query from the same dataset. all the variation should have the same axis label . 

Expected Results:
date column range on X should be 1955-2008 

Actual Results:
on Bar chart, 1955-2017, while tooltip is still showing 2008...

Compared to other chart types 
<img width=""1166"" alt=""Screen Shot 2020-12-10 at 7 01 45 AM"" src=""https://user-images.githubusercontent.com/67837651/101799514-1926bb80-3ac1-11eb-9e23-5a2808d8e5e4.png"">

<img width=""1179"" alt=""Screen Shot 2020-12-10 at 7 02 07 AM"" src=""https://user-images.githubusercontent.com/67837651/101799587-2e034f00-3ac1-11eb-92e9-433ebe8a3c7e.png"">

To Reproduce:
- top right ""+"" Create a new chart with `birth_name` dataset 
- select time-series chart(last one in the gallery) 
- in METRICS, select `num` in COLUMN, `SUM` in AGGREGATE
- GROUP BY `name`
- Switch to CUSTOMIZE tab on the top of the control panel 
- select `Bar` in SERIES STYLE
- view results to see the last year in the time range and compare the x axis label 

",,,,,,,,,,,,,,
11988,OPEN,"Percentage metrics can result in ""unsupported operant type(s) for /: 'decimal.Decimal' and 'float'.",bug,2021-02-16 19:21:28 +0000 UTC,altef,In progress,,"For some datasets, using percentage metrics results in the error: `unsupported operand type(s) for /: 'decimal.Decimal' and 'float'`.

I'm using Postgres, and I've traced this back a few ways - 

1. It _doesn't_ happen if the column is of type INT (under dataset/columns/Data Type).
2. It _does_ seem to occur if the column is of type BIGINT or NUMERIC.
3. It appears to come from: (https://github.com/apache/incubator-superset/blob/master/superset/viz.py#L780)

Changing that line to `(df[percent_columns].astype(float).div(df[percent_columns].sum()).add_prefix(""%"")),` seems to fix the issue for me, but it may have ramifications I'm unaware of (I'm not very familiar with this portion of the code-base).

*Disclaimer:* This is on yesterday's master branch.  (I'm not sure if we're supposed to report bugs on master or not.)

### Expected results

Ideally it would show a table with data, including percentages.

### Actual results

It shows the error message `unsupported operand type(s) for /: 'decimal.Decimal' and 'float'`.

#### Screenshots

![image](https://user-images.githubusercontent.com/4316295/101687408-49b61900-3a1f-11eb-8d6e-2eeb5eaa2917.png)

![image](https://user-images.githubusercontent.com/4316295/101687978-25a70780-3a20-11eb-913d-7d5ff398712e.png)

#### How to reproduce the bug

1. Create a table and add some data to it:
```sql
CREATE TABLE decimaltest (
	""name"" varchar NULL,
	value int8 NULL
);

INSERT INTO decimaltest (""name"", value) VALUES
    ('one',	478696),
    ('two',	344274),
    ('three',	215815);
```
I specified `int8` since that seems to result in a `BIG INT` classification.
2. Add the dataset to Superset.
3. Create a new chart on the dataset, of type Table.  Set `GROUP BY` to `name`, and `PERCENTAGE METRICS` to `SUM(value)`.
4. Click `RUN`.
5. See error.

### Environment

(please complete the following information):

- superset version: `1.0`
- python version: `3.7.9`
- node.js version: `v13.11.0`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
```
ERROR:superset.views.base:unsupported operand type(s) for /: 'decimal.Decimal' and 'float'
Traceback (most recent call last):
  File ""/home/ubuntu/newersuperset/venv/lib/python3.7/site-packages/pandas/core/ops/array_ops.py"", line 143, in na_arithmetic_op
    result = expressions.evaluate(op, left, right)
  File ""/home/ubuntu/newersuperset/venv/lib/python3.7/site-packages/pandas/core/computation/expressions.py"", line 233, in evaluate
    return _evaluate(op, op_str, a, b)  # type: ignore
  File ""/home/ubuntu/newersuperset/venv/lib/python3.7/site-packages/pandas/core/computation/expressions.py"", line 68, in _evaluate_standard
    return op(a, b)
TypeError: unsupported operand type(s) for /: 'decimal.Decimal' and 'float'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ubuntu/newersuperset/superset/views/base.py"", line 178, in wraps
    return f(self, *args, **kwargs)
  File ""/home/ubuntu/newersuperset/superset/utils/log.py"", line 125, in wrapper
    value = f(*args, **kwargs)
  File ""/home/ubuntu/newersuperset/superset/utils/cache.py"", line 108, in wrapper
    return f(*args, **kwargs)
  File ""/home/ubuntu/newersuperset/superset/views/core.py"", line 537, in explore_json
    return self.generate_json(viz_obj, response_type)
  File ""/home/ubuntu/newersuperset/superset/views/core.py"", line 433, in generate_json
    payload = viz_obj.get_payload()
  File ""/home/ubuntu/newersuperset/superset/viz.py"", line 488, in get_payload
    payload[""data""] = self.get_data(df)
  File ""/home/ubuntu/newersuperset/superset/viz.py"", line 780, in get_data
    (df[percent_columns].div(df[percent_columns].sum()).add_prefix(""%"")),
  File ""/home/ubuntu/newersuperset/venv/lib/python3.7/site-packages/pandas/core/ops/__init__.py"", line 651, in f
    new_data = self._combine_frame(other, na_op, fill_value)
  File ""/home/ubuntu/newersuperset/venv/lib/python3.7/site-packages/pandas/core/frame.py"", line 5870, in _combine_frame
    new_data = ops.dispatch_to_series(self, other, _arith_op)
  File ""/home/ubuntu/newersuperset/venv/lib/python3.7/site-packages/pandas/core/ops/__init__.py"", line 275, in dispatch_to_series
    bm = left._mgr.operate_blockwise(right._mgr, array_op)
  File ""/home/ubuntu/newersuperset/venv/lib/python3.7/site-packages/pandas/core/internals/managers.py"", line 364, in operate_blockwise
    return operate_blockwise(self, other, array_op)
  File ""/home/ubuntu/newersuperset/venv/lib/python3.7/site-packages/pandas/core/internals/ops.py"", line 38, in operate_blockwise
    res_values = array_op(lvals, rvals)
  File ""/home/ubuntu/newersuperset/venv/lib/python3.7/site-packages/pandas/core/ops/array_ops.py"", line 190, in arithmetic_op
    res_values = na_arithmetic_op(lvalues, rvalues, op)
  File ""/home/ubuntu/newersuperset/venv/lib/python3.7/site-packages/pandas/core/ops/array_ops.py"", line 150, in na_arithmetic_op
    result = masked_arith_op(left, right, op)
  File ""/home/ubuntu/newersuperset/venv/lib/python3.7/site-packages/pandas/core/ops/array_ops.py"", line 92, in masked_arith_op
    result[mask] = op(xrav[mask], yrav[mask])
TypeError: unsupported operand type(s) for /: 'decimal.Decimal' and 'float'
```
- [x] I have reproduced the issue with at least the latest released version of superset.
```
I've only tried on master, which... This may answer my earlier question about reporting bugs on master.  I'm going to submit it anyway, just in case, but apologies if that is not the accepted approach.
```
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
```
Though I am not the best at constructing search queries.  If something similar has been reported, apologies.
```

### Additional context

None
",,,altef,"
--
(looks like this bug made it into 1.0)
--
",RageAgainstTheMachine101,"
--
Same problem!
--
",,,,,,,,
11974,OPEN,autocomplete for presto does not always find table name,bug,2020-12-09 13:09:58 +0000 UTC,tooptoop4,Opened,,"I have a presto table called some_schema.my_table

after typing select * from some_schema.my_t

it does not prompt for my_table

### Expected results

it should prompt  for my_table in the dropdown list

### Actual results

no autocomplete prompt found (it disappears as I type more characters, and it was not showing in the list ever)


#### How to reproduce the bug

1. Go to sql lab
2. type select * from <someschema>.<first_few_characters_of_sometablename>

### Environment

- superset version:  36
- python version: 3.7 ",,,,,,,,,,,,,,
11966,OPEN,SQL lab results cut off,bug:cosmetic,2021-01-06 01:16:06 +0000 UTC,tooptoop4,In progress,,"1. returning 100 rows then can't scroll down to see the last row
2. wide columns (ie values of 5000 characters) show ... on the right and can't be clicked in to see the actual value (need to copy to clipboard, paste somewhere outside of superset to see)",,,yousoph,"
--
Hi @tooptoop4, could you clarify what you mean by #1 and include a screenshot if possible? Thanks! 
--
",tooptoop4,"
--
@yousoph https://github.com/apache/incubator-superset/issues/9869 has example for first point
--
",eschutho,"
--
I'm working on this currently, fyi.
--
",,,,,,
11953,OPEN,Missing data (null) in Time Series Table when no filter applied | Not a data issue from our side,assigned:flexiana; bug; viz:chart-time-table,2021-02-04 05:15:47 +0000 UTC,lctdulac,In progress,,"Hi SuperSet community.

We've been encountering an UI issue with SuperSet when plotting Time Series table. Some datapoints are missing from the graphs and showing ''nulls''. However, the weird thing is, when filtering the data in the dashboard to display only the rows where we witness the issue, those data points appear correctly.

### Expected results

We except the time series to be plotted without these missing data points (nulls) as there are no nulls in the data.

### Actual results

Some random nulls appear in the chart though there are data to display.
When filtering the data, the bug disappears. 

#### Screenshots

**Without filtering :**

![Screenshot 2020-12-07 at 10 35 57_censored](https://user-images.githubusercontent.com/46674763/101335015-ea1e0900-3878-11eb-9f6c-1be7eecb0148.jpg)

![Screenshot 2020-12-07 at 10 36 22_censored](https://user-images.githubusercontent.com/46674763/101335033-eee2bd00-3878-11eb-8340-3730cbaf76d0.jpg)

**After filtering to only the rows displayed above:**

![Screenshot 2020-12-07 at 10 37 20_censored](https://user-images.githubusercontent.com/46674763/101335078-fe620600-3878-11eb-9f7c-daa152ae2612.jpg)

#### How to reproduce the bug

1. Create Time Series table.
2. Add the time series table to a dashboard.
3. Filter down on the column used to group by in the time series table.
4. Compare

### Environment

The table used for the time series table is (Rows 166,645 | Cols 42) queried using Presto SQL Engine.
We are aggregating on a VARCHAR column and the data plotted is of type BIGINT.

- superset version: 0.37.2
- python version: 3.6.12

### Additional context

The dashboard is very big (close to the size limit). ",,,junlincc,"
--
thanks for reporting, we are looking into it! 

@eugeniamz do you have this issue? im not able to reproduce this issue with our sample datasets. 
--
",eugeniamz,"
--
This chart has a limit of 5000 rows, check that it did not reach the limit 
![image](https://user-images.githubusercontent.com/58375897/104179360-b8e5ab00-53d9-11eb-9c7d-030d582a5a08.png)

@junlincc  the row limit seems to be fixed, we should allow users to change this limit as in other charts. is there a reason why is limited to 5K? 
--
",lctdulac,"
--
Thanks for the answer @eugeniamz ! This is exactly our issue. I did not acknowledge the limit was so low but this is exactly the reason why some datapoints are missing in the charts as one row is one daily point on a chart. Any idea of a follow up action to increase that limit, or make it editable? 
Thanks all.
--
",,,,,,
11949,OPEN,qlite3.OperationalError: no such column: alerts.sql,install:docker,2020-12-10 00:34:57 +0000 UTC,magic-overflow,Opened,,"After upgrade Superset
```
Sorry, something went wrong
500 - Internal Server Error
Stacktrace
        Traceback (most recent call last):
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1276, in _execute_context
    self.dialect.do_execute(
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: no such column: alerts.sql

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask/app.py"", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask/app.py"", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask/app.py"", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask/_compat.py"", line 39, in reraise
    raise value
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask/app.py"", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask/app.py"", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask_appbuilder/security/decorators.py"", line 109, in wraps
    return f(self, *args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask_appbuilder/views.py"", line 552, in list
    return self.render_template(
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask_appbuilder/baseviews.py"", line 280, in render_template
    return render_template(
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask/templating.py"", line 137, in render_template
    return _render(
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask/templating.py"", line 120, in _render
    rv = template.render(context)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/jinja2/environment.py"", line 1090, in render
    self.environment.handle_exception()
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/jinja2/environment.py"", line 832, in handle_exception
    reraise(*rewrite_traceback_stack(source=source))
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/jinja2/_compat.py"", line 28, in reraise
    raise value.with_traceback(tb)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/superset/templates/appbuilder/general/model/list.html"", line 20, in top-level template code
    {% import 'appbuilder/general/lib.html' as lib %}
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask_appbuilder/templates/appbuilder/base.html"", line 1, in top-level template code
    {% extends base_template %}
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/superset/templates/superset/base.html"", line 20, in top-level template code
    {% from 'superset/partials/asset_bundle.html' import css_bundle, js_bundle with context %}
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/superset/templates/appbuilder/baselayout.html"", line 20, in top-level template code
    {% import 'appbuilder/baselib.html' as baselib %}
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask_appbuilder/templates/appbuilder/init.html"", line 46, in top-level template code
    {% block body %}
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/superset/templates/appbuilder/baselayout.html"", line 39, in block ""body""
    </div>
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/superset/templates/appbuilder/general/model/list.html"", line 26, in block ""content""
    {% block list_search scoped %}
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/superset/templates/appbuilder/general/model/list.html"", line 27, in block ""list_search""
    {{ widgets.get('search')()|safe }}
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask_appbuilder/widgets.py"", line 115, in __call__
    form_fields[col] = self.template_args[""form""][col]()
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/wtforms/fields/core.py"", line 160, in __call__
    return self.meta.render_field(self, kwargs)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/wtforms/meta.py"", line 56, in render_field
    return field.widget(field, **render_kw)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask_appbuilder/fieldwidgets.py"", line 176, in __call__
    return super(Select2ManyWidget, self).__call__(field, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/wtforms/widgets/core.py"", line 300, in __call__
    for val, label, selected in field.iter_choices():
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask_appbuilder/fields.py"", line 208, in iter_choices
    for pk, obj in self._get_object_list():
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask_appbuilder/fields.py"", line 128, in _get_object_list
    objs = self.query_func()
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask_appbuilder/forms.py"", line 139, in <lambda>
    return lambda: self.datamodel.get_related_interface(col_name).query()[1]
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask_appbuilder/models/sqla/interface.py"", line 365, in query
    count = self.query_count(query, filters, select_columns)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask_appbuilder/models/sqla/interface.py"", line 288, in query_count
    return self._apply_inner_all(
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py"", line 3803, in count
    return self.from_self(col).scalar()
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py"", line 3523, in scalar
    ret = self.one()
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py"", line 3490, in one
    ret = self.one_or_none()
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py"", line 3459, in one_or_none
    ret = list(self)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py"", line 3535, in __iter__
    return self._execute_and_instances(context)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py"", line 3560, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1011, in execute
    return meth(self, multiparams, params)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/sql/elements.py"", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1124, in _execute_clauseelement
    ret = self._execute_context(
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1316, in _execute_context
    self._handle_dbapi_exception(
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1510, in _handle_dbapi_exception
    util.raise_(
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 182, in raise_
    raise exception
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/engine/base.py"", line 1276, in _execute_context
    self.dialect.do_execute(
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such column: alerts.sql
[SQL: SELECT count(*) AS count_1 
FROM (SELECT alerts.id AS alerts_id, alerts.label AS alerts_label, alerts.active AS alerts_active, alerts.crontab AS alerts_crontab, alerts.sql AS alerts_sql, alerts.alert_type AS alerts_alert_type, alerts.recipients AS alerts_recipients, alerts.log_retention AS alerts_log_retention, alerts.grace_period AS alerts_grace_period, alerts.slice_id AS alerts_slice_id, alerts.dashboard_id AS alerts_dashboard_id, alerts.database_id AS alerts_database_id, alerts.last_eval_dttm AS alerts_last_eval_dttm, alerts.last_state AS alerts_last_state 
FROM alerts) AS anon_1]
(Background on this error at: http://sqlalche.me/e/13/e3q8)
```

### Expected results

what you expected to happen.

### Actual results

what actually happens.

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

### Environment

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,dpgaspar,"
--
That seems like a db migration issue, did your `superset db upgrade` executed without issues?
--

--
Has your `SECRET_KEY` changed? also `no such column: alerts.sql` is a migration issue for sure, if the upgrade procedure had no errors make sure your upgrading the same database has your flask process. I see your using `sqlite3` is this correct?
--

--
Do you have any important data on your local (assuming that) sqlite db? If not just delete it from `${HOME}/.superset/superset.db` and repeat the install process
--

--
If you have data there, you need to troubleshoot your current db, some questions: 
- What version (or SHA) of superset you running?
- Are you overriding you db connection on `superset_config.py` or `config.py`?
- Did you upgrade superset?
- second run `superset db upgrade` and post the entire log

--
",magic,"
--
> That seems like a db migration issue, did your `superset db upgrade` executed without issues?

yes without issue. Error when superset init.
--

--
log:
```
 ubuntu:~$ superset init
logging was configured successfully
INFO:superset.utils.logging_configurator:logging was configured successfully
/home/ubuntu/venv/lib/python3.8/site-packages/flask_caching/__init__.py:191: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
  warnings.warn(
Syncing role definition
INFO:superset.security.manager:Syncing role definition
Syncing Admin perms
INFO:superset.security.manager:Syncing Admin perms
Syncing Alpha perms
INFO:superset.security.manager:Syncing Alpha perms
Syncing Gamma perms
INFO:superset.security.manager:Syncing Gamma perms
Syncing granter perms
INFO:superset.security.manager:Syncing granter perms
Syncing sql_lab perms
INFO:superset.security.manager:Syncing sql_lab perms
Fetching a set of all perms to lookup which ones are missing
INFO:superset.security.manager:Fetching a set of all perms to lookup which ones are missing
Creating missing datasource permissions.
INFO:superset.security.manager:Creating missing datasource permissions.
Creating missing database permissions.
INFO:superset.security.manager:Creating missing database permissions.
Traceback (most recent call last):
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy_utils/types/encrypted/encrypted_type.py"", line 126, in decrypt
    decrypted = decrypted.decode('utf-8')
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x9e in position 2: invalid start byte

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ubuntu/venv/bin/superset"", line 8, in <module>
    sys.exit(superset())
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask/cli.py"", line 586, in main
    return super(FlaskGroup, self).main(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/click/core.py"", line 1259, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/click/decorators.py"", line 21, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask/cli.py"", line 426, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/click/decorators.py"", line 21, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/flask/cli.py"", line 426, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/superset/cli.py"", line 75, in init
    security_manager.sync_role_definitions()
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/superset/security/manager.py"", line 632, in sync_role_definitions
    self.create_missing_perms()
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/superset/security/manager.py"", line 575, in create_missing_perms
    databases = self.get_session.query(models.Database).all()
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/orm/query.py"", line 3373, in all
    return list(self)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/orm/loading.py"", line 100, in instances
    cursor.close()
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py"", line 68, in __exit__
    compat.raise_(
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/util/compat.py"", line 182, in raise_
    raise exception
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/orm/loading.py"", line 80, in instances
    rows = [proc(row) for row in fetch]
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/orm/loading.py"", line 80, in <listcomp>
    rows = [proc(row) for row in fetch]
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/orm/loading.py"", line 579, in _instance
    _populate_full(
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/orm/loading.py"", line 725, in _populate_full
    dict_[key] = getter(row)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy/sql/type_api.py"", line 1278, in process
    return process_value(impl_processor(value), dialect)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy_utils/types/encrypted/encrypted_type.py"", line 469, in process_result_value
    value = super().process_result_value(value=value, dialect=dialect)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy_utils/types/encrypted/encrypted_type.py"", line 414, in process_result_value
    decrypted_value = self.engine.decrypt(value)
  File ""/home/ubuntu/venv/lib/python3.8/site-packages/sqlalchemy_utils/types/encrypted/encrypted_type.py"", line 128, in decrypt
    raise ValueError('Invalid decryption key')
ValueError: Invalid decryption key
```
--

--
I didn't change SECRET_KEY. I'm using sqlite3.
--

--
Will my saved queries and chart be deleted as well?
--

--
* Running version 0.38.0
* superset_config.py
* I upgraded superset by following command
 * pip install apache-superset --upgrade
 * superset db upgrade
 * superset init

Log:

> (venv) ubuntu@ip-5:~$ superset db upgrade
> logging was configured successfully
> INFO:superset.utils.logging_configurator:logging was configured successfully
> /home/ubuntu/venv/lib/python3.8/site-packages/flask_caching/__init__.py:191: UserWarning: Flask-Caching: CACHE_TYPE is set > to null, caching is effectively disabled.
>   warnings.warn(
> WARNI [alembic.env] SQLite Database support for metadata databases will         be removed in a future version of Superset.
> INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
> INFO  [alembic.runtime.migration] Will assume transactional DDL.
--
",,,,,,,,
11942,OPEN,SQLLAB - Postgres Error when column is not quotations,bug,2021-02-17 22:22:13 +0000 UTC,hughhhh,Opened,,"A clear and concise description of what the bug is.

### Expected results
User should be able to write a query without having to wrap all columns in quotations marks.

```
SELECT 
      DATE_TRUNC('week', year) AS __timestamp,
      SUM(NY_GNP_PCAP_CD) AS ""(NY_GNP_PCAP_CD)""
FROM wb_health_population
GROUP BY DATE_TRUNC('week', year)
LIMIT 10000
```

### Actual results

When writing a query to postgres and the column doesn't have quotation the user will receive the following error:

```
postgresql error: column ""ny_gnp_pcap_cd"" does not exist
LINE 5:       SUM(NY_GNP_PCAP_CD) AS ""(NY_GNP_PCAP_CD)""
                  ^
```

```
SELECT 
      DATE_TRUNC('week', year) AS __timestamp,
      SUM(""NY_GNP_PCAP_CD"") AS ""(NY_GNP_PCAP_CD)"" -- this works
FROM wb_health_population
GROUP BY DATE_TRUNC('week', year)
LIMIT 10000
```


#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Go to 'sqllab'
2. Select a postgres database
3. Write a query with columns not wrapped in columns (see the example above)
4. See error

### Environment

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,yousoph,"
--
 sql_lab
--
",eschutho,"
--
 sql_lab:editor
--
",,,,,,,,
11935,OPEN,[chart]Echarts boxplot needs axis label input fields in Customize,bug:cosmetic; enhancement:request; viz:echarts,2020-12-04 18:24:24 +0000 UTC,junlincc,Opened,,"Boxblot current Customize tab 
<img width=""1784"" alt=""Screen Shot 2020-12-04 at 10 18 52 AM"" src=""https://user-images.githubusercontent.com/67837651/101199809-446d5e80-361a-11eb-8587-9905fb35fd22.png"">

vs. 
Line chart 
<img width=""1774"" alt=""Screen Shot 2020-12-04 at 10 18 30 AM"" src=""https://user-images.githubusercontent.com/67837651/101199976-8a2a2700-361a-11eb-8fb5-001f8fe4c698.png"">

cc: @eugeniamz ",,,,,,,,,,,,,,
11932,OPEN,[list view]sort icons inconsistency,bug:cosmetic,2020-12-04 21:11:51 +0000 UTC,junlincc,Opened,,"Sort icons should be displayed on Columns - Charts, Visualization Type, Dataset, Modified By, Last Modified, Created By
Im not sure missing on `Dataset` and `Created` By is by design 

<img width=""892"" alt=""Screen Shot 2020-12-04 at 9 49 20 AM"" src=""https://user-images.githubusercontent.com/67837651/101196773-0706d200-3616-11eb-88cd-8b6171025ed9.png"">

related issue: 
https://github.com/apache/incubator-superset/issues/11689#issue-742170841",,,nytai,"
--
We can probably add sorting on ""Created by"" but ""Dataset"" is by design. There some technical reasons related to the weird dataset relationship in charts that doesn't allow for us to sort on it. 
--
",,,,,,,,,,
11917,OPEN,Allow to change number of rows in Time series table,bug; enhancement:request; viz:chart-pivot,2021-02-08 04:32:33 +0000 UTC,eugeniamz,Opened,,"By default time series table retrieve 5000 rows but it does not give the option of changing or show that you reach the limit (in other cases the number of rows are marked red) 

I know that it would better to process less number of rows but the chart is misleading if people do not know the limitation 

![image](https://user-images.githubusercontent.com/58375897/101078867-bb501c00-3574-11eb-82a1-4516316dd7cf.png)

If I filter the rows clearly show the misleading 
![image](https://user-images.githubusercontent.com/58375897/101079425-75e01e80-3575-11eb-817a-4ab3d54c5e59.png)



### Expected results

At least a red mark to show that the data hit or Limit of 5K or the ability to change the count
### Actual results
Show the chart with missing data in not a particular order so the chart is missliding 

#### How to reproduce the bug
Produce a Timer series chart with more than 5K rows

### Environment
0.38

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [X ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,maloun96,"
--
@junlincc It was fixed https://github.com/apache/superset/pull/12887
--
",junlincc,"
--
@eugeniamz we added row limit to time series table. please let me know if this solution is good enough. thanks @maloun96 
<img width=""407"" alt=""Screen Shot 2021-02-07 at 8 31 47 PM"" src=""https://user-images.githubusercontent.com/67837651/107176186-881a7680-6983-11eb-8fd5-fde9709dc3d3.png"">

--
",,,,,,,,
11916,OPEN,Design Proposal: Explore Control Panel Improvements,.pinned; change:ui; enhancement:committed,2021-03-03 12:55:11 +0000 UTC,mihir174,In progress,,"<img width=""1456"" alt=""Explore Layout@2x"" src=""https://user-images.githubusercontent.com/64227069/101077985-69020180-355a-11eb-9a38-5d57115bc808.png""> _Figma Screenshot @ 12/3/2020 11:26am_

Hey all, here is the Figma page that contains the latest mockups & design assets for the Explore Control Panel:
https://www.figma.com/file/JWaGztdhZS0kS5ruG7x9tB/Control-Panel?node-id=195%3A36068

These designs are a step towards arriving at the functionality and designs accepted in [SIP-34](https://github.com/apache/incubator-superset/issues/8976). The other key motivations are to move closer to industry standards & best practices, and to make the experience more intuitive for new & non-technical users. 

These are some of the design considerations and things we thought about:

### Overall Layout & Functionality  - 

The [Cartel Explore design](https://projects.invisionapp.com/share/BNVBOEJRWQ2#/screens/398392104) has the following features - 
A. View columns & metrics next to the control panel
B. Drag & drop columns & metrics into the control panel 
C. Toggle between similar chart types
D. View the data under the chart

Here's what we are able to currently implement as a step towards these - 
A. View columns & metrics next to the control panel

D. View the data under the chart

Some FAQ & info about how this design state works - 

(A,B) What does the columns & metrics panel do if users cant drag and drop? 
Currently, the columns and metrics panel are read-only, and functions as a browser. It is in an awkward in-between phase in terms of functionality, but for the time being there is added value in:
- Understanding the contents of large datasets
- Finding columns & metrics more easily


(A) Why cant we collapse this read-only panel? 

Making any part of this side panel collapsible is outside of the technical scope for this effort. I did think about whether a collapsable panel was critical for this iteration of the design, and here are my thoughts:
- The panel is the main editable surface - seeing just the chart and data doesnt have much value in the Explore view, in which users primarily manipulate data visualizations
- The current layout is also not ideal for a collapsible panel in its collapsed state. The Cartel design contains additional layout elements that have not been implemented (eg. C - chart type toggle selector) that make the layout conducive for both the collapsed and expanded states.
- The panel in Superset currently does not collapse 

(A) The Columns and Metrics Panel is split into 2 collapsable sections (Columns, Metrics) - this doesn't follow the Cartel design exactly, which had them all in 1 list. There is an added search to make finding a column or a metric simpler for very large datasets. 
There is only a small amount of additional horizontal real estate taken up by the control panel.

### Control Panel - 

The sections in the [Line Chart example](https://www.figma.com/file/JWaGztdhZS0kS5ruG7x9tB/Control-Panel?node-id=195%3A36132) are based on a larger effort to make the sections for each chart type consistent. The thinking is that users should expect to find the same kind of input/control under the same section of the control panel for each chart type. Some of the changes as per the SIP-34 designs- 
- The datasource and chart type selectors are now a permanent sections and are not grouped with the rest of the controls
- Time column is removed - not all datasources have temporal columns and even if they do, the user may not use it as x. Instead, the time inputs are moved to the `Query` section, that has the sub-sections `Columns`, `Filters`, `Grouping` and `Other` - that can be consistent across other temporal chart types.

This is how I think about some of the key UI components for inputs:
- There are 2 types of items that can be taken as inputs into fields on the control panel:
    - Simple items: these do not need any configuration before adding. Ex. Column, Line Style
    - Complex items: these need configuration before adding. Ex. Metric, Filter (you have to select an aggregator or you can use custom SQL), Time Range (There are many settings and options to play with).
- There are also input fields that can take 1 item and input fields that can take multiple items
- So in total there are 4 types of inputs:

1. Single Simple Item

2. Multiple Simple Items

3. Single Complex Item

4. Multiple Complex Items
- Figma frame that shows each: https://www.figma.com/file/JWaGztdhZS0kS5ruG7x9tB/Control-Panel?node-id=195%3A36838
- For complex inputs, the add flow is most clear (especially for new or inexperienced users) when the entire input is configured before adding. This is why the config pop-over is moved to the start of the flow.",,,junlincc,"
--
Thank you so much @mihir174 for sharing Superset upcoming designs in Explore, and the rationale behind those changes with the community! This is a huge step forward in informing and further collaborating with the broader Superset community in Design transparently. 

Community folks, if you have not met Mihir, he is our designer at Preset, with a focus on Explore and Dashboard. Our latest Dashboard Filter Indicator, Superset Home Screen, and upcoming Native Dashboard Filter, New Explore view are some of his latest design projects. We have been receiving valuable feedback regarding these projects and we are committed to iterating on all our Design mentioned above continually. 

For this particular project, we will not be calling for a vote, as it is consistent with the proposed design in SIP-34, which has been voted in previously. 
We will be rolling out `D. View the data under the chart` mentioned in this proposal next week, and starting the implementation `A. View columns & metrics next to the control panel`, as well as new control panel soon. 

Please leave your comments here or in the Figma file directly; it's open for comments for everyone. 

Thanks so much in advance for all your input! 
--

--
> I think the control panel doesn't have to collapse. We could just add resizing to both.

@ktmud we are adding collapse to the data panel for 1.0. you mentioned that you planed to work on the resizing  if you have a timeline in mind, we are happy to let you execute it. if not, we may go with both collapsible for now, you or anyone can pick it up from there later. let us know you plan~ cc: @pkdotson

> is there a way to make the collapse icon lighter?

@mihir174 light blue does stand out. can we use a different color? 

let's lock the scope soon, as development has started already.  

--

--
@kgabryje 
--
",mihir174,"
--
Nice to meet you all and I'm looking forward to working and iterating together! :)
--

--
@etr2460 Thanks for the quick feedback! I totally agree that the chart should have enough space.

At these screen dimensions, the current panel has a width of 494px, and the proposed one is 576px (82px difference is about the length of the current `run` button for ref). 

We still haven't finalized the area split between the chart and the data, so there is an opportunity to increase the chart size there - what do you think is a good min. size for the chart or ratio between chart and data table areas?

Here's the main layout issue around a collapsable panel illustrated:

SIP-34 Design-

Expanded
![Screen Shot 2020-12-03 at 12 22 1](https://user-images.githubusercontent.com/64227069/101084252-89ce5500-3562-11eb-8242-14eaaf10e3b8.png)

Collapsed - the panel just slides to the left
![Screen Shot 2020-12-03 at 12 22 2](https://user-images.githubusercontent.com/64227069/101084286-9783da80-3562-11eb-9bd0-2ebdedb561ef.png)

This design (if we were to have the section collapse)-

Expanded
![Expanded@2x](https://user-images.githubusercontent.com/64227069/101084324-a5d1f680-3562-11eb-9f43-4f0a9a61af55.png)

Collapsed - the panel has to move under the datasource selector, which is a really awkward motion
![Collapsed@2x](https://user-images.githubusercontent.com/64227069/101084352-b1bdb880-3562-11eb-9b85-734079f8d241.png)


--

--
@etr2460 @ktmud 

Hey all, thanks for the feedback on the collapsable sidebars and the references!
I had a mental block earlier that the datasource selector always has to be visible - but realized that it shouldn't have to.

I tried a few different versions of collapsible side panels, but here's a clickable prototype of the version I thought was most compelling:
https://www.figma.com/proto/todPvrS5oBo1NckmHrLVtp/Collapsable-Control-Panel-Prototypes?node-id=1%3A14066&scaling=min-zoom

In this prototype, both the datasource panel and the viz control panel are collapsable. We could choose either or both.

Here's a screen recording GIF:

![Collapsable Sidebar](https://user-images.githubusercontent.com/64227069/101228683-97163d00-3651-11eb-97ab-be76a6da3e01.gif)


Here's the Figma file with all the explorations and other prototypes (each page is a different version):
https://www.figma.com/file/todPvrS5oBo1NckmHrLVtp/Collapsable-Control-Panel---Prototypes?node-id=1%3A7853
--
",etr2460,"
--
This looks great directionally! I'll probably have more time to go through all the designs in more depth later.

One point that I did disagree with was the answer to: ""Why cant we collapse this read-only panel?""

I'm a bit concerned that the existence of 2 side panels will meaningfully affect the amount of space for the chart more than a single panel does today. Without exact sizes, it's tough to tell. I don't see a reason why we couldn't add a ""Collapse sidebar"" option to the 3 dot menu (or a left chevron icon somewhere) to allow collapsing of the datasource panel if a user so chooses. 

I think this is especially important now that we've also added the ""Data"" section to explore: it looks like the chart only takes up about 20% of the full screen space now! Let me know your thoughts.

--

--
Yes, what @ktmud is suggesting is what i had in mind!

As for the split between the chart and the data, i'm a bit less concerned about that, as charts should generally be in a landscape ratio. Honestly, adding data makes that easier, but it's probably important to keep the chart the focus of explore view too. I'd say we should prefer keep the chart as wide as possible, and then make the default height something pleasing to look at. Super scientific i know :P
--
",graceguo,"
--
I also have concern about new datasource section.
- read only:  
Recently there are a lot discussion about dataset editor: user want to see the definition of metrics and columns, and want to test their change **_safe and easy_**. If you can pull out some information that was nested in the tabs of Dataset Editor into this flat straightforward view, that will be very helpful.
Also, why not allow user edit metric/column from here? user can do change => run  => preview, then decide if they want to save the change the definition of a metric or calculated column. It will be much more convenient than current flow (edit in the Dataset Editor Modal), many clicks to find the metric/column, and change is not reversible.

- performance risk:
Maybe just in airbnb, we have some dataset has huge number of metrics and dimensions. It already caused performance issue when we open it in Dataset Editor: page will freeze and crash. So now we don't show option for the users to edit this huge dataset. With this new design, we don't have option to hide(?), it might introduce performance issue when user load small chart from a huge dataset.
--
",pkdotson,"
--
Thanks @mihir174 for the designs! This look good as far as implementation. @graceguo-supercat I believe the team discussed at cutting off the dataset columns and metrics at 50 and implementing a search feature later on.
--
",mistercrunch,"
--
Looks neat! Some notes:

- about sections, I feel like we need to replicate the constructs in the dataset editor: dataset columns, metrics and calculated columns. For metrics and calculated dimensions, we have some components that expose info-bubbles + tooltips to show the SQL expression and the long description if/when available. We probably want to leverage those or do something similar here 

- about responsiveness: I think currently the left panel becomes full-width as the width gets smaller, and the chart is shown underneat that section. Wondering what we'll do now on smaller width.

- +1 on @graceguo-supercat idea of enabling adding/editing dataset objets (metrics, calculated columns, ...) from here eventually (not required for MVP).
--

--
Seems like ""TIME GRAIN"" belongs under the ""Grouping"" section not the ""Filter"" section
--
"
11906,OPEN,[import]importing dashboard lost annotation on the charts,bug,2020-12-03 01:27:56 +0000 UTC,junlincc,Opened,,"issue 1: annotation layers are not imported or shown when user import a dashboard with annotations on charts
issue 2: user can not explore the charts on an imported dashboard 

![ezgif-2-7102aefec615](https://user-images.githubusercontent.com/67837651/100951520-8f289280-34c3-11eb-9ab0-c15f7466844e.gif)

cc: @betodealmeida ",,,,,,,,,,,,,,
11898,OPEN,Adding annotation to dual line chart,bug; enhancement:request; viz:chart-line,2021-01-02 18:28:10 +0000 UTC,agatapst,Opened,,"In dual line chart, which according to [superset-ui](https://github.com/apache-superset/superset-uil) does not have any supported annotation types, it is possible to open modal with adding new annotation. Bar chart does not have any supported annotation types and this section is hidden - it works fine in that case.

In [superset-ui](https://github.com/apache-superset/superset-uil) `controlPanel` for `DualLine`, `sections.annotations` should be removed. That way it will be consistent with `DualLineChartPlugin` metadata, that Dual Line chart does not have any supported annotation types.

It is worth checking if it is consistent for other charts to avoid future bugs. 
### Expected results
In the left bar, Add Annotation Layer button and the whole annotation section should be hidden. 

### Actual results
The annotation section is displayed and the modal opens. It is impossible to add any working annotation.

#### Screenshots
![annotations_dual_line_before](https://user-images.githubusercontent.com/47450693/100920371-4ed50e80-34db-11eb-9204-2e0626e8161a.gif)

#### How to reproduce the bug

1. Go to Explore
2. Create dual line chart
3. Click Add Annotation Layer in the left bat

### Environment

- superset version: current master (commit 44e80e)
- python version: 3.7.9
- node.js version: 14.15.1

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Partly connected with PR #11876
",,,junlincc,"
--
this is awesome!thank you Agata for point out issues as you move.  i removed the Polidea label for now as this issue may not land on you guys. 
--
",,,,,,,,,,
11886,OPEN,Do we have any suggestion to support Mechine Learning like Kibana ML?,enhancement:request; need:followup,2021-02-21 11:00:01 +0000 UTC,wangqinghuan,In progress,,,,,junlincc,"
--
@wangqinghuan we might, could you please provide more details to this request? thanks. 

--
",wangqinghuan,"
--
Like [this](https://www.elastic.co/guide/en/kibana/current/xpack-ml.html).  
--
",,,,,,,,
11882,OPEN,[chart] New chart data API should format errors according to SIP-40,enhancement:request; global:error,2020-12-02 06:19:49 +0000 UTC,robdiciuccio,Opened,,"Currently, requests to `/api/v1/chart/data` return an error response in the following format:
```
{ message: ""error message text here"" }
```
Error responses should be formatted according to [SIP-40](https://github.com/apache/incubator-superset/issues/9194) in order to enable rich error reporting for charts in the frontend. 

**Example of chart error reporting with legacy (`/superset/explore_json`) chart requests:**
<img width=""458"" alt=""Screen Shot 2020-12-01 at 3 54 43 PM"" src=""https://user-images.githubusercontent.com/296227/100810629-bc0a7600-33ed-11eb-9d18-66b5d5690ec5.png"">
<img width=""595"" alt=""Screen Shot 2020-12-01 at 3 54 50 PM"" src=""https://user-images.githubusercontent.com/296227/100810653-c3ca1a80-33ed-11eb-8cba-09355bdd561c.png"">

**Example of chart error reporting with new (`/api/v1/chart/data`) chart requests:**
<img width=""460"" alt=""Screen Shot 2020-12-01 at 3 54 57 PM"" src=""https://user-images.githubusercontent.com/296227/100810727-ee1bd800-33ed-11eb-9190-fd61a1bfe8e5.png"">
<img width=""593"" alt=""Screen Shot 2020-12-01 at 3 55 19 PM"" src=""https://user-images.githubusercontent.com/296227/100810735-f247f580-33ed-11eb-9a90-fb1a1ce7b3d3.png"">

**Additional context**
- https://github.com/apache/incubator-superset/issues/9298 (SIP-41)
",,,,,,,,,,,,,,
11879,OPEN,[explore] improve database error handling,bug:cosmetic; global:error,2020-12-02 06:20:00 +0000 UTC,mistercrunch,Opened,,"## Screenshot

<img width=""1171"" alt=""Screen Shot 2020-12-01 at 9 40 40 AM"" src=""https://user-images.githubusercontent.com/487433/100776417-60be9080-33b9-11eb-96dc-7b704721f98f.png"">
<img width=""432"" alt=""Screen Shot 2020-12-01 at 9 40 34 AM"" src=""https://user-images.githubusercontent.com/487433/100776421-61efbd80-33b9-11eb-8be9-cf00ef145b25.png"">

## Description

This kind of error (DatabaseError) should be expected and handled well. It should really say something like `BigQuery error` and offer the right detail as well as monospace-formatted details.

Somehow SQL Lab does a better job at surfacing the error message from the database:
<img width=""1155"" alt=""Screen Shot 2020-12-01 at 9 43 11 AM"" src=""https://user-images.githubusercontent.com/487433/100776664-a8451c80-33b9-11eb-9e6c-e568e5de2129.png"">

We may need database-specific handling of error message strings, so that we can decompose a title, subtitle (one line message) and a longer description. For BigQuery for example, it'd be nice to show only that first string and allow to expand the longer multi-line blob.",,,,,,,,,,,,,,
11873,OPEN,Translation issue,bug; i18n:others,2020-12-01 16:03:24 +0000 UTC,eugene-belarus,Opened,,"I'm used the superset is released 1-2 years ago and I don't have untranslatable words but when I've updated superset to current version lots of words don't have translations.
Navigation bar (""Data"", ""Charts"", ""Dashboards"", ""SQL Lab"", ""Setting"") can't also be translate for any languages.

### Expected results

There's no untranslatable words.

### Actual results

A lot of words don't have translations

#### Screenshots

![image](https://user-images.githubusercontent.com/71097330/100737956-f1ab6100-33e5-11eb-9098-d1d4e334a033.png)
![image](https://user-images.githubusercontent.com/71097330/100738009-0687f480-33e6-11eb-8044-6199bac4b7f3.png)
",,,issue,"
--
Issue Label Bot is not confident enough to auto-label this issue. See [dashboard](https://mlbot.net/data/apache/incubator-superset) for more details.
--
",,,,,,,,,,
11846,OPEN,Overwriting deckgl plugin gives error: module not found.,bug,2020-11-30 12:21:12 +0000 UTC,DrissiReda,Opened,,"I am trying to use tiles from an offline tileserver for maps instead of mapbox.

I've tried changing the plugin @superset-ui/legacy-preset-chart-deckgl. But then the plugin isn't found.

I've also disabled the plugin @superset-ui/legacy-plugin-map-box just in case it's that as well.

Inside superset-frontend: I run:
```
npm install
```

No problem, everything goes:

I then run:

```
npm link ./plugins/legacy-preset-chart-deckgl 
```
This path has my modified deckgl chart plugin.

I finally run:

```
npm run build
```

I get the error:

```
ERROR in ./src/visualizations/presets/MainPreset.js
Module not found: Error: Can't resolve '@superset-ui/legacy-preset-chart-deckgl' in '/root/work/superset/incubator-superset/superset-frontend/src/visualizations/presets'

```
<details>
<summary>
The whole output
</summary>
ERROR in chunk addSlice [entry]
addSlice.8d7e36a138fea26409fa.entry.js
/root/work/superset/incubator-superset/superset-frontend/node_modules/thread-loader/dist/cjs.js!/root/work/superset/incubator-superset/superset-frontend/node_modules/babel-loader/lib/index.js??ref--5-1!/root/work/superset/incubator-superset/superset-frontend/node_modules/ts-loader/index.js??ref--5-2!/root/work/superset/incubator-superset/superset-frontend/src/setup/setupPlugins.ts 0679771b1f82c6d5a0c81f94218ef1db
Unexpected token (52:20)
|     super({
|       name: 'Legacy charts',
|       presets: [new !(function webpackMissingModule() { var e = new Error(""Cannot find module '@superset-ui/legacy-preset-chart-deckgl'""); e.code = 'MODULE_NOT_FOUND'; throw e; }())()],
|       plugins: [
|       new __WEBPACK_MODULE_REFERENCE__124_64656661756c74__().configure({ key: 'area' }),

ERROR in chunk crudViews [entry]
crudViews.077c1a2bba108cabec92.entry.js
/root/work/superset/incubator-superset/superset-frontend/node_modules/thread-loader/dist/cjs.js!/root/work/superset/incubator-superset/superset-frontend/node_modules/babel-loader/lib/index.js??ref--5-1!/root/work/superset/incubator-superset/superset-frontend/node_modules/ts-loader/index.js??ref--5-2!/root/work/superset/incubator-superset/superset-frontend/src/setup/setupPlugins.ts 0679771b1f82c6d5a0c81f94218ef1db
Unexpected token (52:20)
|     super({
|       name: 'Legacy charts',
|       presets: [new !(function webpackMissingModule() { var e = new Error(""Cannot find module '@superset-ui/legacy-preset-chart-deckgl'""); e.code = 'MODULE_NOT_FOUND'; throw e; }())()],
|       plugins: [
|       new __WEBPACK_MODULE_REFERENCE__124_64656661756c74__().configure({ key: 'area' }),

ERROR in chunk dashboard [entry]
dashboard.df09bfca764f5d43cf51.entry.js
/root/work/superset/incubator-superset/superset-frontend/node_modules/thread-loader/dist/cjs.js!/root/work/superset/incubator-superset/superset-frontend/node_modules/babel-loader/lib/index.js??ref--5-1!/root/work/superset/incubator-superset/superset-frontend/node_modules/ts-loader/index.js??ref--5-2!/root/work/superset/incubator-superset/superset-frontend/src/setup/setupPlugins.ts 0679771b1f82c6d5a0c81f94218ef1db
Unexpected token (52:20)
|     super({
|       name: 'Legacy charts',
|       presets: [new !(function webpackMissingModule() { var e = new Error(""Cannot find module '@superset-ui/legacy-preset-chart-deckgl'""); e.code = 'MODULE_NOT_FOUND'; throw e; }())()],
|       plugins: [
|       new __WEBPACK_MODULE_REFERENCE__124_64656661756c74__().configure({ key: 'area' }),

ERROR in chunk explore [entry]
explore.fb84e5ba94ba22631009.entry.js
/root/work/superset/incubator-superset/superset-frontend/node_modules/thread-loader/dist/cjs.js!/root/work/superset/incubator-superset/superset-frontend/node_modules/babel-loader/lib/index.js??ref--5-1!/root/work/superset/incubator-superset/superset-frontend/node_modules/ts-loader/index.js??ref--5-2!/root/work/superset/incubator-superset/superset-frontend/src/setup/setupPlugins.ts 0679771b1f82c6d5a0c81f94218ef1db
Unexpected token (52:20)
|     super({
|       name: 'Legacy charts',
|       presets: [new !(function webpackMissingModule() { var e = new Error(""Cannot find module '@superset-ui/legacy-preset-chart-deckgl'""); e.code = 'MODULE_NOT_FOUND'; throw e; }())()],
|       plugins: [
|       new __WEBPACK_MODULE_REFERENCE__124_64656661756c74__().configure({ key: 'area' }),

</details>

The change I have made is minimal, I just change this block from `src/utilities/Shared_DeckGL.jsx`:
```js
export const mapboxStyle = {
  name: 'mapbox_style',
  config: {
    type: 'SelectControl',
    label: t('Map Style'),
    clearable: false,
    renderTrigger: true,
    choices: [
      ['mapbox://styles/mapbox/streets-v9', 'Streets'],
      ['mapbox://styles/mapbox/dark-v9', 'Dark'],
      ['mapbox://styles/mapbox/light-v9', 'Light'],
      ['mapbox://styles/mapbox/satellite-streets-v9', 'Satellite Streets'],
      ['mapbox://styles/mapbox/satellite-v9', 'Satellite'],
      ['mapbox://styles/mapbox/outdoors-v9', 'Outdoors'],
    ],
    default: 'mapbox://styles/mapbox/light-v9',
    description: t('Base layer map style'),
  },
};
```

With this one:

```js
export const mapboxStyle = {
  name: 'mapbox_style',
  config: {
    type: 'TextControl',
    label: t('Map Style'),
    renderTrigger: true,
      default: 'localhost:18080/data/openmaptiles_satellite_lowres.json',
    description: t('Base layer map style'),
  },
};
```",,,,,,,,,,,,,,
11832,OPEN,SqlLab ResultsTable Formatting long strings,bug:cosmetic,2020-11-27 00:33:29 +0000 UTC,pabrahamusa,Opened,,"## Screenshot


![image](https://user-images.githubusercontent.com/47830507/100398789-2dc57700-301e-11eb-991e-f81ae215d235.png)


## Description

I am using SqlLab to query my data that having very long String i.e. > 10000 , SqlLab results table at the moment truncates the string with ellipsis and the result table width is too big .This all makes it hard to read. I tried to add some word=wrap
in FilterableTableStyles.less , However it is overflowing and I was not able to fix it. Is there a way I can enable word wrap , reduce the result table width and stop overflow so all results appear nicely?

`.ReactVirtualized__Table__rowColumn {
  /* text-overflow: ellipsis; */
  /* white-space: nowrap; */
  white-space: pre-wrap !important;
  word-break: break-all !important;
  flex-wrap: wrap !important;`


",,,issue,"
--
Issue Label Bot is not confident enough to auto-label this issue. See [dashboard](https://mlbot.net/data/apache/incubator-superset) for more details.
--
",,,,,,,,,,
11829,OPEN,Categorical color/dimension issue,bug; bug:cosmetic,2020-12-23 14:49:06 +0000 UTC,wildsonc,In progress,,"## Screenshot

![NcAoRifnIK](https://user-images.githubusercontent.com/72408418/100382173-7ae32200-2ff9-11eb-92ab-d03b7c650824.png)

## Description
I'm not able to add any more columns in the dimension field, which was previously called a categorical color. I am also not able to add to the extra data field for js.

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.85. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",sashankpappu,"
--
Even i am facing the same issue . Can someone please help resolve this . 
--
",wildsonc,"
--
> Even i am facing the same issue . Can someone please help resolve this .

I found this solution #10277, but I couldn't solve it on my environment
--
",junlincc,"
--
To reproduce:

use long_lat datasource to create deck.gl.Scartterplot with LON as the longitude and LAT as the latitude
scroll down to the Point Color section and click DIMENSION
There supposed to be 15 options but nothing is showing

cc: @nikolagigic
--
",ali,"
--
i'm facing the same issue on the latest version.
--
",,
11826,OPEN,"charts colors forced in dashboard do not work anymore (""label_colors"")",.pinned; bug,2020-12-09 07:10:25 +0000 UTC,squalou,In progress,,"See #2524

In a dashbord properties, one can set ""label_colors"", so that all charts on the dashboard will share the same colors for identical data.

It's been broken in 0.37.2 and 0.38.0. 

Last time seen working was 0.36.0

### Expected results

all charts share the predefined colors of each category, set using ""label_colors""

### Actual results

charts original colors arekept, or the one chosen in dashboard is used, but label_colors is ignored

#### Screenshots

When it works :

![image](https://user-images.githubusercontent.com/4623644/100337275-76a60d00-2fd7-11eb-978a-8e15510c0716.png)


With newer version :

![image](https://user-images.githubusercontent.com/4623644/100337499-be2c9900-2fd7-11eb-9055-56fbce12be5a.png)




#### How to reproduce the bug

1. create two bar charts showing categories ""category1, category2, ... for instance"". Define a different custom palette for each
2. add them on a dashboard
3. go to edit / edit dashboard properties
4. in dashboard properties set for instance 

```
""label_colors"": {
    ""category1"": ""rgb(248, 166,28)"",
    ""category2"": ""black"",
    ""category3"": ""rgb(227, 59, 5)"",
    ""category4"": ""rgb(245, 104, 7) "",
    ""category5"": ""rgb(230, 28, 96)"",
    ""category6"": ""rgb(0, 228, 201) ""
  },
```
5. look at the dashboard : it does never use these ""label_colors""

NOTE : in dashboard settings, selecting a palette does overwrite charts palettes, removing a palette tehre leaves the charts one,
but ""label_colors"" is never used.

### Environment

- superset version: `superset version`

```bash
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Superset 0.38.0
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
```

- python version: `python --version`

```python
- Python 3.6.7
```

node.js version: `node -v`

```js
v12.16.3
```
### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,junlincc,"
--
try refresh the page and see if changes take place? 
same and related issues [#11677](https://github.com/apache/incubator-superset/issues/11677) [#11674](https://github.com/apache/incubator-superset/issues/11674)
--
",squalou,"
--
I tried. So many refreshes and combinations of actions. Save, save as new, ne charts, remove colors entirely from dashborad properties, etc etc.

a word about  #11674 : in that issue, the consistence in dashboard --> explore chart worflow is disccussed.

In my case I'm more looking for consistency *on a single dashboard* accross different charts. It's slightly different and the solution may be the same in the end. But as a ""use case"" it's very different, I hope the initial explanation was undertandable :)

The need underneath is : affect *one fixed color* to a category once and for all. (Once per dashboard or even one for all susperset, in my specific case it would be fine)




--

--
Already tried that, or maybe I might not understand really what you mean. Could you elaborate a bit ? I'd be more than happy to make it work.
--

--
That's weird.
I can edit as you do from dashboard list, or even from dashboard edit itself (choosing a color scheme from dropdown).
It does persist ""color_scheme"" alright.
But then : these colors are applied to all charts in dashboard.
(It seems funny when I write it because : it's exactly what one would expect I guess when choosing a dashboard color scheme.)

my ""label_colors"" are ignored.

example of my json (masked names but you'll get the idea). I don't see label_colors in your case, could you expand a bit the view ? maybe I set them in a wrong place (that used to work though)

![image](https://user-images.githubusercontent.com/4623644/101480636-3f7b1880-3954-11eb-9944-f7d41da5e1d7.png)





--

--
(additional info : there seems to be indeed some cases when it works, then something break it, still investigating to see from scratch at what moment soemthing goes wrong)
--

--
Finally I've one thing that works, I hope it will help tackle the root issue, which is in the end : one has to guess the right **charts** settings and nothing guides the tries and errors :) ... and it changed over time.

If you have
- charts with ""**superset**"" color palette chosen  (AND NO OTHER)
- note : any custom 'label_colors' can be defined on the chart itself, not an issue
- dashboard with *any* color palette chose or even `null`
- then add `{""label_colors"":{""name"":""#123456""}}` to Dashboard json properties 
- don't forget to refresh Dashboard after each label_color change

=> in this case it works

Once upon a time : Charts required ti use *rb&b* palette, now it requires *superset*. 

--
",lilila,"
--
You simply need to set up a default color for your dashboard. 
--

--
I can ensure you that label_colors work in dashboards for superset 38. 
I have trouble editing dashboard metadata with the menu so I usually go on dashboard list, click on the edit buttom and change the value of `colors_scheme`  as shown on the picture.

 
<img width=""923"" alt=""Screenshot 2020-12-08 at 11 55 03"" src=""https://user-images.githubusercontent.com/607430/101475089-38500c80-394c-11eb-8969-2a0170bfa0d7.png"">
 

--
",,,,,,
11823,OPEN,dates not aligned in tables,bug:cosmetic,2020-12-04 07:34:27 +0000 UTC,squalou,In progress,,"In a table with a column containing dates, data are misaligned.
Probably due to a bad font choice for number renderning.

Used to be fine in < 0.36 version if my memories are good

### Expected results

Cleanly verticallly aligned data

### Actual results

see screenshot

#### Screenshots

![image](https://user-images.githubusercontent.com/4623644/100318003-9c261d00-2fbd-11eb-8531-0c4ee364bad2.png)


#### How to reproduce the bug

1. Create any table containing date data, with for instance 2020 and 2019 year (as the ""1"" and the ""0"" wont have the same width)
4. See error

### Environment

(please complete the following information):

- superset version: `superset version`

```bash
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
Superset 0.38.0
-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
```

- python version: `python --version`

```python
- Python 3.6.7
```

node.js version: `node -v`

```js
v12.16.3
```

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,squalou,"
--
BTW : the font is ""Inter"", if I remove it (by editing css for instance, it's fine with Helvetica)

![image](https://user-images.githubusercontent.com/4623644/100318815-f07dcc80-2fbe-11eb-89fc-cf6bcef2d0fd.png)


(I did tweak the file `superset-frontend/stylesheets/less/variables.less`, removed 'Inter', rebuilt, and it is now aligned but I have no idea of other fonts side effects)
--
",rusackas,"
--
I'll try to get to this soon, but I'd like to keep Inter as the font, which [supports tabular numbers](https://rsms.me/inter/). We should be able to turn on the feature by setting the css `font-feature-settings` attribute to include ""tnum"". It *probably* makes sense to apply that style within any `table` elements, but this is where the real experimentation lies :D
--
",,,,,,,,
11819,OPEN,Improve loading state in the new doc site,bug:cosmetic,2020-12-08 05:14:22 +0000 UTC,ktmud,Opened,,"## Screenshot

![image](https://user-images.githubusercontent.com/335541/100295368-49f1e700-2f3e-11eb-806b-e0191331d913.png)

## Description

The doc site looks broken when JS files are still loading.

To reproduce:

<img src=""https://user-images.githubusercontent.com/335541/100295439-74dc3b00-2f3e-11eb-9f96-e775b0c1434f.png"" width=""500"">

Go to Network tab, check ""Disable cache"" and ""Fast 3G"".


## Design input

No design input needed. Just fix the broken page by adding some sized placeholder elements or blocking the page from rendering when CSS/content is incomplete.
",,,junlincc,"
--
@ktmud hey Jesse, we probably can't get to this issue by the EOY, if this is important to you and it doesn't take long to fix, do you mind.....
--
",,,,,,,,,,
11796,OPEN,ERROR:superset.sql_lab:Query 14: <class 'UnicodeDecodeError'>,bug,2020-11-24 11:30:27 +0000 UTC,ysrotciv,In progress,,"A clear and concise description of what the bug is.

### Expected results

show content correctly in preview tab under SQL lab

### Actual results

clickhouse error: 'utf-8' codec can't decode byte 0xcc in position 69: invalid continuation byte

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. add a SQLAlchemy URI of clickhouse in databases list with format mentioned in document 'clickhouse://default@XXXXXXXX:8124/tutorial'
2.choose a table with utf-8 character 
3. Preview tab show error 'clickhouse error: 'utf-8' codec can't decode byte 0xcc in position 69: invalid continuation byte'

### Environment

(please complete the following information):
deploy with docker
- superset version: `0.37.2`
- python version: `3.7.9`
- node.js version: `v4.2.6`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.

Log in container

Triggering query_id: 17
INFO:superset.views.core:Triggering query_id: 17
timeout can't be used in the current context
WARNING:superset.utils.core:timeout can't be used in the current context
signal only works in main thread
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/superset/utils/core.py"", line 624, in __enter__
    signal.signal(signal.SIGALRM, self.handle_timeout)
  File ""/usr/local/lib/python3.6/signal.py"", line 47, in signal
    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))
ValueError: signal only works in main thread
ERROR:superset.utils.core:signal only works in main thread
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/superset/utils/core.py"", line 624, in __enter__
    signal.signal(signal.SIGALRM, self.handle_timeout)
  File ""/usr/local/lib/python3.6/signal.py"", line 47, in signal
    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))
ValueError: signal only works in main thread
SQLite Database support for metadata databases will be removed             in a future version of Superset.
WARNING:superset.sql_lab:SQLite Database support for metadata databases will be removed             in a future version of Superset.
Query 17: Executing 1 statement(s)
INFO:superset.sql_lab:Query 17: Executing 1 statement(s)
Query 17: Set query to 'running'
INFO:superset.sql_lab:Query 17: Set query to 'running'
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 192.168.100.47:8124
DEBUG:urllib3.connectionpool:http://192.168.100.47:8124 ""POST /?user=default HTTP/1.1"" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 192.168.100.47:8124
DEBUG:urllib3.connectionpool:http://192.168.100.47:8124 ""POST /?database=tutorial&user=default HTTP/1.1"" 200 None
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 192.168.100.47:8124
DEBUG:urllib3.connectionpool:http://192.168.100.47:8124 ""POST /?database=tutorial&user=default HTTP/1.1"" 200 None
Query 17: Running statement 1 out of 1
INFO:superset.sql_lab:Query 17: Running statement 1 out of 1
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 192.168.100.47:8124
DEBUG:urllib3.connectionpool:http://192.168.100.47:8124 ""POST /?query_id=47f0c45e-2e2a-11eb-a7a5-0242ac110002&database=tutorial&user=default HTTP/1.1"" 200 None
Query 17: <class 'UnicodeDecodeError'>
ERROR:superset.sql_lab:Query 17: <class 'UnicodeDecodeError'>",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.81. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",zhaoyongjie,"
--
Which clickhouse dialect do you use

On Tue, Nov 24, 2020 at 3:54 PM issue-label-bot[bot] <
notifications@github.com> wrote:

> Issue-Label Bot is automatically applying the label #bug to this issue,
> with a confidence of 0.81. Please mark this comment with  or  to give
> our bot feedback!
>
> Links: app homepage <https://github.com/marketplace/issue-label-bot>,
> dashboard <https://mlbot.net/data/apache/incubator-superset> and code
> <https://github.com/hamelsmu/MLapp> for this bot.
>
> 
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/apache/incubator-superset/issues/11796#issuecomment-732721884>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAPMKUTYLYXG27GBFUJUDNTSRNRDZANCNFSM4UAQSFEA>
> .
>

--

--
@yahwang  try setting engine_params to UTF-8

DATA -> DATABASES -> select clickhouse instance. editing Extra
```
{
    ""metadata_params"": {},
    ""engine_params"": {""encoding"": ""UTF-8""},
    ""metadata_cache_timeout"": {},
    ""schemas_allowed_for_csv_upload"": []
}
```
--

--
Try to use this script in the same environment to check whether if you can display UTF-8 table name
```
from sqlalchemy import create_engine, inspect

engine = create_engine(""clickhouse://<DBURI>"")
inspector = inspect(engine)
inspector.get_table_names()
```
--

--
@yahwang 

which table contains UTF-8 column? fetch the columns by the SQLAlchemy inspector. Is it raise an exception?
```
inspector.get_columns('<TABLE NAME>')
```
--
",ysrotciv,"
--
> Which clickhouse dialect do you use
> [](#)
> On Tue, Nov 24, 2020 at 3:54 PM issue-label-bot[bot] < ***@***.***> wrote: Issue-Label Bot is automatically applying the label #bug to this issue, with a confidence of 0.81. Please mark this comment with  or  to give our bot feedback! Links: app homepage <https://github.com/marketplace/issue-label-bot>, dashboard <https://mlbot.net/data/apache/incubator-superset> and code <https://github.com/hamelsmu/MLapp> for this bot.  You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <[#11796 (comment)](https://github.com/apache/incubator-superset/issues/11796#issuecomment-732721884)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAPMKUTYLYXG27GBFUJUDNTSRNRDZANCNFSM4UAQSFEA> .

sqlalchemy-clickhouse
--

--
> @yahwang try setting engine_params to UTF-8
> 
> DATA -> DATABASES -> select clickhouse instance. editing Extra
> 
> ```
> {
>     ""metadata_params"": {},
>     ""engine_params"": {""encoding"": ""UTF-8""},
>     ""metadata_cache_timeout"": {},
>     ""schemas_allowed_for_csv_upload"": []
> }
> ```

not work, nothing changed
--

--
> Try to use this script in the same environment to check whether if you can display UTF-8 table name
> 
> ```
> from sqlalchemy import create_engine, inspect
> 
> engine = create_engine(""clickhouse://<DBURI>"")
> inspector = inspect(engine)
> inspector.get_table_names()
> ```

>>> from sqlalchemy import create_engine, inspect
>>> engine = create_engine(""clickhouse://default@192.168.xxx.xxx:8124/tutorial"")
>>> inspector = inspect(engine)
>>> inspector.get_table_names()
['hits_v1', 'hits_v1_nre', 'visits_v1']
--
",,,,,,
11793,OPEN,Flaky test: test_get_report_schedule,bug,2021-04-07 17:30:09 +0000 UTC,etr2460,In progress,,"It looks like the `test_get_report_schedule` test fails intermittently because the owners array isn't guaranteed to be in the same order every time. It's been preventing me from getting CI in https://github.com/apache/incubator-superset/pull/11785 to pass.

@dpgaspar, is this related to the FAB ModelRestApi?",,,etr2460,"
--
This is also blocking https://github.com/apache/incubator-superset/pull/11791
--
",amitmiran137,"
--
is this still relevant?

--
",,,,,,,,
11788,OPEN,Cannot login with default password,bug,2020-12-16 16:48:08 +0000 UTC,yshrotciv,Opened,,"A clear and concise description of what the bug is.

### Expected results

sucessfully login

### Actual results

Failed to login.

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. login with admin/admin

### Environment

(please complete the following information):

deployed with docker-compose
- superset version: `0.37.2`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

log from superset_init

Init Step 1/4 [Starting] -- Setting up admin user ( admin / admin )


######################################################################

Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 567, in _build_master
    ws.require(__requires__)
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 884, in require
    needed = self.resolve(parse_requirements(requirements))
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 775, in resolve
    raise VersionConflict(dist, req).with_context(dependent_req)
pkg_resources.ContextualVersionConflict: (pyarrow 0.17.0 (/usr/local/lib/python3.7/site-packages), Requirement.parse('pyarrow<1.1,>=1.0.1'), {'apache-superset'})

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/bin/superset"", line 33, in <module>
    sys.exit(load_entry_point('apache-superset', 'console_scripts', 'superset')())
  File ""/usr/local/lib/python3.7/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/flask/cli.py"", line 586, in main
    return super(FlaskGroup, self).main(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/usr/local/lib/python3.7/site-packages/click/core.py"", line 1254, in invoke
    cmd_name, cmd, args = self.resolve_command(ctx, args)
  File ""/usr/local/lib/python3.7/site-packages/click/core.py"", line 1297, in resolve_command
    cmd = self.get_command(ctx, cmd_name)
  File ""/usr/local/lib/python3.7/site-packages/flask/cli.py"", line 527, in get_command
    self._load_plugin_commands()
  File ""/usr/local/lib/python3.7/site-packages/flask/cli.py"", line 517, in _load_plugin_commands
    import pkg_resources
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 3238, in <module>
    @_call_aside
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 3222, in _call_aside
    f(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 3251, in _initialize_master_working_set
    working_set = WorkingSet._build_master()
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 569, in _build_master
    return cls._build_from_requirements(__requires__)
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 582, in _build_from_requirements
    dists = ws.resolve(reqs, Environment())
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 770, in resolve
    raise DistributionNotFound(req, requirers)
pkg_resources.DistributionNotFound: The 'pyarrow<1.1,>=1.0.1' distribution was not found and is required by apache-superset
",,,qiaofeng1227,"
--
It is same with me, the command `superset` error when run, may be lack  the python  packages .
--
",AsjadMahmood,"
--
Hey Guys i am having the same issue. I completed the installation steps. And in the last step, i had to login. But i am unable to login with username=""admin"" and password=""admin""

<img width=""1440"" alt=""Screenshot 2020-12-14 at 12 51 45 PM"" src=""https://user-images.githubusercontent.com/38811693/102054672-8ad96f00-3e0b-11eb-8614-a84259479e5d.png"">


--

--
> Hi @AsjadMahmood . Could you show me the Superset log output?

hey i solved the problem. The problem was not with superset, actually i was previously running a container of postgres which was kind of colliding with the the container of  superset-postres. so i resolved that. 
Thanks for the quick reply though.  
--
",zhaoyongjie,"
--
Hi @AsjadMahmood . Could you show me the Superset log output?
--
",rtjarvis,"
--
I have this issue too

--

--
```
root@90da97871614:/app# flask fab list-users
Loaded your LOCAL configuration at [/app/pythonpath/superset_config.py]
INFO:superset.utils.logging_configurator:logging was configured successfully
/usr/local/lib/python3.7/site-packages/flask_caching/__init__.py:189: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
  ""Flask-Caching: CACHE_TYPE is set to null, ""
List of users
-------------

```
--

--
OK - a little more debugging. I tried `flask fab create-admin` and created an admin user. This is accepted by the UI but I get a stack trace response:

```sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation ""user_attribute"" does not exist
LINE 2: FROM user_attribute
^

[SQL: SELECT user_attribute.welcome_dashboard_id AS user_attribute_welcome_dashboard_id
FROM user_attribute
WHERE user_attribute.user_id = %(user_id_1)s]
[parameters: {'user_id_1': '1'}]
(Background on this error at: http://sqlalche.me/e/f405)```

--
",dpgaspar,"
--
Can you post the output from `flask fab list-users` please
--
",,
11777,OPEN,clickhouse Unserializable object of type <class 'ipaddress.IPv4Address'>,bug; data:connect:clickhouse,2021-01-02 21:43:04 +0000 UTC,chen-ABC,Opened,,"A clear and concise description of what the bug is.

### Expected results
return data
use DBeaver can return data
![image](https://user-images.githubusercontent.com/30097790/99933828-17f25080-2d97-11eb-96fa-da94c4266d6d.png)

![image](https://user-images.githubusercontent.com/30097790/99933807-0ad56180-2d97-11eb-8c5a-3bbfbd0975fa.png)

### Actual results
![image](https://user-images.githubusercontent.com/30097790/99933626-97cbeb00-2d96-11eb-9496-55871550de97.png)



### Environment

(please complete the following information):

- superset version: `0.37`
- python version: `3.7.9`
- node.js version: `v12.18.3`
- sqlalchemy-clickhouse  `0.1.5`

",,,issue,"
--
Issue Label Bot is not confident enough to auto-label this issue. See [dashboard](https://mlbot.net/data/apache/incubator-superset) for more details.
--
",mistercrunch,"
--
We've seen a fair amount of drivers returning non-native/serializable types, wondering how we should best handle this. @robdiciuccio any thoughts?

My guess is we need something in `db_engine_spec` that enables us to map types to a serialization method, but wondering if it's reasonable to just cast to string as a catchall.
--

--
It's also apparent that the driver/dialect may need some work too, I'm guessing the driver return `NullType`.
--
",,,,,,,,
11768,OPEN,SQL editing not obvious for metrics in datasource editor in explore view,bug:cosmetic,2020-11-20 19:19:18 +0000 UTC,bkyryliuk,Opened,,"## Screenshot

![image](https://user-images.githubusercontent.com/5727938/99840863-2a178900-2b22-11eb-84fc-ead5acf76639.png)


## Description

Small piece of feedback for metric editor in the datasource editor for explore view. It looks a bit confusing, most of the fields are in the dropdown, but metric needs to be edited inline. E.g. metric is editable, but label next to it is not, also when creating a new metric - sql field is not showing u
",,,,,,,,,,,,,,
11767,OPEN,"In dashboard; ""Time column"" Datetime format incorrectly displayed for ""Table viz"" after applying ""Time Grain"" filter from ""filter_box"".",bug,2020-11-20 18:45:33 +0000 UTC,javidov,Opened,,"After creating a **table visualization** in ""Explore view"", I can change the ""**time grain**"" after checking **""Include Time""** and see the displayed results with correct **Datetime** formats (**""Table Timestamp Format""** is set to **""Adaptative formating""**). Next I added the **saved table chart** to a **""dashboard""** along with a **""filter_box""** created to provide the ability to change the **""time grain""** of **Datetime** (noted as **Time**) column of the table viz. However when I choose a **""time grain"" from the ""filter""box""**, the new **Datetime format of Time column** corresponding to **new** ""time grain"" is **incorrectly displayed**. 
 New fetched records are displayed with **""Time"" column format** corresponding to the initial **""time_grain_sqla""** format saved in **chart metadata** after saving the chart from the **""Explore view""**. 

### Expected results

**""Time"" column format** change in dashboard after changing **""time grain""** value from **filter_box.**

### Actual results

Records are correctly fetched from database (**clickhouse**), but the displayed **Datetime format** in **""Time"" column** are based on **""time grain"" set after creating and saving table viz from ""Explore view"".**

#### Screenshots
This is when I set ""time grain"" to **year** in Explore view, results are correct
![year](https://user-images.githubusercontent.com/74787835/99833639-02004800-2b6b-11eb-8c42-2039f1d8b1fc.PNG)

This is when I set ""time grain"" to **month** in Explore view, results are correct. **The chart is **SAVED** with month as time grain.**
![month](https://user-images.githubusercontent.com/74787835/99833805-42f85c80-2b6b-11eb-922e-d1b7620ab205.PNG)

This is the **initial view from the dashboard**. The table is displayed with **""time grain"" equal to month** (table viz is saved with month time grain in explore view).
![initial](https://user-images.githubusercontent.com/74787835/99833852-57d4f000-2b6b-11eb-870a-29c73603ef42.PNG)

When new **""Time grain"" is selected** from ""Time grain"" **dropdown of filter_box,** records are fetched successfully, but **Datetime format for ""Time"" column is incorrect**.
![thebug](https://user-images.githubusercontent.com/74787835/99834087-b13d1f00-2b6b-11eb-9b95-34f9e01593aa.PNG)

This is the expected results in **Dashboard** (as seen in explore view).
![expected](https://user-images.githubusercontent.com/74787835/99834325-07aa5d80-2b6c-11eb-8e78-4d1802bfb405.PNG)





#### How to reproduce the bug

1. **Create** table visualization.
2. Choose **""Include time""**.
3. Select metrics to display.
4. Select a **""Time grain""** from **""Time related form attributes""**
5. **Save** the chart.
6. Create **Filter_box** viz,  check **""the Show SQL Granularity Dropdown""**
7. Add table and filter_box viz to a dashboard and select a ""Time grain"" value from filter_box dropdown 
### Environment

(please complete the following information):

- superset version: `0.37.2`
- python version: `2.7.17`
- Clickhouse version : `19.17.6.36`
- Ubuntu version : `Ubuntu 18.04.5`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.95. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",,,,,,,,,,
11758,OPEN,Hexagons chart broken in deck.gl demo,bug,2021-01-02 18:18:54 +0000 UTC,craig-rueda,Opened,,"Hexagons demo chart shows error after loading samples:

<img width=""736"" alt=""Screen Shot 2020-11-19 at 3 25 23 PM"" src=""https://user-images.githubusercontent.com/2595291/99736561-72c83700-2a7b-11eb-8e9a-2b36616d04fe.png"">

<img width=""627"" alt=""Screen Shot 2020-11-19 at 3 26 24 PM"" src=""https://user-images.githubusercontent.com/2595291/99736646-97241380-2a7b-11eb-892a-ac69bf1cd5ca.png"">

POST to `explore_json` fails with error:
```
POST https://my.host.here/superset/explore_json/?form_data=%7B%22slice_id%22%3A77%7D&dashboard_id=5 500 (INTERNAL SERVER ERROR)

Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/superset/views/base.py"", line 177, in wraps
    return f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/superset/utils/cache.py"", line 108, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/superset/views/core.py"", line 534, in explore_json
    return self.generate_json(viz_obj, response_type)
  File ""/usr/local/lib/python3.7/site-packages/superset/views/core.py"", line 430, in generate_json
    payload = viz_obj.get_payload()
  File ""/usr/local/lib/python3.7/site-packages/superset/viz.py"", line 480, in get_payload
    payload = self.get_df_payload(query_obj)
  File ""/usr/local/lib/python3.7/site-packages/superset/viz.py"", line 512, in get_df_payload
    query_obj = self.query_obj()
  File ""/usr/local/lib/python3.7/site-packages/superset/viz.py"", line 2375, in query_obj
    self.add_null_filters()
  File ""/usr/local/lib/python3.7/site-packages/superset/viz.py"", line 2356, in add_null_filters
    for column in self.get_spatial_columns(key):
  File ""/usr/local/lib/python3.7/site-packages/superset/viz.py"", line 2291, in get_spatial_columns
    raise ValueError(_(""Bad spatial key""))
ValueError: Bad spatial key
```
### Expected results

Hexagons should load properly

what you expected to happen.

#### How to reproduce the bug

1. Load examples from scratch
2. Navigate to deck.gl dashboard
",,,junlincc,"
--
`Bad spatial key` map layer problem? man i dont know what to do with all these broken charts  im also not sure how many people are still using this chart 
--
",,,,,,,,,,
11756,OPEN,Bottom border missing from chart list,bug:cosmetic; viz:dashboard:ui,2021-01-02 18:37:11 +0000 UTC,craig-rueda,Opened,,"## Screenshot

<img width=""392"" alt=""Screen Shot 2020-11-19 at 3 13 07 PM"" src=""https://user-images.githubusercontent.com/2595291/99735649-d5203800-2a79-11eb-8939-a8ae511f0908.png"">

## Description

In the create/edit dashboard screen, the items in the chart list are missing their bottom border
",,,nytai,"
--
Only seems to be an issue when there's only 1 chart
<img width=""366"" alt=""Screen Shot 2020-11-19 at 3 20 19 PM"" src=""https://user-images.githubusercontent.com/10255196/99736159-c5edba00-2a7a-11eb-9672-6329cd2633b2.png"">

--

--
Yup, looks like we need to apply a border on the `last-of-type`
--

--
Looks like the issue is actually not accounting for the height of borders.

Bumping this line  +2 should account for the 1px border above and below 

https://github.com/apache/incubator-superset/blob/cdd01f4851d39d8d03dcd92653252cbbba52b6ed/superset-frontend/src/dashboard/components/SliceAdder.jsx#L67
--
",craig,"
--
Yep. From your screenshot, it looks like there's double borders between the items as well. Prob need always need a top-border and only a bottom for the last...
--
",junlincc,"
--

<img width=""367"" alt=""Screen Shot 2020-11-19 at 3 25 08 PM"" src=""https://user-images.githubusercontent.com/67837651/99736559-71970a00-2a7b-11eb-981a-d30d325b037d.png"">


happening to me when more than one chart..... 

--

--
> Bumping this line +2 should account for the 1px border above and below

thank you @nytai for the hint 
--

--
> Actually, it wasn't an issue with borders, but line height being inherited from `.ant-tabs`. It caused the cards to be far larger than before and they were overlapping each other. The fix is here: #11766

Kamil, you are awesome and impressively fast i cant even catch up! thanks for the fix! @kgabryje 
--

--
spotted additional cosmetic issues, let's keep it open until they are addressed. 

fix: Chart select borders in BuilderComponentPane #11766 
--
",kgabryje,"
--
Actually, it wasn't an issue with borders, but line height being inherited from `.ant-tabs`. It caused the cards to be far larger than before and they were overlapping each other. The fix is here: https://github.com/apache/incubator-superset/pull/11766
--
",,,,
11745,OPEN,Timestamp is not parsed properly in Charts,bug,2020-11-19 21:55:35 +0000 UTC,pabrahamusa,In progress,,"While creating the Charts the timestamp queries are not parsed correctly. I tried to add table chart and the query as per the logs is 

 SELECT COUNT(*) AS ""count""
FROM ""default"".""mydata""
WHERE ""timemillis"" >= '2020-11-12 00:00:00.000000'
  AND ""timemillis"" < '2020-11-19 00:00:00.000000'
ORDER BY ""count"" DESC

Which fails even if I try in SQLlab because I assume we cannot compare date as varchar. So it could be a potential bug in superset. I see in other tickets this issue is fixed however I am using the latest tag (docker pull apache/incubator-superset:latest) dated 18 Nov 2020 , DIGEST:sha256:92db38930faa65024558242d55e01eb57b5b5359c895aad3da007fcb16520896

![image](https://user-images.githubusercontent.com/47830507/99622397-0e17c880-29f8-11eb-8af0-22176254b929.png)











",,,nytai,"
--
This is likely due to your column not being correctly identified as timestamp by superset. If you open the ""edit dataset"" modal,  got to the columns tab, is the column Data Type a timestamp type and is Is Temporal checked? 
--

--
ah, could it be this? https://stackoverflow.com/questions/38037713/presto-static-date-and-timestamp-in-where-clause
--

--
hmm interesting. So is it the explore ""Time Range"" option that isn't working? 
--

--
@villebro have you seen this before? 
--
",pabrahamusa,"
--
 @nytai I can see the timestamp is identified by superset and Temporal checked automatically as well.  I am assuming the superset should cast the varchar '2020-11-12 00:00:00.000000' to timestamp using some date functions.

![image](https://user-images.githubusercontent.com/47830507/99624613-8da79680-29fc-11eb-950d-62b5f48fabe8.png)

--

--
Actually thats what I am doing now, Going to SQLLab and create and test the query with proper date cast and explore -> create charts from there.  Add new Chart option directly will not work.
--

--
@villebro Actually this is automatically mapped from Pinot schema . In pinot it is LONG , EPOCH timestamp in millis. The pinot presto connector maps it to TIME automatically I presume.  Not sure why it is mapped to TIME rather than TIMESTAMP. https://prestodb.io/docs/current/connector/pinot.html 
--

--
@villebro Actually pinot.infer-timestamp-type-in-schema is set to true and presto identifies the column as TIMESTAMP, however the comment section says TIME. I assume we can ignore the comment part. 

presto:default> show columns from pinot.default.mydata;

```
 Column       |   Type         | Extra |  Comment
-----------------------------------------------
 cluster        | varchar       |           | DIMENSION
 data            | varchar       |           | DIMENSION
 host            | varchar       |           | DIMENSION
 timemillis    | timestamp |           | TIME
```

--
",villebro,"
--
Thanks for tagging @nytai . @pabrahamusa it seems you're using the `TIME` type, which is fairly poorly supported in Superset. Can you elaborate on your use case, and why this isn't a `TIMESTAMP` or `FLOAT` or similar?
--

--
@pabrahamusa oh I didn't realize this was Pinot over Presto. Hmm, not sure how to handle this. Do you think it would be possible to set `pinot.infer-timestamp-type-in-schema` to true? I think Superset is going to have a hard time understanding that the `TIME` type is in fact referring to a Pinot datatype, not the Presto `TIME` one.
--
",,,,,,
11742,OPEN,dashboard owner is not able remove themselves from dashboard owners,bug,2020-11-18 23:24:24 +0000 UTC,junlincc,Opened,,"_**using reference in new issue feature**_ 

_1. If you are dashboard owner - you can add chart from chart explore to dashboard directly. This works.
2. If you are NOT dashboard owner but are CREATOR of the dashboard (for example I created a dashboard, and then I asked an admin to remove me as owner, but I was still the creator of that dashboard) - I could still edit the dashboard (when on dashboard) and add charts, but I wasn't able to see this dashboard in the dropdown on chart explore, so I actually couldn't add chart directly from chart explore. --> assuming creator always keeps edit rights, this potentially might be a small bug, that creator can't add from chart explore directly (but simple workaround to add form dashboard)
3. And if I was not owner or creator, I couldn't edit dashboard and couldn't add chart into dashboard from chart explore. This is expected._

**Unrelated thing I noticed, is that user (not admin) who is dashboard owner is not able remove themselves from dashboard owners (even if there are additional owners). This could be a bug.** 

_Originally posted by @zuzana-vej in https://github.com/apache/incubator-superset/issues/11662#issuecomment-730014498_",,,,,,,,,,,,,,
11741,OPEN,[sqllab]table schema select pattern is confusing,enhancement:request; sql_lab:control:ui,2021-01-03 05:28:47 +0000 UTC,junlincc,Opened,,"Currently, the pattern of selecting table schema in SQL Lab is very confusing.
1. it's a dropdown selection menu but the selected table is not populated in the box  
2. latest selected table goes all the way to the bottom in the panel, users have to scroll down to view column types, or take actions 
even with the data preview section under the editor, this flow is very unintuitive and hard for the users to tell which table is actually being selected from the dropdown 

![flow](https://user-images.githubusercontent.com/67837651/99592507-2c080d80-29a5-11eb-8f98-8f34fd0dc191.gif)
",,,yousoph,"
--
Agreed that it's confusing, thanks for bringing this up! Would like to spend some time soon on the left panel. 
--
",,,,,,,,,,
11738,OPEN,[Landing page] page load is very slow,bug,2021-04-09 10:58:34 +0000 UTC,graceguo-supercat,Opened,,"The landing page loads very slow for airbnb users. Everyday the first time i open Superset landing page, it took > 2 minutes to show content. I thought the page was broken. Next page load will be very fast, but only last 1 day (i assume it because airbnb datasource has daily partition).

#### Screenshots

<img width=""1101"" alt=""Screen Shot 2020-11-18 at 12 17 48 AM"" src=""https://user-images.githubusercontent.com/27990562/99570635-14ba2780-2987-11eb-9d9f-33f1f59cb8b5.png"">


#### How to reproduce the bug

1. Go to Superset landing page
1. wait to see content showing up

### Environment

lastest master


### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,nytai,"
--
I spoke to @etr2460 this morning and we were able to determine that it's a combination of the [recent activity api](https://github.com/apache/incubator-superset/blob/86651cd451a29335b5e9aee6600c831074a510ed/superset/views/core.py#L1197) being very slow and the loading state for the entire page [depending on that api request](https://github.com/apache/incubator-superset/blob/86651cd451a29335b5e9aee6600c831074a510ed/superset-frontend/src/views/CRUD/welcome/Welcome.tsx#L125)
--
",junlincc,"
--
@ktmud 
--
",amitmiran137,"
--
@graceguo-supercat does this still happens?
--
",,,,,,
11735,OPEN,superset load examples fail,need:screenshot,2020-11-18 18:25:15 +0000 UTC,pengtaomach,Opened,," using superset load-examples  loading the examples data, an error was reported saying that the link timed out. Was the example data deleted at the target end",,,junlincc,"
--
can you provide more details? version, screenshot etc. 
--
",willbarrett,"
--
The example data is loaded as part of our CI runs - no, the example data has not been deleted.
--
",,,,,,,,
11733,OPEN,[time-series chart]area chart style variation,enhancement:request; viz:chart-area,2021-01-02 18:28:26 +0000 UTC,junlincc,In progress,,"the new Echarts time-series chart looks great, but we should consider hardening it and increase time-series area chart style variation(Stack, Stream and Expand) to reach NVD3 area chart feature parity
after that, we can deprecate the old chart that has multiple issues not worth fixing
related https://github.com/apache-superset/superset-ui/issues/799

**Echarts - timeseries area chart**:
<img width=""1791"" alt=""Screen Shot 2020-11-17 at 6 33 31 PM"" src=""https://user-images.githubusercontent.com/67837651/99475511-8baddc80-2903-11eb-99d3-afc97f4ef88a.png"">


**NVD3 area chart**:
<img width=""1471"" alt=""Screen Shot 2020-11-17 at 6 26 51 PM"" src=""https://user-images.githubusercontent.com/67837651/99475367-412c6000-2903-11eb-87f8-41643dd460c5.png"">
<img width=""1469"" alt=""Screen Shot 2020-11-17 at 6 26 43 PM"" src=""https://user-images.githubusercontent.com/67837651/99475372-42f62380-2903-11eb-841b-7bc2ec842cee.png"">
<img width=""1469"" alt=""Screen Shot 2020-11-17 at 6 26 58 PM"" src=""https://user-images.githubusercontent.com/67837651/99475380-44bfe700-2903-11eb-9e86-5f6ba58866c2.png"">
",,,junlincc,"
--
@willbarrett @nytai this can be good first Echarts tasks, what do you guys think? 
https://echarts.apache.org/examples/zh/editor.html?c=themeRiver-basic
https://echarts.apache.org/examples/zh/editor.html?c=area-stack

cc: @rusackas 
--
",willbarrett,"
--
Let's see what shakes out after we get ramped up. This looks like a good candidate for the hardening phase in January to me, but we'll see what the rest of the team thinks!
--
",,,,,,,,
11725,OPEN,SQL Lab: widen left panel; limit type width,bug:cosmetic; needs:design-input,2020-11-17 22:32:38 +0000 UTC,mistercrunch,In progress,,"_I used the feature to create issue from comment on GitHub_, bypassed the template

@yousoph the bug is fixed, but the left panel still seems too crowded. I think the sql editor doesn't need that much of the space, but your call! 
<img width=""320"" alt=""Screen Shot 2020-11-16 at 6 15 44 PM"" src=""https://user-images.githubusercontent.com/67837651/99337774-c64a4300-2837-11eb-8939-b99849f6b347.png"">

_Originally posted by @junlincc in https://github.com/apache/incubator-superset/issues/11715#issuecomment-728641732_",,,mistercrunch,"
--
Some ideas:
- increase the left panel width by ~33%?
- map long type names like `TIMESTAMP WITHOUT TIME ZONE` to something like `TIMESTAMP...` with details on hover?
--

--
For explore it's a responsive grid. For SQL Lab it's fixed I think
--
",junlincc,"
--
@mihir14 what's the width are you settingfor the left panel in Explore(Data type fields + control)? 
i suggest to keep the left panel spaces in Explore and SQL lab the same! 
@mistercrunch 

--

--
can we coordinate to make that change?
--

--
im trying to figure out a good timing to discuss major changes on those panels. @rusackas and I wont be able to dive deep  into until mid December, i also want to do a more throughout product discovery before our discussion. +1 on resizable though!  
--
",ktmud,"
--
We should make both resizable just like the SQL editor + result pane. Was actually working on that over the past weekend.
--

--
It was just an idea I've been wanting to do for a while but never committed to doing it. Feel free to create another github issue or roadmap item if you feel it's necessary.
--
",yousoph,"
--
Agreed that resizing would be a good first step! 

For the SQL Lab panel, I think it's worth revisiting how the column types are displayed (max character limit for column types, font styles, etc) 
--
",,,,
11713,OPEN,Inline script execution and styling in superset violating content security policy,enhancement:request,2020-11-17 03:17:35 +0000 UTC,mistaanthony,Opened,,"Using Superset frontend in environments that enforce strict content security policies, for example <meta http-equiv=""Content-Security-Policy"" content=""default-src 'self' 'unsafe-eval' ""> which blocks the use of inline css and JavaScript causes some elements not to be rendered, including data visulisations. 

Ref: https://developers.google.com/web/fundamentals/security/csp#inline_code_is_considered_harmful


**Examples of the inline code are as follows:**

```
<div class=""dataTables_scrollHeadInner"" style=""box-sizing: content-box; width: 756px; padding-right: 0px;"">
    <table class=""table table-striped table-condensed table-hover dataTable no-footer"" role=""grid"" style=""margin-left:0px; 
     width: 756px;"">
        <thead>
            <tr role=""row"">
                <th class=""sorting"" title=""Message Status"" tabindex=""0"" aria-controls=""DataTables_Table_1"" rowspan=""1"" 
                colspan=""1"" aria-label=""Message Status: activate to sort column ascending"" style=""width:335px;"">Message Status
                </th>
                <th class=""sorting"" title=""count"" tabindex=""0"" aria-controls=""DataTables_Table_1"" rowspan=""1"" colspan=""1""
                aria-label=""count: activate to sort column ascending"" style=""width: 152px;"">count
                </th>
                <th class=""sorting"" title=""% count"" tabindex=""0"" aria-controls=""DataTables_Table_1"" rowspan=""1"" colspan=""1"" aria- 
                 label=""% count: activate to sort column ascending"" style=""width: 194px;"">% count
                 </th>
            </tr>
       </thead>
   </table>
</div>
```


**There are also inline functions on buttons such as the example below**

```
<a href=""javascript:void(0)"" class=""btn btn-sm btn-default confirm"" rel=""tooltip"" title="""" onclick=""var a = new AdminActions(); return a.execute_single_delete('/dashboard/delete/2','You sure you want to delete this item?');"" data-original-title=""Delete record"">
    <i class=""fa fa-eraser""></i>
</a>

```
### Expected results

Superset should be able to run in secure environments with strictly enforced policies. It should not have any inline JS or css, css should be class oriented and both css/js should be referenced in an external file.

### Actual results

Superset UI loads, but it's not working, strict Content Security Policy will block inline styles and inline functions from executing.

#### Screenshots

<img width=""1680"" alt=""Screen Shot 2020-11-16 at 13 48 37"" src=""https://user-images.githubusercontent.com/1929967/99277394-ab6ed300-2825-11eb-86f5-a5bf670839aa.png"">

<img width=""1385"" alt=""Screen Shot 2020-11-16 at 15 44 36"" src=""https://user-images.githubusercontent.com/1929967/99277292-8bd7aa80-2825-11eb-84fb-c85349525696.png"">





#### How to reproduce the bug

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

### Environment

(please complete the following information):

- superset version: `0.36.0`
- python version: `3.6.0`
- node.js version: `10-jessie`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,willbarrett,"
--
I fear that creating a system of data visualizations with no inline CSS will be impossible. We will take this under advisement as a possible future enhancement to the platform.
--
",,,,,,,,,,
11710,OPEN,Dashboard/Chart emails scheduled is not working,bug,2020-11-17 05:02:45 +0000 UTC,krishna9626,In progress,,"
### Expected results

The mail should be received once scheduled

### Actual results

While debugging, Succeeded status on log 

#### Screenshots


![issu1](https://user-images.githubusercontent.com/36255856/99245252-4cab5880-2829-11eb-8eef-98901f53d54f.PNG)
![issu3](https://user-images.githubusercontent.com/36255856/99245256-4ddc8580-2829-11eb-870d-eec3bc3e5a94.PNG)




#### How to reproduce the bug

1. Go to 'config.py' and follow the instruction for Email scheduling
2. Run the Celery worker and celery Beat command
3. Go to manage/email dashboard  then schedule
4. See error on celery worker terminal

### Environment

(please complete the following information):

- superset version: `0.37.2`
- python version: `python 3.6`


### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Using chrome as webdriver.

",,,junlincc,"
--
@benceorlai 
--
",dpgaspar,"
--
Hi @krishna9626, on the logs it seems you have `dry run enabled` can you confirm?
--
",krishna9626,"
--
@dpgaspar Yes it is enabled.
--
",,,,,,
11705,OPEN,[filter]Display label(value in different column) when filtering a specific column,enhancement:request,2021-02-16 06:05:12 +0000 UTC,duynguyenhoang,Opened,,"**Is your feature request related to a problem? Please describe.**

Currently selection box in FilterBox only filter by value from Column configuration. It would be nice to have label and value in configuration and in FilterBox render. Let assume we have merchant_name and id_merchant, we want to show merchant_name but when filter we filter by id_merchant.

**Describe the solution you'd like**


* This is my suggestion for configuration

![Screenshot_20201114_182431](https://user-images.githubusercontent.com/7106179/99146045-0b8b3b00-26a7-11eb-8ac7-64dad0dc2a8b.png)

* And from new configuration, we can change data in https://github.com/apache/incubator-superset/blob/master/superset/viz.py#L2003 and https://github.com/apache/incubator-superset/blob/master/superset/viz.py#L2009.


",,,Papipo,"
--
I really need this!
--
",junlincc,"
--
@simcha90 @amitmiran137
I think we can add this feature to native dashboard filter config modal -> display filtered value in another column when filtering a specific column 
Can we commit to this enhancement? 
--
",,,,,,,,
11696,OPEN,Scatterplot deck.gl; header of point size popup does not change when field is changed,bug,2021-01-02 18:16:50 +0000 UTC,eugeniamz,Opened,,"When changing the point size field of a scatterplot deck.gl, the header of the popup menu does not get changed what can create confusion. 
### Expected results
When changing a field, the header should be updated accordingly
### Actual results
When changing a field the header does not change, but the query is constructed correctly. 

#### Screenshots

![2020-11-13_08-26-34 (1)](https://user-images.githubusercontent.com/58375897/99077478-ce318b00-258a-11eb-8c40-3fbd44cd55a2.gif)

### Environment

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x ] I have reproduced the issue with at least the latest released version of superset.
- [x ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.93. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",junlincc,"
--
to add on it, the dropdown is all messed up in the point size field. 
I can't think about a good candidate to work on these issues now, maybe it's time to open up tasks around data viz to the rest of team, since half of the charts have all sorts of different issues. @mistercrunch 
<img width=""333"" alt=""Screen Shot 2020-11-13 at 8 04 28 AM"" src=""https://user-images.githubusercontent.com/67837651/99093091-e1daf280-2586-11eb-80d7-2d83e9c6e528.png"">

--
",,,,,,,,
11694,OPEN,Loading custom 'Viz' plugin,viz:dynamic-plugins,2021-01-03 05:06:47 +0000 UTC,robinbakkerdemcon,In progress,,"Hello,

Recently I have started setting up Apache Superset and to archieve what I am looking for (a custom table view) I want to make a plugin. I have followed this tutorial (maybe 4 times) https://preset.io/blog/2020-07-02-hello-world/

When I follow this tutorial and write `npm run dev-server`  it will start the dev-server at: http://localhost:9000. It perfectly loads my plugin, but the problem is that I work from home and do everything through Putty on another laptop within the company network. Now, since I work from home I want to test my plugin, how can I archieve to run the dev-server on IP '0.0.0.0'?

Normally to run my superset server I write `superset run -h 0.0.0.0 -p 8088` this perfectly loads up superset and I can access it at home but this does not load the hello-world plugin!

**What I already tried**
1. I created a new plugin from scratch following the same steps
2. I tried running `npm run dev-server --host 0.0.0.0`

So I see two possibilities:

1. Can I load my plugin when running `superset run -h 0.0.0.0 -p 8088`
2. Can I access the dev server from home? 

",,,robinbakkerdemcon,"
--
Further I would like to mention that this issue is nearly the same as issue #10433 . Running `npm run prod` also throws the following error:
![image](https://user-images.githubusercontent.com/74405562/99068040-10b78f80-25ac-11eb-9f0b-24bd4b6fc780.png)

I have tried every solution from this issue (copying it manually instead of npm link, deleting node_modules etc.).
--

--
Hello and thanks for the response. I have edited the `webpack.config.js` and edited the following line:

```
const PREAMBLE = [path.join(APP_DIR, '/src/preamble.ts')];
if (isDevMode) {
  PREAMBLE.unshift(
    `webpack-dev-server/client?http://172.17.6.165:${devserverPort}`,
     // I've kept the dev server port the same. Only changed the IP!
  );
}
```

But when I run `sudo npm run dev-server` it will listen on http://localhost:9000 again. Is there also a tutorial for building my plugin as module and then importing it from (a locally hosted??) NPM server to superset?

--

--
@rusackas 
Alright so I have been busy with this problem again. Now, I have installed a desktop environment on the machine rnning Superset and through VNC I can access the dev server that is running locally, I verify this because the browser tab shows [DEV]. I have tried to use the plugin now and the command `npm run dev-server` correctly shows

![image](https://user-images.githubusercontent.com/74405562/99406081-c86ed900-28ed-11eb-8713-c9faf796b18f.png)


I assumed that I would work, but it doesn't work. It does not show up in Visualizations whatever I do.

**My package.json**
![image](https://user-images.githubusercontent.com/74405562/99405531-2c44d200-28ed-11eb-9f00-bbd412549194.png)

**My MainPreset.js**
Imports
![image](https://user-images.githubusercontent.com/74405562/99405649-48487380-28ed-11eb-9f7c-6b9aa1ca63b5.png)

Configure
![image](https://user-images.githubusercontent.com/74405562/99405872-86459780-28ed-11eb-9b65-464becfdb9c5.png)



**Suspected problem**

When I run the `npm run dev-server` at the end I get these errors:
![image](https://user-images.githubusercontent.com/74405562/99402948-35806f80-28ea-11eb-8427-0295b1a979bb.png)



--

--
Update, I have reinstalled everything, even Ubuntu server 20.04. Reinstalled Superset through this tutorial
https://medium.com/@kharekartik/a-better-guide-to-building-apache-superset-from-source-2c8dbad38b2b

Created a plugin from scratch using the preset.io hello world tutorial, and still no result. Is there anyone who is able to help my with this issue? I want to use Superset but I cannot find a solution to this particular problem. @rusackas  @mistercrunch 

Currently the plugin does not load inside the dev-server environment. The symlink message does appear but no plugin in my visualization list (Charts -> create new chart). When I want to run `yarn build` inside the superset-ui folder I get a message that 'build is no file or directory' maybe this is related to the issue?
--

--
> @robinbakkerdemcon if you want to use yarn build first you'll need to use yarn install, and maybe you'll also need npm install. I'm running in the same problem than you... Are you using docker environment?

Yes I have done that, yarn install and npm install but sadly I am running into the same issue as before. I am not running in a docker environment. I have installed all python dependencies inside a Virtual Environment as recommended. I am sorry that you have the same problem, I tried reinstalling everything (even Ubuntu Server) and installing everything from scratch again using the guides provided by Apache Superset but it still won't work.

For now I have given up on trying everything I can, but if someone is able to help me (and now you) with this problem I still want to try and get this working.
--
",mistercrunch,"
--
@rusackas 
--
",rusackas,"
--
Hmmm... I haven't personally run into this, so I don't have a good answer without doing some investigation, but I'm glad the plugin process itself is working!

My _suspicion_ is that you'll have to dig into `incubator-superset`'s `webpack.config.js` to move away from `localhost` and port `9000`.

The alternative, if you have superset running for you in other circumstances, would be to go further towards production by actually `build`ing the plugin as a module, publish it on NPM, and import the package in your instance of Superset. 
--

--
@returnzer0 the most common path for that would be to publish the package on NPM (including the built assets) from your own repo, so that you can then import the NPM module as a dependency in your Superset implementation.
--
",avicenteg,"
--
@robinbakkerdemcon if you want to use yarn build first you'll need to use yarn install, and maybe you'll also need npm install. I'm running in the same problem than you... Are you using docker environment?
--

--
> I'm running into the same issue when adding custom plugins, I'm using docker.
> 
> When I add a new custom visualization plugin using [this](https://superset.apache.org/docs/installation/building-custom-viz-plugins) tutorial and run docker-compose, I get this error
> 
> ```
> superset_node            | npm ERR! code E404
> superset_node            | npm ERR! 404 Not Found - GET https://registry.npmjs.org/@superset-ui%2fplugin-chart-hello-world - Not found
> superset_node            | npm ERR! 404 
> superset_node            | npm ERR! 404  '@superset-ui/plugin-chart-hello-world@^0.0.0' is not in the npm registry.
> superset_node            | npm ERR! 404 You should bug the author to publish it (or use the name yourself!)
> superset_node            | npm ERR! 404 It was specified as a dependency of 'superset-frontend'
> superset_node            | npm ERR! 404 
> superset_node            | npm ERR! 404 Note that you can also install from a
> superset_node            | npm ERR! 404 tarball, folder, http url, or git url.
> superset_node            | 
> superset_node            | npm ERR! A complete log of this run can be found in:
> superset_node            | npm ERR!     /root/.npm/_logs/2020-11-30T21_19_48_140Z-debug.log
> ```
> 
> I can run the dev server using `npm run dev-server` inside the superset-frontend folder and it gets served via localhost:9000, but docker doesn't seem to play that well, the frontend at localhost:8088 and logo keeps spinning.

Hello @returnzer0 I reach a solution with docker. I'm using a old plugin (JS instead of TS) but I think it should work. 
1. I created a folder in the superset-frontend where I put my new plugins (p.e: superset-ui/plugins).
2. I create manually an entrance in the package.json, linking the version of the plugin with a file. 
`""@superset-ui/legacy-plugin-chart-mychart"": ""file:superset-ui/plugins/legacy-plugin-chart-mychart""`
3. I run npm install in superset-fronted in order to update the package-lock.json file. After the process is complete, delete the node-modules folder and its content.
4. In the Dockerfile, I added: 
`COPY ./superset-frontend/superset-ui /app/superset-frontend/superset-ui`
This command should be before this other command.
```
RUN /frontend-mem-nag.sh \
	&& cd /app/superset-frontend \
        && npm ci
```
5. In the docker-compose I created a volume (this volume was present in old versions of superset). 

```
x-superset-volumes: &superset-volumes
  - ./docker/docker-init.sh:/app/docker-init.sh
  - ./docker/pythonpath_dev:/app/pythonpath
  - ./superset:/app/superset
  - ./superset-frontend:/app/superset-frontend
  - node_modules:/app/superset-frontend/node_modules  <- this line was changed
  - superset_home:/app/superset_home
```

```
volumes:
  superset_home:
    external: false
  db_home:
    external: false
  node_modules:   <- this line was changed
    external: false
  redis:
    external: false
```

6. In the docker-compose I use install without the --f option for the superset-node.
```
  superset-node:
    image: node:12
    container_name: superset_node
    command: [""bash"", ""-c"", ""cd /app/superset-frontend && npm install --global webpack webpack-cli && npm install && npm run dev""]
    env_file: docker/.env
    depends_on: *superset-depends-on
    volumes: *superset-volumes
```
7. I docker-compose up. 
8. Probably, when you do that, you won't have any error, but the new plugin will be still missing. In this case, goes to the folder of your plugin and check if there are a node_modules folder. If it is in there, delete it. 
9. If you haven't still, shut down the superset instance, and change the superset-node command in docker-compose, deleting the npm install option. 
```
  superset-node:
    image: node:12
    container_name: superset_node
    command: [""bash"", ""-c"", ""cd /app/superset-frontend && npm install --global webpack webpack-cli && npm run dev""]
    env_file: docker/.env
    depends_on: *superset-depends-on
    volumes: *superset-volumes
```
10. docker-compose up again, and it should be working, at least for me.

The steps from 1-8 should be repeated when you add a new plugin, but if you are doing some changes in your plugin you can change the files in the plugin and the changes will be reflected in your instance when you restart it.

I hope this works for you. I know how frustating is it, I spend 2 whole weeks to understand it.
--
",returnzer0,"
--
I'm running into the same issue when adding custom plugins, I'm using docker.

When I add a new custom visualization plugin using [this](https://superset.apache.org/docs/installation/building-custom-viz-plugins) tutorial and run docker-compose, I get this error 

```
superset_node            | npm ERR! code E404
superset_node            | npm ERR! 404 Not Found - GET https://registry.npmjs.org/@superset-ui%2fplugin-chart-hello-world - Not found
superset_node            | npm ERR! 404 
superset_node            | npm ERR! 404  '@superset-ui/plugin-chart-hello-world@^0.0.0' is not in the npm registry.
superset_node            | npm ERR! 404 You should bug the author to publish it (or use the name yourself!)
superset_node            | npm ERR! 404 It was specified as a dependency of 'superset-frontend'
superset_node            | npm ERR! 404 
superset_node            | npm ERR! 404 Note that you can also install from a
superset_node            | npm ERR! 404 tarball, folder, http url, or git url.
superset_node            | 
superset_node            | npm ERR! A complete log of this run can be found in:
superset_node            | npm ERR!     /root/.npm/_logs/2020-11-30T21_19_48_140Z-debug.log

```

I can run the dev server using `npm run dev-server` inside the superset-frontend folder and it gets served via localhost:9000, but docker doesn't seem to play that well, the frontend at localhost:8088 and logo keeps spinning.
--

--
@nytai  Thanks for the reply, once I'm finishing developing the plugin and want to show it in prod, is that possible? Or are custom plugins limited to dev only?
--

--
@rusackas Thank you, I have another question in hopes of not swaying away from the original issue.

 If I don't really care about publishing the custom viz on npm, but I simply want to create a docker-compose build which I can then host anywhere, privately, is there any way to go about it? I think that my concern and @robinbakkerdemcon are quite similar. (please correct me if they aren't)

Currently, I'm just getting npm linking errors, and following  #10433 doesn't get me anywhere. `docker-compose up` still throws `module not found` errors. I think the problem exists in the `docker-entrypoint.sh` script, where it installs the npm packages.
--

--
Thank you, everyone! It was indeed an issue with npm package linking (or lack of).

I tried what @avicenteg outlined and I am able to run it with `docker-compose up`.

I would add that, in step 6, I didn't change anything. Instead, I went into `docker/docker-frontend.sh` bash file to change the commands, but after running it once, I realized that if I added any new plugins, I'd need to bring down the volumes as well, so that npm can install packages, which is something I want to avoid, just in case. So I keep the `-f and --no-optional` flags in the bash script so that new packages are installed regardless, every time I run `docker-compose up`.


@robinbakkerdemcon hope this also shines some light on your issue, I am able to run it via docker and expose a port on my gateway and can access it from anywhere and all of the plugins are loading just fine for me.

Edit: I wanted to add that this method works for new files (typescript), and for both class and functional react components.
--

--
Did you try adding it via the `file:...` directive? I don't have any experience with publishing the package, but try to see if you can first include it locally.
--
",nytai,"
--
for developing a custom plugin, you'll want to use the `dev-server`. Docker is running in prod mode, so it will not handle any npm liking magic
--

--
You'd have to publish the plugin as an npm package and then add it to superset. I'm not sure if we'll be merging community plugins into upstream master yet, but you can always maintain your fork with a commit that adds the custom viz plugin. Of course that means building your own custom prod version of superset from source, but you should have all the tools necessary. 

We have a dynamic plugin marketplace type feature in the works, but not sure when that will be coming. 
--

--
You could try building the viz plugin and installing it using the `file:...` reference instead of linking, mentioned here https://stackoverflow.com/a/17371987/13977852
--

--
actually, I bet the issue with docker (via npm link) is that the linked plugin isn't in the docker file system. I'd try moving the plugin into `superset-frontend` (which should be volume mounted) and try pointing to it using the `file:...` reference (which should be easier to debug than `npm link`)
--
"
11689,OPEN,[list view]jumping list view when click on sort icons,bug:cosmetic,2020-12-03 01:45:19 +0000 UTC,junlincc,In progress,,"![ezgif-3-2c5e487ae82a](https://user-images.githubusercontent.com/67837651/99033897-c8a75700-2530-11eb-9ddf-9cf625e04433.gif)

it happens on all columns, hope that's not intentional
cc: @nytai ",,,nytai,"
--
This is mainly due to us using the `auto` table layout that has some variation based on the content of each column. The only solution that comes to mind would be to switch over to using a `fixed` table layout with column widths based on percent values. We'd have to make quite a few changes and figure out what percent values to assign to every column in each of the list views we've already implemented. Layout will also be affected, to what degree, I'm not entirely sure right now. 
--
",benceorlai,"
--
Added to the roadmap in the 1.0 Hardening & Polish backlog

https://github.com/apache-superset/superset-roadmap/projects/1#card-49550172
--
",junlincc,"
--
does dataset column have sort icon originally? @nytai 
--
",,,,,,
11680,OPEN,Owner names not rendered in React Datasource editor,assigned:nielsen; bug,2021-04-07 17:34:33 +0000 UTC,john-bodley,Opened,,"
### Expected results

The dataset owners should be rendered.

### Actual results

<img width=""392"" alt=""Screen Shot 2020-11-12 at 2 37 42 PM"" src=""https://user-images.githubusercontent.com/4567245/99004984-d2ab6480-24f4-11eb-97c9-ec36900ea5cd.png"">

#### How to reproduce the bug

1. Go to a chart.
2. Click on 'Edit Datasource'
3. Click on the 'SETTINGS' tab
4. Scroll to the bottom and and noticed that the owner names aren't rendered

### Environment

(please complete the following information):

- superset version: `master`
- python version: `3.7`
- node.js version: `12`

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,john,"
--
@rusackas I believe this regression may be related to a recent change from Preset related to the Antd refactor. 
--

--
@junlincc unsure. We believe this regression may have been introduced in one of [these changes](https://github.com/airbnb/incubator-superset/compare/release--0.92.2%E2%80%A6airbnb:release--0.93).
--

--
@junlincc I'm going to see if the issue is present in our internal weekly deploy which occurs on Wednesday. If it's resolved it's likely our last week branch cut did not contain the commit and I'll close the issue, otherwise I'll dig into this in more detail. 
--

--
@junlincc and @rusackas I'm able to reproduce this from `master` using our production data. It seems like the issue is the fetched set of potential owners is quite restricted (the CRUD view fetches all users) and the prescribed owners are not within said set and thus the labels aren't rendered. My hunch is you may not be able to repo it as the corpus of users is defined entirely within the null state response. 

The logic is defined [here](https://github.com/apache/incubator-superset/blob/55a3404b71232a1cfd6d978f666945cde37c99e2/superset-frontend/src/datasource/DatasourceEditor.jsx#L544-L552) and I only see one request to the `api/v1/dataset/related/owners` endpoint regardless of the typeahead state.
--

--
@dpgaspar do you know of any differences between the `/api/v1/dataset/related/owners` and `/users/api/read` API endpoints?
--
",rusackas,"
--
@kgabryje 
--

--
Thanks John, will investigate.
--

--
> @junlincc unsure. We believe this regression may have been introduced in one of [these changes](https://github.com/airbnb/incubator-superset/compare/release--0.92.2%E2%80%A6airbnb:release--0.93).

I'm not able to repro this either yet. I would try a little `git bisect` action, but I need a failing state to do so. :/
--
",junlincc,"
--
<img width=""871"" alt=""Screen Shot 2020-11-12 at 2 59 49 PM"" src=""https://user-images.githubusercontent.com/67837651/99006755-cffe3e80-24f7-11eb-8e35-7c68896bb13e.png"">
@john-bodley hey John, im not able to reproduce it, do you have an idea which PR might cause it and when did you first notice it? 
--

--
@john-bodley could you help narrow it down a bit  

> > @junlincc unsure. We believe this regression may have been introduced in one of [these changes](https://github.com/airbnb/incubator-superset/compare/release--0.92.2%E2%80%A6airbnb:release--0.93).
> 
> I'm not able to repro this either yet. I would try a little `git bisect` action, but I need a failing state to do so. :/


--

--
@john-bodley thanks for the additional info we are looking into it! 
--
",graceguo,"
--
I found another JS bug for owners selector (besides the issue John just reported). this can be easily reproduced in open source master branch: 
1. if the owners list is empty, add one or more names in the owners list.
1. then remove all owners one by one. on removal the last one from owners list, you will see js error.
--
",kgabryje,"
--
@junlincc Can you assign me please?

--

--
@john-bodley I think that this issue might have been introduced by https://github.com/apache/incubator-superset/pull/11221. I'm not able to verify it as I can't directly reproduce this bug with my test data. Is it possible that the new endpoint `api/v1/dataset/related/owners` returns fewer owners than the old `/users/api/read` and your data actually uses the missing users? Could you please verify it?
CC @lilykuang @junlincc 
--

--
@nytai I think your guess is correct - I could reproduce John's bug by limiting a page size.
I wonder if paginated endpoint has any value for us in this case? We always need to load all options to make sure that we can display all selected values. Unless we implemented some logic ""keep loading next pages until all saved values have a matching option"" - though it doesn't sound like this optimization is worth the effort.
--
",nytai,"
--
One difference is that `/api/v1/dataset/related/owners` is paginated (with `pageSize` and `pageIndex` params) while I'm guessing `/users/api/read` is not paginated. My guess is that airbnb is hitting this issue because they have >25 users which is the default page size. The `/api/v1/dataset/related/owners` also supports searching via `filter` param. 


My guess is that because the existing owners is not in the list of select options it's resulting in this weird rendering issue. We may want to consider just switching back to `/users/api/read` or trying to use the [pagianted select component](https://github.com/apache/incubator-superset/blob/fbe4a6622e0affd6568ba0d5c429dc6bec18f626/superset-frontend/src/components/Select/SupersetStyledSelect.tsx#L295) which may fix this issue 
--

--
@kgabryje That's a great question. I have spend quite a bit of time working around this (adding `PaginatedSelect`) and have often questioned the value of paginating these payloads. @dpgaspar do you have any thoughts around removing pagination for these endpoints? 
--

--
We could also send the request with `page_size=2000` or something large enough to solve this issue for most practical cases. 
--
"
11677,OPEN,[dashboard properties] metadata saving behavior on SAVE,bug; viz:dashboard:properties,2021-01-02 18:37:43 +0000 UTC,junlincc,Opened,,"![ezgif-3-d5c8ef83fcf9](https://user-images.githubusercontent.com/67837651/98999044-edc5a680-24eb-11eb-9ec5-3826b3ddcfbe.gif)

Actual Behavior:
To see changes made in JSON metadata in Dashboard Properties - Advanced, user needs to click `SAVE`in the modal, and `SAVE` in Dashboard, AND refresh... 

Expected Behavior 
like edits in other Basic Information fields, when users edit json metadata in Dashboard Properties - Advanced, they should be able to see changes by first `SAVE` after exiting the modal


![ezgif-3-a758b432e0f9](https://user-images.githubusercontent.com/67837651/99000384-0a62de00-24ee-11eb-8e40-21c0d70b5365.gif)
",,,,,,,,,,,,,,
11674,OPEN,[dashboard properties] multiple issues around dashboard properties color labels,bug; enhancement:committed; viz:dashboard:properties,2021-01-02 18:39:31 +0000 UTC,junlincc,In progress,,"<img width=""1633"" alt=""Screen Shot 2020-11-12 at 1 02 41 PM"" src=""https://user-images.githubusercontent.com/67837651/98997228-097b7d80-24e9-11eb-8460-a5897a7a7059.png"">

This line chart only has 5 lines^^
When exploring a chart from the dashboard, to preserve the colors in the dashboard all label colors are persisted in the chart metadata, causing lots of unnecessary color metadata in the payload

Expected behavior: all non-relevant labels should be dropped",,,mistercrunch,"
--
+1, let's rip this out. I think the intent was to offer color consistency while going from dashboards to explore, but it wasn't implemented the right way and creates a bunch of metadata in the wrong place.
--

--
If the feature request is to have color consistency through the dashboard->explore flow, it should be implemented by passing a dashboard_id_for_context to explore, and explore can get the dashboard metadata it needs to guarantee that.

I think color consistency on that flow is nice, but not a huge priority given the complexity behind doing this nicely. 

Long term solution may be around the SPA (single page app) where explore may be a huge modal on top of the dashboard in some cases, and just have that context available in Redux.
--
",zuzana,"
--
I was just about to report related bug, which I think it covered in above comments (if needed let me know and I can open separate issue).
- User can select color scheme on chart. (This works as expected.)
- user can select color scheme on dashboard. (This works as expected.)
- If both color schemes are selected, dashboard color scheme takes priority over chart color scheme. (This works as expected.)
- When a user clicks on Explore chart on Dashboard and got into Chart Explore, same colors on that specific chart which user saw on dashboard should be passed to chart Explore. This used to work in the past as per @graceguo-supercat  but is **currently broken - Bug.**
--
",junlincc,"
--
> * When a user clicks on Explore chart on Dashboard and got into Chart Explore, same colors on that specific chart which user saw on dashboard should be passed to chart Explore. This used to work in the past as per @graceguo-supercat  but is **currently broken - Bug.**

Yea i am aware of this bug this entire area needs to be worked on. i am gonna change the issue title to more generic to cover all these issues. thanks for reporting as always! 
--
",,,,,,
11669,OPEN,"New Box Plot ignore ""Series Limit"" when selecting ""Distribute Across""",bug; bug:cosmetic; viz:chart-boxplot,2021-01-02 18:21:15 +0000 UTC,eugeniamz,Opened,,"Testing new echart Box Plot I notice that when selecting the Distribute Across, it ignores the ""Series Limit"" 

### Expected results

If I select ""Series Limit"", I should always that number of records 
### Actual results
When I select Distribute Across, Series limit is ignored and shows all the data points

#### Screenshots

![2020-11-12_10-54-21 (1)](https://user-images.githubusercontent.com/58375897/98972655-bac9e580-24e0-11eb-953f-bda0d75914ac.gif)

#### How to reproduce the bug

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

### Environment

Latest Master
### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ X] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,villebro,"
--
Thanks @eugeniamz , I know what the problem is and there's a few solutions, but I need to digest it a little more to find the optimal one.
--

--
@junlincc: https://github.com/apache-superset/superset-ui/pull/834
--
",junlincc,"
--
<img width=""887"" alt=""Screen Shot 2020-11-12 at 11 04 11 AM"" src=""https://user-images.githubusercontent.com/67837651/98986298-90c0f500-24d9-11eb-865a-a3452f99a68e.png"">
<img width=""896"" alt=""Screen Shot 2020-11-12 at 11 06 26 AM"" src=""https://user-images.githubusercontent.com/67837651/98986311-97e80300-24d9-11eb-8681-f27f511a2602.png"">


@villebro also, can we show all x axis labels by default, like the old chart does? 
--

--
> @junlincc: [apache-superset/superset-ui#834](https://github.com/apache-superset/superset-ui/pull/834)


let's keep rotate 45 by default for now we can explore 'smarter' solutions post 1.0. 
--
",,,,,,,,
11666,OPEN,UK Country Map chart not visible (extremly small in the screen),bug,2020-11-17 09:59:32 +0000 UTC,zsellami,Opened,,"I added a country map UK with UK data but the map is really very small. Any explanation about it ? also the isle of man tooltip text are present when i hoover the cursor on the border of the map.

With other country and other data of country the map are visible.

My superset version is : 0.37.2
Web browser : Chrome, Firefox

### Expected results

what you expected to happen.

### Actual results

what actually happens.

#### Screenshots
![UK_Country_Map](https://user-images.githubusercontent.com/22955406/98969173-e1067a00-250e-11eb-84aa-312c76760296.PNG)

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

### Environment

(please complete the following information):

- superset version: `0.37.2`
- python version: `3.6`
- node.js version: `node -v` No node installed

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,zufolo441,"
--
Usually it happens when the map has a very far overseasy territory. Search for some island near the edges. Maybe could be Gilbratar.
--
",,,,,,,,,,
11662,OPEN,Chart can be added to Dashboard by Non-admin,enhancement:request,2021-01-02 01:58:35 +0000 UTC,kamalk-github,Opened,,"If a user can access some dashboard(via access to underlying datasource(s)) where the user is not owner/admin, the user can create a new chart and add it to the dashboard without admin rights.

Follow below steps to reproduce:
1. A user who is not owner/admin of a dashboard can view it(via access to underlying datasource(s)).
2. Create a new chart from available datasources.
3. Edit the chart properties, and add the dashboard name to ""Dashboards"" section of ""Edit Chart"" screen.
4. New chart created by user is added to dashboard and can be seen on Dashboard by everyone.

This should not be allowed since the user is not owner/admin of Dashboard.

Superset version: 0.36.0",,,zuzana,"
--
I couldn't see issue you are talking about in current behavior (on master). Here are the scenarios and results.

1. If you are dashboard owner - you can add chart from chart explore to dashboard directly. This works.
2. If you are NOT dashboard owner but are CREATOR of the dashboard (for example I created a dashboard, and then I asked an admin to remove me as owner, but I was still the creator of that dashboard) - I could still edit the dashboard (when on dashboard) and add charts, but I wasn't able to see this dashboard in the dropdown on chart explore, so I actually couldn't add chart directly from chart explore. --> assuming creator always keeps edit rights, this potentially might be a small bug, that creator can't add from chart explore directly (but simple workaround to add form dashboard)
3. And if I was not owner or creator, I couldn't edit dashboard and couldn't add chart into dashboard from chart explore. This is expected.

Unrelated thing I noticed, is that user (not admin) who is dashboard owner is not able remove themselves from dashboard owners (even if there are additional owners). This could be a bug.
--

--
Another point is: Should the creator of dashboard have edit rights forever? (e.g. just because you create the dashboard, should you forever have edit right even though admin removes you as one of the owners)? (probably pretty corner case, but could happen if someone creates many dashboard and change team and still have access to all those dashboards, and admin can't do anything about it)
--
",junlincc,"
--
> Unrelated thing I noticed, is that user (not admin) who is dashboard owner is not able remove themselves from dashboard owners (even if there are additional owners). This could be a bug.

agreed that dashboard owner should be able to remove anyone, including themselves from dashboard owners. a strong case here is when there are personnel changes in the enterprise. 

cc. @benceorlai 
--
",nytai,"
--
Wouldn't an admin usually handle personnel changes? 
--
",,,,,,
11658,OPEN,Numeric columns in pivot table when using postgresql,bug,2021-04-08 20:46:56 +0000 UTC,kakoni,In progress,,"Commit https://github.com/apache/incubator-superset/commit/fc28c92f57edd7bccac57715bef159c0f18daef1 introduced new [get_aggfunc](https://github.com/apache/incubator-superset/blob/master/superset/viz.py#L875-L879) method to pick right aggregation function (especially when working with non-numeric columns)

Unfortunately this method fails when using postgres/psycopg and columns that are numeric. Psycopg maps psql numeric  into python `decimal` type and pandas/Numpy doesnt seem to work that well with it (example;  `dtype` returns `object` for `decimal`).

Main culprit here is  `pd.api.types.is_numeric_dtype` which obviously returns `False` for `decimal`. 

So perhaps as a fix I propose something like 
```
pd.api.types.is_numeric_dtype(df[metric]) or all(isinstance(x, decimal.Decimal) for x in df[metric])
```

#### How to reproduce the bug
1) Create simple table in postgres;
```sql
CREATE TABLE test (a text, b numeric);
INSERT into test VALUES ('a', 1)
INSERT into test VALEUS ('b', 2)
```
2) Add test as datasource
3) Create new pivot table chart
Use sum(b) as metrics, group by A

### Expected results

<img width=""125"" alt=""image"" src=""https://user-images.githubusercontent.com/812150/99070470-f1732f00-25b8-11eb-9362-d949d8fc3c78.png"">

### Actual results

<img width=""118"" alt=""image"" src=""https://user-images.githubusercontent.com/812150/99070401-d2749d00-25b8-11eb-93a9-dfd4bd4a4d91.png"">

### Environment
- superset version: from 0.37 to master
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.71. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",kakoni,"
--
Ping @villebro Any comments?
--

--
Perhaps you don't even need do this is_numeric_dtype check? (I mean now the function is bit of a black box when it changes aggfunction into max) 
--
",villebro,"
--
Thanks @kakoni for reporting and sorry for missing the first ping. Let me try to repro and come up with a fix.
--

--
@kakoni there was some very clever reasoning behind it, but I can't remember what it was   It would be really helpful if you could summarize your own expectations of how the summary row should handle non-numeric data, and I'll try to use that to write some unit tests to develop against.
--
",pavlozt,"
--
I reproduce this with latest git master branch, mysql mysql  10.3.27-MariaDB
with simple sql:
create table piv_table( i int(11) primary key auto_increment, dim_a int(11),dim_b int(11), val int(11));
insert into piv_table  (dim_a,dim_b,val) values (10,10,1000),(10,10,1000),(10,20,1000),(20,20,1000);

doing pivot with dim_a , dim_b, sum()

![](https://user-images.githubusercontent.com/55258742/113623182-adb3bb80-9666-11eb-8d1c-9959f006cea8.png)


add some debug code to get_aggfunc():

logger.info(""df.dtypes:"",df.dtypes)

it outputs :

Message: 'df.dtypes:'
Arguments: (dim_a        int64
dim_b        int64
SUM(val)    object
dtype: object,)


Whats wrong with this code ? where did the object data type come from ?
of course, in clean pandas code no object types with this query.
--

--
@villebro what is the reason for this happening here for the type object ?
Recent commits still not fix this. Calculated sum column returned to dataframe with dtype = object.
maybe a lot of errors are associated with this
--
",,,,
11657,OPEN,[SIP-15] time_range_endpoints not working on database extra parameters,bug,2021-01-05 21:09:30 +0000 UTC,rubenSastre,In progress,,"With SIP_15 the default time_range_endpoints = [start, end)
There is an option to set you own option on the database extra parameter.
The value for time_range_endpoints on database extra parameter is not working when you create a new chart.


### Expected results
While creating a new Chart it should get the value for extra param on the database configuration instead of ""time_range_endpoints"":[""inclusive"",""exclusive""]

### Actual results

Although there is a value set on de database param as ""time_range_endpoints"":[""inclusive"",""inclusive""] is taking  ""time_range_endpoints"":[""inclusive"",""exclusive""]
#### Screenshots

![image](https://user-images.githubusercontent.com/25669722/98826661-46d30300-2436-11eb-9980-8abb56923b56.png)
![image](https://user-images.githubusercontent.com/25669722/98826742-610ce100-2436-11eb-8a5e-03728585082f.png)

#### How to reproduce the bug
1. Set  ""time_range_endpoints"":[""inclusive"",""inclusive""]  on the database extra param
2. Create a new chart
3. Check on the form_data the value for time_range_endpoints

### Environment

- superset version: 0.37.2
- python version: 3.7
- node.js version: `node -v`
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.94. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",villebro,"
--
Thanks for reporting @rubenSastre , I'll take a look
--
",rubenSastre,"
--
 Any idea? i tried to look on the code but with my level I am not able to solve this part...
--

--
https://github.com/apache/incubator-superset/pull/11998
--
",,,,,,
11641,OPEN,How to fix 404 error while Emailing Dashboards in Superset,bug,2020-11-10 13:48:49 +0000 UTC,i4vijay,Opened,,"I am getting the following error while enabling email dashboards in superset.
![image](https://user-images.githubusercontent.com/36433544/98682038-48a6a480-2318-11eb-83f8-717f538009e7.png)

Can someone pls help me fix this...",,,,,,,,,,,,,,
11639,OPEN,Wrong SQL generated when filter is used,bug,2020-11-10 10:13:44 +0000 UTC,igor-tk,Opened,,"I have a data table with a field that contains data with double quoted text at the end: 
```
some text ""another text""
```
The last double quote is dropped in generated SQL when I use this field in a filter.

### Actual results
```
WHERE org_name IN ('some text ""another text')
```

### Expected results
```
WHERE org_name IN ('some text ""another text""')
```

#### How to reproduce the bug

1. Create dataset with data as above
2. Create chart
3. Create filter
4. Create dashboard
5. Use filter in dashboard
6. See chart is empty.
7. Explore chart. View chart SQL code in SQL Lab 

### Environment

- DB - PostgreSQL 10
- superset version: `0.37.2`
- python version: `3.6`


",,,,,,,,,,,,,,
11633,OPEN,[dashboard] prevent co-edit error in Dashboard Properties Modal,enhancement:request,2020-11-10 01:15:53 +0000 UTC,graceguo-supercat,Opened,,"**Is your feature request related to a problem? Please describe.**
There are a few PRs (and bug fixes) to prevent users edit the same dashboard at the same time: #11220 #11305 and #11614. Right now this feature is kind of stable, but it is **_not available_** when user update dashboard metadata in Dashboard Properties modal. Because `/dashboard/api/v1` does not check dashboard version in update requests.


**Describe the solution you'd like**
Is possible that `/dashboard/api/v1` PUT request also support dashboard version check, and return last_updated_time in response?

**Additional context**
maybe @dpgaspar can help? 
cc @mistercrunch @eschutho @junlincc ",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.84. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",,,,,,,,,,
11625,OPEN,Please provide documentation for deckgl charts,enhancement:request,2021-01-10 20:28:44 +0000 UTC,songololo,In progress,,"It is difficult to find any meaningful information about the deckgl charts.

Would it please be possible to add at least rudimentary documentation providing a brief overview of the accepted geospatial formats (e.g. GeoJSON) and any requirements such as coordinate reference system (e.g. epsg 4326?), geometry types (e.g. do Multipolygons work or only Polygons?) etc.

Thanks.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.90. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",junlincc,"
--
@srinify 
--
",mistercrunch,"
--
There's a bit of info here https://docs.preset.io/docs/scatterplot
--

--
Yes, I can confirm that we're expecting a geo json object as text. We could do more magic to understand the Postgres/postGIS geoJSON type and convert it accordingly, but it gets tricky for us to support special types that are database specific.
--
",songololo,"
--
Thank you. Can anyone give some clues as to what the polygon chart expects in terms of geometry? (e.g. whether geojson polygon in epsg 4326 coordinate reference system works?)
--

--
@srinify @mistercrunch @junlincc

I'm able to get it working for point data (e.g. GeoJSON in EPSG 4326 CRS) but not for Polygon data:

Here are some examples that should be fairly easy to reproduce. These make use of the SQL editor and I'm using the amancevice docker container (v0.37.2).

Running a query to create a rectangular polygon (using PostGIS):
`SELECT 1 as mock_data, ST_MakeEnvelope(-0.127677, 51.507515, -0.122161, 51.510021, 4326) as geom;`

Gives a jagged polygon (instead of rectangular):
<img width=""1585"" alt=""Screen Shot 2020-11-10 at 11 40 31"" src=""https://user-images.githubusercontent.com/4347878/98671432-26c71500-234c-11eb-946c-ee6811b121ed.png"">

Similarly, when casting the geometry to Well Known Text format:
`SELECT 1 as mock_data, ST_AsText(ST_MakeEnvelope(-0.127677, 51.507515, -0.122161, 51.510021, 4326)) as geom;`

<img width=""1589"" alt=""Screen Shot 2020-11-10 at 11 46 36"" src=""https://user-images.githubusercontent.com/4347878/98671479-3cd4d580-234c-11eb-82d8-21590a117656.png"">

And with GeoJSON it displays nothing:
`SELECT 1 as mock_data, ST_AsGeoJSON(ST_MakeEnvelope(-0.127677, 51.507515, -0.122161, 51.510021, 4326)) as geom;`
<img width=""1585"" alt=""Screen Shot 2020-11-10 at 11 49 50"" src=""https://user-images.githubusercontent.com/4347878/98671551-5aa23a80-234c-11eb-838c-41b7420c4c04.png"">

Any information or advice on how to display a polygon would be much appreciated.

Thanks.
--

--
I managed to make some headway, posting here for others:

The JSON strings returned from `postGIS` using the `ST_AsGeoJSON` function contain only the geometry component of the geoJSON spec. This means that it is necessary to wrap these geometries in a full-fledged geoJSON object before `deck.gl` can recognise it.

For example:
```postgresql
SELECT *, json_build_object(
  'type',       'Polygon',
  'geometry',   ST_AsGeoJSON(ST_Transform(geom, 4326))::json)::text as geoJSON
FROM <my-schema>.<my-table>;
```

If you are working with MultiPolygons then you need to extract a Polygon first:
```postgresql
SELECT *, json_build_object(
  'type',       'Polygon',
  'geometry',   ST_AsGeoJSON(ST_Transform((ST_DUMP(geom)).geom::geometry(Polygon, 27700), 4326))::json)::text as geoJSON
FROM <my-schema>.<my-table>;
```

Note that in both cases, the geoJSON result has to be cast to a text type otherwise superset will have issues when it runs internal queries, which are otherwise unable to sort the results (duplicate checking?) if using JSON.
--
",sazary,"
--
@songololo I can't tell you how much you've helped me with this! i have been trying for some time, even asked some questions here and in stackoverflow, and no answer... until I saw you solution, and it worked! Thanks dude
--
",,
11623,OPEN,[Explore]Title change made in Edit Chart Properties is not showing on the title but actually is saved,assigned:polidea; bug,2020-11-09 18:57:11 +0000 UTC,junlincc,Opened,,"![ezgif-7-c7fec0f7bdea](https://user-images.githubusercontent.com/67837651/98583126-b9967f80-2278-11eb-955e-3d6de2f13e51.gif)

Expected behavior: title change should reflect in Title as soon as the change is made and saved in  modal ",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.85. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",,,,,,,,,,
11622,OPEN,Make CRUD view URLs more user-friendly,enhancement:request,2020-11-25 05:39:00 +0000 UTC,ktmud,Opened,,"## Screenshot

<img src=""https://user-images.githubusercontent.com/335541/98577919-7ab0fb80-2271-11eb-8020-b1ca291a7a74.png"" width=""500"" />

## Description

As a professional software, the URLs should be intuitive and predictable. Currently most CRUD view pages have automatically-generated URL pathnames which looks kind of hacky. We should change them to something more user-friendly, let them follow the Superset nomenclature instead the internal Python model/view names.

```
Datasets: /tablemodelview/ -> /datasets
Databases: /databaseview/ -> /databases
```

## Design input
[describe any input/collaboration you'd like from designers, and
tag accordingly. For design review, add the
label `design:review`. If this includes a design proposal,
include the label `design:suggest`]
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",nytai,"
--
This has been bothering me too. There's some legacy here, as you've pointed out regarding 
> internal Python model/view names

Listing out what would be required to make this happen:
- Set up redirects for the old routes (in case users have bookmarked these urls)
- set up backend routing for these new routes 
- set up react-router routes for the new routes

* There's likely something else I'm missing.

We should wait until the react crud views are complete and we can fully deprecate the old FAB views since I don't think there's a way to change the routes for the old FAB views. 
--
",,,,,,,,
11612,OPEN,Error logging in to superset (sqlalchemy.exc.ProgrammingError),bug,2020-12-23 19:12:05 +0000 UTC,mjdhasan,Opened,,"A clear and concise description of what the bug is.

### Expected results

login panel

### Actual results

sqlalchemy.exc.ProgrammingError
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation ""ab_permission_view_role"" does not exist
LINE 2: FROM ab_permission_view JOIN ab_permission_view_role ON ab_p...
                                     ^

[SQL: SELECT ab_permission_view.id AS ab_permission_view_id, ab_permission_view.permission_id AS ab_permission_view_permission_id, ab_permission_view.view_menu_id AS ab_permission_view_view_menu_id 
FROM ab_permission_view JOIN ab_permission_view_role ON ab_permission_view.id = ab_permission_view_role.permission_view_id JOIN ab_role ON ab_role.id = ab_permission_view_role.role_id JOIN ab_permission ON ab_permission.id = ab_permission_view.permission_id JOIN ab_view_menu ON ab_view_menu.id = ab_permission_view.view_menu_id 
WHERE ab_permission.name = %(name_1)s AND ab_role.id IN (%(id_1)s)]
[parameters: {'name_1': 'menu_access', 'id_1': 2}]
(Background on this error at: http://sqlalche.me/e/13/f405)

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug
Following the instructions on https://preset.io/blog/2020-05-11-getting-started-installing-superset/, 
1.  git clone https://github.com/apache/incubator-superset.git
2. cd incubator-superset
3. docker-compose up
but unable to get to the l
4.open: http://localhost:8088

**In the docker-compose up output, I see an error related to ab_permission, pasted below**
superset_init            |     sqlalchemy_exception, with_traceback=exc_info[2], from_=e
superset_init            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/util/compat.py"", line 182, in raise_
superset_init            |     raise exception
superset_init            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py"", line 1277, in _execute_context
superset_init            |     cursor, statement, parameters, context
superset_init            |   File ""/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute
superset_init            |     cursor.execute(statement, parameters)
superset_init            | sqlalchemy.exc.InternalError: (psycopg2.errors.InFailedSqlTransaction) current transaction is aborted, commands ignored until end of transaction block
superset_init            | 
superset_init            | [SQL: SELECT ab_permission.id AS ab_permission_id, ab_permission.name AS ab_permission_name 
superset_init            | FROM ab_permission 
superset_init            | WHERE ab_permission.name = %(name_1)s]
superset_init            | [parameters: {'name_1': 'all_datasource_access'}]
superset_init            | (Background on this error at: http://sqlalche.me/e/13/2j85)
superset_app             | 127.0.0.1 - - [07/Nov/2020 08:34:58] ""GET /health HTTP/1.1"" 200 -
superset_app             | INFO:werkzeug:127.0.0.1 - - [07/Nov/2020 08:34:58] ""GET /health HTTP/1.1"" 200 -
superset_init exited with code 1

### Environment

(please complete the following information):

- python version: Python 3.6.3

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.89. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",GiannisDimitriou,"
--
I have exactly the same error. 

Table 'ab_permission_view_role' is missing but the db migrations run successfully.
Superset version: 0.37.2
@villebro can you help on this one?
--
",IronOnet,"
--
Im also facing the same issue on windows, im really waiting for help, i can't run my superset instance!!
--
",jerryo,"
--
I share the following in hopes that my struggle will help someone else.   After battling with this issue for over an hour, I was  able to get past the error described in this issue by doing the following:
1. In a terminal window run `docker-compose up`
2. Once running, open the browser and observe the error described in this issue.
3. In a separate terminal window run `docker-compose down -v --remove-orphans`
4. Wait for both docker-compose processes to terminate.
5. In the original terminal window run `docker-compose pull` and then `docker-compose up --force-recreate --build`

At this point, I could access the login screen at `http://localhost:8088`, but I could not login with the admin user.  I shut down the instance (I don't recall how, most likely `docker-compose down` or `docker-compose down -v --remove-orphans`) and called it a day.  

Today, running `docker-compose up` creates an instance that I can access with admin/admin. 

`$git status `reports
```
On branch master
Your branch is up to date with 'origin/master'.

nothing to commit, working tree clean
```
I hope this helps. 

--
",,,,
11605,OPEN,Deck.gl: set up the arc width based on a data field,enhancement:request,2020-11-06 10:47:59 +0000 UTC,S-eb,Opened,,"**This is a request for a new feature**
I would like to build data viz using Deck.gl Arc where the width of the arc would depend on a numerical data field (a good example would be to have the width depending on the traffic between two points) 

**Describe the solution you'd like**
Today is it possible to change the color of the arc based on categorical value.
The same principle would be for adjusting the width of the arc based on a numerical value

Thanks!
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.82. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",,,,,,,,,,
11599,OPEN,[Explore]popover should follow the query field as user scrolls page,assigned:polidea; bug:cosmetic; enhancement:request,2020-11-06 22:17:46 +0000 UTC,junlincc,Opened,,"![ezgif-7-9cc7a29fa5ab](https://user-images.githubusercontent.com/67837651/98339569-be310e80-1fc0-11eb-9667-9df8208b6102.gif)
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.68. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",mistercrunch,"
--
That may be really hard - I'd suggest time boxing this, wouldn't spend more than a few hours on this.
--
",,,,,,,,
11589,OPEN,"SQL Lab: CTRL+T behaved differently from clicking ""+""",assigned:polidea; bug,2020-11-17 03:43:06 +0000 UTC,betodealmeida,Opened,,"There are two ways of adding a new tab in SQL Lab:

1. CTRL+T
2. Clicking the ""+"" button

The new tab is different depending on the method. The first one creates an empty query, the second one creates a dummy query `SELECT ...`

### Expected results

Behavior should be consistent.

### Actual results

New query is different depending on the method.

#### Screenshots

N/A

#### How to reproduce the bug

1. Go to SQL Lab
2. Click on ""+"" button, new query has `SELECT ...`
3. Press CTRL+T, new query is empty

### Environment

(please complete the following information):

- superset version: master
- python version: 3.8
- node.js version: v14.11.0

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

N/A",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.91. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",,,,,,,,,,
11586,OPEN,Improve selection of LIMIT in SQL Lab,enhancement:request,2020-11-10 18:05:52 +0000 UTC,betodealmeida,Opened,,"**Is your feature request related to a problem? Please describe.**

In SQL Lab, to change the current limit the user needs to:

1. Click the LIMIT button
2. Click the input field
3. Delete the current value
4. Type in the new value
5. Press OK

This is a lot.

**Describe the solution you'd like**

In SQL Lab, after clicking the ""LIMIT"" button the input element in the dialog that pops should be focused and the current value selected. Pressing enter should apply and close the dialog, equivalent to clicking OK.

1. Click LIMIT button
2. Type in the new value
3. Press ENTER

**Describe alternatives you've considered**

N/A

**Additional context**

N/A",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",mistercrunch,"
--
We have an internal design to improve the SQL Lab toolbar, and that includes improvement to the LIMIT control, it'd be great if we can share the design here in the open.
--
",Steejay,"
--
hey @betodealmeida ! We're exploring this new limit design shown in the screenshot here. I agree that the current limit interaction should be simplified. We also saw an opportunity to eliminate some of the annoying error states a user might encounter (empty text field and integer over max limit). We believe that this dropdown of pre defined limits can help assuage some of these pain points. If a user wants to define a custom limit they still have the ability to do so in SQL Editor. 


![Frame 213 (1)](https://user-images.githubusercontent.com/60786102/98712579-5588bf80-233b-11eb-948d-d5ad37b00908.jpg)

cc @yousoph @mistercrunch 
--
",,,,,,
11576,OPEN,Add a display /chart/data api sharing button to the chart visualization editor,enhancement:request,2020-11-10 11:05:42 +0000 UTC,trepmag,Opened,,"The API at `/swagger/v1` provides a navigation across web services which is great. But, especially for the `/chart/data` route it might be difficult to guess how to build its required `query context`.

To ease the usage of the `/chart/data` route and it's `query context` I'm proposing to expose its usage within the chart visualisation editor beside the other ""sharing"" buttons (link, iframe, json, etc, ...). See proposition screenshot bellow and core: trepmag/incubator-superset@0d25115b5aae54fc84db72898699e846caf1f771.

Having this way of building the query context directly in the swagger ui would be perhaps more logical but I'm quite unsure of the feasible the reproduce the versatility of what exists in superset's chart editor...

![image](https://user-images.githubusercontent.com/875021/98229077-bdb15d00-1f59-11eb-84a2-610691fe93fb.png)
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.93. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",villebro,"
--
This is a great idea! With JSON and CSV export buttons, the Explore view is getting slightly crowded. Before adding this it might make sense to put all export options under a single collapsible button to avoid cluttering the main view.

Ping @steejay , any ideas on how the export options could be cleaned up in Explore View?
--
",,,,,,,,
11560,OPEN,Sorting charts by dataset is broken,bug,2020-11-04 23:08:52 +0000 UTC,betodealmeida,Opened,,"When sorting charts by dataset the results are ordered by dataset ID instead of name.

### Expected results

Should sort by name.

### Actual results

Sorts by ID.

#### Screenshots

If applicable, add screenshots to help explain your problem.
<img width=""902"" alt=""Screen Shot 2020-11-04 at 2 57 00 PM"" src=""https://user-images.githubusercontent.com/1534870/98176751-03b0e700-1eae-11eb-916e-40b3a722b830.png"">


Note that `energy_usage` has ID 1, and `wb_health_population` has ID 2.

#### How to reproduce the bug

1. Go to 'Charts'
2. Click on the 'Dataset' column
3. Sort is by ID

### Environment

- superset version: master
- python version: 3.8
- node.js version: v14.11.0

### Checklist

Make sure to follow these steps before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

N/A",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.98. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",nytai,"
--
@betodealmeida this should be ""fixed"" in the latest master with https://github.com/apache/incubator-superset/pull/11370, we removed sorting on this column due to the non-standard/polymorphic nature of the dataset relation. 
--
",,,,,,,,
11552,OPEN,Generating simple email reports fills up RAM,bug,2020-11-10 10:54:12 +0000 UTC,JMGGarcia,In progress,,"Hello everyone! I'm having trouble generating email reports. Running on docker, I've set everything up with geckodriver. When I create a email schedule and request a test email (on a simple dashboard with just a markup card), I can see in the logs the worker requesting the dashboards, but then nothing seems to happen, the RAM fills up slowly, eventually I get a SoftTimeLimit exception, but the process keeps running until it runs out of RAM. 

I know Selenium can consume a lot of resources, but it seems quite a lot for such a simple task (important to note that at the beginning of the task I have more than 10Gb of free RAM). 

Additionally, I was able to run this whole setup on my PC without problems, but get this problem on the server. 

### Expected results

Generate simple email report. 

### Actual results

No email is sent, process fills up RAM until it runs out of it. 

#### How to reproduce the bug

1. Generate email schedule with a generic dashboard, request test email

### Environment

(please complete the following information):

- superset version: `0.37.2`
- python version: `3.6.9`
- node.js version: `10.15.3`

### Checklist

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,benceorlai,"
--
Hello @JMGGarcia 

Thanks for reporting this issue!

We are working on major changes in the Reports feature, targeting completion sometimes this calendar quarter (before mid-December). The changes include significant improvements to the backend of Reports, and I expect that it will make the backend more robust. Feel free to track [this item](https://github.com/apache-superset/superset-roadmap/projects/1#card-46699872) on the roadmap for more updates and/or I will be glad to share implementation details as needed. I am over here --> bence@preset.io
--
",JMGGarcia,"
--
Hey! That's great to hear, I will be looking forward to these developments, thanks! 
--
",,,,,,,,
11520,OPEN,Inconsistent Behavior for CRUD view modals,bug; bug:cosmetic; org:preset,2020-11-13 19:01:16 +0000 UTC,ktmud,In progress,,"## Screenshot

![dashboard-modal](https://user-images.githubusercontent.com/335541/97817105-22fe0900-1c4f-11eb-84f8-d41b03dd16b0.gif)

## Description

In the new React-based Dashboard & Chart CRUD view. The edit modal has animation on both opening and closing, but the edit modal has only opening animation.

Also, I think we should probably consider linking the edit button to opening `<dashboard_url?>edit=true` in a new window instead of opening the properties modal, because it's much more commonly needed to edit the dashboard layout/charts rather than just the things in properties modal.

## Design input
[describe any input/collaboration you'd like from designers, and
tag accordingly. For design review, add the
label `design:review`. If this includes a design proposal,
include the label `design:suggest`]
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.68. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",ktmud,"
--
Cc @nytai 
--

--
+1 for changing the edit button to open dashboard in edit mode.
--
",nytai,"
--
Let's have a conversation here (@junlincc @lilykuang @benceorlai @eschutho @mistercrunch ) about switching over the edit action on the dashboard list view. 

Current the edit action only open the dashboard properties modal. Should we just make the call that ""Editing a dashboard means opening the dashboard in edit mode and not opening the dashboard properties modal""? It seems that everyone I've talked to advocates for

> linking the edit button to opening <dashboard_url?>edit=true in a new window instead of opening the properties modal, because it's much more commonly needed to edit the dashboard layout/charts rather than just the things in properties modal.

I think this should clear up a number of bugs/cosmetic issues around the current dashboard edit experience. Does anyone feel differently? 
--

--
Looks like the portion about the strange saving behavior on the properties modal is covered by https://github.com/apache/incubator-superset/issues/11677
--
",mistercrunch,"
--
+1 for changing the edit button to open dashboard in edit mode

Also! In dashboard edit mode, consolidate the 2-3 other modals (filter mapping, colors, auto-refresh) under other tabs under ""Edit Dashboard Properties"", and make that an `Ok` dialog (not `Save`), affect the local redux state, and be kept there until the dashboard's `Save` is hit.
--
",junlincc,"
--
> +1 for changing the edit button to open dashboard in edit mode
> 
> Also! In dashboard edit mode, consolidate the 2-3 other modals (filter mapping, colors, auto-refresh) under other tabs under ""Edit Dashboard Properties"", and make that an `Ok` dialog (not `Save`), affect the local redux state, and be kept there until the dashboard's `Save` is hit.

Also Edit Chart Properties modal please~  @nytai 
--
",,
11513,OPEN,Connect to Google Sheets - SSL: CERTIFICATE_VERIFY_FAILED,change:backend; install:docker,2020-11-03 02:03:40 +0000 UTC,GonzaloGit2020,In progress,,"i try to connect and query a Google Sheets, i am using gsheetdb, i have followed the steps from this link: https://superset.apache.org/docs/databases/google-sheets

But i am getting some issues. Could you help me to know why i can't connect to Google Sheet?

### Expected results

Connect and get the information from Google Sheets

### Actual results

I get the next error message: 

gsheets error: HTTPSConnectionPool(host='docs.google.com', port=443): Max retries exceeded with url: google_sheet_URL  (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:852)'),))

- superset version: 0.37.2
- python version: Python 3.6.9 (SCL)

",,,issue,"
--
Issue Label Bot is not confident enough to auto-label this issue. See [dashboard](https://mlbot.net/data/apache/incubator-superset) for more details.
--
",dpgaspar,"
--
It seems strange that your superset server is not recognising google's certificate. Are you running locally? If yes take a look at: https://stackoverflow.com/questions/42098126/mac-osx-python-ssl-sslerror-ssl-certificate-verify-failed-certificate-verify

--
",GonzaloGit2020,"
--
> It seems strange that your superset server is not recognising google's certificate. Are you running locally? If yes take a look at: https://stackoverflow.com/questions/42098126/mac-osx-python-ssl-sslerror-ssl-certificate-verify-failed-certificate-verify

Yes, we are, we have a Linux server running Superset in the company where i work. I am not sure if the proxy's company is affecting the communication with google sheet, anyway let me look the link you shared.

Regards
--
",,,,,,
11489,OPEN,Filtering per Time Column doesn't work without hack,bug,2020-10-29 17:36:30 +0000 UTC,durchgedreht,Opened,,"I have a table chart with 5min granularity and I'm not able to filter per time column. Only Datetime column available is ""delivery_date"".

First strange behaviour is I can add the ""Date"" Column, but it automatically assigns a count() aggregate:

<img width=""511"" alt=""Screen Shot 2020-10-29 at 18 27 52"" src=""https://user-images.githubusercontent.com/4490840/97610265-f068cc00-1a14-11eb-8393-24f38004fc7b.png"">

When removing the count() aggregate I get a DB Error (which makes technically sense as the query does use __timestamp - see text below):
<img width=""876"" alt=""Screen Shot 2020-10-29 at 18 28 58"" src=""https://user-images.githubusercontent.com/4490840/97610404-1bebb680-1a15-11eb-8c4e-8931e2cff2eb.png"">

Looking into the query, the column ""delivery_date"" gets mapped to __timestamp internally. This should be selected by superset if I add the Date. Hacking it manually leads to the wanted result:

<img width=""784"" alt=""Screen Shot 2020-10-29 at 18 27 05"" src=""https://user-images.githubusercontent.com/4490840/97610551-56edea00-1a15-11eb-88e0-db0c43b9ceae.png"">

",,,,,,,,,,,,,,
11488,OPEN,Embed Tabs inside Rows and Columns,enhancement:request; viz:dashboard:tab,2021-01-02 18:38:47 +0000 UTC,durchgedreht,Opened,,"Then dealing with multiple tabs I sometimes need to have the same filter for all charts. Thus it would be great for the user to have the FilterBox sticky beside the tabs (you can just put it inside a single tab, above or below).

The solution from a user persepctive would be either:

1. Embed the complete filterbox in a single table element
2. Make a single Row/Column available (left or right) from the tab element 

Example of filter inside tab (selecting B will move filter out of viewable area):
<img width=""1163"" alt=""Screen Shot 2020-10-29 at 18 18 08"" src=""https://user-images.githubusercontent.com/4490840/97609127-826fd500-1a13-11eb-8e6d-150eba4bee1e.png"">

Goal: Move tabs in empty box (or other way round - optionally leave a space left from the tabs):
<img width=""1172"" alt=""Screen Shot 2020-10-29 at 18 20 25"" src=""https://user-images.githubusercontent.com/4490840/97609563-0c1fa280-1a14-11eb-963a-50bcacee19fb.png"">


",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",junlincc,"
--
Right now, filters are at the dashboard level despite the number of layers of tab in the dashboard. Meaning changing filter values in the filter box and chart filtering action could happen on different pages, which creates confusion for the dashboard consumer. The UX certainly has room for improvement. 
We will be implementing dashboard native filter soon and may not further invest in the current filter box solution by adding new features to it. Though all filters will still be placed at the global/dashboard level, hopefully the confusion will be eliminated by the new UI.  
Thanks for suggesting! 
--
",,,,,,,,
11478,OPEN,Editing Datasource edits the name globally for all charts; not locally for the given chart,enhancement:request; viz:explore:dataset,2021-01-02 18:45:01 +0000 UTC,zuzana-vej,Opened,,"Currently when user edits datasource name of the **Edit Dataset** modal on Chart Explore, it looks like this change is saved globally for all charts / the original datasource gets renamed. 

When user edits chart using Change Dataset option on Chart Explore, it only changes dataset locally for the chart.

### Expected results

Is this behavior expected? Changing the dataset globally for all charts / actually renaming a dataset (which impacts all charts using this dataset) seems problematic to do on chart explore, when a user who doesn't own the other charts can do this. It is really not clear to the user that it will change for all charts - especially because in the past, this only changed locally for the chart. 

### Actual results

Renames dataset, so it's easy to break all charts! 

#### Screenshots
I had two different chars using same dataset (broken). Wanted to change this for one of the charts. It got changed for both charts. (In this case it sounds great, but in most cases, it will unexpectedly break charts of other people).

Two different charts:
<img width=""602"" alt=""Screen Shot 2020-10-28 at 8 27 13 PM"" src=""https://user-images.githubusercontent.com/61221714/97522664-87724d00-195d-11eb-8f23-201d46802061.png"">
<img width=""600"" alt=""Screen Shot 2020-10-28 at 8 27 07 PM"" src=""https://user-images.githubusercontent.com/61221714/97522668-88a37a00-195d-11eb-8720-2f7b9a275da4.png"">

This modal changes dataset for both:
<img width=""965"" alt=""Screen Shot 2020-10-28 at 8 40 40 PM"" src=""https://user-images.githubusercontent.com/61221714/97522815-e0da7c00-195d-11eb-85f2-82c45d746ace.png"">


#### How to reproduce the bug

1. Go to Chart Explore
2. Click on Edit dataset and change dataset
3. See that all charts using the old dataset changed to new dataset

### Environment

(please complete the following information):

- superset version: `master`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context
Recent discussion: https://github.com/apache/incubator-superset/issues/11190
Recently closed bug: https://github.com/apache/incubator-superset/issues/11380
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.76. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",zuzana,"
--
Opened this as a potential BUG / DISCUSSION point. If this really is intended behavior - it is quite problematic if you have many users, and we might need to look into some other options how to prevent users from breaking other people's charts.
--

--
We think that the editing metric, calculated columns, etc. isn't THAT problematic. Once in a while, someone deletes some metric and someone else's charts break, but this is really rare. What we think is problematic is specifically the change dataset - the 1st tab - the fact that you can **change the table that backs a dataset**. And this changes for everyone. For physical datasets this is a very rare scenario, that a physical table gets actually renamed, and it shouldn't be so easy for user to do this change. In terms of virtual dataset - no problem, they are rarely shared among users (or at most shared within a single team). In the past this might have been the existing behavior, but it was in the last tab on that modal, so it wasn't so obvious. 


> One thing that could help would be to actually list out the list of charts (and perhaps associated owners) that will be affected.

 ---> agreed
--

--
I agree that the behavior is consistent across the entire modal - and there should be a consistency. 

We are just trying to surface that is is an issue for larger organizations. It is not a problem if you are the only person who owns charts using specific datasource, or if you don't have production dashboards shared across. As soon as you deploy Superset for a larger group of people where they share the underlying datasets this becomes a problem - one user making change not knowing that it might impact all other charts and other people's charts break (user is used to the calculated columns and metrics, but not to the first tab when currently they think it just changes for the specific chart). Users broke charts 2x since this was moved from last tab to 1st tab of that modal (and there might have been more cases not reported). 
--

--
Thanks for the notes @mistercrunch , @benceorlai , @graceguo-supercat . 

Based on @mistercrunch summary above:

> landing on Source tab is confusing, since the user is likely to want to edit metrics or calculated dimensions
> Source tab is ""most descriptive"" of the dataset, what it is and where it's pointing to
> spoke about maybe point to a different default Tab selected (say metrics)

.. can we agree to move Source tab before (or after) settings right now?
![Screen Shot 2020-11-02 at 9 05 18 AM](https://user-images.githubusercontent.com/61221714/97897101-dc5dec80-1cea-11eb-8d75-f6d634f7d491.png)


> Disabling the content of Source tab, showing may a lock icon that can be unlocked with an appropriate message
 
... is this part of the roadmap item as well?

> Eventually (more complicated, probably phase 2, if ever), we could categorize destructive and non-destructive changes and act accordingly. Say adding a metric could not show any warnings, but deleting one would show you warning. Pushing this idea we could even try to see if/which chart is using that metric.

...this is part of the roadmap item added by @benceorlai (once some of these better ideas are implemented the source tab can be moved back to be the 1st page if decided so)

--

--
Generally we don't want user to make the edits here. But I agree that this could differ across all users using Superset across many organizations (likely the smaller want to enable this, while the larger might want to disable this). I understand we need to have solution that works for everyone not just the larger groups. So I think

> purposefully creating friction in the flow so that we focus their attention to the impact.

will work.

While we might not be able to do the best case scenario right away (e.g. categorizing potentially destructive / non destructive scenarios), we would like to make some simple change soon so that we eliminate the impact. Either disabling it (temporarily), moving the tab (temporarily) or adding the lock (disabled) which can be unlocked after acknowledging warning. If you can keep us posted we can probably make the first small change right away (within a week or two).
--

--
Thanks @benceorlai  for sharing the proposed designs! I assume these will be used for both editing dataset, as well as in future, for when user is editing metrics, or calculated columns (which still impacts all datasets).

In the meanwhile, @graceguo-supercat has a solution aligned to the discussion from the meetup and some of above notes, specifically only for the data source tab:
_when user open source tab, by default all the table/schema name should be read-only, there will be a padlock, click it to enter edit mode_ 

The solution are complementary.
--

--
With option 2, I don't think the ""copy dataset"" should not be default. 

How exactly will the ""copy dataset"" behave? 
The dataset for virtual datasource --> create different virtual datasource (easy) 
But how about dataset which is a physical table? Each physical table should exist only once, it's a pointer / reference to a table in database which shares same name. If user wants to ""copy"" dataset to change the name of dataset used for the chart - what do they actually want to do? Do they just want to change it for the specific chart?
--

--
We aren't planning to change the source tab order. As a first step, @graceguo-supercat  has a draft PR to have the lock and warning message (without number of impacted charts - that can be enhancement). 

One thing for option 3 (listing all the charts) which might not work in all cases is if the number of charts is large. For some popular datasets there could be tens of hundreds of charts potentially. So having a count and option to click on that looks best. Additionally it could be good to highlight if those impacted charts are owned by other people, and having the message as ""... 5 other charts using this dataset, owned by 2 other owners"" or just generically ""... 5 other charts using this dataset, some of which are owned by another owner."" I think that could really add some importance to the message. If I am the only owner I might go ahead with the change but otherwise I might think twice. 
--

--
#11781 solves our immediate need and that's something we feel comfortable with, and I agree they are not exclusive, it's step in the right direction either way. We don't plan at this point to work on the next steps - but I am glad we were aligned this first step is in the right direction for everyone.

I get your point about user intents, and I think we should be very careful if we allow duplicating datasets, even though I see the point - that could be OK for small user base, but with 2500 WAUs and hundreds of data sets, this will cause clutter - many duplicate datasets, resulting in any migrations being possibly much harder, possibly scaling issues, users getting confused if each of their charts uses different dataset they duplicated earlier and forgot, and more. So would love to be part future discussions on this topic if this is to be considered. 
 
--
",junlincc,"
--
problematic indeed... I am seeing the same errors.  @lilykuang please prioritize it. 
When user changes dataset of one chart in Explore, it should only affect the local chart. Also, i agree that being able to change a dataset in **Edit Dataset** modal seems to be a confusing/unnecessary feature while user can actually complete the task by going to **Change Dataset**. 
--

--
I agree @nytai, allowing user to configure the underlying dataset while user is working on a specific chart in Explore does not make sense to me. This action should happens in **Datasets**. we should either remove this change dataset feature in **Edit Dataset**, or remove this modal from explore page entirely. 
--

--
We should have a clear definition of each module(Data, Explore, Dashboard, SQL lab)'s primary purposes. Shortcuts are great, but they don't necessarily live on the same page. as long as we have clear redirection between modules, then user should be able to stay in the context without getting confused. 

The problem i have been seeing in Superset is that user have multiple entry points to complete one task, in some cases, it is convenient, but most of time users get confused. I strongly suggest to remove this modal from Explore, and create a better flow between Datasets and Explore. @mistercrunch 
--

--
sounds good, that's a well defined problem to solve - create a better flow between Datasets and Explore  
--

--
that's helpful. what about dataset changes affecting other people's charts? setting permission maybe? 
--
",mistercrunch,"
--
Wait. This is totally the expected behavior, it's the ""Dataset Editor"" and there's not one but two alert/warnings clarifying this. Duplicating datasets so that each chart would have its own makes no sense to me, you'd have to define the configuraiton / metrics / calculated dimensions for each chart and wouldn't be able to reuse the work done there. 

<img width=""607"" alt=""Screen Shot 2020-10-28 at 10 08 09 PM"" src=""https://user-images.githubusercontent.com/487433/97528055-16856200-196a-11eb-876a-6b21d4e5f36b.png"">
<img width=""893"" alt=""Screen Shot 2020-10-28 at 10 08 02 PM"" src=""https://user-images.githubusercontent.com/487433/97528056-171df880-196a-11eb-87a3-6a5a11f2a442.png"">

One thing that could help would be to actually list out the list of charts (and perhaps associated owners) that will be affected.
--

--
This shortcut is super useful, you can add new metrics and calculated dimensions without loosing context. @eugeniamz can chime in as a power user.
--

--
I strongly suggest that we keep this modal until we create a better flow between Datasets and Explore
--

--
In the meantime, we know for sure that regardless of the point of entry (explore or dataset), listing the affected charts on save bellow that alert/warning would be helpful and hard really really hard to disregard/misunderstand. 
--

--
Things that can break charts:
- pointing to another table or schema
- changing or deleting a metric definition
- changing or deleting a calculated dimension
- altering the SQL
- changing settings (this shouldn't break a chart)
--

--
At the meetup, we clarified that:
- landing on `Source` tab is confusing, since the user is likely to want to edit metrics or calculated dimensions
- `Source` tab is ""most descriptive"" of the dataset, what it is and where it's pointing to
- spoke about maybe point to a different default Tab selected (say metrics)
- Disabling the content of `Source` tab, showing may a lock icon that can be unlocked with an appropriate message
- Eventually (more complicated, probably phase 2, if ever), we could categorize destructive and non-destructive changes and act accordingly. Say adding a metric could not show any warnings, but deleting one would show you warning. Pushing this idea we could even try to see if/which chart is using that metric.

In any case, it's pretty clear that showing the list of associated charts that will/may be affected by the change seems like a positive thing.
--

--
About option 1, I don't think it's a common need or flow we want to steer towards, plus I think currently it's not allowed to have 2 datasets pointing to the same table (though maybe it should be allowed, but that's another topic). Forking the dataset is not really desirable. Keep in mind that most changes (add a metric or add a calculated dimension) should not be breaking as well.

Option 3 seems alright, and originally was thinking it could show a small table of ""name, owners"", though now we're a bit in overthinking-it territory.

For the rare case where there are dozens/hundreds of charts, I think scrollbar is ok.
--
",nytai,"
--
We actually have an endpoint that responds with how many objects a dataset is connected to, used on the datasets page. 

I think the question here is really, should we remove this modal from the explore page and instead redirect to the datasets page? 
--
",willbarrett,"
--
For clarity, this task is not yet actionable. @lilykuang please do not work on this until we achieve clarity.
--
"
11475,OPEN,Follow up on ace library fix to fix Markdown with Emoji issue,enhancement:request,2020-10-29 08:56:32 +0000 UTC,zuzana-vej,Opened,,"As per conclusion in this bug https://github.com/apache/incubator-superset/issues/11331 this enhancement request is just a reminder to wait for a fix in the Ace Editor and validate it fully fixes the bug with Shifting cursor (forward) when editing Markdown with emoji. 
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.83. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",nytai,"
--
Have we opened an issue with https://github.com/ajaxorg/ace? 
--

--
@mistercrunch yes, it appears it's broken in their demo too (as per https://github.com/apache/incubator-superset/issues/11331) 
--
",zuzana,"
--
I haven't. @kkucharc have you / if not could you please open one since you discovered issue is with that?
--
",mistercrunch,"
--
Did we check that we're on the latest Ace, sometimes bumping to latest version addresses little bugs like this one?
--

--
Gotcha. I just looked and our `react-ace` is super behind though. It moved from being based on `brace` to something new `ace-builds`. 
--
",junlincc,"
--
@kkucharc asked in Ace editor issue about any news in that. Upgrading react-ace to 9.2.0 didnt help. 

added Markdown component to roadmap inbox 
https://github.com/apache-superset/superset-roadmap/projects/1
--
",kkucharc,"
--
I asked in this issue about any news in the topic: https://github.com/ajaxorg/ace/issues/4142
--
"
11470,OPEN,Surface slice ID,enhancement:request,2020-10-28 23:47:54 +0000 UTC,rubypollev,Opened,,"**Is your feature request related to a problem? Please describe.**
Difficult to identify slice ID. Needed for filtering audit log by slice ID. Strange to have a filtering option for data not surfaced readily to the user. Right now it's buried in the URL. 

**Describe the solution you'd like**
Slice ID somewhere useful: hamburger dropdown, properties, etc.  

**Describe alternatives you've considered**
N/A

**Additional context**
Add any other context or screenshots about the feature request here.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.87. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",,,,,,,,,,
11462,OPEN,Error when adding a SQLite DB does not include message,bug; change:frontend; org:preset,2020-11-17 17:19:05 +0000 UTC,earwig,Opened,,"The error when adding a SQLite DB shows ""Connection failed. [object Object]"" instead of the real error message.

### Expected results

I see the real error message: ""SQLite database cannot be used as a data source for security reasons.""

### Actual results

I see ""ERROR: Connection failed. [object Object]"".

#### Screenshots

<img width=""628"" alt=""superset-error-message"" src=""https://user-images.githubusercontent.com/637235/97478467-61ca5100-1927-11eb-937b-b7c91812bd4e.png"">

<img width=""1026"" alt=""superset-error-response"" src=""https://user-images.githubusercontent.com/637235/97478431-51b27180-1927-11eb-92e0-3d06ab90dbd3.png"">

#### How to reproduce the bug

1. Try to add a database
2. Enter a `sqlite://` URI
3. Try to add the database

### Environment

- superset version: current master (894b94a3451d8c6fb6f55ea8d3725f1ecec2e9c4)
- python version: 3.7.9
- node.js version: 12.19.0
- npm version: 6.14.8

### Checklist

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.93. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",nytai,"
--
I believe this was addressed in https://github.com/apache/incubator-superset/pull/11618 which also raised a few other issues
--
",,,,,,,,
11461,OPEN,Multi-line chart with uniform Y-scale,enhancement:request,2020-10-28 18:03:36 +0000 UTC,irwvanroosmalen,Opened,,"Currently when using a multi-line chart it can be hard to understand the relative scales of the lines I'm viewing. What I would like is the possibility to make the y-scale uniform for both lines (which is essentially the same as just having a single y-axis), and with that to be able to set the base of the scale to 0 or some other value for optimal viewability.

The alternative is for me to create a custom chart, but I feel that the current multi-line chart is so close to what I need that I'd prefer to see if you could implement a solution to this problem.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.69. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",,,,,,,,,,
11448,OPEN,Refactor Cypress tests to make less page visits,Turing; enhancement:committed,2021-02-16 17:13:54 +0000 UTC,ktmud,Opened,,"**Is your feature request related to a problem? Please describe.**

Currently the Cypress tests are slow because many of them make new page requests for each test case. 

**Describe the solution you'd like**

A lot of those are not really necessary and can be eliminated by combining similar tests on the same page to run consecutively. We should also add a section in CONTRIBUTING to describe how to write efficient E2E tests.

Making consecutive test cases on the same page also most closely simulate real user actions.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**

For example, a lot of these  `beforeEach` are not necessary:
<img src=""https://user-images.githubusercontent.com/335541/97370050-65989d80-186b-11eb-8b39-834f66f533ae.png"" width=""400"" />
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",junlincc,"
--
@rusackas on-boarding task for Bruno @yardz? 
--
",,,,,,,,
11443,OPEN,Label_colors not work on Scatterplot,bug,2021-02-03 13:26:41 +0000 UTC,fedecere94,Opened,,"I have a scatterplot chart in which I set the COLOR SCHEME to `D3 Category 10` and the CATEGORICAL COLOR is related to a column (named ""ovt"") that contains 2 possible values: ""overtarget"" and ""non overtarget"".
I set the variable **label_colors** in the JSON Metadata of my dashboard as following:
`""label_colors"": {
    ""overtarget"": ""red"",
    ""non overtarget"": ""green""
}` 
The problem is that the scatterplot in the dashboard is not affected by this rule (it takes the first two colors of the D3 Category 10 scheme: `#1f77b4` and `#ff7f0e`, ignoring my label_colors rule). But if choose I explore chart from dashboard in the new window the chart is affected by the rule and the colors being showed are `red` and `green` (in fact if I check the URL parameter form_data I see the label_colors rule with its fields)

How can I fix this problem?",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.76. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",antobenve,"
--
I confirm this issue on both the old version and the new 1.0.0 version.

--
",,,,,,,,
11441,OPEN,SQL init sentences per session as part of the Sources->Databases configuration,enhancement:request; inactive,2020-12-26 08:13:42 +0000 UTC,germanblanco,Opened,,"**Is your feature request related to a problem? Please describe.**
I have recently had the case of several queries originated in Superset running for around 45 hours in our database server. It seems to me that neither SQLLAB_ASYNC_TIME_LIMIT_SEC or the ""STOP"" button for asynchronous queries seem to be able to kill the query processing in the database server. I am using PostgreSQL (versions 9.6 and 12.1). In any case, I would rather enforce this timeout in the server using the statement_timeout PostgreSQL parameter. That seems to be a safer way to avoid this problems. I have searched for a way to configure Superset so that it runs a sort of ""init query"" in every session, but I haven't found one (I must also say I am no expert on Superset, so I apologize in advance if such thing is available or if there is another obvious solution to this).

**Describe the solution you'd like**
I would like to have ""session init queries"" as part of the configuration of the Source->Database so that Superset runs some sentences when starting each session (e.g. the update of the statement_timeout parameter in PostgreSQL).

**Describe alternatives you've considered**
If there is something that I might have done wrong when using the SQLLAB_ASYNC_TIME_LIMIT_SEC parameter, then please help me since that would be an alternative (although I still prefer enforcing the limit in the server).

**Additional context**
No more context for now.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.65. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
11436,OPEN,Arbitrary expressions for rolling windows,enhancement:request; viz:echarts,2021-03-31 05:36:28 +0000 UTC,asu04,Opened,,"The rolling windows functionality for line charts is nice - but it'd be nice to either have more aggregation options (max, mins, percentiles), or be allowed to compute arbitrary expressions.

Ideally this just expands on the current list for rolling windows by aggregations types above, and also having an option for specifying custom expressions - this would let us generate more views and charts on the same underlying data without having to rely on ETL to compute it.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.99. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",mistercrunch,"
--
The feature is based on pandas' `rolling` function https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html

It should be easy to expand it to support more options. Here's where it sits in the codebase https://github.com/apache/incubator-superset/blob/master/superset/utils/pandas_postprocessing.py#L307-L315
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",junlincc,"
--
We should include requested enhancement to Echarts time-series line. Will revisit when we get there

cc @zhaoyongjie 
--
",,,,
11406,OPEN,How to get filter box time grain in SQL query,inactive,2020-12-25 17:55:10 +0000 UTC,mavilar,Opened,,"I was trying to write an SQL query to build a chart that gets affected by the time filters of a filter box in a dashboard. More specifically, I would like to get the time grain selection.

I found this next issue that leads me to believe that it would be possible to get the time grain with the expression `filter_values('__time_grain')`, but I tried and it doesn't work. The expression `filter_values('__time_range')` doesn't work either.
[https://github.com/apache/incubator-superset/issues/8183](url)

After finding out, I went to the function's (`filter_value`) code and saw that there is a call to a function called `merge_extra_filters` that removes this time filter from `form_data`, and so the call `filter_values('__time_grain')` returns nothing. I also reviewed the Git history, and found that this behavior was introduced in a commit on 20 April 2020, so it looks like this should work before this commit. Is this the expected behavior for an expression like `filter_values('__time_grain')`? If so, how do I get the time grain from a filter box to use it in an SQL query?",,,issue,"
--
Issue Label Bot is not confident enough to auto-label this issue. See [dashboard](https://mlbot.net/data/apache/incubator-superset) for more details.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
11405,OPEN,Problem with Teradata Database,bug,2021-03-05 23:24:46 +0000 UTC,ogrehunter,In progress,,"From the Official Documentation it clearly stated that the recommended connector library is [sqlalchemy-teradata](https://github.com/Teradata/sqlalchemy-teradata). As far as I know this library is no longer maintained since last commit on its github page is on Jul 14, 2018. Furthermore Teradata has other SQL Driver Dialect for SQLAlchemy which is always updated until now, the last version is on Jun 16, 2020, that is [teradatasqlalchemy](https://pypi.org/project/teradatasqlalchemy/). 
There are some advantage with [teradatasqlalchemy](https://pypi.org/project/teradatasqlalchemy/). This library require no ODBC connector, and it patch up  [sqlalchemy-teradata](https://github.com/Teradata/sqlalchemy-teradata) on its inability to adding table data source from another schema.
Okay. I tried using [teradatasqlalchemy](https://pypi.org/project/teradatasqlalchemy/), running on SQL Lab, and this problem happen.

### Expected results

SQL Lab show table result as usual.

### Actual results

`base error: [Version 17.0.0.6] [Session 288395] [Teradata Database] [Error 3706] Syntax error: expected something between the word 'PENERIMAAN' and the 'LIMIT' keyword. at gosqldriver/teradatasql.`

#### Screenshots

![image](https://user-images.githubusercontent.com/29318048/96951788-749fe880-1517-11eb-856c-6d7f0cd4f18e.png)
The complete Error Message

#### How to reproduce the bug

1. Create Database Source using [teradatasqlalchemy](https://pypi.org/project/teradatasqlalchemy/) library connector.
2. Go to SQL Lab and try some queries.
3. Run.

### Environment

- superset version: `Superset 0.37.2`
- python version: `Python 3.8.5`
- node.js version: `node -v`
- npm version: `npm -v`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [v] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [v] I have reproduced the issue with at least the latest released version of superset.
- [v] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

I think this error raised because in Teradata, you don't do `LIMIT` to limiting the row result, but you use `SAMPLE` instead.

This is some additional log.

```
Triggering query_id: 84
INFO:superset.views.core:Triggering query_id: 84
/home/bram/pysuperset/lib/python3.8/site-packages/sqlalchemy/sql/sqltypes.py:661: SAWarning: Dialect sqlite+pysqlite does *not* support Decimal objects natively, and SQLAlchemy must convert from floating point - rounding errors and other issues may occur. Please consider storing Decimal numbers as strings or integers on this platform for lossless storage.
  util.warn(
timeout can't be used in the current context
WARNING:superset.utils.core:timeout can't be used in the current context
signal only works in main thread
Traceback (most recent call last):
  File ""/home/bram/pysuperset/lib/python3.8/site-packages/superset/utils/core.py"", line 624, in __enter__
    signal.signal(signal.SIGALRM, self.handle_timeout)
  File ""/usr/lib/python3.8/signal.py"", line 47, in signal
    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))
ValueError: signal only works in main thread
ERROR:superset.utils.core:signal only works in main thread
Traceback (most recent call last):
  File ""/home/bram/pysuperset/lib/python3.8/site-packages/superset/utils/core.py"", line 624, in __enter__
    signal.signal(signal.SIGALRM, self.handle_timeout)
  File ""/usr/lib/python3.8/signal.py"", line 47, in signal
    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))
ValueError: signal only works in main thread
INFO:werkzeug:127.0.0.1 - - [23/Oct/2020 10:14:22] ""GET /superset/queries/1603419084426.791 HTTP/1.1"" 200 -
SQLite Database support for metadata databases will be removed             in a future version of Superset.
WARNING:superset.sql_lab:SQLite Database support for metadata databases will be removed             in a future version of Superset.
Query 84: Executing 1 statement(s)
INFO:superset.sql_lab:Query 84: Executing 1 statement(s)
Query 84: Set query to 'running'
INFO:superset.sql_lab:Query 84: Set query to 'running'
Query 84: Running statement 1 out of 1
INFO:superset.sql_lab:Query 84: Running statement 1 out of 1
Query 84: <class 'teradatasql.OperationalError'>
ERROR:superset.sql_lab:Query 84: <class 'teradatasql.OperationalError'>
INFO:werkzeug:127.0.0.1 - - [23/Oct/2020 10:14:24] ""POST /superset/sql_json/ HTTP/1.1"" 500 -
```",,,issue,"
--
Issue Label Bot is not confident enough to auto-label this issue. See [dashboard](https://mlbot.net/data/apache/incubator-superset) for more details.
--
",mistercrunch,"
--
I don't know any committers/PMC who actively use Teradata. Please update our docs with your recommendation if you can!
--
",ogrehunter,"
--
Found temporary solution,
1. add keyword 'SAMPLE' in `venv\lib\python\site-packages\sqlparse\keyword.py`
```
KEYWORDS = {
        ...,
       'SAMPLE': tokens.Keyword,
        ...,
}
```
2. replace 'LIMIT' and 'limit' with 'SAMPLE' and 'sample' in `superset/sql_parse.py`

TODO: apply those replacement only when Teradata connection is used. But how?
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",cwiebe18,"
--
> This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

We are actively seeking this problem to be fixed.  This seems to be a ""chicken and egg"" problem...no Teradata User will use  Superset unless the limit problem is fixed.  @marcchernoff
--

--
@ogrehunter would you like to take a look at the above-mentioned SIP, as we think it will resolve this issue
@marcchernoff  @mccushjack

thank you @tdEmpl064
--
",,
11364,OPEN,Ability to launch SQL Lab with database; schema and SQL query pre-populated using URL params or JSON payload,enhancement:request,2021-02-12 08:12:07 +0000 UTC,apankha,Opened,,"**Is your feature request related to a problem? Please describe.**

We are using a Data Cataloging tool which allows our users to view available datasets in our data lake. We would like to integrate this tool with Superset. The functionality would allow the user to launch Superset inside SQL Lab view and query the table that they were viewing within the Data Cataloging tool.

**Describe the solution you'd like**

We would like to be able to pass parameters to 
https://ourhost:ourport/superset/sqllab

eg:

https://ourhost:ourport/superset/sqllab?sqlstring=blah

![image](https://user-images.githubusercontent.com/57229804/96748558-96875700-13c1-11eb-8299-5b8290bd4104.png)


**Describe alternatives you've considered**

We have had a look at https://github.com/apache/incubator-superset/blob/0.36.0/superset/views/sql_lab.py to see if we can define a separate to overwrite the query editor from the params. We've also looked at the static JS generated for the UI but couldn't see a clear way to do this.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.95. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",tooptoop4,"
--
cc: @yousoph @junlincc @Steejay
--
",zhaoyongjie,"
--
Hi @apankha,

Now Superset does not seem to generate SQLlab query through URL parameters. But you can use work around to do that.

```
SavedQueryRestApi.post                  POST       /api/v1/saved_query/
```
1. Create a saved query from the above API, then saved post response id to the `Data Catalog app`
2. Query this Saved query from `http://<hostname>:<port>/superset/sqllab?savedQueryId=<saved id>`

_Yongjie_
--
",,,,
11356,OPEN,SQL Statement on QUERY_LOGGER prints none to log,bug,2021-03-31 05:51:36 +0000 UTC,lily-liu,Opened,,"`log_query` function on sql_lab.py passes `query.executed_sql` instead of `query.sql`, thus causing it to print TypeNone on log. This issue only happened when QUERY_LOGGER property is configured.

### Expected results

actual SQL statement string from sql_lab printed on the log

### Actual results

TypeNone printed on logfile

#### Screenshots

<img width=""1440"" alt=""Screen Shot 2020-10-21 at 14 53 49"" src=""https://user-images.githubusercontent.com/23091708/96690692-34712680-13ae-11eb-9021-7c08e7aeff37.png"">


#### How to reproduce the bug

1. Set QUERY_LOGGER property on to function that will log the query (e.g. 
`def log_sql_statements(database, query, schema, user, client, security_manager, log_params):
    logger.info(""query is:"" + str(query))
QUERY_LOGGER = log_sql_statements`
2. run any query from sql_lab
3. check the log on terminal

### Environment

- superset version: `0.37.2`
- python version: `3.6.0`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Superset runs on Ubuntu 18.04 VM, Host machine is macOS 10.15.7
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
11328,OPEN,Enable to change currency format from dollars to euros,.pinned; enhancement:request; good first issue,2021-02-16 06:11:42 +0000 UTC,zsellami,Opened,,"Hello,
I want to show euros values on dashboard. But only the $ format are enable. When i changed the $ symbol to  i obtained an error.
How can I show euros value in a chart ?

I want something like ""y_axis_format"": ""$,.2f"" ==> to ""y_axis_format"": "",.2f""

Regards,
Zied Sellami",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.55. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",junlincc,"
--
Thank you, Zied for suggesting the enhancement. This feature is not currently on Superset 1.0 roadmap so we won't be able to get to it till early next year likely. However, this looks like a 'good first issue' for any community contributors to work on. If you are interested in contributing, I can point you to the right direction and connect you with you engineers. 
--

--
@carlosaccp, @sahilkamboj3 @dstadz sorry I missed your comments!!  If you all are still interested in contributing, feel free to reach out to me in Slack. you will need to add this y-axis format to multiple viz plug-ins. We can help you navigate if needed. thanks! 
--
",dstadz,"
--
Hey @junlincc, I am interested in contributing to this issue. Is it still available to be worked on? 
--
",willbarrett,"
--
Yup, go ahead and work on it!
--
",leocape,"
--
Also with specifying a currency it would be nice to have the option to remove decimals eg. `$,.2f` becomes  `$,.0f`
--
",carlosaccp,"
--
Is this issue still open? I would like to help!
--
"
11323,OPEN,v1/api/database/test_connection without uri,bug,2021-01-05 16:21:18 +0000 UTC,zzhuangqian,Opened,,"
There is no parameter of sqlalchemy_uri in the v1/api/database/test_connection documents",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.86. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",dpgaspar,"
--
Thank you for reporting this!
It does accept `sqlalchemy_uri` on the POST request schema, but the OpenAPI spec was not reflecting it, fixed it on #12274
--
",,,,,,
11322,OPEN,[Errno 99] Cannot assign requested address in superset_init for docker-compose,bug,2021-03-19 13:01:20 +0000 UTC,siben168,In progress,,"I can log in the superset but see nothing but a button ""close""(refer to attached screenshot).

I've used docker-compose with latest code to build superset, the only thing i've changed is in docker-compose.yml I used different port since the host already had an existing postgresql instance. 
` ports:
      - ""127.0.0.1:5433:5432""`

after run the docker-compose up, the only errors i can see was in superset_init step 4:
`superset_init            | urllib.error.URLError: <urlopen error [Errno 99] Cannot assign requested address>`


### Expected results

I want the home page functional.

### Actual results

only a ""close"" button after logged in.

#### Screenshots
![superset220201019132916](https://user-images.githubusercontent.com/10054371/96406576-4918b780-1212-11eb-878f-fd2326844e31.png)

#### How to reproduce the bug

1. git clone
2. run ""docker compose up""
3. access http://localhost:8088, login with default user admin with password admin
4. See error

### Environment

#### Superset version
```
git log
commit 31e4a90440885c5e0100408e65fedc33786240db (HEAD -> master, origin/master, origin/HEAD)
Author: Kamil Gabryjelski <kamil.gabryjelski@gmail.com>
Date:   Fri Oct 16 23:23:21 2020 +0200

    Fix overflowing tab's styling in TabbedSqlEditors (#11303)

commit 94e23bfc82613bbca9c6d4c55766836597907b3e
Author: Beto Dealmeida <roberto@dealmeida.net>
Date:   Fri Oct 16 11:10:39 2020 -0700

    feat: export databases as a ZIP bundle (#11229)
    * Export databases as Zip file
    * Fix tests
    * Address comments
    * Implement mulexport for database
    * Fix lint
    * Fix lint

commit 8863c939ad01fe1e065ce60edc93a683e6828d80
Author: Grace Guo <grace.guo@airbnb.com>
Date:   Fri Oct 16 10:54:35 2020 -0700

    fix: should update last_modified_time in client-side after save dash
```
#### Log from docker-compose:
```
sudo docker-compose up
Creating network ""incubator-superset_default"" with the default driver
Creating superset_cache ... done
Creating superset_db    ... done
Creating superset_init         ... done
Creating superset_app          ... done
Creating superset_worker       ... done
Creating superset_node         ... done
Creating superset_tests_worker ... done
Attaching to superset_cache, superset_db, superset_tests_worker, superset_init, superset_app, superset_worker, superset_node
superset_cache           | 1:C 19 Oct 05:32:59.656 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
superset_cache           |                 _._
superset_cache           |            _.-``__ ''-._
superset_cache           |       _.-``    `.  `_.  ''-._           Redis 3.2.12 (00000000/0) 64 bit
superset_cache           |   .-`` .-```.  ```\/    _.,_ ''-._
superset_cache           |  (    '      ,       .-`  | `,    )     Running in standalone mode
superset_cache           |  |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379
superset_cache           |  |    `-._   `._    /     _.-'    |     PID: 1
superset_cache           |   `-._    `-._  `-./  _.-'    _.-'
superset_cache           |  |`-._`-._    `-.__.-'    _.-'_.-'|
superset_cache           |  |    `-._`-._        _.-'_.-'    |           http://redis.io
superset_cache           |   `-._    `-._`-.__.-'_.-'    _.-'
superset_cache           |  |`-._`-._    `-.__.-'    _.-'_.-'|
superset_cache           |  |    `-._`-._        _.-'_.-'    |
superset_cache           |   `-._    `-._`-.__.-'_.-'    _.-'
superset_cache           |       `-._    `-.__.-'    _.-'
superset_cache           |           `-._        _.-'
superset_cache           |               `-.__.-'
superset_cache           |
superset_cache           | 1:M 19 Oct 05:32:59.659 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
superset_cache           | 1:M 19 Oct 05:32:59.659 # Server started, Redis version 3.2.12
superset_cache           | 1:M 19 Oct 05:32:59.659 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
superset_cache           | 1:M 19 Oct 05:32:59.659 * DB loaded from disk: 0.000 seconds
superset_cache           | 1:M 19 Oct 05:32:59.659 * The server is now ready to accept connections on port 6379
superset_db              |
superset_db              | PostgreSQL Database directory appears to contain a database; Skipping initialization
superset_db              |
superset_db              | 2020-10-19 05:33:02.644 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
superset_db              | 2020-10-19 05:33:02.644 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
superset_db              | 2020-10-19 05:33:02.926 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
superset_db              | 2020-10-19 05:33:03.347 UTC [25] LOG:  database system was shut down at 2020-10-19 04:13:54 UTC
superset_db              | 2020-10-19 05:33:03.670 UTC [1] LOG:  database system is ready to accept connections
superset_init            |
superset_init            | ######################################################################
superset_init            |
superset_init            |
superset_init            | Init Step 1/4 [Starting] -- Applying DB migrations
superset_init            |
superset_init            |
superset_init            | ######################################################################
superset_init            |
superset_node            | npm WARN using --force I sure hope you know what you are doing.
superset_app             |  * Serving Flask app ""superset.app:create_app()"" (lazy loading)
superset_app             |  * Environment: development
superset_app             |  * Debug mode: on
superset_app             |  * Running on http://0.0.0.0:8088/ (Press CTRL+C to quit)
superset_app             |  * Restarting with stat
superset_app             |  * Debugger is active!
superset_app             |  * Debugger PIN: 145-496-381
superset_tests_worker    | INFO:superset.utils.logging_configurator:logging was configured successfully
superset_tests_worker    | Loaded your LOCAL configuration at [/app/pythonpath/superset_config.py]
superset_tests_worker    | /usr/local/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
superset_tests_worker    |   ""Flask-Caching: CACHE_TYPE is set to null, ""
superset_tests_worker    | ERROR:flask_appbuilder.security.sqla.manager:DB Creation and initialization failed: (psycopg2.OperationalError) could not connect to server: Connection refused
superset_tests_worker    |      Is the server running on host ""localhost"" (::1) and accepting
superset_tests_worker    |      TCP/IP connections on port 5432?
superset_tests_worker    | FATAL:  no pg_hba.conf entry for host ""127.0.0.1"", user ""superset"", database ""test""
superset_tests_worker    |
superset_tests_worker    | (Background on this error at: http://sqlalche.me/e/13/e3q8)
superset_tests_worker exited with code 1
superset_worker          | INFO:superset.utils.logging_configurator:logging was configured successfully
superset_init            | INFO:superset.utils.logging_configurator:logging was configured successfully
superset_app             | INFO:superset.utils.logging_configurator:logging was configured successfully
superset_app             | INFO:superset.utils.logging_configurator:logging was configured successfully
superset_init            | /usr/local/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
superset_init            |   ""Flask-Caching: CACHE_TYPE is set to null, ""
superset_init            | INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
superset_init            | INFO  [alembic.runtime.migration] Will assume transactional DDL.
superset_init            | Loaded your LOCAL configuration at [/app/pythonpath/superset_config.py]
superset_worker          | /usr/local/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
superset_worker          |   ""Flask-Caching: CACHE_TYPE is set to null, ""
superset_worker          | [2020-10-19 05:33:45,149: INFO/MainProcess] Connected to redis://redis:6379/0
superset_worker          | [2020-10-19 05:33:45,175: INFO/MainProcess] mingle: searching for neighbors
superset_init            |
superset_init            | ######################################################################
superset_init            |
superset_init            |
superset_init            | Init Step 1/4 [Complete] -- Applying DB migrations
superset_init            |
superset_init            |
superset_init            | ######################################################################
superset_init            |
superset_init            |
superset_init            | ######################################################################
superset_init            |
superset_init            |
superset_init            | Init Step 2/4 [Starting] -- Setting up admin user ( admin / admin )
superset_init            |
superset_init            |
superset_init            | ######################################################################
superset_init            |
superset_app             | /usr/local/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
superset_app             |   ""Flask-Caching: CACHE_TYPE is set to null, ""
superset_app             | 127.0.0.1 - - [19/Oct/2020 05:33:46] ""GET /health HTTP/1.1"" 200 -
superset_app             | INFO:werkzeug:127.0.0.1 - - [19/Oct/2020 05:33:46] ""GET /health HTTP/1.1"" 200 -
superset_worker          | [2020-10-19 05:33:46,237: INFO/MainProcess] mingle: all alone
superset_worker          | [2020-10-19 05:33:46,303: INFO/MainProcess] celery@455da5275a13 ready.
superset_init            | INFO:superset.utils.logging_configurator:logging was configured successfully
superset_init            | Loaded your LOCAL configuration at [/app/pythonpath/superset_config.py]
superset_init            | Recognized Database Authentications.
superset_init            | Error! User already exists admin
superset_init            | /usr/local/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
superset_init            |   ""Flask-Caching: CACHE_TYPE is set to null, ""
superset_init            |
superset_init            | ######################################################################
superset_init            |
superset_init            |
superset_init            | Init Step 2/4 [Complete] -- Setting up admin user
superset_init            |
superset_init            |
superset_init            | ######################################################################
superset_init            |
superset_init            |
superset_init            | ######################################################################
superset_init            |
superset_init            |
superset_init            | Init Step 3/4 [Starting] -- Setting up roles and perms
superset_init            |
superset_init            |
superset_init            | ######################################################################
superset_init            |
superset_init            | INFO:superset.utils.logging_configurator:logging was configured successfully
superset_init            | /usr/local/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
superset_init            |   ""Flask-Caching: CACHE_TYPE is set to null, ""
superset_init            | INFO:superset.security.manager:Syncing role definition
superset_init            | INFO:superset.security.manager:Syncing Admin perms
superset_init            | INFO:superset.security.manager:Syncing Alpha perms
superset_init            | INFO:superset.security.manager:Syncing Gamma perms
superset_init            | INFO:superset.security.manager:Syncing granter perms
superset_init            | INFO:superset.security.manager:Syncing sql_lab perms
superset_init            | INFO:superset.security.manager:Fetching a set of all perms to lookup which ones are missing
superset_init            | INFO:superset.security.manager:Creating missing datasource permissions.
superset_init            | INFO:superset.security.manager:Creating missing database permissions.
superset_init            | INFO:superset.security.manager:Cleaning faulty perms
superset_init            | Loaded your LOCAL configuration at [/app/pythonpath/superset_config.py]
superset_init            |
superset_init            | ######################################################################
superset_init            |
superset_init            |
superset_init            | Init Step 3/4 [Complete] -- Setting up roles and perms
superset_init            |
superset_init            |
superset_init            | ######################################################################
superset_init            |
superset_init            |
superset_init            | ######################################################################
superset_init            |
superset_init            |
superset_init            | Init Step 4/4 [Starting] -- Loading examples
superset_init            |
superset_init            |
superset_init            | ######################################################################
superset_init            |
superset_init            | INFO:superset.utils.logging_configurator:logging was configured successfully
superset_app             | 127.0.0.1 - - [19/Oct/2020 05:34:16] ""GET /health HTTP/1.1"" 200 -
superset_app             | INFO:werkzeug:127.0.0.1 - - [19/Oct/2020 05:34:16] ""GET /health HTTP/1.1"" 200 -
superset_init            | /usr/local/lib/python3.7/site-packages/flask_caching/__init__.py:192: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
superset_init            |   ""Flask-Caching: CACHE_TYPE is set to null, ""
superset_init            | DEBUG:superset.models.core:Database.get_sqla_engine(). Masked URL: postgresql://superset:XXXXXXXXXX@db:5432/superset
superset_init            | Loaded your LOCAL configuration at [/app/pythonpath/superset_config.py]
superset_init            | Loading examples metadata and related data into examples
superset_init            | Creating default CSS templates
superset_init            | Loading energy related dataset
superset_init            | Traceback (most recent call last):
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 1350, in do_open
superset_init            |     encode_chunked=req.has_header('Transfer-encoding'))
superset_init            |   File ""/usr/local/lib/python3.7/http/client.py"", line 1277, in request
superset_init            |     self._send_request(method, url, body, headers, encode_chunked)
superset_init            |   File ""/usr/local/lib/python3.7/http/client.py"", line 1323, in _send_request
superset_init            |     self.endheaders(body, encode_chunked=encode_chunked)
superset_init            |   File ""/usr/local/lib/python3.7/http/client.py"", line 1272, in endheaders
superset_init            |     self._send_output(message_body, encode_chunked=encode_chunked)
superset_init            |   File ""/usr/local/lib/python3.7/http/client.py"", line 1032, in _send_output
superset_init            |     self.send(msg)
superset_init            |   File ""/usr/local/lib/python3.7/http/client.py"", line 972, in send
superset_init            |     self.connect()
superset_init            |   File ""/usr/local/lib/python3.7/http/client.py"", line 1439, in connect
superset_init            |     super().connect()
superset_init            |   File ""/usr/local/lib/python3.7/http/client.py"", line 944, in connect
superset_init            |     (self.host,self.port), self.timeout, self.source_address)
superset_init            |   File ""/usr/local/lib/python3.7/socket.py"", line 728, in create_connection
superset_init            |     raise err
superset_init            |   File ""/usr/local/lib/python3.7/socket.py"", line 716, in create_connection
superset_init            |     sock.connect(sa)
superset_init            | OSError: [Errno 99] Cannot assign requested address
superset_init            |
superset_init            | During handling of the above exception, another exception occurred:
superset_init            |
superset_init            | Traceback (most recent call last):
superset_init            |   File ""/usr/local/bin/superset"", line 33, in <module>
superset_init            |     sys.exit(load_entry_point('apache-superset', 'console_scripts', 'superset')())
superset_init            |   File ""/usr/local/lib/python3.7/site-packages/click/core.py"", line 829, in __call__
superset_init            |     return self.main(*args, **kwargs)
superset_init            |   File ""/usr/local/lib/python3.7/site-packages/flask/cli.py"", line 586, in main
superset_init            |     return super(FlaskGroup, self).main(*args, **kwargs)
superset_init            |   File ""/usr/local/lib/python3.7/site-packages/click/core.py"", line 782, in main
superset_init            |     rv = self.invoke(ctx)
superset_init            |   File ""/usr/local/lib/python3.7/site-packages/click/core.py"", line 1259, in invoke
superset_init            |     return _process_result(sub_ctx.command.invoke(sub_ctx))
superset_init            |   File ""/usr/local/lib/python3.7/site-packages/click/core.py"", line 1066, in invoke
superset_init            |     return ctx.invoke(self.callback, **ctx.params)
superset_init            |   File ""/usr/local/lib/python3.7/site-packages/click/core.py"", line 610, in invoke
superset_init            |     return callback(*args, **kwargs)
superset_init            |   File ""/usr/local/lib/python3.7/site-packages/click/decorators.py"", line 21, in new_func
superset_init            |     return f(get_current_context(), *args, **kwargs)
superset_init            |   File ""/usr/local/lib/python3.7/site-packages/flask/cli.py"", line 426, in decorator
superset_init            |     return __ctx.invoke(f, *args, **kwargs)
superset_init            |   File ""/usr/local/lib/python3.7/site-packages/click/core.py"", line 610, in invoke
superset_init            |     return callback(*args, **kwargs)
superset_init            |   File ""/app/superset/cli.py"", line 172, in load_examples
superset_init            |     load_examples_run(load_test_data, only_metadata, force)
superset_init            |   File ""/app/superset/cli.py"", line 110, in load_examples_run
superset_init            |     examples.load_energy(only_metadata, force)
superset_init            |   File ""/app/superset/examples/energy.py"", line 41, in load_energy
superset_init            |     data = get_example_data(""energy.json.gz"")
superset_init            |   File ""/app/superset/examples/helpers.py"", line 73, in get_example_data
superset_init            |     content = request.urlopen(f""{BASE_URL}{filepath}?raw=true"").read()
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 222, in urlopen
superset_init            |     return opener.open(url, data, timeout)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 531, in open
superset_init            |     response = meth(req, response)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 641, in http_response
superset_init            |     'http', request, response, code, msg, hdrs)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 563, in error
superset_init            |     result = self._call_chain(*args)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 503, in _call_chain
superset_init            |     result = func(*args)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 755, in http_error_302
superset_init            |     return self.parent.open(new, timeout=req.timeout)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 531, in open
superset_init            |     response = meth(req, response)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 641, in http_response
superset_init            |     'http', request, response, code, msg, hdrs)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 563, in error
superset_init            |     result = self._call_chain(*args)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 503, in _call_chain
superset_init            |     result = func(*args)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 755, in http_error_302
superset_init            |     return self.parent.open(new, timeout=req.timeout)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 525, in open
superset_init            |     response = self._open(req, data)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 543, in _open
superset_init            |     '_open', req)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 503, in _call_chain
superset_init            |     result = func(*args)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 1393, in https_open
superset_init            |     context=self._context, check_hostname=self._check_hostname)
superset_init            |   File ""/usr/local/lib/python3.7/urllib/request.py"", line 1352, in do_open
superset_init            |     raise URLError(err)
superset_init            | urllib.error.URLError: <urlopen error [Errno 99] Cannot assign requested address>
superset_init exited with code 1
superset_app             | 127.0.0.1 - - [19/Oct/2020 05:34:46] ""GET /health HTTP/1.1"" 200 -
superset_app             | INFO:werkzeug:127.0.0.1 - - [19/Oct/2020 05:34:46] ""GET /health HTTP/1.1"" 200 -
superset_app             | 127.0.0.1 - - [19/Oct/2020 05:35:17] ""GET /health HTTP/1.1"" 200 -
superset_app             | INFO:werkzeug:127.0.0.1 - - [19/Oct/2020 05:35:17] ""GET /health HTTP/1.1"" 200 -
superset_app             | 127.0.0.1 - - [19/Oct/2020 05:35:47] ""GET /health HTTP/1.1"" 200 -
superset_app             | INFO:werkzeug:127.0.0.1 - - [19/Oct/2020 05:35:47] ""GET /health HTTP/1.1"" 200 -
superset_app             | 127.0.0.1 - - [19/Oct/2020 05:36:18] ""GET /health HTTP/1.1"" 200 -
superset_app             | INFO:werkzeug:127.0.0.1 - - [19/Oct/2020 05:36:18] ""GET /health HTTP/1.1"" 200 -
superset_app             | 172.24.0.1 - - [19/Oct/2020 05:36:23] ""GET / HTTP/1.1"" 302 -
superset_app             | INFO:werkzeug:172.24.0.1 - - [19/Oct/2020 05:36:23] ""GET / HTTP/1.1"" 302 -
superset_app             | 172.24.0.1 - - [19/Oct/2020 05:36:23] ""GET /superset/welcome HTTP/1.1"" 200 -
superset_app             | INFO:werkzeug:172.24.0.1 - - [19/Oct/2020 05:36:23] ""GET /superset/welcome HTTP/1.1"" 200 -
superset_app             | 172.24.0.1 - - [19/Oct/2020 05:36:23] ""GET /static/assets/images/loading.gif HTTP/1.1"" 404 -
superset_app             | INFO:werkzeug:172.24.0.1 - - [19/Oct/2020 05:36:23] ""GET /static/assets/images/loading.gif HTTP/1.1"" 404 -

```
### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.60. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",siben168,"
--
I've finally been able to see the correct home page by waiting for an hour until the webpack completed its jobs. 
This error only impacted the sample data so i've not seen any data examples. I'm happy if anyone could help to fix this minor bug, but i'm also fine to close this ticket up. 
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",srinify,"
--
Hi @siben168 to help with this, a non-dev docker compose file was added so frontend assets don't have to be built!

https://github.com/apache/superset/pull/13143
--
",,,,
11295,OPEN,PRESTO_EXPAND_DATA causes SQL Lab crash,bug,2021-02-11 16:25:07 +0000 UTC,etr2460,In progress,,"When running a query with PRESTO_EXPAND_DATA enabled, we get a frontend crash with the following trace:
```
Uncaught TypeError: Cannot convert object to primitive value
    at Array.join (<anonymous>)
    at prepareCopyToClipboardTabularData (common.js?cf45:119)
    at ResultSet.renderControls (ResultSet.tsx?3731:192)
    at ResultSet.renderControls (VM202 react-hot-loader.development.js:714)
    at ResultSet.render (ResultSet.tsx?3731:300)
    at finishClassComponent (VM213 react-dom.development.js:17185)
    at updateClassComponent (VM213 react-dom.development.js:17135)
    at beginWork (VM213 react-dom.development.js:18654)
    at HTMLUnknownElement.callCallback (VM213 react-dom.development.js:189)
    at Object.invokeGuardedCallbackDev (VM213 react-dom.development.js:238)
```

I'm uncertain exactly what column is causing the error, but the table we're querying has these complex columns:
![image](https://user-images.githubusercontent.com/7409244/96199353-1e75e880-0f0c-11eb-9a60-75bd93c26275.png)

@betodealmeida could you take a look and see if you can repro this? I'm happy to work with you to debug/repro",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.99. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",etr2460,"
--
Hey @betodealmeida bumping this as we're getting requests to be able to re-enable this flag again internally. I might look into fixing it myself, but wanted to know if you had any thoughts first
--
",,,,,,
11284,OPEN,[table]Hide/select table columns on dashboards and export filtered table in CSV,enhancement:request; good first issue; viz:dashboard:native-filter,2021-02-16 21:10:14 +0000 UTC,NikeNano,In progress,,"**Is your feature request related to a problem? Please describe.**
I would like to filter a table in a dashboard to only show a subset of the columns in the original table in Superset. I would also like to have the possibility to select a subset of columns to export. This feature would bring values to non technical users that extract data from Superset to other tools. 


**Describe the solution you'd like**
The possibility for a user to select which columns that are displayed in a dashboard table. 

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
I would be happy to contribute to this, but might need some pointers since I am new to Superset. 


",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.96. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",NikeNano,"
--
@filipvitez

--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",junlincc,"
--
Changed title to [table]Hide table columns on dashboards and export filtered table in CSV 

It does seems helpful. we should implement at least the first request. @villebro 
--
",,,,
11282,OPEN,Map Tokens link is broken,bug; inactive,2020-12-25 17:55:30 +0000 UTC,junlincc,Opened,,"<img width=""953"" alt=""Screen Shot 2020-10-14 at 9 53 52 PM"" src=""https://user-images.githubusercontent.com/67837651/96079793-8247d500-0e6a-11eb-9c79-4f56a615d9a6.png"">

The map token links are broken, happening in all geospatial charts... ",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 1.00. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
11269,OPEN,Data with HTML tags break Explore chart,bug,2020-12-25 18:14:38 +0000 UTC,tooptoop4,In progress,,"superset 0.36, presto 336

run this presto query in SQL lab:

``` select '<table border=""0"" cellpadding=""0"" cellspacing=""0"" width=""394""><tbody><tr height=""19""><td height=""19"" width=""394"">Short name mappings to actual AD groups</td></tr></tbody></table>' text_value```

i have actual table with this data, so in my case --> ```select text_value from mytable``` gives the error
 
click Explore

error:

explore.e9de4d7e8c553ec56a7b.entry.js:2 TypeError: Cannot read property 'mData' of undefined
    at HTMLTableCellElement.<anonymous> (12.5e392cfe50d97f72c830.chunk.js:2)
    at Function.each (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at A.fn.init.each (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at HTMLTableElement.<anonymous> (12.5e392cfe50d97f72c830.chunk.js:2)
    at Function.each (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at A.fn.init.each (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at A.fn.init.s [as dataTable] (12.5e392cfe50d97f72c830.chunk.js:2)
    at A.fn.init.t.fn.DataTable (12.5e392cfe50d97f72c830.chunk.js:2)
    at 43.2d0745699313a33fbd16.chunk.js:2
    at as (explore.e9de4d7e8c553ec56a7b.entry.js:2)
es @ explore.e9de4d7e8c553ec56a7b.entry.js:2
o.componentDidCatch.n.callback @ explore.e9de4d7e8c553ec56a7b.entry.js:2
fo @ explore.e9de4d7e8c553ec56a7b.entry.js:2
os @ explore.e9de4d7e8c553ec56a7b.entry.js:2
pu @ explore.e9de4d7e8c553ec56a7b.entry.js:2
t.unstable_runWithPriority @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Ha @ explore.e9de4d7e8c553ec56a7b.entry.js:2
du @ explore.e9de4d7e8c553ec56a7b.entry.js:2
$s @ explore.e9de4d7e8c553ec56a7b.entry.js:2
(anonymous) @ explore.e9de4d7e8c553ec56a7b.entry.js:2
t.unstable_runWithPriority @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Ha @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Va @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Wa @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Qs @ explore.e9de4d7e8c553ec56a7b.entry.js:2
enqueueSetState @ explore.e9de4d7e8c553ec56a7b.entry.js:2
w.setState @ vendors-major.26c6716120c96d10daf2.chunk.js:2
t @ explore.e9de4d7e8c553ec56a7b.entry.js:2
(anonymous) @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Promise.then (async)
n._loadModule @ explore.e9de4d7e8c553ec56a7b.entry.js:2
n.componentWillMount @ explore.e9de4d7e8c553ec56a7b.entry.js:2
_o @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Ii @ explore.e9de4d7e8c553ec56a7b.entry.js:2
gs @ explore.e9de4d7e8c553ec56a7b.entry.js:2
lu @ explore.e9de4d7e8c553ec56a7b.entry.js:2
su @ explore.e9de4d7e8c553ec56a7b.entry.js:2
$s @ explore.e9de4d7e8c553ec56a7b.entry.js:2
(anonymous) @ explore.e9de4d7e8c553ec56a7b.entry.js:2
t.unstable_runWithPriority @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Ha @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Va @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Wa @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Qs @ explore.e9de4d7e8c553ec56a7b.entry.js:2
enqueueSetState @ explore.e9de4d7e8c553ec56a7b.entry.js:2
w.setState @ vendors-major.26c6716120c96d10daf2.chunk.js:2
u.onStateChange @ explore.e9de4d7e8c553ec56a7b.entry.js:2
d @ explore.e9de4d7e8c553ec56a7b.entry.js:2
(anonymous) @ explore.e9de4d7e8c553ec56a7b.entry.js:2
(anonymous) @ explore.e9de4d7e8c553ec56a7b.entry.js:2
dispatch @ explore.e9de4d7e8c553ec56a7b.entry.js:2
(anonymous) @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Promise.then (async)
(anonymous) @ explore.e9de4d7e8c553ec56a7b.entry.js:2
(anonymous) @ explore.e9de4d7e8c553ec56a7b.entry.js:2
(anonymous) @ explore.e9de4d7e8c553ec56a7b.entry.js:2
runQuery @ explore.e9de4d7e8c553ec56a7b.entry.js:2
componentDidMount @ explore.e9de4d7e8c553ec56a7b.entry.js:2
os @ explore.e9de4d7e8c553ec56a7b.entry.js:2
pu @ explore.e9de4d7e8c553ec56a7b.entry.js:2
t.unstable_runWithPriority @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Ha @ explore.e9de4d7e8c553ec56a7b.entry.js:2
du @ explore.e9de4d7e8c553ec56a7b.entry.js:2
$s @ explore.e9de4d7e8c553ec56a7b.entry.js:2
(anonymous) @ explore.e9de4d7e8c553ec56a7b.entry.js:2
t.unstable_runWithPriority @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Ha @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Va @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Wa @ explore.e9de4d7e8c553ec56a7b.entry.js:2
Qs @ explore.e9de4d7e8c553ec56a7b.entry.js:2
enqueueSetState @ explore.e9de4d7e8c553ec56a7b.entry.js:2
w.setState @ vendors-major.26c6716120c96d10daf2.chunk.js:2
(anonymous) @ explore.e9de4d7e8c553ec56a7b.entry.js:2
y @ explore.e9de4d7e8c553ec56a7b.entry.js:2
w @ explore.e9de4d7e8c553ec56a7b.entry.js:2
x @ explore.e9de4d7e8c553ec56a7b.entry.js:2
setTimeout (async)
b @ explore.e9de4d7e8c553ec56a7b.entry.js:2
A @ explore.e9de4d7e8c553ec56a7b.entry.js:2
(anonymous) @ explore.e9de4d7e8c553ec56a7b.entry.js:2
requestAnimationFrame (async)
(anonymous) @ explore.e9de4d7e8c553ec56a7b.entry.js:2
(anonymous) @ explore.e9de4d7e8c553ec56a7b.entry.js:2
explore.e9de4d7e8c553ec56a7b.entry.js:2 TypeError: Cannot read property 'mData' of undefined
    at HTMLTableCellElement.<anonymous> (12.5e392cfe50d97f72c830.chunk.js:2)
    at Function.each (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at A.fn.init.each (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at HTMLTableElement.<anonymous> (12.5e392cfe50d97f72c830.chunk.js:2)
    at Function.each (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at A.fn.init.each (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at A.fn.init.s [as dataTable] (12.5e392cfe50d97f72c830.chunk.js:2)
    at A.fn.init.t.fn.DataTable (12.5e392cfe50d97f72c830.chunk.js:2)
    at 43.2d0745699313a33fbd16.chunk.js:2
    at as (explore.e9de4d7e8c553ec56a7b.entry.js:2)",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.95. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",tooptoop4,"
--
![image](https://user-images.githubusercontent.com/33283496/96018745-8b717b80-0e43-11eb-8cbf-fb5d0f819b14.png)

--

--
crispy
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
11245,OPEN,[Bug] Area chart tooltip doesn't highlight a row anymore,.pinned; bug,2020-10-20 18:25:03 +0000 UTC,bkyryliuk,In progress,,"Area chart tooltip doesn't highlight a row anymore

## Screenshot
Current:

![image](https://user-images.githubusercontent.com/5727938/95800256-d789b800-0cab-11eb-8a1a-a4087ee3701f.png)


Previous:
![Screen Shot 2020-10-12 at 4 55 03 PM](https://user-images.githubusercontent.com/5727938/95800237-cc368c80-0cab-11eb-87c0-40bd48bffef9.png)

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.99. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",bkyryliuk,"
--
cc @junlincc @eugeniamz - please let me know who from engineering may have some context about it.
--

--
saw similar issue on the line chart as well
--

--
> @bkyryliuk what browser is this on? I am unable to repro both on Chrome and Firefox running current master.

@villebro it was latest master as of yesterday on chrome
tried just now on the latest master, line chart has highlighting, area charts still do not

![image](https://user-images.githubusercontent.com/5727938/95913340-ec705500-0d58-11eb-9e05-73a03eb3996e.png)

--

--
Was able to get a bit more information:

Small number of values in the chart works:
![image](https://user-images.githubusercontent.com/5727938/96040247-d200af00-0e1e-11eb-8018-860976d5e7f6.png)

Bug larger does not:
![image](https://user-images.githubusercontent.com/5727938/96040312-eb096000-0e1e-11eb-806b-b25dfc230bc9.png)

I was able to replicate it in firefox, safari and chrome on mac

This is how to create a similar chart:
![image](https://user-images.githubusercontent.com/5727938/96040421-17bd7780-0e1f-11eb-8007-0ef1b7626114.png)

--

--
curious if https://github.com/apache-superset/superset-ui/pull/764 is a root cause
--
",junlincc,"
--
@villebro 
@bkyryliuk thanks Bogdan we will prioritize it! 

--

--
As we are in the process of deprecating the NVD3 charts, we decide not to fix #11245, instead, we will work on the enhancement for ECharts area chart => highlight which series is hovered in tooltip to reach feature parity. 

Hope this work for the community! 
--
",villebro,"
--
@bkyryliuk what browser is this on? I am unable to repro both on Chrome and Firefox running current master.
--
",,,,
11205,OPEN,Add data sampling in line charts,enhancement:request; viz:chart-line,2021-01-02 18:21:46 +0000 UTC,oashton,Opened,,"**Is your feature request related to a problem? Please describe.**
In dashboards with a lot of line chart with high-density/high-frequency data (> 50.000 point per line), the user experience is not the best because the performance decrease while navigating or zooming data in the mini-chart, also the initial rendering is slow.

**Describe the solution you'd like**
I propose to enable an option to do a sampling of the data in the backend after the query is done and results are fetched.
To do the sampling we can use some algorithms like lttb (largest_triangle_three_buckets - https://github.com/devoxi/lttb-py/), simple max/min value per bucket/bin (https://docs.microsoft.com/en-us/power-bi/create-reports/desktop-high-density-sampling) or the Ramer-Douglas-Peucker (https://github.com/omarestrella/simplify.py)

**Describe alternatives you've considered**
An alternative is to simple add the required sampling logic in the query, but I'm looking for something more straightforward for the user (dashboard creator).

**Additional context**
I made an initial implementation adding the sampling just before the response is delivered
![image](https://user-images.githubusercontent.com/6887197/95514594-d130ce80-0981-11eb-8e25-ffcf01fa943c.png)
I'm open to hear your opinions and to contribute these feature if it's something useful",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.98. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",leeoniya,"
--
v5 might interest you:

https://github.com/apache/incubator-echarts/pull/13314
https://github.com/apache/incubator-echarts/pull/13337
--
",,,,,,,,
11202,OPEN,BUG: Unable to select column with byte datatype,bug; sql_lab,2020-11-25 06:52:48 +0000 UTC,sivasakthipt,In progress,,"I am using python 3.6.9 to install Apache Superset and elasticsearch version 7.9.1. I have created doc with type as byte but I couldn't query the value in SQL LAB in a superset, getting the following error while querying,

### Expected results
Select * from sample;
```
age    email                   name
10     admin@superset.com      6
```

what you expected to happen.
It should return the value

### Actual results
elasticsearch error: 'byte'

what actually happens.
Showing Error: elasticsearch error: 'byte'

#### Screenshots
![screenshot-ec2-15-207-37-206 ap-south-1 compute amazonaws com_8080-2020 10 08-12_42_12 (1)](https://user-images.githubusercontent.com/52786918/95464689-def14e80-0997-11eb-9001-4049c6dacaee.png)

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

Create index from Kibana,
```
PUT /sample
{
  ""mappings"": {
    ""properties"": {
      ""age"":    { ""type"": ""integer"" },  
      ""email"":  { ""type"": ""keyword""  }, 
      ""name"":   { ""type"": ""byte""  }     
    }
  }
}

PUT sample/_doc/1
{
  ""age"": ""10"",
  ""email"": ""admin@superset.com"",
  ""name"": ""admin""
}
```

1. Go to 'Superset SQL Lab'
2. Click on 'SQL Editor'
3. Select Database
4. Select Schema
5. Select table (index: sample)
6. See an error

### Environment

(please complete the following information):

- superset version: `Superset 0.37.2 `
- python version: `Python 3.6.9`


CC: @dpgaspar ",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.81. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",mistercrunch,"
--
Superset server logs would help here. Those kinds of error are hard to debug if you dont have an ES cluster laying around...
--
",sivasakthipt,"
--
I am getting the following error in superset while selecting the sample schema,

```
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:9200
INFO:werkzeug:223.228.146.132 - - [09/Oct/2020 03:53:13] ""GET /superset/extra_table_metadata/1/sample/default/ HTTP/1.1"" 200 -
DEBUG:urllib3.connectionpool:http://localhost:9200 ""POST /sample/_search?size=1 HTTP/1.1"" 200 277
INFO:elasticsearch:POST http://localhost:9200/sample/_search?size=1 [status:200 request:0.006s]
DEBUG:elasticsearch:> None
DEBUG:elasticsearch:< {""took"":1,""timed_out"":false,""_shards"":{""total"":1,""successful"":1,""skipped"":0,""failed"":0},""hits"":{""total"":{""value"":1,""relation"":""eq""},""max_score"":1.0,""hits"":[{""_index"":""sample"",""_type"":""_doc"",""_id"":""1"",""_score"":1.0,""_source"":{""name"":6,""age"":""10"",""email"":""admin@superset.com""}}]}}
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:9200
DEBUG:urllib3.connectionpool:http://localhost:9200 ""POST /_sql/ HTTP/1.1"" 200 210
INFO:elasticsearch:POST http://localhost:9200/_sql/ [status:200 request:0.004s]
DEBUG:elasticsearch:> {""query"":""SHOW COLUMNS FROM \""sample\""""}
DEBUG:elasticsearch:< {""columns"":[{""name"":""column"",""type"":""keyword""},{""name"":""type"",""type"":""keyword""},{""name"":""mapping"",""type"":""keyword""}],""rows"":[[""age"",""INTEGER"",""integer""],[""email"",""VARCHAR"",""keyword""],[""name"",""TINYINT"",""byte""]]}
WARNING:es.basesqlalchemy:Unknown type found byte reverting to string
INFO:werkzeug:223.228.146.132 - - [09/Oct/2020 03:53:13] ""GET /api/v1/database/1/table/sample/default/ HTTP/1.1"" 200 -
Triggering query_id: 31
INFO:superset.views.core:Triggering query_id: 31
timeout can't be used in the current context
WARNING:superset.utils.core:timeout can't be used in the current context
signal only works in main thread
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/superset/utils/core.py"", line 624, in __enter__
    signal.signal(signal.SIGALRM, self.handle_timeout)
  File ""/usr/lib64/python3.6/signal.py"", line 47, in signal
    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))
ValueError: signal only works in main thread
ERROR:superset.utils.core:signal only works in main thread
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/superset/utils/core.py"", line 624, in __enter__
    signal.signal(signal.SIGALRM, self.handle_timeout)
  File ""/usr/lib64/python3.6/signal.py"", line 47, in signal
    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))
ValueError: signal only works in main thread
SQLite Database support for metadata databases will be removed             in a future version of Superset.
WARNING:superset.sql_lab:SQLite Database support for metadata databases will be removed             in a future version of Superset.
Query 31: Executing 1 statement(s)
INFO:superset.sql_lab:Query 31: Executing 1 statement(s)
Query 31: Set query to 'running'
INFO:superset.sql_lab:Query 31: Set query to 'running'
Query 31: Running statement 1 out of 1
INFO:superset.sql_lab:Query 31: Running statement 1 out of 1
DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:9200
DEBUG:urllib3.connectionpool:http://localhost:9200 ""POST /_sql/ HTTP/1.1"" 200 146
INFO:elasticsearch:POST http://localhost:9200/_sql/ [status:200 request:0.011s]
DEBUG:elasticsearch:> {""query"":""SELECT age,\n       email,\n       name\nFROM sample\nLIMIT 100""}
DEBUG:elasticsearch:< {""columns"":[{""name"":""age"",""type"":""integer""},{""name"":""email"",""type"":""keyword""},{""name"":""name"",""type"":""byte""}],""rows"":[[10,""admin@superset.com"",6]]}
Query 31: <class 'KeyError'>
ERROR:superset.sql_lab:Query 31: <class 'KeyError'>
INFO:werkzeug:223.228.146.132 - - [09/Oct/2020 03:53:13] ""POST /superset/sql_json/ HTTP/1.1"" 500 -
```
--
",,,,,,
11198,OPEN,[SIP-55] Extending the security framework to provide data level security,enhancement:request; sip,2020-12-22 07:49:06 +0000 UTC,bolkedebruin,Opened,,"## [SIP] Proposal for Extending the security framework to provide data level security

### Motivation

In certain Enterprises. for example financial services, it is often required to limit the accessibility of data to certain people and to have the ability to manage this centrally. This means that users only have a limited ability to publish results in dashboards to a broader public. 

Typically this is managed on the data/resource level. For example user ""bolke"" has access to transaction data from Asia but cannot access transaction from Europe. It might be that ""bolke"" has access to aggregated data, like a dashboard or a chart, but not to the original datasource.

In such a context you would like to be able to set permissions in a leveled way like:

1. Dashboard
2. Chart -> (Table / View, Function, Column(s))

This type of resource based access is equal to for example what is implemented in Presto.

Superset currently does not have such a model. It only implements table/schema/database with row level filtering it does not have a fine grained permission model on dashboards and charts. This limits the usability in a enterprise context. It also has a technical limitation as we cannot enable the cache as that ignores the permission model that works on the database level (e.g. Presto, Druid etc).

### Proposed Change

A new decorator `@has_access_resource` that takes the action (read/export etc), the resource type, resource name, and its specification (e.g. Chart -> type, table/view, metric, column(s). The request itself (GET/POST) should also be available to this decorator as permissions might be dependent on time of the day, geolocation etc.

Permission checks can be expensive (we have over 1500 policies). Therefore I suggest a default ""pass"" implementation also for backwards compatibility. The check itself can then be 'outsourced' to highly optimized systems like Open Policy Agent or Apache Ranger.

### New or Changed Public Interfaces

Optionally a Securitymanager that implements `has_access_resource` can be provided in the configuration

### New dependencies

None

### Migration Plan and Compatibility

None, this is backwards compatible.

### Rejected Alternatives

It was mentioned that column access could be solved by proxy with views that expose subsets of columns. This creates an extra burden either in maintaining the Dashboard (which is the not possible to centrally manage) or with the Database team , while the global cache can still not be enabled.

cc @mistercrunch ",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.92. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",amitmiran137,"
--
hey @bolkedebruin please review #10408 in which we also describe **data access control** that might be relevant to what you are referring here 
--
",ktmud,"
--
+1 on generalizing access controls with one API. The `SecurityManager` already has [`can_access`](https://github.com/airbnb/incubator-superset/blob/2f0add3aec8f168fb8c37c648f0e29e507f39294/superset%2Fsecurity%2Fmanager.py#L217). Maybe we can do some clean up and start from there.
--
",,,,,,
11195,OPEN,[Question] How can I change chart's legend; axes labels; labels; and annotation colors?,enhancement:request,2021-04-08 11:22:01 +0000 UTC,sweileh,Opened,,"## How can I change the chart's legend, labels, axes labels, annotations colors?

![screenshot-1](https://user-images.githubusercontent.com/34817857/95393966-8c1e8500-090c-11eb-936d-04bb258a178a.png)

I'm currently trying to change the color of chart's legend, labels, axes labels, annotations colors, yet, I don't know where it should be done.
I tried playing with CSS but with no result.
I tried to do it from chart properties, however, I could not.

Can you please help?

Thanks!",,,issue,"
--
Issue-Label Bot is automatically applying the label `#question` to this issue, with a confidence of 0.95. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",junlincc,"
--
@sweileh 
you can change metrics name in Explore-> Data-> Query->METRICS->Edit title, and X Axis and Y Axis labels in Explore Customize. I know that some other BI tools have the ability to change label alias, customize legend color, shape and size etc more intuitively. We can certainly consider adding these features to the charts in the future. thanks for suggesting!
--

--
we should get to it at some point........
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",kunkunhandsome,"
--
I had the same problem.
My background color is black.This will drown out the color of the label text.
Can someone help me solve this problem?
Thanks!
--
",,,,
11186,OPEN,cosmetic: make SQL Lab tabs more visible/readable,assigned:polidea; bug:cosmetic; enhancement:request; inactive; needs:design-input; org:preset,2020-12-25 17:55:11 +0000 UTC,mistercrunch,In progress,,"## Screenshot

<img width=""458"" alt=""Screen Shot 2020-10-07 at 9 03 03 AM"" src=""https://user-images.githubusercontent.com/487433/95356840-f9360a00-087b-11eb-9789-82c9cab27cc2.png"">


## Description
New SQL tab titles are very muted. I'd say bump the font size a notch and use the link-color (darker) instead of the primary color. Also, I don't think we should force to uppercase in this context.

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.66. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",Steejay,"
--
@mistercrunch what font style are you using for this tab text? current size? For us to figure out what the best font styling is we'd like to mock it up in the correct context.  

For color we recommend not using link color since the text is not a link. 

For capitalization we recommend not forcing all caps.
--

--
Larger font size looks better. We would recommend changing the tab background on hover instead of the text color.
--
",mistercrunch,"
--
@kgabryje ^^^
--
",kgabryje,"
--
This blueish text colour is applied only for the active tab, for the rest it's rgba(0, 0, 0, 0.65). The font size is 12px.
Here is how it looks with font size 14px, no capitalization and all colours rgba(0, 0, 0, 0.65). Thoughts? I think it looks better, but I'd keep the blueish colour for hover effect.
![image](https://user-images.githubusercontent.com/15073128/96240184-e2d92e00-0fa0-11eb-858c-f6a163c5d69f.png)

--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,
11167,OPEN,[SIP-54] Proposal to improve export/import of objects,enhancement:request; sip,2020-11-17 09:02:34 +0000 UTC,betodealmeida,In progress,,"## [SIP-54] Proposal to improve export/import of objects

### Motivation

Superset provides functionality for exporting and importing objects. It can be used to export:

- **Datasets**: metadata describing the dataset **definition** and **semantic layer** can be exported to YAML ([example](https://gist.github.com/betodealmeida/0f0fd576b7229a72863b02e0dc2d70f7));
- **Databases**: metadata describing the database **connection**, as well as metadata describing contained **datasets**, can be exported to YAML ([example](https://gist.github.com/betodealmeida/8d4f06b2b002f06722f8c1330da778d1));
- **Dashboards**: metadata describing the dashboard, contained charts and associated datasets can be exported to a single JSON file ([example](https://gist.github.com/betodealmeida/e5aace448357c2e404fe7c8769dc1bc1)).

For import, currently only dashboards can be imported. The import assumes that the databases are identical in both instances, having the same tables. In theory databases [can have different names](https://github.com/apache/incubator-superset/pull/10118), but as @mistercrunch said ""the code in this area is not in a great shape"", with bugs (https://github.com/apache/incubator-superset/issues/11028) and unexpected side-effects (https://github.com/apache/incubator-superset/issues/10479). Additionally, since users need to specify a database when importing, dashboards with charts from multiple databases are currently not supported.

The main motivation for this SIP is to **fix** and **improve** the import/export functionality, proposing a well-defined format for serializing and deserializing collections of Superset objects. This includes databases, datasources, charts and dashboards. The format specification will be versioned, providing backwards compatibility.

Having a well-defined interchange format not only will prevent bugs but also allow us to build new functionality on top of it:

- We can potentially include the data in the download (assuming it's smaller than a given threshold). This would allow users to, eg, download a dashboard from a blog post or repository, load it into their instance, and explore it.
- It would provide a foundation for file-based configuration of dashboards. Other tools like [Grafana](https://grafana.com/) allow users to define dashboards in files and store them under version control, and having a well-defined format will make it easier to implement a similar storage mechanism in Superset.
- It would be easier to build tools that programmatically generate dashboards and charts.

### Proposed Change

The implementation of this SIP will build upon work in progress introducing UUIDs to import/export mixins (https://github.com/apache/incubator-superset/pull/7829). Adding UUIDs to the Superset models will help prevent conflicts when importing/exporting, as well as allowing the import of dashboards that are powered from different databases.

This SIP introduces a specification for serializing Superset objects, focusing on **backwards compatibility** and **readability**. Objects (databases, datasources, charts and dashboards) will be serialized to YAML, one file per object. Files will be grouped into directories according their type, and zipped together into a single file.

For example, exporting the ""Unicode Test"" dashboard would result in a ZIP file with the following structure:

```
# dashboard_unicode-test_20200923T173845.zip
databases/examples.yaml
datasets/examples/unicode_test.yaml
charts/unicode_cloud.yaml
dashboards/unicode_test.yaml
```

Each object will be versioned, and use UUIDs for relationships:

```yaml
# databases/examples.yaml
version: 1.0.0
id: e834a2be-439f-4cdf-bd55-0a1a32df7ceb
title: Examples
...
```

```yaml
# datasets/examples/unicode_test.yaml
version: 1.0.0
id: 9e963850-e556-4c09-bc95-79d1c0c98724
database_id: e834a2be-439f-4cdf-bd55-0a1a32df7ceb
table_name: unicode_test
...
```

When importing, Superset will unzip the file and check if it can import the provided version. Exports in the current format (single YAML or JSON file) will continue being supported, even though they don't declare a version.

On import, objects will be matched against existing objects based on UUID. In the case of a match, users will be presented with options to **upsert** (merge into existing object, with attributes on the file having precedence), **overwrite** or **ignore**. If there's no match, the object will be created if possible.

For databases, sensitive information such as passwords and the `secure_extra` field would be omitted, requiring the administrator to manually provide them on import.

### New or Changed Public Interfaces

The implementation of this SIP would deprecate the `/import_dashboards` endpoint, substituting it for a more generic `/api/v1/import` endpoint for any object type. The initial scope of this SIP is to replace the existing ""Import Dashboards"" menu entry under ""Settings"" in the header, but in the future we could have additional navigation paths, eg, having an ""Import Database"" option close to the button to add a new database.

The current import page is a CRUD UI generated by FAB, and it will be replaced with a React-based UI.

### New dependencies

No new dependencies will be introduced.

### Migration Plan and Compatibility

Implementing UUIDs on the `ImportMixin` would require adding a new column to the models to store the UUID, as well as populating existing objects with a new value. To simplify the export import/export process, the migration script would also add UUIDs to the `position_json` field in dashboards, pointing to the object UUID instead of its primary key.

### Rejected Alternatives

Other serialization formats were considered, but since we want files to be human-readable [YAML stood out](https://en.wikipedia.org/wiki/Comparison_of_data-serialization_formats), especially because it doesn't introduce any new dependencies.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.98. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",bkyryliuk,"
--
@betodealmeida would you see backing up / powering charts & dashboards from the github as a continuation of this effort ?
--

--
> > @betodealmeida would you see backing up / powering charts & dashboards from the github as a continuation of this effort ?
> 
> I think we can come up with nice abstractions when developing the import, so that they can be reused when developing a mechanism for filesystem-based configuration. But I'm not sure on the timeline for that, it's not a high priority AFAIK.

it would be great to keep that in mind, thank you! 
--
",betodealmeida,"
--
> @betodealmeida would you see backing up / powering charts & dashboards from the github as a continuation of this effort ?

I think we can come up with nice abstractions when developing the import, so that they can be reused when developing a mechanism for filesystem-based configuration. But I'm not sure on the timeline for that, it's not a high priority AFAIK.
--

--
@Amanjainnn we do, in the Settings menu:

<img width=""231"" alt=""Screen Shot 2020-10-30 at 1 06 15 PM"" src=""https://user-images.githubusercontent.com/1534870/97752175-bea54300-1ab0-11eb-814d-0042bc39bffd.png"">


--
",CoryChaplin,"
--
> > @betodealmeida would you see backing up / powering charts & dashboards from the github as a continuation of this effort ?
> 
> I think we can come up with nice abstractions when developing the import, so that they can be reused when developing a mechanism for filesystem-based configuration. But I'm not sure on the timeline for that, it's not a high priority AFAIK.

I think there's a nice use case for ""official dashboards"" that can be versioned, changes reviewed and deployed. And even more so for people running multiple instances of Superset, starting by dev/staging/prod or multi-team/multi-customer versions.
--
",villebro,"
--
I think this is a great initiative, really looking forward to all that this will enable, both stability and new functionality!  
--
",eugeniamz,"
--
What about including export and import queries?
--
"
11162,OPEN,Add an Export Excel Function,enhancement:request; inactive,2020-12-25 21:55:11 +0000 UTC,opus-42,Opened,,"**Is your feature request related to a problem? Please describe.**
Many analysts and teams still relies on Microsoft Excel to deal with data and create reports.
Building a bridge between Superset and Excel is key for a greater accessibility.
Adding an Excel Export Button would solve this issue.

**Describe the solution you'd like**
Add an ""Export XLSX"" Button in dashboard slices menu, explore and SQL Lab menu. 

<img width=""273"" alt=""image"" src=""https://user-images.githubusercontent.com/33355870/95112751-a48e7400-0741-11eb-8e3d-e940e9fbb775.png"">
<img width=""521"" alt=""image"" src=""https://user-images.githubusercontent.com/33355870/95112908-dacbf380-0741-11eb-93f9-16dce31018cc.png"">
<img width=""489"" alt=""image"" src=""https://user-images.githubusercontent.com/33355870/95113067-2383ac80-0742-11eb-95f0-f9e37527007f.png"">
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.99. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
11158,OPEN,Prepared Statement/Stored Procedure support with Superset when using MySQL.,bug; inactive,2020-12-25 21:55:12 +0000 UTC,Neel-rishabhsoft,Opened,,"I am trying to use prepared statement with superset, which is working fine when running on SQL Lab of superset. But, when moving to explore view the query is not working and superset is not able to visualize the data. 
### Expected results
The statements should work as a normal query works in MySQL and visualization should be possible.

### Actual results
This is my simple prepared statement:

**PREPARE stmt1 FROM 
	'SELECT 
   	    TaxName 
	FROM TaxRates
        WHERE Id = ?';
        
SET @ti = '1'; 

EXECUTE stmt1 USING @ti;**

Below is a screenshot of the Sql Lab view with results 

![image](https://user-images.githubusercontent.com/60099288/95048759-09c47400-0706-11eb-8f7a-ea25c0c2310d.png)

And this is a screenshot of results in explore view:

![image](https://user-images.githubusercontent.com/60099288/95048820-29f43300-0706-11eb-892b-409f58bef88f.png)

This is the query generated in explore view:

![image](https://user-images.githubusercontent.com/60099288/95048867-41cbb700-0706-11eb-8fb3-e484049dfed5.png)

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

I am facing exactly similar issue when using a stored procedure.

Please let me know if I am doing anything wrong or if the Superset doesn't support such operations.
Hoping for a prompt reply from the community.

Thanks in advance.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.58. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
11130,OPEN,[table]Advanced Analytics for Table Viz,enhancement:committed,2021-02-16 19:10:59 +0000 UTC,kkalyan,Opened,,"**Is your feature request related to a problem? Please describe.**
As of now, there is no way to get Day over day or week over week increases (and percentages) in Table View. There is support for Time shift in Bar charts, but its not there in Table Visualization. 

**Describe the solution you'd like**
Table Viz needs to have Advanced Analytics section. This was described and attempted to implement earlier 
https://github.com/apache/incubator-superset/issues/2324
https://github.com/apache/incubator-superset/pull/2325

**Describe alternatives you've considered**
If backend support window functions like LAG, this would be possible. 
https://www.sisense.com/blog/computing-day-over-day-changes-with-window-functions/
Some of the backends (like MySQL, Druid) does not have support for windowing functions.

**Additional context**
Add any other context or screenshots about the feature request here.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.96. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",ktmud,"
--
Could be related https://github.com/apache/incubator-superset/issues/9887
--

--
Hi, @junlincc , I am indeed working on adding per-column adhoc formattting, but advanced analytics isn't planed for the near term (or long term) yet.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",junlincc,"
--
@ktmud committed to this enhancement from our end. 

cc @villebro 
--
",,,,
11123,OPEN,Change status colors for running vs successful query in sqllab,enhancement:request; good first issue; need:followup,2021-03-17 05:28:54 +0000 UTC,cooley-pe,Opened,,"**Is your feature request related to a problem? Please describe.**
I have a lot of trouble distinguishing between the shades of green signifying a running vs successful query in sqllab. 

**Describe the solution you'd like**
Ideally these would be two different colors for running vs successful. Maybe yellow and green respectively?

**Describe alternatives you've considered**
If there isn't an entirely different color that makes sense, making the greens more distinct would be helpful.

**Additional context**
Success on the left and running on the right:
![Screen Shot 2020-09-30 at 3 40 02 PM](https://user-images.githubusercontent.com/64038793/94747318-6523ea00-0333-11eb-97d5-fb4f398dd39a.png)

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.96. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",sahilkamboj3,"
--
Hey @junlincc , I'd like to contribute. Is the issue available to work on?
--
",junlincc,"
--
@sahilkamboj3 Hi! sorry I missed your comment  proposed solution sounds reasonable. Please go ahead and open a PR, we will review for sure. 

cc @steejay 
--
",Steejay,"
--
Thanks for raising this issue @cooley-pe! 

@sahilkamboj3 Solution sounds good but lmk if/when you need further design input
--
",yousoph,"
--
A suggestion on #13648 was to have a hollow green o for the running icon and a filled in o for the successful query. 
--
",,
11119,OPEN,AUTH_REMOTE_USER and Scheduled email reports,bug,2021-01-25 02:24:35 +0000 UTC,yackushevas,In progress,,"### Expected results

Ability to work with service users when using AUTH_TYPE = AUTH_REMOTE_USER.

### Actual results

The user specified in EMAIL_REPORTS_USER does not have a simplified authorization option (or I could not find it in the documentation).

#### How to reproduce the bug

1. Set AUTH_TYPE = AUTH_REMOTE_USER and use CUSTOM_SECURITY_MANAGER
2. Set ENABLE_SCHEDULED_EMAIL_REPORTS = True
3. See error:
```
Exception on /login/ [GET]
Traceback (most recent call last):
  File ""/opt/superset/venv/lib/python3.6/site-packages/flask/app.py"", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File ""/opt/superset/venv/lib/python3.6/site-packages/flask/app.py"", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/opt/superset/venv/lib/python3.6/site-packages/flask/app.py"", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/opt/superset/venv/lib/python3.6/site-packages/flask/_compat.py"", line 39, in reraise
    raise value
  File ""/opt/superset/venv/lib/python3.6/site-packages/flask/app.py"", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/opt/superset/venv/lib/python3.6/site-packages/flask/app.py"", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/opt/superset/venv/bin/superset_config.py"", line 142, in login
    login_user(user)
  File ""/opt/superset/venv/lib/python3.6/site-packages/flask_login/utils.py"", line 158, in login_user
    if not force and not user.is_active:
AttributeError: 'NoneType' object has no attribute 'is_active'
```",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.53. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",jawabuu,"
--
Hey @yackushevas Are you using any middleware?
Could you share your CustomSecurityManager?
--

--
@mistercrunch Using your example here
https://gist.github.com/mistercrunch/6d31af4a11c47edcedc1ba6ceb5f5fab works fine for remote user login but returns 

`raised unexpected: WebDriverException('Failed to decode response from marionette', None, None)` 
or
` NoSuchElementException('Unable to locate element: .grid-container', None, None)`

for email Reports.
Switching to AUTH_OAUTH or AUTH_DB resolves it.

--
",yackushevas,"
--
> for email Reports.
> Switching to AUTH_OAUTH or AUTH_DB resolves it.

Then AUTH_REMOTE_USER will not work.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,
11105,OPEN,New number format for durations,enhancement:request,2020-11-25 06:49:18 +0000 UTC,avicenteg,In progress,,"**Is your feature request related to a problem? Please describe.**
When I'm working with durations, Superset offers me by default two options:

![image](https://user-images.githubusercontent.com/64475023/94570951-b074c480-026f-11eb-8506-bf44862e7e4c.png)

The first one is my favourite but sometimes I would like to have a simpler format. Instead of _HH:mm:ss.s_ I'd prefer _HH:mm_.

**Describe the solution you'd like**
Add a new format in the format number section for durations coming from ms. 

Duration in ms (66000 => 1m)

**Describe alternatives you've considered**

I think that my proposed solution is easy adding this option to the number-format package.

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.91. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",villebro,"
--
@avicenteg I'd be interested in hearing more about your use case. We could potentially add the `colonNotation` option (`HH:mm:ss` as opposed to the traditional `H m s` format) that [pretty-ms](https://github.com/sindresorhus/pretty-ms) supports as a default duration formatting option. You can check the unit tests there to see which format suits you needs best. In the long term the plan is to make it possible to pass the options to the formatter, much like is done for D3 now.
--
",avicenteg,"
--
@villebro In fact, I'm more interested in not having the seconds part. Maybe use the colonNotation isn't a good idea since it may lead to confussion with Time fields. 
I think that the option `compact` in pretty-ms is the best way to get it, but I'm not sure if it will delete also the minutes. 

For example:

3660000 ms => 1 h 57 m 

It will be great to have an option of passing the options to the formatter, but I understand that it is a little bit trickier than for D3.

--
",,,,,,
11104,OPEN,Database driver for Netezza,enhancement:request; inactive,2020-12-25 21:55:15 +0000 UTC,hainv,In progress,,"Hi,
I'm new to superset and I can see database driver in ""Database Driver"" list. Please work on to add Netezza to support database driver.

Many thanks!",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.81. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",villebro,"
--
@hainv it appears there's a Netezza Sql Alchemy dialect, so it should be pretty straight forward to implement on Superset: https://github.com/IBM/nzalchemy . I can assist if you need help, just reach out on Slack.
--

--
@hainv I assume you tried `pip install nzalchemy`? https://pypi.org/project/nzalchemy/
--
",hainv,"
--
@villebro please help. I can't get driver from IBM link in github.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,
11081,OPEN,Installation Error - Docker-Compose Up on Ubuntu VM on Windows 10,bug; install:docker,2021-01-02 18:41:43 +0000 UTC,sridharj-in,Opened,,"**OS:** Windows 10 Pro Version 10.0.18362 Build 18362
**VM:** Ubuntu 20.04.1 LTS using WSL 2
**Docker Version inside Ubuntu:**Docker version 19.03.13, build 4484c46d9d
Docker Desktop of Windows Installed and running Linux Containers
**Superset version:** Latest version as of 26-Sep-2020

**Steps to Reproduce:**
1. Installed Docker-compose
2. Installed Superset through $ git clone https://github.com/apache/incubator-superset.git
3. Navigate to the newly created folder via cd incubator-superset
5. Run sudo Docker-compose up
```
**Issue:**
Connection errors - log pasted below

Traceback (most recent call last):
  File ""urllib3/connectionpool.py"", line 677, in urlopen
  File ""urllib3/connectionpool.py"", line 392, in _make_request
  File ""http/client.py"", line 1252, in request
  File ""http/client.py"", line 1298, in _send_request
  File ""http/client.py"", line 1247, in endheaders
  File ""http/client.py"", line 1026, in _send_output
  File ""http/client.py"", line 966, in send
  File ""docker/transport/unixconn.py"", line 43, in connect
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""requests/adapters.py"", line 449, in send
  File ""urllib3/connectionpool.py"", line 727, in urlopen
  File ""urllib3/util/retry.py"", line 403, in increment
  File ""urllib3/packages/six.py"", line 734, in reraise
  File ""urllib3/connectionpool.py"", line 677, in urlopen
  File ""urllib3/connectionpool.py"", line 392, in _make_request
  File ""http/client.py"", line 1252, in request
  File ""http/client.py"", line 1298, in _send_request
  File ""http/client.py"", line 1247, in endheaders
  File ""http/client.py"", line 1026, in _send_output
  File ""http/client.py"", line 966, in send
  File ""docker/transport/unixconn.py"", line 43, in connect
urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionRefusedError(111, 'Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""docker/api/client.py"", line 205, in _retrieve_server_version
  File ""docker/api/daemon.py"", line 181, in version
  File ""docker/utils/decorators.py"", line 46, in inner
  File ""docker/api/client.py"", line 228, in _get
  File ""requests/sessions.py"", line 543, in get
  File ""requests/sessions.py"", line 530, in request
  File ""requests/sessions.py"", line 643, in send
  File ""requests/adapters.py"", line 498, in send
requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionRefusedError(111, 'Connection refused'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""bin/docker-compose"", line 3, in <module>
  File ""compose/cli/main.py"", line 67, in main
  File ""compose/cli/main.py"", line 123, in perform_command
  File ""compose/cli/command.py"", line 69, in project_from_options
  File ""compose/cli/command.py"", line 132, in get_project
  File ""compose/cli/docker_client.py"", line 43, in get_client
  File ""compose/cli/docker_client.py"", line 170, in docker_client
  File ""docker/api/client.py"", line 188, in __init__
  File ""docker/api/client.py"", line 213, in _retrieve_server_version
docker.errors.DockerException: Error while fetching server API version: ('Connection aborted.', ConnectionRefusedError(111, 'Connection refused'))
[745] Failed to execute script docker-compose
```",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.87. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",sridharj,"
--
Additional Log info:

```
Traceback (most recent call last)

    File ""/usr/local/lib/python3.6/site-packages/flask/cli.py"", line 240, in locate_app

    __import__(module_name)

    During handling of the above exception, another exception occurred:
    File ""/usr/local/lib/python3.6/site-packages/flask/cli.py"", line 343, in __call__

    rv = self._load_unlocked()

    File ""/usr/local/lib/python3.6/site-packages/flask/cli.py"", line 330, in _load_unlocked

    self._app = rv = self.loader()

    File ""/usr/local/lib/python3.6/site-packages/flask/cli.py"", line 388, in load_app

    app = locate_app(self, import_name, name)

    File ""/usr/local/lib/python3.6/site-packages/flask/cli.py"", line 250, in locate_app

    raise NoAppException('Could not import ""{name}"".'.format(name=module_name))

    flask.cli.NoAppException: Could not import ""superset.app"".

```


--
",trumpetsherald,"
--
I have this same issue on a clean ubuntu 18 image. 
--
",nytai,"
--
Did you clone the repo on windows? If so the files system is incompatible and will cause the scripts to fail once they're moved into docker. 
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,
11080,OPEN,Upgrade to 0.37.1 has bug: column dbs.allow_cvas does not exist,bug,2021-02-08 14:05:32 +0000 UTC,sazary,In progress,,"Hi  
I just upgraded superset from 0.36.0 to 0.37.1 per instructions in docs. Now when I try to open any dashboard or slice, I get a 500 error.

### Expected results

The dashboards to open.

### Actual results

A 500 error, with this stacktrace:

```
Sorry, something went wrong
500 - Internal Server Error
Stacktrace
        Traceback (most recent call last):
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1278, in _execute_context
    cursor, statement, parameters, context
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedColumn: column dbs.allow_cvas does not exist
LINE 1: ...low_csv_upload, dbs.allow_ctas AS dbs_allow_ctas, dbs.allow_...
                                                             ^
HINT:  Perhaps you meant to reference the column ""dbs.allow_ctas"".


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/flask/app.py"", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/flask/app.py"", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/flask/app.py"", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/flask/_compat.py"", line 39, in reraise
    raise value
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/flask/app.py"", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/flask/app.py"", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/flask_appbuilder/security/decorators.py"", line 109, in wraps
    return f(self, *args, **kwargs)
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/superset/views/core.py"", line 1616, in dashboard
    for datasource, slices in datasources.items()
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/superset/views/core.py"", line 1616, in <dictcomp>
    for datasource, slices in datasources.items()
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/superset/connectors/base/models.py"", line 276, in data_for_slices
    data = self.data
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/superset/connectors/sqla/models.py"", line 602, in data
    data_ = super().data
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/superset/connectors/base/models.py"", line 244, in data
    ""database"": self.database.data,  # pylint: disable=no-member
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/orm/attributes.py"", line 287, in __get__
    return self.impl.get(instance_state(instance), dict_)
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/orm/attributes.py"", line 723, in get
    value = self.callable_(state, passive)
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/orm/strategies.py"", line 760, in _load_for_state
    session, state, primary_key_identity, passive
  File ""<string>"", line 1, in <lambda>
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/orm/strategies.py"", line 850, in _emit_lazyload
    session.query(self.mapper), primary_key_identity
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/ext/baked.py"", line 616, in _load_on_pk_identity
    result = list(bq.for_session(self.session).params(**params))
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/ext/baked.py"", line 445, in __iter__
    return q._execute_and_instances(context)
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3528, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1014, in execute
    return meth(self, multiparams, params)
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/sql/elements.py"", line 298, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1133, in _execute_clauseelement
    distilled_params,
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1318, in _execute_context
    e, statement, parameters, cursor, context
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1512, in _handle_dbapi_exception
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_
    raise exception
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1278, in _execute_context
    cursor, statement, parameters, context
  File ""/home/superset/.env/superset/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedColumn) column dbs.allow_cvas does not exist
LINE 1: ...low_csv_upload, dbs.allow_ctas AS dbs_allow_ctas, dbs.allow_...
                                                             ^
HINT:  Perhaps you meant to reference the column ""dbs.allow_ctas"".

[SQL: SELECT dbs.created_on AS dbs_created_on, dbs.changed_on AS dbs_changed_on, dbs.id AS dbs_id, dbs.verbose_name AS dbs_verbose_name, dbs.database_name AS dbs_database_name, dbs.sqlalchemy_uri AS dbs_sqlalchemy_uri, dbs.password AS dbs_password, dbs.cache_timeout AS dbs_cache_timeout, dbs.select_as_create_table_as AS dbs_select_as_create_table_as, dbs.expose_in_sqllab AS dbs_expose_in_sqllab, dbs.allow_run_async AS dbs_allow_run_async, dbs.allow_csv_upload AS dbs_allow_csv_upload, dbs.allow_ctas AS dbs_allow_ctas, dbs.allow_cvas AS dbs_allow_cvas, dbs.allow_dml AS dbs_allow_dml, dbs.force_ctas_schema AS dbs_force_ctas_schema, dbs.allow_multi_schema_metadata_fetch AS dbs_allow_multi_schema_metadata_fetch, dbs.extra AS dbs_extra, dbs.encrypted_extra AS dbs_encrypted_extra, dbs.impersonate_user AS dbs_impersonate_user, dbs.server_cert AS dbs_server_cert, dbs.created_by_fk AS dbs_created_by_fk, dbs.changed_by_fk AS dbs_changed_by_fk 
FROM dbs 
WHERE dbs.id = %(param_1)s]
[parameters: {'param_1': 2}]
(Background on this error at: http://sqlalche.me/e/13/f405)
```

#### How to reproduce the bug

1. Install version 0.36.0
2. Setup some charts and dashboards
3. Upgrade to version 0.37.0 using commands written in docs:

```
pip install apache-superset --upgrade
superset db upgrade
superset init
```

4. Go to any dashboard/slice.

### Environment

(please complete the following information):

- superset version: `Superset 0.37.1`
- python version: `Python 3.6.11`",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",zhaoyongjie,"
--
@sazary Hi, guy, must run database migration script:
```
$ superset db upgrade
```
--
",sazary,"
--
Hi
I did it, as I've mentioned in steps to reproduce
________________________________
From: Yongjie Zhao <notifications@github.com>
Sent: Sunday, September 27, 2020 10:18:19 AM
To: apache/incubator-superset <incubator-superset@noreply.github.com>
Cc: Soroosh Azary Marhabi <soroosh@azary.ir>; Mention <mention@noreply.github.com>
Subject: Re: [apache/incubator-superset] Upgrade to 0.37.1 has bug: column dbs.allow_cvas does not exist (#11080)


@sazary<https://github.com/sazary> Hi, guy, must run database migration script:

$ superset db upgrade



You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub<https://github.com/apache/incubator-superset/issues/11080#issuecomment-699594284>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AAJ6MLIEBBI2NTBMZC63PBLSH3N3HANCNFSM4R25DEMA>.

--

--
sorry I don't have the output now, but as far as I could see there wasn't any error in it.  
I've installed & upgraded it using pip 
--

--
@keehl1213 no i just gave up 
--
",dpgaspar,"
--
Can you post the output from `superset db upgrade` ? Also are you using `docker-compose` or some other install method?
--
",nh43de,"
--
I am having the same issue as well, when using 0.37.2 release, docker-compose up --build from source control
--

--
Steps to reproduce:

1. Check out 0.37.2 tag
2.  `docker-compose up --build --force-recreate`
3.  < < wait for compose >> 
4. Shell into superset-app, run `superset db upgrade` and I get this error message.

(note: this problem does not happen in main/master branch, everything behaves normally)

```
superset@5c7e780d5110:/app$ superset db upgrade
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 567, in _build_master
    ws.require(__requires__)
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 884, in require
    needed = self.resolve(parse_requirements(requirements))
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 775, in resolve
    raise VersionConflict(dist, req).with_context(dependent_req)
pkg_resources.ContextualVersionConflict: (pyarrow 0.17.0 (/usr/local/lib/python3.7/site-packages), Requirement.parse('pyarrow<1.1,>=1.0.1'), {'apache-superset'})

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/bin/superset"", line 33, in <module>
    sys.exit(load_entry_point('apache-superset', 'console_scripts', 'superset')())
  File ""/usr/local/lib/python3.7/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/flask/cli.py"", line 586, in main
    return super(FlaskGroup, self).main(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/usr/local/lib/python3.7/site-packages/click/core.py"", line 1254, in invoke
    cmd_name, cmd, args = self.resolve_command(ctx, args)
  File ""/usr/local/lib/python3.7/site-packages/click/core.py"", line 1297, in resolve_command
    cmd = self.get_command(ctx, cmd_name)
  File ""/usr/local/lib/python3.7/site-packages/flask/cli.py"", line 527, in get_command
    self._load_plugin_commands()
  File ""/usr/local/lib/python3.7/site-packages/flask/cli.py"", line 517, in _load_plugin_commands
    import pkg_resources
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 3238, in <module>
    @_call_aside
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 3222, in _call_aside
    f(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 3251, in _initialize_master_working_set
    working_set = WorkingSet._build_master()
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 569, in _build_master
    return cls._build_from_requirements(__requires__)
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 582, in _build_from_requirements
    dists = ws.resolve(reqs, Environment())
  File ""/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py"", line 770, in resolve
    raise DistributionNotFound(req, requirers)
pkg_resources.DistributionNotFound: The 'pyarrow<1.1,>=1.0.1' distribution was not found and is required by apache-superset
superset@5c7e780d5110:/app$

```
--
",mistercrunch,"
--
@nh43de , looks like a different error
--
"
11053,OPEN,Mongo and Cassandra connectors,enhancement:request; inactive,2020-11-26 22:19:45 +0000 UTC,joehoeller,Opened,,"REQUEST: Ability to connect Superset to Mongo and Cassandra to run queries against it.
Apache Drill can connect to Mongo and Big Query can connect to Cassandra, so it's def possible.

There is Flask Mongo Alchemy - https://pythonhosted.org/Flask-MongoAlchemy/
SQLAlchemy can connect to Cassandra, apparently - https://www.cdata.com/kb/tech/cassandra-python-sqlalchemy.rst

How can we get this into Superset, even if we cant use sql syntax, cql is a fine option as well perhaps(?)",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.87. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",mistercrunch,"
--
We'd have to confirm that the implementation works for the type of queries that we run, with `.groupby(...)`, `.filter()`, and aggregation expressions like `SUM` and `COUNT`. My guess is that they've implement an OLTP-like subset of SQL and DML.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
11043,OPEN,superset_node fails on Ubuntu 18.04,bug; install:docker,2021-01-02 21:46:15 +0000 UTC,codepic,Opened,,"### Steps to repro

superset_node exits with code 1 when running the following command on Ubuntu 18.04

```bash
docker-compose up --build
```

### Expected results

```bash
superset_node            | No type errors found
superset_node            | Version: typescript 3.8.3
superset_node            | Time: 49404ms
superset_node            | <s> [webpack.Progress] 100% 
superset_node            | 
superset_node            |    7749 modules
```

### Actual results
```bash
superset_node            | 
superset_node            | npm ERR! code EBADPLATFORM
superset_node            | npm ERR! notsup Unsupported platform for fsevents@2.1.3: wanted {""os"":""darwin"",""arch"":""any""} (current: {""os"":""linux"",""arch"":""x64""})
superset_node            | npm ERR! notsup Valid OS:    darwin
superset_node            | npm ERR! notsup Valid Arch:  any
superset_node            | npm ERR! notsup Actual OS:   linux
superset_node            | npm ERR! notsup Actual Arch: x64
superset_node            | 
superset_node            | npm ERR! A complete log of this run can be found in:
superset_node            | npm ERR!     /root/.npm/_logs/2020-09-24T13_16_42_389Z-debug.log
superset_node exited with code 1
```

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Pull incubator-superset, branch master, commit 6181994084067a32883bb01db55356cc7aa4712b
2. cd incubator-superset
3. docker-compose up --build

### Environment

(please complete the following information):

- superset version: `6181994084067a32883bb01db55356cc7aa4712b`
- python version: `Python 2.7.17`
- node.js version: `v12.18.3`
- npm version: `6.14.6`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

superset_node succeeds with 8e9b0b3f1100120a14134c0ca0c5dd4dd4a9e8bb
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.92. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",eschutho,"
--
According to [FsEvents](https://www.npmjs.com/package/fsevents), that is a known warning, but as per their docs it should  just be a warning and should not fail the build, although I do see other people using that library and seeing errors in earlier versions of it. 
```
I'm getting EBADPLATFORM Unsupported platform for fsevents error.
It's fine, nothing is broken. fsevents is macos-only. Other platforms are skipped. If you want to hide this warning, report a bug to NPM bugtracker asking them to hide ebadplatform warnings by default.
```
I'm looking to see what has changed since you say this was working a while ago. What's your current status? We could set that module as an optionalDependency which should clear up the error. 
--
",,,,,,,,
11028,OPEN,Cannot import dashboard to a database with a different name,bug; inactive,2020-11-26 22:19:56 +0000 UTC,betodealmeida,Opened,,"When importing a dashboard from the `examples` database to a database with a different name, the import fails.

### Expected results

Import should complete.

### Actual results

```
sqlalchemy.exc.InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (sqlite3.IntegrityError) UNIQUE constraint failed: tables.table_name
[SQL: INSERT INTO tables (created_on, changed_on, description, default_endpoint, is_featured, filter_select_enabled, ""offset"", cache_timeout, params, perm, schema_perm, table_name, main_dttm_col, database_id, fetch_values_predicate, schema, sql, is_sqllab_view, template_params, created_by_fk, changed_by_fk) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('2020-09-23 12:44:36.044830', '2020-09-23 12:44:36.044837', None, None, 0, 0, 0, None, '{""remote_id"": 4, ""database_name"": ""examples"", ""import_time"": 1600890276}', None, None, 'unicode_test', 'dttm', '3', None, None, None, 0, None, 2, 2)]
(Background on this error at: http://sqlalche.me/e/13/gkpj) (Background on this error at: http://sqlalche.me/e/13/7s2a)
```

#### How to reproduce the bug

0. Set `PREVENT_UNSAFE_DB_CONNECTIONS` to false.
1. Create a new database called `examples 2`, using the same SQLAlchemy connection string as the `examples` database.
2. Export the ""Unicode test"" dashboard from `examples`.
3. Import the dashboard into the `examples 2` database.

### Environment

- superset version: SHA: e4e78b66
- python version: 3.8.5
- node.js version: v14.11.0
- npm version: 6.14.8

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

N/A",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.95. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
11022,OPEN,Feature request: Using tags to help sort and search for item,enhancement:request,2020-12-10 05:05:25 +0000 UTC,junlincc,Opened,,"Chart Menu Organisation
Problem:
Currently if I have 200 charts split across 20 dashboards, I need to navigate through the entire list of charts. It can get a quite inefficient.

Proposed Solution: 
Add one more column for dashboard with filtering capabilities. Current in the datasource column we can only sort-by but not filter.

tag feature could solved this problem but it's in feature flags currently  
https://github.com/apache/incubator-superset/pull/7418 
@betodealmeida 

(post on behalf)",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.96. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
11021,OPEN,Feature Request: Add same chart to Tabbed Dashboard(multiple tabs),enhancement:committed,2021-02-22 20:41:41 +0000 UTC,junlincc,Opened,,"Add charts to Tabbed Dashboard
Scenario:
- I have used the Tabs  feature to segregate various dashboards (Say Tab 1 and Tab 2).
- I have a couple of charts that are common to multiple dashboards(say date filter) and would like to add this multiple tabs.
**Current issue:**
- Charts used once in a tabbed dashboard cannot be used again in another tab.
![Screen Shot 2020-09-18 at 2 19 34 PM](https://user-images.githubusercontent.com/67837651/94039303-65911380-fd7c-11ea-809f-b83752bb8b7b.png)

(post on behalf)
",,,vialcollet,"
--
Other use case would be to have the same chart but with different filters applied.
For example a line chart showing some stats for a given country and we would need several of these charts with each their own country filter.
In such case it will be required to have several instance of the line charts and several instance of the country filter with appropriate mapping.
This would be fantastic.

--
",Dirzel,"
--
@junlincc, Would the solution suggested in #8303 work for this.
--
",,,,,,,,
10997,OPEN,run npm run build failid in superset-frontend (0.37.0),bug; cant-reproduce; need:more-info,2021-02-13 23:03:09 +0000 UTC,michaelai0313,In progress,,"run npm run build failid in superset-frontend

Actual results
```
ERROR in ./src/visualizations/TimeTable/TimeTable.jsx 101:10
Module parse failed: Unexpected token (101:10)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
|       if (fullUrl) {
|         return (
>           <a href={fullUrl} rel=""noopener noreferrer"" target=""_blank"">
|             {column.label}
|           </a>

ERROR in ./src/SqlLab/App.jsx 111:2
Module parse failed: Unexpected token (111:2)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
|
| const Application = () => (
>   <Provider store={store}>
|     <ThemeProvider theme={supersetTheme}>
|       <App />

ERROR in ./src/explore/controls.jsx 135:23
Module parse failed: Unexpected token (135:23)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
|   includeTime: false,
|   description: t('One or many controls to group by'),
>   optionRenderer: c => <ColumnOption column={c} showType />,
|   valueRenderer: c => <ColumnOption column={c} />,
|   valueKey: 'column_name',

ERROR in ./src/explore/components/controls/VizTypeControl.jsx 146:6
Module parse failed: Unexpected token (146:6)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
|
|     return (
>       <div
|         className={`viztype-selector-container ${isSelected ? 'selected' : ''}
`}
|         onClick={this.onChange.bind(this, key)}

ERROR in ./src/dashboard/components/PropertiesModal.jsx 188:6
Module parse failed: Unexpected token (188:6)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
|     const { values, isDashboardLoaded, isAdvancedOpen } = this.state;
|     return (
>       <Modal show={this.props.show} onHide={this.props.onHide} bsSize=""lg"">
|         <form onSubmit={this.save}>
|           <Modal.Header closeButton>

ERROR in ./src/visualizations/FilterBox/FilterBox.jsx 233:26
Module parse failed: Unexpected token (233:26)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
|       }),
|     });
>     const options = (json?.data?.[key] || []).filter(x => x.id);
|     if (!options || options.length === 0) {
|       return [];

ERROR in ./src/components/TableSelector.jsx 243:6
Module parse failed: Unexpected token (243:6)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
|   renderDatabaseOption(db) {
|     return (
>       <span>
|         <Label bsStyle=""default"" className=""m-r-5"">
|           {db.backend}

ERROR in ./src/showSavedQuery/index.jsx 39:4
Module parse failed: Unexpected token (39:4)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
|
|   ReactDom.render(
>     <div>
|       <Form
|         schema={config.JSONSCHEMA}

ERROR in ./src/explore/controlPanels/FilterBox.jsx 41:9
Module parse failed: Unexpected token (41:9)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
|           },
|         ],
>         [<hr />],
|         [
|           {

ERROR in ./src/dashboard/index.jsx 42:16
Module parse failed: Unexpected token (42:16)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
| );
|
> ReactDOM.render(<App store={store} />, document.getElementById('app'));
|

ERROR in ./src/explore/controlPanels/Shared_DeckGL.jsx 42:2
Module parse failed: Unexpected token (42:2)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
|   'blob/master/superset-frontend/src/modules/sandbox.js';
| const jsFunctionInfo = (
>   <div>
|     {t(
|       'For more information about objects are in context in the scope of this
function, refer to the',

ERROR in ./src/explore/index.jsx 44:16
Module parse failed: Unexpected token (44:16)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
| );
|
> ReactDOM.render(<App store={store} />, document.getElementById('app'));
|

ERROR in ./src/components/TooltipWrapper.jsx 47:4
Module parse failed: Unexpected token (47:4)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
| }) {
|   return (
>     <OverlayTrigger
|       placement={placement}
|       overlay={<Tooltip id={`${kebabCase(label)}-tooltip`}>{tooltip}</Tooltip>
}

ERROR in ./src/components/ErrorBoundary.jsx 50:8
Module parse failed: Unexpected token (50:8)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
|       const firstLine = error.toString();
|       const message = (
>         <span>
|           <strong>{t('Unexpected error')}</strong>
|           {firstLine ? `: ${firstLine}` : ''}

ERROR in ./src/components/Button.jsx 54:4
Module parse failed: Unexpected token (54:4)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
|
|   let button = (
>     <BootstrapButton {...buttonProps}>{props.children}</BootstrapButton>
|   );
|   if (tooltip) {

ERROR in ./src/components/TableLoader.jsx 64:13
Module parse failed: Unexpected token (64:13)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
|   render() {
|     if (this.state.isLoading) {
>       return <Loading />;
|     }
|

ERROR in ./src/components/Menu/Menu.jsx 67:4
Module parse failed: Unexpected token (67:4)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
| }) {
|   return (
>     <StyledHeader className=""top"" id=""main-menu"">
|       <Navbar inverse fluid staticTop role=""navigation"">
|         <Navbar.Header>

ERROR in ./src/components/Select/OnPasteSelect.jsx 79:11
Module parse failed: Unexpected token (79:11)
You may need an appropriate loader to handle this file type, currently no loader
s are configured to process this file. See https://webpack.js.org/concepts#loade
rs
|   render() {
|     const { selectWrap: SelectComponent, ...restProps } = this.props;
>     return <SelectComponent {...restProps} onPaste={this.onPaste} />;
|   }
| }
npm ERR! code ELIFECYCLE
npm ERR! errno 2
npm ERR! superset@0.37.0 build: `cross-env NODE_OPTIONS=--max_old_space_size=819
2 NODE_ENV=production webpack --mode=production --colors`
npm ERR! Exit status 2
npm ERR!
npm ERR! Failed at the superset@0.37.0 build script.
npm ERR! This is probably not a problem with npm. There is likely additional log
ging output above.


Environment
(please complete the following information):

superset version: 0.36.0
python version: 3.6.10
node.js version: v12.18.3
npm version: 6.14.6
```",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.62. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",mistercrunch,"
--
`npm ci`  help ? (it nukes /node_modules)
--
",michaelai0313,"
--
> `npm ci` help ? (it nukes /node_modules)

when I used npm installit failed all the time
I use yarn to install successfullybut build  failed all the time
--
",luoxiao123,"
--
i hava the same issue
--

--
0 info it worked if it ends with ok
1 verbose cli [
1 verbose cli   'C:\\Program Files\\nodejs\\node.exe',
1 verbose cli   'C:\\Users\\lx\\AppData\\Roaming\\npm\\node_modules\\npm\\bin\\npm-cli.js',
1 verbose cli   'run',
1 verbose cli   'build'
1 verbose cli ]
2 info using npm@6.14.8
3 info using node@v12.16.2
4 verbose run-script [ 'prebuild', 'build', 'postbuild' ]
5 info lifecycle superset@0.38.0~prebuild: superset@0.38.0
6 info lifecycle superset@0.38.0~build: superset@0.38.0
7 verbose lifecycle superset@0.38.0~build: unsafe-perm in lifecycle true
8 verbose lifecycle superset@0.38.0~build: PATH: C:\Users\lx\AppData\Roaming\npm\node_modules\npm\node_modules\npm-lifecycle\node-gyp-bin;E:\lx_workspace\pythonspace\workspace5\incubator-superset-0.38.0\superset-frontend\node_modules\.bin;E:\lx_workspace\xshell\;C:\Windows\System32;c:\windows\system32\WindowsPowershell\v1.0;E:\lx_workspace\jdk_dir\jdk1.8.0_191\bin;E:\lx_workspace\jdk_dir\jdk1.8.0_191\jre\bin;C:\Users\lx\Android\Sdk\platform-tools;C:\Program Files\Git\cmd;C:\Program Files\nodejs\;E:\lx_soft\Tesseract-OCR;E:\lx_workspace\pythonspace\sdk3.7.7;E:\lx_workspace\pythonspace\sdk3.7.7\Scripts;C:\Users\lx\AppData\Roaming\npm
9 verbose lifecycle superset@0.38.0~build: CWD: E:\lx_workspace\pythonspace\workspace5\incubator-superset-0.38.0\superset-frontend
10 silly lifecycle superset@0.38.0~build: Args: [
10 silly lifecycle   '/d /s /c',
10 silly lifecycle   'cross-env NODE_OPTIONS=--max_old_space_size=8192 NODE_ENV=production webpack --mode=production --colors'
10 silly lifecycle ]
11 silly lifecycle superset@0.38.0~build: Returned: code: 2  signal: null
12 info lifecycle superset@0.38.0~build: Failed to exec build script
13 verbose stack Error: superset@0.38.0 build: `cross-env NODE_OPTIONS=--max_old_space_size=8192 NODE_ENV=production webpack --mode=production --colors`
13 verbose stack Exit status 2
13 verbose stack     at EventEmitter.<anonymous> (C:\Users\lx\AppData\Roaming\npm\node_modules\npm\node_modules\npm-lifecycle\index.js:332:16)
13 verbose stack     at EventEmitter.emit (events.js:310:20)
13 verbose stack     at ChildProcess.<anonymous> (C:\Users\lx\AppData\Roaming\npm\node_modules\npm\node_modules\npm-lifecycle\lib\spawn.js:55:14)
13 verbose stack     at ChildProcess.emit (events.js:310:20)
13 verbose stack     at maybeClose (internal/child_process.js:1021:16)
13 verbose stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:286:5)
14 verbose pkgid superset@0.38.0
15 verbose cwd E:\lx_workspace\pythonspace\workspace5\incubator-superset-0.38.0\superset-frontend
16 verbose Windows_NT 6.1.7601
17 verbose argv ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Users\\lx\\AppData\\Roaming\\npm\\node_modules\\npm\\bin\\npm-cli.js"" ""run"" ""build""
18 verbose node v12.16.2
19 verbose npm  v6.14.8
20 error code ELIFECYCLE
21 error errno 2
22 error superset@0.38.0 build: `cross-env NODE_OPTIONS=--max_old_space_size=8192 NODE_ENV=production webpack --mode=production --colors`
22 error Exit status 2
23 error Failed at the superset@0.38.0 build script.
23 error This is probably not a problem with npm. There is likely additional logging output above.
24 verbose exit [ 2, true ]


![image](https://user-images.githubusercontent.com/13344955/100408292-999ef280-30a5-11eb-9910-8089c0fba000.png)

--
",eschutho,"
--
can you share the errors that you saw when you ran `npm install`? It looks like you don't have `babel/preset-react` installed correctly.
--
",lybtt,"
--
I hava the same issue in windows,  adding this might help.

![image](https://user-images.githubusercontent.com/38394058/106452635-2493d300-64c3-11eb-9e7c-69c6d03bf426.png)

--
"
10993,OPEN,Superset connection MS SQL Server failed.(WARNING:superset.views.core:Connection failed (pymssql.OperationalError)),bug; data:connect:mysql,2021-01-02 21:45:36 +0000 UTC,Zhiyuancheng,In progress,,"A clear and concise description of what the bug is.

### Expected results

I want to connection mssql use pymssql model(Install: pip3 install pymssql)

### Actual results

but it doesn't work.

my connection link is :
`'mssql+pymssql://username:password@db_host:1433/database'`

the error logs : 
```
WARNING:superset.views.core:Connection failed (pymssql.OperationalError) (20002, b'DB-Lib error message 20002, severity 9:\nAdaptive Server connection failed (10.xx.xx.xx)\n')
(Background on this error at: http://sqlalche.me/e/13/e3q8)
INFO:werkzeug:10.xx.xx.xx0 - - [22/Sep/2020 11:10:56] ""POST /superset/testconn HTTP/1.1"" 400 -
Connection failed (pymssql.OperationalError) (20002, b'DB-Lib error message 20002, severity 9:\nAdaptive Server connection failed (10.xx.xx.xx)\n')
(Background on this error at: http://sqlalche.me/e/13/e3q8)
WARNING:superset.views.core:Connection failed (pymssql.OperationalError) (20002, b'DB-Lib error message 20002, severity 9:\nAdaptive Server connection failed (10.xx.xx.xx)\n')
(Background on this error at: http://sqlalche.me/e/13/e3q8)
INFO:werkzeug:10.xx.xx.xx- - [22/Sep/2020 11:16:57] ""POST /superset/testconn HTTP/1.1"" 400 -
```

#### Screenshots

![image](https://user-images.githubusercontent.com/16459956/93841178-4be0c500-fcc5-11ea-9185-44ce24000f28.png)


#### How to reproduce the bug

1. Go to 'source--> databases-->add a new record -->Database [mssql] --> SQLAlchemy URI -->mssql+pymssql://username:password@db_host:1433/database'
2. Click on 'TEST CONNECTION'
3. Scroll down to You will see some error'
4. See error:ERROR: Connection failed, please check your connection settings.

### Environment

(please complete the following information):

- superset version: `superset version`
   `Superset 0.37.1`
- python version: `python --version`
  `Python 3.6.8`
- node.js version: `node -v`
- npm version: `npm -v`
- pip3 list 
```
(venv) [root@operation ~]# pip3 list 
DEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.
aiohttp (3.6.2)
alembic (1.4.3)
amqp (2.6.1)
apache-superset (0.37.1)
apispec (3.3.2)
async-timeout (3.0.1)
attrs (20.2.0)
Babel (2.8.0)
backoff (1.10.0)
billiard (3.6.3.0)
bitarray (1.5.3)
bleach (3.2.0)
Brotli (1.0.9)
cachelib (0.1.1)
celery (4.4.7)
cffi (1.14.3)
chardet (3.0.4)
click (7.1.2)
colorama (0.4.3)
contextlib2 (0.6.0.post1)
croniter (0.3.34)
cryptography (3.1)
dataclasses (0.6)
decorator (4.4.2)
defusedxml (0.6.0)
dnspython (2.0.0)
email-validator (1.1.1)
Flask (1.1.2)
Flask-AppBuilder (3.0.1)
Flask-Babel (1.0.0)
Flask-Caching (1.9.0)
Flask-Compress (1.5.0)
Flask-JWT-Extended (3.24.1)
Flask-Login (0.4.1)
Flask-Migrate (2.5.3)
Flask-OpenID (1.2.5)
Flask-SQLAlchemy (2.4.4)
flask-talisman (0.7.0)
Flask-WTF (0.14.3)
future (0.18.2)
geographiclib (1.50)
geopy (2.0.0)
gunicorn (20.0.4)
humanize (2.6.0)
idna (2.10)
idna-ssl (1.1.0)
impala (0.2)
importlib-metadata (1.7.0)
impyla (0.16.2)
isodate (0.6.0)
itsdangerous (1.1.0)
Jinja2 (2.11.2)
jsonschema (3.2.0)
kombu (4.6.11)
Mako (1.1.3)
Markdown (3.2.2)
MarkupSafe (1.1.1)
marshmallow (3.8.0)
marshmallow-enum (1.5.1)
marshmallow-sqlalchemy (0.23.1)
msgpack (1.0.0)
multidict (4.7.6)
mysqlclient (2.0.1)
natsort (7.0.1)
numpy (1.19.2)
packaging (20.4)
pandas (1.0.5)
parsedatetime (2.6)
pathlib2 (2.3.5)
pip (9.0.3)
ply (3.11)
polyline (1.4.0)
prison (0.1.3)
py (1.9.0)
pyarrow (0.17.1)
pycparser (2.20)
PyHive (0.6.3)
PyJWT (1.7.1)
pymssql (2.1.5)
pyodbc (4.0.30)
pyparsing (2.4.7)
pyrsistent (0.17.3)
python-dateutil (2.8.1)
python-dotenv (0.14.0)
python-editor (1.0.4)
python-geohash (0.8.5)
python3-openid (3.2.0)
pytz (2020.1)
PyYAML (5.3.1)
retry (0.9.2)
sasl (0.2.1)
selenium (3.141.0)
setuptools (39.2.0)
simplejson (3.17.2)
six (1.15.0)
slackclient (2.5.0)
SQLAlchemy (1.3.19)
sqlalchemy-exasol (2.2.0)
SQLAlchemy-Utils (0.36.8)
sqlparse (0.3.0)
thrift (0.13.0)
thrift-sasl (0.4.2)
thriftpy2 (0.4.11)
typing-extensions (3.7.4.3)
urllib3 (1.25.10)
vine (1.3.0)
webencodings (0.5.1)
Werkzeug (1.0.1)
WTForms (2.3.3)
WTForms-JSON (0.3.3)
yarl (1.5.1)
zipp (3.1.0)

```

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [* ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ *] I have reproduced the issue with at least the latest released version of superset.
- [ *] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

I found the superset Documentation suggestion to use pymssql too : 
https://superset.incubator.apache.org/docs/databases/sql-server
![image](https://user-images.githubusercontent.com/16459956/93841970-d7f3ec00-fcc7-11ea-9857-f8ca0b72f2c1.png)
I used the suggestion and  test but it still doesn't work:
mssql+pymssql://UserName@DB:Password@DB_Host:1433/TestSchema

**Please help me how to resolve this problem ? Have any suggestion pleast just let me know , thanks.**


",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.65. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",eugeniamz,"
--
Hi, I have a similar issue when I was trying MSSQL in Azure in Preset, and I documented what [worked](https://docs.preset.io/docs/en/azure-microsoft-sql-server), ignore the Preset IP and see if the extra parameters help you. 
--
",Zhiyuancheng,"
--
> Hi, I have a similar issue when I was trying MSSQL in Azure in Preset, and I documented what [worked](https://docs.preset.io/docs/en/azure-microsoft-sql-server), ignore the Preset IP and see if the extra parameters help you.

Thanks @eugeniamz  I have solved it , use pyodbc instead of pymssql . 

- apt-get install unixODBC
- pip install pyodbc

eg :
![image](https://user-images.githubusercontent.com/16459956/94358118-46beb400-00d1-11eb-8787-9510b14d21a2.png)

![image](https://user-images.githubusercontent.com/16459956/94358134-5ccc7480-00d1-11eb-8a60-d08242a681b9.png)

and you can refer to MS related link introduce:
https://docs.microsoft.com/en-us/sql/connect/python/pyodbc/step-1-configure-development-environment-for-pyodbc-python-development?view=sql-server-ver15
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,
10960,OPEN,Error in dual_line chart scale group by months,bug; viz:chart-line,2021-01-02 18:22:10 +0000 UTC,Provoo,In progress,,"How to modify the scale on dual_line chart with time, I have this issue:
I use count and sum 2 diferent variables, de X axis is the time variable, group by month but the scale appears with 21 days of diference: [https://apache-superset.slack.com/files/U01APJV08VC/F01AWHHD6CT/captura_de_pantalla_de_2020-09-17_09-54-21.png](url)

",,,junlincc,"
--
hi Provoo, I'm not able to view the link, can you reproduce the issue? thanks. 
--

--
Thanks for reporting, we will get to this issue early next week and get back to you once it's fixed. 
--
",Provoo,"
--
![Captura de pantalla de 2020-09-28 15-30-28](https://user-images.githubusercontent.com/4282730/94482973-c31fc880-019f-11eb-842f-b66efcdef0c2.png)

Sorry something happend with the link aslo i think is releated with this issue: #11088 
--
",,,,,,,,
10953,OPEN,[Feature Request] Large CSV Dumps,.pinned; enhancement:request,2021-01-26 18:25:19 +0000 UTC,kaplanmaxe,Opened,,"Hey, all, I've talked to the Preset team a few times on this but wanted to formally document it here and have a place to discuss.

We run a Superset instance with 100+ users and one of the biggest issues users complain about **by far** is the lack of an ability to produce large CSV dumps. It really pains me to hear this as trust me, I cringe every time I hear this as well, however I've started to understand the use case a lot more. A lot of times users want to:

- Test POCs of integrations of third parties where they need to upload a CSV to an sftp bucket or something
- Export data to move it into another source
- Let's face it... some accountants are always going to use CSVs

**How we handle this today**

We have tables with well over 10 billion rows that are super wide and just increasing the row limit in Supeset doesn't work obviously because the browser can't handle it (not that a user is actually trying to export 10 billion rows, but 500k for example is very common).

We actually run Superset and metabase mainly because of the CSV issue. Metabase handles CSV dumps much better than superset atm. However druid is a very large part of our stack and metabase doesn't support druid SQL which makes the metabase feature very limiting as our end users don't want to learn druid native queries.

**Why this would be a really useful feature for superset**

You won't find a modern BI tool that's good for this feature, and for some reason, most OSS tools don't think about this use case. You won't find an OSS BI tool for example that's good for large Druid or Presto csv dumps (please correct me if I'm wrong :) ). This would be a really cool feature for Superset, especially since it's origins come from Druid.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.78. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",frafra,"
--
Similar to #10331.
--
",,,,,,
10945,OPEN,Semicolon error in Explore,bug; good first issue; viz:explore:dataset,2021-02-20 07:32:51 +0000 UTC,tooptoop4,In progress,,"select * from table;

that query (with semicolon) runs fine in SQLLab but after clicking Explore on the results the Chart page gives an error about semicolon not allowed",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.96. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",nytai,"
--
@tooptoop4 I just tried this and was unable to repro. Can you confirm this is still an issue using the latest version of master? 
--
",mistercrunch,"
--
@tooptoop4 which version are you on?
--
",tooptoop4,"
--
0.36
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",amitmiran137,"
--
@junlincc could we get a reproduction of this issue in a video?
--
"
10940,OPEN,SQL Lab does not show table names in the dropdown box,bug; need:more-info,2021-02-20 07:47:38 +0000 UTC,rimolive,In progress,,"A clear and concise description of what the bug is.
While selecting Database, schema, and table to inspect the table structure, I noticed that the table dropdown box only shows the schema name, not the table name. If I try selecting any of the options, I get an `An error occurred while fetching table metadata` error.

### Expected results

I can see the columns that belong to the table

### Actual results

An `An error occurred while fetching table metadata` error message appear

#### Screenshots

https://imgur.com/tfSfvyg

#### How to reproduce the bug

1. Go to 'SQL Lab > SQL Editor'
2. Click on any Database in the dropdown box
3. After refreshing the schema dropdown box, select any schema with tables
4. Check the tables dropdown box. It should appear only the schema name, not the tables name

### Environment

- superset version: `0.37.1`
- python version: `Python 3.6.12`
- node.js version: N/A
- npm version: N/A

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

N/A
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.96. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",rimolive,"
--
![](https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png)

--

--
@mistercrunch The engine in question is Hive.
--

--
pyhive version is 0.6.1. As for hive version it's a bit more complicated. :)

Actually, we use Spark to enable a Hiveserver2 special implementation to run queries against it. Currently Spark 2.4.3 is being used but we have plans to use Spark 3.0.0.
--

--
Tracking requests from Superset UI, I see that when you select a Database the following URL is requested:
```
http://<superset-hostname><:superset_port>/superset/schemas/1/false/
```
Which returns a JSON with the following contents and populates the Schema selector:
```
{""schemas"": [""main"",""examples""]} # This is just an example JSON output returned
```
When you select any schema, the following URL is requested:
```
http://<superset-hostname><:superset_port>/superset/tables/1/main/undefined/false/
```
This undefined in the URL clearly indicates that the Schema selector is not building the component correctly.

I'm still debugging the code and if I have a fix I'll cook a PR for it. Stay tuned!

Edited: hmm, so undefined seems acceptable but anyways the code that responds for that URL seems the issue. Still investigating.
--

--
It is a bug in pyhive:

https://github.com/dropbox/PyHive/blob/master/pyhive/sqlalchemy_hive.py#L364

That line should not return `row[0]` since the result will be the schema name, not the table name. I'm going to propose a fix in pyhive.

Meanwhile, can this issue keep opened until pyhive team accept and fixes in latest releases?

Edit: Problem is even worse. See https://github.com/dropbox/PyHive/issues/146
I'll see how to workaround this.
--

--
Unfortunately, I did not have an answer from Spark community but definitely this is a compatibility issue between Spark HiveServer2 implementation and Hadoop one. For now, I have no idea how this can be fixed.
--
",mistercrunch,"
--
What database engine is this?
--
",bkyryliuk,"
--
Hm, can't repro on the latest master for both hive & presto.
@rimolive maybe provide more info:
1. what hive version
2. what pyhive version

--
",fan,"
--
> pyhive version is 0.6.1. As for hive version it's a bit more complicated. :)
> 
> Actually, we use Spark to enable a Hiveserver2 special implementation to run queries against it. Currently Spark 2.4.3 is being used but we have plans to use Spark 3.0.0.

I also find this question when I use superset to connect to SparkThrift server.
My  connection url is  hive://ip:port/default?auth=KERBEROS&kerberos_service_name=hive  . 
Spark version is 2.4.0, How can I solve this problem?
--

--
> Hm, can't repro on the latest master for both hive & presto.
> @rimolive maybe provide more info:
> 
> 1. what hive version
> 2. what pyhive version

I also have the similar problem.  hive  connect is ok,  but when  connect to the sparkThrifttable scheme cann't appear table name, only  schema name and the schema_name's number is euqal to talbe_number. eg. ods database has 100 tables, now ,you will see 100 ods in table schema in superset's sqllab
url is hive://ip:10001/default?auth=KERBEROS&kerberos_service_name=hive 
hive version is 2.1.1+cdh6.2.0
pyhive version is 0.6.3
--
",junlincc,"
--
@rimolive Ricardo, any updates?
--
"
10927,OPEN,the picture is on the lower part of the web page,bug; cant-reproduce,2021-01-02 18:26:58 +0000 UTC,appleyuchi,In progress,,"A clear and concise description of what the bug is.

### Expected results
I want the picture to be in the center part of the web page.

### Actual results



#### Screenshots

If applicable, add screenshots to help explain your problem.
![bug](https://user-images.githubusercontent.com/27613483/93437973-381d1380-f8ff-11ea-807b-f81468755925.png)

#### How to reproduce the bug
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.88. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",appleyuchi,"
--
if I press `Ctrl+0`,no luck...
--

--
![bug](https://user-images.githubusercontent.com/27613483/93439399-17ee5400-f901-11ea-8279-3872f0cc44dc.png)

and words overlap with each other.
--

--
Thanks for your replies,
but could you developers set it to center of the page automatically?
Much Thanks for your hard work~~~
--

--
> Can't reproduce, what version of Superset and Chrome is this?

The Chrome is in Linux Desktop
Ubuntu Linux 20.04

The effect is in:
http://49.235.108.13:5000/superset/dashboard/2/

it's deployed in single node,so it's slow,
wait for 25 seconds please
--
",zhaoyongjie,"
--
Hi, @appleyuchi. have a trick, can do it.
1. append a ""&standalone=true"" parameter to explore url
2. zoom in / zoom out by browser can apply to sankey diagram
--
",mistercrunch,"
--
Can't reproduce, what version of Superset and Chrome is this?
--
",,,,
10917,OPEN,"style: missing padding in VizTypeControl on ""New Chart"" page",assigned:polidea; bug; bug:cosmetic,2021-04-07 18:33:52 +0000 UTC,ktmud,In progress,,"Missing some padding in the viz type selector on ""Add new chart"" page:

### Actual results

![image](https://user-images.githubusercontent.com/335541/93382265-bf428c80-f816-11ea-8c3d-fcaacd829300.png)

### Expected results

The viz type control should look the same as on Explore page:

![image](https://user-images.githubusercontent.com/335541/93382336-d8e3d400-f816-11ea-83e5-0c56dd4ff9f0.png)

#### Screenshots

See above

#### How to reproduce the bug

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

### Environment

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`
- npm version: `npm -v`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.91. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",junlincc,"
--
@@kgabryje 
--

--
not sure which pr fixed it, but it's fixed. closing the issue
--
",ktmud,"
--
This issue is back
--
",,,,,,
10904,OPEN,[BUG] deck.gl scatterplot has black checkboxes instead of colored ones,bug; inactive,2020-11-26 22:20:19 +0000 UTC,s4ke,In progress,,"On Chrome (Linux) Version 85.0.4183.83 (Official Build) (64-bit) the deck.gl scatterplot legend displays as black checkboxes instead of coloured ones. Firefox is unaffected.

### Expected results

Legend should have colored checkboxes instead of black checkboxes

### Actual results

Legend has black checkboxes

#### Screenshots

![image](https://user-images.githubusercontent.com/719760/93306201-d7f37800-f7ff-11ea-86de-6247dc201892.png)

#### How to reproduce the bug

1. Create a deck.gl scatterplot
2. Set Legend position to anything but none
3. Display data in the scatterplot

### Environment

- superset version: `0.37.0` (from amancevice/superset)
- python version: `3.6.12`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset. *(on 0.37.0)*
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.99. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",s4ke,"
--
I just checked on a 0.36.0 installation and it has the same checkbox issues on Chrome.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10857,OPEN,Time-series bar/line chart - Cannot select TOP/LIMIT/MOST RECENT rows,bug; inactive,2020-11-26 22:20:24 +0000 UTC,PowerPlop,Opened,,"A clear and concise description of what the bug is.

### Expected results

When using the ""Sort By"" and ""Row Limit"" option, I expect to receive the top x rows. (e.g. I want to select the 250 most recent records).


### Actual results

The line/bar chart viz keeps adding the default sorting by SUM(metric); Other sorting options are added in the subquery, which have no impact on the final results.

#### Screenshots

Default ordering with row limit:

![image](https://user-images.githubusercontent.com/2457335/93090850-a4004180-f69d-11ea-87c1-f7faa0909afe.png)

When adding a timestart sort, an additional subquery is added:

![image](https://user-images.githubusercontent.com/2457335/93091126-00636100-f69e-11ea-84bf-cb8351602186.png)



#### How to reproduce the bug

1. create a line chart with datasource of + 250 records
2. add row limit
3. inspect created query and results

### Environment

Docker superset 0.37

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x ] I have reproduced the issue with at least the latest released version of superset.
- [x ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

The goal is to have max of 250 records returned. If there is any other option I can use, please let me know.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.89. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10828,OPEN,Add to Line Chart ability to select x-axis other than date field; or add a new Line Chart that is not time series,enhancement:committed; viz:echarts,2021-02-15 06:14:43 +0000 UTC,eugeniamz,In progress,,"At the moment, Line chart is a time series chart and X-Axis always is the date in the time section 

![image](https://user-images.githubusercontent.com/58375897/92760106-611d3180-f35e-11ea-9ed0-81f2b712a8de.png)
 
It would be a great addition to have Line Chart that allows X-axis to be any filed so can be used to compare trends despite without link with dates. 

Use case at the moment, I  want to compare different customers' growth by the month of Customer Acquisition date but all of them have a different start dates. 

For example this dataset : 
![image](https://user-images.githubusercontent.com/58375897/92762932-105b0800-f361-11ea-8b52-e06594687d42.png)

This is the best chart that I can do with the Line Chart 
![image](https://user-images.githubusercontent.com/58375897/92763068-2e286d00-f361-11ea-9fef-c0a1c1663a23.png)

If I create a metric as the:  datediff('month', start_date,sales_date) I would like to be able to do a chart like this. 
![image](https://user-images.githubusercontent.com/58375897/92761791-0258b780-f360-11ea-9c8f-81b594343e1a.png)

It is possible with Bars but it is harder to see trends and do comparison with the bars, in addition, bars get even more confused when the field to breakdown has many distinct values 
![image](https://user-images.githubusercontent.com/58375897/92763917-f40b9b00-f361-11ea-8db8-f957977e88ea.png)
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",bkyryliuk,"
--
dropboxers are requesting a similar feature
--
",junlincc,"
--
will implement numeric and categorical line chart with new echarts soon
--
",eugeniamz,"
--
@junlincc  any ETD?
--
",amitmiran137,"
--
this chart is support the above and more
https://5fec4c81935a8c002151e85f-nfsmejbgts.chromatic.com/?path=/story/plugins-composed-chart--all-types
https://github.com/nielsen-oss/superset-viz-plugins/
--
",,
10817,OPEN,Jinja filter_values function not working in charts used as annotation in another chart,bug,2020-12-09 07:10:49 +0000 UTC,oashton,In progress,,"### Expected results

the `filter_values` function should work in a datasource query used in a chart, even if the chart is used as a annotation chart

### Actual results

if a chart is used as a annotation in another chart, the `filter_values` function used inside the datasource query doesn't work, it doesn't return any value. Important to note: a filter value is selected in the dashboard.

#### How to reproduce the bug

1. Create a line chart to be used as a annotation in another chart. The datasource query should use the `filter_values` function
2. Create another line chart, add the previosly created line chart as a annotation
3. Add the line chart created in the previous step in a dashboard, add a filter box with the same field used as a parameter in the `filter_values` of the Step 1
4. The annotation doesn't show up as expected

### Environment

- superset version: `superset version`
Superset 0.37.0:
Python 3.6.9
Flask 1.1.2
Werkzeug 1.0.1
- python version: `python --version`
Python: 3.6.9
- node.js version: `node -v`
node: v12.10.0
- npm version: `npm -v`
npm: 6.10.3

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

I changed the following the line `if (!isDashboardRequest && fd) {` (line 254) in `chartAction.js` to `if (fd !== null) {` and it solved the problem but I'm not sure about the impact in other behaviours
![image](https://user-images.githubusercontent.com/6887197/92616252-9dc82a80-f283-11ea-953d-02b973d19f1e.png)


",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.95. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",lilila,"
--
@oashton , does this change fix the behaviour in explore chart mode for you? Do you know how to change this? 
Considering your question on the impact, it seems  (#10115) this behaviour was chosen on purpose. 

I did not get the point of  having no control on annotation.  

For me if you want to avoid this problem  (mentioned in #10115)

` "" This will cause chart and annotation layer both have same query, so we can't compare filtered data vs original annotation layer."" `

You can simply create a new view/table for this annotation using fields name different from the ones you use in your filters. 

@graceguo-supercat Any clue on this? Thank you 

--

--
Dear @oashton , 

I wrote this lite patch to get annotation considering extra_filters in Explore view as well. 
I add these lines at line 260 in your screenshot 

```
    //case for explore view (have extra params given to annotation)       
    var new_extra_from_adhoc = [];
    if (!fd.extra_filters && fd.adhoc_filters){
        for(var i=0; i<fd.adhoc_filters.length; i++){
            if (fd.adhoc_filters[i]['isExtra']){
                    var new_extra = {""col"":fd.adhoc_filters[i]['subject'],
                                     ""op"": fd.adhoc_filters[i]['operator'],
                                     ""val"": fd.adhoc_filters[i]['comparator']};
                    new_extra_from_adhoc.push(new_extra);}
            }
        sliceFormData.extra_filters = new_extra_from_adhoc;
    }
```

@villebro , I don't know if you remember but we discussed this point. 
The problem was that in explore mode, extra_filters are already merge with adhoc filters so they were not passed to the annotations. The code snippet (sorry for the quality I never code in Javascript) simply recreates the extra_filters from the adhoc_filters available in Explore mode 
--
",oashton,"
--
@lilila , it doesn't work in explore mode, right now I don't know how to change it.
Thanks for the reference to the issue #10115... so the behaviour is by design.

I agree with you, I don't get why lose the possibility to add dynamic behaviours to the annotation. Let's see what @graceguo-supercat says about this.

cc @camilo-uptime, check this thread, this is why we weren't receiving the filter values in the annotation line chart from the dashboard
--
",villebro,"
--
Over the holidays I'll try to take a look at how this could be properly integrated into the annotation requests.
--
",,,,
10812,OPEN,Uncaught Error: Target container is not a DOM element.,bug; inactive,2020-11-26 22:20:03 +0000 UTC,badman-rodriguez,Opened,,"When running `npm run dev-server` this route:
`http://localhost:9000/chart/add` Leads to a React JS error: Uncaught Error: Target container is not a DOM element.

Which indicates that the Dom isnt available yet for the React Scripts to kick off. 
This only happens when kicking off `webpack-dev-server`

It works just fine when running the app without webpack-dev-server.... so it must be a config issue, but not sure what it could be.

Any insight would be great on this.

Node: 12.14.1
typescript 3.8.3
Superset:  0.37.0 as of (2020/08/01 06:59 +00:00)
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.60. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",thisishantzz,"
--
I too am getting this error. It does not happen all the time. I encounter this only when try to add a chart (on the page /chart/add). Adding a database, table or dashboard works fine. 
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10808,OPEN,No sandbox.js ? chart tooltips,bug; inactive,2020-12-25 17:55:31 +0000 UTC,eried,Opened,,"I am trying to add tooltips to a deck.gl Scatterplot, but I cannot find any documentation and the link for sandbox.js is broken

### Expected results
Any type of documentation on how to add tooltips with ""Extra data for JS""

### Actual results
No documentation, broken link to the sourcecode

#### Screenshots
![image](https://user-images.githubusercontent.com/1091420/92362378-375ed300-f0f0-11ea-8e95-02adc0ec4f25.png)

#### How to reproduce the bug

1. Add a chart using deck.gl Scatterplot
2. Edit the chart
3. In Advanced -> Edit javascript in modal
4. See error
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.69. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",jtotoole,"
--
Broken link (https://github.com/apache/incubator-superset/blob/master/superset/assets/src/modules/sandbox.js)  is here: https://github.com/apache/incubator-superset/blob/7f1012360a8c0dd6860e80f852026017c09b746c/superset-frontend/src/explore/controlPanels/Shared_DeckGL.jsx#L33-L34

I don't see a `sandbox.js` file anywhere in the project at this point, so I'm not sure what that should be corrected to.

This should probably be two separate issues: one for the broken link and one for the lack of documentation on user-defined functions for Deck.gl visualizations, which has also been mentioned here:

https://github.com/apache/incubator-superset/issues/7174
https://github.com/apache/incubator-superset/issues/6020
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10807,OPEN,altered column type not saved switch back to the default one,bug; inactive,2020-12-25 17:55:38 +0000 UTC,Asturias-sam,Opened,,"A clear and concise description of what the bug is.

### Expected results

 can alter columns' type when needed.

### Actual results

column types are not altered 

what actually happens.

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Alter field type of one column in the List Columns and click SAVE BUTTON;
2  Check the type is really altered;
3. Switch to the Detail tab page and click SAVE BUTTON to explore charts;
4. The altered type restores the earlier version.

### Environment

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`
- npm version: `npm -v`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.87. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",youwanttoeatpeach,"
--
superset
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10792,OPEN,Error on Delete. sqlalchemy.exc.ResourceClosedError: This transaction is closed,bug; inactive; need:more-info,2020-11-26 22:20:27 +0000 UTC,nomulex,Opened,,"I used the official superset helm charts [here](https://github.com/apache/incubator-superset/tree/master/helm/superset)  to  install superset. But I get an error on save. 


### Expected results

Superset to save results with no error. 

### Actual results

`ERROR:flask_appbuilder.models.sqla.interface:Delete record error: This transaction is closed
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/session.py"", line 2597, in _flush
    flush_context.execute()
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/unitofwork.py"", line 422, in execute
    rec.execute(self)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/unitofwork.py"", line 624, in execute
    uow,
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py"", line 326, in delete_obj
    _organize_states_for_delete(base_mapper, states, uowtransaction)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py"", line 463, in _organize_states_for_delete
    base_mapper, uowtransaction, states
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/persistence.py"", line 1602, in _connections_for_states
    connection = uowtransaction.transaction.connection(base_mapper)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/session.py"", line 311, in connection
    self._assert_active()
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/session.py"", line 304, in _assert_active
    raise sa_exc.ResourceClosedError(closed_msg)
sqlalchemy.exc.ResourceClosedError: This transaction is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/flask_appbuilder/models/sqla/interface.py"", line 634, in delete
    self.session.commit()
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/scoping.py"", line 162, in do
    return getattr(self.registry(), name)(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/session.py"", line 1036, in commit
    self.transaction.commit()
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/session.py"", line 503, in commit
    self._prepare_impl()
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/session.py"", line 482, in _prepare_impl
    self.session.flush()
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/session.py"", line 2496, in flush
    self._flush(objects)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/session.py"", line 2637, in _flush
    transaction.rollback(_capture_exception=True)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py"", line 81, in __exit__
    compat.raise_(value, with_traceback=traceback)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_
    raise exception
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/session.py"", line 2637, in _flush
    transaction.rollback(_capture_exception=True)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/session.py"", line 519, in rollback
    self._assert_active(prepared_ok=True, rollback_ok=True)
  File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/session.py"", line 304, in _assert_active
    raise sa_exc.ResourceClosedError(closed_msg)
sqlalchemy.exc.ResourceClosedError: This transaction is closed
`

#### Screenshots

![image](https://user-images.githubusercontent.com/3070004/92218867-6c75e600-eea2-11ea-872c-242e70dd29c4.png)




### Environment

(please complete the following information):

- superset version: `superset version`  Superset 0.999.0dev
- python version: `python --version`   Python 3.6.9
- node.js version: `node -v` none in [this](https://hub.docker.com/r/preset/superset) container
- npm version: `npm -v` none in [this](https://hub.docker.com/r/preset/superset) container

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

none.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.76. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",dpgaspar,"
--
Could you please fill out a detailed description of the steps to reproduce this issue, also add a chunk of log prior to the error itself.

--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10779,OPEN,Dashboard CSS keeps disappearing for v0.37,bug,2020-12-05 16:35:54 +0000 UTC,stevensuting,In progress,,"### Expected results

Once CSS is written on the Dashboard CSS feature, it should remain persistent. 

### Actual results

The CSS once applied reflects on the dashboard for the current session. In the subsequent login the CSS script disappears. The number of sessions are arbitrary. Sometimes it happens after one session, sometimes after several.

#### Screenshots
Before
![Before](https://user-images.githubusercontent.com/8875448/92116491-874c4a00-ee11-11ea-834d-e3544ea11c9b.png)
After
![After](https://user-images.githubusercontent.com/8875448/92116477-84515980-ee11-11ea-8ac6-958c8b5c6735.png)



#### How to reproduce the bug

1. Go to Dashboards -> Edit Record -> Scroll to CSS text box
2. Paste some CSS, like
```
.grid-container{
    margin: 18px;
}
.dashboard-header {
display: none;
}
```

3. Save and go to Dashboard. 
4. You will see the CSS changes. Now log out and log back in, CSS disappears.

### Environment
- superset version: `0.37`
- python version: `3.6.9`


### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [ Y] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ Y] I have reproduced the issue with at least the latest released version of superset.
- [ Y] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.99. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",junlincc,"
--
 @rusackas 
--
",gtg472b,"
--
I think this has already been fixed. I had the issue a couple of months ago, but it's no longer a problem for me.
--
",willbarrett,"
--
I think this is related to https://github.com/apache/incubator-superset/issues/10655
--

--
Some members of the community are working on a refactor of the related code, there are a number of related data handling issues. We do not have an ETA for completion or release, but the problem is being actively worked on.
--
",liangsieng,"
--
I also facing the same problem. What is the solution for this?
--
",squalou,"
--
Same issue here, 0.37.2


--

--
update : appears as fixed for me in 0.38.0, not tested all use cases yet but edit / add css / does persist things
--
"
10764,OPEN,Error adding DB2/400 iSeries database,bug; inactive,2020-12-25 21:55:19 +0000 UTC,frbarthe,In progress,,"When i add new iSeries database to superset the test work fine. but when i click on save, the following error appears .

![Capture decran 2020-09-02 a 11 08 03](https://user-images.githubusercontent.com/46246371/91962298-9eedda80-ed0c-11ea-8540-d7d4dc9a1ce0.png)


my config is 
------------

CentOS Linux release 7.6.1810

Superset               0.37.0
pyodbc                 4.0.30
ibm-db                 3.0.2
ibm-db-sa            0.3.5
iaccess                 1.1.0.14

best regards

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.83. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",willbarrett,"
--
That's a really interesting error @frbarthe ! Are you able to add other types of databases correctly? Is this isolated just to  the DB2/400 iSeries?
--
",frbarthe,"
--
Yes i can add other kind of datasource without any trouble.
I think this bug https://github.com/apache/incubator-superset/issues/5614 is the same as me.

I suspect there is a problme when superset try to list the table inside the ISeries schema
--

--
If you mean select from a dummy table like in Oracle (dual). In DB2/400 you must query like this
Select 1 from sysibm.sysdummy1

only ""select 1"" didn't work
 
--
",dpgaspar,"
--
Note: Could be around DB2/OS400 not supporting `SELECT 1`
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,
10756,OPEN,superset-logo@2x.png static asset missing from pip distribution,bug,2021-04-09 11:13:10 +0000 UTC,brokenjacobs,In progress,,"The logo is broken in the install from pip currently. Version 0.37.0

### Expected results
The log displays

### Actual results
You get a broken image.

#### Screenshots
![image](https://user-images.githubusercontent.com/3159463/91906705-e1a8a780-ec65-11ea-9073-6be58e74c75c.png)



#### How to reproduce the bug
Install superset and load it

### Environment
I downloaded the image from a site online and placed it in the static assets directory... and then I had a logo.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.94. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",brokenjacobs,"
--
Dead project?
--
",amitmiran137,"
--
could you check if this is still happening on a newer version?
--
",,,,
10749,OPEN,Superset Permissions,enhancement:committed,2021-01-31 19:57:53 +0000 UTC,Asturias-sam,Opened,,"For Gamma Users we have given SQL LAB access and access to all the databases, so we have give the following roles :
`all_database access on all_database access , all_datasource access on all_datasource access, all_query access on all query access`

1. Now Gamma users can see the charts created by others but they can't see the dashboard created by others Is this expected ? 
2. Can we have some permissions to see dashboard from others as in the case of charts  ?
3. Can we restrict gamma users to see charts from other Gamma users with access to all data sources ?

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.82. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",amitmiran137,"
--
Here is a proposal on Dashboard access permission  that might help
https://github.com/apache/incubator-superset/issues/10408
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10737,OPEN,Sort Option disappears/removed when using multiple group by columns in PivotTable Viz,bug; inactive,2020-12-25 21:55:28 +0000 UTC,Neel-rishabhsoft,Opened,,"With adding more then one group by columns in Pivot Table viz, the sort button in columns is getting removed. 

### Expected results
The Sort option needs to be available.


### Actual results

The sort button disappears.
With one group by column: 
![image](https://user-images.githubusercontent.com/60099288/91724907-98038280-ebbb-11ea-842b-c678d633beb6.png)

With multiple group by columns: 
![image](https://user-images.githubusercontent.com/60099288/91725003-b5d0e780-ebbb-11ea-829f-49f60b36fa83.png)

Also, we are looking for a way to merge/eliminate the multiple headers created while using Pivot Table viz. 
Please let me know if there is anything that above mentioned issues can be resolved. 

Thanks.


### Environment

(please complete the following information):

- superset version: `superset version 0.36`
- python version: `python --version 3.6`


### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.95. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10734,OPEN,Event to detect Superset Viz is loading/reloading & is loaded after filters applied/refreshed in custom JS used.,enhancement:request; inactive,2020-12-25 21:55:29 +0000 UTC,Neel-rishabhsoft,Opened,,"I have used a custom Javascript file to customise superset for client requirement which I have added in basic.html. But I am struggling to keep the changes on chart refresh as the charts get refreshed from superset side without any event on the windows. I have considered getting a on click/change event of filters but since everything is getting updated dynamically with react I am not able to do that either. 
Can anyone please suggest anything that I can use to detect the viz getting refreshed on my JS file.

Thanks.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.57. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10726,OPEN,Automatically rename _col columns; to fix user flow SQLLab -> Explore resulting in error,enhancement:request; global:error; sql_lab,2021-02-23 04:22:07 +0000 UTC,zuzana-vej,Opened,,"**Is your feature request related to a problem? Please describe.**
Currently when writing a query with an aggregation in SQL Lab (e.g. `select avg(my_column) from my_table`) the unnamed column is renamed as `_col0`, When clicking Explore to go to Chart Explore, the Presto query will fail with `presto error: line 3:8: Column '_col0' cannot be resolved`. 

This causes the user to either think it's a **BUG** in Superset, or user realizes they need to go back to Explore, rename the column (`select avg(my_column) AS avg_my_column from my_table` , wait for the query to run again, and click ""Explore again"". 

So the current user flow is:
SQL Lab --> Explore --> User sees error --> back to SQL Lab --> Explore & it hopefully works

Instead of:
SQL Lab --> Explore

**Describe the solution you'd like**
There are two options really:
(1) Renaming the column automatically, so that Explore doesn't result in error. This isn't necessary when displaying results in SQL Lab, only when user clicks on ""Explore"".

In an ideal case, the columns return as `_col0` would be renamed based on some logic - for example concatenation of the aggregate function and the column (so `count(my_column)` can be named `count_my_column`, `avg(load_time)` could be `avg_load_time`. 

(2) Better Error so that user can fix it before going to Explore:
An alternative solution is to display an error message when user is trying to click explore and not allow user to go to Explore, forcing the user to update the query to add the column name.

**Describe alternatives you've considered**
Current state isn't acceptable (looks like a bug to end user). Two options described in above section.

**Additional context**
Step 1:
![sqllab](https://user-images.githubusercontent.com/61221714/91627160-42846700-e96a-11ea-8ff5-8cf04597f359.png)

Step 2 --> Error:
![explore](https://user-images.githubusercontent.com/61221714/91627156-39939580-e96a-11ea-8159-52449e191884.png)


",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.70. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",zuzana,"
--
Here screenshots to different existing flow (similar use case) when user tries to click explore, after joining and having multiple timestamp type of columns in the result, and receives error warning. Option (2) from solution options would align with this experience - providing warning and not letting user proceed to Explore. 

1. Situation: User joins two tables and has two timestamp type columns in result
![sqllab02](https://user-images.githubusercontent.com/61221714/91627355-ad826d80-e96b-11ea-8520-d50380f420b4.png)

2. Onclick(Explore) user receives this notification.
![warning02](https://user-images.githubusercontent.com/61221714/91627357-aeb39a80-e96b-11ea-9879-671fdbcb9713.png)





--
",junlincc,"
--
 error
--
",willbarrett,"
--
@mistercrunch what do you think of this?
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,
10713,OPEN,Annotation don't update when filter change,bug; inactive,2020-12-25 21:55:31 +0000 UTC,Leandro-GPIN,Opened,,"When select a filter region the annotation don't update. 

We selected for example: North America. 

### Expected results

Chart: ""Growth Rate and Annotation Region"": 
- Bermuda 
- Canada 
- United States 
- Annotation, North America

### Actual results

Chart: ""Growth Rate and Annotation Region"": 
- Bermuda 
- Canada 
- United States 
- Annotation, North America
- Annotation, East Asia & Pacific
- Annotation, Europe & Central Asia
- Annotation, Latin America & caribbean
- Annotation, Middle East & North Africa
- Annotation, South Asia
- Annotation, Sub-Saharan Africa

#### Screenshots

![annotation_filter_error16](https://user-images.githubusercontent.com/19553149/91585167-1517c900-e92a-11ea-814a-b847b2b95be8.gif)


#### How to reproduce the bug

1. Create a new chart line with region group.
2. Add the new chart as annotation in Growth Rate chart.
3. Save as ""Growth Rate and Annotation Region"".
4. Select region in Region Filter .
5. See error in  ""Growth Rate and Annotation Region"".
6. Annotation don't update when filter change.

####  Create a new chart line with region group

![chart_growth_rate_region](https://user-images.githubusercontent.com/19553149/91585088-f6b1cd80-e929-11ea-814a-e98f405889b5.png)

#### Add the new chart as annotation in Growth Rate chart.
![chart_growth_rate_and_annotation_region_config](https://user-images.githubusercontent.com/19553149/91571725-ce6fa200-e91c-11ea-9585-4692b7b870b3.png)


### Environment

- superset version: 0.37.0 with docker
- python version: 3.7
- node.js version: v14.8.0

### Checklist

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.


",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.89. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10711,OPEN,sqlalchemy.exc.ProgrammingError when installing via docker-compose up on Windows as of Commit 3e374dab0,bug,2021-03-05 22:58:46 +0000 UTC,alittlesliceoftom,Opened,,"A clear and concise description of what the bug is.

### Expected results

`docker-compose up` as per instructions. 
Open http://localhost:8088/ and see home page of Superset. 

### Actual results

sqlalchemy.exc.ProgrammingError. 
Reading the logs it looks like one of the dbs or tables hasn't fully initialised. 

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### Traceback:

```
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation ""ab_permission_view_role"" does not exist
LINE 2: FROM ab_permission_view JOIN ab_permission_view_role ON ab_p...
                                     ^

[SQL: SELECT ab_permission_view.id AS ab_permission_view_id, ab_permission_view.permission_id AS ab_permission_view_permission_id, ab_permission_view.view_menu_id AS ab_permission_view_view_menu_id 
FROM ab_permission_view JOIN ab_permission_view_role ON ab_permission_view.id = ab_permission_view_role.permission_view_id JOIN ab_role ON ab_role.id = ab_permission_view_role.role_id JOIN ab_permission ON ab_permission.id = ab_permission_view.permission_id JOIN ab_view_menu ON ab_view_menu.id = ab_permission_view.view_menu_id 
WHERE ab_permission.name = %(name_1)s AND ab_role.id IN (%(id_1)s)]
[parameters: {'name_1': 'menu_access', 'id_1': 2}]
(Background on this error at: http://sqlalche.me/e/13/f405)

Traceback (most recent call last)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1278, in _execute_context
cursor, statement, parameters, context
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute
cursor.execute(statement, parameters)
The above exception was the direct cause of the following exception:
File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 2464, in __call__
return self.wsgi_app(environ, start_response)
File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 2450, in wsgi_app
response = self.handle_exception(e)
File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1867, in handle_exception
reraise(exc_type, exc_value, tb)
File ""/usr/local/lib/python3.6/site-packages/flask/_compat.py"", line 39, in reraise
raise value
File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 2447, in wsgi_app
response = self.full_dispatch_request()
File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1952, in full_dispatch_request
rv = self.handle_user_exception(e)
File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1821, in handle_user_exception
reraise(exc_type, exc_value, tb)
File ""/usr/local/lib/python3.6/site-packages/flask/_compat.py"", line 39, in reraise
raise value
File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1950, in full_dispatch_request
rv = self.dispatch_request()
File ""/usr/local/lib/python3.6/site-packages/flask/app.py"", line 1936, in dispatch_request
return self.view_functions[rule.endpoint](**req.view_args)
File ""/usr/local/lib/python3.6/site-packages/flask_appbuilder/security/views.py"", line 501, in login
self.login_template, title=self.title, form=form, appbuilder=self.appbuilder
File ""/usr/local/lib/python3.6/site-packages/flask_appbuilder/baseviews.py"", line 281, in render_template
template, **dict(list(kwargs.items()) + list(self.extra_args.items()))
File ""/usr/local/lib/python3.6/site-packages/flask/templating.py"", line 140, in render_template
ctx.app,
File ""/usr/local/lib/python3.6/site-packages/flask/templating.py"", line 120, in _render
rv = template.render(context)
File ""/usr/local/lib/python3.6/site-packages/jinja2/environment.py"", line 1090, in render
self.environment.handle_exception()
File ""/usr/local/lib/python3.6/site-packages/jinja2/environment.py"", line 832, in handle_exception
reraise(*rewrite_traceback_stack(source=source))
File ""/usr/local/lib/python3.6/site-packages/jinja2/_compat.py"", line 28, in reraise
raise value.with_traceback(tb)
File ""/usr/local/lib/python3.6/site-packages/flask_appbuilder/templates/appbuilder/general/security/login_db.html"", line 2, in top-level template code
{% extends ""appbuilder/base.html"" %}
File ""/usr/local/lib/python3.6/site-packages/flask_appbuilder/templates/appbuilder/base.html"", line 1, in top-level template code
{% extends base_template %}
File ""/app/superset/templates/superset/base.html"", line 20, in top-level template code
{% from 'superset/partials/asset_bundle.html' import css_bundle, js_bundle with context %}
File ""/app/superset/templates/appbuilder/baselayout.html"", line 20, in top-level template code
{% import 'appbuilder/baselib.html' as baselib %}
File ""/usr/local/lib/python3.6/site-packages/flask_appbuilder/templates/appbuilder/init.html"", line 46, in top-level template code
{% block body %}
File ""/app/superset/templates/appbuilder/baselayout.html"", line 45, in block ""body""
<div id=""app"" data-bootstrap=""{{ bootstrap_data() }}""></div>
File ""/app/superset/views/base.py"", line 326, in serialize_bootstrap_data
{""common"": common_bootstrap_payload()},
File ""/app/superset/views/base.py"", line 318, in common_bootstrap_payload
""menu_data"": menu_data(),
File ""/app/superset/views/base.py"", line 259, in menu_data
menu = appbuilder.menu.get_data()
File ""/usr/local/lib/python3.6/site-packages/flask_appbuilder/menu.py"", line 64, in get_data
self.get_flat_name_list()
File ""/usr/local/lib/python3.6/site-packages/flask_appbuilder/security/manager.py"", line 1169, in get_user_menu_access
None, ""menu_access"", view_menus_name=menu_names
File ""/usr/local/lib/python3.6/site-packages/flask_appbuilder/security/manager.py"", line 1141, in _get_user_permission_view_menus
permission_name, db_role_ids
File ""/usr/local/lib/python3.6/site-packages/flask_appbuilder/security/sqla/manager.py"", line 362, in find_roles_permission_view_menus
self.role_model.id.in_(role_ids),
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3341, in all
return list(self)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3503, in __iter__
return self._execute_and_instances(context)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/query.py"", line 3528, in _execute_and_instances
result = conn.execute(querycontext.statement, self._params)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1014, in execute
return meth(self, multiparams, params)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/sql/elements.py"", line 298, in _execute_on_connection
return connection._execute_clauseelement(self, multiparams, params)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1133, in _execute_clauseelement
distilled_params,
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1318, in _execute_context
e, statement, parameters, cursor, context
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1512, in _handle_dbapi_exception
sqlalchemy_exception, with_traceback=exc_info[2], from_=e
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/util/compat.py"", line 178, in raise_
raise exception
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/base.py"", line 1278, in _execute_context
cursor, statement, parameters, context
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/engine/default.py"", line 593, in do_execute
cursor.execute(statement, parameters)
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation ""ab_permission_view_role"" does not exist
LINE 2: FROM ab_permission_view JOIN ab_permission_view_role ON ab_p...
^

[SQL: SELECT ab_permission_view.id AS ab_permission_view_id, ab_permission_view.permission_id AS ab_permission_view_permission_id, ab_permission_view.view_menu_id AS ab_permission_view_view_menu_id
FROM ab_permission_view JOIN ab_permission_view_role ON ab_permission_view.id = ab_permission_view_role.permission_view_id JOIN ab_role ON ab_role.id = ab_permission_view_role.role_id JOIN ab_permission ON ab_permission.id = ab_permission_view.permission_id JOIN ab_view_menu ON ab_view_menu.id = ab_permission_view.view_menu_id
WHERE ab_permission.name = %(name_1)s AND ab_role.id IN (%(id_1)s)]
[parameters: {'name_1': 'menu_access', 'id_1': 2}]
(Background on this error at: http://sqlalche.me/e/13/f405)
The debugger caught an exception in your WSGI application. You can now look at the traceback which led to the error.
To switch between the interactive traceback and the plaintext one, you can click on the ""Traceback"" headline. From the text traceback you can also create a paste of it. For code execution mouse-over the frame you want to debug and click on the console icon on the right side.

You can execute arbitrary Python code in the stack frames and there are some extra helpers available for introspection:

dump() shows all variables in the frame
dump(obj) dumps all that's known about the object
```

#### How to reproduce the bug

1. `git pull`
2. `git checkout 3e374dab0` or `git checkout master`
3. docker-compose up
4. Open http://localhost:8088/

### Environment

Docker as per instructions. 

- superset version: `superset version`  - 3e374dab0 - commit
- python version: `python --version` 
- node.js version: `node -v`
- npm version: `npm -v`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [x ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x ] I have checked the issue tracker for the same issue and I haven't found one similar.
- There is one also about docker install. , but on an ubuntu environment with a different error. 

### Additional context

Windows 10 running WSL as a backend. 

If merged this PR perhaps means it's not a valid bug as windows: https://github.com/apache/incubator-superset/pull/10511
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.75. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",subhashb,"
--
I can confirm the same error on Mac (OS Catalina - 10.15.6 (19G2021))
--
",gtg472b,"
--
Just a shot in the dark:

have you run init?

Either use the Docker init container, or run the superset command line
superset init

I ran into (unrelated) issues because I had disabled the init container
--
",brylie,"
--
> Either use the Docker init container, or run the superset command line
superset init

How do you run init? Are the commands documented?
--

--
I am also getting the missing table error when following the [Installing Superset Locally Using Docker Compose](https://superset.incubator.apache.org/docs/installation/installing-superset-using-docker-compose) instructions.

```
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation ""ab_permission_view_role"" does not exist
LINE 2: FROM ab_permission_view JOIN ab_permission_view_role ON ab_p...
^

[SQL: SELECT ab_permission_view.id AS ab_permission_view_id, ab_permission_view.permission_id AS ab_permission_view_permission_id, ab_permission_view.view_menu_id AS ab_permission_view_view_menu_id
FROM ab_permission_view JOIN ab_permission_view_role ON ab_permission_view.id = ab_permission_view_role.permission_view_id JOIN ab_role ON ab_role.id = ab_permission_view_role.role_id JOIN ab_permission ON ab_permission.id = ab_permission_view.permission_id JOIN ab_view_menu ON ab_view_menu.id = ab_permission_view.view_menu_id
WHERE ab_permission.name = %(name_1)s AND ab_role.id IN (%(id_1)s)]
```
--

--
From what I can see on the `master` branch, there isn't a DB migration to create the `ab_permission` table. There are only a couple of test files that reference the table.

https://github.com/apache/incubator-superset/search?q=ab_permission
--

--
Ping
--

--
Thanks @uZer I'll try again now, since it's been a few months since I last experimented with Superset.

By the way, are you attending the [Superset 1.0 Meetup](https://us02web.zoom.us/w/84078341941?tk=m0tQ6AMt-V6vJBQxX2TvyDFJXCHbXQ11goALSme2U7Q.DQIAAAATk3WvNRZnUmRuT2JrRFI2SzhvaWZzckxQRkp3AAAAAAAAAAAAAAAAAAAAAAAAAAAA&uuid=WN_iqD-UkIPRJ2LTjFYCuwPpQ) that is on air right now?

--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",uZer,"
--
Hey @brylie, I had the same issue as you during my superset setup.
Problem was: my first container has been OOMKilled during the execution of the migration script. After the container restart, db migration script didn't detect the setup was incomplete. 

I dropped all the tables from my database and ran the migration script again, it works now :)
--
"
10703,OPEN,"when i execute oracle sql statements like update in SQLLAB; get ""oracle error: not a query"" error",bug; inactive,2020-12-25 21:55:32 +0000 UTC,chuancyzhang,Opened,,"A clear and concise description of what the bug is.

### Expected results

it should be success.

### Actual results
get error  ""oracle error: not a query""
![image](https://user-images.githubusercontent.com/23111194/91517359-28be3200-e920-11ea-902e-1935a0383d1c.png)

#### Screenshots

![image](https://user-images.githubusercontent.com/23111194/91517293-02989200-e920-11ea-8650-d5d4b12ccc18.png)


#### How to reproduce the bug

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

### Environment

(please complete the following information):

- superset version: `0.36.0`
- python version: `3.6`
- node.js version: `node -v`
- npm version: `npm -v`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.57. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10701,OPEN,Cursor move up in legend in Arc chart,bug; inactive,2020-12-25 21:55:33 +0000 UTC,GGPay,Opened,,"Cursor move up when tick in legend in Arc chart 


![Superset_legent_issue](https://user-images.githubusercontent.com/17413180/91505987-99b90780-e896-11ea-88e5-e38db6f64883.gif)


#### How to reproduce the bug

1. Add legend in Arc chart 
2. In ""Arc"" section - add ""Categorical color""  - in my case - ""AIRLINE"" - save.
2. Move Arc chart in bottom section in dashboard
2. Click on legend
3. Cursor move up

![image](https://user-images.githubusercontent.com/17413180/91506440-b0ac2980-e897-11ea-961e-5301608b8031.png)


",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.86. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",junlincc,"
--
Thank you for reporting!  Welcome to join our slack if you have not, to provide more product feedback. https://app.slack.com/client/T7GMC60JX/G017M6QNUKA, 
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10697,OPEN,Data modeling layer to enable GUIs for creating queries,enhancement:request,2021-02-16 06:30:18 +0000 UTC,howardmalec,Opened,,"Data customers often have varying degrees of familiarity with SQL. Some BI tools allow analysts to code data models for common analytical patterns. These models can power click and drop GUIs that are used by customers who aren't familiar with SQL to create queries for custom data analysis. In addition to enabling analysis for users without SQL skills, the data modeling helps to enforce data quality and governance.  

Looker's [data modeling layer](https://looker.com/platform/data-modeling) (along with blocks) is a practical example of this. 
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10696,OPEN,Color scale in calendar heatmap not reflective of the actual values in the heatmap,bash!; bug; enhancement:request,2021-02-17 13:29:53 +0000 UTC,CurtLH,In progress,,"I'm using a calendar heatmap to show the number of records per hour/day within my dataset.  Here is my query:

<img width=""433"" alt=""image"" src=""https://user-images.githubusercontent.com/6969525/91464967-0b1a9b00-e842-11ea-9260-4690a90f6d65.png"">

While a chart is rendered in the format that I would expect, the colors are not what I would expect.  It seems that the max value on the color scale 28, but the majority of my values are above 28, so nearly everything in the heatmap is red.

<img width=""1361"" alt=""Screen Shot 2020-08-27 at 8 51 03 AM"" src=""https://user-images.githubusercontent.com/6969525/91465301-7a908a80-e842-11ea-83e3-9a958151d749.png"">

I know that I can change the color scale, but when I do that, the majority of the colors in the heatmap still remain whatever the max color is.  Is there a configuration setting that I'm overlooking to readjust the color scale?  I've tried re-running the query, but the output in consistent. 

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.57. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",junlincc,"
--
charts
--
",PowerPlop,"
--
It seems the colors of the heatmap are based on the raw values before aggregation.
If I use a grouping by day, the colors use the min and max of the input values instead of the aggregated values.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",CurtLH,"
--
Has there been on progress on this bug?
--

--
@maloun96 -- here is a sample of the data.  I'm not sure if it matches exactly to what was seen in the initial issue, but it should be close enough to get an idea of what the data looks like. 

[date_posted.txt](https://github.com/apache/superset/files/5990088/date_posted.txt)


--
",maloun96,"
--
@CurtLH Can you please provide sample data? In CSV or in TEXT format, it will be very helpfully.
--
"
10694,OPEN,How to make a crosstab,enhancement:request; viz:chart-pivot,2021-01-02 18:26:08 +0000 UTC,TdzWork,In progress,,I can't find the charting function to do crosstab,,,issue,"
--
Issue Label Bot is not confident enough to auto-label this issue. See [dashboard](https://mlbot.net/data/apache/incubator-superset) for more details.
--
",willbarrett,"
--
Hi @TdzWork - can you elaborate? I'm not sure what a crosstab is.
--
",zhaoyongjie,"
--
@TdzWork may be pivot table?
--
",TdzWork,"
--
> Hi @TdzWork - can you elaborate? I'm not sure what a crosstab is.

like this. 
The value of the column is the original value, not the aggregate value
![1598578465(1)](https://user-images.githubusercontent.com/22335038/91511326-032a2c00-e912-11ea-9497-e9502ebff565.jpg)

--

--
> @TdzWork may be pivot table?

pivot table column values are aggregate rather than raw, and there is no concept of dimensions or measures, I don't think it meets my needs.
--
",,,,
10686,OPEN,Option to select Dashboard tabs instead of default tab while scheduling emails on a Dashboard,enhancement:request; inactive,2020-12-25 21:55:36 +0000 UTC,Energytechglobal,Opened,,"As per the documentation on [Email reports](http://superset.apache.org/installation.html#email-reports) from Superset, we are able to schedule and receive them on Dashboards. In a dashboard, there are can be multiple tabs and in our case, we are having multiple tabs and we are seeing snapshots for the default tab of a Dashboard, there is no provision to select a Dashboard tab while configuring it for a dashboard.

Provide an option to select tabs in a Dashboard while configuring Dashboards for notifications.

Snapshot for ""Schedule Email Reports for Dashboards""
Adding an option for selecting the dashboard tab or in the Dashboard list box itself will be helpful ![image](https://user-images.githubusercontent.com/40420807/91289924-977c7d80-e7b0-11ea-97bf-7c8fe67e815c.png)
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",bkyryliuk,"
--
+1 to this feature request, it would probably be really hard to implement but nice to have
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10666,OPEN,Making a timestamp a column in a pivot table,enhancement:request; inactive,2020-12-25 22:54:49 +0000 UTC,malcomwilk,Opened,,"Hi there y'all I was wanting to know the SQL code I'd have to write in order to make a timestamp a column in Apache Superset. My idea is to have a time stamp that will show data for the current year and 2 previous years. I'd like to try and make it update as time moves forward so that when we hit 2021 the data base will then show 2021 as the current year.

Would I use a could similar to this? Or would I need something more extensive to get the features I'm looking for?

    SELECT  distinct(dt) FROM mytable
WHERE 
    dt >= date_add('day', -7, from_iso8601_timestamp('{{ to_dttm.isoformat() }}'))
    AND
    dt <= date_add('day', 0, from_iso8601_timestamp('{{ to_dttm.isoformat() }}'))
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.55. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10662,OPEN,Y axis bar values are overlapping with each other in apache superset bar chart,bug,2021-02-23 07:22:25 +0000 UTC,piyush-singhal,Opened,,"When I try to show the bar value in the bar chart of apache superset, y axis bar values overlap with each other.

To fix this issue, I want to rotate the y-axis values with 45 degree so that those values will look vertically and won't overlap with each other, just like it is there in x-axis values.

![image](https://user-images.githubusercontent.com/5344732/90985535-498e2c80-e59a-11ea-833d-a0abcdf3f9c8.png)

Can someone suggest me a way to fix this issue?",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.68. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",Neel,"
--
Still facing this issue. Please suggest any way to resolve this
--
",,,,,,
10654,OPEN,% symbol not displayed correctly in partition chart,bug; inactive,2020-10-22 04:12:14 +0000 UTC,aman101097,In progress,,"A clear and concise description of what the bug is.

When I try to display % display, it is displayed in its code rather as '%' symbol

### Expected results

'%' symbol should have been displayed from result

### Actual results


a&#x2f;c is getting displayed instead of '%' but on hovering it is getting displayed correctly



#### Screenshots

![image](https://user-images.githubusercontent.com/58933394/90857756-6fe97780-e3a2-11ea-983e-6aa9293c3aed.png)


#### How to reproduce the bug

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

### Environment

(please complete the following information):

- superset version: `superset version`
- python version: `python --version`
- node.js version: `node -v`
- npm version: `npm -v`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.96. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",aman101097,"
--
@mistercrunch any idea why?

--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10646,OPEN,Support Azure Application Insights &,enhancement:request,2021-04-07 14:32:13 +0000 UTC,zheya08,Opened,,"I'm always frustrated when I want to use superset to  explore my data in  Azure Application Insights. I need to use its api to pull data into postgresql, and use superset connect postgresql, but when data amount get larger, it's very inefficient. I think if superset can directly connect to  Azure Application Insights,  it has api but use kusto sql, something differ from ansi sql.

Azure Application Insights docs: https://dev.applicationinsights.io/documentation/Overview
kusto sql: https://docs.microsoft.com/azure/data-explorer/kusto/query/
python lib: https://github.com/Azure/azure-kusto-python

I want to write a new connection to visit Azure Application Insights, is any guides or examples?
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.64. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",villebro,"
--
@zheya08 Based on this [SQL to Kusto cheat sheet](https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/sqlcheatsheet), Kusto (or KQL) is a subset of SQL but with different syntax than SQL. Instead of writing a custom connector that can only be used with Superset, have you considered authoring a [Sql Alchemy connector](https://docs.sqlalchemy.org/en/13/dialects/) for Kusto? In addition to being able to connect Superset to Azure Application Insights, it could be leveraged by the whole Python ecosystem.
--

--
It's not directly relevant I'm afraid. What's really needed is a Kusto SQLAlchemy connector.
--
",junlincc,"
--
sources
--

--
@villebro did `pysuperset ` take us one step closer to supporting Azure application? or is it relevant? 
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,
10640,OPEN,"Hide ""Edit chart metadata"" and ""Explore Chart"" options in SliceHeaderControls when we embed URL in iframe",enhancement:request; inactive,2020-12-25 22:54:48 +0000 UTC,pallaviMN,In progress,,"**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.91. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",Neel,"
--
This can be done by adding css

--

--
Yes true this would merely be a workaround.
--
",pallaviMN,"
--
yes, using""standalone"" css its possible, but along ""Edit Chart"" and ""Explore Chart"" we want other options to be present, If we are doing via CSS and in future if they change class, again i have to modify. 
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,
10638,OPEN,"Hi Team ; I am embedding superset demo URL in our web app using iframe. I need to include a javascript file ""iframeresizer.contentwindow.min.js"" to fit the height to content height of the screen",enhancement:request; inactive; need:more-info,2020-12-25 17:55:14 +0000 UTC,pallaviMN,Opened,,"**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.65. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10629,OPEN,List User is not opening After SSO Integration,bug; inactive,2020-10-22 04:12:13 +0000 UTC,Asturias-sam,Opened,,"I have integrated the SSO with superset after integration for admin users  `List User` is not opening 

Here is my login function:
```
@expose(""/login/"")
    def login(self):
        provider = 'login'
        if g.user is not None and g.user.is_authenticated:
            log.info(""Already authenticated {0}"".format(g.user))
            if(request.args[""next""] is not None):
                    log.info(""Checking the URL----->{0}"".format(request.args[""next""]))
                    return redirect(request.args[""next""])

            return redirect(self.appbuilder.get_url_for_index)
        else:
            log.info(""Going to call authorize for: {0}"".format(provider))
            state = jwt.encode(
                request.args.to_dict(flat=False),
                self.appbuilder.app.config[""SECRET_KEY""],
                algorithm=""HS256"",
            )
            try:
                if provider is not None:
                    return self.appbuilder.sm.oauth_remotes[provider].authorize(
                        callback=url_for(
                            "".oauth_authorized"", provider=provider, _external=True
                        ),
                        state=state,
                    )
            except Exception as e:
                log.error(""Error on OAuth authorize: {0}"".format(e))
                return redirect(self.appbuilder.get_url_for_index)
```


Here is my oauth authorisation 
```
 @expose(""/oauth-authorized/<provider>"")
    def oauth_authorized(self, provider):
        log.info(""Authorized init"")
        resp = self.appbuilder.sm.oauth_remotes[provider].authorized_response()
        if resp is None:
            flash(u""You denied the request to sign in."", ""warning"")
            return redirect(""login"")
        log.info(""OAUTH Authorized resp: {0}"".format(resp))
        # Retrieves specific user info from the provider
        try:
            self.appbuilder.sm.set_oauth_session(provider, resp)
            userinfo = self.oauth_user_info(provider, resp)
        except Exception as e:
            log.error(""Error returning OAuth user info: {0}"".format(e))
            user = None
        else:
            log.info(""User info retrieved from {0}: {1}"".format(provider, userinfo))
            # User email is not whitelisted
            whitelist = None
            try:
                #is_authorized = self.is_user_authorized(userinfo[""email""].lower())
                #enable this when authorization is required 
                is_authorized = True
            except Exception as e:
                log.error('failed to validate authority of user')
                return redirect(""unauthorized"")
            allow = False
            if is_authorized:
                allow = True
            if not allow:
                return redirect(""unauthorized"")
            user = self.appbuilder.sm.auth_user_oauth(userinfo)

        if user is None:
            return redirect(""login"")
        else:
            login_user(user)
            try:
                state = jwt.decode(
                    request.args[""state""],
                    self.appbuilder.app.config[""SECRET_KEY""],
                    algorithms=[""HS256""],
                )
            except jwt.InvalidTokenError:
                raise Exception(""State signature is not valid!"")

            try:
                next_url = state[""next""][0]

            except (KeyError, IndexError):
                next_url = self.appbuilder.get_url_for_index
            return redirect(next_url)

```

I have added the logs so to check why it's not going there so logs says:

```
INFO:custom_sso_security_manager:Already authenticated Som Pathak
INFO:custom_sso_security_manager:Checking the URL-----> http://10.22.266.62:8088/users/list/
INFO:custom_sso_security_manager:Already authenticated Som Pathak
INFO:custom_sso_security_manager:Checking the URL----->http://10.22.266.62:8088/users/list/
INFO:custom_sso_security_manager:Already authenticated Som Pathak
INFO:custom_sso_security_manager:Checking the URL----->http://10.22.266.62:8088/users/list/
```

It going  again to login method hence resulting in to many redirect  error any help regarding this ?",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.69. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",Asturias,"
--
For the first time it was opening then it's says `redirected you too many times.`
--
",syazwan0913,"
--
Maybe you need to run `superset init` to initialize the user roles
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,
10623,OPEN,Processing large files,enhancement:request,2021-02-16 06:36:52 +0000 UTC,OSINTAI,Opened,,"**Upload large files (>100 mb) / more than 500k records**

The following feature will is recommended:

- Load by drag and drop
- Background processing in case the file is large
- Notification when the CSV file parsing has been completed
- Show estimated time to completion",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.90. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",junlincc,"
--
sources
--

--
@betodealmeida do you think we can work on 2,3,4 for next major release? If not, we can pick it up from our end, those request makes sense to me.  
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10598,OPEN,Add hex based alpha/opacity steps to theme,enhancement:request; inactive; org:preset,2020-12-25 17:55:23 +0000 UTC,rusackas,Opened,,"Would like to be able to set transparency stops with two more hex characters, to build hex alpha colors? I.e. 

```
background: linear-gradient(
      180deg,
      ${({ theme }) => theme.colors.grayscale.dark2}00 47.83%,
      ${({ theme }) => theme.colors.grayscale.dark2}00 79.64%,
      ${({ theme }) => theme.colors.grayscale.dark2}${({ theme }) => theme.opacities.medium} 100%
);
```

_Originally posted by @rusackas in https://github.com/apache/incubator-superset/diffs_",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.98. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10596,OPEN,Getting query timeout on running cassandra queries to create chart on Superset dashboard,data:connection:others,2021-01-02 21:42:02 +0000 UTC,amritkrishan,Opened,,"Hi,

The issue is cassandra is unable to run raw query which is sent from superset to presto as it requires allow filtering to process query. I need some mechanism to add allow filtering clause in superset query builder. Can anyone please help on this? ",,,issue,"
--
Issue Label Bot is not confident enough to auto-label this issue. See [dashboard](https://mlbot.net/data/apache/incubator-superset) for more details.
--
",eugeniamz,"
--
  database

--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10591,OPEN,Certification of Data Entities,.pinned; enhancement:request,2020-10-26 23:42:00 +0000 UTC,etr2460,Opened,,"**Is your feature request related to a problem? Please describe.**
In larger deployments of Superset, it can be difficult to know which tables, datasources, and metrics are the correct ones to use. Many entities may have similar names, but no structured way exists to find the single source of truth.

**Describe the solution you'd like**
I propose the addition of a certified field on various data entities (tables and sql_metrics to start). Certification represents the review of a data entity by someone (a person, a group, a process) which adds an additional level of trust and verification to the entity.

This field will live JSON encoded in a new `extra` column added to the metadata database for these two tables. The structure of the data will be as follows:
```json
{
  ""certification"": {
    ""certified_by"": ""Erik Ritter"",
    ""details"": ""This metric is the single source of truth for births."" 
  }
}
```

These certifications will be exposed in the UI anywhere a table or metric is referenced with the use of an icon before the table/metric name. In the future, we can also extend this approach to other data entities such as columns, slices, and dashboards.

**Describe alternatives you've considered**
I'm proposing a generic `extra` column as opposed to a more structured `certifications` table because I anticipate some iteration on this feature, and using a generic JSON string column allows for faster and safer iteration, as well as reduces the number of db migrations required. After all work concludes and the product is in a stable place, we can consider a migration to a more structured db representation.

**Additional context**
[Tableau's implementation of certification](https://help.tableau.com/current/server/en-us/datasource_certified.htm)

I'll be PR-ing out work around this feature in the next couple weeks, so here's the context behind the what/why of these PRs!",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.84. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10586,OPEN,Partition chart mixes up results,bug; viz:chart-partition,2021-04-01 06:13:44 +0000 UTC,jeremy-ecoenergy,Opened,,"I've set up a partition chart with multiple levels of groupings:

shop -> sales owner -> sale/goal

![Screenshot 2020-08-11 at 4 29 13 PM](https://user-images.githubusercontent.com/65153090/89997457-996d1980-dca5-11ea-8017-6c19d53287cb.png)

Superset is not rendering this data correctly, despite the raw results being accurate.

![Screenshot 2020-08-12 at 2 09 09 PM](https://user-images.githubusercontent.com/65153090/89997509-a7229f00-dca5-11ea-84c5-0d8ee1cad251.png)

### Expected results

A partition chart with the following:

Karachi | Wali Mukhtar | Goal (some number)
              | Adnan Shamim | Goal (some number)
              | Jeremy Higgs | Sale (some mumber)

### Actual results

Instead, entries from other sale owners are getting mixed in.

![Screenshot 2020-08-12 at 2 07 37 PM](https://user-images.githubusercontent.com/65153090/89997684-e18c3c00-dca5-11ea-91e0-a131635c79e2.png)


#### Screenshots

Added inline.

#### How to reproduce the bug

Config given above.

### Environment

Running on Preset.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.77. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",junlincc,"
--
 chart 
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",McHollands,"
--
I was able to fix this by reverting the changes to PartitionViz.nest_values made in this MR https://github.com/apache/incubator-superset/pull/9943. Changing the dims input from a tuple to a list means it isn't getting concatenated correctly and so when evaluating partiiton charts with greater than 2 levels it breaks
--
",guang,"
--
rip partition charts
--
",,
10574,OPEN,deck.gl Polygon doesn't render anything; shows error,bug; viz:chart-deck.gl,2021-01-02 18:49:53 +0000 UTC,sazary,Opened,,"When I try to render some polygons using deck.gl, it shows an error, noting that `TypeError: Cannot use 'in' operator to search for 'geometry' in undefined`.

### Expected results

Render polygons on screen. The datasource that I'm trying to render is this:

```
SELECT 
    now() as datetime, 
    name as name,
    ST_AsGeoJSON(polygon) AS polygon
FROM public.area_driverarea
WHERE 
    polygon IS NOT NULL 
    and not archived
limit 1
```

And the result is this:

```
2020-08-11T14:31:29.670272+04:30		{""type"":""Polygon"",""coordinates"":[[[51.302861661377,35.7162896874761],[51.310581661377,35.7169196869251],[51.3356516613769,35.7117596914374],[51.3371016613769,35.7024896995422],[51.3341916613769,35.6999797017365],[51.301661661377,35.6993497022872],[51.302861661377,35.7162896874761]]]}
```

Please note that I've included the `datetime` column just to make sure that lacking a time column isn't the source of error.

### Actual results

It shows an error. You can see it in the screenshot below and this is printed in the browser's console:

```
react_devtools_backend.js:2273 TypeError: Cannot use 'in' operator to search for 'geometry' in undefined
    at I (27.2a2d76d7d2deda70e1c0.chunk.js:1)
    at Array.flatMap (<anonymous>)
    at Function.getDerivedStateFromProps (27.2a2d76d7d2deda70e1c0.chunk.js:1)
    at new V (27.2a2d76d7d2deda70e1c0.chunk.js:1)
    at yo (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at Ii (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at gs (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at lu (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at su (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at $s (explore.e9de4d7e8c553ec56a7b.entry.js:2)
```


#### Screenshots

![image](https://user-images.githubusercontent.com/1304109/89883783-e4235e80-dbdd-11ea-879f-a7cdb8b5a644.png)

#### How to reproduce the bug

1. Explore a chart with deck.gl polygon type.
2. Set it's polygon type to JSON
4. See error

### Environment

(please complete the following information):

- superset version: `0.36.0`
- python version: `3.6.11`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional Context

I've noted that in the response of `explore_json` request, the `data` field is as follows:

```
""data"":{
   ""features"":[
      {
         ""count"":1,
         ""__timestamp"":null,
         ""elevation"":1000
      }
   ],
   ""mapboxApiKey"":""[redacted]"",
   ""metricLabels"":[
      ""count""
   ]
}
```

I guess that `features` should include `geometry` field, and it isn't.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10560,OPEN,Restrict number of failed login attempts,enhancement:request; inactive; security,2021-01-03 05:15:11 +0000 UTC,gk1089,Opened,,"**Is your feature request related to a problem? Please describe.**
In its default state, the Superset login page permits any number of failed login attempts. This has been flagged as a security issue by our sysadmin team, and I agree with them. I have only tested users created in the database and have not tried other authentication methods.

**Describe the solution you'd like**
The login page should permit, say, 3 incorrect login attempts and should ask the user to try again after a period of time. This time should be customizable from the config file.

**Describe alternatives you've considered**
I am not sure if this already works for other authentication methods.

**Additional context**
None.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.81. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",junlincc,"
--
 security
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",nytai,"
--
This issue is probably better suited for https://github.com/dpgaspar/Flask-AppBuilder. You could also implement your own custom security manager and override the login method, there should be info on how to do this in the flask_appbuilder docs. 
--
",,,,
10559,OPEN,Customize superset; How to add a new number format(Indian number format) option in the charts.,enhancement:request; inactive,2020-10-10 12:37:42 +0000 UTC,piyush-singhal,Opened,,"Hey Team, 

I am new to superset customization so need some guidance, I want to change the number format option available inside the chart, I want to add or modify existing format to indian number format. I will write the logic in javascript to change the original number to indian number format, but I don't know where to write my code for this.

Could you please guide me, how can I get it done. I found that superset is using D3 format and a superset-ui-number-format package is there for it. but next don't know how to modify it. Let me know if you need more information on this problem.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.55. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10546,OPEN,When the name of the email attachment is Chinese; the name of the attachment received by the email client is garbled,bug; i18n:chinese,2021-01-03 05:18:30 +0000 UTC,wzJudy,Opened,,"A clear and concise description of what the bug is.

### Expected results
the email client can see the right attachment name

![image](https://user-images.githubusercontent.com/8433942/89606943-de5b0f80-d8a3-11ea-9fac-6db9571f30b6.png)

### Actual results

when the attachment name is in Chinese, the email client will see the garbled code
![image](https://user-images.githubusercontent.com/8433942/89605770-f2514200-d8a0-11ea-909a-9f104729eac7.png)

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

### Environment

(please complete the following information):

- superset version: `latest released`
- python version: `Python 3.7.6`
- node.js version: `v10.16.0`
- npm version: `6.9.0`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.91. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10543,OPEN,[dashboard edit] No Switch to view mode option,bug,2021-04-09 07:59:48 +0000 UTC,graceguo-supercat,Opened,,"#### How to reproduce the bug

1. Go to Dashboard
2. Click on ""Edit"", dashboard will be in edit mode
3. Now I want to switch back to view mode **without** any change:

### Expected results

<img width=""1152"" alt=""Screen Shot 2020-08-06 at 2 56 24 PM"" src=""https://user-images.githubusercontent.com/27990562/89586701-0c354b00-d7f5-11ea-8ebb-1319c93dbf05.png"">

<img width=""378"" alt=""Screen Shot 2020-08-06 at 3 21 19 PM"" src=""https://user-images.githubusercontent.com/27990562/89588446-81564f80-d7f8-11ea-9679-328693d8dc1d.png"">

### Actual results

<img width=""1374"" alt=""Screen Shot 2020-08-06 at 2 44 50 PM"" src=""https://user-images.githubusercontent.com/27990562/89586272-3e927880-d7f4-11ea-98f4-07c4cfadfb4c.png"">

Click ""DISCARD CHANGES"" will reload page, it will take some time (if dashboard is large). **And** if user was in a tab, reload page will lost user's previous focused tab.

The old behavior is, if there is no change in the dashboard, we show ""SWITCH TO VIEW MODE"" button, and click button will only switch mode, no page reload.

I understand Colors button is confusing. But now we do not want the functionality to change dashboard color-scheme?

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Related to #10394 

@mistercrunch 
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.69. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",graceguo,"
--
ping @mistercrunch @rusackas What do you think of setting color scheme function for dashboard? Do we just remove it? 
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",junlincc,"
--
> What do you think of setting color scheme function for dashboard?

do you mean, instead of setting it in the modal, have the setting on the dashboard? 
--
",,,,
10491,OPEN,can not parse table if blank after punctuation while using sql_parse.py,bug; inactive,2020-10-12 00:38:24 +0000 UTC,SilvesSun,In progress,,take sql `select * from ods.  tmp_table a` for example. the sql_parse can not parse correct table name `ods.tmp_table` but return table as `set([])`,,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.95. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",willbarrett,"
--
Thanks for the bug report @SilvesSun. If you have a recommended fix we'd be excited!
--
",SilvesSun,"
--
@willbarrett sqlparse has method get_parent_name, why not use this to get parent name?


--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,
10484,OPEN,Shortlinks do not pass standalone=true param to the chart url,.pinned; bug,2020-10-05 21:01:23 +0000 UTC,bkyryliuk,Opened,,"Links like http://superset.com/r/2999&standalone=true get used to preserve standalone=true in the url, but doesn't anymore

Tested on the latest master",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.85. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10483,OPEN,DB password special character bug,bug,2021-03-31 06:31:33 +0000 UTC,GiannisDimitriou,In progress,,"When a special character '@' exists in the sqlalchemy URI it is not store correctly in superset metastore.

example URI: 
postgresql://<user>:123@456@<hostname>:5432/<db>

### Expected results

escape the special character correctly after saving the Database entry

### Actual results

what actually happens.

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Go to Sources -> Datasources -> Create New
2. Construct the URI and make sure the password has '@' character.
3. Test Connection (Success)
4. Save Datasource
5. Open the newly created datasource
6. Test Connection (Failed)

In this stage if you replace the password part ('XXXXXXXXXX') in the URI with the actual password its working correctly

### Environment


- superset version: `0.37.0rc1`


",,,willbarrett,"
--
Thanks for the report @GiannisDimitriou - if you have a recommended code fix we'd be really excited!
--
",villebro,"
--
@GiannisDimitriou the password needs to be entered pre-quoted. You can check what the password/username would be when quoted with the following snippet:
```python
>>> from urllib.parse import quote_plus
>>> quote_plus('my@password/with!weird\characters')
'my%40password%2Fwith%21weird%5Ccharacters'
```
While it would be simple to perform this quoting automatically, current users are relying on the existing functionality. I can open a PR explaining how to properly quote username/password, unless you wish to that is!
--
",mistercrunch,"
--
Side note: I had a recent conversation with one of our designers and he mentioned the idea of moving away from URIs and having a more complete form with different fields for each component of the URI. Maybe an ""advanced"" option let's you use the URI if you prefer. Then the UI would quote things for you.
--
",GiannisDimitriou,"
--
> Side note: I had a recent conversation with one of our designers and he mentioned the idea of moving away from URIs and having a more complete form with different fields for each component of the URI. Maybe an ""advanced"" option let's you use the URI if you prefer. Then the UI would quote things for you.

@mistercrunch  That's a very good idea, looking forward to help with requirement/testing if needed

@villebro Thanks for the snippet!
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",priteshwatch,"
--
Hello, is there an update on this . my password has an asterix. I would atleast love to get a work around.  

postgresql+psycopg2://test:test123**@play-domain.1.rds.amazonaws.com:5432/keymakerdb
--

--
If I just escape the password, it works at connection time but does not recognize the password post connection. so essentially unusable. 
--
"
10428,OPEN,Integrity error; probably unique constraint,bug; inactive,2020-12-25 21:55:34 +0000 UTC,danilostochi,Opened,,"A clear and concise description of what the bug is.

### Expected results
save two tables with the same names but which have different schemes.

### Actual results
when trying to call as different schema tables, we have an error.
""Integrity error, probably unique constraint""

what actually happens.

#### Screenshots
![image](https://user-images.githubusercontent.com/68624533/88449957-88a55280-ce21-11ea-9f9c-63418c97bf3b.gif)


If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Go to 'Sources > Tables'
2. Click on '+New, Database-> Schema->Table Name '
3. Scroll down to '....'
4. See error

### Environment

(please complete the following information):

- superset version: 0.36.0
- python version: Python 3.6.9
- node.js version: v8.10.0


### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.86. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",rvanaalderen,"
--
Had the same problem in release 0.36 and 0.37. I hope it will be fixed soon.

Will PR 9882 fix this problem? 
See https://github.com/apache/incubator-superset/pull/9882 

--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10417,OPEN,Year-over-year chart JS error; unknown cause,bug; inactive,2020-10-10 12:38:01 +0000 UTC,rubypollev,Opened,,"### Expected results

The chart.

### Actual results

A JS error in the console, and no chart. 

#### Screenshots

![Screen Shot 2020-07-23 at 3 46 47 PM](https://user-images.githubusercontent.com/45610410/88345899-e2abf800-ccfb-11ea-9b8f-f7d9f3cd528b.png)

#### How to reproduce the bug

1. Create line graph of numerical data longer than 2 years
2. In Advanced Analytics run a 7-day moving average and a 52-week ""actual value"" difference of the metric

I have a dashboard with many of these types of graphs and some of them work, sometimes. Haven't found a pattern yet. 

### Environment

(please complete the following information):

- superset version: `0.36.0`
- python version: `3.6.10`
- node.js version: `v12.16.3`
- npm version: `6.14.4`
- OS: `macOS 10.15.5 (19F101)`
- browser: `Chrome Version 84.0.4147.89 (Official Build) (64-bit)`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

JS error in console:
```
2.aca4ad85030d56974bec.chunk.js:1 Uncaught TypeError: Cannot read property 'y0' of undefined
    at 2.aca4ad85030d56974bec.chunk.js:1
    at f (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at SVGPathElement.<anonymous> (2.aca4ad85030d56974bec.chunk.js:1)
    at SVGPathElement.<anonymous> (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at explore.e9de4d7e8c553ec56a7b.entry.js:2
    at he (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at Array.J.each (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at Array.J.attr (explore.e9de4d7e8c553ec56a7b.entry.js:2)
    at SVGGElement.<anonymous> (2.aca4ad85030d56974bec.chunk.js:1)
    at explore.e9de4d7e8c553ec56a7b.entry.js:2
```",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.92. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",villebro,"
--
@rubypollev are you able to reproduce this with the example datasets? Also, have you tried with `master`? I believe fixing this should be doable (it it hasn't already been fixed in `0.37.0`) if we can only reproduce the error.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10409,OPEN,Superset colors on deck.gl geojson,bug; viz:chart-deck.gl,2021-01-02 18:29:25 +0000 UTC,jbsilvalink,Opened,,"After creating a data visualization in apache superset using the deck.gl geojson I was expecting to be able to change the colors of both the line and the fill of the polygon created using the color picker in the geojson settings.

### Expected results

Using the color picker the color would update to the color I selected.

### Actual results

Nothing happens, both the stroke and the fill remain black.

#### Screenshots

![geojson colors](https://user-images.githubusercontent.com/68691634/88281482-a8ccf880-ccdf-11ea-997e-fa4718225ee8.png)

#### How to reproduce the bug

1. Create a chart with the option deck.gl Geojson
2. After the map and the polygons are loaded change the colors in the picker provided

### Environment

- superset version: 0.36.0

### Checklist

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.64. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10395,OPEN,Boxplot is showing lines; and the min; max; median; 1st quantile; and 3rd quantile are the same.,bug; inactive,2020-11-26 22:20:01 +0000 UTC,edwardlxb,Opened,,"A clear and concise description of what the bug is.
Boxplot is showing lines instead of boxplots.
### Expected results
Show boxplot
what you expected to happen.
show boxplots
### Actual results
showed lines
what actually happens.
Standards
#### Screenshots
<img width=""1179"" alt=""Screen Shot 2020-07-22 at 11 00 29 PM"" src=""https://user-images.githubusercontent.com/16586994/88192529-33294400-cc6f-11ea-9ca8-45717d423c0f.png"">

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Go to 'boxplot'
2. select 'Group by' numbers and 'matrices', then query.
3. See error

### Environment

(please complete the following information):

- superset version: `superset version`: 0.36.0
- python version: `python --version`: 3.6
- node.js version: `node -v` 14.0.0
- npm version: `npm -v`6.14.5

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [y] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [y] I have reproduced the issue with at least the latest released version of superset.
- [y] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context
There was an issue in 2016, but it's closed. Now I'm facing the same issue. 

Appreciate all the hard work, and making this framework opensource! 

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.88. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",junlincc,"
--
Hi, we may not get to this issue as we will be replacing the current box plot with Echarts box plot soon. @rusackas do you know if this is related to PR [#9392](https://github.com/apache/incubator-superset/pull/9392)? 
--
",,,,,,
10387,OPEN,Request to support filtering array columns,enhancement:request; viz:explore:filter,2021-02-15 06:50:49 +0000 UTC,Mikedu1988,Opened,,"**Is your feature request related to a problem? Please describe.**
We have a column contains customer defined tags, an array of string. We use athena as datasource. We can't filter the tags.
A screenshot of what the array looks like:
![image](https://user-images.githubusercontent.com/24424690/88130680-c16cde00-cc0d-11ea-856b-111aa566db12.png)


**Describe the solution you'd like**
We would like to:
1. split all the tags, and user can click the tags with multiple choice
2. filter all the records contains any of the selected tags 

**Describe alternatives you've considered**
1. Add type of the column when add a new customer filter
2. If it's an array, split the array and add strings to the options
3. build sql accordingly

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.99. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",amitmiran137,"
--
Just another ability in the filter component
https://github.com/apache/incubator-superset/issues/10418
--
",junlincc,"
--
 filter 
--

--
@Mikedu1988 Superset currently doesn't support storing extracted data, which preventing us from solving this problem from the UI. we suggest you to transform this table to relational structure before exploring. 
meanwhile, you can use jinja as a workaround, https://www.youtube.com/watch?v=blhWKmxkIHE&t=28s 

--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,
10334,OPEN,[search]Sorting by popularity in dashboard / chart list,enhancement:request; global:search,2021-02-16 06:42:30 +0000 UTC,zuzana-vej,Opened,,"**Is your feature request related to a problem? Please describe.**
The search output is sorted by last modified but what's more helpful generally is which dashboard is seen by a lot of people - sorting by popularity makes more sense.

**Describe the solution you'd like**
When searching for a dashboard by name give the ability to sort results by popularity or usage count, rather than last modified date.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.91. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",junlincc,"
--
 search 
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10333,OPEN,[SQL Lab] Make Query results table column width adjustable,enhancement:request,2021-02-08 16:55:39 +0000 UTC,zuzana-vej,Opened,,"**Is your feature request related to a problem? Please describe.**
As a data analyst I need to be able to modify the column width in SQL-lab results, so that my results are more readable.
Currently it's fixed width, and if I want to analyze / read data returned by a query it's difficult / impossible in some cases.

**Describe the solution you'd like**
Ideally I am able to adjust the column width in the result of a SQL Lab query.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.84. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",Steejay,"
--
Heres an exploration that looks at introducing resizable column widths. feedback welcome! 

_default widths:_
default width = based on longest content in column
default max width = 360px

_resizable widths:_
min width = 32px
max width = none

_truncation+ellipsis:_
truncation + ellipsis is triggered anytime 
the cell content is cut off by the column

_tooltip:_
tooltip appears upon hover over a cell with 
truncated content. the tooltip will show the 
complete string.

![for Github](https://user-images.githubusercontent.com/60786102/107253121-db73df80-69ea-11eb-8571-364e7bf74c8f.jpg)


cc @zuzana-vej @yousoph @mihir174 
--
",,,,,,
10332,OPEN,[SQL Lab] Copy SQL Lab result to clipboard in a way that you can paste directly into Google sheet,.pinned; enhancement:request,2021-02-09 06:08:33 +0000 UTC,zuzana-vej,Opened,,"**Is your feature request related to a problem? Please describe.**
As a superset user, I need the clipboard button which copies data, to copy the data in such a format that you can paste it right into a Google Sheet, including column names, without having first to save as CSV, then import the CSV, so that I save time while taking results from the SQL run.

**Describe the solution you'd like**
Feature expected requirements:
1. Copy will extract the resulting data (in table format) into the clipboard of the computer.
2. Pasting into a document should paste a table (not data which have spaces in between). Ideally it will be smart enough to limit the width of a column if there is massively long text in it.
3. Pasting into a spreadsheet should paste into correct individual cells (not one cell).
4. Ideally it should be smart enough not to pull quotation marks ("""") that sometimes appear in the output of SQL lab when the value is numeric.
5. It should pull the entire dataset.
6. If the dataset is too large, it may be worth adding functionality to ask the user if they are sure they want to copy it (tell them it takes too long).
7. Data copied should include column names.
8. Ideally no 'formatting' is copied (eg., bold font).
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.83. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",bkyryliuk,"
--
+1 for this feature request
--
",yousoph,"
--
I believe item 3 (Pasting into a spreadsheet should paste into correct individual cells (not one cell)) is solved by using ""paste values only"" (command + shift + V on a Mac) in Sheets. It's probably not obvious though so maybe there's more we can do to make it easier for the user. 
--
",junlincc,"
--
we should address 3 at product level as we have multiple clipboard buttons across the product.
--
",,
10330,OPEN,[SQL Lab] Add an alert in SQL Lab when results are limited,.pinned; enhancement:request; sql_lab,2021-02-10 19:26:28 +0000 UTC,zuzana-vej,Opened,,"**Is your feature request related to a problem? Please describe.**
As a SQL Lab user, I would like to see a warning when rows returned by my query are limited and when exporting to CSV, so that I can prevent mistake / needing to re-export CSV. SQL Lab will currently add a limit (of 1M) on rows returned in the UI and when exporting to csv. 

**Describe the solution you'd like**
The feature request is to make it more visible that the rows are truncated, so that the user knows data they are exporting is not all data.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.98. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",yousoph,"
--
Mocks for indicator & messaging: 

- When config DISPLAY_MAX_ROW is limiting the results: #13018  
- When config SQL_MAX_ROW is limiting the results for [admins](https://www.figma.com/file/5db2MpaKNnC1c7MK9ysdeV/SQL-Limit-Indicator?node-id=90%3A281) and [non admins](https://www.figma.com/file/5db2MpaKNnC1c7MK9ysdeV/SQL-Limit-Indicator?node-id=90%3A423)
- [When the limit dropdown is limiting the results displayed ](https://www.figma.com/file/5db2MpaKNnC1c7MK9ysdeV/SQL-Limit-Indicator?node-id=90%3A1)
- [When the user has a limit in the query text itself that is limiting the results displayed](https://www.figma.com/file/5db2MpaKNnC1c7MK9ysdeV/SQL-Limit-Indicator?node-id=90%3A140)
--
",eschutho,"
--
this is great. Thanks @yousoph! What if more than one condition are true, like the limit dropdown and the limit in the query are both the same? Also, what if the total results happen to equal the limit? Anything different?
--
",Steejay,"
--
thanks for bringing up this edge case @eschutho 

 When more than one condition are true for [admins](https://www.figma.com/file/5db2MpaKNnC1c7MK9ysdeV/SQL-Limit-Indicator?node-id=133%3A70) and [non admins](https://www.figma.com/file/5db2MpaKNnC1c7MK9ysdeV/SQL-Limit-Indicator?node-id=133%3A212)

For these conditions, the configuration in the message is always behind a tooltip. ie if [query = display_max_row](https://www.figma.com/file/5db2MpaKNnC1c7MK9ysdeV/SQL-Limit-Indicator?node-id=135%3A333)

Reasoning: show what is visible in the UI first (query/dropdown) so that users can establish a visual relationship and use progressive disclosure to reveal additional matching limit constraints on the server side (configs). 


cc @yousoph 
--

--
We did consider a more generalized message like ""The number of rows displayed is limited to 1 000 by more than one limit constraint"" but opted for a more specific message for additional clarity. Thoughts?
--

--
@zuzana-vej Thank you for the feedback! You bring up a great point. This is something we did consider and proposed that admins and non admins have different messages (see links attached in above messages).

re ""It appears"" copy. Yes great call out. This has just been updated. Currently the system doesnt know if thats the case but we are doing the work to change that so that Superset _will_ know. 
--
",zuzana,"
--
End user doesn't know about DISPLAY_MAX_ROW or SQL_MAX_ROW and there is nothing they can do (besides contacting the admin). Do we want to expose this level of detail to them?

I think the messaging for when user limits in dropdown, or when user limits in query is great. When there is systems settings which admin (maintaining team) needs to config, I would think we can keep more generic message like one of these:

- It appears that the number of rows displayed is limited to 1000 by system configuration. 
- It appears that the number of rows displayed is limited to 1000 by system configuration. Contact your admin for more details.

Also do we need to word ""It appears""? Or can we just say ""The number of rows displayed is limited to 1000 by system configuration."" The ""It appears"" make is sounds like we are not sure whether that's the case (and if we aren't sure then it appears is correct, if we are sure that this is limiting factor and query would otherwise return more results, we don't need ""it appears"".
--

--
Sounds good yes the admin/non admin makes sense thanks for clarifying.
--
"
10285,OPEN,[SIP-49] Dashboard Filter Component,enhancement:request; sip,2020-12-19 16:30:16 +0000 UTC,graceguo-supercat,Opened,,"## [SIP] Proposal for Dashboard Filter Component

### Motivation

This SIP is **not** about design or implementation. Instead I want to discuss the _**requirements**_ of dashboard filters: what functionalities the dashboard filters should offer. Based on this discussion, we will do architecture design and implementation plan.

_Similar discussions:_ [[SIP-9](https://github.com/apache/incubator-superset/issues/5925)][[SIP-25](https://github.com/apache/incubator-superset/issues/8452)]

Over the last a few years, we got many, many suggestions and feature requests to improve the usability and interactions for Superset dashboard filters. Currently dashboard filters are _charts_, when it is loaded each filter field is a query (select DISTINCT _column_name_ from _table_), and results are rendered as filter_box viz_type. It works good for most of use-cases, but still missed a few functionalities if compared with dashboard filter from other BI tools.

### Proposed Change

I want to propose a solution to improve the functionalities for dashboard filters. Instead of using a separated slice component, I think dashboard should have **build-in Filter component**. It should carry over all the features from existed filter_box slice, understand properties that slice filter used (datasource, column, default values, sorting, .etc), and re-use dashboard specific properties, like filter scope and immune. All these **filters related properties should be persistent in dashboard metadata**.

For component UI/UX, we should follow [SIP-34](https://github.com/apache/incubator-superset/issues/8976):
**view mode: apply filter**
![6LmlSMwgEz](https://user-images.githubusercontent.com/27990562/87103811-e2473380-c20a-11ea-9e07-d471dc4b287f.gif)


**add/edit filter config and scopes**
<img width=""1304"" alt=""Screen Shot 2020-07-09 at 5 35 56 PM"" src=""https://user-images.githubusercontent.com/27990562/87103807-deb3ac80-c20a-11ea-9fe4-796f12980bd6.png"">



### New or Changed Public Interfaces

I list all dashboard filters related feature requests here, so that we can decide which ones should be implemented.

- **Section 1:** Current features should be kept:

  1. Filter indicators
  2. Filter scopes
  3. Share dashboard with snapshot of filter state
  4. Chart filter: user select a few rows in a table (or a piece in a pie chart etc), selected rows works like temporary filter and applied to other charts in the dashboard. This feature was lost during filter-scope implementation, but should bring it back.

- **Section 2:** Extra filter functionalities candidates:

  1. Allow user filter on metric (as well as column), similar to adhoc filter [[9502](https://github.com/apache/incubator-superset/issues/9502)]
  2. To support ""less than"" and ""greater than"" (or ""BETWEEN""), current filter_box can only select one or more fixed values [[9974](https://github.com/apache/incubator-superset/issues/9974)]
  3. Non-editable filter box, or some options are not removable. [[10230](https://github.com/apache/incubator-superset/issues/10230)]
  4. Cascading filters: when user changes filter A, it will automatically trigger filter B loaded with new options, in a pre-defined sequence [[5664](https://github.com/apache/incubator-superset/pull/5664)][[10013](https://github.com/apache/incubator-superset/issues/10013)]
  5. Allow user to define static option values instead of running a query [[SIP-25](https://github.com/apache/incubator-superset/issues/8452)]
  6. Allow filter only apply to charts from same datasource (filter scope settings).
  7. Ensuring filter eligibility to the chart [[SIP-45](https://github.com/apache/incubator-superset/issues/9935)]

- **Section 3:** Extra intra-dashboard interaction:

  1. Time-based Visual Correlation between Charts on a Dashboard [[SIP-10](https://github.com/apache/incubator-superset/issues/5955)]
  2. Intra-dashboard interaction [[7512](https://github.com/apache/incubator-superset/issues/7512)]


### Migration Plan and Compatibility

Currently Superset filters for dashboard are _slices_, and all filter properties are stored in slices table. After we implemented dashboard filter component, existed _filter slice_ should be converted to dashboard properties. We will offer a Superset db migration script, which will automatically convert filter_box properties into dashboard filter properties. But since each filter_box took some space in the dashboard, it will still need some manual work to remove filter_box from dashboard, and re-arrange dashboard layout to make it nice and clean. 


### Rejected Alternatives

Describe alternative approaches that were considered and rejected.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.89. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",villebro,"
--
I love this proposal!  
--
",michalmisiewicz,"
--
How about not in / exclude filter ?
--
",graceguo,"
--
> How about not in / exclude filter ?

Good point! I added in the request list.
--
",amitmiran137,"
--
If I understand this SIP correctly it is all about making a component which is responsible both Visually and logically to consolidate the set of filters that will be applied to all of the existing charts(**slices**) when fetching the data

so the component responsibilities are :
1. get a final distinctive list of filters that needs to be applied in this dashboard after merging filters from (query params, available filters from on the filter component UI, Chart plugins cross filtering, what else?) 
2. apply filters to slices and send to BE by applying rules of filter scopes and by relevant datasources

is this correct?


Additionally there can be multiple filter components : global and one for each tab 
and global need be consolidate into each of the tabs separately 
--
",ckdarby,"
--
@graceguo-supercat One of the things I just bumped into today is wanting the ability to not load charts **_until_** the dashboard filter has been applied by the user.
--

--
@graceguo-supercat I wanted to provide additional feedback to this SIP & to the community to understand our frustration as a user of Superset with the disconnect between charts and the filters.

One of the major issues that we struggle with is we define our generic charts with nearly no filters and then apply the filters only from the dashboard perspective to allow all our charts to interactively be changed through the dashboard filter.

When we're making the individual charts the problem is the filter box is not tied into that chart yet. This means the individual charts when crafting them are running queries that basically are along the lines of, ""fetch all data"". For us, that is not doable because it is billions upon billions and PB kind of storage. 

Once on the dashboard, the same issue happens when the filter box hasn't had values selected by the user yet but the charts try to run with the ""defaults"" on the filter box.

These queries usually have to be manually killed on Presto's side and then we apply the filter box from the dashboard which allows us to narrow into exactly the range we want to see with the specific partitions.

If there is an existing functionality to tackle this I'm all ears, the only thing we saw as a potential stop gap was custom viz plugin hacks.
--
"
10249,OPEN,How to reset the state of a WebGL context when switching to a different tab,inactive,2020-11-26 22:20:25 +0000 UTC,akelai,In progress,,"I'm using Kepler.gl and Chrome, but maybe this happens also with something else (whatever uses WebGL) and other browsers:

when I reach a number of Kepler.gl maps in a dashboard, more than the max number of active WebGL contexts allowed in Chrome, the first ones get lost.

This happens even if the Kepler.gl maps are in different ""tabs"" (of the same Superset dashboard): that is, going from a tab to another one, doesn't reset the active WebGL contexts, so every new Kepler.gl map loaded contributes to reaching the max allowed even if they're wisely positioned in different tabs in the (vain) effort to avoid this problem.

I've found [this package](https://github.com/stackgl/gl-reset) from stack.gl:

> **gl-reset**
> *Completely reset the state of a WebGL context, deleting any allocated resources.*

Looks like something that could help here, but I'm not sure how.
I'd like the WebGL contexts to be reset when I switch to a different tab, instead of keeping all of them.

Any suggestion on how to achieve that? (using gl-reset or any other way)
",,,stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",akelai,"
--
I forgot to mention that the effect is quite bad on the dashboards, in fact when I say that something ""gets lost"" I mean that the iframe content (e.g. Kepler.gl maps, or even charts in iframes coming from 3rd-party service) is not shown, and in its place there is a browser-window-like rectangular sad face with ""X"" eyes.
--
",,,,,,,,
10185,OPEN,Integrity error; probably unique constraint,bug; inactive,2020-12-25 21:55:34 +0000 UTC,yuexianchao,Opened,,"
For different dB, adding the same table name will result in an error. The source is two different dB, the index of tables table unique (table_ Name) should be changed to unique (database_id,table_name)I hope this problem can be corrected in the next version. Thank you",,,issue,"
--
Issue Label Bot is not confident enough to auto-label this issue. See [dashboard](https://mlbot.net/data/apache/incubator-superset) for more details.
--
",rvanaalderen,"
--
Same problem as #10428 ?
I face the same problem in release 0.36 and 0.37

See also PR fix: old unique constraint remnance #9882
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,
10181,OPEN,Closing many SQL Lab tabs in succession returns 404s,bug,2021-02-18 23:51:57 +0000 UTC,etr2460,In progress,,"When you close many SQL Lab tabs in succession with SQL Lab backend persistence enabled, a lot of 404s are returned from the server. Sometimes this results in many toasts on the screen, sometimes not.

### Expected results

No errors

### Actual results

Errors

#### Screenshots
https://i.imgur.com/fw2qnsB.gif

#### How to reproduce the bug
Enable SQL Lab Backend persistence
Open a bunch of tabs, then close them

### Environment

(please complete the following information):

- superset version: master
- python version: 3.6

@betodealmeida can you take a look? Thanks!",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",etr2460,"
--
bumping this back up again @betodealmeida , any thoughts?
--
",betodealmeida,"
--
I'm going to take a look at this.
--
",,,,
10166,OPEN,Unable to use Propagate feature on timeseries charts,bug,2020-11-23 14:45:45 +0000 UTC,at-tn,Opened,,"We are unable to use the Propagate option on charts. It is available through customize chart action for ""Time Series"" chart (a line chart for example). Our use case is to trigger updates in other charts in the parent dashboard when the time range in one of the child chart is updated. 
Our environment:
OS: Debian GNU/Linux 10 
Superset version: 0.35.2

### Expected results
Selecting Propagate on one chart should reflect selection on other charts in the same dashboard. 
Alternatively some documentation on how to get the propagate feature to work would be tremendously helpful.

### Actual results
Selecting Propagate on one chart does not reflect selection on other charts in the same dashboard. 

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

### Environment
OS: Debian GNU/Linux 10 
- superset version: 0.35.2
- python version: Python 3.6.10
- node.js version:  v. 12
- npm version: n/a

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,issue,"
--
Issue Label Bot is not confident enough to auto-label this issue. See [dashboard](https://mlbot.net/data/apache/incubator-superset) for more details.
--
",rumbin,"
--
I can confirm this bug on 0.35
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",iercan,"
--
This bug still exist on 0.37.2.
I'm using amancevice/superset:0.37.2 docker image,  using mysql as metastore and configured celery backend. 

--
",,,,
10132,OPEN,Error with Login on Windows 10 using Docker: sqlalchemy.exc.ProgrammingError,bug; inactive,2020-10-10 12:37:48 +0000 UTC,ckaldemeyer,In progress,,"A clear and concise description of what the bug is.

### Expected results

Successful docker login on localhost as described within the documentation: https://superset.incubator.apache.org/installation.html#start-with-docker

### Actual results

First problem: The user admin with password admin ist not working and no login possible as opposed to the documentation.

So I tried the following based on this question (https://stackoverflow.com/questions/55961428/default-login-for-docker-image-of-superset) in order to create a user admin with password admin:

    docker-compose exec superset bash -c ""export FLASK_APP=superset && flask fab create-admin""

This seems to have succeeded:

      C:\Users\foo\Documents\Programmierung\incubator-superset>docker-compose exec superset bash -c ""export FLASK_APP=superset && flask fab create-admin""
      Username [admin]: admin
      User first name [admin]: admin
      User last name [user]: admin
      Email [admin@fab.org]: bla@blub.com
      Password:
      Repeat for confirmation:
      Loaded your LOCAL configuration at [/app/pythonpath/superset_config.py]
      INFO:superset.utils.logging_configurator:logging was configured successfully
      /usr/local/lib/python3.6/site-packages/flask_caching/__init__.py:189: UserWarning: Flask-Caching: CACHE_TYPE is set to null, caching is effectively disabled.
        ""Flask-Caching: CACHE_TYPE is set to null, ""
      Recognized Database Authentications.
      Admin User admin created.

After a ""successfull"" login, I get the following error:

    sqlalchemy.exc.ProgrammingError
    sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation ""user_attribute"" does not exist

This is also included in the logs:

    sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedTable) relation ""user_attribute"" does not exist

and it seems as if there is a missing table. But I haven't even touched the database yet..

#### Screenshots

https://pasteboard.co/JeggARr.png

#### How to reproduce the bug

Just follow the official documentation using Windows 10: https://superset.incubator.apache.org/installation.html#start-with-docker

### Environment

(please complete the following information):

- superset version: 0.36.0
- python version: Python 3.7.6
- node.js version: ? 
- npm version: ?

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

The error log is identical to the one in this comment: https://github.com/apache/incubator-superset/issues/7637#issuecomment-498609977


Any hints?",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.57. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",ckaldemeyer,"
--
 
--

--
Here's the relevant part of the log file:

    $ docker-compose up
    Starting superset_db    ... done
    Starting superset_cache ... done
    Starting superset_init         ... done
    Starting superset_node         ... done
    Starting superset_app          ... done
    Starting superset_worker       ... done
    Starting superset_tests_worker ... done
    Attaching to superset_cache, superset_db, superset_tests_worker, superset_node, superset_worker, superset_app, superset_init
    superset_cache           | 1:C 25 Jun 11:20:04.394 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
    superset_cache           |                 _._
    superset_cache           |            _.-``__ ''-._
    superset_cache           |       _.-``    `.  `_.  ''-._           Redis 3.2.12 (00000000/0) 64 bit
    superset_cache           |   .-`` .-```.  ```\/    _.,_ ''-._
    superset_cache           |  (    '      ,       .-`  | `,    )     Running in standalone mode
    superset_cache           |  |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379
    superset_cache           |  |    `-._   `._    /     _.-'    |     PID: 1
    superset_cache           |   `-._    `-._  `-./  _.-'    _.-'
    superset_cache           |  |`-._`-._    `-.__.-'    _.-'_.-'|
    superset_cache           |  |    `-._`-._        _.-'_.-'    |           http://redis.io
    superset_cache           |   `-._    `-._`-.__.-'_.-'    _.-'
    superset_cache           |  |`-._`-._    `-.__.-'    _.-'_.-'|
    superset_cache           |  |    `-._`-._        _.-'_.-'    |
    superset_cache           |   `-._    `-._`-.__.-'_.-'    _.-'
    superset_cache           |       `-._    `-.__.-'    _.-'
    superset_cache           |           `-._        _.-'
    superset_cache           |               `-.__.-'
    superset_cache           |
    superset_cache           | 1:M 25 Jun 11:20:04.398 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
    superset_cache           | 1:M 25 Jun 11:20:04.398 # Server started, Redis version 3.2.12
    superset_cache           | 1:M 25 Jun 11:20:04.398 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
    superset_db              |
    superset_db              | PostgreSQL Database directory appears to contain a database; Skipping initialization
    superset_db              |
    superset_db              | 2020-06-25 11:20:04.504 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
    superset_db              | 2020-06-25 11:20:04.504 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
    superset_cache           | 1:M 25 Jun 11:20:04.399 * DB loaded from disk: 0.001 seconds
    superset_cache           | 1:M 25 Jun 11:20:04.399 * The server is now ready to accept connections on port 6379
    superset_db              | 2020-06-25 11:20:04.507 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
    superset_db              | 2020-06-25 11:20:04.519 UTC [25] LOG:  database system was shut down at 2020-06-25 11:19:56 UTC
    superset_db              | 2020-06-25 11:20:04.526 UTC [1] LOG:  database system is ready to accept connections
    superset_init            | /usr/bin/env: bash\r: No such file or directory
    superset_init exited with code 127

Any hints?
--
",willbarrett,"
--
@ckaldemeyer when you run `docker-compose up` please look carefully at the output from the `superset-init` container. This is the container responsible for running database migrations. Are there any errors, or does the container finish its work and exit with status 0?
--

--
Your `superset_init` container is unable to find the program `bash` - this is a very odd error that I have not seen before. Perhaps some of the other folks who are running Superset on Windows in the community can shed some light. You may have a bad container build? Perhaps attempt to rebuild your containers from scratch with `docker-compose build` - that's a guess, I have no idea if it will help.
--
",rgarci,"
--
#10547 Same issue, this is how I fixed it:

>  I guess you're running on windows, this is a problem caused by git cause it has a default configuration that changes Unix-style line breaks to windows ones, in order to surpass this you need to:
> 
> delete your current repo
> apply git config --global core.autocrlf false so it doesn't change the line breaks
> clone the repo again
> Now you won't be seing this error
> checkout : https://stackoverflow.com/questions/29045140/env-bash-r-no-such-file-or-directory
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,
10109,OPEN,"Make it posible to hide ""Login"" top right for public user",enhancement:request; inactive,2020-12-25 22:54:49 +0000 UTC,larsmars,In progress,,"**Is your feature request related to a problem? Please describe.**
Login btn is visible for public user when viewing published dashboard/charts no need to expose this to public user

**Describe the solution you'd like**
Posible to hide login btn top right by config

**Additional context**
Superset version: 0.36.0
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",BLasan,"
--
@larsmars Is this still opened? Can I work on this?
--
",larsmars,"
--
Yes
--

--
Yes, but then you lose the logo (APP_ICON) top left
--

--
The ""hack"" not the solution is:

```
.navbar-right {
    display: none;
}
```
--

--
bump
--
",karen,"
--
@larsmars doesn't adding `?standalone=true` or` &standalone=true` to the chart/dashboard url fix this for you?
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,
10108,OPEN,[chart]sort legends by name or custom sort,enhancement:request,2021-02-16 06:40:02 +0000 UTC,larsmars,In progress,,"**Is your feature request related to a problem? Please describe.**
Legends label is not sorted in the same order every time, it depends on count

ie:
Visualization Type: Pie Chart
Metric: COUNT_DISTINCT(unitId)
The count of unitId will determin the sort of legends/labels
The sort order will change over time as the unitId count changes.
Not posible to have static order of legends/labels

**Describe the solution you'd like**
1. Posible to sort in alphabetical order
2. Dictate sort based on label names (query result)

**Additional context**
Superset version: 0.36.0
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.96. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",larsmars,"
--
bump
--
",curiousus,"
--
I would also like to have the ability to determine the sorting order of the legends in the chart types that can have legends. This is especially useful when working i.e. with data buckets or categories that express a certain gradation (e.g. ""minor damage"", ""considerable damage"", ""heavy damage"" etc).

It would be nice to have this in a future update. :-)
--
",,,,
10103,OPEN,[Register Page] Make Recaptcha Optional.,bug; enhancement:request; inactive,2020-12-25 21:55:21 +0000 UTC,badman-rodriguez,Opened,,"Is it possible to make this optional?

<img width=""1202"" alt=""Screen Shot 2020-06-18 at 4 01 41 PM"" src=""https://user-images.githubusercontent.com/15989103/85071760-99590d80-b17d-11ea-8b99-587e982458b5.png"">",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.74. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",willbarrett,"
--
@badman-rodriguez can you provide more context? I'm not sure what you're referring to here.
--
",badman,"
--
ok. @willbarrett 

In `superset/config.py` there is a setting:
```
# Will allow user self registration, allowing to create Flask users from Authorized User
AUTH_USER_REGISTRATION = True
```

^ when that is set to true, it allows a user to register to get access.
Which is great, but there is a requirement to have GOOGLE RECAPTCHA...

Because you get an error that says `RECAPTCHA_PUBLIC_KEY` this is missing.

I was hoping to make this not a requirement but optional when registering a user.
--

--
> ok. @willbarrett
> 
> In `superset/config.py` there is a setting:
> 
> ```
> # Will allow user self registration, allowing to create Flask users from Authorized User
> AUTH_USER_REGISTRATION = True
> ```
> 
> ^ when that is set to true, it allows a user to register to get access.
> Which is great, but there is a requirement to have GOOGLE RECAPTCHA...
> 
> Because you get an error that says `RECAPTCHA_PUBLIC_KEY` this is missing.
> 
> I was hoping to make this not a requirement but optional when registering a user.

@willbarrett does this make sense? let me know if you need more info. Thanks!
--
",skillnull,"
--
I have the same problem.
--
",nytai,"
--
Looks like Flask-WTF has a pr open to expose a config to disable recaptcha, but it hasn't been merged. https://github.com/lepture/flask-wtf/pull/214

I think the only way to disable this is to implement a custom form/view. @dpgaspar may be able to give some pointers here.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
"
10098,OPEN,Filter on bool column not working properly in SQL Lab -> Explore,.pinned; bug,2020-10-29 06:13:09 +0000 UTC,dolorez,In progress,,"When a query in SQL Lab is executed and a filter is applied to a boolean column, it errors out on datatype comparison (boolean vs string). 
Note - the behavior
### Expected results
The filter should work - the generated query should not enclose the field value in single quotes. 

### Actual results
The query generated encloses the filter value in single quotes so the where clause generated looks (some_bool_flag = 'true') and this that in turn generates type mismatch

#### How to reproduce the bug

1. Go to SQL Lab -> SQL Editor
2. Write a query that returns a boolean column
3. Execute the query
4. Click on Explore
5. Scroll down to filters
6. Add a filter on a boolean column and make sure you stay in Simple (not Custom SQL)
7. Use equals and set filter value to ""true"" (without double quotes)
8. Execute the query to see the error messge
9. Click on menu -> View query to inspect that the filter value was enclosed in single quotes

### Environment

(please complete the following information):

- superset version: `superset version
0.34.1
- python version: `python --version`
Python 3.6.9
- node.js version: `node -v`
v12.18.0
- npm version: `npm -v`
6.14.4
### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
I have no control over the environment :(
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

When the filter is entered using the Custom SQL instead of Simple, it works fine
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.95. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",rumbin,"
--
Could you add the type of database, please?
--
",dolorez,"
--
It's presto and that particular query goes to hive
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",bkyryliuk,"
--
issue still persists on presto database. It's not possible to replicate for mysql as mysql interpret boolean as tinyint.

Selector looks like this:
![image](https://user-images.githubusercontent.com/5727938/96056939-b0162500-0e3c-11eb-99d8-b9349343cc21.png)

Clicking on it results in: 
![image](https://user-images.githubusercontent.com/5727938/96056973-bf956e00-0e3c-11eb-917e-167f4c515297.png)

Sqllab correctly sees the type of the column:
![image](https://user-images.githubusercontent.com/5727938/96057098-05523680-0e3d-11eb-8c00-a9f1eee768d3.png)


@etr2460, @john-bodley  - curious if you have similar issue @ airbnb.
--
",eugeniamz,"
--
I see the same issue with Vertica DB. It does not show the option but if you choose the second one you have a TRUE but the first is empty. I work around by going to CUSTOM SQL but this should be resolved. 
![2020-10-28_18-07-06 (1)](https://user-images.githubusercontent.com/58375897/97501996-a4e8ec00-1948-11eb-9367-aa4c0fd33032.gif)

--
"
10093,OPEN,Support passing X-Presto-Extra-Credential containing values (from LDAP login to Superset UI) when using impersonation on presto data source,enhancement:request,2020-12-07 23:05:43 +0000 UTC,tooptoop4,In progress,,"current behaviour:
1. Presto datasource can be setup by an admin in superset ui, with impersonation ticked
2. Normal users can then login to superset ui and run queries in SQLlab (even though they never created the datasource, their username is being sent to presto since impersonation was enabled)

proposal:
3. I assume since the user logged into superset UI via LDAP, that superset/flask component has their username/password in memory, and can then send these to presto in the X-Presto-Extra-Credential header (this is needed so the presto jdbc catalogs are being queried as the user logged in to superset) ie superset > presto > some sqlserver/oracle.etc catalog to presto

See 
https://github.com/prestosql/presto/issues/4006#issuecomment-643795271
https://github.com/prestosql/presto-python-client/issues/6

",,,issue,"
--
Issue Label Bot is not confident enough to auto-label this issue. See [dashboard](https://mlbot.net/data/apache/incubator-superset) for more details.
--
",tooptoop4,"
--
@etr2460 @villebro any idea on this one?
--

--
crispy
--

--
crispy
--
",mistercrunch,"
--
Should be possible through
https://github.com/apache/incubator-superset/blob/master/superset/config.py#L712-L727

You may have to read some of the pyhive code to figure out exactly how to pass the connection arguments through. I think we did something similar at previous companies.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,
10091,OPEN,ClickHouse IPv6 error,data:connect:clickhouse,2021-01-02 21:45:50 +0000 UTC,kanadaj,Opened,,"When trying to query a ClickHouse database from SQL Lab with an IPv6 field, Superset gives the following error:
```
Could not convert ::ffff:0000:0000 with type IPv6Address: did not recognize Python value type when inferring an Arrow data type
```
(IP address data replaced with 0s). I can't see any stack traces in the console output.

#### How to reproduce the bug

1. Create a new ClickHouse database
2. Create a table with at least one IPv6 field
3. Insert some data
4. Add the database to Superset
5. Go to SQL Lab and try to query the data via `SELECT * FROM ...`

### Environment

(please complete the following information):

- superset version: Superset 0.999.0dev
- python version: Python 3.6.9
- node.js version: v10.19.0
- npm version: 6.13.4

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.92. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
10088,OPEN,[SIP-47] modernize/facelift superset.apache.org,org:preset; sip,2020-07-16 12:58:13 +0000 UTC,mistercrunch,In progress,,"<img width=""1043"" alt=""Screen Shot 2020-06-17 at 10 20 56 PM"" src=""https://user-images.githubusercontent.com/487433/84981210-d7f0b880-b0e8-11ea-9c20-f0174af32569.png"">

### Motivation

Our current documentation at superset.apache.org has a certain number of issues:
1. look/feel is dated
1. RST/Sphinx/ old templates is a fading/dying standard, markdown seems much more popular
1. can't add rich/dynamic content - it's super static
1. Content is a bit of a patchwork, needs review/restructure/rewrite

This SIP looks to address 1,2 and 3, and hope that those provide a foundation to docs that people want to contribute to and improve, content-wise. 

### Proposed Change

Migrating the current website from/to:
- **framework**: moving from Sphinx to [Gatsby](https://github.com/gatsbyjs/gatsby) (45k stars) / [DocZ](https://www.docz.site/) (19k stars)
- **language**: moving from RST to markdown
- **hosting**: moving from [readthedocs](https://readthedocs.org/) to GitHub Pages (or similar)

Gatsby would allow us to build a fast and modern static site. It provides a lot of flexibility / extensibility if we wanted to do more complex things like exposing a community blog.

DocZ is markdown + jsx, allowing us to integrate arbitrarily complex things in our docs, including things like live examples (embedded Superset components) or anything really.

Markdown is a more common markup language for docs. Most people know it or can learn it very quickly, including tech writers, it's becoming fairly standard.

### New or Changed Public Interfaces

N/A

### New dependencies

Gatsby / Docz and whatever plugins and things we need there

### Migration Plan and Compatibility

Would keep everything under the current main repo under `docs/`
* individual RST files need to be converted to MD, this can be mostly automated https://cloudconvert.com/rst-to-md , we probably need to do some manual work
* build a new Gatsby / DocZ site, with a good looking theme, maybe with [AntD](https://ant.design/) + [gatsby-plugin-antd](https://www.npmjs.com/package/gatsby-plugin-antd)


### Rejected Alternatives
- Improving / theming readthedocs
- Other static sites / documentation builder",,,issue,"
--
Issue Label Bot is not confident enough to auto-label this issue. See [dashboard](https://mlbot.net/data/apache/incubator-superset) for more details.
--
",etr2460,"
--
100% in favor of moving off of RST files and onto something markdown driven. 

I'm curious what other static sites/documentation builders you considered when evaluating options. I'm not super familiar with the space, but I know [Docusaurus](https://docusaurus.io/) is pretty commonly used within the frontend world (Babel, Jest, Redux), also supports adding a blog, and has other features like translated and versioned docs. What tradeoffs did you consider when picking Gatsby + DocZ?
--

--
I think your pros and cons list is accurate. When I was looking at Docusaurus, it seemed like the v1 branch supported versioning and i18n, but not JS in Markdown. v2 has JS in Markdown (which seems like a really good selling point), but hasn't implemented i18n yet.

To me it all comes down to how much effort we want to spend to start up and maintain the new docs. Docusaurus seems easier to spin up and would free up more time to build Superset features vs. documentation. But at the cost of less overall flexibility (an unknown cost for me since it seems to still support all the use cases we want here).

I'd be happy with either option, but might lean slightly towards Docusaurus as it feels like our competitive advantage should live with the product vs. the documentation (thus we should spend more time building the product and less time building docs).
--
",willbarrett,"
--
+1 :shipit: 
--
",rusackas,"
--
Wondering if it's reasonable to use some CSS-in-JS/Emotion in the mix, in order to leverage the @superset-ui theme package here, and ""tie the room together"" a bit. This would make sense if there's ever an interest in pulling some of the docs/tutorials back into Superset (i.e. inline help buttons popping up tutorial content in a modal).
--
",mistercrunch,"
--
@etr2460 I have to admit I didn't go super deep in the research. I think Gatsby is super connected to everything and very extensible. DocZ being markdown + jsx/React seemed perfect.

Check out the Gatsby plugin ecosystem
https://www.gatsbyjs.org/plugins/

DocZ appeared to be top of their doc plugins. I have some experience with both and I think it's hard to top.
--

--
@etr2460 Docusaurus looks pretty sweet too, here's their take on how it relates to Gatsby:
https://v2.docusaurus.io/docs/#gatsby

---------

Gatsby is packed with a lot of features, has a rich ecosystem of plugins and is capable of doing everything that Docusaurus does. Naturally, that comes at a cost of a higher learning curve. Gatsby does many things well and is suitable for building many types of websites. On the other hand, Docusaurus tries to do one thing super well - be the best tool for writing and publishing content.

GraphQL is also pretty core to Gatsby, although you don't necessarily need GraphQL to build a Gatsby site. In most cases when building static websites, you won't need the flexibility that GraphQL provides.

Many aspects of Docusaurus 2 were inspired by the best things about Gatsby and it's a great alternative.

---------


--

--
In short:
* Docusaurus is more fitted to a simpler use case
* Gatsby is more extensible / flexible

Key immediate things:
* Docusaurus has better support for versioning, DocZ is working on it, there are ways to do it with DocZ, it's more involved though
* Docusaurus has a i18n ""coming soon"" thing that could be interesting to us
--

--
Looking at the Airflow docs http://airflow.apache.org (rewrite was sponsored by Google I believe), I'm inspired that we could do more eventually, building a blog, events page (gatsby has a meetup.com plugin for instance), maybe even things like a dynamic FAQ or some integrations with StackOverflow, Github and whatnot. Unclear how much flexibility is offered on the Docusaurus to do that.

The beauty of the markdown-based solution is that it becomes very much the meat and it seems like changing the shell should be easy.

----------

I terms of effort, we're interested in sponsoring / leading this at Preset. We're not looking for any sort of editorial control on it, just to have a rich, beautiful and modern community website.
--
",ktmud,"
--
Another thing to note is should we or how can we integrate autogenerated superset Python module/CLI and Swagger API docs.

DocZ have moved to Gatsby, so it seems any plugin available in Gatsby might also have become available for DocZ.
--
"
10082,OPEN,Y Axis percentage (%) in Time-series Bar Chart do not respect Y Axis Format,bug; inactive,2020-10-22 04:12:33 +0000 UTC,larsmars,In progress,,"Y Axis percentage (%) in Time-series Bar Chart do not respect Y Axis Format

### Expected results
Percentage to be displayed without decimals when .1s format is selected ie: 20%

### Actual results
Percentage is displayed with decimals ie: 20.0%

#### Screenshots
![percentage](https://user-images.githubusercontent.com/5154589/84889880-e4015980-b099-11ea-819a-c93fcbfe5c5f.PNG)

#### How to reproduce the bug
1. Create chart of type Time-series Bar Chart with percentage
2. Select Y Axis Format .1s
3. Observe numbers is displayet with decimals

### Environment
superset version: 0.36.0
python version: 3.6.10
node.js version: 12.16.1
npm version: 6.13.4

### Checklist

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.88. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",larsmars,"
--
bump
--
",,,,,,
10064,OPEN,Item with nordic letters is not properly sorted in lists,bug; i18n:others,2020-10-28 11:16:25 +0000 UTC,larsmars,In progress,,"Item with nordic letters is not properly sorted in lists
Sort Ascending is selected

### Expected results
Correct sorting:
 -  - 

### Actual results
Actual sorting:
 -  

The rest of the alphabet is ordered correct (A-Z)

#### Screenshots
![filter_let](https://user-images.githubusercontent.com/5154589/84767721-3461b400-afd3-11ea-84d1-0600db026fe5.png)

![sort_kom](https://user-images.githubusercontent.com/5154589/84768737-01b8bb00-afd5-11ea-8a0e-71125179da4a.PNG)

#### How to reproduce the bug
1. Go to dashbord with filter with items starting with   
2. Observe list is not correctly sorted

### Environment
- superset version: 0.36.0
- python version: 3.6.10
- node.js version: 12.16.1
- npm version: 6.13.4

### Checklist
Make sure these boxes are checked before submitting your issue - thank you!
- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.96. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",larsmars,"
--
bump
--

--
You are right, but the dropdown sorting feature on chart in superset should work regardless of this. 
Code in parameters (edit chart, superset): `""filter_configs"": [{""asc"": true, `
--
",nytai,"
--
I could be mistaken, but isn't sorting handled by the database therefore it's the underlying database that is incorrectly sorting these results? 
--
",,,,
10051,OPEN,Default format for DATE columns is '%Y-%m-%d %H:%M:%S.%f',bug,2021-03-31 06:41:51 +0000 UTC,EliYk,Opened,,"Hi,

First of all, cheers! This project is awesome, growing magnificently.


It seems the default format for temporal columns is the same for DATE and DATETIME.

This causes errors, which I can solve by editing the DATE columns manually, and adding `%Y-%m-%d` at the  'Datetime Format' field.

Would it be feasible to make DATE columns use `%Y-%m-%d` / `YYYY-MM-DD` format by default?


Thanks,
Eli",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.65. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",allantaylor81,"
--
We have the same problem  Did you solve it in the end Look forward to your reply
--
",,,,,,
10024,OPEN,Handle empty data vizualisation,enhancement:request; inactive,2020-10-10 12:37:43 +0000 UTC,DovAlduin,Opened,,"**Is your feature request related to a problem? Please describe.**
I have no data on a time series in line chart  but a linear interpolation is applied by default so I can't see at first glance whether I have data recorded or not.

**Describe the solution you'd like**
Have a way (checkbox, list) to choose how no data in time series are processed : no visualisation, linear interpolation or any other suggestion

**Describe alternatives you've considered**

**Additional context**
The line chart I got with Superset :
![compensation_des_donnees_superset](https://user-images.githubusercontent.com/30531836/84170668-33cc9900-aa7a-11ea-9e1d-2e42262f592d.jpg)

And that is what I would like to achieve :
![compensation_des_donnees](https://user-images.githubusercontent.com/30531836/84170946-81e19c80-aa7a-11ea-8c3d-7cdfd6b6ed42.jpg)",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.88. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",rumbin,"
--
I think that this is an issue that beginners are struggling with quite often...
--
",,,,,,
10004,OPEN,[TRACKER] Migrate JavaScript files to TypeScript,change:frontend; enhancement:request; good first issue,2021-03-23 05:29:56 +0000 UTC,etr2460,In progress,,"This issue tracks Superset's migration from JavaScript to TypeScript (as started in [SIP-36](https://github.com/apache/incubator-superset/issues/9101)). If you'd like to help with the migration, feel free to take an unchecked directory and convert the files within the immediate directory from JavaScript/JSX to TypeScript/TSX. #9162 and #9180 provide some tips for performing the migration. Once completed and PR'ed out, feel free to tag myself (@etr2460) or anyone else in the file's git-blame for review.

We really appreciate any work done here, and are happy to help resolve any issues you may come across!

## Tracker

- [x] `src`
  - [ ] `src/CRUD`
  - [ ] `src/SqlLab`
    - [ ] `src/SqlLab/actions`
    - [ ] `src/SqlLab/components`
    - [ ] `src/SqlLab/reducers`
    - [ ] `src/SqlLab/utils`
  - [x] `src/addSlice`
  - [x] `src/api`
  - [ ] `src/chart`
    - [x] `src/common/components`
      - [x] `src/common/components/Collapse`
      - [x] `src/common/components/Modal`
      - [x] `src/common/components/Radio`
      - [x] `src/common/components/Tabs`
      - [x] `src/common/components/Tooltip`
    - [x] `src/common/hooks`
      - [x] `src/common/hooks/apiResources`
      - [x] `src/common/hooks/useComponentDidMount`
  - [ ] `src/components`
    - [x] `src/components/Alert`
    - [x] `src/components/AsyncAceEditor`
    - [ ] `src/components/AsyncSelect`
    - [x] `src/components/Button`
    - [x] `src/components/ButtonGroup`
    - [x] `src/components/CachedLabel`
    - [ ] `src/components/Checkbox`
    - [x] `src/components/DeleteModal`
    - [x] `src/components/DynamicPlugins`
    - [x] `src/components/ErrorMessage`
    - [x] `src/components/FacePile`
    - [x] `src/components/FaveStar`
    - [x] `src/components/FilterableTable`
    - [x] `src/components/FlashProvider`
    - [ ] `src/components/Icon`
    - [ ] `src/components/Icons`
    - [x] `src/components/ImportModal`
    - [x] `src/components/Label`
    - [x] `src/components/LastUpdated`
    - [x] `src/components/ListView`
      - [x] `src/components/ListView/Filters`
    - [ ] `src/components/ListViewCard`
    - [x] `src/components/Loading`
    - [x] `src/components/Menu`
    - [ ] `src/components/ModalTrigger`
    - [x] `src/components/NavDropdown`
    - [x] `src/components/Pagination`
    - [x] `src/components/Popover`
    - [ ] `src/components/Select`
      - [x] `src/components/Select/WindowedSelect`
    - [x] `src/components/TableLoader`
    - [x] `src/components/TableView`
    - [ ] `src/components/URLShortLinkButton`
    - [x] `src/components/dataViewCommon`
  - [ ] `src/dashboard`
    - [ ] `src/dashboard/actions`
    - [ ] `src/dashboard/components`
      - [x] `src/dashboard/components/FiltersBadge`
      - [ ] `src/dashboard/components/dnd`
      - [ ] `src/dashboard/components/filterscope`
      - [ ] `src/dashboard/components/gridComponents`
        - [ ] `src/dashboard/components/gridComponents/new`
      - [ ] `src/dashboard/components/menu`
      - [x] `src/dashboard/components/nativeFilters`
        - [x] `src/dashboard/components/nativeFilters/FilterBar`
        - [x] `src/dashboard/components/nativeFilters/FiltersConfigModal`
          - [x] `src/dashboard/components/nativeFilters/FiltersConfigModal/FiltersConfigForm`
            - [x] `src/dashboard/components/nativeFilters/FiltersConfigModal/FiltersConfigForm/FilterScope`
          - [x] `src/dashboard/components/nativeFilters/FiltersConfigModal/Footer`
      - [ ] `src/dashboard/components/resizable`
    - [ ] `src/dashboard/containers`
    - [ ] `src/dashboard/fixtures`
    - [ ] `src/dashboard/reducers`
    - [ ] `src/dashboard/util`
      - [x] `src/dashboard/util/charts`
      - [ ] `src/dashboard/util/logging`
  - [ ] `src/datasource`
  - [ ] `src/explore`
    - [ ] `src/explore/actions`
    - [ ] `src/explore/components`
      - [x] `src/explore/components/DatasourcePanel`
      - [ ] `src/explore/components/controls`
        - [x] `src/explore/components/controls/DateFilterControl`
          - [x] `src/explore/components/controls/DateFilterControl/frame`
        - [x] `src/explore/components/controls/DndColumnSelectControl`
          - [x] `src/explore/components/controls/DndColumnSelectControl/components`
          - [x] `src/explore/components/controls/DndColumnSelectControl/utils`
        - [ ] `src/explore/components/controls/FilterControl`
        - [ ] `src/explore/components/controls/MetricControl`
    - [ ] `src/explore/controlPanels`
    - [ ] `src/explore/propTypes`
    - [ ] `src/explore/reducers`
  - [x] `src/filters`
    - [x] `src/filters/components`
      - [x] `src/filters/components/Range`
      - [x] `src/filters/components/Select`
      - [x] `src/filters/components/Time`
  - [x] `src/logger`
    - [x] `src/logger/actions`
  - [x] `src/messageToasts`
    - [x] `src/messageToasts/actions`
    - [x] `src/messageToasts/components`
    - [ ] `src/messageToasts/containers`
    - [x] `src/messageToasts/enhancers`
    - [ ] `src/messageToasts/reducers`
    - [ ] `src/messageToasts/utils`
  - [ ] `src/middleware`
  - [ ] `src/modules`
  - [x] `src/profile`
    - [x] `src/profile/components`
  - [x] `src/setup`
  - [ ] `src/showSavedQuery`
  - [x] `src/types`
  - [ ] `src/utils`
  - [x] `src/views`
    - [x] `src/views/CRUD`
      - [x] `src/views/CRUD/alert`
        - [x] `src/views/CRUD/alert/components`
      - [x] `src/views/CRUD/annotation`
      - [x] `src/views/CRUD/annotationlayers`
      - [x] `src/views/CRUD/chart`
      - [x] `src/views/CRUD/csstemplates`
      - [x] `src/views/CRUD/dashboard`
      - [x] `src/views/CRUD/data`
          - [x] `src/views/CRUD/data/components/SyntaxHighlighterCopy`
        - [x] `src/views/CRUD/data/database`
        - [x] `src/views/CRUD/data/dataset`
        - [x] `src/views/CRUD/data/query`
        - [x] `src/views/CRUD/data/savedquery`
      - [x] `src/views/CRUD/welcome`
    - [ ] `src/visualizations/FilterBox`
    - [ ] `src/visualizations/TimeTable`
    - [ ] `src/visualizations/presets`
  - [ ] `spec/__mocks__`
  - [ ] `spec/fixtures`
  - [ ] `spec/helpers`
    - [ ] `spec/javascripts/CRUD`
    - [x] `spec/javascripts/addSlice`
    - [ ] `spec/javascripts/chart`
    - [ ] `spec/javascripts/components`
      - [x] `spec/javascripts/components/ErrorMessage`
      - [x] `spec/javascripts/components/FilterableTable`
      - [ ] `spec/javascripts/components/ListView`
      - [x] `spec/javascripts/components/TableView`
      - [ ] `spec/javascripts/components/fixtures`
      - [ ] `spec/javascripts/dashboard/actions`
      - [ ] `spec/javascripts/dashboard/components`
        - [ ] `spec/javascripts/dashboard/components/dnd`
        - [ ] `spec/javascripts/dashboard/components/gridComponents`
          - [ ] `spec/javascripts/dashboard/components/gridComponents/new`
        - [ ] `spec/javascripts/dashboard/components/menu`
        - [x] `spec/javascripts/dashboard/components/nativeFilters`
        - [ ] `spec/javascripts/dashboard/components/resizable`
      - [ ] `spec/javascripts/dashboard/fixtures`
      - [ ] `spec/javascripts/dashboard/reducers`
      - [ ] `spec/javascripts/dashboard/util`
    - [ ] `spec/javascripts/datasource`
    - [ ] `spec/javascripts/explore`
      - [ ] `spec/javascripts/explore/components`
        - [x] `spec/javascripts/explore/components/DateFilterControl`
    - [x] `spec/javascripts/filters`
    - [ ] `spec/javascripts/messageToasts`
      - [ ] `spec/javascripts/messageToasts/components`
      - [ ] `spec/javascripts/messageToasts/reducers`
      - [ ] `spec/javascripts/messageToasts/utils`
    - [ ] `spec/javascripts/middleware`
    - [ ] `spec/javascripts/modules`
    - [x] `spec/javascripts/profile`
    - [ ] `spec/javascripts/showSavedQuery`
    - [ ] `spec/javascripts/sqllab`
      - [ ] `spec/javascripts/sqllab/actions`
      - [ ] `spec/javascripts/sqllab/reducers`
      - [ ] `spec/javascripts/sqllab/utils`
    - [ ] `spec/javascripts/utils`
        - [ ] `spec/javascripts/views/CRUD/alert`
        - [ ] `spec/javascripts/views/CRUD/annotation`
        - [ ] `spec/javascripts/views/CRUD/annotationlayers`
        - [ ] `spec/javascripts/views/CRUD/chart`
        - [ ] `spec/javascripts/views/CRUD/csstemplates`
        - [ ] `spec/javascripts/views/CRUD/dashboard`
          - [ ] `spec/javascripts/views/CRUD/data/database`
          - [ ] `spec/javascripts/views/CRUD/data/dataset`
          - [ ] `spec/javascripts/views/CRUD/data/savedquery`
        - [x] `spec/javascripts/views/CRUD/welcome`",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.87. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",tanmaylaud,"
--
@etr2460 I noticed issues in src/components/ListView/Filters.tsx (introduced by tsx migration) in PaginatedSelectComponent. Can you please confirm if it requires a fix?
--

--
@etr2460 I propose adding one more requirement here: Changing the class components to functional components and using react hooks. 
--

--
> > I noticed issues in src/components/ListView/Filters.tsx (introduced by tsx migration) in PaginatedSelectComponent.
> 
> @nytai is this related to the issues you were seeing? If so, were you still planning on addressing it?
> 
> > I propose adding one more requirement here: Changing the class components to functional components and using react hooks.
> 
> In general, I agree. However we have some class components that are extremely complex and difficult to migrate to functional components. Additionally, some of our class components might simply be better off remaining as classes because of their complexity. I'd like to scope this to only requiring TypeScript migration, but with the option of also moving towards functional components with hooks. At the very least, migrating to TypeScript will make it safer and easier to go from class based to functional react components in the future, and we can do that in future work

@etr2460 Agreed! Let's encourage people to go for functional components with hooks. Beginners will also get to learn something new in the process  
--
",etr2460,"
--
> I noticed issues in src/components/ListView/Filters.tsx (introduced by tsx migration) in PaginatedSelectComponent.

@nytai is this related to the issues you were seeing? If so, were you still planning on addressing it?

> I propose adding one more requirement here: Changing the class components to functional components and using react hooks.

In general, I agree. However we have some class components that are extremely complex and difficult to migrate to functional components. Additionally, some of our class components might simply be better off remaining as classes because of their complexity. I'd like to scope this to only requiring TypeScript migration, but with the option of also moving towards functional components with hooks. At the very least, migrating to TypeScript will make it safer and easier to go from class based to functional react components in the future, and we can do that in future work
--
",bera5186,"
--
@etr2460 I would also like to work on this issue !! Can you guide me on where to start !!!
--
",,,,
9998,OPEN,[Feature request] Bar chart custom ordering,.pinned; enhancement:request; viz:chart-bar,2021-03-31 06:27:59 +0000 UTC,bkyryliuk,Opened,,"**Is your feature request related to a problem? Please describe.**
Enable custom ordering of the bar chart, e.g. currently only alphabetical ordering exists and that makes it hard to work with ranges:
e.g. 101-200 would be before 51-100

Users would love to see the feature to specify custom ordering of the values.
This could be implemented in a similar way to the table `sort_by` 

Similar request was here as well:
https://github.com/apache/incubator-superset/issues/3380

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.99. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",rbk,"
--
Adding a note here.  For some reason, I am not seeing ordering in alphabetical order (as per @bkyryliuk 's observations) in its existing state, but seemingly ordered using the metrics from highest to lowest.
My x-axis is just 5 columns (years 2017 ~ 2021 inclusive)

If ordered alphabetically, I would assume the ordering would still be in chronological order, but currently, it's ordered 2018, 2017, 2019, 2020, 2021 from left-to-right.  It just so happens the metrics are from highest-to-lowest in that order.
--

--
> @rbk-paul have you tried the Sort Bars option in the Chart Options of the Customize tab? This should lead to alphabetical ordering.

Thanks @rumbin!  That worked!
--
",rumbin,"
--
@rbk-paul have you tried the Sort Bars option in the Chart Options of the Customize tab? This should lead to alphabetical ordering.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,
9990,OPEN,SQL Lab editor shortcuts override other common shortcuts,.pinned; bug,2021-03-27 08:13:36 +0000 UTC,qyra,Opened,,"The keyboard shortcuts in the ""SQL Editor"" view cause issues while editing because they override common text editing and browser shortcut conventions. I've had issues with them in Chromium and Firefox on Linux, and a colleague has had issues in Safari on a mac as well as well.

On Chrome:
Control-X is usually ""cut"" in a text box, in the editor it has been overridden to stop the query.
Control-F (find) is overridden and I have no idea what it was changed to (I don't see it documented)

Also, the ""Keyboard Shortcuts"" widget says that Control-T should open a new query tab but it actually opens a new browser tab so whatever override is attempted there doesn't work - but overriding standard browser shortcuts doesn't seem great so I would argue this is a feature, not a bug.

On Safari:
On safari Cmd-X still works for ""cut"" so presumably the shortcuts treat control and command differently. Cmd-F (find) is still broken.

Just for completeness the following work as expected:
Control-C (copy), Control-V (paste), Control-Z (undo), Control-Y (redo), Control-A (select all), Control-/ (comment code)

### Suggested Fix
Any of the following would be good:
- Pick new default keyboard shortcuts that do not override common text and browser shortcuts
- Allow changing individual shortcuts permanently (Either with some superset config, or with a user preference)
- If a keyboard shortcut change menu is too much work for this, being able to just disable individual shortcuts would also work as a stopgap.

### Server Environment
Superset 0.36.0
Python 3.6.9
Flask 1.1.2
Werkzeug 1.0.1

### Browser
Chromium on Linux, Version 83.0.4103.61

### Checklist

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.81. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",rumbin,"
--
Do not close, please
--

--
Related: #12429 
--
",,,,,,
9976,OPEN,Chart tooltips not showing in Mozilla Firefox,.pinned; browser:firefox; bug; cant-reproduce,2020-12-09 21:10:08 +0000 UTC,rvabhijith,Opened,,"Chart(Pie, Bar, Line, etc.) tooltips not showing in Mozilla Firefox.

In chrome and opera tooltips are working.

Using apache superset - Master Branch

Please see the image
![Screenshot 2020-06-03 at 6 50 27 PM](https://user-images.githubusercontent.com/39004901/83641822-adbcd800-a5cb-11ea-9cc6-effab3e264fc.png)
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.95. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",junlincc,"
--
@rvabhijith is it still an issue? we are not able to reproduce. 
--
",,,,,,
9974,OPEN,How to add greater than filter in apache superset?,enhancement:committed; viz:explore:filter,2021-02-15 06:00:11 +0000 UTC,sedhha,In progress,,"
Has superset changed its folder locations and interface? From a youtube tutorial, I found these options available in filter box:
![image](https://user-images.githubusercontent.com/36355685/83600250-c86f5c80-a58b-11ea-8e90-22c9de9ae1b8.png)
But what my filter looks like is:
![image](https://user-images.githubusercontent.com/36355685/83600336-fb195500-a58b-11ea-853a-003ac6f48a7a.png)


I want to apply a filter box which gives me flexibility to apply a filter over ranges. For example, this is my table:
![image](https://user-images.githubusercontent.com/36355685/83600423-3025a780-a58c-11ea-8354-a2eaa583ccf1.png)

I want to add a filter where it should display only those table results where videos>=100 and videos<=10000. Although filter allows me to select multiple value but what it doesn't allow me is to apply ranges, and as you know there are so many values between these two ranges, it gets very inefficient to apply these filters.

I tried to apply the filter as shown below:
![image](https://user-images.githubusercontent.com/36355685/83601395-2735d580-a58e-11ea-8eac-3d71ee20228e.png)
![image](https://user-images.githubusercontent.com/36355685/83601435-37e64b80-a58e-11ea-80b5-5305fc83288f.png)
![image](https://user-images.githubusercontent.com/36355685/83601480-50566600-a58e-11ea-93fa-69c4187580ba.png)
But how it works is this way:
![image](https://user-images.githubusercontent.com/36355685/83601525-63693600-a58e-11ea-9a83-28d7caa0b52a.png)
When we look at time filter:
![image](https://user-images.githubusercontent.com/36355685/83601599-84318b80-a58e-11ea-826a-5da1a5f9ecd7.png)
It works well and offers tons of customization, I was just curious if we can dynamically filter time then why not other values. Please do let me know if you have something for this.

**Additional context**
Add any other context or screenshots about the feature request here.
",,,issue,"
--
Issue Label Bot is not confident enough to auto-label this issue. See [dashboard](https://mlbot.net/data/apache/incubator-superset) for more details.
--
",aussat,"
--
@sedhha Agree, this would be great if dynamic filters added to Superset. I am not sure if it works for your problem, but you can try to apply CASE function to the range you need to filter. In order to do this, just add a new column in datasource editor with required SQL, then use this column in FIlter View.

```
CASE
    WHEN condition1 THEN result1
    WHEN condition2 THEN result2
    WHEN conditionN THEN resultN
    ELSE result
END;
```
--
",sedhha,"
--
Oh amaing @aussat Thanks a lot will try this out.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",AsjadMahmood,"
--
Any update how we can add  less than greater than filter in Filter Box so we can apply it on the dashboard, we do get to add these filters in other charts, but not in Filter Box
--
",lozbrown,"
--
@junlincc 
Any chance this might be possible soon?

--
"
9953,OPEN,[SIP-46] Proposal for strict pylint enforcement,sip,2020-06-18 15:32:27 +0000 UTC,john-bodley,Opened,,"## [SIP] Proposal for strict pylint enforcement

### Motivation

Over the past number of years there have been a number of initiatives to improve the quality of the Python code via a number of linters (`flake8`; deprecated), formatters (`black`, `isort`), and type checkers (`mypy`). The one elephant in the room is [`pylint`](https://en.wikipedia.org/wiki/Pylint), a powerful yet [sometimes perceived unusable](https://pythonspeed.com/articles/pylint/) quality checker.

Though `pylint` is enabled as part of [CI](https://github.com/apache/incubator-superset/blob/b8eaa114eddc6a42e2b69b7ab88479c3128a52be/.github/workflows/superset-python.yml#L28) and [`tox`](https://github.com/apache/incubator-superset/blob/52285aeb04ccad1611f88afdec92080ae04bdc0e/tox.ini#L142) many of the checks are ignored either [globally](https://github.com/apache/incubator-superset/blob/980dd2fd419893ec5ed9386ca28fa87115a3753d/.pylintrc#L84) or at the file level via: 

```
# pylint: disable=C,R,W
```

From previous experiences the value of many of these linters/checkers only materializes when strict enforcement exists for the entire repo (somewhat akin to heard immunity). Hence rather than merely trying to ensure that new code meets a higher bar, we (as a community) need to roll up our sleeves and take the somewhat painstaking approach of systematically re-enabling checks, removing blanket disablement, etc. somewhat akin to how we were able to strictly enforce `flake8` ([PRs](https://github.com/apache/incubator-superset/pulls?q=is%3Apr+%5Bflake8%5D+Resolving)) and `mypy` ([PRs](https://github.com/apache/incubator-superset/pulls?q=is%3Apr+style%28mypy%29+)). Once the entire code base adheres to the strictly enforced rules it is then relatively simple (in terms of effort) to ensure a PR adheres to the necessary checks.

### Proposed Change

I propose that by strictly enforcing `pylint` we will drastically improve the quality, readability, and maintainability of the Python code. I have worked on a number of repos which strictly enforce `pylint` (with very few checks disabled) and unequivocally can say that having `pylint` enabled helped to improve the code quality (preventing a number of bugs), consistency, readability, etc. Additionally I have not found `pylint` to be unusable as others have stated and do not find it cumbersome to use (once the necessary ground work has been laid).

Note the scale and effort involved is greater than any of the previous checkers and thus will require a systematic approach (hopefully undertaken by a number of individuals in the community). This initiative may take a number of months to complete, i.e., for context having the Python code fully adhere to having all function definitions be typed took around three months with limited involvement. 

The three steps which need to be undertaken (in order) are:

1. Bumping the version of `pylint` to the latest stable version ([2.5.2](https://pypi.org/project/pylint/2.5.2/) at time of writing). Currently we are using `pylint` [1.9.2](https://pypi.org/project/pylint/1.9.2/) which was released on June 6, 2018. 
2. Systematically remove the globally [disabled](https://github.com/apache/incubator-superset/blob/980dd2fd419893ec5ed9386ca28fa87115a3753d/.pylintrc#L84) checkers.
3. Systematically reenable checking for all files, i.e., removing the `# pylint: disable=C,R,W` comments. 

 Note _systematically_ implies via a slew of PRs which re-enables a check (or small subset of checks) and/or file(s) at a time.

 Note based on other repos which use `pylint` the only the following checks should be _disabled_:

- `bad-continuation` which is incompatible with `black`.
- `missing-module-docstring` since Python modules are typically not documented at the module/file level.

Additionally the following checks should be _enabled_: 

- `useless-suppression` ensures there are no unnecessary `# pylint: disable` suppressions.

### New or Changed Public Interfaces

N/A

### New dependencies

N/A

### Migration Plan and Compatibility

N/A

### Rejected Alternatives

A list of Python static analysis tools are described [here](https://luminousmen.com/post/python-static-analysis-tools).  Note that we previously used `flake8` (in conjunction with `pylint`) but later deprecated it as felt that `black`, `isort`, and `pylint` represented a superset in terms of functionality.",,,mistercrunch,"
--
+1, I'm supportive of the effort, happy to take one some of the work and I would love to involve the wider community for this effort. Maybe we can make a call to action on Slack / the mailing list and use this as on ramp for more contributions.

Side note I remember I tried to bump `pylint` to `2.x` at least twice in the past while we were on travis and it was slower than the current version, and would go 10+ minutes without std output (there was no verbose option that would output progress) and Travis would die / timeout. Hoping most of this is resolved.
--

--
Tried to bump `pylint` here https://github.com/apache/incubator-superset/pull/10095 just to see if that'll work now that they release new versions and that we're using Github Actions
--
",willbarrett,"
--
Bumped pylint and fixed most of the new checks here: https://github.com/apache/incubator-superset/pull/10101
--
",,,,,,,,
9947,OPEN,row limit settings are not clear,.pinned; bug; doc:developer,2021-01-03 05:02:36 +0000 UTC,tooptoop4,In progress,,"```python
ROW_LIMIT = 50000
VIZ_ROW_LIMIT = 10000
FILTER_SELECT_ROW_LIMIT = 10000
QUERY_SEARCH_LIMIT = 1000
SQL_MAX_ROW = 10000000
DISPLAY_MAX_ROW = 10000
DEFAULT_SQLLAB_LIMIT = 10000
```",,,mistercrunch,"
--
Agreed, this needs to get cleaned up and better documented.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",tooptoop4,"
--
can this be reopened/pinned @mistercrunch ?
--
",,,,,,
9935,OPEN,[SIP-45] Proposal for improved UX for dashboard filter eligibility,sip,2020-12-19 16:29:27 +0000 UTC,john-bodley,Opened,,"## [SIP] Proposal for improved UX for dashboard filter eligibility

### Motivation

Currently a user can configure dashboard filters to either apply (enabled; the chart is non-immune to the filter) or not apply (disabled; the chart is immune to the filter) to specific charts. For the former there is no validation on whether the filter is eligible, i.e., can be applied to the datasource, for the specific chart and currently if the filter is deemed irrelevant it is simply ignored by the chart. The issue with this approach is there is no visual indication that the chart has ignored said filter(s) and thus the viewer could be misguided into believing a set of filters have been applied, thus making an incorrect conclusion.

This SIP proposes a couple of visual treatments to provide more clarify with regards to filter viability. 

### Proposed Change

The root problem is there is no validation if enabled filters are actually eligible for said charts. We propose two (somewhat related) approaches. We prefer the first approach from a simplicity and consistency standpoint as immunity is treated in a consistent manner.

#### 1. Ensuring eligibility when configuring filter scopes [Preferred]

The first (pseudo-proactive) approach is ensuring filters can only be enabled for the charts which are eligible. Figure 1 shows how scoped filters are currently enabled by the user. A modification could be to only allow filters to be selected if they relate to said chart, i.e., if the ""region"" filter is deemed ineligible for the ""Treemap"" chart the checkbox would be disabled and thus the green filter pill will not appear next to the ""Treemap"" chart in the dashboard. 

<img width=""981"" alt=""Screen Shot 2020-05-28 at 2 36 05 PM"" src=""https://user-images.githubusercontent.com/4567245/83196420-ba838c80-a0f0-11ea-8e5d-1a21418c4e6d.png"">

**Figure 1:** Configuring filter scopes.

The dashboard UX is relatively intuitive as users can easily determine which filters are being applied to said chart based on the filter pills knowing that by construction all the defined filters for said chart are eligible.

Note for this option a migration would be required in order to uncheck any existing enabled filters which do not apply to the chart.

#### 2. Ensuring eligibility at the chart

The second (retroactive) approach would be to modify the UI treatment based on the filter eligibility. There are two different states/scenarios we need to consider in order to provide the appropriate UX:

1. A user is interacting with a dashboard chart where during a prior state the chart was deemed correct however after applying an ineligible filter the chart is deemed incorrect. 
2. A user is interacting with a dashboard chart where an existing (possibly saved) ineligible filter has been applied where the chart is deemed incorrect. Note there is no prior valid state for the chart.

The lefthand portion of Figure 2 illustrates these two states. The upper-left corner shows the treatment for (1) where the previously rendered chart is greyed out (via an opacity layer) and a message is show. The lower-left corner shows the treatment for (2) where since there was no prior valid state only the message is shown.

The upper-right portion of the figure provides context as to which filters don't apply (which contain an exclamation point), and the lower-right portion shows the message treatment being applied to a smaller chart. 

<img width=""1190"" alt=""Filters_safe"" src=""https://user-images.githubusercontent.com/4567245/83078812-446b2100-a02f-11ea-8008-af324887575f.png"">

**Figure 2:** The various visual treatments for handling filters which do not apply to a chart. The lefthand portion illustrates the the two different states, the upper-right portion provides context as to which filters don't apply (which contain an exclamation point), and the lower-right portion shows the message treatment being applied to a smaller chart. 

Note the issue will this approach is we treat charts which are immune to a filter and charts which have the filter enabled though not eligible differently, i.e., the former will be rendered where as the later will have the error message. The inconsistent UX is why we prefer the former approach.

### New or Changed Public Interfaces

For both approaches, in order for the frontend to know if a filter can be applied a new RESTful endpoint needs to be added which would leverage the metadata of the underlying datasource. The endpoint should have the ability to handle multiple charts in bulk.

For the first approach if the bulk RESTful API request was made by the dashboard this would be a blocking request during the dashboard mount and would add a speculative 0.5s to the dashboard load time. An alternative non-blocking approach would be for each chart to perform said check as part of the data fetch and propagate the response to the frontend which would then retroactively disable the filter scopes. This is somewhat akin to option two but with different messaging, i.e., rather than the filter pill containing an exclamation point, the pill simply wouldn't exits as said filter doesnt apply to the ineligible chart.     

The one caveat of this approach is the [`filter_values`](https://github.com/apache/incubator-superset/blob/7f6dbf838e4e527e640a002ce20bf5da1abf4a98/superset/jinja_context.py#L37) Jinja macro, which per the documentation states: 

> This is useful if:
>   - you want to use a filter box to filter a query where the name of filter box
>     column doesn't match the one in the select statement
>   - you want to have the ability for filter inside the main query for speed
        purposes

This could be problematic because it is not viable to determine whether the filter is applicable by simply inspecting the column/metric metadata of the underlying datasource. There are two approaches to remedying this problem:

1. Extend the metadata to include the SQL for virtual datasources which needs to be parsed. 
2. Deprecate the `filter_values` Jinja macro. The first point can be circumvented by adding a virtual column (alias) to a SQL datasource which adheres to the filter where the expression is merely the name of the corresponding column. The second point cannot be circumvented resulting in sub-optimal queries. 

Note for context < 0.1% of SQL datasources and < 0.1% of slices at Airbnb use the `filter_values` macro. 

### New dependencies

N/A.

### Migration Plan and Compatibility

The first option would require a one off database migration to retroactively remove all filters which have been enabled for a chart even though said chart is ineligible. 

### Rejected Alternatives

N/A.",,,ktmud,"
--
Two immediate issues come to my mind:

1. IIRC, filter indicators do not show up for non-applicable charts. If we are to adopt this design, does that mean all charts will have all filters indicators? That might turn problematic when a dashboard has many filters, especially if filters and charts are in different tabs. (Although this is solvable by redesign the indicators.)
2. Blocking users from viewing a chart when not all filters are applicable sounds too strict. Sometimes it makes sense to include charts using different datasources in the same dashboard, but they may not share the same filter columns.

I've used filter scopes extensively in Tableau yet never felt the need for Tableau to make it clearer what filters apply to which charts in my dashboards. The added clarity is definitely nice, but overall I feel it should probably be dashboard designers' responsibility to make filter usage unambiguous and clear---which can be achieved via adding sep lines, subsections, etc.
--

--
This looks great!

I wonder whether we can also add a shortcut near filter control themselves to quickly change the filter scope like in Tableau:
![image](https://user-images.githubusercontent.com/335541/83204402-9cbe2380-a100-11ea-9b73-5eff7773e843.png)
--

--
Tableau also allows applying filters to selected worksheets (charts):

https://help.tableau.com/current/pro/desktop/en-us/filtering_global.htm#apply-filters-to-select-worksheets-tableau-desktop-only

It doesn't conflict with having a shortcurt control to quickly change filter scopes between the two most used modes: charts with the same datasource, and charts with related datasource (those having the same filter column).

I don't think we need to add filter selectors to charts, just a quick switch to filter charts is enough. For large dashboard, users can always go back to use the  all-in-one modal, but having a quick switch would make life easier for other dashboards, which are probably more common.
--

--
Yeah, it's a nice-to-have and could be implemented separately. 
--

--
Based on previous offline discussions with @graceguo-supercat and @john-bodley, here's what needs to be done in the near term:

### 1. Present more accurate filter indicators for charts

Pass all in-scope filter fields and values to charts (together with the `extraFilers` field), the chart slice API returns whether a field is valid (exists in datasource) or not (could be another field `activeFilers` in the payload). The frontend then have a way to display only actual `activeFilers` for a chart.

### 2. Clarify applicable scopes for filters in filter scope editor modal

We could take either approach below:

1. [Easier] Add a help text explaining filters can only apply to charts whose datasources have corresponding fields
2. [More efforts but more robust] Create an endpoint that given a list of filter field names and charts returns applicable charts for each filter field. The backend will query database schema for all associated charts and check whether they contain columns that match the filter fields. The the frontend will use this information to disable any charts in the filter scope tree that do not match certain filter.
--
",john,"
--
@ktmud 

>  1. IIRC, filter indicators do not show up for non-applicable charts. If we are to adopt this design, does that mean all charts will have all filters indicators? That might turn problematic when a dashboard has many filters, especially if filters and charts are in different tabs. (Although this is solvable by redesign the indicators.)

This only impacts non-immune charts, i.e., those charts where said filter is said to apply. Note as mentioned previously there is no eligibility check on whether a filter can be applied or not. 

> 2. Blocking users from viewing a chart when not all filters are applicable sounds too strict. Sometimes it makes sense to include charts using different datasources in the same dashboard, but they may not share the same filter columns.

I updated the ""Rejected Alternatives"" section to include an option which would provide an eligibility check and thus ensure that a filter can only be enabled on a chart if it is supported by the underlying datasource.
--

--
@graceguo-supercat in the ""New or Changed Public Interfaces"" section there would need to be a new RESTful API endpoint which checks the datasource metadata. The request should be relatively efficient (we could add a bulk option to check multiple charts) though it would be more complex if we need to handle the `filter_values` Jinja macro.
--

--
@ktmud I wonder if your suggestions are probably a nice to have (for improved UX) but not required in order to implement the filter eligibility. 
--

--
I prefer option (2) as I think that relates to the first (and preferred) solution proposed. 
--
",graceguo,"
--
Thank you very much this SIP. 
May i know what are possible technical solution to implement this feature? 
When a user creates a filter and added to dashboard, how do we know which charts are applicable? I assume each chart may have complicated sub queries, Jinja macro, so a query has to hit query engine to know if the filter is valid or not?


--

--
@ktmud Actually show a control in the filter_box to define its own scope **_was_** one of filter scope design. 
The problem is right now Superset allow filter apply, and allow chart to immune. 
If we offer a control for filter be global or tab, then each chart also need a control to set which filter to immune. User needs to go through each filter to set scope, and each chart to set immune. It seems not efficient for large dashboard with large number of filter fields and charts. So this design was rejected pretty early.
--

--
Provide an option to set filter only applicable to charts from same datasource, probably is a quick win? @john-bodley

But you know Superset has virtual datasource, a dashboard **_generally_** contains a few different datasources. So we still need a more general solution and error handling like John proposed.
--

--
This is a similar work we did before (but was reverted) https://github.com/apache/incubator-superset/pull/7888
--
",junlincc,"
--
https://github.com/apache/incubator-superset/issues/12148
dashboard native filter and cascading filter are available for testing!! thanks in advance for feedback, comments, critics, love and support!! 
--
",,,,
9930,OPEN,Codecov migration to marketplace app,.pinned,2021-02-25 15:17:48 +0000 UTC,thomasrockhu,In progress,,"
Hi, Tom from [Codecov](https://codecov.io) here.

We noticed that you are using Codecov with fairly high frequency, and were so excited to see that! However, because you are not using our app, you may have experienced issues with uploading reports or viewing coverage information. This is due to rate-limiting issues from GitHub.

**In order to prevent any future outages, we ask that you move over to our [GitHub app integration](https://github.com/apps/codecov).**

The process is extremely simple and shouldnt require more than a few clicks, and you should not expect any downtime. By moving to our app, you will no longer need an admin or separate account to manage the relationship with GitHub as the team bot.

Let me know if you have any questions, or if I can help at all with this process.
",,,thomasrockhu,"
--
I noticed that a few other `apache` repositories have migrated over to the integration including [incubator-mxnet](https://github.com/apache/incubator-mxnet/issues/18421) and [skywalking](https://github.com/apache/skywalking/issues/4835). The process should be fairly easy now for the organization in general. Let me know if I can do anything to help.
--

--
@willbarrett thanks for responding back here! I think you are looking for [carryforward flags](https://docs.codecov.io/docs/carryforward-flags). If I'm reading your comment right, you don't run every test suite for every commit? If so, carryforward flags will pull the coverage data (by flag) from the last applicable commit. Let me know if that's what you were asking or if you had meant something else.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",willbarrett,"
--
Hi @thomasrockhu - we have migrated to the Github application. Apologies for the long delay in responding! We have conditional builds set up for different test suites and often face long CI delays. We find this situation leads to inaccurate results in Codecov. We've updated our configuration recently here: https://github.com/apache/superset/pull/13329 but I'm wondering if you have any other suggestions on how to make Codecov's interaction with Superset's tests more accurate and reliable now that we've migrated to the Github app?
--
",,,,,,
9928,OPEN,timestamp is not prefixed on log messages,.pinned; install:config,2020-12-19 05:55:32 +0000 UTC,tooptoop4,In progress,,"my startup command is:

/usr/bin/python3 /usr/local/bin/gunicorn -w 4 -k gevent --timeout 300 -b 0.0.0.0:8786 --limit-request-line 0 --limit-request-field_size 0 superset.app:create_app() --access-logfile /home/ec2-user/superset/logs/access.log --error-logfile /home/ec2-user/superset/logs/err.log

my config.py contains
```
LOGGING_CONFIGURATOR = DefaultLoggingConfigurator()

# Console Log Settings

LOG_FORMAT = ""%(asctime)s:%(levelname)s:%(name)s:%(message)s""
#todo? info?
LOG_LEVEL = ""DEBUG""

# ---------------------------------------------------
# Enable Time Rotate Log Handler
# ---------------------------------------------------
# LOG_LEVEL = DEBUG, INFO, WARNING, ERROR, CRITICAL

ENABLE_TIME_ROTATE = True
TIME_ROTATE_LOG_LEVEL = ""DEBUG""
#FILENAME = os.path.join(DATA_DIR, ""superset.log"")
FILENAME = '/home/ec2-user/superset/logs/superset.log'
ROLLOVER = ""midnight""
INTERVAL = 1
BACKUP_COUNT = 2

# Custom logger for auditing queries. This can be used to send ran queries to a
# structured immutable store for auditing purposes. The function is called for
# every query ran, in both SQL Lab and charts/dashboards.
# def QUERY_LOGGER(
#     database,
#     query,
#     schema=None,
#     user=None,
#     client=None,
#     security_manager=None,
# ):
#     pass
QUERY_LOGGER = None
```

but lines in /home/ec2-user/superset/logs/superset.log are not prefixed with a timestamp",,,tooptoop4,"
--
any idea @mistercrunch ?
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",bkyryliuk,"
--
I saw smth similar, curious to learn about the solution.
--
",,,,,,
9892,OPEN,[SIP 44] Proposal for Dart Module implementation,sip,2020-10-26 23:04:21 +0000 UTC,kalimuthu123,In progress,,"*Please make sure you are familiar with the SIP process documented*
(here)[https://github.com/SvayamLabs/superset-ui-number-Format]

## [SIP 44 ] Proposal for Dart Module implementation

### Motivation
   **My Google-fu is strong.**

Description of the problem to be solved.
the reason for the migration 
     1) typescript chunk size which makes the browser not responding whenthe data is large
     2)Flutter is multiplatform (native )  we can port to android devices 
     3) some package deprecation leads  tools  as incapable
     4)flutter widget have native performance which reduces  the browser hangover
   

### Proposed Change
the migration of typescripts to flutter Language:
the syntax of typescript and dart language are similar in variable declaration and functional s declarations and OOPs concepts , the process of converting is easily carryout
the difference in between the both the language is vary only in the export and import process

### New or Changed Public Interfaces
AS a native interface we can  create a more animative dashboard which can be competetive with other products like tableau,PowerBi

### New dependencies

dart version > 2.0
flutter version > 1.17

### Migration Plan and Compatibility

formatting modules ,connection modules =======>>  flutter Packages
Superchart , tooltips ,ui-plugins modules  =======>> flutter widgets

my plan is implementing our charting tool  in a stable products like flutter.

Flutter has variety of widgets 
which are suitable  for common 3 interfaces which are

1)Android
2)Ios
3)web Applications

Cupertino (iOS-style) widgets
https://flutter.dev/docs/development/ui/widgets

AS a initial step i  created a module based on the @Superset-ui/Number-Format
the github link of my dart coded package
https://github.com/SvayamLabs/superset-ui-number-Format",,,Vino2530,"
--
https://github.com/Vino2530/dart.git
--
",suddjian,"
--
Do you have specific data on how typescript impacts chunk size and browser performance?
--
",kalimuthu123,"
--
@suddjian 
![oie_NoPPEtlSz8If](https://user-images.githubusercontent.com/42966797/83321874-aff00100-a270-11ea-8a37-d6e0bc3449a6.png)

when i refer the chunck size of the superset  it looks pretty bigger when data flows
and the major aim of this Proposal is implementing the superset across all platforms
1)Desktop applications
 a)linux
b)mac
c)windows
2)Mobile application
a)ios
b)android
3)web application
flutter having such ability of working across multi platforms compared to other languages
https://github.com/google/flutter-desktop-embedding
--
",mistercrunch,"
--
-1 
I think moving from typescript to dart is a bad idea
<img width=""1211"" alt=""Screen Shot 2020-10-22 at 10 18 33 PM"" src=""https://user-images.githubusercontent.com/487433/96959310-92883180-14b4-11eb-98eb-49fd1f33128c.png"">

--
",willbarrett,"
--
-1 - we already have multiple frontend migrations in process and I don't think changing languages is prudent at this time.
--
",,
9887,OPEN,[SIP-43] Unified Chart Controls,sip,2020-05-27 06:50:32 +0000 UTC,ktmud,In progress,,"*Disclaimer: regardless of the details embedded, this document is a conceptual proposal intended to start conversations. We are not committed to implement all or any of the proposed changes yet.*

# [SIP-43] Unified Chart Controls

### Motivation

After obtaining tabular data from datasources, visualizations need to know how to map data columns to marks and channels in visualizations. Sometimes the visualization may also require or the users would like to perform additional transformations before plotting (compute moving average, pivot, aggregate, transpose, sample, window, etc.), which are not possible/convenient to do at the datasource/SQL query level. 

The concept of post-processing is not new to Superset. Historically, we had some flavors of these known as *advanced analytics* in a few charts such as the line chart, but they lack clarity and consistency.

__Consistency:__ Most of these transformations are implemented in [viz.py](https://github.com/apache/incubator-superset/blob/5ab5457522a141139958a52c88e021c3e5a50ad7/superset/viz.py), with each visualization having its own non-standardized post-processor. This Python module has grown out of control and become difficult to maintain. The coupling between visualization and query response makes it difficult to improve some important features such as chart slice caching, embeddable charts and visualization plugins.

We have been slowly migrating to a new visualization-independent query API ([SIP-5](https://github.com/apache/incubator-superset/issues/5680), [SIP-6](https://github.com/apache/incubator-superset/issues/5692), [#6220](https://github.com/apache/incubator-superset/pull/6220)). The idea is to decouple data querying with visualizations, move most visualization-specific transformations to the frontend. The new API will always return tabular data. It is up to each visualization plugin to take the output, optionally run it through a generalized post-processing API on the server side before ([#9427](https://github.com/apache/incubator-superset/pull/9427)), and pass the tabular data to the visualization code, which handle the rest of the visualization-specific transformation on the client-side.

__Clarity:__ Some post-processing can be implicit but straightforward, e.g., when users want to add moving averages to a line chart, we simply introduce a form control that computes a derived column. It is easy to infer how that column should be presented in the visualization. However, while there is room for abstraction, this approach requires developers to create custom controls for every visualization, each with their own `transformProps` logics. Like many basic chart controls in Superset, these controls affect both data manipulation and presentation, so they still unavoidably bind data querying to visualizations. It may be straightforward to use them in one simple chart, but it quickly becomes confusing when there are a lot of different controls across many visualizations---both developers and users sometimes have to guess whats happening under the hood.

__Flexibility:__ In addition to lack of clarity and consistency, the custom control approach also fall short of supporting more powerful visualizations. For example, following table chart with mock data is common in top-line business reports. It compactly displays multiple metrics (bookings and revenue) across multiple dimensions (state, user type) and multiple time periods (point-in-time measurement and 7 day moving average):

![Snip20200521_5](https://user-images.githubusercontent.com/335541/82700410-fc10c500-9c22-11ea-9534-c4aca5777f58.png)

Suppose each metric and dimension is a column in the database, and each row is their values at a given date. Currently, in order to create this output table, users have to write very complex SQL queries and use a virtual datasource. But its actually possible to very quickly build the same chart using a combination of Pandas post-processing operators, without writing complex database queries.

### Proposed Change

To solve the challenges above, we propose to (1) add a __Transform__ section for server-side post-processing and (2) rearrange the __Customize__ controls in the control panel.

<img width=""400"" src=""https://user-images.githubusercontent.com/335541/82695411-e3e87800-9c19-11ea-8a4e-35fe1582e815.png"" />

#### The Transform Section

In the Transform section, users can specify stackable atomic transform operators mapped directly to the [pandas_postprocessing](https://github.com/apache/incubator-superset/blob/a52cfcd234aa7d506c5d0ee659492e772940dbba/superset/utils/pandas_postprocessing.py) API already implemented in the backend. For example, the screenshot below shows the controls popup for *Rolling Window* transformation:

![Snip20200520_9](https://user-images.githubusercontent.com/335541/82698525-c3232100-9c1f-11ea-9142-bd4838196211.png)

This one-on-one mapping between transform controls and post processing operators makes it easy for documentation. We can just point users to the Python API spec for [pandas_postprocessing](https://github.com/apache/incubator-superset/blob/a52cfcd234aa7d506c5d0ee659492e772940dbba/superset/utils/pandas_postprocessing.py).

After applying a transformation, users should be able to view the intermediate and final tabular data (*the final results returned by the server will always be tabular*). We can add a button to switch between data view and chart view:

![Snip20200520_12](https://user-images.githubusercontent.com/335541/82695344-c0253200-9c19-11ea-848c-db315a4ce9d5.png)

Or implement [the split-view](https://user-images.githubusercontent.com/812905/72408878-05d86800-3719-11ea-947d-8fe28fd6802b.png) proposed in SIP-34.

Clicking on the accordion list items in Transform will switch between the intermediate transformed results.

#### The Customize Section

Superset introduced the *Data* vs *Customize* tab to reduce clutter in the control panel. This is helpful for charts with many options. But it also makes the Customize options difficult to discover. A lot of users dont even know they can add pagination to the table chart.

To simplify the user experience, we intend to move the Customize tab to a new Customize section under the main tab and add a __Columns__ section before other chart rendering options.

The Columns section configures per-column meta data such as d3 format, tooltip template, suffix/prefix, and conditional formatting, corresponding to the final data output. Not all visualizations have customizable rendering options, but those with rendering options will always have the Columns section before other rendering options. We believe with refined UI hierarchy, its possible to resurface the customize controls without creating clutters.

The full mockup can be found [here](https://www.figma.com/proto/O28oGbqtjVFTYrAxgyLrBJ/Superset-Chart-Explore-Redesign?node-id=24%3A22&scaling=min-zoom).

#### Long-term Plan

In the future, all visualization controls will follow three simple steps: Query (datasource queries)  Transform (server side post processing)  Customize (chart rendering). This is akin to the visualization grammar used by [Vega](https://vega.github.io/vega/) and [Vega-lite](https://vega.github.io/vega-lite/). This separation of concerns and unification of control semantics make it super clear what each control is responsible for.

By moving column mapping and chart rendering logics to *Customize*, we can also remove control overrides in the *Query* section---currently there are too many variants of `metrics`, `columns`, and `groupby`  fields querying the same thing but are stored differently ([apache-superset/superset-ui#485](https://github.com/apache-superset/superset-ui/pull/485) provided a mask to help developers; end users may still be confused). This not only greatly simplifies the code, but also makes it easier to switch between visualization types---all control values for *Query*, *Transform*, and even *Columns* can be easily retained.

In the future (beyond the scope of this SIP), if its too tiresome to edit all the transformations one by one, there are many ways to simply the Transform section for users:

1. We can hide complex operations behind custom operators that are either a preset of other operators, or an arbitrary Python function running in sandbox.
2. We can add a switch of Simple v.s. Advanced mode. The Advanced mode is what described above. In the Simple mode, users specify transformations using less controls, similar to *Advanced Analytics*. Each control could potentially represent one or multiple transformations with reasonable defaults.

### New or Changed Public Interfaces

- New React components for the Transform controls and popup modals, which will interact with the `post_processing`  field in the new API (`/api/v1/chart/data`).
- New shared controls for the Columns section.
- New design pattern for control panel to optimize the hierarchy of existing controls (data source, time, etc.).
- Refactor control panel config registrations and chart control overrides.

### New dependencies

No new NPM/Pip dependencies needed.

### Migration Plan and Compatibility

This change has profound implications on the way we think of charting in Superset. In addition to exposing post-processing API, it also proposes changes to the _Query_ controls.

We will start with implementing the controls for one example visualization type (the table chart for example), then migrate others one by one. For each visualization type, we have to do the following:

- Step 1: migrate to the new `/chart/data` API
- Step 2: add _Transform_ controls, implement the data vs chart split view
- Step 3: move _Customize_ controls to the main tab, add _Columns_ controls
- Step 4: rename and simplify _Query_ controls, db migration may be needed

### Rejected Alternatives

* __Add yet another custom control or visualization type for the advanced table chart:__ the use case is too specific. There are too many operations in the underlying transformation, it becomes too opaque to the users.
* __Only add the transform section to charts which need it:__ this does not solve the long-standing problem of inconsistent and speculative chart controls, but still introduces a new pattern for charting.


### References

- [[SIP-34] A new design direction, system, and process for Superset](https://github.com/apache/incubator-superset/issues/8976)
- [Full mockup for the unified controlled based on current UI](https://www.figma.com/proto/O28oGbqtjVFTYrAxgyLrBJ/Superset-Chart-Explore-Redesign?node-id=24%3A22&scaling=min-zoom)
",,,ktmud,"
--
I feel the SIP template could use a ""Technical Challenges and Risks"" section.
--

--
Was [this](https://projects.invisionapp.com/share/V5VH03AHBER#/screens/399817645) the query panel design you were looking for? (Thanks @graceguo-supercat for sharing!)

I like its simpleness and directness---the grouping of visualization types and unification of control UI are really slick. It does seem difficult to fit the query + transform + map model in this design, though.

There is definitely a tradeoff between simplicity and flexibility. To better understand the problem, I looked at what other BI tools are doing in this regard:

1. [Redash](https://redash.io/help/user-guide/getting-started#2-Write-A-Query)
   - Users always write SQL queries before visualization.
   - No post-processing transformation possible.
   - Property mapping mostly manual.
2. [Metabase](https://www.metabase.com/docs/latest/getting-started.html)
   - Similar to Redash, separate steps for query building and visualization
   - No post-processing transformations, except simple cumulative measurements (mixed with query-level aggregation functions).
   - Property mapping is manual or based on metric/dimension definitions at the query stage.
3. [Looker](https://docs.looker.com/exploring-data/retrieve-chart-intro)
   - A combined Explore view for query + visualization
   - Support ""Table Calculation"" (post-processing on query results) and ""Custom Fields"" (query level calculated columns), both have similar interactions
   - Data retrieval not tied to visualization
   - Post-processing and ad-hoc columns achieved via DLS
4. [Mode](https://mode.com/help/articles/getting-started-with-mode/#create-analysis)
   - Query with SQL
   - Apply advanced data transformations with R or Python notebook
   - Charting controls only operate on query results
   - Property mapping is totally manual
5. [Tableau](https://help.tableau.com/current/guides/get-started-tutorial/en-us/get-started-tutorial-drag.htm)
   - Visualization centric, no separate querying step for most cases
   - Powerful custom functions and on-chart controls to apply transformations
   - Custom function could trigger both optimized queries or post-processing, but the whole process is fully opaque to users.
6. [Domo](https://knowledge.domo.com/Visualize/Adding_Cards_to_Domo/KPI_Cards/KPI_Card_Building_Part_2%3A_The_Analyzer/011Analyzer_Overview)
   - Visualization centric, all charting/querying options in one page called Analyzer
   - Transformations like rolling average [must be applied at the data source level](https://knowledge.domo.com/Prepare/DataFlow_Tips_and_Tricks/Creating_a_Rolling_Average_Using_DataFlows) via SQL queries
7. [Power BI](https://docs.microsoft.com/en-us/power-bi/fundamentals/desktop-getting-started)
   - Very similar to Tableau
   - Most transformations (including data source joins) probably happen locally (in desktop app or on Power BI server, as oppose to the datasource)

TL;DR: Tableau, Domo, Power BI use the visualization-centric approach; others data-centric.

In terms of functionality and philosophy, _Looker_ is the closest to what's been proposed in this SIP; _Domo_ is the closest to SIP-34. Either way seems to work from an end-user's point of view. For Superset, it's a matter of which mode is the best for our users and whether we can smoothly get there.

To answer some of your concerns:

- **data <-> viz property mapping**: it's possible to keep track of derived metrics and use the same property type for source and derived metrics. Pivot tables also know which output columns are dimensions which are metrics.
- **granularity**: I'm not sure I understand this correctly. ""Grain change"" is the same as adding/removing a column in regular controls, isn't it? I don't think we have to have restrictions on query output. Like Redash/Mode and others, excessive columns can simply be ignored---during chart configuration, you select only the columns you need for visualization. Visualizations should not run any implicit aggregations on the client side, and should throw errors if the final output has excessive/missing dimensions or is not in desired shape.
- **complexity**: simple bar chart will not need transform. The transform controls could be part of an ""Advanced"" section hidden (but discoverable) from regular users. There are ways to make the UI intuitive and non-intrusive. Both Looker and Tableau, the most powerful of above all, use DSL to hide the complexity of the transformation layer. We don't have to go there yet, but it could eventually be a choice.

In summary, the addition of a transform layer does involve a lot of work, but I'd argue it adds valuable flexibility for the users which would really differentiate Superset from its competitors.

--
",mistercrunch,"
--
The data-centric approach (query + transform + map-data-to-viz-properties) as opposed to the visualization-property-centric approach is certainly interesting, but that last map phase is fairly challenging to design well so that it's intuitive to the user.

This was discussed at length in the sessions leading to SIP-34. Some of the challenges with the query-centric approach are:
- doing good guesswork around mapping data to viz properties attribute is challenging and we need to allow the user to override
- forcing the right granularity: some visualization require different things (a particular number of dimensions, a particular number of metrics). While the ""Query"" panel seem like a conceptually universal thing, the mechanics / restrictions applied to it are tricky (say if we need to force you to pick exactly 2 dimensions and a single metric, or if you're not allowed to pick a dimensions [big number], or if you have to pick a metric and a secondary one is optional, ....).
- for many users, if you're just trying to make a bar chart, or a big number, the whole query -> transform -> map mind gymnastic is a bit complicated.

We should dig out the best design we had for this during the design biltz. The design would show a query panel with the viz-property-mapping within the dropped ""pill"" in the droppable area of the query (the wireframe would be worth 10k words here...). That seemed somewhat intuitive but I feel like that design/model couldn't support another layer of transformation that we disregarded at that time.

One more thought, it's the fact that transformations are unevenly tricky, and we may want to categorize them:
- **in-place mutations** - transforms that don't change the shape of the result set - are easy
- **metric derivative** - a new computed metric work generally well with visualizations that support multiple metrics
- **pivots** - where the shape of the data following the transformation is dynamic are generally tricky, the visualization has to expect a certain shape here. Clearly not any pivot table can be visualized by any type of visualization...
- **grain changing**: many viz care a lot about having the perfect ""grain"" and some transforms could mess with that
--
",,,,,,,,
9869,OPEN,SQL Lab results row height/width is too short for last row,.pinned; bug,2021-01-06 01:15:13 +0000 UTC,tooptoop4,Opened,,"see attached, if i change height pixels from 32 to 40 in developer tools then it is readable
![last row height cut](https://user-images.githubusercontent.com/33283496/82511042-1c962d80-9b04-11ea-9273-302973e09aae.png)
",,,stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",eschutho,"
--
I'm working on this. Should have a fix soon.
--
",,,,,,,,
9855,OPEN,FilterBox visualization error with empty results from Druid datasources,bug; inactive,2020-10-10 12:37:44 +0000 UTC,elukey,In progress,,"I am testing 0.36 with two backported patches of mine:
- https://github.com/apache/incubator-superset/commit/a52b9ee8fffb123d98e111c26ca82714deae49de
- https://github.com/apache/incubator-superset/commit/5ab5457522a141139958a52c88e021c3e5a50ad7

I noticed that when using FilterBox with Druid datasources (so not via SqlAlchemy) if the time range returns an empty result then I see:

```
An error occurred while rendering the visualization: TypeError: Cannot read property 'map' of undefined
```
That expands to:
```
    in q
    in n
    in div
    in me
    in Se
    in je
    in div
    in div
    in C
    in Re
    in Connect(Re)
    in div
    in r
    in div
    in t
    in div
    in t
    in Uncontrolled(t)
    in ForwardRef
    in div
    in bn
    in div
    in div
    in div
    in nr
    in Connect(nr)
    in div
    in i
    in ir
```

### Expected results

A nice ""no results"" message that is less confusing.

### Actual results

What written above, that is not super easy to decrypt for users. We are currently migrating to SQLAlchemy-based Druid tables, but the migration will be long so it would be great to get a patch for this in the meantime :)

#### Screenshots

![Screen Shot 2020-05-20 at 9 02 42 AM](https://user-images.githubusercontent.com/1536240/82415502-e495d880-9a78-11ea-95f1-0a557e9a1169.png)


#### How to reproduce the bug

1. Select a FilterBox with a Druid Datasource (not one defined via SQLAlchemy, but via the ""old"" way)
2. Select one filter that you want.
3. Run a query with a time range that is not available in Druid segments.
4. See error


### Environment

(please complete the following information):

- superset version: `0.36` (plus two commits mentioned above)
- python version: `3.7`
- node.js version: `10.19`
- npm version: `5.8`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

On Superset 0.35.2 the same workflow leads to another error:

```
 Traceback (most recent call last)
   File ""/srv/deployment/analytics/superset/venv/lib/python3.7/site-packages/superset/views/base.py"", line 120, in wraps
     return f(self, *args, **kwargs)
   File ""/srv/deployment/analytics/superset/venv/lib/python3.7/site-packages/superset/utils/decorators.py"", line 69, in wrapper
     return f(*args, **kwargs)
   File ""/srv/deployment/analytics/superset/venv/lib/python3.7/site-packages/superset/views/core.py"", line 1092, in explore_json
     viz_obj, csv=csv, query=query, results=results, samples=samples
   File ""/srv/deployment/analytics/superset/venv/lib/python3.7/site-packages/superset/views/core.py"", line 1013, in generate_json
     payload = viz_obj.get_payload()
   File ""/srv/deployment/analytics/superset/venv/lib/python3.7/site-packages/superset/viz.py"", line 381, in get_payload
     payload[""data""] = self.get_data(df)
   File ""/srv/deployment/analytics/superset/venv/lib/python3.7/site-packages/superset/viz.py"", line 1832, in get_data
     utils.get_metric_name(metric), ascending=flt.get(""asc"")
   File ""/srv/deployment/analytics/superset/venv/lib/python3.7/site-packages/pandas/core/frame.py"", line 4719, in sort_values
     k = self._get_label_or_level_values(by, axis=axis)
   File ""/srv/deployment/analytics/superset/venv/lib/python3.7/site-packages/pandas/core/generic.py"", line 1706, in _get_label_or_level_values
     raise KeyError(key)
 KeyError
```

So in the current Superset, the error seems to be in JS, meanwhile before was in viz.py.",,,elukey,"
--
ping :)
--

--
ping :) Anybody willing to review the above? Should be very quick to fix :)
--

--
Didn't test if this is fixed in 0.37, will do!
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
9792,OPEN,[SIP-42] Option to set a user's profile as their welcome page,sip,2020-05-14 04:59:09 +0000 UTC,altef,In progress,,"*Please make sure you are familiar with the SIP process documented*
(here)[https://github.com/apache/incubator-superset/issues/5602]

## [SIP] Proposal for an option to set a user's profile as their welcome page

### Motivation

In certain situations a user's profile page may be more useful than Superset's default welcome page.  And while the welcome dashboard is a nice feature, the option to use the user's profile page might be nice.  It lists their favourite dashboards, rather than a single dashboard (like the welcome dashboard) or all dashboards they can access (like the default welcome page).

### Proposed Change

I propose we add a config setting to allow this, defaulting to false so as not to impact any existing installs.  
`WELCOME_WITH_USER_PROFILE = False`

Welcome dashboards can still take precedence, if supplied - all this will do is swap out the default welcome page for the user's profile page.

### New or Changed Public Interfaces

The only change should be exposing an additional config setting.

### New dependencies

None.

### Migration Plan and Compatibility

None necessary.

### Rejected Alternatives

For our use-case we attempted using the welcome dashboards, but they did not suit our purpose.  An iframe could have worked, however there seems to be no jinja access to the user id there so we can't easily differentiate between users.
",,,altef,"
--
I'm not super clear on if this warrants a SIP, but it does impact the configuration which is specifically mentioned in the list of things to be SIPed.  Apologies if this in fact was inappropriate.
--

--
That would be nice, especially if combined with some form of dashboard-level permissions, though I guess that's probably not part of that SIP? 
--
",craig,"
--
Thanks for your submission! As you pointed out in your description, it probably doesn't warrant a SIP. Either way, I'm for this change as it will be behind a flag with a default of the current behavior. 
--
",robdiciuccio,"
--
[SIP-34](https://github.com/apache/incubator-superset/issues/8976) includes a design for a new landing/home page experience. While this proposed change does not directly conflict with the direction of SIP-34, due to the config flag, we should evaluate this proposal within that context.
--
",mistercrunch,"
--
@betodealmeida did some work in the past where each user gets their own dashboard as a landing page. That dashboard would be cloned from a template, and contain object lists based on on tags. A cool idea.
--

--
Longer term the vision is to fit the SIP-34 design that @robdiciuccio mentionned
<img src=""https://user-images.githubusercontent.com/812905/72391546-4a94dc80-36e2-11ea-8925-16833d2ba81b.png"">
--
",,,,
9725,OPEN,Remove share and favorite,.pinned; enhancement:request,2020-07-03 17:56:32 +0000 UTC,stefanmo,Opened,,"**Is your feature request related to a problem? Please describe.**
Remove the share and favorite buttons

**Describe the solution you'd like**
I would like to be able to remove the share and favorite buttons from the dashboards because in our implementation users will not have direct access to superset but thru iframes and custom authentication.

**Describe alternatives you've considered**
I have looked in other issues on github and read the documentation countless times but I could not find a solution.
Not adding the fave permissions only ends up in an error that says there is no access to the favorite list.

**Additional context**

",,,stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",mistercrunch,"
--
Seems reasonable, it should show based on whether the user (or the Public role) has the right to do favorite or not.
--
",,,,,,,,
9722,OPEN,"Edit the default ""No Results"" message",enhancement:request; need:followup,2021-02-16 09:44:59 +0000 UTC,larsmars,In progress,,"**Is your feature request related to a problem? Please describe.**
When superset chart is used as iFrame make it posible to edit the default ""No Results"" message to you pref.

Default text today:
_No Results
No results were returned for this query. If you expected results to be returned, ensure any filters are configured properly and the datasource contains data for the selected time range._

**Describe the solution you'd like**
Config parameter to edit default text and header for ""No Results""

**Additional context**
Screenshot of as is today:
![no_results](https://user-images.githubusercontent.com/5154589/80940364-db054300-8ddf-11ea-8788-68ba6ed1790d.PNG)

",,,stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",larsmars,"
--
bump
--
",mistercrunch,"
--
There's been some design work between Airbnb and Preset to redesign error messages from the ground up. I don't think that work is available in the open just yet, but there's a plan to make error messages much more clear and actionable. 
--

--
FYI this is where it lives: https://github.com/apache-superset/superset-ui/blob/master/packages/superset-ui-core/src/chart/components/NoResultsComponent.tsx#L8
--
",sarahbarberuk,"
--
I'm also interested in this
--
",Neel,"
--
I am as well interested. But, meanwhile they become open is there anything I can do alter this default message temporarily?
I have been stuck on changing this for weeks, any suggestion would be really helpful.
Thanks in advance.
--

--
Hi @mistercrunch, thanks for the reply.
I am sorry if I sound really daft. But I am not able to detect how this superset-ui plugins are connect with core ""incubator-superset"" code. I have searched the libraries getting installed with Superset & superset-ui doesnt get installed with it. So I am guessing there must be a connection point somewhere in the core code.
Can you please give me some direction in which I can move to alter the superset-ui code?
--
",baltom,"
--
I'm also interested in this. Would be a nice feature
--
"
9697,OPEN,Chosen colors not reflected in Deck.GL grid,bug; viz:chart-deck.gl,2021-01-02 18:29:47 +0000 UTC,Sumit12594,Opened,,"A clear and concise description of what the bug is.
When we try to set the color of extruded bar, the color reflected on the map is not the same as we set. It shows red and orange only.


### Expected results
I want the color that I set, to be reflected in the map.

### Actual results
![GITquestion](https://user-images.githubusercontent.com/36192149/80674751-63c66b00-8ad0-11ea-9797-6395d33ec10a.PNG)

![Gitquestionmap](https://user-images.githubusercontent.com/36192149/80674754-66c15b80-8ad0-11ea-9559-a63b1c060c8c.PNG)

Like you see in the screenshot, I have set the fixed color to Green but even then I get to see red bars on the map.

### Environment
Windows 10 OS

(please complete the following information):

- superset version: 0.30.1
- python version: 3.6

### Checklist


- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
-  I have reproduced the issue with at least the latest released version of superset.
-  I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context
",,,junlincc,"
--
 chart
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
9620,OPEN,[BUG]: Sort By on table viz produces undesired results.,.pinned; bug; viz:chart-table,2021-02-01 00:57:00 +0000 UTC,mmuru,In progress,,"A clear and concise description of what the bug is.
Sort By on table viz produces undesired results.
### Expected results
It should able to sort the result set.
### Actual results
Adding an extra column in the result set - which is confusing. 
#### Screenshots
![image](https://user-images.githubusercontent.com/11397834/80009779-5f6dd280-847e-11ea-82d7-5bf7fd2d874e.png)
![image](https://user-images.githubusercontent.com/11397834/80009881-84624580-847e-11ea-8660-234d11c0b446.png)
![image](https://user-images.githubusercontent.com/11397834/80010006-a3f96e00-847e-11ea-9c18-fb7487daf7fc.png)
#### How to reproduce the bug
1. Go to source->tables->select bart_lines
2. Click on bart_lines to explore data set
3. Scroll down to ""Sort By""  (choose a column or aggregate function). Unable to select the Group by columns (see the screen shot). It forces to select aggregate function or custom sql otherwise it throws duplicate columns exception if custom sql column doesn't have alias name.
4. See error
See attached screen shot (result window)
### Environment

(please complete the following information):

- superset version:  0.36.0, 0.35.2 and the master 
- python version: 3.6.8
- node.js version: `node -v`
- npm version: `npm -v`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context
The explore request 
`http://localhost:8088/superset/explore/?form_data={""datasource"":""13__table"",""viz_type"":""table"",""url_params"":{},""time_range_endpoints"":[""inclusive"",""exclusive""],""granularity_sqla"":null,""time_grain_sqla"":""P1D"",""time_range"":""Last+week"",""groupby"":[""name""],""metrics"":[{""expressionType"":""SIMPLE"",""column"":{""id"":456,""column_name"":""color"",""verbose_name"":null,""description"":null,""expression"":null,""filterable"":true,""groupby"":true,""is_dttm"":false,""type"":""VARCHAR(255)"",""optionName"":""_col_color""},""aggregate"":""COUNT"",""sqlExpression"":null,""hasCustomLabel"":false,""fromFormData"":true,""label"":""COUNT(color)"",""optionName"":""metric_qqaak8p1tnh_c7yss63hqv7""}],""percent_metrics"":[],""timeseries_limit_metric"":{""expressionType"":""SIMPLE"",""column"":{""id"":455,""column_name"":""name"",""verbose_name"":null,""description"":null,""expression"":null,""filterable"":true,""groupby"":true,""is_dttm"":false,""type"":""VARCHAR(255)"",""optionName"":""_col_name""},""aggregate"":""COUNT_DISTINCT"",""sqlExpression"":null,""hasCustomLabel"":false,""fromFormData"":false,""label"":""COUNT_DISTINCT(name)"",""optionName"":""metric_r322b0kf66c_hctq4pk133v""},""row_limit"":10000,""include_time"":false,""order_desc"":true,""all_columns"":[],""order_by_cols"":[],""adhoc_filters"":[],""table_timestamp_format"":""%Y-%m-%d+%H:%M:%S"",""page_length"":0,""include_search"":false,""table_filter"":false,""align_pn"":false,""color_pn"":true,""show_cell_bars"":true}`

the query:
`SELECT name AS name,
       count(color) AS ""COUNT(color)"",
       **count(DISTINCT name) AS ""COUNT_DISTINCT(name)""**
FROM bart_lines
GROUP BY name
ORDER BY ""COUNT_DISTINCT(name)"" DESC
LIMIT 10000;`

To explore table viz, it should not force to select metrics aggregation instead it should provide columns to be selectable. I noticed the explore request ""metrics"" key in form_data includes ""Sort By"" value and it should not add it to the list. The ""order_by"" key has already the same value. Before, I submit the PR for this issue, need to understand the intend of ""sort_by"" for Table viz. Please, provide the context. 
",,,mmuru,"
--
@villebro: Please, can you review this issue and provide your feedback?
--

--
@villebro: I tested it with latest version 1.0 and sort_by issue is still exist. Although it is still forced to select aggregate function  for sort_by column but it did not get included in the result. 
`SELECT name AS name,
       count(color) AS ""COUNT(color)"",
       count(name) AS ""COUNT(name)""
FROM public.bart_lines
GROUP BY name
ORDER BY ""COUNT(name)"" ASC
LIMIT 10000;`

Noticed the following issue: In the custom SQL, changing column name as name2 the following query generated and throws exception.
`SELECT name AS name,
       count(color) AS ""COUNT(color)"",
       name as name2 AS ""name as name2""
FROM public.bart_lines
GROUP BY name
ORDER BY ""name as name2"" DESC
LIMIT 10000;`

`Traceback (most recent call last):
  File ""/Users/muru/venv367/lib/python3.6/site-packages/superset/connectors/sqla/models.py"", line 1274, in query
    df = self.database.get_df(sql, self.schema, mutator)
  File ""/Users/muru/venv367/lib/python3.6/site-packages/superset/models/core.py"", line 390, in get_df
    self.db_engine_spec.execute(cursor, sqls[-1])
  File ""/Users/muru/venv367/lib/python3.6/site-packages/superset/db_engine_specs/base.py"", line 877, in execute
    cursor.execute(query)
psycopg2.errors.SyntaxError: syntax error at or near ""AS""
LINE 3:        name as name2 AS ""name as name2""`
--
",villebro,"
--
I believe this is a problem in the way table viz is currently implemented, and getting this functionality into the current sorting control can be difficult. The table viz is currently undergoing some refactoring, and I'll try to keep this in mind to make sure it is addressed when the time is right.
--
",rea725,"
--
**Expected Result addendum**
- ""Sort By"" parameter should be basis for sort, but not basis for display.
- Chart can be configured to sort by a column that is not the last (right-most) column in the table.
- If the column being sorted upon is a metric with a d3 number format specified, the number format should be applied.

**Current Behavior**
- Including a column as a ""Sort By"" parameter will cause it to be displayed.
- The column will be displayed without its intended d3 number format.







--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",RageAgainstTheMachine101,"
--
> I believe this is a problem in the way table viz is currently implemented, and getting this functionality into the current sorting control can be difficult. The table viz is currently undergoing some refactoring, and I'll try to keep this in mind to make sure it is addressed when the time is right.

Hello, @villebro!
Do you have some news about this bug?
Is there some patches witch can fix this?
--
",ktmud,"
--
@RageAgainstTheMachine101 I think the issue with `sort by` being automatically added to display has already been fixed.
--

--
Pin the issue to track the progress on allowing GROUP BY columns in sorting.
--
"
9597,OPEN,Cache warm-ups never succeed,bug,2021-03-22 11:21:18 +0000 UTC,Pinimo,In progress,,"Globally: the **cache warm-up tasks launched by Celery workers all silently fail**. Indeed, they perform `GET`s on the main server's URL without providing the required authentication. However, dashboards may not be loaded without being logged in.


**Related bugs:**

- unit tests on this feature miss the error
- the documentation should mention that the Celery worker needs the `--beat` flag to listen on CeleryBeat schedules (cf `docker-compose.yml` configuration)

At stake: long dashboard load times for our users, or outdated dashboards.

**Main files to be fixed:** 

- `superset/tasks/cache.py`

### Expected results

When the Celery worker logs this (notice `'errors': []`):

    superset-worker_1  | [2020-04-20 13:05:00,299: INFO/ForkPoolWorker-3] Task cache-warmup[73c09754-4dcb-4674-9ac2-087b04b6e209] 
                         succeeded in 0.1351924880000297s: 
                         {'success': [
                             'http://superset:8088/superset/explore/?form_data=%7B%22slice_id%22%3A%2031%7D', 
                             'http://superset:8088/superset/explore/?form_data=%7B%22slice_id%22%3A%2032%7D', 
                             'http://superset:8088/superset/explore/?form_data=%7B%22slice_id%22%3A%2033%7D'], 
                         'errors': []}


... we would expect to have something (more or less) like this in the Superset server logs:
```
superset_1         | 172.20.0.6 - - [2020-04-20 13:05:00,049] ""POST /superset/explore_json/?form_data=%7B%22slice_id%22%3A HTTP/1.1"" 
                     200 738 ""http://superset:8088/superset/dashboard/1/"" ""python-urllib2""
```

Of course, we also hope to have a bunch of items in the Redis logs, and that loading dashboards is lightning-quick.

### Actual results

But we get these logs instead, which show there is a 302 redirect to the login page, followed by a 200 on the login page. This redirect is interpreted as a success by the tests.

```
superset_1         | 172.20.0.6 - - [20/Apr/2020 08:12:00] ""GET /superset/explore/?form_data=%7B%22slice_id%22%3A%2030%7D HTTP/1.1"" 
                     302 -
superset_1         | INFO:werkzeug:172.20.0.6 - - [20/Apr/2020 08:12:00] ""GET /superset/explore/?form_data=%7B%22slice_id%22%3A%2030%7D HTTP/1.1"" 
                     302 -
superset_1         | 172.20.0.6 - - [20/Apr/2020 08:12:00] ""GET /login/?next=http%3A%2F%2Fsuperset%3A8088%2Fsuperset%2Fexplore%2F%3Fform_data%3D%257B%2522slice_id%2522%253A%252030%257D HTTP/1.1"" 
                     200 -
```

(I added a few line returns)

In the Redis, here is the only stored key:
```
$ docker-compose exec redis redis-cli
127.0.0.1:6379> KEYS *
1) ""_kombu.binding.celery""
```

Last, the dashboards take time loading the data on the first connection.

#### Screenshots

None

#### How to reproduce the bug

I had to patch the master branch to get this to work. In particular, I have to admit it was not very clear to me whether the config was read from file `docker/pythonpath_dev/superset_config.py` or file `superset/config.py`. So I kind of adapted `superset/config.py` and copied it over to the `pythonpath` one (which looks like it is read by the celery worker, but not the server). 

Anyway, this reproduces the bug:

<ol>
<li><code>$ docker system prune --all</code> to remove all dangling images, exited containers and volumes.</li>
<li><code>$ git checkout master && git pull origin master</code></li>
<li><code>$ wget -O configs.patch https://gist.githubusercontent.com/Pinimo/c339ea828974d2141423b6ae64192aa4/raw/e449c97c11f81f7270d6e0b2369d55ec41b079a9/0001-bug-Patch-master-to-reproduce-sweetly-the-cache-warm.patch && git apply configs.patch</code><br>This will apply patches to master to make the scenario work out neatly, in particular add the <code>--beat</code> flag and specify a cache warmup task on all dashboards every minute.</li>
<li><code>$ docker-compose up -d</code></li>
<li>Wait for the containers to be built and up.</li>
<li><code>$ docker-compose logs superset-worker | grep cache-warmup</code></li>
<li><code>$ docker-compose logs superset | grep slice</code></li>
<li><code>$ docker-compose exec redis redis-cli</code> then type <code>KEYS *</code></li>

</ol>

### Environment

(please complete the following information):

- superset version: 0.36.0
- python version: dockerized
- node.js version: dockerized
- npm version: dockerized

### Checklist

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.",,,Pinimo,"
--
BTW, I noticed there is such a login procedure (with a headless browser) in the email report generator. Perhaps that procedure could be factored out and reused to warm up the cache in our case?
--

--
Hi @jayhjha, thank you for looking into this. Interesting bug. :thinking:

My cache config is one of these two (which are identical): [one](https://gist.github.com/Pinimo/c339ea828974d2141423b6ae64192aa4/#file-0001-bug-patch-master-to-reproduce-sweetly-the-cache-warm-patch-L390) and [two](https://gist.github.com/Pinimo/c339ea828974d2141423b6ae64192aa4/#file-0001-bug-patch-master-to-reproduce-sweetly-the-cache-warm-patch-L994). 

I've put everything in a git patch so you can normally start from master, apply the patch and reproduce.
--

--
@jayhjha Perhaps it would be worth changing your config variable `SUPERSET_SERVER_ADDRESS` to `""superset""`.
--

--
A colleague made a POC on this, but came to the conclusion it is already quite difficult to have the email reports working... He wanted to use part of that code (headless browser + login) to work around the login problem. I think he found out the dependencies for the feature were not included in the Dockerfile.
--

--
To my knowledge the feature is (and will stay...) broken :cry: 
--

--
Any news from the community on this issue?
--

--
@mukulsaini I have not found the time to address the issue, to my best knowledge it has not been solved yet. If you too find this issue is a real problem, I invite you to talk it over on [Superset's Slack :left_speech_bubble: ](https://join.slack.com/t/apache-superset/shared_invite/enQtNDMxMDY5NjM4MDU0LWJmOTcxYjlhZTRhYmEyYTMzOWYxOWEwMjcwZDZiNWRiNDY2NDUwNzcwMDFhNzE1ZmMxZTZlZWY0ZTQ2MzMyNTU)

Here are a few educated guesses as how to solve the issue:

1. We could work around the auth by... **signing in through a headless browser** in the Celery process. After thinking it over, it seems difficult to me: 
    * the browser software needs to be installed and launched on the Celery worker (looks like an overkill to me)
    * the passphrase and user name need to be handled as secrets

2. Perhaps a better solution would involve **setting up an API server** with its own authentication procedure -- **or a new auth method on the same server**, to allow the Celery worker to perform cache requests:
    * Logging in would require a login token. The token would need to be handed over to the Celery worker on start-up, possibly through an environment variable or shell argument.
    * When the cache warm-up starts, the worker would use the token to authenticate through this special auth channel
    * He would not be granted any rights on the server except performing cache requests, i.e. `POST` requests that only return empty documents (not usable to extract data from the instance).
    * However, we should make sure this would not breach the server's security or reliability. For instance, the token should not be used to flood the server, DB engine and Redis instance (DDOS).

3. Not sure at all about this last idea: we could code a new CLI route to return the chart data. The Celery worker would then execute the CLI (if I'm remembering right, all the configs and docker images are the same). However, that would possibly infringe memory limits for the Celery worker.
--

--
Yet another draft solution:

4. Simpler than (2): create a user/password for the Celery worker on `db-init`s, with a very specific caching role. I find it important that this role should never be able to actually extract any data (so as not to care too much for the password being stolen), just to ping the server and get it to cache the data. It would even be possible to modify the `@login_required` decorator to add the constraint:
    * ""if the user belongs only to group `__cache_worker`, then
        * he should never POST or PUT
        * he should never see any data he requested
        * but his data should be sent out to Redis just like for anybody""
--
",jay,"
--
@Pinimo what does your cache config look like?
I setup redis caching with with a timeout of 5 minutes and cache warmup with the `topndashboard` strategy every 2 minutes (just to test). 
I can see this in Celery worker logs:
```
[2020-04-21 00:36:00,009: INFO/ForkPoolWorker-1] cache-warmup[e41de539-0bf7-4e70-b02b-4d2a132d8d0e]: Loading strategy
[2020-04-21 00:36:00,010: INFO/ForkPoolWorker-1] cache-warmup[e41de539-0bf7-4e70-b02b-4d2a132d8d0e]: Loading TopNDashboardsStrategy
[2020-04-21 00:36:00,014: INFO/ForkPoolWorker-1] cache-warmup[e41de539-0bf7-4e70-b02b-4d2a132d8d0e]: Success!
[2020-04-21 00:36:00,043: INFO/ForkPoolWorker-1] cache-warmup[e41de539-0bf7-4e70-b02b-4d2a132d8d0e]: Fetching http://0.0.0.0:8088/superset/explore/?form_data=%7B%22slice_id%22%3A%201%7D
[2020-04-21 01:06:00,131: INFO/ForkPoolWorker-2] cache-warmup[d2d68627-adce-4fa5-852e-522e95350a6c]: {'success': ['http://0.0.0.0:8088/superset/explore/?form_data=%7B%22slice_id%22%3A%201%7D'], 'errors': []}
```

but in Superset logs, I only see:
```
superset_1      | 2020-04-21 00:36:00,049 [DEBUG] [stats_logger] (incr) explore
```

Needless to say, my charts are not being updated

This is my config:
```
CACHE_DEFAULT_TIMEOUT = 300
CACHE_CONFIG = {
    'CACHE_TYPE': 'redis',
    'CACHE_DEFAULT_TIMEOUT': 180,
    'CACHE_KEY_PREFIX': 'superset_results',
    'CACHE_REDIS_URL': 'redis://localhost:6379/0',
}

class CeleryConfig(object):
    BROKER_URL = 'redis://localhost:6379/0'
    CELERY_IMPORTS = (
        'superset.sql_lab',
        'superset.tasks',
    )
    CELERY_RESULT_BACKEND = 'redis://localhost:6379/0'
    CELERYD_LOG_LEVEL = 'DEBUG'
    CELERYD_PREFETCH_MULTIPLIER = 10
    CELERY_ACKS_LATE = True
    CELERYBEAT_SCHEDULE = {
        'cache-warmup-hourly': {
            'task': 'cache-warmup',
            'schedule': crontab(minute='*/2', hour='*'),
            'kwargs': {
                'strategy_name': 'top_n_dashboards',
                'top_n': 5,
                'since': '7 days ago',
            },
        },
    }

CELERY_CONFIG = CeleryConfig
```


--
",stefanmo,"
--
Any news on this?
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",mukulsaini,"
--
@Pinimo any workaround you find to this issue? I am getting the same exact issue.
--

--
@mistercrunch What I see from the previous commits, previously the route used to cache warmup in `cache.py` get_url  was `/explore_json`. Any reason it was changed to `/explore` ?
--
",mistercrunch,"
--
Mmmh, maybe I'm missing something, but it seems like we shouldn't have to go through the web server to do this.

Refactoring / mimicking what `explore_json` does might be an option. 
https://github.com/apache/incubator-superset/blob/master/superset/views/core.py#L525-L536
--
"
9551,OPEN,Ability to Group Charts And Dashboards via Tags or Folders,enhancement:committed; global:search,2021-01-02 19:02:17 +0000 UTC,kevinpostlewaite,In progress,,"**Is your feature request related to a problem? Please describe.**
It's difficult to find specific charts and dashboards that I've created.

**Describe the solution you'd like**
As a user, I would like to be able to logically group my charts and my dashboards. I would like either folders or to be able to filter by tags when accessing the list of charts and dashboards.

**Additional context**
This is a much requested feature with many closed issues:
- [Allow users to tag dashboards](https://github.com/apache/incubator-superset/issues/3821) from November, 2017
- [Folder structure for dashboard and slice](https://github.com/apache/incubator-superset/issues/4055) from December, 2017
- [Organising Charts List](https://github.com/apache/incubator-superset/issues/7367) from April, 2019
- [Group Dashboards folders or tags](https://github.com/apache/incubator-superset/issues/7239) from April, 2019
- [Ability to Tag Charts for Easy Filtering](https://github.com/apache/incubator-superset/issues/7968)

There's even a PR but it appears that it may need to be rewritten to be accepted:
[Tagging frontend](https://github.com/apache/incubator-superset/pull/7418#pullrequestreview-251214308)

",,,eugeniamz,"
--
Did you see [SIP-34](https://github.com/apache/incubator-superset/issues/8976). 
A lot of changes in the UI are happening.  Check there and add your comments
--

--
Great feedback!! I will share this with Preset designers team. 
--
",kevinpostlewaite,"
--
Thanks @eugeniamz! That proposal is really impressive and I look forward to seeing the results. There's a lot there and I may have missed something but I didn't see any impact on grouping charts/dashboards although it looks like search in general is improved. What I would like (and don't see in that proposal) is a way for me to put charts/dashboards for, say ""Marketing"" or ""Finance"" in one directory, or apply different tags, so that I can limit my search/list display appropriately. Even though Search is improved in that proposal I don't see a way to limit to a cross-cutting subset of my charts/dashboards. Maybe this could be done by artificially creating duplicates of underlying data references, one per discriminating category value, but that would be sub-optimal.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",bkyryliuk,"
--
+1 to the @kevinpostlewaite feature request.
This is smth we would love to see as well
--
",rubypollev,"
--
+1, this is important to our team.
--

--
Tags more useful at this time than folders, but folders would be great.  Many tags to one chart / query for analytics approved queries, OKR queries, marketing-related / product-related queries, etc. would be soo helpful.
--
",isunix,"
--
What @kevinpostlewaite said is really what we need now, folders to organize our dashboards
--
"
9361,OPEN,SQL QUERY PROBLEM,,2020-03-24 09:58:44 +0000 UTC,YanXinLiu,Opened,,"### connected hive database,when I execute sql,I got this
![1585043136(1)](https://user-images.githubusercontent.com/35486255/77411385-50e8b880-6df7-11ea-9c58-f4e3520cf6b2.jpg)
---
```
some execute work well ,but others can't work.
I can't stop this query,and can't close all these tabs.but when I use mysql,everything seems be ok.
how could I solve this and close all these process? 
Thanks
```",,,,,,,,,,,,,,
9354,OPEN,Treemap splits labels on `.`,.pinned; bug; viz:chart-treemap,2021-01-02 18:27:31 +0000 UTC,serenajiang,Opened,,"In the treemap viz, labels are split on `.`, and the last ""word"" is used for the viz.

### Expected results

For the label `google.com`, the label should read `google.com`

### Actual results
![image](https://user-images.githubusercontent.com/14146019/77365155-dc3c4e00-6d12-11ea-9f03-b19ffa12760b.png)  

#### How to reproduce the bug

1. `SELECT 'google.com' AS col_a` in sqllab
2. Explore chart
3. Create a treemap viz with metric `COUNT`, grouped by `col_a`
4. Run Query

### Environment

(please complete the following information):

- superset version: `superset version`: Up to date with master as of 2020-03-18
- python version: `python --version`: 3.6
- node.js version: `node -v`
- npm version: `npm -v`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.
",,,stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",mistercrunch,"
--
For whoever wants to work on this, here's a pointer to the source:
https://github.com/apache-superset/superset-ui/blob/master/plugins/legacy-plugin-chart-treemap/src/Treemap.js#L135-L158
--
",,,,,,,,
9298,OPEN,[SIP-41] Proposal for Alignment of Backend Error Handling,change:backend; sip,2020-07-20 16:59:37 +0000 UTC,john-bodley,Opened,,"## [SIP-41] Proposal for Alignment of Backend Error Handling

### Motivation

[[SIP-40] Proposal for Custom Error Messages](https://github.com/apache/incubator-superset/issues/9194) proposes a unified pattern for handling API errors on the frontend however currently it is not apparent what error types (surfacing from the backend) need to be supported. 

Reading through the code it is not super apparent:

- What exceptions are raised.
- How errors are handled and by whom, i.e., via the Flask or FAB error-handler, re-wrapped, etc. 

Superset defines a [number](https://github.com/apache/incubator-superset/blob/master/superset/exceptions.py) of exceptions each deriving from the `SupersetException` base class which contains an HTTP response status code in the [400, 600) range. Additionally there are a [number](https://github.com/apache/incubator-superset/blob/52c59d689063bfac9cba73b416d2b1bdc6a1275e/superset/datasets/commands/exceptions.py) of exceptions related to dataset commands.

#### Raised Exceptions

It can be quite difficult to understand exactly where and when an exception will be handled. The Python community as a whole does a fairly poor job of documenting (sadly I sense a shortage of typing was not adding support for exceptions) what exception types a method can raise and thus  from a development perspective tracing the exception flow can be challenging. 

#### Error Handling

A common approach in Flask for [handling errors](https://flask.palletsprojects.com/en/1.1.x/errorhandling/#error-handlers) is registering error handlers which can defined at either the application or blueprint level. This pattern is also supported by a number of the Flask RESTful extensions. Currently there only exists [one](https://github.com/apache/incubator-superset/blob/406ad8778c4b83461e534b03d165c279484b2d2c/superset/views/core.py#L2628) registered error handler in Superset and [one](https://github.com/dpgaspar/Flask-AppBuilder/blob/cfb324007ca12c6d5939e05521880ca7ba888236/examples/simpleform/app/views.py#L36) in FAB.

Even though Superset has a number of defined exceptions there is no consistent way in how these are propagated/handled and thus it is hard to do an audit on which errors exists. For example:

- [Here](https://github.com/apache/incubator-superset/blob/406ad8778c4b83461e534b03d165c279484b2d2c/superset/views/core.py#L325) is an example of returning a Flask Response with an error status. It seems this pattern is mostly used for non-API calls.
- [Here](https://github.com/apache/incubator-superset/blob/406ad8778c4b83461e534b03d165c279484b2d2c/superset/views/core.py#L315) is an example using the [`json_error_response`](https://github.com/apache/incubator-superset/blob/116200cf73c016337aa7ef1b80e80f78b14cf30e/superset/views/base.py#L69) which wraps the Flask Response ensuring that the MIME type is `application/json`. This is akin to this Flask error-handler [example](https://flask.palletsprojects.com/en/1.1.x/errorhandling/#generic-exception-handlers).
- [Here](https://github.com/apache/incubator-superset/blob/78ba7d52f64950f97e8ef0498aac594fd8b3dd42/superset/views/dashboard/api.py#L59) is an example where the `SupersetException` is re-raised as a `ValidationError`.
- [Here](https://github.com/apache/incubator-superset/blob/52c59d689063bfac9cba73b416d2b1bdc6a1275e/superset/datasets/api.py#L211) is an example where exceptions are being handled as FAB responses defined [here](https://github.com/dpgaspar/Flask-AppBuilder/blob/f412c9291f98f7d2f5450e2d9463fbc7469dcdd1/flask_appbuilder/api/__init__.py) which return a Flask `Response` where the MIME type is `application/json` (per [here](https://github.com/dpgaspar/Flask-AppBuilder/blob/f412c9291f98f7d2f5450e2d9463fbc7469dcdd1/flask_appbuilder/api/__init__.py#L663)).
- [Here](https://github.com/apache/incubator-superset/blob/607cfd1f29736590fbba397c4f8a04526be66aff/superset/views/base.py#L114) seems to be where some API exceptions are handled. Note that the `handle_api_exception` method is a decorator, rather than serving as a Flask error-handler and i.e.,

```python
@handle_api_exception
@expose(""/v1/query/"", methods=[""POST""])
...
```

### Proposed Change

In order to better understand the various types of errors and scenarios in which they can surface I propose we undertake **three** broad approaches: 

1. **Documentation:** Per the [CONTRIBUTING.md](https://github.com/apache/incubator-superset/blob/master/CONTRIBUTING.md#python-1) suggestion, documenting which exception a method raises. 
2. **Standardization**: Ensuring that all API errors are handled using the same handling mechanism (be that Flask, FAB, decorators etc.), providing consistent status codes, error payloads, etc.
3. **Simplification:** Removing unnecessary redirection and obfuscation with error handling. Ideally all relevant exceptions will be handled in one place which ensures consistent responses.

To address  (2) and (3) personally I prefer the Flask error-handler approach, though due to our dependency on FAB we may need to align with that model.

I don't sense there's a quick fix for any of these steps, though I think there's merit in aligning on an approach which we can then i) manually enforce in code reviews, and ii) systematically refactor the code if necessary.

### New or Changed Public Interfaces

There are no new or change public interfaces. This SIP merely proposes consolidating API error handling.

### New dependencies

None.

### Migration Plan and Compatibility

N/A.

### Rejected Alternatives

None.",,,dpgaspar,"
--
Good initiative and now is a very good time to define the path forward.

Adding a bit of more detail regarding new API implementation:

Currently we are migrating MVC and ""old"" API's to new REST API's. At the same time we will refactor charts and dashboards to follow SIP-35 approved proposal. So, new API's that use PUT, POST, DELETE are planned to always call a command that implements the required business logic.

A command will succeed or raise a defined custom exception, these exceptions are structured the following way:

- All command exceptions inherit from `CommandException`
- All command validation exceptions inherit from `CommandInvalidError`

A `CommandInvalidError` is a wrapper around a list of marshmallow `ValidationError` exceptions, this way we leverage mashmallow JSON error responses for field validation, aligning business validation with marshmallow schema validation. So we get expected coherent error responses like the following:

HTTP 422
``` json
{
  ""message"": {
    ""database"": [
      ""Database does not exist""
    ],
    ""owners"": [
      ""Owners are invalid""
    ]
  }
}
```
Generic structure:
```
{
    ""message"": {
        ""<FIELD_NAME>"": [""<ERROR_MSG1>"", ""<ERROR_MSG2>"", ...],
        ...
   }
}
```
The main structure for errors come from FAB default `{""message"": ... }` but we can override it or just plain raise and let Flask handle errors



--
",etr2460,"
--
Tagging @ktmud and linking to some of his thoughts on this topic from a PR comment: https://github.com/apache/incubator-superset/pull/10274#discussion_r454127147
--
",,,,,,,,
9281,OPEN,[SIP-53] Incorporating other map sources in Superset,.pinned; viz:chart-maplayer,2021-01-02 18:31:19 +0000 UTC,gk1089,In progress,,"## [SIP] Proposal for Incorporating other map sources in Superset

### Motivation

Superset currently uses MapBox to display maps which has its own limitations. There are other map sources available as Web Map Services (WMS). For example, Open Street Maps (OSM) that can be called upon using many js libraries like Leaflet or OpenLayers. This would pave way for more GIS functionalities in SuperSet in true open source spirit.

Later, the same could also apply to the deck,gl visualizations that simply use mapbox for its simplicity.

### Proposed Change

Module can be created that permit adding WMS services during the creation of charts. Other features may include:
1. Adding multiple layers and providing facility to turn them on/off.
2. Permitting clicking on features (points) to show a popup with more data/links on the map itself.

### New or Changed Public Interfaces

Same as described above.
### New dependencies
Leaflet (https://www.npmjs.com/package/leaflet) and OpenLayers (https://www.npmjs.com/package/ol)

### Migration Plan and Compatibility

Should work with existing setup.

### Rejected Alternatives
I am proposing considering the open data sources right now. So, no Google/Bing maps etc.

Describe alternative approaches that were considered and rejected.

I would like to attempt this but I am really at a loss for where to begin with implementing it. I have seen discussions about new chart types here and although the documentation is old regarding that, some of the issue threads provide pointers in the right direction.

I intend this thread to be something of the same kind where some of the developers may point us to the right section of the code where we can begin exploring and implementing open source mapping visualizations.

I hope the developers see some merit in it.",,,stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",dharamgajera,"
--
Any Update on this ?
--
",juen1jp,"
--
I would be interested in engaging with the community about this. MapBox is a non-starter for us and a solid block for us to use Superset. Are there plans to implement support for open mapping projects? We would be interested in helping to get open mapping projects added so the package is a truly open source solution. 
--

--
I think there is some confusion. Those of us who work for corporations don't want a commercial provider (Mapbox, Google, etc)

We need to keep to freely available solutions. This generally looks like running our own tileset servers and then something like leaflet to serve the mapping visualization. 

ECharts looks nice and even has a rudimentary leaflet plugin. My only initial concern is that it is introducing more layers of complexity; however, that may be fine.

In general, I am new to the community. I am currently scoping the work that would need to be done in order for us to use this project. Defining a good default visualization framework is great. Outlining a formal way to contribute plugins and defining how to create a plugin is much better.

It looks like echarts will make an excellent default for the visualizations. However, realistically we will probably have to create some more custom working visualizations for our use cases. If they are generalizable we would be happy to push them to a less formal community plugin visualization library that users can pick/choose what types to install. I would greatly recommend such a ""visualization library"" be created to allow folks to contribute a wider variety of charts. No matter what library you go with, you will get a much larger variety of use cases covered if you harness the open source community to contribute new chart types.
--

--
Keeping the alternate plugins outside seems like the smart way to go for me too. Perhaps we should instead close this and open another SIP focused on formalizing the plugin ecosystem (although I am assuming that is already on the roadmap)
--
",villebro,"
--
@juen1jp I'm sure a contribution would be warmly welcomed by the community. As it will probably be a major change, I would propose opening a SIP (Superset Improvement Proposal) to make sure the community is able to chime with ideas and concerns prior to any major development work taking place.
--

--
@gk1089 my bad, I didn't notice this was in fact opened as a SIP as it wasn't indicated in the title. The title should be prefixed with `[SIP-XX]` where XX is the next available unused SIP id. I believe `SIP-50` is the next available id.

I assume most mapping systems require some for of token. This should be added to `config.py` so that it can be defined in the local `superset_config.py`. Currently the mapbox token is returned by `viz.py` to the relevant visualizations; I believe the plan is to return this type of data in the bootstrap payload in the future. The committers can surely help with the details once we get that far. I propose reaching out on Slack, e.g. the `#visualization_plugins` channel.
--

--
In case there is no need for a token, then this will be quite straight forward, and should not be any different from any other new viz plugin. Please see the [Hello World blog post](https://preset.io/blog/2020-07-02-hello-world/) on how to get started of you haven't already.
--
",gk1089,"
--
> 
> 
> @juen1jp I'm sure a contribution would be warmly welcomed by the community. As it will probably be a major change, I would propose opening a SIP (Superset Improvement Proposal) to make sure the community is able to chime with ideas and concerns prior to any major development work taking place.

@villebro I opened this thread as a SIP itself. Are you proposing a fresh SIP be opened? 

I just need a few indicators regarding what part of the code to look into, as in, the code files that make the mapbox magic happen. Just searching with 'mapbox' keyword in the code files has not given me much to go upon.
--

--
@villebro Thanks for your response. As far as I understand it, tokens are compulsory only for metered map services like MapBox, ESRI, Bing, Google map services etc. For the open map services like Open Street Map (OSM), this is not a requirement and maps can be accessed directly by including the corresponding (Web Map Service) WMS URL through some js libraries like Leaflet or OpenLayers.

For example, I found the WMS URL (http://maps.heigit.org/osm-wms/service?REQUEST=GetCapabilities&SERVICE=WMS) for a world map on the info section of https://www.osm-wms.de/, which in turn was found on the OSM page on WMS (https://wiki.openstreetmap.org/wiki/WMS). I can use this WMS URL to show a map in my own page and the corresponding Leaflet/OpenLayers library will provide me tools to navigate on the map, show popups, make measurements etc.

It would be great to have the same control through a custom Superset visualization. 

I shall try to join the slack and discuss it there and I request @juen1jp to chime in with his ideas. Meanwhile, may I request you to tag contributors who may have worked on the mapbox module in the past to share their take on this in this thread.
--

--
@villebro May I also request for this thread to be marked as active. It is currently marked as 'closed' by the activity bot.
--

--
> In case there is no need for a token, then this will be quite straight forward, and should not be any different from any other new viz plugin. Please see the [Hello World blog post](https://preset.io/blog/2020-07-02-hello-world/) on how to get started of you haven't already.

Thanks! I was not aware of this post. 

While I have a go at it, for the sake of reusing the code is it possible to point to the files which deal with the existing MapBox code?
--
",roaur,"
--
I was curious to know the status of this SIP so I was digging around in the repo and saw that SIP-50 already exists in the SIP Project board. Here's SIP-50 as it exists on the project board: https://github.com/apache/incubator-superset/issues/10418

May I suggest this SIP number be updated to 53?
--

--
That sounds fair. Between the [Hello World blog post](https://preset.io/blog/2020-07-02-hello-world/l) (thanks a lot, @villebro) and [ECharts examples](https://echarts.apache.org/en/download-extension.html) could a JS newbie implement the leaflet EChart example into Superset? Could you point me in other places you'd look if you were to implement such a chart into Superset?
--
"
9223,OPEN,[rbac]clone roles Management,,2021-03-31 05:57:25 +0000 UTC,CraigChaffee,Opened,,"**Is your feature request related to a problem? Please describe.**
It's incredibly difficult to manage, create roles.

**Describe the solution you'd like**
The ability to clone a role or copy/export/import roles. Or just fix the UI so I can paste permissions as text and have them work. Then we could easily test roles before.

**Describe alternatives you've considered**
Scraping values from the DB directly.:(

**Additional context**
Having to comb through permissions one-by-one and paste them in to merge is a brutal waste of time. Instead if we could just clone gamma and add what we actually want.
",,,stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",stevensuting,"
--
@junlincc This feature existed in version 0.36. Does not anymore in 0.37.1
Not sure why. Perhaps @villebro could comment.

--

--
<img width=""199"" alt=""Screenshot 2020-10-13 at 3 15 19 PM"" src=""https://user-images.githubusercontent.com/8875448/95844787-f305c900-0d66-11eb-9610-2c2c7de9e5b8.png"">

I re-looked at v 0.37.1, the function of copy role works fine in the default setup.

I have noticed it ONLY allowing copying of the Public role when using AUTH_TYPE setup enabled, even when the logged in user is an Admin. Perhaps that's what the other are facing as well.


--
",junlincc,"
--
@dpgaspar 
--
",,,,,,
9194,OPEN,[SIP-40] Proposal for Custom Error Messages,change:backend; change:frontend; sip,2020-03-02 22:36:48 +0000 UTC,etr2460,In progress,,"## [SIP-40] Proposal for Custom Error Messages

### Motivation

The current user experience for error cases in Superset does not provide actionable feedback to users. Error messages often are displayed straight from the backend and are fairly inscrutable. Additionally, theres no way to customize or add other functionality to these messages on a deployment by deployment basis. A key part of the user experience is providing friendly, actionable messaging when something goes wrong.

### Proposed Change

We propose a new, unified pattern for error handling in API endpoints across all of Superset. This will consist of the following work:
1. Unify the backend around a standard error payload for all API errors returned from Superset of the format:
```
{
  errors: [
    {
      message: string,
      error_type: Enum,
      level: info | warning | error,
      extra: Dict[string, Any],
    }, ...
  ]
}
```
2. Create an Enum on both the server and client side for uniquely identifying error types. The specific type of an error is included in the standard error payload (e.g. ACCESS_DENIED, DB_ENGINE_SYNTAX_ERROR, etc.)
3. Refactor Supersets frontend to go through a single `SupersetAlert` component for handling all error messages. This component will import a renderer file that defaults to some generic error customization messages (ex. Adding Request Access links).
4. Allow a Superset admin to specify an override renderer file that gets used instead of the default renderer. This override renderer is injected by leveraging the Registry class from @superset-ui/core to handle the custom renderers registration.

### New or Changed Public Interfaces

- All APIs within Superset will return the above error payload on 4xx and 5xx status code responses.
- Client side options will be made public for injecting your own error rendering file

### New dependencies

N/A 
### Migration Plan and Compatibility

No migrations should be necessary, but the new API may not be backwards compatible. Any changes to the current API will be notated in UPDATING.md and should take place prior to v1.0.0.

### Rejected Alternatives

Server side rendering of errors was rejected as were trying to remove the existing templates and move all rendering to the client side

cc: @kristw @rusackas @nytai @graceguo-supercat @ktmud",,,kristw,"
--
@etr2460 The error code and `SupersetAlert` component seems useful for usage outside of Superset app as well. Do you want to implement the front-end error handling as a new package `@superset-ui/superset-error` to provide typings and components? 

> Allow a Superset admin to specify an override renderer file that gets used instead of the default renderer. This override renderer is injected with a Webpack Plugin.

Can create a registry for renderers and use error code as `key`, similar to the chart registries. The `SupersetAlert` component then can ask the registry if a custom renderer for the received error exist, otherwise fallback to default renderer.  Adding a custom renderer will be the same way with registering chart plugin. 

> We propose a new, unified pattern for error handling in API endpoints across all of Superset. This will consist of the following work:

Could you add a few examples of the current error api result that are fragmented?
--

--
@etr2460 Got it. Also to keep the scope actionable, I agree with modifying the code inside superset as a start. You can still leverage the `Registry` class from `@superset-ui/core` to handle the custom renderers registration.
--
",craig,"
--
@etr2460, this is great. It looks like a step towards getting all API responses to have a standard shape. One thing that I would change a tiny bit would be the response shape. Enums are awesome for this sort of thing, but one thing I would add would be an error ""code"" which would simply be an `int` which can then be taken action upon by the consumer. There's also the potential for having multiple errors in a given response (think API validation where several keys are required). In my view, Enum names should be used solely for the purpose of code organization, and should not be transmitted to consumers. Instead, error state should depend on the ""error code"".

A slight variation to your structure:
```
{
    ""errors"": [{
       ""code"": 123,
       ""message"": ""Something is wrong here"",
       ""extra"": {
           // Additional contextual stuff in here, for instance field validation messages
       }
    }]
}
```
--

--
Makes sense. I think one nice thing about the int is that it augments the status code and can be grouped in much the same way as HTTP statues, i.e. [10000,11000) are for error class A, and [11000,12000) are for class B, etc.. I can see what you're saying from the point of view of improving the readability of the errors based on code. Seems fine either way :)
--
",etr2460,"
--
@kristw:

One concern I had about moving this out into a Superset error package was that I don't think strings for default error messages would get translated in those packages. That seemed like a non-starter to me, so I didn't consider it. If you have a way to resolve this, then I'd be happy to hear it!

For examples of fragmented error handling, in SQL Lab we render alerts based on error messages injected inside the query object (as well as the poorly defined `link` parameter): https://github.com/apache/incubator-superset/blob/291306392443a5a0d0e2ee0cc4a95d37c56d4589/superset-frontend/src/SqlLab/components/ResultSet.jsx#L201-L212
but in Charts we render them based off the `chartAlert` param (in a completely different component too):
https://github.com/apache/incubator-superset/blob/291306392443a5a0d0e2ee0cc4a95d37c56d4589/superset-frontend/src/chart/Chart.jsx#L136-L143

@craig-rueda:

I totally agree with an `errors` array inside the structure, that makes a bunch of sense. We'll need to consider what the default UI experience is (multiple alerts? merging errors?) but I think it'll be worth it.

One thing I don't agree with is using an error code as opposed to an enum. This is for two reasons:
- The error code in the response would be easily confused with a http status code that's already an int.
- By using an Enum, we can both provide some idea of what the error is to the user, but also provide a unique string for searching error resolutions online. Instead of getting a `Superset Error 1432` you'd get a `Superset Error PRESTO_INTERNAL_SERVER_ERROR` which i think is much more readable and could save the extra trouble of mapping a code to resolutions online. The enum provides additional value to the error message as an error like this might include the message: `Presto encountered an unknown issue when running this query. Please try again later.` and the enum specifies a more technical error
--

--
@suddjian:

Dynamic error messages should be supported on the frontend with the translation package: https://github.com/apache-superset/superset-ui/tree/master/packages/superset-ui-translation#api

My vision is to have a default renderer that simply renders the message passed from the backend, something we can guarantee exists every time. That's why i'd prefer not to label it as `debug` since if it's not handled in any other way, we'd display this field. However, if you wanted to render a more specific component, you'd rely on the `error_type` passed and the content in `extra` to render it. I would envision that applying to form validation errors too. An InputValidationError would be a subtype of Error where error_type = `INPUT_VALIDATION_ERROR` and `extra` is defined as an object that must contain a `field_name` and a `message_params` attribute. This InputValidationError could also be defined on the backend so the extra field is guaranteed to match what's expected. Then you'd be able to get type safety on the front end as well. Thoughts?
--

--
Yeah, if you have a better name than `extra` i'm all ears. Maybe `metadata` or `details`? Not sure, but I think I've seen `extra` before.

On the note of splitting extra out into other objects, my goal was to allow `extra` to be as free form and flexible as possible so that you could add whatever relevant data was needed within it to render an error (e.g. on a chart error in a dashboard maybe we want to know the chart owner's name, in a SQL Lab failed query maybe the url of the db engine where the failed query was run). I'm hesitant to add too much structure explicitly, but i hope that norms will quickly present themselves when migrating the current errors over to the new format.
--

--
I've updated the SIP summary to include feedback from the thread and will be kicking off a vote now
--
",suddjian,"
--
I love it! Been wanting to address this for a while, so thank you. I have some questions/suggestions.

How should translation fit into this? Error messages are sometimes written to have dynamic values contained in them, such as `""{input}"" is not a valid url`, or `{some_resource} is already in use`. Do we only allow static error messages? Is there a way for dynamic error messages to play well with the translation system? Dynamic error messages can often make a big difference in UX: it can be tricky to write static messages that accurately describe every situation where you might see the error.

I lean towards a system where the text of the error message is defined entirely on the frontend, with the error object containing parameters with which to construct a message. It makes sense for the backend to supply an error message for debugging purposes, but that field should indicate that it's not meant to be used in UI.

I'd also like form validation errors to have a standard schema, so that the frontend can display validation errors in the appropriate place in the UI. Since we seem to be going towards having an array of error objects, we could attach a `field_name` value to an error when it relates to a specific field. So if a hypothetical url field called `custom_url` was submitted with a bad value, you'd get back:

```json
{
 ""errors"": [{
    ""error_type"": ""INPUT_INVALID_URL"",
    ""level"": ""error"",
    ""debug_message"": ""Invalid url value for custom_url"",
    ""field_name"": ""custom_url"",
    ""message_params"": {
      ""input"": ""https//notvalid""
    }
  }]
}
```

~Lastly, I could use some clarification: what's the purpose of allowing admins to override the renderer?~ Nevermind, I get it now - it's just for the generic messages, that makes sense.
--

--
I think that vision makes sense. The additional definition on the type contract of `extra` is extremely helpful.

I wonder if there is a more descriptive name out there than `extra` for that info. Or maybe splitting `extra` out into more specific objects with contracts related to their domain would be useful. (`validation`, `documentation`, etc.)
--
",,,,
9187,OPEN,[SIP-38] Visualization plugin refactoring,org:preset; sip,2020-07-07 02:59:03 +0000 UTC,rusackas,In progress,,"## [SIP-38] Visualization plugin refactoring



### Motivation

One of the most commonly reoccurring questions in the Superset community, on Slack and elsewhere, is that of how to add a new data visualization. The answer, in short, has been its hard. While that may be true, the goal of this SIP is to lay out both tactical refactor needs for the current implementation to mature, as well as proposing a handful of roadmap features to make plugin development significantly easier. These changes will make upcoming modifications of existing plugins (see [SIP-34](https://github.com/apache/incubator-superset/issues/8976)) drastically simpler, and steer toward opening an ecosystem of Superset visualization plugins.

Much planning and work has already been done to address the difficulty of adding/editing plugins, including a new query API endpoint, but there are many blocking issues and code migrations remaining to complete this process. Special thanks to @kristw, @williaster, @xtinec, and @conglei for their significant contributions to the frontend and API work thus far. These issues, and proposed solutions for them, are enumerated below. Additional suggestions are welcome.

### Proposed Changes

#### General Goals:
- As much code and configuration as possible for individual visualization plugins should be moved out of `incubator-superset` and into the individual plugins repos (in a perfect world, a new plugin wouldnt require touching two repos and opening two PRs).
- Reduce frustration in working on plugin repos, allowing people to more easily see changes as they make them


<table>
<tbody>

<tr>
<td valign=""top"">

**Issue:**
Control panel configurations for visualizations are centralized in a difficult to maintain [controls.jsx](https://github.com/apache/incubator-superset/blob/master/superset-frontend/src/explore/controls.jsx) file. All controls are located in `incubator-superset` , necessitating writing code in two PRs for two repos.

</td>
<td  valign=""top"">

**Proposal:**
Control configurations (particularly the ones that are unique to any given plugin) should be migrated into the correct individual [control panel config files](https://github.com/apache/incubator-superset/tree/master/superset-frontend/src/explore/controlPanels) . An example of this can be found in [This PR](https://github.com/apache/incubator-superset/pull/8222/files). These individual configs should then be migrated to the individual plugins, and references removed from [setupPlugins.ts](https://github.com/apache/incubator-superset/blob/master/superset-frontend/src/setup/setupPlugins.ts).

</td>
</tr>




<tr>
<td valign=""top"">

**Issue:**
Plugins (particularly when using the legacy api (`/explore_json`) require an entry in [viz.py](https://github.com/apache/incubator-superset/blob/master/superset/viz.py).  In addition to requiring code changes to two repos, the logic in [viz.py](https://github.com/apache/incubator-superset/blob/master/superset/viz.py) has proven to be fragile and cumbersome to maintain.

</td>
<td  valign=""top"">

**Proposal:**
Use of `viz.py` should be deprecated in favor of the viz-agnostic `api/v1/query` endpoint. In an effort to decouple this, `viz.py` logic (data transformations) should be broken out into individual modules and/or reusable methods, which should be invoked by the new endpoint. This will additionally require that controls should be consolidated wherever possible, e.g. use a single control for `metric`, `metrics`, `metric_2`, `secondary_metric`, etc.

</td>
</tr>


<tr>
<td valign=""top"">

**Issue:**
New and existing plugins cannot yet fully utilize the new `api/v1/query`endpoint due to the following issues:
- Superset does not yet respect a plugins `useLegacy` flag to call the correct endpoint when required
- The API has no means to accept data transformation options needed for post-processing (e.g. Pandas) to reach feature parity with the legacy API.
- The API does not have unit tests
- The API is not documented

</td>
<td  valign=""top"">

**Proposal:**
- Modify [exploreUtils.js](https://github.com/conglei/incubator-superset/blob/5ea282e45fc5d7a8f7fbe93641d6caf2825abd25/superset/assets/src/explore/exploreUtils.js) such that the `getURIDirectory` method calls the right endpoint depending on the `useLegacy` flag
- Add configuration options to the API call to invoke backend post-processing operations, returning transformed data
- Write unit tests and documentation
- Deprecating the `explore_json` endpoint
</td>
</tr>



<tr>
<td valign=""top"">

**Issue:**
Each plugin must be registered manually in `incubator-superset`s [MainPreset.js](https://github.com/apache/incubator-superset/blob/master/superset-frontend/src/visualizations/presets/MainPreset.js) file. Additionally, customizing the plugins loaded for a deployment (i.e. disabling some) is done via [setupPluginsExtra.js](https://github.com/apache/incubator-superset/blob/master/superset-frontend/src/setup/setupPluginsExtra.js), meaning the plugins are still loaded as dependencies. And this method only supports plugins removal, but does not let you add new plugins that are not listed in `package.json` from `master`.

</td>
<td  valign=""top"">

**Proposal:**
Attempting to load plugins via [ES2020s dynamic imports](https://github.com/tc39/proposal-dynamic-import). The exact implementation of this is a bit TBD, but the idea would be to move the responsibility for registering/loading plugins away from [MainPreset.js](https://github.com/apache/incubator-superset/blob/master/superset-frontend/src/visualizations/presets/MainPreset.js). Instead, the plugin paths/packages (and their associated `keys`) could be bootstrapped as an overridable configuration file, and Superset could lazy-load the plugins accordingly. (note: dynamic imports are not [supported natively](https://caniuse.com/#feat=es6-module-dynamic-import) by IE, but Babel provides [potential recourse](https://babeljs.io/docs/en/babel-plugin-syntax-dynamic-import) for that).

</td>
</tr>

<tr>
<td valign=""top"">

**Issue:**
Development work on plugins requires manually running a `npm link` operation to load the local plugin, and thus see updates/edits in Superset - this is troublesome in that it is both fragile, and difficult for many developers to discover, as its not a common pattern).

</td>
<td  valign=""top"">

**Proposal:**
Automate the process! Create a plugin dev mode NPM script that automatically links (or unlinks) viz plugin packages. See a working example of this concept in [this PR](https://github.com/apache/incubator-superset/pull/8638). This would involve refactoring the `NVD3` plugins to not rely on `/lib` path, `preset-chart-xy` to not rely on `/esm` path - all plugins should follow the same build and source directory pattern.

</td>
</tr>

</tbody>
</table>

### Additional (follow-up) refactoring tasks

- Follow CSS-in-JS patterns (see [SIP-37](https://github.com/apache/incubator-superset/issues/9123)) in viz components, sharing common theme styles/variables with `incubator-superset`. Theme variables may need to be moved to `superset-ui` to be consumed by both `superset-ui-plugins` and `incubator-superset`.
- Audit and address issues with, and completeness of, i18n of plugin text.
- Converting all viz components to TypeScript (see [SIP-36](https://github.com/apache/incubator-superset/issues/9101))

### New or Changed Public Interfaces

The query endpoint at `/api/v1/query` needs significant enhancement, as laid out in the proposals above (post-processing options, tests, docs).

### New dependencies

N/A

### Migration Plan and Compatibility

N/A

### Rejected Alternatives

- **Reintroducing viz plugins into incubator-superset**
Having the plugins be in their own repos is troublesome from a workflow perspective (due to the multiple PRs required, NPM Link work needed, and separate build processes required). The proposals laid out above seek to minimize this difficulty. While it is certainly possible (and indeed likely easier) to move the plugins back into Superset itself (like [Redash](https://github.com/getredash/redash/tree/master/client/app/visualizations) and [Metabase](https://github.com/metabase/metabase/tree/master/frontend/src/metabase/visualizations/visualizations) do), solving these more difficult problems seems more likely to open the door to a true plugin ecosystem for Superset.
- **Moving data transformations to plugins (JS), deprecating Pandas**
The idea has been floated that perhaps data transformation (at least in some cases) might be more the responsibility of the viz plugin itself than the backend, and maybe if we moved that logic, we could deprecate Pandas. To test the theory, some basic benchmarking attempts were made on large rollup and pivot tasks, to compare the performance of Pandas against [Zebras](https://github.com/nickslevine/zebras), [Datalib](https://github.com/vega/datalib), [Ramda](https://ramdajs.com/), and [Lodash](https://lodash.com/). This approach, at least as a global migration, was decided against for these reasons:
    - Sending an entire dataset over the wire, if the frontend just needs a rollup, is a waste of resources
    - If post-processing is done on the backend, the result can be cached for use by multiple charts (or multiple clients and reloads)
    - Neither Zebras nor Datalib provides an out-of-the-box pivot function on par with Pandas, and the `pivotWith` ""recipe"" from the Ramda cookbook looked to be significantly slower than Pandas (approx 10x).
    - All these libraries provide grouping, sorting, map/reduce functionality, so you can pivot the data manually. But then, so does [Lodash](https://lodash.com/), which matched (or slightly beat) the other JS libraries' performance. This was still about 2x slower than Pandas.
    - **TL;DR**: If you want to avoid writing Python for a new viz or calling it through the new API, and want to do a little data munging on the frontend, just use lodash or vanilla JS for best results.",,,robdiciuccio,"
--
Looks like a solid plan, thanks for writing this up. A few questions (for you or the community):

- What was the original rationale in moving plugins out into their own repo(s)?
- How are you thinking about exposing Pandas data transformation functionality to the plugins via the `/api/v1/query` endpoint. Will there be a whitelisted set of transformation methods that can be called? Will it support chaining these transformations?
- Should we rename the `/api/v1/query` endpoint to something less ambiguous?
- For i18n, I realize this is bigger issue than just visualizations, but have you looked into libraries like [globalize](https://github.com/globalizejs/globalize) or other [CLDR](http://cldr.unicode.org/)-based solutions?
- Will migration to the new plugin architecture simply be handled by the `useLegacy` frontend flag? What should the deprecation plan for the old architecture/endpoints be?
--
",kristw,"
--
> What was the original rationale in moving plugins out into their own repo(s)?

* The collection of visualizations in Superset had grown organically over
time, with varying quality and lack of ownership for some charts. All
charts were shipped with every release of Superset. There was no easy way
to get rid of them.
* The chart code with no owner (after the PR was merged and the chart
author abandoned them) became dead code that nobody owns in the main repo,
and the list kept growing. Maintaining the ""visualizations"" folder at that
time was a monumental tasks.
* Developers have custom needs and keep asking to add more types of charts.
Some are very specific and debatable whether that is useful for anyone
else.
* Very difficult to test the charts.

Breaking them into independent packages and lighter weight repositories allow:

* more stable `incubator-superset` and less code to maintain. If a chart
will only be used by single organization, then the entire community should
not have to maintain it.
* chances to deprecate uncommon charts by dropping the plugin from the main
bundle, but still make them available as packages for developers to install
them into their own deployment.
* enforced decoupling. remove hacks for special handlings of certain charts.
* less arguments about adding certain charts or not.
* visual inspection of every PR with storybook and easier maintenance.
* developer productivity: reduced ci build time, smaller codebase, able to
apply more strict linting rules.

> One key part of the embeddable project is to move towards *chart plugins system*, which we can register only necessary charts for superset or register custom ones as wish. This will give more flexibility to the developers to customize their superset instances (making it more lightweight, include-only-needed, or include custom vis type) as well as improve maintainability of the superset codebase (instead of hosting every visualization in the main repo). In order to do that, we aim to split the chart types (i.e. most of the things in src/visualizations) into one or more plugins (npm packages), independent from superset. Then, we will implement a registry mechanism for importing plugins.

from https://github.com/apache/incubator-superset/issues/5680


--

--
@rusackas I have re-read this item again and it is still not quite clear how the proposal (dynamic import) will address the issue. 

![image](https://user-images.githubusercontent.com/1659771/77015471-1af88f80-6932-11ea-9dd2-96ca3b2e176b.png)

1)  how will dynamic import help with adding new plugins that are not listed in `package.json` from `master`?

2) not registering all the plugins

> bootstrapped as an overridable configuration file, and Superset could lazy-load the plugins accordingly.
* Generating `MainPreset.js` from the configuration file could be another solution as well.
* For clarification, if the bundle size is concerned, Superset is already lazy-loading the plugins via dynamic imports.


--

--
Thank you for your prompt reply and clarification.

> > Generating MainPreset.js from the configuration file could be another solution as well.
> This is essentially what I'm trying to take a step toward. My understanding is that to do that, you'd have to load in an array of plugins/paths from the config (or some state driven by a future UI, even), and then you could then dynamically load those packages and/or paths. This is what that other WiP PR is taking a stab at.

What do you think if the `MainPreset.js` generation happen as part of `webpack` build or some build script, so the app remains unaffected and can treat `MainPreset.js` as a regular file?
--
",ktmud,"
--
Thanks for the writeup, @rusackas! This raises a lot of great points and cleared many of my doubts.

Im still new to Superset, so apologies if I missed some historical context. Just to be clear, what I meant in [#8638](https://github.com/apache/incubator-superset/pull/8638) was to **temporarily** move plugins and their registries back to `incubator-superset` **until things are more settled**plugin API matured, core plugin quality under control, and [the big UI overhaul (SIP-34)](https://github.com/apache/incubator-superset/issues/8976) finally starting to take shape.

My biggest concern was it might take a lot of efforts to get there and having to sync code between multiple repos slows the whole process down. Even if we can solve the linking issue with a custom `npm` command, people still need to make (and review) two (maybe three) PRs whenever the registry or control panel API needs to change. Overall, the overhead the 3-repo structure imposes _right now_ seemed overweight the potential benefits it may bring in the futureconsidering there might be other solutions to achieve the same goal.

In general, I agree it makes a lot of sense to have optional visualizations as separate packages and in their own repo. Future Superset admins may even install a plugin from the web UI so it's only natural to do this. But for core visualizations, I had doubts. Superset as a software needs to ship with some visualizations anyway and having the source code in one place makes it easier for developers to start writing their own plugins or upgrading an existing onesimpler dev env setup, easier navigation, better git history... many of which not easily replaceable by npm scripts. Most OSS with a plugin system took this approach, in additional to Redash and Metabase mentioned before, think [WordPress](https://github.com/WordPress/WordPress/tree/master/wp-content/plugins), [Gatsby](https://github.com/gatsbyjs/gatsby/tree/master/packages), and [Strapi](https://github.com/strapi/strapi/tree/master/packages). 

----

@kristw , to mitigate some of the original concerns, maybe we can take following actions (if we haven't):

- Officially freeze the `visualization` folder, no PRs will be accepted or reviewed until we clean things up and plugin API stabilized
- Require all new visualizations to have unit test that do not depend on Superset backend API
- Place additional more strict `.eslintrc` to the folders of migrated components.

----

Alternatively, we can also have a frontend ""monorepo"" within `incubator-superset`, publishing `superset-ui` and ""official"" plugins as separate packages but track them under the same git repo with the backend code. This way embeddable charts also get to stay on the roadmap.
--

--
I think dynamically generating a JS file might not be very practical if we want users (Sueperset admins) to manage plugins in a future UI. I'd imagine all plugins are loaded dynamically by just checking whether a file exists in some folder. There shouldn't be the need to pre-register a chart type. You just load it as you need it.
--
",rusackas,"
--
@kristw Thanks for your thoughts on this

> Generating MainPreset.js from the configuration file could be another solution as well.

This is essentially what I'm trying to take a step toward. My understanding is that to do that, you'd have to load in an array of plugins/paths from the config (or some state driven by a future UI, even), and then you could then dynamically load those packages and/or paths. This is what that other WiP PR is taking a stab at.

> For clarification, if the bundle size is concerned, Superset is already lazy-loading the plugins via dynamic imports.

Bundle size isn't the main concern here, so much as developer convenience.

> how will dynamic import help with adding new plugins that are not listed in package.json from master?

My thought (though admittedly half baked, since we're not at this stage yet) would be that people could have any number of plugins added to their local/deployed filesystem, and dynamic loading would let them add the point to the module or just the file path of that plugin, once loaded from the aforementioned config/state. That would all be done dynamically in MainPreset.
--

--
> What do you think if the `MainPreset.js` generation happen as part of `webpack` build or some build script, so the app remains unaffected and can treat `MainPreset.js` as a regular file?

I think this is well worth a Spike, and feel like you or @ktmud are probably well qualified to advise on the webpack aspects of it in particular when we get to it.
--
",betodealmeida,"
--
I love this, specially the part about consolidating the data model in `api/v1/query` so that we don't have `metric` vs `metrics`. Having a common data model for the different plugins will make it much easier to switch visualizations without losing context, which is very powerful feature IMHO.
--
",mistercrunch,"
--
Lots of work has been done and summarized / documented here:
https://preset.io/blog/2020-07-02-hello-world/
--
"
9182,OPEN,Users can query bigquery datasets without permissions,.pinned,2020-08-28 19:46:53 +0000 UTC,micimize,In progress,,"Once a user has permissions to any bigquery dataset, they can query any other dataset superset has access to, just by using the dataset prefix. This is probably because the bigquery client jobs have no dataset restrictions https://github.com/googleapis/google-cloud-python/issues/6042",,,willbarrett,"
--
As mentioned in the linked ticket, the current correct behavior is to manage access at the Google Cloud API layer. The client library is not able to make restrictions like the one described in this ticket, thus Superset is similarly limited. Thank you for bringing this to our attention.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",micimize,"
--
This should probably be `.pinned`
--

--
The referenced `python-bigquery` issue was closed in favor of [adding conditional IAM support to bigquery](https://issuetracker.google.com/issues/143959429). Once/if that is done, I think superset could add a field to the request with a list of authenticated datasets and have a recommended IAM policy to set.
--
",,,,,,
8924,OPEN,Ability to parameterize and embed dashboards or charts into external web applications,.pinned; enhancement:request,2020-03-05 23:13:13 +0000 UTC,denkab,Opened,,"**Is your feature request related to a problem? Please describe.**
There is an existing application where addition of charts or dashboards managed in external tools (like Kibana and Tableau) is an established practice.

**Describe the solution you'd like**
I wish to be able to embed (worst case, as an iframe, but ideally, by somehow proxying content of the charts) to make those an inseparable part of another application. This would be beneficial from at least 2 perspectives: convenience and access control. This way, by parameterizing (or pre-selecting filters, either hidden or available in the slice-and-dashboard UI), users can land in an experience that is contextualized to their domain/area, but also, it can be hoped that data belonging to areas that aren't ""theirs"" can be concealed.

**Describe alternatives you've considered**
IDK, really... Forgoing the superset's UI altogether and interacting with backend does not seem too appealing.

**Additional context**
Nothing else to think of at the moment.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.96. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,,,,,
8776,OPEN,Toggle to NOT render HTML on Table viz,.security; enhancement:request,2019-12-10 06:53:24 +0000 UTC,rubypollev,In progress,,"**Is your feature request related to a problem? Please describe.**
Sometimes I'm querying data with URL params. I always get people injecting SQL or HTML in here. Not usually a problem, but the Table viz is rendering this HTML. This is a security concern. 

**Describe the solution you'd like**
I'd like to toggle this ON on a case-by-case basis in the ""customize"" tab. Might need a config flag for default behaviour, too. 

**Describe alternatives you've considered**
Maybe a feature flag to turn this off for the whole implementation? ",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.92. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",nytai,"
--
@rubypollev looks like that's likely a bug in the table implementation. Looking at the logic: https://github.com/apache-superset/superset-ui-plugins/blob/master/packages/superset-ui-legacy-plugin-chart-table/src/Table.js#L154 it seems like the intent is for all content to be DOM/HTML escaped, however it seems like it's only applying it to string columns. 

When you've experienced the issue, have people been injecting SQL and HTML in non-string fields? 
--

--
yep, looking at https://github.com/cure53/DOMPurify it seems that the content is sanitized and not escaped. 

I wonder if there's a strong case for rendering the strings as HTML in the table. Seems to be like it should default to rendering strings and render HTML based on some config (if there's a strong case for doing so -- I can't think of any right now).  
--
",rubypollev,"
--
@nytai I've only experience this with string fields. I see that string fields are sanitized, so this is less of a security issue than I thought it was. I'd still like an option to render string fields as... strings and not HTML. 
--
",mistercrunch,"
--
There are lots of use cases for allowing links in tables. You might want a ""leaderboard"" type chart that links out to pages to individual entities, or a list of ""support tickets"" that links to the ticketing system.

We've also used this feature to link to other dashboards with a preselected filter, meaning clicking on a specific country entity in a data table could open some sort of country scorecard, ...

Do we think there could be a security concern outside of a type string?
--

--
Side note, I'm not sure if I like this `<span class=""like-pre"">` wrapping, seems like it could add a lot (tens of thousands) of not super necessary dom elements just to `white-space: pre-wrap;`. We could probably just do that on `table.dataframe td` instead.
--
",,,,
8690,OPEN,"Right ""schema access on [my_pg_instance].[my_schema]"" of uploaded ""CSV to Database"" table does not appear in Rights list of roles (PostgreSQL)",.pinned; bug,2020-04-01 18:06:17 +0000 UTC,sfkeller,In progress,,"### Expected results

When importing a CSV e.g. to PostgreSQL database (e.g. ""my_pg_instance"") through menu item ""CSV to Database"" (path /csvtodatabaseview/form/ ); and when indicating a schema ""my_schema"" in field ""Schema"" (=>""Specify a schema ...""). Then when opening menu item ""List Roles"" (path /roles/list/) the right ""schema access on [my_pg_instance].[my_schema]"" should appear.

### Actual results

What actually happens is, that the newly created schema (e.g. ""my_schema"") does _not_ appear when trying to assign this schema to another role when editing a role in ""List Roles"" .

#### How to reproduce the bug

1. Go to menu ""CSV to Database"", select a PostgreSQL instance with CSV upload enabled.
2. Click on field 'Schema' and enter schema ""my_schema""
3. Go to menu ""List Roles"", edit a role and try to add ""schema access on [my_pg_instance].[my_schema]"".
4. ""schema access on [my_pg_instance].[my_schema]"" does not appear.

Issue is, that the user who just uploaded a CSV into a table can't handle rights on this table.

### Environment

- superset version: `0.34` 
- python version: `3.7` 
- node.js version: `N/A`
- npm version: `N/A`

### Checklist

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

We're struggling with a Superset Cloud Instance which is multi-tenant, i.e. where ""producers"" (roles) only see and share own databases, schemas, charts and dashboards with their ""consumers"" (roles).",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.73. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",sfkeller,"
--
Pls. pin this issue. It doesn't make much sense to label a bug as stale. 
--
",willbarrett,"
--
@rusackas @craig-rueda can you pin this?
--
",,,,
8655,OPEN,[SIP-30] Remove Tabs in SQL Lab,enhancement:request; org:preset; sip,2020-06-30 21:47:29 +0000 UTC,suddjian,In progress,,"## [SIP-30] Remove Tabs in SQL Lab

### Motivation

SQL Lab is devoting screen space to a custom tab UI that is less effective than that offered natively by the browser. Other upcoming features such as more powerful edit history/undo, and better navigation/discovery make tabs less important. The new design directions coming from the superset design group specify a tabless UI. Browser-based tabs would be a better solution for nearly every SQL Lab use case.

### Proposed Change

- Remove the tab bar in SQL Lab
- Include the id of the current `queryEditor` in the SQL Lab url: `myapp.com/superset/sqllab/42`
- Links throughout Superset referencing SQL Lab queries should use urls with a `queryEditor` id
- Implement a user-level flag to switch between the tabful and tabless UI
- Stabilize backend-managed SQL Lab state, and make it the default option.

This change can actually be made without significant changes to state management. SQL Lab state includes some number of `queryEditor`s (""tabs""). Instead of each `queryEditor` referencing an in-page tab, the SQL Lab url will contain the id of the `queryEditor` being used.

This will depend on the [backend implementation of state management for SQL Lab](https://github.com/apache/incubator-superset/pull/8060) which is currently behind a feature flag. Without backend SQL Lab state, your local state would accumulate until there is too much to manage, without a practical way to clean it besides clearing your localStorage or editing it laboriously.

One change that will be tricky is offering users a quality migration experience to the tabless UI. More details on that down the page.

#### A note on preventing lost work in a tabless UI

Nobody wants to write a query, then close their browser and lose it forever. Mitigating that in this new design will be important. The full UI/UX redesigns call for a robust history feature, but there are some things we can do to protect people's work without the history feature.

When you navigate to SQL Lab without specifying an id in the URL, an id will be generated for you just as if you had clicked the ""+ tab"" button in the current UI. This will allow users to re-open a closed tab from the browser history, even if they didn't save the query. If a user wrote a query but didn't save it, and closes the browser tab, SQLLab should also warn and offer to save the query before the tab closes. If you run or save your query, those will also be ways to recover your work.

Just for reference, here's the design we're working towards. This SIP is not about implementing everything on this screen, only replacing the in-page tabs with browser tabs.

<img width=""1360"" alt=""Screen Shot 2019-11-25 at 7 04 12 PM"" src=""https://user-images.githubusercontent.com/1858430/69596121-60293800-0fb6-11ea-9a8c-e231544eee78.png"">

### New or Changed Public Interfaces

- ""Run query in a new tab"" button in the query history will open a new browser tab.
- ""Share query"" should use the new url format, and should save the query if it hasn't been saved.
- Links in the saved queries screen and query search should use the new url format.
- Remove the ctrl-t keyboard shortcut?

### Migration Plan and Compatibility

We should allow users to migrate to the new system at their own pace. Tabs are a heavily-used feature, and should not be ripped out without a proper transition phase to educate and re-orient users.

A per-user feature flag will be added indicating whether they are using the tabful or tabless version of SQLLab. SQLLab will appear unchanged as long as a user's flag is off, with the exception of a small banner in the corner notifying the user of the new tabless experience, and allowing them to flip the flag on. Once they flip the flag SQLLab will change to the tabless UI. There should be an explanation of the change, with useful info on how to navigate this brave new world.

After some time, we can turn this flag on by default for new users. Eventually when the tabless UI is deemed stable, people are happy with the change, and we've made sure that there isn't a negative impact to existing user needs, we can switch everyone over.

### Rejected Alternatives

- Keeping in-page tabs - too cumbersome to use, not as accessible, adds unnecessary state complexity
- Switching over immediately to a tabless UI without a feature flag - too jarring to users, want to give them the opportunity to change to the new system when it's convenient for them.
- Developing the new SQL Lab designs in a separate screen alongside the original SQL Lab - too much code complexity and chance of breaking changes, not enough opportunity to reuse existing components.
- Re-organizing SQL Lab frontend state around a tabless UI - too much work for now, and breaks compatibility between the new and old version of the page. It will likely make sense to re-organize state in a future release once all users have migrated to the tabless UI.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.81. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",dpgaspar,"
--
Can you rename this to SIP-30 ?
--
",suddjian,"
--
Oops! Thanks
--

--
Well the idea here is to better support native browser tab workflows so that people can still have multiple queries open with a less complicated UI. Do you think that could be a good option for your use case? Is there specific value you see in page-based tabs that can't be done with browser tabs?
--

--
I've edited the SIP with the latest updates to this proposal. I think this addresses the concerns that have been raised.
--

--
@michellethomas 

The redesign of sqllab is being treated as an ever-evolving north star to align everyone on while developing Superset. This SIP is just about how we can go about achieving a tabless UI.

I agree that feedback from users will be highly useful in implementing the new designs, but I think that's out of scope for this SIP. There are other ongoing efforts to set up user feedback channels.

Migration is an important part of this work, since this has such a large impact on users. My proposal as outlined in the SIP is to give each user a button to switch to a tabless UI. Until the tabless UI is deemed ""stable"", there will be an option to switch back to tabs, with your tab state kept intact. Before you decide to go tabless, we can have a message describing the change to the user. We will need to explain that saving queries is the best way to avoid lost work, but you will also be able to use the sqllab history feature or your browser history to find past queries.

@robdiciuccio 

I think you're correct regarding state management. The way I'm thinking of this change is to use the existing tab ID in the same way that you're describing a unique session ID. I think migrating to UUIDs or something similar is a good idea. We can differentiate between ID types with different URL paths, perhaps `/sqllab/saved/:id` and `/sqllab/session/:id`.
--

--
Thanks for all the feedback @sylvia-tomiyama! I think I can address all of these, but let me know if I missed anything.

1) In short, yes. This is addressed in the migration section and in my other comment above.

2) A goal of allowing users to switch back is for them to be comfortable with the change. You're right that users could potentially choose not to switch, but I don't think anyone can offer an answer to that at this stage. We'll need to see what happens, gather feedback, and understand why users have made their choice before we can respond effectively. It may just be an issue of messaging, or a problem with the implementation, a missing feature people need, or something else entirely.

3) I admit that ""remove tabs"" doesn't sound like it has value for users, but nestled within that proposal is ""support web browser navigation patterns"", which offers a lot of value. Web browsers have more effective methods of organization and navigation than are available in SQL Lab. Many users we've spoken with have a knee-jerk reaction against removing tabs initially, but conclude that it is a good change after further reflection. I agree that the benefits should be demonstrated to outweigh the costs before users would be required to switch, which is the motivation for the migration strategy. The language around that could use some clarification though, so I'll work on that.

4) I think removing tabs has sufficient value on its own. Saving your query automatically may be warranted, but it may also clutter users' saved query lists with work that wasn't meant to be saved. Maybe a list view of your SQL Lab ""sessions"" could serve that purpose? An advantage of the migration strategy is that if such changes are necessary, we can build them while continuing to user-test the tabless SQL Lab in parallel.

5) I recommend saving your query if it's something you'll be working on for an extended period of time, even if we don't remove tabs. An issue with tabs is they appear to be persistent but they are actually quite brittle and unstable. As for organization, native browser tabs and windows will offer better features than the current UI does. A unique URL for each session also allows using other means of organization such as browser bookmarks, spreadsheets or other documents.

6) While this is a necessary step for the long-term plan, I also think that SQL Lab would function better without tabs than with tabs, today. Discussions regarding the long-term plan should happen in [SIP-34](https://github.com/apache/incubator-superset/issues/8976).
--

--
I want to respond to questions raised in the Superset Meetup, and clarify the scope and purpose of this proposal.

This proposal is specifically about improving user experience by moving the tab implementation from the UI to the browser, and not about changing state management - though it does depend on the state management improvements from [SIP-23](https://github.com/apache/incubator-superset/issues/7748). This should allow the same user productivity, while being a more modern and practical solution.

Browser tabs will allow for better support of upcoming features such as more powerful edit history/undo. Supporting unique URLs for each SQL Lab session will also unlock new use cases that are currently impossible. For example, if you close a tab in SQL Lab you can never get it back, but in this design, if you close a tab you simply have to get back to the same URL again.

This pattern was identified during the [redesign process](https://github.com/apache/incubator-superset/issues/8976) as a better UX pattern than the current design. Participants were generally in favor of the change due to the improved usability, stability, and cleaner UI of browser tabs, as well as some positive user feedback.

Id like to get some more votes on this SIP in the email thread. If you have not voted yet, your vote will be much appreciated.
--

--
@jiegzhan No timelines currently to my knowledge, but I'll let you know if that changes.
--
",toop,"
--
This good ideal We are looking forward to your revised Thank you very much.  @suddjian 
--
",cgivre,"
--
Personally, I do not like this as I like to have multiple queries open for reference.  This would get a -1 from me. 
--
",mistercrunch,"
--
About change management, we did something similar for both ""explore v2"" and ""dashboard v2"", where in the case of ""dashboard v2"", the dashboards had to be validated (and in some cases tweaked) manually individually.

Things can get intricate as components, endpoints and objects may become shared in intricate ways. For example, the work of deprecating ""explore v1"" became fairly intricate as it was sharing things with v2. Clearly there are pros and cons here, and if we decide to have 2 versions overlap, we should be deliberate on whether we want to go with a ""not DRY"" (where code is duplicated to evolve independently) or a tangled approach (where both versions share components and objects).

One thing to mention is that the design is not very clear around state management, and making sure people don't loose work. Personally I'd push towards a ""Google Docs""-type pattern, forcing people to title the session quickly, and autosaving as the query is executed. We want to make sure that whatever happens to the browser tab, no work will be lost. We should have clear answers to ""what happens if I have 6 tabs open, close the browser, and want to get back to some of the tabs that were opened before? what if I had not explicitly saved? is a tab really a query session? can I restore the session if I close the tab, or just the latest state?
--
"
8639,OPEN,[SIP-27] Proposal for Paranoid Deletes,enhancement:request; sip,2021-03-17 05:12:16 +0000 UTC,john-bodley,Opened,,"## [SIP] Proposal for Paranoid Deletes

### Motivation

At Airbnb we have a vast number of entities housed within Superset. Our deployment has tens of thousands of charts (both manually and procedurally generated), thousands of dashboards, and tens of thousands of registered datasources and tables (both physical and virtual). 

In a recent analysis of a specific Druid NoSQL (native) cluster, from a sample of ~ 5k charts only 34% of charts rendered, i.e., returned a 200 status code from the `/supserset/slice_json` route.

The following chart shows the renderability of charts as a function of last saved, which shows that a chart's viability often decays over time due to creep in the datasource metadata and the saved chart parameters.  

![Screen Shot 2019-11-22 at 5 50 14 PM](https://user-images.githubusercontent.com/4567245/69471248-193b0880-0d52-11ea-920a-01bea6511628.png)

Ideally we would like to have a mechanism to clean up obsolete resources (charts, dashboards, or datasources) in a somewhat paranoid manner, i.e., using soft deletes. This should help keep our deployment at a manageable size and improve the perceived reliability and quality (from a usability standpoint) of Superset assets.  

### Proposed Change

The proposed solution was originally mentioned by @etr2460 but I thought it was worthwhile formalizing this as a SIP. This borrows an idea from [Ruby](https://github.com/rubysherpas/paranoia) where we first soft delete records my marking them as deleted (with an associated timestamp) before performing a hard delete (deleting the record _n_-days later). Users could be prompted that their charts were being deleted and they can take corrective action to undelete it if they see fit.

There's actually a Python package [sqla-paranoid](https://github.com/jeanphix/sqla-paranoid) which brings transparent soft deletes to SQLAlchemy which we could use or replicate. The TL;DR is this would add a `deleted_at` (or `deleted_on` for consistency) column which would track soft deleted records. Records which are soft deleted wouldn't show up in the CRUD views by default unless the filter was enabled (not unlike how SQL Lab Views are ignored by default in the `tablemodelview`).   

Records could be marked using a hook, trigger, or cron as deletable based on various criterion using cascading context:

#### Charts 

- Consistently returns an error. 
- Consistently returns no data.
- Has not been viewed for _n_-days.

 Note this could leverage a cron (or similar) and be based on customizable rules, i.e., _x_ of the last _n_-days.

#### Dashboards

- Contains no charts. 

##### Tables/Datasources 

- Not referenced by any charts.

### New or Changed Public Interfaces

We would need to updated the data model and leverage `sql-paranoid` (or similar) for enabling the soft-deletes. We would also need to update the FAB views to handle filtering/exclusion of soft deleted records. Finally we would need to implement triggers or similar to i) soft delete records, and ii) hard delete records. 

### New dependencies

The only new dependency would be `sqla-paranoid` (no public license) if we decided not to write this ourself. Note the package only contains several hundred lines of codes. 

### Migration Plan and Compatibility

We would need to update the schema to include the `deleted_at` (or `deleted_on`) column for certain tables. Note I think we only need this for charts, dashboards, and datasources (the cascade deletes should handle the cleanup of columns and metrics). 

### Rejected Alternatives

None. 

to: @etr2460 @mistercrunch @villebro @willbarrett 
cc: @vylc ",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.71. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",willbarrett,"
--
@john-bodley are you thinking that the cascade deletes will also be soft/paranoid? In general I would be very in favor of moving to soft-deletes.
--
",john,"
--
@willbarrett I think it depends on the entity type. For charts, dashboards, and datasources I think these should be soft-deletes, however cascade deletes are valid for removing columns/metrics once/if the datasource/table is hard deleted.

I think there's merit in enumerating which tables should support soft and cascading deletes. 
--
",bkyryliuk,"
--
+1 for this proposal, we are thinking of implementing this @ dropbox as well either through APIs & some heuristics. It would be great to have 1st class support for this in Superset
--

--
+1 to @etr2460 point

time to live should be configurable in the superset config or per database and yeah it should be set by admins

As for implementation, it is unlikely that we would be able to commit to it in the next couple quarters.
--
",junlincc,"
--
+1 for this proposal 
We added this request to roadmap lately, as we consider this feature as _must have_ for an enterprise software.  this sip seems to be the perfect solution! Please help me understand a couple of things~ 
1. how long can a soft deleted object be stored until it get premaritally deleted? 
2. can user set the time for hard delete? 

will either of you be interested in collaborating on this feature? @john-bodley @bkyryliuk? 
--

--
thanks both! @etr2460 and @bkyryliuk 
To summarize 
- Superset currently only supports hard delete entities, which needs to be changed to soft delete 
- Entity include datasets, charts, dashboards and saved queries
- Soft deleted records will not appear in search results or in list view, but they are not permanently deleted within a set period of time (time to live) 
- Only system admin can set the _time to live_ ,at database level, for the users 
- Soft deleted records are temporary stored in a trash can 
- Soft deleted records can be restored by both admin and users from the trash can

I'm not very familiar with the role permission in Superset tbh. By reading the documentation, seems like we should change the logic of can_delete, and add something like can_restore? 

_Model & Action: models are entities like Dashboard, Slice, or User. Each model has a fixed set of permissions, like can_edit, can_show, can_delete, can_list, can_add, and so on. For example, you can allow a user to delete dashboards by adding can_delete on Dashboard entity to a role and granting this user that role._

Please educate me or lmk if I missed anything 
@amitmiran137 thoughts? 

--
",etr2460,"
--
Replying here @junlincc (although i'm not John or Bogdan  ):

I think the thought is to have the time between soft delete and hard delete be configured by the Superset admin.

Note that the discussion here is more from the admin perspective than the user perspective. From the user's viewpoint, once something is soft deleted, they won't be able to find it in the Superset application (or maybe will only be able to find it in a trash can page). But if it's right after the deletion occurs, they could talk to the admin or self serve recover it from the trash can. But once the time between soft and hard delete expires, the entity is gone for good
--
"
8574,OPEN,[SIP-26] Proposal for Implementing Connection Pooling for Analytics Database Connections,data:connect:suggest; enhancement:request; sip,2021-01-02 21:42:32 +0000 UTC,willbarrett,In progress,,"## [SIP] Proposal for Implementing Connection Pooling for Analytics Database Connections

### Motivation

Currently, Supersets connections to analytics databases do not have long-lived connection pools. In most instances, a database connection is spawned immediately before a query is executed and discarded after a single use. This introduces a small amount of latency into every query. While most queries run against data warehouses are expected to be longer-running than a typical web application query, this latency will be noticeable when performing operations such as loading schema and table lists for display in the UI, or loading table definitions and previews. 
A more serious concern is that the number of open database connections to analytics databases is only bounded by the number of threads available to the application across all processes. Under peak load, this can lead to hammering databases with a large number of connection requests and queries simultaneously. This does not allow us to provide meaningful upper bounds for the number of available database connections. Implementing connection pooling at the process level will allow us to provide a configurable maximum number of connections that Superset is able to leverage.

### Proposed Change

I recommend we add a singleton object to hold a SQLAlchemy Engine instance for each configured database in the application. I believe that engines should not be instantiated on startup, but instead instantiated on first use to avoid unnecessary connection negotiation.

I further recommend that we use the [SQLAlchemy QueuePool](https://docs.sqlalchemy.org/en/13/core/pooling.html#sqlalchemy.pool.QueuePool) as the default pool implementation while retaining the ability to configure Superset to use a NullPool, configurable via the Database setup system. I would like to make the `pool_size` and `max_overflow` properties configurable, as well as whether to treat the queue as FIFO or LIFO and the `pool_pre_ping` option, and customization of the `connect_args` passed on engine instantiation (which controls things like connection timeouts). I believe that LIFO queues will be preferable for infrequently-accessed database connections, as they will generally maintain a lower number of connections in the pool, and thus should be the default. I would also recommend that for LIFO queues we default to the `pool_pre_ping` option to trigger pool member invalidation when necessary, as stale connections are more likely under the LIFO configuration.

As part of this work, I recommend moving engine instantiation code out of the Database model and into its own module, probably as part of the singleton that will maintain an in-memory list of database pools. We will need to update the code that alters database records to reinitialize the processes engine after Database record creation and update.

One further change will be in regards to Celerys connection pooling. Right now, we use the NullPool in Celery and instantiate database connections when needed. For Celery, I would recommend moving to the StaticPool, which will create one database connection per worker process. Because Celery reuses worker processes, this will reduce the overhead on backgrounded queries. An alternative would be to move to threaded workers (gevent or eventlet) and maintain the same pool configuration as the UI. Id love suggestions from the community on what to recommend here.

### New or Changed Public Interfaces

This change should have minimal impact on the UI, the primary change being the addition of more configuration options in the Databases section. I would recommend having sensible defaults and hiding the pool setup under an `Advanced` configuration section. I plan to provide guidance on the meaning of the `pool_size`, `max_overflow`, and FIFO vs LIFO configuration parameters, both in the UI and in new documentation. The configuration approach will be hybrid, allowing global configuration of defaults in `config.py`, with overrides available on a per-database basis in the UI.

### New dependencies

No additional dependencies will be necessary.

### Migration Plan and Compatibility

A database migration will be necessary to add an additional field to the DBs table to hold connection pooling arguments.

No URLs will change as part of this work. I would like feedback from the community, particularly engineers at Airbnb, Lyft, and other organizations with large Superset installs, on what sensible defaults for connection pools would look like.

### Rejected Alternatives

The primary alternative rejected is the current, connection-pool-less state. While this state allows for only the number of connections needed at any given time to be in use, it falls down with regards to performance and predictability of number of open connections at any given time.

I also considered the other connection pool implementations in SQLAlchemy, but it appears that our use-case is best served by the QueuePool implementation.

One additional piece I considered was providing an option to the user of configuring an overall, rather than per-process, maximum number of connections. In that case, processes would need to check out the ability to make a connection from a distributed lock built in Redis, or the max size would need to be large enough to provide at least one connection per live process. While I think this would be a better experience for most users, Im concerned about the additional application complexity required by such a change. Would processes need to register themselves in Redis on boot so we could get a correct count of the number of live processes? What happens when we need to scale up beyond the global maximum number of database connections? I think solving those problems is not easy, and most use-cases will be well-enough served by a per-process max number of connections.

",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.96. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",mistercrunch,"
--
There's a challenge here around the fact that each subprocess (gunicorn worker and celery worker) gets its own pool, and that each one of those can connect to multiple databases. Depending on whether you configure these things to use threads or workers (subprocesses), you can end up with a lot of connections very quickly that is vastly bigger than the number of **active** connections. One problem is that while you probably want to cap the number of connections to your db, you want do have a dynamic number of workers as you need more capacity. 

There's also a challenge around the fact that threading and SQLAlchemy pools have intricate issues. There are endless stackoverflows documenting this.

Another thought is that we may want to limit concurrency to analytics databases, but this approach is not achieving that in any way as there's no global state shared across server/workers. This would need to get handled as some sort of environment global variable (redis?) that would keep track of the number of active connections.

What's the cost of spawning / destroying a connection? On the server / on the client? Milliseconds of CPU time? Worth it?
--

--
Another thing to cover as part of this sip is the configurability of the pools per database. Potentially heterogenous params based on pool type, it's hard to come up with a balance between something comprehensive and static VS flexible. We could have both: maybe different presets in a dropdown list, and the possibity to override with actual pool objects in a dict in superset_config.py ...

Planning on making another comment addressing your points above.
--

--
I think allowing for people to configure their connection pool is a great thing, let's provide ways to do this. I see two main approaches as well as a hybrid.

**1 - by config:** Doing it as configuration, in `superset_config.py`. If going that route,  I think it's best to give user the full power of configuration as code, and allow them to pass an instantiated `SQLAlchemy` pool object. This enables them to use whatever pool class they see fit, and use whatever parameter they want. In theory they could even create their own pool class here and do as they see fit. This should probably be a `dict` keyed by `database_name` using instantiated pool objects as values.

**2 - in UI** Doing it in the UI, and expose only the common pool type(s) and parameters that matter. This is likely to be simple at first (only allow `QueuePool` and common parameters like `pool_size, max_overflow, timeout, use_lifo`). An alternative and less form-heavy approach would be to use the current `Extra` json approach and add a key for `queue_pool_parameters` and document how people can pass things like `pool_size, max_overflow, timeout, use_lifo`.

**3 - hybrid** A hybrid approach could be easy to implement to, where we look for the dict config and fall back on the UI config.

For reference, where `Extra` looks like today:
<img width=""1183"" alt=""Screen Shot 2020-01-07 at 9 39 57 AM"" src=""https://user-images.githubusercontent.com/487433/71915961-b7800380-3131-11ea-94f4-97b04336223c.png"">

--

--
I think it should be possible to build a SQLAlchemy-compatible pool called `CappedDistributedPool` that'd leverage Redis or something else (as long as it's not Zookeeper :) to limit concurrency, and hook it in using the configuration hook.
--

--
I think priority #1 is pool ""configurability"" per database (achieved with a `superset_config` dict hook), and #2 is some way to do this in the UI (important for Preset until we have some sort of per-tenant configuration hook).

I'd wait for a direct request prior to actually building a `CappedDistributedPool` POC and assume that it'd be possible as it becomes needed, building on top of #1 and #2.
--

--
I don't feel strongly about adding a column or adding on to `extra`, but I think using `extra` here for this long tail of semi-structured configurability is *ok* given the use case. The ""proper"" 3NF model that would support all the options would be super unmanageable. 

Hybrid data models (mixing structured and semi-structured fields) are becoming more common over time, database support them much better than they used to. That doesn't mean that's right, but clearly reflecting a reality of rapidly-evolving / complex schemas.
--

--
[tangential to this SIP and related to @etr2460 's comment] about Celery connection pooling to the metadata database, it seems reasonable to think that the connection requirements in the Celery context are different from the typical web-server-building-a-page use case, and it'd be good to offer configurability that is context aware (celery / web). Personally I don't think it requires a SIP though if it's just a configuration hook.
--
",willbarrett,"
--
Yes, there is a challenge around each process getting its own pool. I see your point that for some systems (Redshift is getting my side-eye here) having connections held open could be a larger problem. I'll amend the recommendation above to make one of the options a NullPool. This would retain the existing behavior for databases that are unable to handle enough open connections.

RE: the cost of spawning/destroying the connection, I think it's impossible to come up with a really solid specific number. I think the range is likely to be between around 10 milliseconds in the case where the server is a more application-focused one (Postgres, MySQL, etc.) living on the same network up to potentially multiple seconds for systems separated geographically or with a chattier protocol for initiating a database connection. Under load, these numbers can get quite large.

A goal down the line would be to limit total connections, but I'd like to push that off into a future SIP. I believe a reasonable way to attack that would be to implement a custom pool that leverages Redis as a distributed lock for the system. The Redis lookups for this system will potentially add a fair amount of latency, so that's something we should discuss separately in my mind.

RE: the intricacies of SQLAlchemy in a threaded environment, it appears that connection pools and engines are safe, but that sessions and connections are not. This makes sense intellectually - the connection pool and engine are designed to protect the non-thread-safe resources they contain. None of this is safe across a process boundary, so the multiprocessing module in Python is a danger to connection pools. We already have this issue when it comes to the metadata database. Post-fork, any existing connection pools would need to be recreated. Some database engines implement database connections as blocking calls from my research, which will break multithreading due to the Global Interpreter Lock. I think for us to really achieve the best throughput we will want to use lazy-loaded connections from process-based Celery workers that then become long-running connections. This, however, is multiple SIPs away, and I anticipate that we will need to retain the ability to run all queries in the foreground for the foreseeable future.

RE: is it worth it? I think that depends heavily on the workload. In terms of freeing up processor cycles on the web server, it could be very worth it. If there is a substantial geographical separation between Superset and the database accepting the connection, or if connections are slow to instantiate on that server, it will definitely be very worth it. I think providing the option of connection pooling could greatly accelerate certain workloads, though you have convinced me that retaining the option of a NullPool is a wise choice.
--

--
I like the hybrid approach as well. I'm not excited about adding this configuration to though `extra` - I think it would deserve its own database columns, especially considering the table holding these records is unlikely to be massive, thus making migrations manageable. Unstructured data in an RDBMS is a pretty strong antipattern that I'd like to avoid exacerbating in this case.

I think there is a valid use-case for un-capped pools, especially when the database connections are to datastores like BigQuery and Athena. Anywhere that supports massively concurrent access. Capped pools are more important for systems like Redshift, Postgres, MySQL, etc. where too many connections open can cause difficulty. I can put together a proof of concept of a Redis-lock-based, capped, distributed pool and we can take a look. I expect to be able to get to it in a couple of days.
--

--
I'm willing to compromise on the unstructured column. I've updated the SIP to reflect this discussion.
--

--
Some answers/responses for @etr2460 

This SIP is for connection pooling for analytical databases, not the metadata database. I'd be happy to talk with you to understand your metadata DB issues, but do not want to consider that problem as part of this SIP.

I think the most reasonable behavior for a sync query that cannot check out a connection would be to block and wait for a connection to be available, with the failure state being an eventual timeout, but this isn't a strong opinion. I'd be interested in other's thoughts on the matter.

I understand the desire for a `CappedDistributedPool`, but want to treat that as a separate piece of work. The work proposed in this SIP would be a precondition for creating a `CappedDistributedPool`, so we'd be moving in the right direction.
--

--
@villebro completely agreed RE: connection pooling everywhere being desirable. Given the structure of the system though, pooling for the metadata database is a special case. Connection pooling should already be in use for foreground connections, but Celery presents special concerns, which is why I'd like to treat it separately. We definitely share the same end goal though!
--
",suddjian,"
--
A global redis-based pool would be very useful from a user perspective. Doing that in a future SIP makes a lot of sense though, since right now there is no connection limit at all.

I suggest avoiding offering any configuration in the UI until then, as a config intended to be applied per-process would be difficult to reliably translate to a global config.
--
",villebro,"
--
Personally I like the idea of supporting a hybrid approach, giving precedence to the code based config. However, given that Superset usually runs on multiple concurrent worker processes, I think the only way of achieving true pooling would require some sort of locking outside python scope (Redis being the top contender as mentioned above). While it does propose it's own set of challenges (not to mention added complexity), somehow it feels simple enough to be manageable, especially if it can be rolled out as an opt-in feature. Therefore I'd vote to at least try building a Redis locking POC, as it should be pretty quick to put together and see what type of overhead or other problems it might introduce.
--

--
I like the idea of being able to use connection pooling wherever necessary, even for metadata connections if necessary.
--
",metaperl,"
--
> I think priority #1 is pool ""configurability"" per database (achieved with a `superset_config` dict hook),

Related: https://github.com/apache/incubator-superset/issues/9029
--
"
8452,OPEN,[SIP-25] Proposal for text based dashboard filters,enhancement:request; sip,2019-12-02 00:54:42 +0000 UTC,stswn,In progress,,"## [SIP-25] Proposal for text based dashboard filters

### Motivation

In our projects analysts need to browse through large dataset (7M rows). They often need to search for rows by values which are unique or have very large number of unique values. Current implementation of dashboard filters is based on limited number of value buckets and does not allow them to do this.

### Proposed Change

Add new visualization type - 'Search Box' which would allow to specify filter values by free typing.

New chart type should at least enable filtering with '==' operator. Additionally every defined filter could allow for different filtering operators to be used:

For text columns:
- contains => `column LIKE '%value%'`
- does not contain => `column NOT LIKE '%value%'`
- starts with => `column LIKE '%value'`
- ends with => `column LIKE 'value%'`
- is empty => `column IS NULL`
- is not empty => `column IS NOT NULL`

For number columns: arithmetical comparison operators.

Similar functionality is already present in chart query configuration section. We would like to see similar functionality on dashboard level available to the end users of the dashboards.

New visualization would not need to perform any queries to render. It would be based on table metadata only.

### New or Changed Public Interfaces

Base version of this improvement containing only ability to filter by exact value(`==`) requires only new visualization to be prepared. It would emit filters in the same way that 'Filter Box' and 'Table' charts do.

Extended version requires changes in how filter state is stored on Dashboard level - it should contain information about used comparison operator. Also new addFilter action should be added and `getEffectiveExtraFilters` function should be extended.

### New dependencies

None

### Migration Plan and Compatibility

n/a

### Rejected Alternatives

Existing 'Filter Box' visualization could be extended to allow for free input, but it is probably better to separate slice which performs query from one which can be rendered without fetching any data.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.96. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stswn,"
--
I would like to prepare a sample WIP PR for this. Should code for new search/filter visualization be placed in [superset-ui-plugins](https://github.com/apache-superset/superset-ui-plugins) or here?
--

--
@betodealmeida I was thinking about it but I wanted to narrow the scope of this SIP.
I have seen #5269 and #5925 and this efforts to add native dashboard components seemed abandoned... If there is a consensus that filters should be native components in the future, I can try to prepare this as a component instead of chart.
--
",betodealmeida,"
--
@swian did you consider doing this as an native dashboard component, instead of a new viz type?
--

--
@swian my understanding is that having filters as native components was the direction we wanted to go. Having them as visualization types has always been a hack that works well for most of the common cases, but the abstraction breaks when you try to push it further.

@mistercrunch @graceguo-supercat do you have any thoughts on this?
--
",jackyq2015,"
--
@swian Any progress on this?

Perhaps this ""text based filter"" can be merged into the current ""filter box"" since their similarity.  The main difference is that the former filter will be applied to wide like operation but the later filter is only applied to an enumeration which is supposed to have limited candidate.
--
",,,,
8318,OPEN,[SIP-24] Proposal to introduce Flask app factory pattern,enhancement:request; org:preset; sip,2020-09-03 03:21:59 +0000 UTC,craig-rueda,Opened,,"### Motivation

Supersets singletons are currently configured in the `superset` packages `__init__.py` python module which means that the app itself (Superset) cannot be configured any other way than is described in the root package. The act of simply importing `superset` for other purposes, such as the `cli` forces the entire app to load, even though all you might need is SqlAlchemy, or some other component. A better approach is to leverage a pattern such as the Factory Pattern in which one or more functions can compose several components into a single app.  Whats more is that certain portions of the app might need to be overridden in test, or for different use cases. Making the app more composable makes it easier to alter functionality as needed and customize.

### Proposed Change

In order to get this done, we will need to add a few new modules, clean up `superset.__init__.py` and update the way views are added to FAB. This change is largely a refactor, with no change to functionality.

We should perform this migration in a few phases (likely several PRs):
	1. Migrate all current direct references to `app` to leverage `flask.current_app` proxy
	2. Introduce a new `extensions.py` in the base of the `superset` package whose job it will be to instantiate the Flask app, and other singletons, such as AppBuilder.
	3. Move all logic currently in `superset.__init__.py` into a new `app.py` file which will define a `create_app()` function whose responsibility will be to call `init_app()` on the Flask app, along with other flask app aware objects
	4. Move all calls to `appbuilder.add_view_xxx()` to `app.py`

### New or Changed Public Interfaces

The biggest change will be the removal of `superset.app` from the global scope. Instead, we will leverage Flasks `current_app` proxy in order to reference the current running app. There will be no changes to the set of dependent libraries.

### New dependencies

n/a

### Migration Plan and Compatibility

Documentation will likely need to be added which describes the ""new way"" of doing things once this is merged into `master`. Contributors that are accustomed to the current structure will likely need to spend a little time coming up to speed with the location of various parts.

### Rejected Alternatives

n/a
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.65. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",mistercrunch,"
--
+1!

Note: FAB still has some intricacies around using the factory pattern, for instance, as single security manager can be used across all generated apps.

Based on SIP-0, I think you're supposed to start a `[DISCUSS]` thread on the mailing list that points to this issue

Also I think this should help with the looming circular dependencies we've been dealing with.

--

--
Quick note to say that we'll need to remove all module scoped configuration references like this one:
https://github.com/apache/incubator-superset/blob/master/superset/views/schedules.py#L294
--

--
I feel like we've made a lot of progress in this direction. Can we identify the gap here?
--
",dpgaspar,"
--
+1

This would be an awesome improvement. FAB supports loading external security managers defined on the config, yet, any blockers found I'm happy to help.
--
",willbarrett,"
--
I like this plan. I do wonder if we should get some eyes from someone like @betodealmeida or other contributors from non-Preset organizations?
--
",betodealmeida,"
--
@willbarrett I'm +1 on this. I think it would really simplify development and debugging, and I see no downsides to it.
--
",craig,"
--
I opened a super early WIP PR (#8418) so I could share my current direction with @john-bodley . Right now, it's looking like it will be a bit more difficult than I had perviously thought to break things into multiple ""smaller"" chunks.

The biggest issue I've come across is needing to change the way people start up Superset from a Flask point of view, from 
```
FLASK_APP=""superset.app""
```
to 
```
FLASK_APP=""superset.app:create_app()""
```
--

--
Just more chipping away is needed. We're currently in a spot where we can start migrating away from referring to global extensions in various layers (ie models). 
--
"
8241,OPEN,Superset fails to build JSON (specifically with producing label colors) for very large sets of data,.pinned; bug,2019-09-18 18:22:27 +0000 UTC,nmits,In progress,,"A clear and concise description of what the bug is.

### Expected results

Produces UI for large quantity of elements

### Actual results

Superset throws 500 error and UI becomes unusable for listing dashboards or viewing affected dashboard

#### How to reproduce the bug

1. Create a pie chart with large limit for rows (Enough that string length of Parameters, specifically with label colors > 65530 characters)
2. Set a color scheme (To generate Label Colors dynamically)
3. Save in Superset UI
4. 500 Error on listing dashboards or viewing affected dashboard

Error is due to String length of Parameters exceeding 65530 characters (2^16) and being truncated therefore invalid JSON leading to JSON deserialize error

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Add any other context about the problem here.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.90. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",mistercrunch,"
--
@khtruong I think this is related to the dashboard color feature you added, should we limit the payload there? I noticed charts getting saved with large payloads too related to this
--
",nmits,"
--
I don't see a reason for it to build lists of colors over a certain size, as specifying colors for that many things intentionally would seem unlikely as the list would be over ~2500 items (Assuming `"".{10}"": ""#000000"",` so 24 character average size, 65530/24 = 2730.416) and each slice would be 0.15 degrees
--
",,,,,,
8230,OPEN,Deck GL Screengrid overlay does not update,.pinned; bug; good first issue; viz:chart-deck.gl,2021-03-31 06:17:24 +0000 UTC,micimize,In progress,,"Probably root cause of #5470 - The deck gl screengrid overlay displays the same values for the same locations no matter how you pan and zoom the map

### Expected results
Popup data corresponds to underlying map data

### Actual results
The map renders with the popups misaligned (seems like largest point is in the upper left) and the overlays don't change

#### Screenshots
<img width=""940"" alt=""Screen Shot 2019-09-16 at 12 23 05"" src=""https://user-images.githubusercontent.com/8343799/64979291-094c0780-d87d-11e9-905e-f1df208fb823.png"">

With the following **Javascript tooltip generator** we can illustrate the problem:
`(o) => console.log(o) || JSON.stringify([o.object.position, o.lngLat]);`

<img width=""481"" alt=""Screen Shot 2019-09-16 at 12 17 13"" src=""https://user-images.githubusercontent.com/8343799/64979007-63000200-d87c-11e9-8c19-e405e21783d1.png"">
<img width=""473"" alt=""Screen Shot 2019-09-16 at 12 17 40"" src=""https://user-images.githubusercontent.com/8343799/64979008-63000200-d87c-11e9-8592-ffea250813c4.png"">

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Create a screengrid visualization
2. Pan and zoom the map

### Environment

(please complete the following information):
- the [`amancevice/superset:edge`](https://hub.docker.com/layers/amancevice/superset/edge/images/sha256-a6ba4bac37c80366c910284a93c6e37bd476c2c61f7bf2aae861ba410d7c79aa) container
- superset version: `Superset 0.999.0dev` (post `0.34.0`)
- python version: `Python 3.6.9`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [X] I have reproduced the issue with at least the latest released version of superset.
- [X] I have checked the issue tracker for the same issue and I haven't found one similar (there are, but they were stale-botted
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.97. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",micimize,"
--
worth noting that I've observed this behavior on versions as early as `0.28`. [This deck.gl](https://github.com/uber/deck.gl/issues/1880) issue is possibly related
--

--
This should be `.pinned`
--
",aboganas,"
--
Also on Screen Grid Play no longer works
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,
8167,OPEN,PUT requests to /tablecolumninlineview/api/update/<id> fail,.pinned; bug,2020-05-27 12:09:38 +0000 UTC,etr2460,In progress,,"Making a PUT request to the tablecolumninlineview api fails when not all required fields on the model are provided.

### Expected results

Expect a 200 when providing a payload of `{""description"": description}`.

### Actual results

Get a 500
`{""error_details"":{""column_name"":[""This field is required.""]},""message"":""Validation error"",""severity"":""warning""}`

#### How to reproduce the bug

1. Create a dimension in superset
2. Hit the API to change the dimension description
3. Get an error

Note that when I include the `column_name` field, this doesn't fail. It seems like there's an issue with merging the payload with the existing object, perhaps with the model or in FAB. I would expect to not need the `column_name` field when making a PUT request if it already exists on the existing object

### Environment

Repros on superset master, up to date versions of python and js

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

cc @john-bodley @dpgaspar 
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.93. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",etr2460,"
--
@dpgaspar do you have any thoughts here?
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",zengqinchris,"
--
 superset API     export_dashboards
--
",,,,
8022,OPEN,deck.gl Screen Grid doesn't display colour selector,cant-reproduce; good first issue; viz:chart-deck.gl,2021-03-31 07:13:36 +0000 UTC,fzzylogic,In progress,,"For current master branch (2019-08-10):

""deck.gl Screen Grid"" doesn't display colour selector or grid size. ""Switching to deck.gl Grid"" shows the colour selector and grid size and setting the colour there, affects the colour displayed when switching back to deck.gl Screen Grid.

### Expected results

Colour selector and grid size controls should display on deck.gl Screen Grid, not just Grid. Not sure if colour selector is supposed to display on Grid, as it doesn't seem to work for Grid.

### Actual results

As described above.

#### Screenshots

If applicable, add screenshots to help explain your problem.

#### How to reproduce the bug

1. Make a new chart of type deck.gl Screen Grid.
2. See error

### Environment

(please complete the following information):

- superset version: `0.999.0dev` as shown in packages.json
- python version: `3.7.2`
- node.js version: `v12.8.0`
- npm version: `6.10.3`

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

### Additional context

Loving Superset ^^
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.94. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",fzzylogic,"
--
@stale It's only stale if it's been fixed ^^
--

--
@Stale It's only stale if it's been fixed ^^
--

--
@mistercrunch I got Screen Grid and Grid confused sorry for the mixup. Screen Grid works correctly. At https://demo.superset.cloud/chart/list/ open 'Grid' and expand 'Map'. It has 'Fixed Color' in there too. Adjusting it makes no difference to Grid, but if one subsequently switches 'Visualisation Type' to 'Deck.gl - Screen Grid', the color will have been set there. Thanks. (and clearly i didn't see the Customize tab on Screen Grid either ^^)

![image](https://user-images.githubusercontent.com/8237722/70802976-4a10d980-1df6-11ea-9207-64bc1e6f2439.png)

--

--
@Stale It's only stale if it's been fixed ^^
--

--
@Stale It's only stale if it's been fixed ^^
--

--
'Fixed color' still appears in two places: 1) Deck.gl Grid under the Map accordion and 2) Deck.gl Screen Grid in the Visual properties tab. Changing it under Grid has no effect on the map. Switching visualization type to Screen Grid, shows whatever color was selected under Grid and the color is shown on the map. Superset is fantastic btw.
--

--
@mistercrunch Screenshots may help.

![grid_colour_change_does_not_work](https://user-images.githubusercontent.com/8237722/84494425-72fe2280-ace4-11ea-9d02-35a08dfbd233.png)

![screen_grid_colour_change_works](https://user-images.githubusercontent.com/8237722/84494459-814c3e80-ace4-11ea-8714-65cab5d60eed.png)


--

--
https://demo.superset.cloud/chart/list/ open 'Grid' and expand 'Map' -> fixed colour still present, see screenshots above.
--

--
@junlincc Thanks for the reply! This is not a big issue, just kept answering stale bot because it's still a thing. Fwiw, the screenshots are from the [online demo](https://demo.superset.cloud/chart/list/), not sure what version it is. On the demo site, open ""deck.gl Demo"". Then for the ""Grid"", click the ellipsis to the right and ""Explore chart"". As in the screenshot, under the ""Map"" section, there is a ""Fixed Color"" option. Changing it has no effect. That's all, thanks ^^.
--
",mistercrunch,"
--
Do you not see it under the ""Customize"" tab?
<img width=""1168"" alt=""Screen Shot 2019-12-09 at 10 28 12 PM"" src=""https://user-images.githubusercontent.com/487433/70501558-36eaca80-1ad3-11ea-9664-fc85aa64b9f3.png"">

--
",junlincc,"
--
@fzzylogic looks like you are on a very old version... i doubt it has been fixed in master but please try..unlikely we will push a fix for deck.gl chart, but if you can open a PR, we will help get it through:) 
--
",,
7958,OPEN,Timezone offset doesn't work.,.pinned; bug; global:timezone; good first issue,2021-03-31 04:47:29 +0000 UTC,maltitco,In progress,,"The ""Timezone offset"" doesn't work for the postergsql database.
I am trying to display a list of records for my local time, despite setting the offset to 2 hours I still have UTC time.

I tested two ways to install the superset: using Docker and local development installation.

Edit:
I also tested this for example data that can be initiated by installing a superset.
Timezone offset setting for the flight table also does not shift time from UTC to my local.",,,ericandrewmeadows,"
--
@maltitco - which version are you running?  Have you been able to test this on v0.33.0rc1?
--
",viveksatasiya,"
--
I have also found same issue. I have configured time offset into data source configuration to 11(Australia/Melbourne). After configuration, still I am not able to get the data according to the time offset. I am using druid cluster.
--

--
@maltitco how to do it with the druid cluster?
--
",maltitco,"
--
@ericandrewmeadows version from git in file superset/assets/package.json is ""0.999.0dev"" 
@viveksatasiya I am using the postgresql database and as a workaround, I set my time zone for the user connecting to the database
--

--
@viveksatasiya i don't use druid cluster.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",TheRum,"
--
Any Update on this !
--
",jiade52000,"
--
update! I have found the same issue. T.T
--
"
7822,OPEN,Line chart tooltips broken on Firefox,.pinned; browser:firefox; bug; good first issue; viz:chart-line,2021-03-31 04:48:06 +0000 UTC,CoryChaplin,In progress,,"A clear and concise description of what the bug is.

### Expected results

Hover point on the line, see a tooltip.

### Actual results

No visible tooltip on Firefox (but it works on Chrome).

#### Screenshots

<img width=""1063"" alt=""Capture decran 2019-07-04 a 17 18 16"" src=""https://user-images.githubusercontent.com/5139669/60676736-eca85200-9e7f-11e9-9062-43b4b9bded9c.png"">


#### How to reproduce the bug

1. Go to Explore view or a Dashboard with a line chart
2. Hover a point on the line
3. See no tooltip

### Environment

(please complete the following information):

- superset version: 0.33.0.rc1
- python version: 3.6
- node.js version: 12.5.0
- npm version: 6.1.0

### Checklist

Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.


",,,issue,"
--
Issue-Label Bot is automatically applying the label `#bug` to this issue, with a confidence of 0.99. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",schoel,"
--
I am seeing the same here. This is due to #7102, more precisely the style for the body element in ` superset/assets/stylesheets/less/index.less`.

The `position: absolute` setting on the `body` element causes Firefox to calculate the height (and consequently the `clientHeight`) of the `html` element as 0 as absolutely positioned elements have no influence on their parent's height. But NVD3 uses `document.documentElement.clientHeight` (`documentElement` is the `html` element) in the formula for calculating the position of the tooltip. Since it is always `0` on Firefox the calculation produces a grossly false result and places the tooltip off-screen.

My understanding of the change from the pull request above is that is meant to ensure that the body will always cover the entire window. A similar effect should be achievable with something like the following while avoiding the problem described above:

```less
html {
  height: 100%;
}

body {
  min-height: 100%;
  display: flex;
  flex-direction: column;
}
```
--

--
Well, frankly I wouldn't bet on this being the one and only problem with tooltips or NVD3 on Firefox or anywhere. Anyway, I wasn't seeing any tooltips whatsoever in Firefox and with that patch applied I have them back.
--

--
So, if the fix has been reverted, this needs to be re-opened, I suppose. I can't find how to do that, though.

Although I do not currently have the time to investigate any further, I'd like to add a few observations:

* Embedded charts work just fine in Firefox with the fix (#7929). Chrome exhibits the behaviour described in the revert (#8147).
* With the fix, in Chrome, the `#chart-container` div is missing entirely from the embedded chart, so it looks like something is going badly wrong in the process of creating the chart. No exceptions are ever logged to the console, though.
* With the fix, Chrome does display the chart if you remove the `standalone=true` query  although this is not what you would want to have displayed when embedding things
* Embedded dashboards are *not* affected
--
",etr2460,"
--
@schoel-bis Are you sure this is the issue here? I've been investigating this same issue and am pretty sure the root cause is here: https://github.com/apache-superset/superset-ui-plugins/blob/master/packages/superset-ui-legacy-preset-chart-nvd3/src/NVD3Vis.js#L1081

We're removing all tooltips on the page whenever we rerender a chart, which is problematic especially on dashboards where multiple charts could have tooltips in the DOM. If your fix solves the issue here, than awesome, but I think there's another deeper problem that needs to be solved with the nvd3 chart plugin
--
",CoryChaplin,"
--
@xtinec maybe you can confirm this as a side effect of your commit ?
--
",graceguo,"
--
@CoryChaplin This PR caused an issue: the standalone mode chart didn't show anymore. please see #8147  for details. We have reverted this PR from master branch.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
"
7748,OPEN,[SIP-23] Move SQL Lab storage out of browser localStorage,enhancement:request; sip,2019-07-12 19:35:03 +0000 UTC,betodealmeida,In progress,,"## [SIP] Proposal for moving SQL Lab storage out of browser localStorage

### Motivation

Currently, we store SQL Lab state in the browser `localStorage`, including tabs, their queries and results. The redux state is persisted to `localStorage` using the `redux-localstorage` library.

While the implementation is clean, it provides a few drawbacks:

- The **state is not preserved across browsers**, of if the user clears the application data in the browser.
- Upgrades might leave the **state in a bad shape**, preventing SQL Lab from working successfully. We observed this a few times at Lyft, and users would use incognito mode until we instructed them to delete the application data.
- Storage is **limited to 5 MB**, hardcoded on the browser.

At Lyft we're currently migrating from BigQuery to Superset, and the project requires querying tables with nested fields (see the work on https://github.com/apache/incubator-superset/pull/7625, https://github.com/apache/incubator-superset/pull/7627 and https://github.com/apache/incubator-superset/pull/7693). Here's what we see when querying the first 100 rows from one of our tables:

<img width=""445"" alt=""Screen Shot 2019-06-19 at 1 23 01 PM"" src=""https://user-images.githubusercontent.com/1534870/59797784-6a236e00-9295-11e9-8956-50feaa9fcc97.png"">

In this case, the query is running automatically for the data preview when the user selects the table in the table browser (left of SQL Lab) in order to inspect it. This makes the browser extremely sluggish, even crashing it.

### Proposed Change

I propose moving the persistence of SQL Lab's state from the browser `localStorage` to the metadata database. State would be synced the following way:

#### backend -> frontend

- On load, the bootstrap payload contains a list of tab IDs, and the active tab ID.
- SQL Lab loads the active tab asynchronously by ID. This will load:
  - selected database
  - selected schema
  - any table schemas
  - query in textarea
  - results (if query has run)
  - any table previews
- On tab switch, the corresponding tab is loaded asynchronously in a similar way.

#### frontend -> backend

- Tabs are saved every time a query changes (user typing, eg), with debouncing.
- Tabs are saved every time a query is executed.
- Tabs are saved every time results are loaded.
- Similar for other changes (database changed, schema, table preview).

These changes should be transparent to the user, with the exception that there will be an additional latency from having to request the state from the server, instead of having it in `localStorage`.

### New or Changed Public Interfaces

For this work, we need to create the following models:

```python
# superset/models/sql_lab.py
class TabState(Model):

    __tablename__ = 'tab_state'

    # basic info
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('ab_user.id'))
    label = Column(String(256))
    active = Column(Boolean, default=False)

    # tables that are open in the schema browser and their data previews
    table_schemas = relationship('TableSchema')

    # the query in the textarea, and results (if any)
    # we'll reuse the Query model, since it has everything we need
    # (note that this require having results_key set even for sync queries)
    query_id = Column(Integer, ForeignKey('query.id'))
    query = relationship('Query')


class TableSchema(Model):

    __tablename__ = 'table_schema'

    id = Column(Integer, primary_key=True)
    tab_state_id = Column(Integer, ForeignKey('tab_state.id'))

    # DB
    database_id = Column(Integer, ForeignKey('dbs.id'), nullable=False)
    database = relationship('Database', foreign_keys=[database_id])
    schema = Column(String(256))
    table = Column(String(256))
  
    # JSON describing the schema, partitions, latest partition, etc.
    results = Column(Text)
```

These will be exposed via automatically generated views by FAB.

Note that since we're loading the query results from the server, for synchronous queries we need to store the results in a results backend. We can fallback to a simple results backend (`werkzeug.contrib.cache.BaseCache`, eg) when none is set.

Optionally, we can store results ourselves in the database with a simple model:

```python
class Results(Model):

    __tablename__ = 'results'

    # used as results_key for sync queries
    id = Column(Integer, primary_key=True)

    # JSON with the results payload
    results = Column(Text)
```

### New dependencies

None.

### Migration Plan and Compatibility

We should implement logic that moves the state from `localStorage` to the backend when we detect that it's stored in the browser.

### Rejected Alternatives

None.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.61. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",betodealmeida,"
--
 @mistercrunch @graceguo-supercat @khtruong 
--

--
@etr2460 

> What happens if someone has the same SQL Lab tab in two different browsers? Do we provide a cursor for updating the tabs/queries, or do we just let people overwrite it automatically? How do we sync the state of the backend to the frontend?

I'm not sure what's the best way of handling this. Currently the last edit is persisted, but not propagated to other open tabs. The implementation proposed would preserve that behavior.

We could use `localStorage`(or https://developer.mozilla.org/en-US/docs/Web/API/BroadcastChannel) to store if a given tab is being edited, so that when the user opens the same tab in a new browser tab they get a message warning them, or the tab textarea is rendered as read-only, with option to takeover. Any thoughts?

> Can the bootstrap payload include the tab details for the active tab too? That would improve load time I think

Good point, thanks!
--

--
> Why are both of query_id and query required in TabState?

`query` is just a lazy attribute, not a real column. Accessing it returns the `Query` object.
--
",mistercrunch,"
--
Some notes:
* how do we store/retrieve query history? Seems reasonable to fetch it async when navigating to that tab
* nit: I'd rename `Tab` to `TabState`
* thought: I'd add a `json` blob field in `TabState` that matches what goes into the redux store, denormalizing and removing the need for `TableSchema` altogether
--
",etr2460,"
--
Thoughts...

- What happens if someone has the same SQL Lab tab in two different browsers? Do we provide a cursor for updating the tabs/queries, or do we just let people overwrite it automatically? How do we sync the state of the backend to the frontend?
- I was thinking about simply storing a `json` blob in `TabState` too, but it has the downside of making updates to the client store and state very difficult (wouldn't address your second drawback above).
- Can the bootstrap payload include the tab details for the active tab too? That would improve load time I think
- What's the tradeoff between falling back to a default results backend vs. not caching query results at all? I'm concerned with adding additional caching complexity that might not be necessary.
--
",khtruong,"
--
I also agree that adding a json blob would make it difficult to make updates to the store, especially since the state structure can change as often as we want it to. 
--
",DiggidyDave,"
--
I suggest keeping the narrative clean, stick to the general principle of this SIP, and NOT use local storage. What about?

- put a guid on TabState
- have browser do conditional PUT for TabState, if the GUID doesn't match then fail write and query latest/correct state 
- on `window.focus` fetch latest state, to make sure user doesn't start working on top of stale state
- on `window.blur` (visible but not focus) poll for updates
- handle `onvisibilitychange`to stop polling (if not visible) or transition to correct state if becoming visible

The last 3 bullets make a foundational assumption that a user will not be simultaneously editing in 2 windows. They may have 2 windows open, even side by side on the same screen, but they will only have focus and be actively typing in one of them.

EDIT to note: thie TL;DR of this is that when you are typing, if another editor window is open and visible, it will update at the polling frequency to reflect what you are typing in the focus window.
--

--
Why are both of `query_id` and `query` required in `TabState`?
--
"
7656,OPEN,[SIP-15A] Proposal for inferring temporal formatting and parsing,enhancement:request; sip,2019-10-10 19:11:41 +0000 UTC,john-bodley,Opened,,"## [SIP-15A] Proposal for inferring temporal formatting and parsing

### Motivation

In [SIP-15](https://github.com/apache/incubator-superset/issues/6360) we surfaced examples were lexicographical sorting could result in incorrect time intervals when the optional configuration fields weren't specified. Initially the proposal was to ensure all temporal fields were cast to a timestamp and filter comparisons were between the appropriate timestamp (or similar) types, i.e., for Presto: 

    DATE_PARSE(ds, '%Y-%m-%d') >= TIMESTAMP '2018-01-01 00:00:00'

Additionally if all temporal fields were transformed to a timestamp the time grains would also work correctly (the current logic assumes that said type can be successfully cast to a timestamp which is not always the case). 

@mistercrunch correctly pointed out out that not all query planners would be able to rewrite these filters to take advantage of indexes (if present) resulting in sub-performant queries and thus the status quo of ensuring the _left-hand-side_ (LHS) of the filter comparison remains unchanged and that the _right-hand-side_ (RHS) formats the Python `datetime` object appropriately, i.e., 

    ds >= '2018-01-01'

There are two problems with the current functionality: 

1. One must _explicitly_ define the format of the temporal column for non date/date-time like types, i.e., strings and numbers as there are multiple temporal encodings, e.g. `%Y-%m-%d`, `%Y-%d-%m`, epoch timestamp (in seconds), epoch timestamp (in milliseconds), etc. 

2. The time grains incorrectly assume that the temporal column can simply be cast to timestamp (or equivalent) type.  

Really this can be seen as two conversions: 

1. Converting a Python `datetime` object into the appropriate database type for filtering. 
2. Converting a database type into a timestamp (or equivalent) which is necessary for the time grain transformations for grouping. 

Note that the `convert_dttm` handles both of these already for date/date-time like types so the problem really lies with string like and numeric types which have temporal encoding.

### Proposed Change

We propose the following to address the two problems outlined above:

#### Format Inferencing 

Rather than having to explicitly define the format for all non date/date-time temporal columns it would be great to infer this from a sample. There are a few Python libraries ([`arrow`](https://github.com/crsmithdev/arrow), [`datetutil`](https://github.com/dateutil/dateutil), [`maya`](https://github.com/kennethreitz/maya), etc.) which can parse non a priori defined date-time formats, i.e., 

    >>> from dateutil.parser import parse
    >>> parse('2018-01-01')
    datetime.datetime(2018, 1, 1, 0, 0)

Sadly none of these libraries will provide the underlying format though there are [ways](https://stackoverflow.com/questions/53892450/get-the-format-in-dateutil-parse) of inferring it. It's also worth pointing out that a single value could be expressed by multiple formats, e.g. `2018-01-01` could be either `%Y-%m-%d` or `%Y-%d-%m`. Taking a sample of values should help further restrict the set of plausible formats.

Regarding epoch timestamps which can be defined via integers (representing seconds or milliseconds) or floats, to differentiate between these one could use basis logic like [www.epochconverter.com](https://www.epochconverter.com) where if there are less than 12 digits the timestamp is assumed to be in seconds, 12 - 14 as milliseconds, and 15+ as microseconds (see [here](https://stackoverflow.com/questions/23929145/how-to-test-if-a-given-time-stamp-is-in-seconds-or-milliseconds) for detail and the rare occurrences where this fails). 

We propose the following solution: 

1. Add a database specific column name/type to format mapping for non-explicit temporal columns, i.e., we use the `ds` column of type `VARCHAR` with the `%Y-%m-%d` format to represent a date-stamp.

2. Whenever a SQLAlchemy column is marked as temporal and the column type is not explicitly a temporal type and no formatting is present then: 
    - If it exists in the mapping apply the format.
    - Otherwise sample the column (using say 100 values) and try to infer the best format. 

3. Rather than using a free-form text box the `python_date_format` field should represent a selector with the various types where either the mapped or best format is selected. This allows users to override the format if the inferencing was incorrect. Why a drop-down and not pre-populated free-form text? Mostly because some formats are not overly human readable and have an example/more detailed description would help.

4. Deprecate the `database_expression` field. This logic should be obtainable via i) using the `python_date_format` field, ii) using a custom expression, or iii) ensuring the type mapping exists in the engine spec.

#### Time Grains 

Given that the format of the type is already inferred we simply need to provide at the engine level in [db_engine_specs.py](https://github.com/apache/incubator-superset/blob/master/superset/db_engine_specs.py) functionality (by ways of a SQL expression) to map from a string or numerical type to a timestamp which will then be wrapped inside of the SQL expressions representing the `time_grain_functions`, i.e., for the example of a date encoded as a string, for the Presto engine we would use the `DATE_PARSE(string, format)` UDF. Note there is already some logic [here](https://github.com/apache/incubator-superset/blob/master/superset/db_engine_specs.py#L147) regarding converting various types to a timestamp.

 For reference here's a few patterns for date-time formats: 

- [MySQL](https://dev.mysql.com/doc/refman/5.5/en/date-and-time-functions.html#function_date-format)
- [Oracle](https://docs.oracle.com/cd/B19306_01/server.102/b14200/sql_elements004.htm)
- [Postgres](https://www.postgresql.org/docs/8.2/functions-formatting.html#FUNCTIONS-FORMATTING-DATETIME-TABLE)
- [Presto](http://teradata.github.io/presto/docs/127t/functions/datetime.html#mysql-date-functions)

It seems SQLAlchemy doesn't provide any abstraction and thus the only viable solution may be to explicitly define a mapping of the various date-time formats similar to [this](http://www.sqlines.com/mysql/functions/str_to_date). 

#### ISO 8601

For string like temporal columns they must adhere to the [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601). The reason being is strings use lexicographical ordering thus we need to ensure the representation coincides with the chronological ordering which is the case for the ISO 8601 format. 

For example say you had dates of the form `%m/%d/%Y` (MM/DD/YYYY in ISO 8601 syntax) and we were fixed on not converting types to timestamps then the date filtering would fail, 

    >>> '12/31/2018' < '01/01/2019'
    False

as opposed to dates of the form `%Y-%m-%d` (YYYY-MM-DD in ISO 8601 syntax), 

    >>> '2018-12-31' < '2019-01-01'
    True

Note if a string column doesn't adhere to the ISO 8601 format one will have to use a SQL expression in order to convert the column to either a date or timestamp (and possibly forgo leveraging the index), i.e., for Presto this would be defined as:

- `type`: `TIMESTAMP`
- `expression`: `DATE_PARSE(ds, '%m/%d/%Y')`

### New or Changed Public Interfaces

N/A.

### New dependencies

Depends on what Python package we use for inferring the date-time format.

### Migration Plan and Compatibility

A migration would be needed to: 

1. Add support for the database specific column name/type mapping. 
2. Re-encoding the `table_columns.python_date_format` column.
3. Bulk inferring temporal formats.

Note the migration and change to type interfering should be rolled out in conjunction with SIP-15 given that by remedying the lexicographical sorting issue outlined in SIP-15 we would fundamentally be changing the time filters for existing charts. Please refer to [here](https://github.com/apache/incubator-superset/issues/6360#issuecomment-495014185) for more details.

### Rejected Alternatives

See [SIP-15](https://github.com/apache/incubator-superset/issues/6360).

to: @agrawaldevesh @betodealmeida @mistercrunch @michellethomas @villebro 
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.69. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",,,,,,,,,,
7571,OPEN,[SIP-20] Proposal for Improving Examples Interface; Organization and Storage,enhancement:request; sip,2019-06-21 22:27:55 +0000 UTC,rjurney,In progress,,"# [SIP] Proposal for Improving Examples Interface, Organization and Storage
The goal of the changes in this proposal is to improve the examples capabilities of Superset so as to foster an ecosystem of examples which will sustain and grow as the platform continues to develop. First I will characterize the existing system of examples and then propose changes to improve the number and quality of examples. 

## Current Examples
Examples are currently programmatically defined in the `superset.data` module. An abstract interface summarizing these examples looks like the following:

```python
from abc import ABC

class AbstractSupersetExample(ABC):
    """"""Defines interface through which superset examples load themselves.""""""

    def __init__(self, description):
        self.description = description

    def load_data(self):
        # Task 1: Load file and create pandas.DataFrame
        # Task 2: Load data into SQL with pandas.DataFrame.to_sql() 
        # Task 3: Process through ORM to get back workable Table object from whichever data source the table is in
        pass

    def create_metrics(self): 
        # Task 1: Build any TableColumns
        # Task 2: Build Metrics - SQLMetrics
        # Task 3: Store metrics in DB via ORM
        pass
    
    def create_charts(self, chart):
        # Task 1: Build chart from config/JSON
        # Task 2: Store to DB via - misc_dash_slices.add(slc.slice_name) / merge_slice(slc)
        pass
    
    def create_dashboards(self, name, config):
        # Task 1: Instantiate Dash via ORM
        # Task 2: Configure Dash via JSON
        # Task 3: Store to DB via ORM
        pass
```

While this mechanism jump started the collection of Superset examples, defining examples as code will not appeal to most Superset users of growing a community that contributes examples.

### Current Dashboard Import/Export

Dashboard example creation could utilize the export feature by adding example oriented fields to the export. Dashboards can be exported via the Dashboard List interface at `/dashboard/list/` via its Export action or at the command line via  `superset export_dashboards`. Dashboard and chart export JSON includes everything needed to reproduce a dashboard save the actual data table: dashboard, chart and datasource information. 

The `datasources. __SqlaTable__.database` element will need to be removed when examples are created and recreated when they are loaded to match `SQLALCHEMY_EXAMPLES_URI` or a `database-uri` the user specifies. Each Slices `datasource_id` and `datasource_name` must be changed.

```
{
  ""dashboards"": [
    {
      ""__Dashboard__"": {
        ""created_on"": {
          ""__datetime__"": ""2019-04-19T10:29:21""
        },
        ""changed_by_fk"": null,
        ""slug"": ""world_health"",
        ""json_metadata"": ""{\""remote_id\"": 1}"",
        ""description"": null,
        ""dashboard_title"": ""World's Bank Data"",
        ""changed_on"": {
          ""__datetime__"": ""2019-05-02T17:55:47""
        },
        ""created_by_fk"": null,
        ""css"": null,
        ""position_json"": {...},
  ""id"": 1,
  ""slices"": [
    {
      ""__Slice__"": {
        ""created_by_fk"": null,
        ""cache_timeout"": null,
        ""params"": ""{\""compare_lag\"": \""10\"", \""compare_suffix\"": \""o10Y\"", \""country_fieldtype\"": \""cca3\"", \""date_filter\"": false, \""entity\"": \""country_code\"", \""filter_configs\"": [{\""asc\"": false, \""clearable\"": true, \""column\"": \""region\"", \""key\"": \""2s98dfu\"", \""metric\"": \""sum__SP_POP_TOTL\"", \""multiple\"": true}, {\""asc\"": false, \""clearable\"": true, \""column\"": \""country_name\"", \""key\"": \""li3j2lk\"", \""metric\"": \""sum__SP_POP_TOTL\"", \""multiple\"": true}], \""granularity_sqla\"": \""year\"", \""groupby\"": [], \""limit\"": \""25\"", \""markup_type\"": \""markdown\"", \""metric\"": \""sum__SP_POP_TOTL\"", \""metrics\"": [\""sum__SP_POP_TOTL\""], \""row_limit\"": 50000, \""secondary_metric\"": \""sum__SP_POP_TOTL\"", \""show_bubbles\"": true, \""since\"": \""2014-01-01\"", \""time_range\"": \""2014-01-01 : 2014-01-02\"", \""until\"": \""2014-01-02\"", \""viz_type\"": \""filter_box\"", \""where\"": \""\"", \""remote_id\"": 371, \""datasource_name\"": \""wb_health_population\"", \""schema\"": \""wb_health_population\"", \""database_name\"": \""main\""}"",
        ""datasource_name"": null,
        ""datasource_type"": ""table"",
        ""slice_name"": ""Region Filter"",
        ""changed_on"": {
          ""__datetime__"": ""2019-05-02T18:00:39""
        },
        ""changed_by_fk"": 1,
        ""perm"": ""[main].[wb_health_population](id:2)"",
        ""description"": null,
        ""viz_type"": ""filter_box"",
        ""datasource_id"": 2,
        ""id"": 371,
        ""created_on"": {
          ""__datetime__"": ""2019-05-02T17:55:47""
        },
        ""owners"": []
      }
    }
  ],
  ""datasources"": [
    {
      ""__SqlaTable__"": {
        ""created_on"": {
          ""__datetime__"": ""2019-04-19T10:29:20""
        },
        ""sql"": null,
        ""cache_timeout"": null,
        ""changed_on"": {
          ""__datetime__"": ""2019-05-02T17:55:47""
        },
        ""is_sqllab_view"": false,
        ""params"": ""{\""remote_id\"": 2, \""database_name\"": \""main\""}"",
        ""id"": 2,
        ""template_params"": null,
        ""perm"": ""[main].[wb_health_population](id:2)"",
        ""description"": ""<!--\nLicensed to the Apache Software Foundation..."",
		  ""created_by_fk"": null,
        ""table_name"": ""wb_health_population"",
        ""default_endpoint"": null,
        ""changed_by_fk"": null,
        ""main_dttm_col"": ""year"",
        ""is_featured"": false,
        ""database_id"": 1,
        ""filter_select_enabled"": true,
        ""fetch_values_predicate"": null,
        ""offset"": 0,
        ""schema"": null,
        ""columns"": [
          {
            ""__TableColumn__"": {
              ""created_by_fk"": null,
              ""python_date_format"": null,
              ""is_dttm"": false,
              ""description"": null,
              ""groupby"": false,
              ""type"": ""FLOAT"",
              ""verbose_name"": null,
              ""id"": 4,
              ""changed_by_fk"": null,
              ""created_on"": {
                ""__datetime__"": ""2019-04-19T10:29:20""
              },
              ""table_id"": 2,
              ""filterable"": false,
              ""expression"": """",
              ""database_expression"": null,
              ""is_active"": true,
              ""column_name"": ""NY_GNP_PCAP_CD"",
              ""changed_on"": {
                ""__datetime__"": ""2019-04-19T10:29:20""
              }
            }
          },
			...
		  ],
        ""metrics"": [
          {
            ""__SqlMetric__"": {
              ""created_by_fk"": null,
              ""table_id"": 2,
              ""d3format"": null,
              ""description"": null,
              ""metric_name"": ""sum__SP_POP_TOTL"",
              ""changed_on"": {
                ""__datetime__"": ""2019-04-19T10:29:20""
              },
              ""metric_type"": null,
              ""changed_by_fk"": null,
              ""expression"": ""sum(SP_POP_TOTL)"",
              ""warning_text"": null,
              ""is_restricted"": false,
              ""verbose_name"": null,
              ""id"": 3,
              ""created_on"": {
                ""__datetime__"": ""2019-04-19T10:29:20""
              }
            }
          },
			...
		  ],
		  ""database"": {
          ""__Database__"": {
            ""allow_csv_upload"": true,
            ""verbose_name"": null,
            ""created_by_fk"": null,
            ""allow_ctas"": false,
            ""database_name"": ""main"",
            ""changed_by_fk"": null,
            ""allow_dml"": false,
            ""sqlalchemy_uri"": ""sqlite:////Users/rjurney/.superset/superset.db"",
            ""force_ctas_schema"": null,
            ""password"": null,
            ""allow_multi_schema_metadata_fetch"": false,
            ""cache_timeout"": null,
            ""created_on"": {
              ""__datetime__"": ""2019-04-19T10:28:59""
            },
            ""extra"": ""{\n    \""metadata_params\"": {},\n    \""engine_params\"": {},\n    \""metadata_cache_timeout\"": {},\n    \""schemas_allowed_for_csv_upload\"": []\n}\n"",
            ""select_as_create_table_as"": false,
            ""perm"": ""[main].(id:1)"",
            ""changed_on"": {
              ""__datetime__"": ""2019-04-19T10:28:59""
            },
            ""expose_in_sqllab"": true,
            ""impersonate_user"": false,
            ""id"": 1,
            ""allow_run_async"": false
          }
        }
      }
    },
```

A Slice object has a `params_dict` which contains the following. Note that this includes references to the `datasource_name` `wb_health_population`.

```python
 {'compare_lag': '10',
  'compare_suffix': 'o10Y',
  'country_fieldtype': 'cca3',
  'entity': 'country_code',
  'granularity_sqla': 'year',
  'groupby': ['region', 'country_code'],
  'limit': '25',
  'markup_type': 'markdown',
  'metric': 'sum__SP_POP_TOTL',
  'metrics': ['sum__SP_POP_TOTL'],
  'row_limit': 50000,
  'secondary_metric': 'sum__SP_POP_TOTL',
  'show_bubbles': True,
  'since': '1960-01-01',
  'time_range': '2014-01-01 : 2014-01-02',
  'until': 'now',
  'viz_type': 'treemap',
  'where': '',
  'remote_id': 708,
  'datasource_name': 'wb_health_population',
  'schema': 'wb_health_population',
  'database_name': 'main'}]
```

## Example Components
A Superset example is a SQL oriented dashboard and is composed of the following:

	* Physical database with tables loaded with a dataset
	* Superset Datasource object
	* Table object
		* Column objects
		* Metric objects
	* Dashboard object
	* One or more chart objects

All of the above with the exception of the Datasource.Database entry will need to be serialized, stored, contributed, approved, listed, deserialized and loaded by the example system. The Datasource.Database entry will need to be removed on exporting and replaced on importing of examples.

## Scope of Improvement
This proposal improves the superset example process in three areas: example creation, data storage and discoverability.

In order to improve the range and quality of Superset examples we need to first improve the process for creating and loading examples. While examples can be created programmatically, the more natural process is to use Superset to create them. This requires that we automate the process to persist and restore the combined state of the Superset Dashboard, Database and related objects as well as the contents of the datasource itself.

We also need a directory to which examples can be uploaded and a corresponding user interface and process of governance over that repository. This directory should be independent of the Superset project release process and code repository. Current processes for management of Supersets code assets would transition directly to the management of its examples: changes would be created by creating Github issues and pull requests, data assets would be versioned and managed in a central repository.

Finally we need a user interface for finding, listing and loading examples from the repository. It should be simple and can exist as an `examples` command as part of the superset CLI which will have `export`, `list` , `import` and `remove` sub-commands.

## Example Repository Requirements
The requirement for example storage are that it have the following properties:

* Any user can view a list of all examples that have been published
* Any user can upload an example and publish it - pending approval
* Examples can be approved by committers by some mechanism
* Approved examples can be published/released by committers by some mechanism
* Management process/interface should match as closely as possible existing tools and practices.
* The project is separate from Superset to enable more rapid and independent release cycles

Git and Github are a desirable mechanism for publishing and approval but an undesirable mechanism for storage. Git LFS (Large File System) offers scalable storage while still using Github for project management. With a 2GB file limit and support on Github for 250 of these files, it scales well and is the proposed storage system. Other options are explored in the addendum.

### Example File Format

Examples should be defined and packaged in a standard manner and each example should be self contained in its own file system directory. The existing `Dashboard` export format adequately describes a dashboard, its charts and the associated datasources but is missing human readable fields describing the contents of the dataset and dashboard as well as the physical location of the contents of the tables the datasource metadata describes. These fields will be added to the Superset dashboard export format.

Data location information will be stored in a top level `files` key next to the existing `dashboards`, `slices` and `datasources` keys. A top level `description` field will fill out the fields of a description of the dataset in the examples directory. The existing `Worlds Bank Data` example is extended below:

```
{
    dashboards: [ ... ],
    datasources: [ ... ],
    description: {
        ""created_at"": ""2019-05-20T16:20:24.883125"",
        ""description"": ""World Bank Data example about world health populations from 1960-2010."",
        ""file_count"": 1,
        ""license"": ""Apache 2.0""
        ""title"": ""World Bank Health Information"",
        ""total_rows"": 11770,
        ""total_size"": 22561353,
        ""total_size_mb"": 21.52,
    },
    files: [ 
        {
            ""file_name"": ""wb_health_population.csv.gz"",
            ""rows"": 11770,
            ""size"": 22561353,
            ""table_name"": ""wb_health_population""
        }
    ],
    slices: [ ... ]
}
```

The file layout for this example appears as follows, with the dashboard slug used as the directory name in the exported tarball and examples directory:

```bash
/
/world_health
/world_health/dashboard.json
/world_health/wb_health_population.csv.gz
```

### Example Data Table Format

In order to manage tables, to create and drop them, it is helpful to assume that an `Integer`  `id`  primary column is present. This is the case for all current example dashboard tables. In the future we may want to support tables with `uuid` or other types of primary column.

### New or Changed Public Interfaces

Changes include the addition of a `SQLALCHEMY_EXAMPLES_URI`  and `EXAMPLES_GIT_TAG` configuration keys and changes to the model classes as well as the CLI.

#### The `examples-data` Repository

Currently the example data is on Github at [apache-superset/examples-data](https://github.com/apache-superset/examples-data). This will continue to be the case, but this repository will now house both Dashboard metadata files as well as data files via Git LFS. Each example will have its own directory with its own `dashboard.json` and data files.

The README.md for this repository in new new form can be accessed here: [GitHub - rjurney/examples-data at lfs](https://github.com/rjurney/examples-data/tree/lfs).

#### `SQLALCHEMY_EXAMPLES_URI` Configuration Key

A `SQLALCHEMY_EXAMPLES_URI` configuration key in `superset/config.py` controls the default location to load examples into. This defaults to `~/.superset/examples.db` and can be over-ridden on a per-import basis using the `--database-uri/-d` option.

#### `EXAMPLE_REPOS_TAGS` Configuration Key

A `EXAMPLE_REPOS_TAGS ` configuration key in `superset/config.py` controls the locations of the examples from which to list and load. This can be set manually using the `--examples-repo/-r` option. The format of the items are a tuple containing the full repository name (ex. `apache-superset/examples-data`) and the git tag/branch of the repository to use (ex. `master`).

In `config.py` the default entry will look like:

```python
# Tuple format: Gitub repo full name, tag/branch
EXAMPLE_REPOS_TAGS = [
    ('rjurney/examples-data', 'v0.0.3')
]
```

#### `GITHUB_AUTH_TOKEN` Configuration Key

Github rate limits the contents API to 50 anonymous requests per hour. While this is unlikely to affect many users, the limit is by IP address which means users behind proxies or developers may sometimes encounter this. I have added the optional configuration key `GITHUB_AUTH_TOKEN`  which provides a way to add a [personal access token](https://help.github.com/en/articles/creating-a-personal-access-token-for-the-command-line) to requests from the examples sub-commands. This increases the API limit.

#### UUIDs via sqlalchemy.types.uuid.UUIDType

In order to export or import assets in a way that doesnt result in integer primary key chaos, we require that each serialized asset have a unique identifier. The `superset.models.helpers.ImportMixin` class has been used to provide a `uuid` field to the following classes:

* `Dashboard`
* `Datasource`
* `Database`
* `DruidCluster`
* `DruidMetric`
* `Slice`
* `SqlMetric`
* `SqlaTable`
* `TableColumn`

This required [patching FlaskAppBuilder](https://github.com/dpgaspar/Flask-AppBuilder/pull/1010) to support UUIDType as a String field type. This will be released with FlaskAppBuilder 2.1.4.

#### ImportMixin > ImportExportMixin

I was confused by the role of `ImportMixin` in model class export, so accordingly I have renamed it to `ImportExportMixin`.

#### Command Line Interface

Example capabilities will be accessed via the command line (CLI) interface. The CLI will be changed, removing the `load_examples` command and replacing it with an `examples` subcommand with `export`,  `list`, `import` and `remove` commands beneath it.

##### Top Level CLI Menu

`superset help`

```bash
Usage: superset [OPTIONS] COMMAND [ARGS]...

  This is a management script for the Superset application.

Options:
  --version  Show the flask version
  --help     Show this message and exit.

Commands:
  db                        Perform database migrations.
  examples                  Manages example chart/dashboards/datasets
  ...
```

##### Dashboard Exports Menu

The dashboard exports menu will be extended to add the `dashboard-titles/-t`, `export-data/-x` and `export-data-dir/-d` options which facilitate example export.

`superset export_dashboards --help`

```bash
Usage: superset export_dashboards [OPTIONS]

  Export dashboards to JSON

Options:
  -f, --dashboard-file TEXT    Specify the the file to export to
  -p, --print_stdout           Print JSON to stdout
  -i, --dashboard-ids INTEGER  Specify dashboard id to export
  -t, --dashboard-titles TEXT  Specify dashboard title to export
  -x, --export-data            Export the dashboard's data tables as CSV
                               files.
  -d, --export-data-dir TEXT   Specify export directory path. Defaults to
                               '/tmp'.
  --help                       Show this message and exit.
```

##### Examples Top Level Menu

`superset examples -help`

```bash
Usage: superset examples [OPTIONS] COMMAND [ARGS]...

  Manages example dashboards/datasets

Options:
  --help  Show this message and exit.

Commands:
  export  Export example dashboard/datasets
  list    List example dashboards/datasets
  import  Import an example dashboard/dataset
  remove  Remove an example dashboard/dataset
```

##### Example Creation Menu

The examples creation command can be used to export a Dashboard JSON file along with its underlying data tables into a gzipped tarball file. These assets can then be uncompressed in the [`examples-data`](https://github.com/apache-superset/examples-data) project, committed, pushed and then submitted by pull request.

`superset examples export --help`

```bash
Usage: superset examples export [OPTIONS]

  Export example dashboard/datasets

Options:
  -i, --dashboard-id INTEGER  Specify dashboard id to export
  -t, --dashboard-title TEXT  Specify dashboard title to export
  -d, --description TEXT      Description of new example  [required]
  -e, --example-title TEXT    Title for new example  [required]
  -f, --file-name TEXT        Specify export file name. Defaults to
                              dashboard.tar.gz
  -l, --license TEXT          License of the example dashboard
  --help                      Show this message and exit.
```

##### Examples List Menu

The examples list command will query the [`examples-data`](https://github.com/apache-superset/examples-data) repository and return a list of available examples along with their metadata. Examples can then be loaded from this list.

`superset examples list -help`

```bash
Usage: superset examples list [OPTIONS]

  List example dashboards/datasets

Options:
  -r, --examples-repo TEXT  Full name of Github repository containing
                            examples, ex: 'apache-superset/examples-data'
  -r, --examples-tag TEXT   Tag or branch of Github repository containing
                            examples. Defaults to 'master'
  --help                    Show this message and exit.
```

The output looks table uses `prettytable` and looks like this:

```bash
+-------------------------------+-------------------------------------------------------+-----------+-------+-------+------------------+-----------------------+--------+
|             Title             |                      Description                      | Size (MB) |  Rows | Files |   Created Date   |       Repository      |  Tag   |
+-------------------------------+-------------------------------------------------------+-----------+-------+-------+------------------+-----------------------+--------+
| World Bank Health Information | World Bank Data example about world health populat... |   21.52   | 11770 |   1   | 2019-05-20T16:20 | rjurney/examples-data | v0.0.3 |
+-------------------------------+-------------------------------------------------------+-----------+-------+-------+------------------+-----------------------+--------+
```

##### Examples Import Menu

The examples import command will download example metadata and data files from the [`examples-data`](https://github.com/apache-superset/examples-data) repository and will load them into the examples database configured via the `SQLALCHEMY_EXAMPLES_URI` configuration key or using the value supplied by the `database-uri/-d` option.

`superset examples load --help`

```bash
Usage: superset examples load [OPTIONS]

  Load an example dashboard/dataset

Options:
  -d, --database-uri TEXT       Database URI to load example to
  -r, --examples-revision TEXT  Revision of examples to list
  -e, --example-title TEXT      Title of example to load  [required]
  --help                        Show this message and exit.
```

##### Examples Remove Menu

The examples remove command will remove the installed example specified from the metadata tables as well as the examples database configured via the `SQLALCHEMY_EXAMPLES_URI` configuration key or using the value supplied by the `database-uri/-d` option.

`superset examples remove --help`

```bash
Usage: superset examples remove [OPTIONS]

  Remove an example dashboard/dataset

Options:
  -e, --example-title TEXT  Title of example to remove  [required]
  -d, --database-uri TEXT   Database URI to load example to
  --help                    Show this message and exit.
```

### New Dependencies

Creating, removing, listing and loading examples can be handled without Git LFS but adding examples to the [`examples-data`](https://github.com/apache-superset/examples-data) Github repository will require it. This is a developer only requirement of the [`examples-data`](https://github.com/apache-superset/examples-data) project and not Superset itself. 

Git LFS can be [installed](https://github.com/git-lfs/git-lfs/wiki/Installation) via:

```bash
# OS X
brew install git-lfs
# port install git-lfs
git lfs install
```

```bash
# Ubuntu
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
sudo apt-get install git-lfs
git lfs install
```

### Migration Plan and Compatibility

Backwards compatibility will be maintained. Existing dashboard export JSON files will continue to work and all existing dashboard examples and their data files will be ported to the new system and stored in the Superset examples repository.

#### Cross-Repository Management

As both the examples and Superset evolve, some will work with newer versions of Superset than others. We must strive to keep all of them up to date, but should also try to make them backwards compatible. It will thus be inevitable that `incubator-superset` releases will have to point to a branch/tag of `superset-examples`. 

A given release of superset must reference a certain version of the `examples-data` repository. This is achieved via the `EXAMPLES_GIT_TAG` configuration key. Alternatively, this could be a branch rather than a tag to facilitate the ongoing update of examples.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.94. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",mistercrunch,"
--
Notes:
* let's rename to `superset examples load` to `superset examples import`
* let's rename to `superset examples create` to `superset examples export`
* let's assume we have multiple repos of examples in the wild, so that means
    * all `superset example` subcommand should support a ""--repo"" argument
    * we probably need a config key `EXAMPLES_REPO_URIS` that's a list by default should be pointing to `examples-data` repo
* remove `Database`, `DruidCluster`, and `.*Schedule.*` from the scope of models exported
* manage/prevent table name collision
* have a `--nodata` option on both export and import
--
",rjurney,"
--
* Table name collisions is not addressed in the PR or this SIP.
* `--nodata` is not implemented in the PR or SIP

The other parts are addressed in the PR and SIP.
--
",,,,,,
7565,OPEN,Spreadsheet; or Google Sheets; data source,.pinned; data:connect:googlesheets; enhancement:request,2021-01-04 17:37:50 +0000 UTC,brylie,In progress,,"We have internal stakeholders who regularly work with data in spreadsheets. We would like to make it simple for them to get spreadsheet data into the data portal. 

Since our staff use Google Sheets (and Drive), it would make sense that they could upload the data spreadsheets to google Drive and then connect them to Superset for dashboarding.

Alternatively, they could upload the spreadsheet to Superset, via the UI.
",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.74. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",francishemingway,"
--
Hi Brylie,

This may be of interest to you:
https://github.com/apache/incubator-superset/pull/5915
Francis
--
",mistercrunch,"
--
This works well, though it's hard to get it to work with non publicly shared google sheets. I'm pretty sure it's doable though.
--

--
@brylie pinned it so it won't get closed

It'd be great to have a blog post walking people step by step.  The short story is something like:
* `pip install gsheetsdb`
* `pip install gsheetsdb[sqlalchemy]`
* create a database connection in Superset with the sqlalachemy url `gsheets://`
* add a ""table"" by using the url of the spreadsheet as the table name
* you can also in SQL Lab go `SELECT * FROM ""{spreadsheet_url}""`
--

--
Full URI is `gsheets://`. There is a way to work with private sheets, and to see ""public within my org"" in GSuite, but it's fairly tricky. @betodealmeida can probably provide pointers. We kind of owe the world a blog post on this whole topic.
--
",brylie,"
--
By way of example, Redash accesses Google Sheets that are shared to a specific ""email""

https://redash.io/help/data-sources/querying/google-spreadsheet
--

--
I think the @stale bot is a bit misguided. I understand it is important to groom the issues, but this feature may be a while in the works.
--

--
Is it possible to flag issues, so the stale bot doesn't close them? I.e. this feature request seems important, particularly the need for private sheets.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",jaksmid,"
--
@mistercrunch How do you make it work with non public sheets? 
https://github.com/betodealmeida/gsheets-db-api mentions generated json file but it is not clear to me how would you inject that the sqlalchemy connection in superset without modifying the source code. Thanks!
--
"
7512,OPEN,[Feature request] Intra Dashboard Interaction (Publish Subscriber Design Pattern),.pinned; enhancement:request; viz:dashboard:native-filter,2020-12-11 20:36:49 +0000 UTC,jitendra-kumawat,Opened,,"Most of BI Tools have features of Data Drill Down and Dashboard Linking but superset doesn't have this in design so this is Design Proposal to add these features in superset and we can serve below use cases

**Use cases**
Intra Dashboard Interactivity means whenever user do selection over any visualization/chart in Dashboard then selection of that chart can apply to rest/set of charts in following manner depends on UX/Usecases-

Apply as filters/global filters in sql query as where clause with required query operators referred as **applyFilter**
Change model or schema of chart referred as **changeConfig**
Show information in popup referred as **showPopup**
Redirect to new URL or any other dashboard url referred as **redirectToUrl**
Mix of any action listed above referred as **applyFilter** and **configChange**
Selection sync or highlight same selections in different charts referred as **highlightSelections**

**Design**

To solve above use cases here is a design proposal to implement above feature in superset and calling it as Publish Subscriber Design Pattern , in this any charts which is used for selection (via click or user-interaction) called as Publisher and rest /set of charts listening these selections and update itself based on action defined called as subscribers.
refer to following terminology

- Publisher - any chart publish selection on click
- Publish Columns - list of columns and respective data published in form of selection
- Subscriber- any chart subscribe selections published by publisher
- Action - action need to execute on subscription, actions can be any above use case listed in introduction (1-6)
- Operator- SQL query operators used while creating where clause like =,!=,IN etc
- Subscribe Columns- subscriber can choose columns from publish columns before execute action
- Extra- store extra metadata required like publish value can map with chart ids to dynamic chart changes
- Interactor - acts mediator between charts and dashboard to identify publisher, subscriber and execute action with correct information
- PubSubMetaData - metadata info related to publisher and subscriber

Design Flow screenshot
1. Over all design flow diagram
<img width=""795"" alt=""Screenshot 2019-05-07 at 10 01 37 AM"" src=""https://user-images.githubusercontent.com/13414728/57750399-bf50ea80-76ff-11e9-97b8-f0da730f9f8c.png"">
<img width=""957"" alt=""Screenshot 2019-05-13 at 12 28 36 PM"" src=""https://user-images.githubusercontent.com/13414728/57750403-c4159e80-76ff-11e9-93cc-0ff18815e931.png"">

** 2. UI for get info of publishers and subscribers from user**
<img width=""952"" alt=""Screenshot 2019-05-13 at 12 32 00 PM"" src=""https://user-images.githubusercontent.com/13414728/57750421-d2fc5100-76ff-11e9-80ac-2ab30d432eba.png"">

**Steps**

1.There is a option on Dashboard to configure above parameters
2. A separate UI at dashboard level open to set PubSub info from USER and store it as PubSubMetaData .
3. once user will choose this option and it will store in Dashboard state and MetaData.


**Describe alternatives you've considered**
We can implement above design in two stages

**Phase -I**

- All Pub sub info will be store at Chart level in form data and new options will be defined and added in PubSub section of Chart
- PubSubMetaData will be created from slices and store at dashboard level and Options of Reconcile will be provided at dashboard level
- once user will choose this option PubSubMetaData will create and store
- interactor will be implemented as per design.

<img width=""947"" alt=""Screenshot 2019-05-13 at 12 57 14 PM"" src=""https://user-images.githubusercontent.com/13414728/57750564-633a9600-7700-11e9-8961-ecfcb9eefd90.png"">

![IMG_20190513_164645](https://user-images.githubusercontent.com/13414728/57750635-a6950480-7700-11e9-864d-c4eb31202c9b.jpg)

![IMG_20190513_164936](https://user-images.githubusercontent.com/13414728/57750647-aeed3f80-7700-11e9-88da-e9439f6380da.jpg)


**Phase -2**

- Designed UI will be implemented as per design and read all pubsub info from UI instead slices formdata
- Reconcile option change to Configure Filters and update functionality


**Additional context**
NA

**If anyone have any thoughts related to this type of feature then we can have a discussion further .**",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.95. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",graceguo,"
--
@xtinec take a look!
--
",jitendra,"
--
@xtinec , if you have any thoughts over this design we can discuss .
--

--
@mistercrunch  @graceguo-supercat  ..any thoughts or input over this feature
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",satishabburi,"
--
Any input on when this feature scheduled for development?
--
",kalimuthu123,"
--
any update about this feature
--
"
7510,OPEN,[SIP-19] Simplify Superset's set of permissions,enhancement:request; security:RBAC; sip,2020-12-15 14:02:06 +0000 UTC,mistercrunch,In progress,,"## [SIP-19] Simplify Superset's permissions 

### Motivation

First let's clarify that this is about feature-related permissions, as opposed to data-access permissions. Also note that what I'll refer to as an ""atomic permission"" is a combination of what FAB calls a ""view_menu"" and a ""permission"".

Also note that this change will be powered by not-so-recently-released FAB features enabling more control over permission definition, as well as migration tools making it straightforward to map and migrate existing role from old to new permission. More information about related FAB features can be found [here](https://flask-appbuilder.readthedocs.io/en/latest/security.html#permission-customization )

Currently, the bulk of feature-related permissions are dynamically generated by Flask App Builder (FAB). For context, FAB generates:
* a set of permission (`can_show`, `can_list`, `can_add`, `can_download`, `can_delete`, `can_update`, `can_muldelete `) for each `ModelView`. Typically each model has one or many ModelViews
* a perm for each `ModelView` method
* a perm for each view method (commonly referred to as ""endpoint"")
* a perm for each menu entry

Now Superset overtime has grown to ship with ~280+ permissions. Most of these permissions are unintelligible to users/administrators/humans. The UI that exposes them suffers from too much options that are not documented. In the current state, it almost only makes sense to generate roles programmatically since the cardinality of permissions is so high, and many organizations do that.

While we want the atomicity of permissions to cover most use cases, we want for permissions to be easy to document and reason about. In cases where it would be unreasonable to have one permission and not another closely related one, we'd like to merge them as a single permission.

### Context

For context, FAB's idea of RBAC has the following entities:
* a role
* with a many-to-many relationship to a `permission_view`, composed of:
   - a single `view_menu`: often representing an object or a class 
   - a single `permission`: often representing an action or a method

Again for context, on top of that Superset adds data-access-related permissions. One for each `database`, `schema` and `dataset` in the system.

### Proposed Change

**First**, group ModelView-related permissions into 2 simple permissions: `read` and `write`, based on this mapping rule:
```
    method_permission_name = {
        'add': 'write',
        'delete': 'write',
        'download': 'write',
        'edit': 'write',
        'list': 'read',
        'muldelete': 'write',
        'show': 'read',
    }
```
This assumes that if you can edit, you can also delete or add (`write`). Similarly if you can `read` you can `show` or `list`.

**Second**, rename and group ModelView names. For clarity drop the ""ModelView"" suffix and match the Model's name. For examples, `DashboardModelView` view_menu becomes `Dashboard` for permission purposes. This in turns takes care of the secondary ModelView derivatives like `DashboardAsyncModelView` and groups it with other `Dashboard`-related ModelView for permission-related purposes.

**Third**, models that are related and tightly coupled, for example every Models living around the connectors should refer to the same set of permissions. DruidDatasource, SqlaTables, and their respective Metric and Column models can all go under `Datasource` permissions. Either you can read or write on `Datasource` or you don't. No one needs to be able to edit metrics but unable to edit columns or datasource property. 

**Fourth**, looking at our custom endpoints, and attempting to get them to piggy back on the existing ""object-action"" existing ones defined by the rules set above around the consolidate `ModelView`-perms. This means that the view `Superset.save_dashboard` can be attached to `Dashboard.write`.

This results [roughly] in the mappings exposed in this [Google Spreadsheet]
(https://docs.google.com/spreadsheets/d/1VzBRUsrf_aMS_QkMXoIaLwpETfJmH3qms3kePywBESI/edit?usp=sharing)

Note that some models like `Dashboard`, `Chart` and `Query` have a notion of ownership as defined by having a many-to-many relationship to our `User` model under a `owners` relationship. This acts as an implicit restrictive modifier to the `write` permission on a model, where only owners are able to alter objects. Perhaps this is out-of-scope for this SIP, but clarifying this pattern with a `OwnershipModelMixin` would help formalizing this permission-related pattern and logic.

### Remaining Questions
* is `can_delete` should be part of `can_write`!?
* how do we role this out?
* change-management, can we trust FAB's perm-reallocation logic?
* How does this affect custom `SecurityManager` out there? how to communicate about it

### New or Changed Public Interfaces

While `Alpha` and `Gamma` will be migrated and effectively the same, and existing roles converged, there may be existing scripts that would be incompatible with this new world. 

### New dependencies

N/A

### Migration Plan and Compatibility

Users will have to run `flask fab security-converge` as part of releasing the new version including this feature.

### Ongoing work
POC here - https://github.com/apache/incubator-superset/pull/7501",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.86. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",aboganas,"
--
This will much simplify Superset Permissions nightmare
--
",andy,"
--
Huge +1 here - managing Superset permissions is a headache. This simplification would be very welcome - in favour of keeping things simple, being able to write implying ability to delete makes sense to me.
--
",mistercrunch,"
--
Rollout strategy and change management are the more complex topics here. @dpgaspar can you talk about how FAB does the perm reassignement / merging? For instance if I have a role that contains both `Dashboard - can_update` and `Dashboard - can_delete` and they get rolled up into a new `Dashboard - write`, how is FAB going to merge things? Logicial `OR`? Logical `AND`? 
--
",elukey,"
--
+1 from my side as well, these changes look really great! 
--
",bkyryliuk,"
--
+1 
I have a preference to have can_delete as a separate permission 

I would be also looking forward read and write access separation for the data access, that potentially could be out of scope for this SIP
--
"
7425,OPEN,[SIP-18] Scheduling queries from SQL Lab,enhancement:request; sip; sql_lab,2021-01-22 20:46:21 +0000 UTC,betodealmeida,In progress,,"## [SIP] Proposal for scheduling queries from SQL Lab

### Motivation

A common approach for building dashboards involves:

1. User writes a complex query in SQL Lab, often with joins, to get the data they need.
2. User clicks ""visualize"", to explore the data.
3. User builds a visualization. This step is usually slow, since the SQL query has to be recomputed every time.
4. User builds a dashboard using the visualization. It's often slow, since the SQL query has to be recomputed every time.
5. In order to get fresh data in the dashboard, user has to either update the underlying SQL query or write expensive queries using macros (scanning 7 days of data, for example).

We want to optimize that process, allowing the user to write **one** query in SQL Lab that runs periodically. The query should scan data only for its interval (1 day of data for a daily schedule, for example). This way dashboards can be kept up-to-date with cheap queries.

In this SIP, I propose a way of scheduling queries from within SQL Lab. The actual scheduling of the query is left to an external service (like [Apache Airflow](https://airflow.apache.org), for example). Superset will simply enrich saved queries with additional metadata for the external scheduler.

The proposal is scheduler-agnostic, and can be used with Apache Airflow, Luigi or any other scheduler, since the form for collecting the metadata needed is defined in `config.py` using [react-jsonschema-form](https://github.com/mozilla-services/react-jsonschema-form).

### Proposed Change

In this SIP we propose adding a new feature flag called `SCHEDULED_QUERIES`. Instead of a boolean, the feature flag would be a dictionary with two keys, `JSONSCHEMA` and `UISCHEMA` (see discussion [here](https://github.com/apache/incubator-superset/pull/6943#issuecomment-469990969) for using a dict as a feature flag). As an example:

```python
FEATURE_FLAGS = {
    # Configuration for scheduling queries from SQL Lab. This information is
    # collected when the user clicks ""Schedule query"", and saved into the `extra`
    # field of saved queries.
    # See: https://github.com/mozilla-services/react-jsonschema-form
    'SCHEDULED_QUERIES': {
        'JSONSCHEMA': {
            'title': 'Schedule',
            'description': (
                'In order to schedule a query, you need to specify when it '
                'should start running, when it should stop running, and how '
                'often it should run. You can also optionally specify '
                'dependencies that should be met before the query is '
                'executed. Please read the documentation for best practices '
                'and more information on how to specify dependencies.'
            ),
            'type': 'object',
            'properties': {
                'output_table': {
                    'type': 'string',
                    'title': 'Output table name',
                },
                'start_date': {
                    'type': 'string',
                    'format': 'date-time',
                    'title': 'Start date',
                },
                'end_date': {
                    'type': 'string',
                    'format': 'date-time',
                    'title': 'End date',
                },
                'schedule_interval': {
                    'type': 'string',
                    'title': 'Schedule interval',
                },
                'dependencies': {
                    'type': 'array',
                    'title': 'Dependencies',
                    'items': {
                        'type': 'string',
                    },
                },
            },
        },
        'UISCHEMA': {
            'schedule_interval': {
                'ui:placeholder': '@daily, @weekly, etc.',
            },
            'dependencies': {
                'ui:help': (
                    'Check the documentation for the correct format when '
                    'defining dependencies.'
                ),
            },
        },
    },
}
```

The configuration is used to dynamically generate a form for collecting the extra metadata needed in order to schedule the query. The example above should work for many schedulers, but it can also be easily adapted (or completely changed) depending on the needs.

If this flag is present, SQL Lab will show a button label ""Schedule Query"":

<img width=""594"" alt=""Screen Shot 2019-05-01 at 11 29 46 AM"" src=""https://user-images.githubusercontent.com/1534870/57034484-79c2f680-6c04-11e9-900e-7dc05e40bf6b.png"">

Clicking it pops up a modal:

<img width=""632"" alt=""Screen Shot 2019-05-01 at 11 31 05 AM"" src=""https://user-images.githubusercontent.com/1534870/57034550-a840d180-6c04-11e9-957f-258d3270a7f6.png"">

When the user clicks ""Submit"" the query is saved (just like a saved query) with the schedule information stored in its JSON metadata. The user can edit the query, like any saved query, and the scheduler can fetch the scheduled queries using the API provided by FAB.

### New or Changed Public Interfaces

None.

### New dependencies

[react-jsonschema-form](https://github.com/mozilla-services/react-jsonschema-form) is an Apache 2 licensed project created by Mozilla. It was last updated 14 days ago, and has ~35k weekly downloads.

### Migration Plan and Compatibility

None.

### Rejected Alternatives

We consider using celery workers to run the queries, but this would add a lot of complexity for backfills, alerting, etc. The proposed approach leverages existing schedulers, leaving to Superset only the task of annotating queries with extra metadata.",,,issue,"
--
Issue-Label Bot is automatically applying the label `#enhancement` to this issue, with a confidence of 0.96. Please mark this comment with :thumbsup: or :thumbsdown: to give our bot feedback! 

 Links: [app homepage](https://github.com/marketplace/issue-label-bot), [dashboard](https://mlbot.net/data/apache/incubator-superset) and [code](https://github.com/hamelsmu/MLapp) for this bot.
--
",mmuru,"
--
@betodealmeida: I got the message ""Your query has been scheduled. To see details of your query, navigate to Saved Queries message"". I could see ""scheduled_info"" stored in saved_query.extra_json column 
`{""schedule_info"":{""output_table"":""mm_schedule_query_test"",""start_date"":""2020-04-18T01:00:00.000Z"",""end_date"":""2020-04-18T02:00:00.000Z"",""schedule_interval"":""@hourly""}}
`
but the query was not running. Please, can you help me to understand how it works? What is the output_table for? Will this table get created automatically or must be exist? Can schedule_interval have a valid cron type syntax? Please, can you share me a sample config file? 
--

--
@betodealmeida and @mistercrunch: Thanks for the response. The sample config file is sorta misleading because its Hive specify use case. I tried to use /savedqueryviewapi/api/read endpoint, it returns everything in saved_query. Is there way to specify url parameters? 
--
",betodealmeida,"
--
@mmuru we have a custom Airflow pipeline that fetches the query information and schedules it. Let me check with Lyft's open source team if we can share it.
--
",mistercrunch,"
--
There was an ongoing conversation about this here https://github.com/apache/incubator-superset/pull/7416 with @ArgentFalcon 
--

--
@mmuru , there's a mini language for all `ModelView` from FAB (`/savedqueryviewapi/api/read` is one of them). The easiest way is to reverse engineer it by using the UI in the list view at `/savedqueryview/list/`, getting the filter you want, and copy pasting the querystring from the URL, applying it to `/savedqueryviewapi/api/read?{HERE!}`

<img width=""1201"" alt=""Screen Shot 2020-07-04 at 10 48 09 AM"" src=""https://user-images.githubusercontent.com/487433/86518157-f1ca1500-bde3-11ea-88f5-2d3431bfe687.png"">

Short term we're working on a state of the art REST API under `/api/v1/` that will be fully documented and type safe.
--
",ktmud,"
--
Re: adding objects to `FEATURE_FLAGS`

I think it's one thing to manage feature configs, it's another to manage whether a feature is turned on or not. If you look at LaunchDarkly's API, their feature flags are created in a very structured manner. And the information stored is only metadata about the variants, nothing related to how the feature is implemented.

It's one thing to store information about variants in feature configs, it's another to store objects required to run the feature. It'd be unscalable and quite dangerous to blur the line here.

Ideally you'd want schema/type enforcement on everything in your program, allowing people to store arbitrary stuff in FEATURE_FLAGS seems to be the opposite of that direction.
--
",edvinas,"
--
@betodealmeida is it possible to edit saved query's scheduling details? How could I disable/stop query from being run in the future?
--
"
7259,OPEN,[SIP-17] Proposal for using new FAB REST API,sip,2019-05-13 15:22:07 +0000 UTC,dpgaspar,In progress,,"Motivation
========
Take advantage of the new FAB REST API feature available since version 1.13.0. The previous
API endpoints available inside ``ModelView`` did not follow a strict RESTful convention, and will be removed in 2 to 3 minors. The following are some improvements:

- API resource protection using JWT and/or flask-login signed cookies (current authentication method). API defaults to JWT only.
- Optional CRUD RESTful API using similar class overrides as ``ModelViews``
- Delegate base API support like handling exceptions to FAB.
- Leverage Rison style URI arguments out of the box with optional JSON schema validation
- OpenAPI automatic spec generation for CRUD, and easy generation for BaseApi method endpoints

Docs: https://flask-appbuilder.readthedocs.io/en/latest/rest_api.html

Proposed Change
==============
I propose migrating current views.Api class to BaseApi. So Superset will have it's new API properly anchored on ``/api/v1/<resource>/`` and start using FAB's new feature.

- The endpoint ``/api/v1/query`` does not need to change it's route or HTTP method.

- The endpoint ``/api/v1/form_data/`` would be changed to /api/v1/slice/<id> using ``ModelRestApi`` on readonly mode (can_get, can_info only permissions).

New or Changed Public Interfaces
==========================
This SIP affects the following API endpoints:

- **/api/v1/form_data** GET -> **/api/v1/slice** GET: exposing a get item and get list leveraging complex filters, pagination, internationalization, ordering, custom select columns (all the way to the database)

Optional Swagger UI for OpenAPI spec visualization.

New dependencies
===============
New dependencies were added to FAB 1.13.0:

- marshmallow
- marshmallow_enum
- marshmallow-sqlalchemy
- Flask-JWT-Extended
- prison
- apispec[yaml]

Migration Plan and Compatibility
=========================
A side by side migration plan can be achieved. Since the new API endpoint is exposed on a different route. Access to the old endpoint would log a deprecation log message.

Rejected Alternatives
================
",,,DiggidyDave,"
--
This is great. I think the deprecation migration plan is a requirement. (wasn't sure about the language ""can be achieved"" so just making sure)
--
",dpgaspar,"
--
I've submitted #7482, but needs FAB>2.1. This PR has no breaking changes.

For the deprecation of the old REST API attached on the ``ModelView's``, I'll be submitting code to replace them. Currently Superset's ``setup.py`` is pinned under the deprecation version. 
But I would say that this is a different problem outside the context of this SIP.

--
",,,,,,,,
7172,OPEN,"Add ""metric"" to show actual value of column; i.e. no aggregate",.pinned,2021-02-12 19:14:42 +0000 UTC,brylie,In progress,,"We have a query that returns timeseries data with two columns:
- date
- count

I would like to simply graph the data as a line chart. However, when selecting the line chart, it requires that I choose a metric for the data, e.g. sum, average, etc

![flameshot-2019-03-29T15:31:01](https://user-images.githubusercontent.com/17307/55236091-cbf4ae80-5237-11e9-9aad-50b5db6c9def.png)

Since my data are already in the desired form, how can I just tell the chart to use the data as-is?

Note: I can also select the ""sum"" metric, since I am not performing any additional aggregation, but this seems a bit ""kludgy"". ",,,rumbin,"
--
You can use a workaround:
Just define an aggregate that would yield the original value if only one element per group is present, like, e.g. sum, min, max, avg.
Then, make sure you configure your chart to use the original time granularity.

However, this workaround is only valid, as long as the timestamps are all distinct. Otherwise the grouping would actually aggregate more than one value...
--
",brylie,"
--
Good workaround.

I still would like to see this as a feature. I.e. aggregations should be optional, since they may be done during the query.
--

--
Bump.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--

--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",khuranabalvinder,"
--
Bump
--
",drewgonzales360,"
--
Bump
--
",benmaier,"
--
I'd like to see this feature, too. having data in two columns and then plotting the first column against the second column is *the* base case, in my opinion. it's very confusing to not have this as an option for a first-time user (such as myself).
--
"
6997,OPEN,[Design Proposal] New Dashboard Creation flow,design:dashboard; sip,2021-03-27 05:53:18 +0000 UTC,dorq,In progress,,"This is a proposal for multiple changes to the current dashboard creation flow.  Please share any concerns or comments directly in this issue.

## How to access the new dashboard flow
We recently added a universal New button to the Superset navigation bar:

![image](https://user-images.githubusercontent.com/2992086/53995953-9b639e00-40eb-11e9-8c95-4aa63d10975d.png)

This New button is available from every page in Superset.  Users enter the flow proposed here by selecting the New Dashboard option.

## New Dashboard Creation Flow (proposed)

### If the user doesnt yet have any of their own charts...

If the user doesnt yet have any of their own charts, this is what they will see:

![dashboard entry point - new user no charts](https://user-images.githubusercontent.com/2992086/53996769-98b67800-40ee-11e9-88c2-0568d113ad08.png)

Note these changes from the current experience:

- Opening directly into this editor shortcuts the current multi-stage process to get to this point, including a placeholder title.
- Sidebar + toolbar at left of the window rather than the right. We're suggesting to move it to the left for two main reasons:
    1) The Chart and SQL editors use side panels on the left, so this makes all the flows consistent.
    2) New users often don't know what to do, so this new design puts the key actions at the top left of the window where users are more likely to see them.
- What was previously under Your charts and filters is expanded to take up the whole sidebar
- Structural tools like tabs, rows, dividers are now contained in a vertical toolbar.  Tooltips will show the associated action (e.g., Drag to insert tabs)
- New textual cue on the blank canvas: Create a chart to place it on the canvas
- There is a new button to create a new chart.  This Create new chart button takes the user to the chart creation flow as described [here](https://github.com/apache/incubator-superset/issues/6996) in a new tab.  This button is sticky at the top of the panel.  Note: If possible, the tabs should communicate so that once the chart is created it shows in the dashboard automatically.

### If the user has already created at least one chart...

If the user has already created at least one chart, this is what they will see:

![dashboard entry point - existing charts](https://user-images.githubusercontent.com/2992086/53996819-c7345300-40ee-11e9-9081-2667486542f7.png)

In addition to the sidebar and toolbar changes mentioned above, note these changes from the current experience:
- Users can preview their charts with thumbnails. This is an optional feature: including a choice to enable/disable it; disabled should be default state.
- New textual cue on the blank canvas: Drag and drop items here

### If the user edits an existing dashboard...

If the user edits an existing dashboard, this is what they will see:

![dashboard entry point - existing dashboard](https://user-images.githubusercontent.com/2992086/53996974-52ade400-40ef-11e9-8474-eb2a68629dd6.png)

This is the same as the current editing experience except for the sidebar and toolbar changes mentioned above.

## How to move forward

Lyft is proposing these changes as a result of research and many iterations with multiple teams.  Airbnb (cc @elibrumbaugh) will further iterate if needed, but at the time of this proposal, the general consensus is that this is an improvement on the current UX for dashboard creation, so we would like to enable development of this ASAP.",,,graceguo,"
--
Hi @dorq Thanks for the proposal, I like it. Here is my input from engineering point of view:

- `We add Sort and Filter capabilities at the top of this list (sticky at top of panel).` 
Current dashboard editor had sort and filter capabilities for the chart lists, right? is there any difference from current feature?

- I noticed there is a preview in the chart list, which is not available in the current dashboard. We didn't implement it mostly from performance concerns. When a user opens edit mode, the chart list shows all the charts that owned by the user, for airbnb users it may have a few hundred items, and each chart is a query (sometimes may be cached data) to backend. To make the list show up and ready to interactive with user need a lot of intelligence, for example, when to send query, send how many queries, how to handle user scrolling in the chart list, etc.  Otherwise user will get stuck waiting for query running and chart render, and the editing dashboard could become unacceptable slow.  

If you decided to implement this chart preview, please offer a choice to enable/disable it, and `disabled` should be default state, so that user won't get stuck when we release the new version. If user really want to see chart preview, they can manually enable it. 

- For the same reason (loading chart list might be time consuming), the old design separated new dashboard component list from chart list. When user entered into edit mode, they see dashboard component list first, then click -> to see chart list. So that when user just wants to re-organize charts layout, resize chart, remove chart, edit title, etc, they don't have to wait for chart list loading. Could this also be considered in the new design?


--
",dorq,"
--
> Current dashboard editor had sort and filter capabilities for the chart lists, right? is there any difference from current feature?

You're exactly right.  I mistakenly noted this as an update while we already have these features.  I've updated the description.

>If you decided to implement this chart preview, please offer a choice to enable/disable it, and disabled should be default state, so that user won't get stuck when we release the new version. If user really want to see chart preview, they can manually enable it.

Also agree with this point.  I've updated this one as well.

Please let me know what you think of the edits!  Thanks for your thoughtful response.
--
",elibrumbaugh,"
--
**Thanks again for sharing this proposal Anita!**

- The create new chart button is outlined while the new button is filled. We should align on a single style for primary actions. In this context create new chart is certainly a primary action. Using the filled style will also help improve discoverability.
- Love the rationale behind moving the panel to the left.
- Re: the name of the dashboard on creation is there a way to make it more human readable? Remove underscore? Also the uppercase U here is at contrast with the lowercase chart placeholder title text.
- I'm on board with removing the charts out of their own tab and bringing them to the forefront. A few things we should consider here is 1. Setting expectations about where the new chart will appear. This could be help text underneath the + Create New Chart button. 
- The way the existing panel improved discoverability of the different components the user could apply to their dashboard was by writing out the entire title. We dual encoded title with icon. To solve for this here would be to write the name of the component under the icon in small text and/or hover tips like you mentioned. It would be cool if we explained purpose of the component as well.
- This structural cue as you called it works better than the button looking style on the create new chart flow. Could we align them?
- Sort by and search/filter your charts looks better here than in create new chart. Seeing the outlined inputs for sort by and search makes me think the create new chart button should be filled to improve the hierarchy.
- Love the thumbnails! Nice considerations for disable as optional.
- Today when I edit a dashboard there is visual context for what charts have and have not been applied. How will this be handled?

Thanks!
--
",junlincc,"
--
We will consider both dashboard and chart creation flow post 1.0. added item to roadmap inbox, and use it as a reference. https://github.com/apache-superset/superset-roadmap/projects/1
--
",,,,
6996,OPEN,[Design Proposal] New Chart Creation flow,sip,2020-10-17 07:59:03 +0000 UTC,dorq,In progress,,"This is a proposal for multiple changes to the current chart creation flow.  Please share any concerns or comments directly in this issue.

## How to access the new chart flow
We recently added a universal New button to the Superset navigation bar:

![image](https://user-images.githubusercontent.com/2992086/53995953-9b639e00-40eb-11e9-8c95-4aa63d10975d.png)

This New button is available from every page in Superset.  Users enter the flow proposed here by selecting the New Chart option.

Users can also access this chart creation flow via all existing modes:
- From the SQL Lab Editor by clicking on the Explore button above the Results tab table
- From the datasource list by clicking on a datasource name
- Anytime they edit an existing chart

When a user selects the New Chart option above, we propose this new chart creation experience...

## New Chart Creation Flow (proposed)

First, the user must choose a datasource:

![chart entry point - wizard 1](https://user-images.githubusercontent.com/2992086/53996025-d8c82b80-40eb-11e9-86c4-12e947ff2d5d.png)

- Chart creation and configuration flow is a 3-step wizard that expands and collapses as an accordion.
- The wizard auto-advances when an item in step 1 or step 2 is clicked.
- We display # Charts that use this datasource to help guide users.
- The Select datasource banner includes an info icon that explains what a datasource is; copy TBD.
- We add **Sort** and **Filter** capabilities at the top of this list.
  - Sort options:
    - \# charts that use it (DEFAULT)
    - Datasource name
    - Type (e.g., Presto)
    - Owner
  - The filter searches:
    - Datasource name
    - Type (e.g., Presto)
    - Owner
- The chart is automatically titled - untitled as a placeholder.
- Users can go back and forth between any of the 3 steps once the datasource has been selected.
- Chart area cues Need datasource until a datasource has been selected
- If a user lands here from the SQL Editor or the Datasource list, we pre-select the datasource for them like we do in the current interface.  So they skip to step 2, with step 1 complete.

Then, the user selects a chart type.  This employs the same thumbnail grid we currently show for changing vis type.

![chart entry point - wizard 2](https://user-images.githubusercontent.com/2992086/53996172-4c6a3880-40ec-11e9-9d98-93b7ae38c6be.png)

Were suggesting here that Line Chart be the default selection, rather than a null selection.  This should be behind a feature flag or a config for orgs to control.

Superset will automatically configure the chart to show some valid data by default; in this example it has selected the first time series value available in the datasource (COUNT(experiment) by experiment_start) and updates the title accordingly (if not already named).

If we default to any particular type (as suggested to use Line Chart above), this second step be skipped/completed by default and the user will jump to step 3.

Notes on previous steps: Now that datasource has been selected, the icons for viewing datasource configuration (eyeball icon - suggestion is to update this icon) and accessing the datasource in SQL Lab (beaker icon - keep existing icon) are viewable in the datasource selection banner:

![image](https://user-images.githubusercontent.com/2992086/53996216-6f94e800-40ec-11e9-90b3-968248b5bdd6.png)

The CTA in the datasource section is Configure rather than Edit because the datasource cannot currently be changed (Were actively discussing bringing back this functionality, but it is not decided yet).

Step 3 is to configure the chart using all the options we currently have in the Superset sidebar:

![chart entry point - wizard 3](https://user-images.githubusercontent.com/2992086/53996223-79b6e680-40ec-11e9-888d-804afaa7dec3.png)

The previous screens detail the flow for when a user creates a new chart.  When users edit an existing chart, these same changes apply there as well, but the user is brought directly into step 3 of the proposed wizard since steps 1 and 2 are already complete.

If the current configuration is sufficient to produce a working chart (that is, one that actually renders a chart rather than an error), this is the Success state:

![chart entry point - wizard 2 - success state](https://user-images.githubusercontent.com/2992086/53996246-8b988980-40ec-11e9-80de-1d65992aec6d.png)

If, however, the state contains an error, we surface it to the accordion header like this:

![chart entry point - wizard 2 - error state](https://user-images.githubusercontent.com/2992086/53996256-96531e80-40ec-11e9-915a-a654b279c449.png)

## How to move forward

Lyft is proposing these changes as a result of research and many iterations with multiple teams.  Airbnb (cc @elibrumbaugh) will further iterate if needed, but at the time of this proposal, the general consensus is that this is an improvement on the current UX for chart creation, so we would like to enable development of this ASAP.",,,michellethomas,"
--
Hi, thanks for creating this design proposal. I recently worked on the datasource control, and have a few comments about that section of the designs. We recently brought back the ability to change datasources from the explore view (https://github.com/apache/incubator-superset/pull/6816), it's been an important feature for users and one we'd like to keep around. We also tried to move away from using icons to avoid having a number of icons when there are lot of actions (I don't have a super strong opinion about this but just thought I'd note it).

@xtinec also had a PR to bring back the + metadata button next to the datasource control (not sure if this is something they still need to keep https://github.com/apache/incubator-superset/pull/6911).

We also have some pretty long table names, so something short like `def.fact_dags` fits into the space between ""Select datasource"" and the icons, but many of our datasource names are longer and showing more of the table name is important. I can get more specific about this if it's helpful.
--
",dorq,"
--
Hi @michellethomas Thanks for bringing up these issues.  I did these designs assuming that we did not have the ability to change datasources from the explore view, because at the time it hadn't been merged.  I agree that it's important and I understand why you want to keep it around.

What do you think about doing this instead of what's mocked up here?
- keep the datasource+dropdown visible when the ""Select datasource"" section is expanded
- keep the same dropdown options you implemented
- remove the icons from the section header that I proposed above; these actions are redundant with the dropdown options
- for long names, we allow them to wrap to a second line if needed (making the header for the section a bit taller in those cases)
--

--
@williaster @kristw Apologies for the delay in responding to your last comment:

> ...our biggest comment is the top-level Data / Query split. we think this probably should be updated from bing a container for the 3 steps, and instead be pushed into the step 3 since it only affects the configuration of the chart. otherwise overall it looks good!

Can you clarify this a bit for me?  I don't understand what you're suggesting here.
--
",elibrumbaugh,"
--
**I could shed a tear for how long I've wanted to see a new button / creation flow.**

- The space between sort by and search is a bit large. I suggest lengthening sort by a bit and then allowing search to fill in the rest of the gap. It would be great if it matched how we do this on the dashboard editor pane > charts section.
- The need datasource notice in the middle of the canvas could use a little more visual hierarchy. Right now it looks slightly like a disabled button. Maybe something happens when you click on it? Focuses the user on section 1.
- The hover state highlight will be a neutral color right? Right now it looks like a Lyft specific color?
- Can we get rid of the ""-"" before untitled? Maybe say Untitled to promote human readable titles.
- Love the thoughtful move to step 2 if coming from SQL Lab!
- Under select a chat type could we add sort by and search? Sort by popularity. You're kind of doing this already with the order roughly I think?
- In place of sort by add a dropdown for selecting the goal of the vis such as comparison and apply a different visual treatment to the types that aren't ideal for comparison. It wouldn't be too hard for us to all align on groupings of the visualizations.
- For eyeball and beaker icons. The icons in Superset today are pretty bad. I would learn on textual buttons in place of icons when/if possible.
- Between step 1 and 3 the styles look similar but appear to me as a little different. To reduce visual fragmentation can we leverage the existing styles as much as possible? Or plan on updating the step 3 styles to match?
- I wonder if the brand green color would work better for the 1, 2, 3 step headers? If we want to adopt the dark headers we should replace it throughout the UI. I'm a little worried the grey isn't as discoverable or communicative of current/active step.
- Re: ""If, however, the state contains an error, we surface it to the accordion header like this"" Why aren't we applying a disabled color to the section and throwing an error message in the notification banner?

This is awesome work Anita! Thank you so much for pushing this forward!
--
",williaster,"
--
thanks @dorq for the designs! 

@kristw and I just looked it over more thoroughly and our biggest comment is the top-level `Data` / `Query` split. we think this probably should be updated from bing a container for the `3` steps, and instead be pushed into the step `3` since it only affects the configuration of the chart. otherwise overall it looks good!
--

--
@dorq sure.

Basically it does not make sense for this set of tabs (`Data`<>`Visual Properties`) to exist **_outside_** of the `1`/`2`/`3` flow. It only makes sense in the context of step `3`, and therefore we think it should **_not contain_** steps `1`/`2`/`3`, and instead be moved **_inside_** step `3` where it's relevant. Make sense?

![image](https://user-images.githubusercontent.com/4496521/55267861-3d395f00-5242-11e9-8107-c12abb61ae69.png)

--
",,,,
6929,OPEN,[SIP] Proposal for Implemention of dynamic theme in superset,sip,2020-11-24 21:29:03 +0000 UTC,rasmi-ranjan-guavus,Opened,,"**Motivation**:

Supporting dynamic theme in superset will provide end-user to use his/her choice of theme on the go.

**Proposed Change**:

1> Provide option to change theme from UI. 
2> Support of multiple themes
3> Separate theme-able style from other styles (position, size etc) 
4> use scss/less mixin to change theme dynamically

this is how the style for any component will look like:

`.btn-primary {
  width: 200px;
  Padding: 2px;
  position: relative;

}

@mixin btn-primary-theme($theme) {

.btn-primary {

color: get-color($theme, primary);
background-colour: get-color($theme, background);
font-size: font-size($theme, button);
}

}
`

**New or Changed Public Interfaces**:

- if we use SCSS
      mixin the we need sass-loader (MIT license)
       Enable scss  style pre-processor  in superset.

- If we use less. 
    Since Less is already supported in superset, don't need any change

**Migration Plan and Compatibility**

for backward compatibility existing styles will be available. 
To use theme user need to use new styles

",,,rasmi,"
--
@mistercrunch any thought on this?
--
",gbrian,"
--
@rasmi-ranjan-guavus @mistercrunch MVP proposal using existing CSS Templates:
https://github.com/gbrian/incubator-superset/pull/2
--

--
@mistercrunch, sorry can't find the issue where you posted about how to customize look and feel.

We are evaluating Superset as an analytics platform for our customers so will need to customize for each of them. Thinking on some sort of Multitenancy so look&feel should be defined at ""role"" level..

BTW What's the difference/relation between: basic.html, base.html, baselayout.html ?

Thanks

--
",rusackas,"
--
Tempted to close this in favor of https://github.com/apache/incubator-superset/issues/9123

The theme from `superset-ui` used by Emotion in an increasing number of places, is the current inroads to this goal. Over time, there will be less LESS (heh) and more styles using this dynamic theme, which can be overridden or extended in the <ThemeProvider> components in use.

Any objections to closing this one @rasmi-ranjan-guavus ?
--
",,,,,,
6814,OPEN,[SIP-16] Proposal for loading additional polygon encodings,sip,2019-07-10 19:44:03 +0000 UTC,betodealmeida,In progress,,"# Motivation

When analyzing geo spatial data, a common work flow consists of grouping the data across different spatial dimensions, e.g., ZIP code, city, state or country. In order to visualize the results, a user can select either the ""Country Map"" or the ""deck.gl Polygon"" visualizations. Neither of them are appropriate for exploring the data across different hierarchical regions: the former is limited to country only, while the latter requires all shapes to be pre-joined (except for geohash), adding duplicate data to the data source.

# Proposed Change

## Current workflow

In this SIP I propose a way of adding new encodings to the ""deck.gl Polygon"" visualization. Currently, it supports:

- Polyline
- JSON
- geohash (square)

The first two require the shape to be present in the datasource, while the third one is computed the shape on the fly based on a column. In order to explore a dataset across a spatial hierarchy, data would have to pre-joined with all polygons in the datasource, eg:

timestamp | country | country_polygon | state | state_polygon | city | city_polygon
-- | -- | -- | -- | -- | -- | --
1549410628 | US | {...} | CA | {...} | San Francisco | {...}
1549410675 | US | {...} | CA | {...} | San Francisco | {...}

This is clearly inefficient.

## Proposed workflow

I propose an alternative workflow where **the polygon shape is joined in the Python backend** (`viz.py`). This is similar to how the geohash encoding currently works: it's computed on the fly by the Python backend based on the value of a column, and sent to the frontend in the payload. The approach described here has been used at Lyft for US and Canada postal codes for more than 6 months; see https://github.com/apache/incubator-superset/commit/9c10547f19b628e81cbcd6e1fbac86a70ea510be for the US ZIP code implementation.

Note that the current approach for geohash is still inefficient, since it sends the joined data to the browser. When a granularity is selected, enabling the play slider, this results in duplicate data being sent. It's better to send the polygon shapes in a separate attribute of the payload, and perform the join on the browser instead.

In the new workflow, users will be able to specify new encodings in `config.py` (or `superset_config.py`). Each encoding is defined by an adapter class, responsible for serializing to JSON the shape associated with the region column. Eg, if the geohash visualization type didn't exist we would implement it in the proposed system as follows:

```python
import geohash

from superset.polygon import PolygonEncoding


class GeohashEncoding(PolygonEncoding):

    name = 'geohash (square)'

    def to_location(codes):
        for code in codes:
            lat, lon = geohash.decode(code)
            yield lon, lat

    def to_polygon(codes):
        for code in codes:
            p = geohash.bbox(code)
            yield [
                [p.get('w'), p.get('n')],
                [p.get('e'), p.get('n')],
                [p.get('e'), p.get('s')],
                [p.get('w'), p.get('s')],
                [p.get('w'), p.get('n')],
            ]
```

This would be registered in `config.py`:

```
FEATURE_FLAGS = {
    'EXTRA_POLYGON_ENCODINGS': [GeohashEncoding],
}
```

Other adapters might perform database queries in order to fetch the polygon associated with each value, which is why the methods take a list of codes for efficiency. At Lyft we cache the shapes, fetching from the database only values that are missing.

# New or Changed Public Interfaces

This SIP affects only the ""deck.gl Polygon"" visualization type. The Python backend will use the adapter classes, and the frontend will display new encodings. Here's a screenshot showing US ZIP codes and Canada FSAs:

<img width=""258"" alt=""screen shot 2019-02-05 at 4 34 02 pm"" src=""https://user-images.githubusercontent.com/1534870/52313592-e14e0a80-2963-11e9-8faf-0e50ec684eaa.png"">

Even though this is a small feature, one of the reasons of why I'm proposing this as a SIP is because it introduces new logic to `viz.py`, and I'm unsure how that will affect embeddable components. ( @xtinec @kristw @williaster)

# New dependencies

No new dependencies are needed.

# Migration Plan and Compatibility

Not necessary.

# Rejected Alternatives

My initial implementation of a visualization for ZIP codes was as a custom new visualization. This was hard to manage (in part because of merge conflicts) and redundant, requiring a lot of duplicate work as features were added to the ""deck.gl Polygon"" visualization. At some point last year I merged the functionality into the deck.gl visualization.

## Future work ##

There was a discussion between me and @mistercrunch where we considered creating ""spatial columns"" in the datasource configuration, similar to how metrics or derived columns can be created. Eg, a datasource with these 4 columns:

- pickup_lat
- pickup_lon
- dropoff_geohash

Would be configured to have 2 spatial columns: ""pickup"", composed from `pickup_lat` and `pickup_lon`, and ""dropoff"", derived from `dropoff_geohash`.

If we had that mechanism for spatial columns in place, it would be useful if we could load a series of polygons using the CLI, eg:

```
# this downloads 2 GBs of data and stores in the main database
$ superset load_polygon US_zip
```

Then in the spatial configuration the user would be able to select a column and mark it as of type ""US_zip"", and the ""deck.gl Polygon"" visualization would just work. We could provide a list of common polygons (ZIP, city, state, country), and users would be able to load their own. This way, the ""Country Map"" visualization could be deprecated in favor of the deck.gl one.

The downside of this approach is that the shapes would be stored in the main database, which might not be inefficient. At Lyft we use Postgres with GIS extensions for the US ZIP codes, but MySQL for the main database.",,,betodealmeida,"
--
Ping: @mistercrunch
--

--
@mistercrunch I'll try to find some open datasource with ZIP to polygon mapping, so we can have a functional example in `config.py`.
--

--
@kristw:

>> data would have to pre-joined with all polygons in the datasource

> I am not sure I fully understand the data. What is each data point? Is it individual data point or aggregated? Could you give a more concrete example?

Imagine you want to look at a metric per ZIP code per hour:

```sql
SELECT COUNT(), zipcode, zipcode_geojson
FROM table
GROUP BY DATE_TRUNC(time, ""hour""), zipcode, zipcode_geojson
```

Even though `zipcode` and `zipcode_geoson` are time-independent, they are repeated in the dataset for each hour. The polygons might be complex shapes, so this is a lot of duplicate data.

Instead, we just pass the ZIP code itself:

```sql
SELECT COUNT(), zipcode
FROM table
GROUP BY DATE_TRUNC(time, ""hour""), zipcode
```

There's still some duplicate data, but now it's 5 bytes per row only.

> What does `geohash.decode` and `geohash.bbox` do? What is the `code` argument?

These are functions from the `geohash` module. The first one decodes a geohash into a lat/lon pair, the second one into a bounding box, IIRC. `code` is the geohash code, something like `9q8yyu`. Let me know if you have suggestions for a better name, it's something that should represent a geohash, a ZIP code, an FSA code.

> Regarding `viz.py` and embeddable components. Most of the visualization `js` code will be move out of this repo as individual `npm` package plugin. I haven't moved the `deck.gl` directory but could do so after the first round or altogether. Ideally, they should be moved too. This may increase the overhead in development a bit but make them embeddable and keep the main repo lean.

+1 on this.

> The backend logic for legacy charts can still live in `viz.py` and be updated in the meantime but no new chart should add logic to `viz.py` and we should gradually reduce the code from this file.

+1 on this, but I'm curious if we have a plan to handle cases like this, where some of the logic lives in the backend.
--

--
@kristw I think I can implement the logic in a custom viz instead. I agree we shouldn't keep adding logic to `viz.py`.
--
",mistercrunch,"
--
LGTM overall, a nice next step beyond this would be for Superset to provide a geo shape store internally, but it's it could be bolted on top of this interface. It could also be considered out-of-scope for the project. 

In the meantime it'll be nice to provide good snippets/examples in `config.py`. 
--
",kristw,"
--
> data would have to pre-joined with all polygons in the datasource

I am not sure I fully understand the data. What is each data point? Is it individual data point or aggregated? Could you give a more concrete example? 

What does `geohash.decode` and `geohash.bbox` do? What is the `code` argument?

Regarding `viz.py` and embeddable components. Most of the visualization `js` code will be move out of this repo as individual `npm` package plugin. I haven't moved the `deck.gl` directory but could do so after the first round or altogether. Ideally, they should be moved too. This may increase the overhead in development a bit but make them embeddable and keep the main repo lean. 

The backend logic for legacy charts can still live in `viz.py` and be updated in the meantime but no new chart should add logic to `viz.py` and we should gradually reduce the code from this file. 
--
",thunter009,"
--
This is something that would greatly benefit my use case and would be happy to help push this feature along @betodealmeida wherever help is needed. Seems like identifying open source polygon encodings could be useful?
--
",,,,
6768,OPEN,config superset for local time zone?,global:timezone,2021-02-07 05:25:45 +0000 UTC,Manoj1881,In progress,,"
Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->
Make sure these boxes are checked before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same open issue and I haven't found any solution.


### Superset version
superset --version
Flask 0.12.4
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 12:04:33)
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]

### Expected results
querying from superset is still in UTC even after changing DRUID_TZ in  config.py to DRUID_TZ = tz.tzlocal() 
",,,srggrs,"
--
I think that when you query Druid with timezone aware, e.g.
`{
""granularity"": {
    ""type"": ""period"",
    ""timeZone"": ""Australia/Sydney"",
    ""period"": ""P1D""
  },
""intervals"": ""2019-01-25T00:00:00+11:00/2019-02-01T00:00:00+11:00""
}`

Druid will give back result with this timestamps:

> 2019-01-25T00:00:00+11:00 xxxxx
2019-01-26T00:00:00+11:00 xxxxx
...

and pandas will convert them to UTC in any case (see https://github.com/apache/incubator-superset/blob/adc9a6b495d42c0e1d35dff5d79ef5534b77a320/superset/viz.py#L197-L241), therefore you need to convert back in the localtimezone, which is not done automatically. The only thing that it is done, it's using a an hour offset defined in the datasource properties, and applied here:
https://github.com/apache/incubator-superset/blob/adc9a6b495d42c0e1d35dff5d79ef5534b77a320/superset/connectors/druid/models.py#L1308-L1330
--
",Manoj1881,"
--
Thanks for the reply @srggrs 
I am in IST with 5 hours and 30 min offset and hour offset is only accepting integers,

I think it should accept float value.

or
the timestamp should be converted back to local.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.

--
",TheRum,"
--
@Manoj1881 I am also having same issue. Did you find any workaround to this ?

--
",med4it,"
--
Same issue, is there any way to change superset timezone from the UI ?
--
",,
6747,OPEN,Stateless Authentication which relies on only Bearer token?,.security; enhancement:request,2020-08-04 20:35:14 +0000 UTC,dbsheta,Opened,,"
Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->
Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [x] I have reproduced the issue with at least the latest released version of superset.
- [x] I have checked the issue tracker for the same issue and I haven't found one similar.

Currently, Superset uses session-based authentication where the cookie is sent with each request. Can we have stateless authentication i.e the backend will not store any session and will rely on access-token sent in Header by the client? eg. Header: ```Authorization: Bearer eerkjerkcwk.....```",,,brentEich,"
--
@dbsheta @kristw Have you had any success with this? I'm also looking to implement something of this nature.

--
",syazwan0913,"
--
@dbsheta You got any update on this?
--
",nishantmonu51,"
--
any updates here ? Is anyone working on this ?
--
",willbarrett,"
--
@dbsheta the recommended path forward here is for users to implement their own custom Security Manager to support the requested functionality. Please see the documentation on [Security in Superset](https://superset.incubator.apache.org/security.html) and [Security in Flask App Builder](https://flask-appbuilder.readthedocs.io/en/latest/security.html) as well as [Superset's security manager class](https://github.com/apache/incubator-superset/blob/master/superset/security/manager.py)
--
",,,,
6681,OPEN,[SQL Lab] Reduce number of queries needed to create a chart,.pinned; enhancement:request; org:lyft; sql_lab,2020-09-23 07:01:35 +0000 UTC,vylc,Opened,,"The number of query executions needed to get from SQL->Dashboard is inefficient. Stresses databases (more redundant query executions), slows down data analysis (waiting for queries to complete)

Typical workflow to create a chart
1st SQL run --> SQL Lab to execute query and get result sets
2nd SQL run --> Clicking Explore, which instantiates the query as a datasource off of the result set
3rd and more SQL runs --> Changing viz type (because user lands on a count of # of rows upon clicking Explore) and subsequent changes on the datasource all require re-execution

Proposal: 
(1) temporarily cache result sets for SQL queries run on a tab? Can be quickly discarded based on last query run on a tab. Persist as long as tab remains open or something reasonable like 24 hours?
(2) If user click on Explore, persist result set in cache, such that subsequent actions don't require re-execution

Open to other suggestions which solve the problem of too many SQL re-executions in order to explore/chart a result set which remains static.


Make sure these boxes are checked before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if there are any.
- [ ] I have reproduced the issue with at least the latest released version of superset.
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar.


### Superset version


### Expected results


### Actual results


### Steps to reproduce


",,,stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",john,"
--
This is pretty relevant.  More generally, the results of an SQL query should probably be savable as a data source from the ""save as"" option.  Then can just use alongside any database table as basis for charts.  
--
",amathurcx,"
--
+1
--
",,,,,,
6568,OPEN,Make line chart support non-temporal variable for x-axis,.pinned; enhancement:committed; org:lyft,2021-02-07 02:20:29 +0000 UTC,snehalkale41995,Opened,,"The x-axis need not necessarily be a time/date, it could just be any number. Would be great to make that possible!


",,,davidhassan,"
--
I like this idea as then I could display threshold lines.
--
",mlboy,"
--
+1
--
",vylc,"
--
@hughhhh 
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.

--
",move,"
--
This issue was moved by [kristw](https://github.com/kristw) to [apache-superset/superset-ui-plugins#51](https://github.com/apache-superset/superset-ui-plugins/issues/51).
--
",Cyprien,"
--
Hello,
Is this feature implemented today ?
--
"
6132,OPEN,[SIP-13] Proposal for Code Review Process,sip,2019-04-19 18:22:24 +0000 UTC,kristw,In progress,,"## Motivation
The Superset community has a large number of contributors and stakeholders, which leads to many people trying to modify the codebase at the same time. 

For project maintainer this introduces a lot of challenges:

- **A large number of issues and pull requests**, which require significant amount of time to triage, review and shepherd until merging. As of Oct 15, 2018, there are 864 open issues and 161 open PRs. Many of these are a few years old and remain in limbo.
- **Keeping the code base healthy.** It is great to receive many community contributions, but the overall project stability and code quality must come first. If the PR idea/work is interesting, but the quality does not match the standard or introduces risky changes without justified reasons, the project maintainers have a hard time handling these PRs. With the proposal for frequent release plan ([SIP-12](https://github.com/apache/incubator-superset/issues/6131)), master stability becomes even more important. 
- **Merging a PR that ends up breaking master branch.** Old community PRs that are finally merged sometimes break the master branch (commonly CI) because they are not rebased against master and tested again before merging. The responsibility then falls to the maintainer to resolve the broken state. 

For contributors, these are the common questions:

- **Who will review my PR?** Currently, it is unclear who should participate in the code review of each PR. Contributors sometimes manually tag github username of project maintainers for reviews. This does not work well for beginners who do not know anyone, perhaps tagging @mistercrunch. Even experienced contributors can have this issue when touching the code in different sections of the project. It is also a manual process.
- **When will someone review my PR?** There are several uncertainties around the code review process. Some are implied (e.g.,no review until the PR pass all CI tests, PRs with [WIP] in the title are considered work-in-progress and will not be reviewed, etc.). However, there are many PRs that are not ready for review, but do not have [WIP] and cause confusion. 
- **When will someone merge my PR?** After PRs are approved, sometimes they are left in that state for a while. It is not explained when it will be merged. This left the contributors hanging. 
- **Which release will include my PR?** Currently, it is unclear when will the change make it to an official release, leading to different organizations maintaining their own branches. Please also see a proposal for release plan in [SIP-12](https://github.com/apache/incubator-superset/issues/6131) for more details. 

To be more efficient at maintaining Superset and improving it in the long-term, we need more workflow clarity that will increase code quality and improve project stability. This proposal wishes to reduce operational workload for the project maintainers, set the right expectations for everyone and make the project keep growing with happy community.  

## Proposed Changes

### [P.1] Project maintainers can request an issue to be created before a PR is reviewed. 

A philosophy we would like to strong encourage is

> ""Before creating a PR, create an issue.""

We would like to update the contributing guidelines for Superset as follows. 

>  **Bug fixes:** If youre only fixing a small bug, its fine to submit a pull request right away but we highly recommend to file an issue detailing what youre fixing. This is helpful in case we dont accept that specific fix but want to keep track of the issue. Please keep in mind that the project maintainers reserve the rights to accept or reject incoming PRs, so it is better to separate the issue and the code to fix it from each other. In some cases, project maintainers may request you to create a separate issue from PR before proceeding. 
>
> **Refactor:** For small refactors, it can be a standalone PR itself detailing what you are refactoring and why. If there are concerns, project maintainers may request you to create a SIP for the PR before proceeding. 
>
> **Feature/Large changes:** If you intend to change the public API, or make any non-trivial changes to the implementation, we requires you to file a new issue as SIP (Superset Improvement Proposal). This lets us reach an agreement on your proposal before you put significant effort into it. You are welcome to submit a PR along with the SIP (sometimes necessary for demonstration), but we will not review/merge the code until the SIP is approved. 
> 
> In general, small PRs are always easier to review than large PRs. The best practice is to break your work into smaller independent PRs and refer to the same issue. This will greatly reduce turnaround time. 
> 
> Finally, never submit a PR that will put master branch in broken state.

### [P.2] Create multiple new templates for new issues/PRs

Currently we only have an issue template but not a PR template.

To facilitate the issue and PR creation, we will provide templates for different scenarios. Github supports multiple templates which users can choose from a menu when creating. 

- [Creating issue templates](https://help.github.com/articles/creating-issue-templates-for-your-repository/)
- [Multiple issue and pull request templates](https://blog.github.com/2018-01-25-multiple-issue-and-pull-request-templates/)
- [Ansible](https://github.com/ansible/ansible/tree/devel/.github) has good examples of templates

![image](https://user-images.githubusercontent.com/1659771/47106355-2c98ad80-d1fb-11e8-8901-79aff47e02af.png)

An issue will be one of the following types:

1. Question
2. Bug
3. Feature request
4. SIP (Superset Improvement Proposal)

A PR will be one of the following types

1. Bugfix - Fix bugs 
2. Feature - Add new features
3. Refactor - Refactor code to improve code quality
4. Test - Add new test cases

A PR should generally includes the following:

- Summary 
- Before/after screenshots or animated GIF (when applicable)
- Test Plan 
- Checkboxes
  - Has associated issue(s):
  - Changes UI
  - Includes screenshot
  - Requires DB Migration
  - Introduces new feature or API
  - Removes existing feature or API

### [P.3] Pre-define reviewers with CODEOWNERS

Github allows us to list mandatory reviewers for each directory in a [CODEOWNERS](https://help.github.com/articles/about-codeowners/) file. When a new PR is opened, the corresponding reviewers will be assigned to review that PR automatically.  Mandatory reviewers do not have to be committers.

CODEOWNERS will be defined as a [team](https://help.github.com/articles/about-teams/). A team contains one or more people, such as (`python-reviewers`, `js-reviewers`, `vis-reviewers`) [See an example.](https://github.com/python/cpython/blob/master/.github/CODEOWNERS)

Note: CODEOWNERS requires changing github repo settings (via Apache) to enable protected branch option. Teams are defined in organization settings. Need to investigate if it has to be the same github organization (Apache) to define the teams. Fallback plan is using another github org or macro/alias.

### [P.4] Define a protocol when a PR is in progress

#### Authoring

- Fill in all sections of the PR template.
- Add label WIP if not ready for review (WIP = work-in-progress)
- Add category label: bugfix, refactor, test or feature
- Remove label WIP and add READY when ready for review
- Changes to user interface require before/after screenshots, or GIF for interactions
  - Recommended capture tools (Kapture, LICEcap, Skitch)
  - If no screenshot is provided, the mandatory reviewers will mark the PR with missing:screenshot label and will not review until screenshot is provided.
- Reviewers will not review the code until all CI tests are passed. Sometimes there can be flaky tests. You can close and open PR to re-run CI test. Please report if the issue persists and rebase your PR after the fix has been deployed to master. 
- If the PR was not ready for review and inactive for > 30 days, we will close it due to inactivity. The author is welcome to re-open and update. Will set up a [stale bot](https://github.com/probot/stale) to do this.

#### Reviewing

- Triage and add labels: Here are some example labels. Please keep in mind that these are examples. The final list of labels and adding a bot to add them will become action items if this SIP is approved.
  - Missing: screenshot, issue, test
  - Language: Javascript, Python, Others
  - Components: Dashboard, SQL Lab, Explore, Charts
- Use constructive tone when writing reviews. 
- If there are changes required, state clearly what needs to be done before the PR can be approved.

#### Merging

- At least one mandatory reviewer is required for merging a PR. 
- A PR with label READY and approved will be merged by a committer. The committer adds label(s) of release branches to go into, i.e., `release/1.1`, `release/1.2`. (See [SIP-12](https://github.com/apache/incubator-superset/issues/6131))
- After the PR is merged, [close the corresponding issue(s)](https://help.github.com/articles/closing-issues-using-keywords/). 

#### Post-merge Responsibility

- Project maintainers may contact the PR author if new issues are introduced by the PR.
- Project maintainers may revert your changes if a critical issue is found, such as breaking master branch CI.

#### Note about labels

Non-committers cannot assign label by default. We will setup bots that allow contributors to add whitelisted labels via bot. User can type in comments such as

```
-label WIP 
+label READY
```

If this does not work, we can fallback to adding `[WIP]` or `[READY]` to PR titles.

### [P.5] New changes to CI

Here are a few proposed improvements to the CI

#### [P.5.1] Add mandatory reviewers hook
The mandatory reviewers are defined in CODEOWNERS file

#### ~~[P.5.2] Enable Stale branch check.~~

~~**Pros**~~

~~- Prevents broken master when a stale branch is approved and merged.~~
~~- Parallelized merging of PRs.~~

~~**Cons**~~

~~- Serialized merging of PRs impies adding overhead (having to re-sync either via the UI or CLI) and rerun the CI.~~
~~- Improvements in CI performance would help reduce some of the burden.~~

~~Note: Require this change to the github repo settings~~

![image](https://user-images.githubusercontent.com/1659771/47108521-b8f99f00-d200-11e8-9604-c3920e322b0d.png)

#### [P.5.3] Automatic checks for code coverage project/patch 
Similar to [scikit-learn](https://github.com/scikit-learn/scikit-learn/blob/master/.codecov.yml), this enforces the contributing guidelines of unit testing. Commits pushed to master should not make the overall project coverage decrease by more than 1%:

This will be enabled, though initially as a non-required check as we iterate on the configuration settings.

Note: These CI changes require Apache admin changes to the Github repo. 

## Action items

Please comment and refer to [P.x.x] if you have any suggestion.

If this SIP is approved, here are the follow-up tasks.

- Update CONTRIBUTING.md
- Create new issues and PRs templates
- Create CODEOWNERS file, teams, and list mandatory reviewers
- Update CI configuration
- Setup bots for handling labels
- Setup a bot for closing stale issues and PR
- Triage the current PRs and issues. 

## References

- [React.js contributing guidelines](https://reactjs.org/docs/how-to-contribute.html)
- [Node.js contributing guidelines](https://github.com/nodejs/node/blob/master/doc/guides/contributing/pull-requests.md)
- [Ansible bots](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md#cmd-bot_broken)
- [Gitissue bot](https://github.com/foosel/GitIssueBot)
- [Probots](https://github.com/search?q=topic%3Aprobot-app+org%3Aprobot&type=Repositories)

@betodealmeida @mistercrunch ",,,mistercrunch,"
--
This is all super great. Some thoughts:
* this seems to very much reflect the general consensus, nothing too controversial here as far as I'm concerned
* guidelines or hard rules? I'm siding towards guidelines unless we can actually enforce with tooling (github or GH integrations)
* I'm not sure whether not-rebasing prior to merge is a real common issue (how many times did that really happen, and how much work is it to fix a basic merge issue like import ordering?), I'd rather deal with the occasional issue with hard reverts than making everyone constantly rebase. I'm thinking code should settle quite a bit after this huge round of refactor that's been taking place (py2 deprecation, major JS restructuring, plugins, ...)
* happy to give code owners a try (haven't used it in the past)
* I used a stale bot in the past, but it was a bit of a manual process (running a script, used my Github personal API token ...). I really think we need this. I'm not sure if Apache will let use probot, they have strict restrictions on giving perms to outside orgs. I can dig out the info as to the thing I used in the past, but it wasn't great...
--

--
Reminder that part of the SIP process should be to notify the Apache Superset dev@ mailing list.
--
",kristw,"
--
> * this seems to very much reflect the general consensus, nothing too controversial here as far as I'm concerned
> * guidelines or hard rules? I'm siding towards guidelines unless we can actually enforce with tooling (github or GH integrations)

Glad to hear that. We intend for this to be a written-down version of the unwritten agreement of how it currently work, with some more explicit instructions of what to do, so there will be less surprise why something is not reviewed. 

> * I'm not sure whether not-rebasing prior to merge is a real common issue (how many times did that really happen, and how much work is it to fix a basic merge issue like import ordering?), I'd rather deal with the occasional issue with hard reverts than making everyone constantly rebase. I'm thinking code should settle quite a bit after this huge round of refactor that's been taking place (py2 deprecation, major JS restructuring, plugins, ...)

We can extract this bullet point `[P.5.2]` into a separate issue if the community has different opinions about it. 

> * happy to give code owners a try (haven't used it in the past)

I have never used CODEOWNERS specifically, but used OWNERS file per directory at my previous job. This can automate some manual taggings we are doing in PRs and is useful for maintaining large codebase in general. An alternative is welcome too. We choose CODEOWNERS in the proposal because it is already built-in by github.

> * I used a stale bot in the past, but it was a bit of a manual process (running a script, used my Github personal API token ...). I really think we need this. I'm not sure if Apache will let use probot, they have strict restrictions on giving perms to outside orgs. I can dig out the info as to the thing I used in the past, but it wasn't great...

Your experience with this will be super helpful. 
--

--
Remaining tasks are

* Create CODEOWNERS file, teams, and list mandatory reviewers. (May consider using the `pull-approve` bot instead because it allows more flexible way to define reviewers.)
* Update CI configuration 
* Setup bots for handling labels
--
",michellethomas,"
--
I created a pull request template to follow up on `[P.2] Create multiple new templates for new issues/PRs`. Having multiple Pr templates is difficult at this point because you need to use the `?template=` query param to use them, so I just created one PR template for now, but once they improve that feature we can add multiple as you suggested.
--
",,,,,,
6131,OPEN,[SIP-12] Proposal for Branch Management and Release Process,sip,2019-03-22 23:26:47 +0000 UTC,john-bodley,Opened,,"## Motivation

Historically organizations partaking in the release process have managed their own fork on incubator-superset which is deployed in production via a Git submodule. Both companies have been actively developing Superset and deploy internally (features and bug fixes) at a higher cadence than the public PyPI releases. These forks came about because of the potential instability of the master branch (see [SIP-13](https://github.com/apache/incubator-superset/issues/6132)) and provided a mechanism to cherry-pick specific commits thus ensuring an increased stability. 

These organizations have discussed ways of trying to fold these forked branches back into the incubator-superset repo to better ensure consistency and to serve as a mechanism for formalizing the release process which is somewhat sporadic.

## Proposed Change

The proposed solution is based primarily on having release branches where certain organizations are the primary beta testers for stabilizing the branch. 

### Master branch

There will be no changes to how the `master` branch currently works. We will do our best to keep the `master` branch healthy in part by the initiatives outlined in [SIP-13](https://github.com/apache/incubator-superset/issues/6132). If you create a pull request, please do it against the `master` branch. We will maintain stable branches for releases separately but we dont accept pull requests to them directly. Instead, we will cherry-pick non-breaking changes from `master` to the two latest releases.

### Release Cadence

We propose a regular release cadence of every **two weeks**. This period is a tradeoff between stability and flexibility ensuring that new features can be deployed within a reasonable time frame. Furthermore the longer the release cadence the greater the potential conflict when cherry-picking commits. A release will be deployed two weeks after it was cut.

We propose that each release branch is fully managed by a single organization.

### Branch Naming

We propose that all Git branches follow the `<prefix>/<suffix>` naming convention:

- Personal: `<username>/<issue>`, i.e., `jane-doe/issue-123`
- Feature: `feature/<name>`, i.e., `feature/dashboard-v2`
- Release: `release/<version>`, i.e., `release/1.1`

Note if `/` is troublesome we suggest using a two hyphens (`--`) instead.

### Semantic Versioning

Superset follows semantic versioning (`<major>.<minor>.<patch>`) although the major version has remained fixed at zero. We should release patch versions for bug fixes, minor versions for new features, and the major version for breaking changes. When we make breaking changes, we should also introduce deprecation warnings in a minor version so that our users learn about the upcoming changes and migrate their code in advance.

Git release branches represent the `<major>.<minor>` level whereas Git tags (associated with a specific commit) represent the fully defined `<major>.<minor>.<patch>` version. Note tags may contain an optional release candidate suffix (`-rc`). 

### The Git Ecosystem

The following diagram describes the `apache/incubator-superset` Git ecosystem and how we plan to couple this with several organization's internal deployment (superset-internal which uses a Git submodule) which will help serve as a mechanism for stabilizing a release.

![release](https://user-images.githubusercontent.com/4567245/47102523-d96e2d00-d1f1-11e8-9291-a9addf007bac.png)

#### Day 0

On day 0 the next release branch (`release/1.1`) will be cut from `master` which references the latest commit. The commit will be tagged with `v1.1.0-rc` (the v1.1.0 release candidate) and deployed to PyPI. 

#### Day 0 - 13

Only commits associated which resolve bugs present in the release candidate will be cherry-picked onto the release branch. Fixes to features added after the cut will not be cherry picked (). Note there may be potential merge conflicts (which will have to be resolved) when cherry-picking a fix due to an upstream commit which is not in the release (). If this is too complex the cherry-pick should be skipped (---) as it will be included in the next release. New features can be committed on `master` **if and only if** theyre defined in their entirety within the two week cycle. Larger features should be developed on a feature specific branch (which is then merged into `master`) or using feature flags. 

The following diagram shows the use of a feature branch which spans multiple releases.

![feature](https://user-images.githubusercontent.com/4567245/47102567-eee35700-d1f1-11e8-8b1b-3de48729ebc3.png)
 
#### Day 7

Organizations partaking in the release process will update the Git submodule reference to point to the release (`release/1.1`). This seven day lag provides a soak period of ensuring that critical bugs are not deployed to our production environments. The update is made to the `.gitmodules` file,
```
[submodule ""incubator-superset""]
    path = incubator-superset
    url = https://github.com/apache/incubator-superset
    branch = release/1.1
```
which ensures it tracks the correct release branch. Like the two week release process the submodule branch will be updated every two weeks. 

#### Day 7 - 20

Periodically organizations partaking in the release process will ensure that the submodule in their internal deployment will reference the latest commit from the remote branch (which contains all the cherries) via,
```
git submodule update --remote --recursive --merge 
```
which will fetch the latest changes for each submodule, merge them in, and check out the latest revision of the submodule. This step allows us to further test and stabilize the branch in a production environment.

#### Day 14

The first (non release candidate) version will be tagged (`v1.1.0`) and deployed to PyPI which references the latest commit on the branch (*). Simultaneously the next release branch (`release/1.2`) will be cut from `master` which will mimic `v1.1.0` except it will also include any feature commits which were merged in the past 14 days. Note a new release branch is cut if and only if feature commits were added, to prevent duplication. 

The following diagram shows an example where on day 14 a new release branch is not cut as there were no new features added since day 0. The next viable release cut date will then be day 28 to preserve the release cadence.

![no-release](https://user-images.githubusercontent.com/4567245/47102650-24884000-d1f2-11e8-8039-e01e55343e17.png)

#### Day 14 - 27

Additional bug fixes to commits in the release candidate will be cherry-picked onto the _n_ most recent releases (where _n_ = 2, i.e.,` release/1.1` and `release/1.2`). Throughout this period the tag can be updated (incrementing the patch) as desired based on the severity of the fix.

#### Day 28 -

Assuming there is a new release branch (`release/1.3`) we will no longer be picking cherries onto `release/1.1` since we only support two releases (`release/1.2` and `release/1.3`). In summary most release branches will have a four week active lifespan. 

Branches should exist for infinitum to ensure critical security fixes can be patched to the necessary branch.

#### Database Migrations

Database migrations (which are handled via [Alembic](https://alembic.zzzcomputing.com/en/latest/)) have an additional lineage component (used to determine upgrades and downgrades) which further complicates matters. We need to ensure the playback of versions is correct when upgrading to a newer release and that a downgrade is never necessary. 

The logic for cherry-picking migrations is similar to above whereas migrations can be cherry-picked onto a release branch **if and only if** all of its ancestors are present on the branch. Given that there no notion of resolving merge conflicts in Alembic, any migration (and thus future migrations) related to a feature will be skipped (`--`) and included in the next release.

## Action Items

Currently we plan on manually cherry-picking the commits from `master` to the desired release branch. This could be automated in the future based on PR tags and/or the assistance from a bot.

We also should check with Apache whether there are any concerns/feasibility issues with releasing on a two week cadence.

@beto @mistercrunch 
",,,mistercrunch,"
--
A few thoughts:
* per ASF guidelines, we should not name orgs here (Airbnb/Lyft), but maybe ""organizations partaking in the release process""
* per ASF process, part of the process should involve a [VOTE] thread on the Apache mailing list, perhaps not every release branch needs an Apache release, but every Apache release needs a [VOTE] thread
* I'm still planning on working on tooling to help with all this mostly around baking release based on a base ref along with Github labels on PR. Cementing the process in this SIP will help dictate what the tooling should do

--

--
Few other thoughts:
* release and collaborative feature branches are collaborative and history should be preserved (idk if we can enforce this as GH protected branches base on a wildcard `release*` or something like that)
* currently we don't use `feature/*` branches in favor of branches on forks, I think that's common and fine. `feature/` branches on the main `apache` repo would be used when people would need to collaborate on a branch, enforce a merge flow (no rebase or history changes on collaborative branches!) and somehow would need to get rebased / squashed prior to merging on master
* maybe we need another section on Apache and/or Pypi (convenience) releases.
--

--
Reminder that part of the SIP process should be to notify the Apache Superset dev@ mailing list.
--
",john,"
--
@mistercrunch I've removed all reference to specific organizations. I also agree that not every release branch needs to be an Apache release and there's definitely merit in cutting new release branches at a regular cadence.
--
",williaster,"
--
> currently we don't use feature/* branches in favor of branches on forks, I think that's common and fine. feature/ branches on the main apache repo would be used when people would need to collaborate on a branch, enforce a merge flow (no rebase or history changes on collaborative branches!) and somehow would need to get rebased / squashed prior to merging on master

Note that we did have a feature branch for `dashboard-v2` within the Apache repo pretty much as @john-bodley depicts above^. @graceguo-supercat and I PR-ed into it. We did actually have to re-base somewhat regularly to say in sync with `master`, but we coordinated a lot about this. I think this worked out okay.
--
",ShaneCurcuru,"
--
> * per ASF process, part of the process should involve a [VOTE] thread on the Apache mailing list, perhaps not every release branch needs an Apache release, but every Apache release needs a [VOTE] thread

Every Apache Superset release (i.e. software product released publicly) **must** have a passing [VOTE] thread on the dev@ (or other appropriate Apache) list.  You can certainly manage branches however the PPMC thinks works best but **only** the PPMC can vote on Apache Superset releases.

As a corollary, other organizations may **not** release software products that use Apache trademarks.  It's not clear from this thread if some of the major contributors here were planning to do their own Superset releases or not, which is why I mention it.  Note I'm not involved in the project, so apologies if I'm misunderstanding your process.

--
",michellethomas,"
--
Superset Deployment Process Update
Sticking to a schedule of a deployment every two weeks has been challenging, and @mistercrunch  proposed another schedule that is slightly more flexible and would allow for more time for bug fixing. When we sort out what we need to do to consider these valid releases by ASF standards, we can start trying out this process.

Proposed process:

1. Always cut on Monday (rc0 and release). If it is a public holiday, postpone to the next business day.
Take a week for testing and bug fixing
2. Cherry pick bug fixes for rc1.
       a. Decide if the release is stable enough to deploy. A release would be stable if it has no known major regressions or new bugs blocking critical functionality.
       b. If not:
       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i) If theres something blocking the release thats easy to revert, well revert it
       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ii) Go back to step 2 for rc2
3. At week 2 if it is not releasable, identify the main blocking issues
4. At week 3 revert the blockers if not resolved and start a [Vote] email
5. Tag the release as release 1.1
6. Continue to fix bugs on this release until the next release goes out.

![image](https://user-images.githubusercontent.com/817955/49607288-6019cd80-f94a-11e8-989b-2882bb167073.png)

Well cut 1.1.0 two weeks after 1.0.0 is cut, and go through the same process. There should not be more than two branches open at a time so 1.2.0 will not be cut until the following Monday after 1.0.0 goes out. If maintaining two branches is too complicated, we may just have one.

A branch shouldnt take more than 4 weeks to be released because blockers should be reverted at week 3.

The goals of this process

- Those involved in feature development have enough time to find, and triage issues. Ideally all contributors would be deploying these changes to a test environment and looking for issues.
- Theres enough time so that whoever introduced the regression can address it. They should be communicating with whoever cut the branch when they expect the bug to be fixed. By default the person cutting the branch will not be responsible for fixing all bugs for a deployment, and contributors should maintain ownership of the successful deployment of their features. If a bug does not get fixed in 4 weeks, the feature will be reverted.
- Theres a regular structure and clear process so that we all can efficiently and regularly make deployments.

Through this process committers should:
- Use a risk:xxx tag to mark PRs that need thorough testing. Things that may require risk tag: db_migration, large feature, or changing large or brittle sections of code.
- Tag PRs with the release they should go to
- Tag issues as a bug and which release(s) the bug is in

CC: @john-bodley @kristw 
--
",xtinec,"
--
Does the new proposal imply we'll attempt to 1) release monthly from master and 2) cut release branches from master every other week?

To clarify, in the diagram/example above, `v1.1.0rc2` exists because we reverted certain changes or made bug fixes in the `v1.1.0` branch after `rc1`? If so, do we also apply the same changes in master?

@michellethomas @john-bodley 
--
"
6032,OPEN,[SIP-11] Proposal for deprecating Druid REST connector,.pinned; sip,2020-08-04 13:45:19 +0000 UTC,john-bodley,Opened,,"### Motivation

Superset currently supports two engine connectors for querying datasources; SQLAlchemy and the Druid REST API. The later was the initial use case for Superset, i.e., a UI for visualizing Druid datasources.

Since version [0.10.0](https://github.com/apache/incubator-druid/releases/tag/druid-0.10.0) Druid has included a built-it SQL server which has a SQLAlchemy binding provided by the [pydruid](https://github.com/druid-io/pydruid) library (courtesy of @betodealmeida and @mistercrunch) and thus the proposed change is to deprecate the REST API interface in favor of having a single interface (SQLAlchemy) to all engines. Note all future engines (there has been mentioned of adding support for Elasticsearch) would require a SQLAlchemy dialect. 

There is a non-insignificant amount of overhead in supporting both connectors including:

#### Code

From a code perspective each connector needs to define similar views and models. The [Druid](https://github.com/apache/incubator-superset/tree/master/superset/connectors/druid) connector alone comprises of around 2,000 lines of code. There is additional frontend logic which needs to construct filters, metrics, etc. for both the Druid REST API and SQLAlchemy. Note there are [74](https://github.com/apache/incubator-superset/search?q=druid&unscoped_q=druid) files (including documentation) which reference Druid in the repo. 

#### Models

In addition to code overhead each connector defines its own models and database tables:

Druid: 
- `clusters`
- `datasources`
- `columns`
- `metrics`

SQLAlchemy: 
- `dbs` 
- `tables`
- `table_columns`
- `sql_metric`

which complicates logic, i.e., the `slices` table does not have a SQLAlchemy relationship to a ""datasource"" table as the datasource type determines the association. This results in denormalized tables with potentially incorrect values, i.e., the `slices` table contains the `datasource_name` column for the FAB CRUD views, however this may not accurately reflect the underlying datasource name. 

#### Proposed Change

The proposed change would be to deprecate all the Druid REST logic from the codebase. This significantly simplifies and streamlines a number of facets of Superset by ensuring that all engines connect via a SQLAlchemy dialect.

Currently there is support for syncing/refreshing Druid datasource associated with the REST API connector which I suspect is leveraged by a number of organizations. [SIP-7](https://github.com/apache/incubator-superset/issues/5842) discussing ""refreshing"" of Superset datasources. 

Note this would be a breaking change for any organizing using a Druid version less than `0.10.0`. Also there may be some instances of post-aggregate Druid functions which are not supported in Druid SQL.

#### New or Changed Public Interfaces

There would be no new or changed public interfaces.

#### New dependencies

There would be no new dependencies.

#### Migration Plan and Compatibility

A non-trivial database migration would be required including:
- All records in the Druid tables listed above would need to be migrated to the SQLAlchemy equivalent table.  
- Existing slices would need to be updated to reference the new SQLAlchemy representation of the Druid datasource.
- Re-normalize the `slices` table.
- Update chart data to remove the obsolete `table__` or `druid__` prefixes.

#### Rejected Alternatives

None.

to: @betodealmeida @graceguo-supercat @kristw @michellethomas @mistercrunch @timifasubaa 
",,,kristw,"
--
I am in favor of consolidating and standardizing in general.  
This should reduce bugs and improve maintainability.
--
",mistercrunch,"
--
That will effectively mean deprecating support for pre-good-SQL-support druid versions, which I think is fine. If people want to use old Druid, they can use old Superset with it :)
--

--
@dharamgajera what does Druid SQL lack at this time?
--
",john,"
--
@mistercrunch are you aware of any functionality that the REST API provides (such as phase II queries) that couldnt be captured in Druid SQL?
--

--
https://github.com/apache/incubator-superset/pull/8512 introduced a feature flag which enables the Druid NoSQL connector. Note by default this is disabled. 
--
",kkalyan,"
--
How are Druid dimension extractions/lookup, filtered aggregations and javascript post-aggregators with Druid SQL? Druid users use them heavily. Some of these are not native to SQL, it would be good to support them. 
--
",datametrics,"
--
I use the REST API of Druid heavily. It makes it totally easy to make discovery of datasources as well as to implement Druid clients from other languages. The rest format is platform interchangeable and there is no need to implement any further sql parser / converter logic. One can just throw some model classes together and serialise that to Json. From my point of view it would be great when superset continues with REST support or at least leaves the opportunity for connector injection.
--
",SpyderRivera,"
--
Druid SQL won't support core features like multi-value dimensions until 0.13.1
--
"
5842,OPEN,[SIP-7] Ensuring Superset accurately represents the underlying data warehouse,sip,2019-06-21 00:01:26 +0000 UTC,john-bodley,Opened,,"[SIP] Proposal for ensuring Superset accurately represents the underlying data warehouse

## Motivation

Over time there can be a discrepancy between the actual metadata of a Druid datasource or SQLA table and the corresponding Superset representation. Columns or metrics can be added, updated, or deleted, and thus overtime creep grows between the systems.  The proposal is to try to add functionality to better ensure consistency between Superset and the underlying data warehouses which should help boost user confidence. 

## Proposed Change

Currently there is a mechanism to refresh Druid metadata per the following menu item:

![screen shot 2018-09-07 at 10 48 34 am](https://user-images.githubusercontent.com/4567245/45234785-9d0eef00-b28b-11e8-961b-0de4daf961ba.png)

thus the plan would be to also include an option for refreshing SQLA table metadata. We could optionally also provide a mechanism to scan for new SQLA tables though we should be aware that there scale of datasources (and their corresponding columns/metrics) could negatively impact the performance of Superset.

### Refreshing

Regarding the term ""refreshing"" I proposed the following is instrumented for both Druid and SQLA datasources.

#### Columns/Metrics

- New entities are added
- Existing entities are updated
- Obsolete entities are deleted

Note derived entities which reference an obsolete entity will also be deleted.

For reference here's the current UI behavior. Starting with a baseline:

![screenshot-baseline](https://user-images.githubusercontent.com/4567245/45452127-fca53a00-b691-11e8-831e-9747d1578d34.png)

when one drops a column (`sum_boys` in this instance) from the underlying table the UI state remains unchanged and the query fails to execute (as expected) with the `no such column: sum_boys` error: 

![screenshot-drop-column](https://user-images.githubusercontent.com/4567245/45452125-fca53a00-b691-11e8-8d29-bd0f8ade0c5b.png)

Finally if one deletes the column and corresponding metrics from the Superset datasource, the UI state remains unchanged (also expected as the state is defined in its entirety from the form-data), though the query never runs as Superset rightfully raises an error stating that the metric is non-valid:  

![screenshot-delete-metric](https://user-images.githubusercontent.com/4567245/45452126-fca53a00-b691-11e8-91db-f785bfd3d11c.png)

These behaviors seem correct and _should_ remain unchanged.

#### Datasources

An unanswered question remains about what should happen when a Druid or SQLA datasource is defined in Superset but no longer resides in the underlying data warehouse. One school of thought is that if the underlying Druid datasource or SQLA table no longer exists we should:

- Delete all slices which reference the datasource
- Delete all empty dashboards (if appropriate)
- Delete the datasource

Note I'm unsure what the current logic is for having a dashboard with no slices.

The concern with this approach is it is a fairly destructive process, i.e., significant IP is potentially lost when one deletes a slice. Would it make more sense that the underlying institution controls when/how datasources are deleted? For example one could define a policy that if said datasource has been deleted (and not restored) for _n_ consecutive days then it's probably safe to delete it from Superset.

## New dependencies

The following existing PRs are required which ensures we have uniqueness at the datasource/column/metric level and the appropriate fields are non-nullable:

- ~~https://github.com/apache/incubator-superset/pull/5445~~
- ~~https://github.com/apache/incubator-superset/pull/7084~~
- https://github.com/apache/incubator-superset/pull/5449
- ~~https://github.com/apache/incubator-superset/pull/5451~~
- ~~https://github.com/apache/incubator-superset/pull/5452~~
- ~~https://github.com/apache/incubator-superset/pull/5453~~

Note many of these PRs require migrations which may need manual intervention as the lack of constraints and non-nullability may have resulted in a corrupted (and thus complex) database in which procedurally defined migration rules are non-viable. 

## Open Questions

1. Should we delete datasources which no longer exist in the data warehouse?
2. How do we deal with custom SQL definitions when the underlying datasource changes? I suspect this is out-of-scope.

to: @betodealmeida @michellethomas @mistercrunch @timifasubaa  ",,,kristw,"
--
Should be SIP-7
--
",,,,,,,,,,
5417,OPEN,[Table config] Add SQL query editor functions in SQL Expression box,.pinned; enhancement:request; good first issue,2021-03-27 05:46:52 +0000 UTC,vylc,Opened,,"Just killed myself trying to edit my SQL formula for a metric in the SQL expression box. Would be really helpful to have the syntax checking/linting/autocompletes of columns from the table - especially with end quotes and parentheses in this box.

Make sure these boxes are checked before submitting your issue - thank you!

- [ ] I have checked the superset logs for python stacktraces and included it here as text if any
- [ ] I have reproduced the issue with at least the latest released version of superset
- [ ] I have checked the issue tracker for the same issue and I haven't found one similar


### Superset version


### Expected results


### Actual results


### Steps to reproduce


",,,stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",IndexOutOfRange,"
--
Not sure if this issue is still relevant as, in the sql editor, there are:

-  autocomplete on a lot of stuff 
- auto end quotes 
- auto closing parenthesis

and various others. Am I missing something ?
--

--
Not sure if the original issue was about the the boxes in the new edit datasource react UI popup or the legacy edit datasources jinja form. 

The issue seems older than the creation of the react popup but i am unable to assess how frequently the legacy UI is still in use. 
--
",mistercrunch,"
--
I think this refers to the SQL Expression box(es), either in the ""table editor"" or the ""metric editor"" in the explore view. Also would be great to have a ""validate"" button that makes sure that you can run a `SELECT {expression} FROM {table} LIMIT 1` or similar
--
",junlincc,"
--
@yousoph related to SQL Expression Validator component project 
--
",,,,
4568,OPEN,Country Map visualisation for UK - zoomed out,.pinned; question; viz:chart-maplayer,2021-03-27 05:49:07 +0000 UTC,liucijalat,In progress,,"Make sure these boxes are checked before submitting your issue - thank you!

- [X] I have checked the superset logs for python stacktraces and included it here as text if any
- [X] I have reproduced the issue with at least the latest released version of superset
- [X] I have checked the issue tracker for the same issue and I haven't found one similar


### Superset version
0.23.2

### Expected results
A blank zoomed in map of UK

### Actual results
A zoomed out map of UK with a square frame around it which shows 'Isle of Man' when hovering over it
<img width=""1662"" alt=""screen shot 2018-03-07 at 20 33 15"" src=""https://user-images.githubusercontent.com/32986613/37117107-3010d20c-2248-11e8-84d9-bc4aa2265859.png"">

### Steps to reproduce
1. Open the Misc Charts dashboard from the sample dashboards provided when installing Superset
2. Click 'Explore chart' on the 'Birth in France by department in 2016' widget
3. Under Options, change the Country Name to 'Uk' and run query
",,,liucijalat,"
--
I found that removing Isle of Man from the uk.geojson file solves the problem. The https://github.com/apache/incubator-superset/pull/3911 pull request added Isle of Man. @alanmcruickshank did you have this problem?
--
",mistercrunch,"
--
@alanmcruickshank 
--
",dominijk,"
--
@mistercrunch We've successfully added a city as a geojson as london.geosjson and then got this to display by adjusting the parameters in the chart metadata, when we then tested adding the country list as [docs](https://superset.incubator.apache.org/visualization.html#need-to-add-a-new-country) it didn't appear. Hacking the Uk file to just contain London geom did however work. Is there a constraint that stops us adding countries that aren't countries?
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",,,,
4040,OPEN,Dark Theme?,.pinned; global:theme,2021-03-18 00:35:38 +0000 UTC,amancevice,Opened,,It would be nice if superset supported a 'dark' theme out of the box so that the app and its dashboards can easily be dimmed without writing a very extensive custom CSS.,,,visshaljagtap,"
--
Yes it will be helpful and also it would be great if we have custom theme option.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions. For admin, please label this issue `.pinned` to prevent stale bot from closing the issue.

--
",agabardo,"
--
Any news on this?
--
",visakhmurukes,"
--
Any updates on this?
Would be a great addition to have this
--

--
I ended up adding a custom CSS for the dashboard.
All except table looks fine at the moment
--

--
@DrissiReda  here you go

Looks like its modifying code when pasted
Please find it attached
[dark.txt](https://github.com/apache/superset/files/5847840/dark.txt)


--
",leon,"
--
bump?
--
",DrissiReda,"
--
@visakhmurukes could you please share that custom CSS
--

--
Thanks! any way that can be added to the whole superset ui?
--
"
2731,OPEN,Give users the rights to change their password,.pinned,2021-01-26 10:53:29 +0000 UTC,lilila,Opened,,"What access should I add to allow users to change their password? I checked, Alpha and Gamma users get an ""access denied"" when clicking on the Profile Icon (upper right corner next to version info icon). 

Only Admin role is allowed to change its password 


Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if any
- [x] I have reproduced the issue with at least the latest released version of superset
- [x] I have checked the issue tracker for the same issue and I haven't found one similar




### Superset version
0.18.0-alpha.1

### Expected results


### Actual results


### Steps to reproduce

",,,stevekaeser,"
--
I was able to work around this issue by creating a separate role that has these permissions:

```
can this form post on ResetMyPasswordView,
can this form get on ResetMyPasswordView,
can this form post on UserInfoEditView,
can this form get on UserInfoEditView,
resetmypassword on UserDBModelView,
can edit on UserDBModelView,
can userinfo on UserDBModelView,
userinfoedit on UserDBModelView
```
Then I added that role to all the non-admin users which allowed them to view/edit their profile and change their own passwords.
--
",mistercrunch,"
--
@stevekaeser  doesn't `can edit on UserDBModelView` give them the right to edit everyone else's too?
--
",guillaumewibaux,"
--
I am not sure but I am afraid there is a mismatch between ResetMyPasswordView and ResetPasswordView. With the defaults Gamma and Alpha roles, it's not possible to access the user info page (profile page) nor see the reset password button on this page. Maybe it's been fixed in later releases but I haven't found any issue on the matter.

I was able to grant access to the profile and reset password pages only by granting permission sthat are more supposed to be admin permissions and yes you can change other user passwords then...
E.g: resetPasswordView (get + post) is necessary to see the rest button on the profile page...

--
",muraiki,"
--
One workaround is to grant a role only the `can this form post on ResetMyPasswordView` and `can this form get on ResetMyPasswordView` permissions, then have the users manually go to your Superset URL + `/resetmypassword/form` to reset their password.

Using the Network developer tool in Firefox, I verified that only the password (and a CSRF token) is sent in the POST request -- there's no user ID, so this doesn't appear to open a way to reset other users' passwords.
--

--
Please don't close this issue, as I think it makes using Superset more difficult for users.
--
",stale,"
--
This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.

--
",Giuzzilla,"
--
It's possible to make them see their own profile + see the password change button + allowing them to change their own password without allowing `can edit on UserDBModelView` or related to `ResetPasswordView`, by enabling only these permissions:

```
can this form post on ResetMyPasswordView,
can this form get on ResetMyPasswordView, 
can userinfo on UserDBModelView, 
resetmypassword on UserDBModelView
```

Is there anything that I'm missing or a potential security issue with these enabled?
--
"
2686,OPEN,[feature] Prometheus (or other TSDB) as data source,.pinned; enhancement:request,2021-03-09 12:31:42 +0000 UTC,alanmcruickshank,In progress,,"Currently we have SQLAlchemy and Druid as datasources for Superset.

Would introducing a connection to a time series database like [prometheus ](https://prometheus.io/) or [graphite](https://graphiteapp.org/) be compatible with the roadmap for Superset? There would be value for our use case to have superset as the central point for all our metrics whether or not they live in a SQL-like datastore.

A couple of considerations for implementation:

- Given the different method of querying (it's not SQL), it would have to be a top level connection type.
- There would be only one possible time column, just ""time"". There is no concept of different times fields.

Do people think this would be a good idea, or something that would actually get in the way more than anything?

",,,mistercrunch,"
--
With the name `Superset` now we have to allow for querying all database :)

But yeah it should work ok and fits the roadmap. As you said it would behave essentially like a table with a single metric and single time column. It might not be the best experience as a bunch of the fields in the explore view become obsolete, but would allow to have dashboard that mix elements coming from multiple data sources.
--

--
Notice: this issue has been closed because it has been inactive for 222 days. Feel free to comment and request for this issue to be reopened.

--

--
+1 for prometheus to support SQL :)
--
",kelein,"
--
The same need for prometheus data source support.
--

--
@xrmx  I have used grafana for a real-time monitor frontend, but I also want a statistics reporter with some metrics aggregation during week or month. Grafana seems not suitable.
--
",xrmx,"
--
Isn't grafana the de-facto ui for metrics?
--
",alanmcruickshank,"
--
@xrmx I would agree for most cases.

I quite like the idea of superset being our one-stop-shop for metrics of all sorts together - which means I think it's valuable to support it - at least to the extent that we can get some metrics in and beside metrics from other sources. If those metrics point to something being up, then I imagine reverting back to Grafana for a deeper look would still be a pretty sensible plan.
--
",gecube,"
--
I'd like to revive thus issue
--

--
@nuhamind2 
> If you guys need timeseries database that support sql, maybe look at timescaledb

The TimeScaleDB is not the best option. The marketing specs of it are superior, but the real adoption faces with different struggles. So it is very interesting to get the native support for prometheus.
--

--
Hi @valyala ! Thanks for your message! I will be glad to see VM as datasource for SuperSet too!

--
",raghu999,"
--
+1 for prometheus support.
--
"
2574,OPEN,mssql stored procedure execution support in SQL Lab?,enhancement:request; sql_lab,2021-03-31 05:45:38 +0000 UTC,SimonTulettIdeaco,In progress,,"Make sure these boxes are checked before submitting your issue - thank you!

- [x] I have checked the superset logs for python stacktraces and included it here as text if any
- [x] I have reproduced the issue with at least the latest released version of superset
- [x] I have checked the issue tracker for the same issue and I haven't found one similar


### Superset version
0.17.3

### Expected results
It would be good if it was possible to use SQLLab to create a slice generated from the execution of a stored procedure when working with MSSQL, via invocation with the exec keyword or with the call keyword in MySQL. Support for dynamically passing arguments would be required and perhaps cold be achieved using Jinja2 as Mode Analytics does with liquid.

https://help.modeanalytics.com/articles/add-parameters-to-reports/

### Actual results
N/A

### Steps to reproduce
N/A

",,,mistercrunch,"
--
Is there a way to express wrap a sproc inside a select statement? As is `SELECT * FROM (EXEC my_sproc);`
--

--
Well assuming that a sproc could be queried like a view, and wrapped in a subquery, yes, but that may not be possible.
--

--
Notice: this issue has been closed because it has been inactive for 377 days. Feel free to comment and request for this issue to be reopened.

--
",SimonTulettIdeaco,"
--
Thanks for getting back to me will give this a try. Would this method be able to support the passing of arguments from a filter slice do you think?
--
",aswathkk,"
--
This issue is not resolved yet.
My use case is to use dynamic tables as data source. In such a case, I've written a stored procedure which will dynamically returns the data. But since Superset doesn't support stored procedures as a data source, I'm not able to visualize the data.

@mistercrunch Please reopen this one.
--
",SWoto,"
--
@mistercrunch  Please reopen this one. I'm also facing the same problem.
--
",villebro,"
--
This sounds like a related feature to extended CTE support which I'm currently looking into. It would be most helpful if you could elaborate with an example to make sure the feature request is as unambiguous as possible.
--
",MarkGStacey,"
--
Please reopen.

Ideally:


Proc is for instance, ""GetReturn_Timeseries_ByFund"", with parameter @FundID int  (e.g. CREATE PROC dbo.GetReturn_Timeseries_ByFund (@FundID INT))

We would then like to be able to execute this proc and get back the data by query folding from the final dashboard.

So we'd pass a Fund Id of 253 or 11792 via the URL, and have the proc execute "" EXEC dbo.GetReturn_Timeseries_ByFund @fundID = 253""

Hopefully this explains the use case
--
"
