Issue id,Status,Summary,Issue Type,Created,Author,Resolution,Resolved,Description,Creator,Labels,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary,Comment author,Commentary
3668,OPEN,restic repository is not ready error on backups with Minio and Restic,,2021-04-08 12:56:25 +0000 UTC,irizzant,Opened,,"**What steps did you take and what happened:**
I have a problem running a backup using Minio as storage backend.

I run the backup with:
```
velero backup create test --default-volumes-to-restic=true --ttl=24h0m0s
```
to backup the whole cluster and the backup starts correctly.
After the backup completes I see:
```
NAME                           STATUS            ERRORS   WARNINGS   CREATED                          EXPIRES   STORAGE LOCATION   SELECTOR
test                           PartiallyFailed   140      1          2021-04-07 19:00:15 +0200 CEST   4h        default            <none>
```
The logs reports:
```
time=""2021-04-07T21:00:29Z"" level=error msg=""Error backing up item"" backup=velero/test error=""restic repository is not ready: "" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/restic/repository_ensurer.go:144"" error.function=""github.com/vmware-tanzu/velero/pkg/restic.(*repositoryEnsurer).EnsureRepo"" logSource=""pkg/backup/backup.go:431"" name=restic-vfm7f
```

I'm not sure what's happening here.

The backup storage location is available:
```
NAME      PROVIDER   BUCKET/PREFIX   PHASE       LAST VALIDATED                   ACCESS MODE
default   aws        k8s-backups     Available   2021-04-08 14:50:22 +0200 CEST   ReadWrite
```

**What did you expect to happen:**
The backup completes successfully

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero` 
[logs-velero.txt](https://github.com/vmware-tanzu/velero/files/6278766/logs-velero.txt)

- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml` [details.txt](https://github.com/vmware-tanzu/velero/files/6278748/details.txt)
- `velero backup logs <backupname>` 
[backup-logs.txt](https://github.com/vmware-tanzu/velero/files/6278771/backup-logs.txt)

- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): 
```
Client:
        Version: v1.5.3
        Git commit: 123109a3bcac11dbb6783d2758207bac0d0817cb
Server:
        Version: v1.5.3
```
- Velero features (use `velero client config get features`): `features: <NOT SET>`
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.0"", GitCommit:""af46c47ce925f4c4ad5cc8d1fca46c7b77d13b38"", GitTreeState:""clean"", BuildDate:""2020-12-08T17:59:43Z"", GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""19"", GitVersion:""v1.19.7"", GitCommit:""1dd5338295409edcfff11505e7bb246f0d325d15"", GitTreeState:""clean"", BuildDate:""2021-01-13T13:15:20Z"", GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kubernetes installer & version: kubespray last release
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3664,OPEN,Support `kstatus` status fields,,2021-04-06 12:29:01 +0000 UTC,joshmue,Opened,,"**Describe the problem/challenge you have**
Trying to get velero working as part of a gitops workflow using flux2. General idea for e. g. disaster recovery is:
1. Flux2 installs velero
1. Flux2 applies a `restores.velero.io`
1. Velero takes care of the heavy lifting regarding restoring the backup
1. When done, Velero patches the status of the `restores.velero.io` to be completed
1. Flux2 checks for the `.status` of the `restores.velero.io`; When it is completed, it continues with further deployments; Possibly takes control of restored workloads.

The problem I see: Flux2/kustomize-controller determines the status of custom resources using `kstatus`, which is not supported yet by velero, I think.

**Describe the solution you'd like**
Implement support for common [kstatus](https://github.com/kubernetes-sigs/cli-utils/tree/master/pkg/kstatus) conditions in velero CRD's.


**Anything else you would like to add:**
I think it would be a great addition to velero - not only because of the specific use case I described, which may be impossible or hard to do due to factors I did not take into account yet.

If I understand it correctly, using common `kstatus` conditions is the way to go either way as it seems to be propagated by K8s upstream.

I wonder what your take on this is: Do you agree on this? Do you think this could be implemented pretty easily?
If you want, I might also take a look into implementing this (cannot promise anything of course), in case this does not mean huge changes.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3661,OPEN,Script for generating release docs doesn't update _index.md version on Linux,Area/Documentation; Enhancement/Dev,2021-04-05 17:47:16 +0000 UTC,zubron,Opened,,"**What steps did you take and what happened:**
The docs for the v1.6.0-rc.1 release were generated on a Linux machine. Due to differences in `sed` syntax, the `hack/release-tools/gen-docs.sh` script has two separate sets of commands to update version specific links in newly generated docs. The Linux version of these scripts [does not update the `_index.md` file to use the new version](https://github.com/vmware-tanzu/velero/blob/main/hack/release-tools/gen-docs.sh#L122-L124) unlike the [macOS version](https://github.com/vmware-tanzu/velero/blob/main/hack/release-tools/gen-docs.sh#L101-L104).

This means that the TOC for v1.6.0-rc.1 docs uses `main` as the version, rather than the RC version of the docs: https://velero.io/docs/v1.6.0-rc.1/
Looking at the RC docs, you will notice that the version in the dropdown is ""main"" and all links in the TOC on the side link to ""main"".

**What did you expect to happen:**
The script should be equivalent on both macOS and Linux, and should produce documentation with correct links.
",,,,,,,,,,,,,,
3657,OPEN,Add a multiple credentials use case to the documentation,Area/Documentation,2021-04-01 20:21:27 +0000 UTC,carlisia,In progress,,"There should be an entry for it in this TOC:

![image](https://user-images.githubusercontent.com/16508/113331741-2ae0d880-92d5-11eb-8c8c-acc53ec5c066.png)
",,,carlisia,"
--
For reference: https://velero.io/docs/v1.6.0-rc.1/locations/#create-a-storage-location-that-uses-unique-credentials
--
",,,,,,,,,,
3656,OPEN,Investigate weird install failure of the deletebackuprequests CRD,Bug,2021-04-01 18:15:23 +0000 UTC,carlisia,In progress,,"Data points:
- We found a [bug](https://github.com/vmware-tanzu/velero/issues/3600) with the install for v1.6.0
- That bug only happened on my machine.
- We found the source of the problem, and [removed it](https://github.com/vmware-tanzu/velero/pull/3652) from the code path for the v1.6.0 release
- We had different people run [a script ](https://gist.github.com/carlisia/8b76bc24b78efbcd8932c60062babba1) to install/uninstall Velero multiple times in a row to try and reproduce the bug. No one else could reproduce it, while I consistently did. 
- The RC1 was used, so that eliminated any suspicion with the problem being in the locally built binary
- The behavior was consistent even when running against the same AWS cluster.
- Applying the deletebackuprequests CRD with `kubetcl` always worked.
- Installing with v1.5 always worked.

TODO:
- Investigate:
   - Why the removed code caused that issue
   - Why it only happened for one user ",,,carlisia,"
--
c/c @dsu-igeek in case you want to add pointers to this.
--

--
Things to try:
- run in debug mode
   - look into the stack returned by the api call so see what is triggering the time out
- exclude the deletebackuprequests crd from the installation process
- change the order of installation of the CRDs
--
",,,,,,,,,,
3655,OPEN,IAM Roles for Service Accounts in cn-north-1: Incorrect token audience,,2021-04-01 17:42:31 +0000 UTC,cabrinha,Opened,,"**What steps did you take and what happened:**

Use IAM Role for Service Accounts in cn-north-1 region.

```
time=""2021-04-01T16:44:14Z"" level=error msg=""Error getting backup store for this location"" backupLocation=default controller=backup-sync error=""rpc error: code = Unknown desc = WebIdentityErr: failed to retrieve credentials\ncaused by: InvalidIdentityToken: Incorrect token audience\n\tstatus code: 400, request id: d2f954d6-f1d4-4bef-b2ad-fd6d34dedfd1"" error.file=""/go/src/velero-plugin-for-aws/velero-plugin-for-aws/volume_snapshotter.go:59"" error.function=main.getSessionlogSource=""pkg/controller/backup_sync_controller.go:168""
```


**What did you expect to happen:**
Expected it to work

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
```
time=""2021-04-01T16:52:41Z"" level=error msg=""Error getting backup store for this location"" backupLocation=default controller=backup-sync error=""rpc error: code = Unknown desc = WebIdentityErr: failed to retrieve credentials\ncaused by: InvalidIdentityToken: Incorrect token audience\n\tstatus code: 400, request id: 618efd75-fe79-4673-8e07-061342663afb"" error.file=""/go/src/velero-plugin-for-aws/velero-plugin-for-aws/volume_snapshotter.go:59"" error.function=main.getSession logSource=""pkg/controller/backup_sync_controller.go:168""
```

**Anything else you would like to add:**
IAM Roles for Service Accounts should work in any region by just passing in the Role ARN.

**Environment:**

- Velero version (use `velero version`): v1.5.3
- Velero features (use `velero client config get features`):  <NOT SET>
- Kubernetes version (use `kubectl version`): 1.18
- Kubernetes installer & version: EKS
- Cloud provider or hardware configuration: AWS
- OS (e.g. from `/etc/os-release`): amazon linux 2 eks optimized ami


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3654,OPEN,Create a guide for application developers on how to structure their app for Velero backup/restore,Area/Documentation; P2 - Long-term important,2021-04-01 16:13:37 +0000 UTC,dsu-igeek,Opened,,"**Describe the problem/challenge you have**
When Velero backs up and restores applications we have frequently had issues.  These include CR changes (e.g. the status field is stripped by Velero but the CRD requires it) and quiescing.

**Describe the solution you'd like**
As a first step toward application aware backups we should generate a guide of what Velero does during the backup and restore process and what developers should do in their application to enable correct backup and restore.


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3653,OPEN,"My full cluster backup using velero struck in ""InProgress"" status from 7 days",,2021-04-01 09:39:51 +0000 UTC,KiranMaheswaram,In progress,,"[root@helper-3igi tmp]# velero get backups
NAME                          STATUS                       CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
londoncluster01-backup        InProgress                   2021-03-29 03:52:42 +0000 UTC   26d       default            <none>

----
[root@helper-3igi tmp]# velero backup describe londoncluster01-backup
Name:         londoncluster01-backup
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  <none>

Phase:  InProgress

Namespaces:
  Included:  *
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  included

Label selector:  <none>

Storage Location:  default

Snapshot PVs:  true

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  1

Started:    2021-03-29 03:52:42 +0000 UTC
Completed:  <n/a>

Expiration:  2021-04-28 03:52:42 +0000 UTC

Persistent Volumes: <none included>

Restic Backups (specify --details for more information):
  Completed:  28
----

Velero deployment logs:

time=""2021-03-31T01:54:39Z"" level=error msg=""Error checking repository for stale locks"" controller=restic-repository error=""**_error running command=restic unlock --repo=s3:http://minio-velero.xxxxxx/velero/restic/dx-tgcf192 --password-file=/tmp/velero-restic-credentials-dx-tgcf192725208639_** --cache-dir=/scratch/.cache/restic, stdout=, stderr=Fatal: Fatal: config cannot be loaded: ciphertext verification failed\n: exit status 1"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/restic/repository_manager.go:277"" error.function=""github.com/vmware-tanzu/velero/pkg/restic.(*repositoryManager).exec"" logSource=""pkg/controller/restic_repository_controller.go:142"" name=dx-tgcf192-default-dv748 namespace=velero
",,,KiranMaheswaram,"
--
Hi Team,

Please consider this as high priority and request for the immediate assistance on this 
--
",,,,,,,,,,
3638,OPEN,"Can't disable Restic and use CSI Snapshotter instead - ""Skipping snapshot of PVC because volume is being backed up with restic""",,2021-03-31 07:22:44 +0000 UTC,maarsaks,Opened,,"Hi, I have a problem that makes me sick cause I wasted a whole day on it and no more ideas what's happening.
How to DISABLE restic?

**What steps did you take and what happened:**
I'm trying to make a backup of PVC based on `BlockStorage` and `FilesystemStorage` (Rook-by-Ceph) using `velero-plugin-for-csi` to `ObjectStorage` (Rook-by-Ceph), but in logs of the Velero pod I can see the info:
```
time=""2021-03-29T17:36:39Z"" level=info msg=
""Skipping snapshot of persistent volume because volume is being backed up with restic.""
backup=velero/test99 logSource=""pkg/backup/item_backupper.go:423"" name=pvc-xxx
namespace= persistentVolume=pvc-xxx resource=persistentvolumes
```
```
Skipping PVC monitoring/data-zabbix-postgresql-0, PV will be backed up using restic%! (EXTRA string=pvc-xxx)""
backup=velero/test cmd=/plugins/velero-plugin-for-csi 
logSource=""/go/src/velero-plugin-for-csi/internal/backup/pvc_action.go:92"" pluginName=velero-plugin-for-csi.
```
(Here's the code source from the last message: [.../velero-plugin-for-csi/internal/backup/pvc_action.go](https://github.com/vmware-tanzu/velero-plugin-for-csi/blob/main/internal/backup/pvc_action.go))



**What did you expect to happen:**
The backup will be successfully created.

**The output of the following commands will help us better understand what's going on**:

`$ kubectl logs deployment/velero -n velero`
[click](https://gist.github.com/maarsaks/af1a0b8bccda01a8e0fbfb336a01278e)

`$ velero backup describe test99 --details`
[click](https://gist.github.com/maarsaks/3003dc472b0456a1371f8d6aa19478f0)

```
$ velero backup logs test99
Logs for backup ""test99"" are not available until it's finished processing. 
Please wait until the backup has a phase of Completed or Failed and try again.
```


**Anything else you would like to add:**
- I tried manually:
```
$ velero install \
    --provider aws \
    --plugins velero/velero-plugin-for-aws:v1.0.0,velero/velero-plugin-for-csi:v0.1.0 \
    --bucket backup-storage-0bb9e3c3-f9eb-4cf7-bb66-dcfb17ad2a30 \
    --secret-file ./credentials-velero \
    --use-volume-snapshots=false \
    --backup-location-config region=us-east-1,s3ForcePathStyle=""true"",s3Url=http://rook-ceph-rgw-my-store.rook-ceph.svc \
    --features=EnableCSI
```
- I tried with Helm:
```
helm install velero vmware-tanzu/velero \
--namespace velero \
--create-namespace \
--set-file credentials.secretContents.cloud=/home/maar/temp/velero/credentials-velero \
--set configuration.provider=aws \
--set configuration.backupStorageLocation.config.s3Url=http://rook-ceph-rgw-my-store.rook-ceph.svc \
--set configuration.backupStorageLocation.config.s3ForcePathStyle=true \
--set configuration.backupStorageLocation.bucket=backup-storage-xxxx \
--set configuration.backupStorageLocation.config.region=us-east-1 \
--set configuration.volumeSnapshotLocation.name=default \
--set configuration.volumeSnapshotLocation.config.region=us-east-1 \
--set configuration.snapshotsEnabled=false \
--set configuration.deployRestic=false \
--set initContainers[0].name=velero-plugin-for-aws \
--set initContainers[0].image=velero/velero-plugin-for-aws:v1.0.0 \
--set initContainers[0].volumeMounts[0].mountPath=/target \
--set initContainers[0].volumeMounts[0].name=plugins \
--set initContainers[1].name=velero-plugin-for-csi \
--set initContainers[1].image=velero/velero-plugin-for-csi:v0.1.0 \
--set initContainers[1].volumeMounts[0].mountPath=/target \
--set initContainers[1].volumeMounts[0].name=plugins
```
- I'm using a correct secret and the bucket name.
- There's no restic deamon or pod after installing Velero with above configs.
- Kubernetes VolumeSnapshots are creating successfully.
- PV doesn't have any restic annotations.

**Environment:**

```
$ velero version
Client:
	Version: v1.5.3
	Git commit: 123109a3bcac11dbb6783d2758207bac0d0817cb
Server:
	Version: v1.5.3
```
```
$ velero client config get features
features: EnableCSI
```
```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""19"", GitVersion:""v1.19.7"", GitCommit:""1dd5338295409edcfff11505e7bb246f0d325d15"", GitTreeState:""clean"", BuildDate:""2021-01-13T13:23:52Z"", GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.4"", GitCommit:""e87da0bd6e03ec3fea7933c4b5263d151aafd07c"", GitTreeState:""clean"", BuildDate:""2021-02-18T16:03:00Z"", GoVersion:""go1.15.8"", Compiler:""gc"", Platform:""linux/amd64""}
```
```
$ cat /etc/os-release
PRETTY_NAME=""Debian GNU/Linux 10 (buster)""
NAME=""Debian GNU/Linux""
VERSION_ID=""10""
VERSION=""10 (buster)""
VERSION_CODENAME=buster
ID=debian
```
- Kubernetes installer & version: `v1.20.4-rancher1-1`
- Cloud provider or hardware configuration: five virtual machine nodes.

**SOME CONFIGS:**
- CSI Snapshotter:
[crds.yaml](https://gist.github.com/maarsaks/050bd5214105f4d1b684d6f29febcd63)
[rbac-csi-snapshotter.yaml](https://gist.github.com/maarsaks/7c11741fe60c8d709a1110c0724ee18a)
[rbac-external-provisioner.yaml](https://gist.github.com/maarsaks/076e793baee5ed47e360e23a59494370)
[setup-csi-snapshotter.yaml](https://gist.github.com/maarsaks/171969ec3a85a200b1b20c5fcc4b8ae2)
[rbac-snapshot-controller.yaml](https://gist.github.com/maarsaks/b7a49918a1821f57d5d663cba5050944)
[snapshot-controller-deployment.yaml](https://gist.github.com/maarsaks/c60734f87ea5e76db5e972d60fa15655)
- Object Storage:
[ceph-object-store-and-storageclass.yaml](https://gist.github.com/maarsaks/6e37427360f8e518c604f1dda38a76e5)
[object-bucket-claim.yaml](https://gist.github.com/maarsaks/eb27b113cdc44576ea8bd315fac6936a)



I have no idea where to even look. Anyway, tommorrow round number two.

**UPDATE_1:**
I exposed the `rook-ceph-rgw-my-store.rook-ceph.svc:80` and now after `velero backup describe test99 --details` there's only `Resource List:  <backup resource list not found> instead of the error.`
But the main problem still exists.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""



",,,,,,,,,,,,,,
3636,OPEN,"Velero Backup gets stuck in ""InProgress"" status while trying to take backup of stateful app from GKE",,2021-03-29 16:54:29 +0000 UTC,kokileswaran-n,Opened,,"**What steps did you take and what happened:**
The velero backup status is showing Inprogress and when tried in the GKE cluster below are the steps followed
 

1. Created Kubernetes Cluster in GKE
2. Created bucket credentials
3. Downloaded Velero from `https://github.com/vmware-tanzu/velero/releases/download/v1.5.3/velero-v1.5.3-linux-amd64.tar.gz `
4. Installed Velero using 
`./velero install --provider aws --bucket xxxxxxxxxxxx --secret-file ./xxxxxxxxxxxx --use-restic --default-volumes-to-restic --backup-location-config region=us-geo,s3ForcePathStyle=""true"",s3Url=http://xxxxxxxxxxxxxxxxxx --plugins velero/velero-plugin-for-aws:v1.1.0`
5. Installed wordpress stateful application in new namespace. Provided the yaml files in the attachments. The persistent volume used here is 1Gi.
6. Ran the velero backup command
`./velero create backup <backupname> --include-namespaces <namespace>`
7. Checked the status of velero backup and shows in InProgress

**What did you expect to happen:**
The backup was supposed to be in completed state but it got stuck in Inprogress state.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
![velero_deployment_logs](https://user-images.githubusercontent.com/77956559/112715200-14b5cf80-8f05-11eb-95b0-d1de6c155958.png)

- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
![backup_describe](https://user-images.githubusercontent.com/77956559/112715369-cc4ae180-8f05-11eb-94c7-b2b0cb7c108a.png)

- `velero backup logs <backupname>` - Not available
- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml` - Not available
- `velero restore logs <restorename>` - Not available


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]
- GKE version
![GKE_version](https://user-images.githubusercontent.com/77956559/112715401-fbf9e980-8f05-11eb-8033-ccd69b0325b1.png)
- Velero get backup
![get_backup](https://user-images.githubusercontent.com/77956559/112715411-103de680-8f06-11eb-8f24-f8360887b9e7.png)
- Velero pods
![velero_pods](https://user-images.githubusercontent.com/77956559/112715436-29df2e00-8f06-11eb-81c7-8cb905340411.png)
- Velero backup location
![backup-location](https://user-images.githubusercontent.com/77956559/112715492-7591d780-8f06-11eb-9807-8cfcf7e75415.png)


**Environment:**

- Velero version (use `velero version`): 
![Velero_version](https://user-images.githubusercontent.com/77956559/112715385-dec51b00-8f05-11eb-8785-dc503af4c783.png)

- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,kokileswaran,"
--
Good day team do we have update on this issue.
--
",,,,,,,,,,
3633,OPEN,Managing multi-tenant environment,,2021-03-26 08:26:02 +0000 UTC,sfelix-orange,Opened,,"Managing several namespaces in a multi-tenant environment while not being the cluster admin.


We are currently working on a k8s cluster with several namespaces, but without being the cluster admin nor having a cluster role. We would like to be able to backup the pod's volumes in our namespaces using Velero. As is, we were not able to make it work without doing some modifications to Velero's code (see PR https://github.com/vmware-tanzu/velero/pull/3476 and https://github.com/vmware-tanzu/velero/pull/3477).
Is there a way to make it work without having to do these modifications?

Thanks and best regards.

Sebastien


**Environment:**

- Velero version (use `velero version`): TBD
- Kubernetes version: v1.19.3
- Kubernetes installer & version: TBD
- Cloud provider or hardware configuration: TBD
- OS (e.g. from `/etc/os-release`): TBD

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3625,OPEN,Consolidate API clients for the e2e tests + clean up client calls on e2e tests,,2021-03-24 16:24:48 +0000 UTC,carlisia,Opened,,"The e2e tests are unnecessarily instantiating API clients multiple times in some cases.

Todo: 

- consolidate the API client instantiation in one place to make it easier to get one instance of it and be done
- clean up the tests

This work had been done in a (very long and now closed) PR:

https://github.com/vmware-tanzu/velero/pull/3605/files#diff-3b20052a1f35a1505eb5b073184ada5a35ad0d1cd0fa1e44e93ca669b3f3892c",,,,,,,,,,,,,,
3624,OPEN,e2e tests not cleaning up after failures,,2021-04-06 21:37:25 +0000 UTC,carlisia,Opened,,"The e2e tests that add k8s resources besides the Velero installation are not removing those resources from the cluster when there is a test failure. This is fine if there is no failure but otherwise it causes the cluster to be in an unexpected state and becomes the source of failure for other tests.

To do: ensure each test removes any added resources at each point where the test can fail.

Also: delete any created backups.",,,,,,,,,,,,,,
3623,OPEN,Release checklist for v1.6.0,release-blocker,2021-04-08 04:38:02 +0000 UTC,carlisia,In progress,,"@vmware-tanzu/velero-maintainers let's all check these boxes as we get thru the steps.

## Beta and prior to the first RC release
- [x] no major bugs outstanding (current issues: #3599, #3624)
- [x] unit tests pass
- [x] e2e tests passing on Kind
- [x] e2e tests passing on vSphere
- [x] e2e tests passing on AWS

## Any RC + plugin release
Manual tests need to be run before the plugins are released. Documentation for manual testing (to be updated after merge): https://github.com/vmware-tanzu/velero/pull/3601
- [ ] manual tests with RC and upgraded plugin passing on:
    - [x]  Kind [@ashish-amarnath]
    - [x] AWS - [@carlisia]
    - [ ] vSphere
    - [x] GCP [@ashish-amarnath]
    - [ ] Azure
- [ ] test [upgrading to RC](https://velero.io/docs/main/upgrade-to-1.6/#docs) on:
    - [x]  Kind
    - [x] AWS - [@carlisia]
    - [x] vSphere
    - [x] GCP [@ashish-amarnath]
    - [ ] Azure
- [x] test restic [@ashish-amarnath]

⚠️ e2e tests need to run against the latest k8s and earliest supported k8s for each plugin

Note: this checklist follows the (not yet merged) release criteria list: https://deploy-preview-3598--velero.netlify.app/docs/main/release-instructions/#release-criteria.",,,dsu,"
--
Please add Azure to the release checklist
--
",ashish,"
--
Please add restic as well
--
",carlisia,"
--
I ran these tests and they were all successful:

### AWS + Volume Snapshotter
Test 1:
Verify that a backup and restore using Volume Snapshots can be performed
https://asciinema.org/a/KB67j2jcYPNOulebx9WQyYXEO

Test 2:
Verify that an installation using the v1.6.0-rc.2 version can be used to restore from backups created with the v1.5.3 version 
recording 1: backup with v1.5.3, then install v1.6.0-rc.2 / break 
https://asciinema.org/a/HWPACBQKdBIS49U8TF0pmIRnq
recording 2: restore previous backup using v1.6.0-rc.2
https://asciinema.org/a/O8l7Y0AIWqrVEYyV8iZ2Bvfly

Test 3:
Ensure v1.6.0-rc.2 compatibility with plugin v1.1.0
https://asciinema.org/a/sJcNXx2Jjht4AHbX9r2uYQxZv

Test 4:
Upgrade the install from v1.5.3 to v1.6.0-rc.2
https://asciinema.org/a/HNdS0ouAT7XQeWwPBZIje0N6W
--
",,,,,,
3622,OPEN,backup from schedule not executing hook on all pods (no such container),,2021-03-24 15:24:49 +0000 UTC,headyj,Opened,,"**What steps did you take and what happened:**

1. create a Schedule definition file with hooks:
```yaml
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: my-velero-schedule
  namespace: velero
spec:
  schedule: 0 4 * * *
  template:
    includedNamespaces:
    - '*'
    snapshotVolumes: true
    hooks:
      resources:
      - name: my-pre-hook
        includedNamespaces:
        - '*'
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: name
            app.kubernetes.io/component: core
        pre:
          - exec:
              command:
                - custom_command
              timeout: 150s
```

2. Create at least 2 namespaces with 2 pods that match the labelSelector
3. Create a backup from the Schedule
4. Hook is correctly executed on the first pod, but the second is in error, like if velero still tries to find the container name of the first pod in the second one:
time=""2021-03-24T14:58:14Z"" level=info msg=""running exec hook"" backup=velero/velero-backup-schedule-20210324145811 hookCommand=""[custom-command]"" hookContainer=container1 hookName=my-pre-hook hookOnError=Fail hookPhase=pre hookSource=backupSpec hookTimeout=""{2m30s}"" hookType=exec logSource=""pkg/podexec/pod_command_executor.go:124"" name=pod1 namespace=namespace1 resource=pods
[...]
Error executing hook"" backup=velero/velero-backup-schedule-20210324145811 error=""no such container: \""**container1**\"""" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/podexec/pod_command_executor.go:184"" error.function=github.com/vmware-tanzu/velero/pkg/podexec.ensureContainerExists hookPhase=pre hookSource=backupSpec hookType=exec logSource=""internal/hook/item_hook_handler.go:240"" name=**pod2** namespace=namespace2 resource=pods

**What did you expect to happen:**
hook should be executed correctly on both pods


**Environment:**

- Velero version: v1.5.3 
- Velero features: - 
- Kubernetes version: v1.19.7
- Kubernetes installer & version:
- Cloud provider or hardware configuration: AWS (EKS)
- OS:
",,,,,,,,,,,,,,
3621,OPEN,Velero Backup failed,,2021-03-24 13:42:02 +0000 UTC,AkhilCh308,In progress,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)
I've deployed minio and installed velero CLI and velero plugin

`root@mcds:/etc# velero backup get
NAME     STATUS   ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
backup   Failed   0        0          2021-03-22 17:35:31 +0000 UTC   28d       default            <none>
your     Failed   0        0          2021-03-24 12:52:59 +0000 UTC   29d       default            <none>
root@mcds:/etc# k get all -n velero
NAME                          READY   STATUS      RESTARTS   AGE
pod/minio-5b84955bdd-zg95x    1/1     Running     0          47h
pod/minio-setup-4sgss         0/1     Completed   0          47h
pod/velero-679457dc45-jpmnk   1/1     Running     0          44h
NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
service/minio     ClusterIP   100.70.41.197   <none>        9000/TCP         47h
service/minio-2   NodePort    100.67.15.84    <none>        9000:31929/TCP   47h
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/minio    1/1     1            1           47h
deployment.apps/velero   1/1     1            1           44h
NAME                                DESIRED   CURRENT   READY   AGE
replicaset.apps/minio-5b84955bdd    1         1         1       47h
replicaset.apps/velero-679457dc45   1         1         1       44h
NAME                    COMPLETIONS   DURATION   AGE
job.batch/minio-setup   1/1           9s         47h
`
**What did you expect to happen:**


**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
time=""2021-03-24T12:52:59Z"" level=info msg=""Getting backup item actions"" backup=velero/your logSource=""pkg/controller/backup_controller.go:545""
time=""2021-03-24T12:53:00Z"" level=info msg=""Setting up backup store to check for backup existence"" backup=velero/your logSource=""pkg/controller/backup_controller.go:551""
time=""2021-03-24T12:53:00Z"" level=error msg=""backup failed"" controller=backup error=""error checking if backup already exists in object storage: rpc error: code = Unknown desc = RequestError: send request failed\ncaused by: Head https://mcds.minio.local:443/velero/backups/your/velero-backup.json: dial tcp: lookup mcds.minio.local on 100.64.0.10:53: no such host"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/backup_controller.go:562"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*backupController).runBackup"" key=velero/your logSource=""pkg/controller/backup_controller.go:279""
time=""2021-03-24T12:53:05Z"" level=error msg=""Error listing backups in backup store"" backupLocation=default controller=backup-sync error=""rpc error: code 

- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
root@mcds:/etc# velero backup describe your
Name:         your
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  velero.io/source-cluster-k8s-gitversion=v1.19.3+vmware.1
              velero.io/source-cluster-k8s-major-version=1
              velero.io/source-cluster-k8s-minor-version=19

Phase:  Failed (run `velero backup logs your` for more information)

Errors:    0
Warnings:  0

Namespaces:
  Included:  *
  Excluded:  tkg-system

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  default

Velero-Native Snapshot PVs:  auto

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  1.1.0

Started:    2021-03-24 12:52:59 +0000 UTC
Completed:  2021-03-24 12:53:00 +0000 UTC

Expiration:  2021-04-23 12:52:59 +0000 UTC

Velero-Native Snapshots: <none included>
root@mcds:/etc#


- `velero backup logs <backupname>`
root@mcds:/etc# velero backup logs your
An error occurred: Get ""https://mcds.minio.local:443/velero/backups/your/your-logs.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minio%!F(MISSING)20210324%!F(MISSING)minio%!F(MISSING)s3%!F(MISSING)aws4_request&X-Amz-Date=20210324T130031Z&X-Amz-Expires=600&X-Amz-SignedHeaders=host&X-Amz-Signature=3fe6a8f6c8da110fc9556f7eeac4ca60c9078cad8d0faef73d59cfa2a5329c03"": x509: certificate signed by unknown authority

The --insecure-skip-tls-verify flag can also be used to accept any TLS certificate for the download, but it is susceptible to man-in-the-middle attacks.


- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): 
- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,AkhilCh308,"
--
Also now I've exposed https for minio host  and tried to create backup, following logs are here 

An error occurred: request failed: <?xml version=""1.0"" encoding=""UTF-8""?>
<Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message><Key>backups/back/back-logs.gz</Key><BucketName>velero</BucketName><Resource>/velero/backups/back/back-logs.gz</Resource><RequestId>166F4AC5B2D453C4</RequestId><HostId>7f149ef3-42f0-441a-b7e6-c4dcffe8ac74</HostId></Error>

--
",,,,,,,,,,
3616,OPEN,Region is calculated incorrectly when gov account is used.,,2021-03-23 19:44:41 +0000 UTC,jala-dx,Opened,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)

Create a Bucket in gov. https://console.amazonaws-us-gov.com/
Create a Bucket with **same name** in non-gov https://s3.console.aws.amazon.com/

Restic ignores the 'config.region' specified in the BackupStorageLocation object. Even though the bucketname is unique globally, it is not unique across 'gov and non-gov' regions. Hence during snapshot creation, it accesses the bucketname in the incorrect region and fails at https://github.com/vmware-tanzu/velero/blob/main/pkg/restic/config.go#L75

https://github.com/vmware-tanzu/velero/blob/main/pkg/restic/config.go#L69

apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
  creationTimestamp: ""2021-03-17T01:12:01Z""
  generation: 24
  labels:
    component: velero
  name: default
  namespace: velero
  resourceVersion: ""4607288""
  uid: 39aeddd0-f4d4-4f92-a296-4ea28da1489b
spec:
  config:
    **region: us-gov-east-1**
    s3Url: https://s3.us-gov-east-1.amazonaws.com
  objectStorage:
    bucket: xxxx-snapshot
    prefix: eastgov
  provider: aws
status:
  lastSyncedTime: ""2021-03-23T19:37:00Z""
  lastValidationTime: ""2021-03-23T19:37:00Z""
  phase: Available

**What did you expect to happen:**
When region is passed, restic should pick the region provided in BackupStorageLocation Spec instead of invoking getAWSBucketRegion https://github.com/vmware-tanzu/velero/blob/main/pkg/restic/config.go#L75


**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
- `velero backup logs <backupname>`
- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): 


- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3615,OPEN,Pass credentials to e2e tests to run in release pipeline.,,2021-03-23 18:42:32 +0000 UTC,ashish-amarnath,Opened,,"**Describe the problem/challenge you have**
e2e tests are currently using minio and restic to backup workloads.
To exercise the volume snaphotter plugins, we need to pass credentials to the e2e tests.


**Describe the solution you'd like**
[A clear and concise description of what you want to happen.]


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`):
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3606,OPEN,[New Plugin] Openstack,,2021-03-21 15:47:43 +0000 UTC,Lirt,Opened,,"Hello,

I am maintainer of Velero Openstack plugin and this is request for including in the community supported list of plugins. I was present in your one of your reviews sometime in December and we agreed that this plugin can be listed in the community supported ones. 

Recently I released new version that supports block volume snapshots and by this most of the planned plugin features are finished.

The GitHub repository is located here https://github.com/Lirt/velero-plugin-for-openstack/ and I am going to open PR for including it in documentation and link it here.",,,,,,,,,,,,,,
3589,OPEN,Document when to add a changelog; and when to not,Area/Documentation,2021-03-18 15:02:58 +0000 UTC,carlisia,In progress,,"Ex: if there was a bug fix on the main branch for an issue that was introduced after the last release, do we add a changelog?

Please add here odd ball instances you are not clear about if they deserve a changelog or not.",,,eleanor,"
--
Sounds like you don't expect changelogs for website changes?
--
",carlisia,"
--
Correct, website changes and documentation definitely don't get a changelog. Only feature changes that affect the user, bug fixes from a previous version, and breaking changes in the API of our code.
--
",,,,,,,,
3588,OPEN,[Snapshots]: PartiallyFailed - Invalid Signature,,2021-03-15 20:34:31 +0000 UTC,Ahmadre,In progress,,"**What steps did you take and what happened:**
I managed to connect to my IONOS S3 Bucket via AWS-Plugin from velero, but somehow it is partially failing and I can't backup my Persistent-Volumes via snapshotting.

**What did you expect to happen:**
Persistent Volumes land on IONOS and gets backuped.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

### Logs

- `kubectl logs deployment/velero -n velero`:

<details><summary>Click to expand</summary>

```bash
time=""2021-03-13T01:57:39Z"" level=info msg=""setting log-level to INFO"" logSource=""pkg/cmd/server/server.go:191""
time=""2021-03-13T01:57:39Z"" level=info msg=""Starting Velero server v1.5.3 (123109a3bcac11dbb6783d2758207bac0d0817cb)"" logSource=""pkg/cmd/server/server.go:193""
time=""2021-03-13T01:57:39Z"" level=info msg=""No feature flags enabled"" logSource=""pkg/cmd/server/server.go:197""
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/crd-remap-version
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pod
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pv
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service-account
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/add-pv-from-pvc
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/add-pvc-from-pod
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/change-pvc-node-selector
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/change-storage-class
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/cluster-role-bindings
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/crd-preserve-fields
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/init-restore-hook
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/job
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pod
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/restic
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/role-bindings
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service-account
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/plugins/velero-plugin-for-aws kind=VolumeSnapshotter logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/aws
time=""2021-03-13T01:57:39Z"" level=info msg=""registering plugin"" command=/plugins/velero-plugin-for-aws kind=ObjectStore logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/aws
time=""2021-03-13T01:57:40Z"" level=info msg=""Checking existence of namespace."" logSource=""pkg/cmd/server/server.go:380"" namespace=velero
time=""2021-03-13T01:57:40Z"" level=info msg=""Namespace exists"" logSource=""pkg/cmd/server/server.go:386"" namespace=velero
I0313 01:57:43.125947       1 request.go:621] Throttling request took 1.029003727s, request: GET:https://10.233.0.1:443/apis/rbac.authorization.k8s.io/v1?timeout=32s
time=""2021-03-13T01:57:44Z"" level=info msg=""Checking existence of Velero custom resource definitions"" logSource=""pkg/cmd/server/server.go:415""
time=""2021-03-13T01:57:44Z"" level=info msg=""All Velero custom resource definitions exist"" logSource=""pkg/cmd/server/server.go:449""
time=""2021-03-13T01:57:44Z"" level=warning msg=""Velero restic daemonset not found; restic backups/restores will not work until it's created"" logSource=""pkg/cmd/server/server.go:495""
time=""2021-03-13T01:57:44Z"" level=info msg=""Starting controllers"" logSource=""pkg/cmd/server/server.go:571""
time=""2021-03-13T01:57:44Z"" level=info msg=""Starting metric server at address [:8085]"" logSource=""pkg/cmd/server/server.go:578""
time=""2021-03-13T01:57:44Z"" level=info msg=""Backup sync period is 1m0s"" logSource=""pkg/controller/backup_sync_controller.go:76""
time=""2021-03-13T01:57:44Z"" level=info msg=""Waiting for informer caches to sync"" logSource=""pkg/cmd/server/server.go:836""
time=""2021-03-13T01:57:47Z"" level=info msg=""Creating schedule velero-saversobackup"" controller=schedule logSource=""pkg/controller/schedule_controller.go:105""
time=""2021-03-13T01:57:47Z"" level=info msg=""Done waiting for informer caches to sync"" logSource=""pkg/cmd/server/server.go:839""
time=""2021-03-13T01:57:47Z"" level=info msg=""Informer cache synced"" informer=""*v1.ResticRepository"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-03-13T01:57:47Z"" level=info msg=""Informer cache synced"" informer=""*v1.Backup"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-03-13T01:57:47Z"" level=info msg=""Informer cache synced"" informer=""*v1.VolumeSnapshotLocation"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-03-13T01:57:47Z"" level=info msg=""Informer cache synced"" informer=""*v1.Restore"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-03-13T01:57:47Z"" level=info msg=""Informer cache synced"" informer=""*v1.DownloadRequest"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-03-13T01:57:47Z"" level=info msg=""Informer cache synced"" informer=""*v1.Schedule"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-03-13T01:57:47Z"" level=info msg=""Informer cache synced"" informer=""*v1.DeleteBackupRequest"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-03-13T01:57:47Z"" level=info msg=""Informer cache synced"" informer=""*v1.PodVolumeBackup"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-03-13T01:57:47Z"" level=info msg=""Server starting..."" logSource=""pkg/cmd/server/server.go:896""
time=""2021-03-13T01:57:47Z"" level=info msg=""Starting controller"" controller=backup-sync logSource=""pkg/controller/generic_controller.go:76""
time=""2021-03-13T01:57:47Z"" level=info msg=""Starting controller"" controller=backup-deletion logSource=""pkg/controller/generic_controller.go:76""
time=""2021-03-13T01:57:47Z"" level=info msg=""Checking for expired DeleteBackupRequests"" controller=backup-deletion logSource=""pkg/controller/backup_deletion_controller.go:595""
time=""2021-03-13T01:57:47Z"" level=info msg=""Done checking for expired DeleteBackupRequests"" controller=backup-deletion logSource=""pkg/controller/backup_deletion_controller.go:623""
time=""2021-03-13T01:57:47Z"" level=info msg=""Starting controller"" controller=backup logSource=""pkg/controller/generic_controller.go:76""
time=""2021-03-13T01:57:47Z"" level=info msg=""Starting controller"" controller=schedule logSource=""pkg/controller/generic_controller.go:76""
time=""2021-03-13T01:57:47Z"" level=info msg=""Starting controller"" controller=gc-controller logSource=""pkg/controller/generic_controller.go:76""
time=""2021-03-13T01:57:47Z"" level=info msg=""Starting controller"" controller=restic-repository logSource=""pkg/controller/generic_controller.go:76""
time=""2021-03-13T01:57:47Z"" level=info msg=""Starting controller"" controller=downloadrequest logSource=""pkg/controller/generic_controller.go:76""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=info msg=""Starting controller"" controller=restore logSource=""pkg/controller/generic_controller.go:76""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=info msg=""Found 5 backups in the backup location that do not exist in the cluster and need to be synced"" backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:197""
time=""2021-03-13T01:57:47Z"" level=info msg=""Attempting to sync backup into cluster"" backup=velero-saversobackup-20210313003726 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-03-13T01:57:47Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:57:47Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:57:47Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:57:47Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:57:47Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:57:47Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:57:47Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:57:47Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:57:47Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:57:47Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:57:47Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:57:47Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:57:47Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:47Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:57:47Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:57:47Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:57:48Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:57:48Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:57:48Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:57:48Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:48Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:48Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:48Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:57:48Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:57:48Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:57:48Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:48Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:48Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:49Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:57:49Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:57:49Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:57:50Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:50Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:50Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:50Z"" level=info msg=""Successfully synced backup into cluster"" backup=velero-saversobackup-20210313003726 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-03-13T01:57:50Z"" level=info msg=""Attempting to sync backup into cluster"" backup=velero-saversobackup-20210313004155 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-03-13T01:57:50Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:57:50Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:57:50Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:57:50Z"" level=info msg=""Successfully synced backup into cluster"" backup=velero-saversobackup-20210313004155 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-03-13T01:57:50Z"" level=info msg=""Attempting to sync backup into cluster"" backup=velero-saversobackup-20210313013406 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-03-13T01:57:50Z"" level=info msg=""Successfully synced backup into cluster"" backup=velero-saversobackup-20210313013406 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-03-13T01:57:50Z"" level=info msg=""Attempting to sync backup into cluster"" backup=velero-saversobackup-20210313013650 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-03-13T01:57:50Z"" level=info msg=""Successfully synced backup into cluster"" backup=velero-saversobackup-20210313013650 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-03-13T01:57:50Z"" level=info msg=""Attempting to sync backup into cluster"" backup=velero-saversobackup-20210313013821 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-03-13T01:57:50Z"" level=info msg=""Successfully synced backup into cluster"" backup=velero-saversobackup-20210313013821 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-03-13T01:57:51Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:57:51Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:57:51Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:57:52Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:52Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:52Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:52Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:57:52Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:57:52Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:57:57Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:57Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:57:57Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:58:03Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:58:03Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:58:03Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:58:08Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:58:08Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:58:08Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:58:23Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:58:23Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:58:23Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:58:28Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:58:28Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:58:28Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:58:51Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:58:51Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:58:51Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:58:51Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:58:51Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:59:04Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:59:04Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:59:04Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:59:09Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:59:09Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:59:09Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T01:59:51Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:59:51Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T01:59:51Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T01:59:51Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T01:59:51Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:00:31Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T02:00:31Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T02:00:31Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T02:00:51Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:00:51Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:00:51Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:00:51Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:00:51Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:01:51Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:01:51Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:01:51Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:01:51Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:01:51Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:02:51Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:02:51Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:02:51Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:02:51Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:02:51Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
I0313 02:02:52.396466       1 request.go:621] Throttling request took 1.029001296s, request: GET:https://10.233.0.1:443/apis/acme.cert-manager.io/v1?timeout=32s
time=""2021-03-13T02:03:15Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T02:03:15Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T02:03:15Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T02:03:51Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:03:52Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:03:52Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:03:52Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:03:52Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:04:32Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:04:32Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:04:32Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:04:52Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:04:52Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:04:52Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:04:52Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:04:52Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:05:52Z"" level=info msg=""Found 5 backups in the backup location that do not exist in the cluster and need to be synced"" backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:197""
time=""2021-03-13T02:05:52Z"" level=info msg=""Attempting to sync backup into cluster"" backup=velero-saversobackup-20210313013650 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-03-13T02:05:52Z"" level=info msg=""Successfully synced backup into cluster"" backup=velero-saversobackup-20210313013650 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-03-13T02:05:52Z"" level=info msg=""Attempting to sync backup into cluster"" backup=velero-saversobackup-20210313013821 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-03-13T02:05:52Z"" level=info msg=""Successfully synced backup into cluster"" backup=velero-saversobackup-20210313013821 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-03-13T02:05:52Z"" level=info msg=""Attempting to sync backup into cluster"" backup=velero-saversobackup-20210313003726 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-03-13T02:05:52Z"" level=info msg=""Successfully synced backup into cluster"" backup=velero-saversobackup-20210313003726 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-03-13T02:05:52Z"" level=info msg=""Attempting to sync backup into cluster"" backup=velero-saversobackup-20210313004155 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-03-13T02:05:52Z"" level=info msg=""Successfully synced backup into cluster"" backup=velero-saversobackup-20210313004155 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-03-13T02:05:52Z"" level=info msg=""Attempting to sync backup into cluster"" backup=velero-saversobackup-20210313013406 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-03-13T02:05:52Z"" level=info msg=""Successfully synced backup into cluster"" backup=velero-saversobackup-20210313013406 backupLocation=backups controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-03-13T02:05:52Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:05:52Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:05:52Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:05:52Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:05:52Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:06:52Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:06:53Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:06:53Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:06:53Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:06:53Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:07:53Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:07:53Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:07:53Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:07:53Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:07:53Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
I0313 02:07:56.066725       1 request.go:621] Throttling request took 1.028712982s, request: GET:https://10.233.0.1:443/apis/authorization.k8s.io/v1beta1?timeout=32s
time=""2021-03-13T02:08:42Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014910\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014910-20210313024947 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T02:08:42Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024455 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T02:08:42Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=downloadrequest error=""backup.velero.io \""velero-saversobackup-20210313014417\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/download_request_controller.go:161"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*downloadRequestController).generatePreSignedURL"" key=velero/velero-saversobackup-20210313014417-20210313024543 logSource=""pkg/controller/generic_controller.go:140""
time=""2021-03-13T02:08:53Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:08:53Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:08:53Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:08:53Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:08:53Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:09:53Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:09:53Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:09:53Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:09:53Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:09:53Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:10:53Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:10:53Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:10:53Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:10:53Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:10:53Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:11:53Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:11:53Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:11:53Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:11:53Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:11:53Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:12:53Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:12:54Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:12:54Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:12:54Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:12:54Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
I0313 02:12:59.736950       1 request.go:621] Throttling request took 1.029399089s, request: GET:https://10.233.0.1:443/apis/acme.cert-manager.io/v1?timeout=32s
time=""2021-03-13T02:13:54Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:13:54Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:13:54Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:13:54Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:13:54Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:14:54Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:14:54Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:14:54Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:14:54Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:14:54Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:15:54Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:15:54Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:15:54Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:15:54Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:15:54Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:16:54Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:16:54Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:16:54Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:16:54Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:16:54Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:17:54Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:17:54Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:17:54Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:17:54Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-13T02:17:54Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
I0313 02:18:03.407511       1 request.go:621] Throttling request took 1.029534245s, request: GET:https://10.233.0.1:443/apis/autoscaling/v1?timeout=32s
time=""2021-03-13T02:18:54Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:18:55Z"" level=warning msg=""The specified default backup location named \""default\"" was not found; for convenience, be sure to create one or make another backup location that is available the default"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:161""
time=""2021-03-13T02:18:55Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-13T02:18:55Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
```

</details>

❯ `velero backup describe velero-saversobackup-20210315030048 --details`

<details><summary>Click to Expand</summary>

```yaml
Name:         velero-saversobackup-20210315030048
Namespace:    velero
Labels:       app.kubernetes.io/instance=velero
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/name=velero
              helm.sh/chart=velero-2.15.0
              velero.io/schedule-name=velero-saversobackup
              velero.io/storage-location=backups
Annotations:  velero.io/source-cluster-k8s-gitversion=v1.18.15
              velero.io/source-cluster-k8s-major-version=1
              velero.io/source-cluster-k8s-minor-version=18

Phase:  PartiallyFailed (run `velero backup logs velero-saversobackup-20210315030048` for more information)

Errors:    13
Warnings:  0

Namespaces:
  Included:  *
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  included

Label selector:  <none>

Storage Location:  backups

Velero-Native Snapshot PVs:  true

TTL:  240h0m0s

Hooks:  <none>

Backup Format Version:  1.1.0

Started:    2021-03-15 04:00:48 +0100 CET
Completed:  2021-03-15 04:01:05 +0100 CET

Expiration:  2021-03-25 04:00:48 +0100 CET

Total items to be backed up:  995
Items backed up:              995

Resource List:  <error getting backup resource list: request failed: <?xml version=""1.0"" encoding=""UTF-8""?><Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. For more information, see REST Authentication and SOAP Authentication for details.</Message><RequestId>bf8c0331-ff41-1fff-90c7-0cc47ad3f142</RequestId><HostId>b5ded39d7e9c43128787ce25e274f47d</HostId></Error>>

Velero-Native Snapshots: <none included>
```

</details>

- `velero backup logs <backupname>`

<details><summary>Click to expand</summary>

```bash
An error occurred: request failed: <?xml version=""1.0"" encoding=""UTF-8""?><Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. For more information, see REST Authentication and SOAP Authentication for details.</Message><RequestId>63ca5b0e-eeec-1fff-ae55-0cc47af2c4a0</RequestId><HostId>8f44ed9fe3434eefafcf2a15300974bc</HostId></Error>
```

</details>

**Environment:**

- helm version (use `helm version`): 

```bash
    $ version.BuildInfo{Version:""v3.5.0"", GitCommit:""32c22239423b3b4ba6706d450bd044baffdcf9e6"", GitTreeState:""dirty"", GoVersion:""go1.15.6""}
```

- helm chart version and app version:

```bash
NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
velero  velero          33              2021-03-12 19:50:09.444304 +0100 CET    deployed        velero-2.15.0   1.5.3  
```

- Kubernetes version (use `kubectl version`):

```bash
Client Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.15"", GitCommit:""73dd5c840662bb066a146d0871216333181f4b64"", GitTreeState:""clean"", BuildDate:""2021-01-13T13:22:41Z"", GoVersion:""go1.13.15"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.15"", GitCommit:""73dd5c840662bb066a146d0871216333181f4b64"", GitTreeState:""archive"", BuildDate:""2021-01-13T19:24:02Z"", GoVersion:""go1.13.15"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Kubernetes installer & version:

__Managed K8S-Cluster__

- Cloud provider or hardware configuration:

__IONOS by 1&1 - Cloud (Intel Skylake)__:
![Bildschirmfoto 2021-03-12 um 20 30 31](https://user-images.githubusercontent.com/18512224/110989205-d13e5b80-8371-11eb-81d9-444cfd4af29a.png)

- S3 Bucket Reference for IONOS:

__[IONOS S3 Bucket API](https://devops.ionos.com/api/s3/?__hstc=94004290.61c6314c7e913008fbf3920436be8227.1613413417846.1615495428227.1615540550167.14&__hssc=94004290.2.1615540550167&__hsfp=3606176375#overview)__


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,Ahmadre,"
--
I forgot to post my HELM-Values here:

```yaml
#image:
#  tag: v1.4.3

initContainers:
  - name: velero-plugin-for-aws
    image: velero/velero-plugin-for-aws:v1.1.0
    imagePullPolicy: IfNotPresent
    volumeMounts:
      - mountPath: /target
        name: plugins

configuration:
  provider: aws

  backupStorageLocation:
    name: backups
    bucket: saverso
    config:
      region: s3-de-central
      s3Url: ""https://s3-de-central.profitbricks.com:443""
      s3ForcePathStyle: true

  volumeSnapshotLocation:
    name: backups
    config:
      region: s3-de-central
      snapshotLocation: backups

logLevel: error
logFormat: json
defaultVolumesToRestic: true

credentials:
  name: credentials
  secretContents:
    cloud: |
      [backups]
      aws_access_key_id=xxx
      aws_secret_access_key=xxxxxx
  extraSecretRef: velero
schedules:
  saversobackup:
    schedule: ""0 3 * * *""
    template:
      includeClusterResources: true
      storageLocation: backups
      snapshotVolumes: true
      ttl: ""240h0m0s""
      includedResources:
        - '*'
      includedNamespaces:
        - '*'
```
--

--
Note: I recreated the keys but no changes....
--
",,,,,,,,,,
3586,OPEN,PodVolumeRestore seemingly making no progress; blank status,Needs info; Needs investigation,2021-03-15 12:20:44 +0000 UTC,ChipWolf,Opened,,"**What steps did you take and what happened:**

`velero backup create -l app=myapp --include-cluster-resources --include-namespaces dev`

> deleted resources from the dev namespace & corresponding pv

`velero restore create --from-backup dev --include-cluster-resources`

**What did you expect to happen:**
The restic backup to restore to the pod volume.

**The output of the following commands will help us better understand what's going on**:

```
> velero describe backup dev
Name:         dev
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  velero.io/source-cluster-k8s-gitversion=v1.20.2+k3s1
              velero.io/source-cluster-k8s-major-version=1
              velero.io/source-cluster-k8s-minor-version=20

Phase:  Completed

Errors:    0
Warnings:  0

Namespaces:
  Included:  dev
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  included

Label selector:  app=myapp

Storage Location:  default

Velero-Native Snapshot PVs:  auto

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  1.1.0

Started:    2021-03-13 21:25:57 +0100 CET
Completed:  2021-03-13 21:26:16 +0100 CET

Expiration:  2021-04-12 22:25:57 +0200 CEST

Total items to be backed up:  15
Items backed up:              15

Velero-Native Snapshots: <none included>

Restic Backups (specify --details for more information):
  Completed:  1
```

```
> velero describe restore dev-20210313221555
Name:         dev-20210313221555
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:  InProgress

Started:    2021-03-13 22:15:55 +0100 CET
Completed:  <n/a>

Backup:  dev

Namespaces:
  Included:  all namespaces found in the backup
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io
  Cluster-scoped:  included

Namespace mappings:  <none>

Label selector:  <none>

Restore PVs:  auto

Restic Restores (specify --details for more information):
  New:  1
```
- `velero backup logs dev`
https://paste.ubuntu.com/p/wWqHS4B2Jc/


**Anything else you would like to add:**
There's nothing discernable in velero or restic's pod logs.

![](https://i.cwlf.uk/Kr9G0)

**Environment:**

- Velero version (use `velero version`): 
```
Client:
        Version: v1.5.3
        Git commit: 123109a3bcac11dbb6783d2758207bac0d0817cb
Server:
        Version: v1.5.3
```
- Velero features (use `velero client config get features`):
```
features: <NOT SET>
```
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.2+k3s1"", GitCommit:""1d4adb0301b9a63ceec8cabb11b309e061f43d5f"", GitTreeState:""clean"", BuildDate:""2021-01-14T23:52:37Z"", GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.2+k3s1"", GitCommit:""1d4adb0301b9a63ceec8cabb11b309e061f43d5f"", GitTreeState:""clean"", BuildDate:""2021-01-14T23:52:37Z"", GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kubernetes installer & version:
```
k3s version v1.20.2+k3s1 (1d4adb03)
go version go1.15.5
```
- Cloud provider or hardware configuration:
```
Metal, 3 nodes.
```
- OS (e.g. from `/etc/os-release`):
```
Ubuntu 20.10
```

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,zubron,"
--
Hi @ChipWolf. I know you said that there's nothing that stands out in the velero or restic pods logs, but we would still like to see them.

Please provide the velero pod logs using `kubectl logs deployment/velero -n velero`.
restic runs as a DaemonSet on the cluster, so please provide the logs from each restic pod using `kubectl logs -n velero <restic-pod>`.

Thanks!
--
",,,,,,,,,,
3579,OPEN,Document manual testing steps,P1 - Important; release-blocker,2021-03-15 23:16:44 +0000 UTC,dsu-igeek,Opened,,"Make a checklist of tests that should be performed by hand before the release is finished.  This is just formalizing the existing steps, no need to add stuff that hasn't been covered in the past.  We should probably keep this as a doc, perhaps TESTING.md but if you think that doesn't work, we can leave it in a google doc or something.",,,carlisia,"
--
I think keeping it in a TESTING.md file is best for visibility, until we have a better system. 
--
",,,,,,,,,,
3575,OPEN,Run e2e tests daily against head of the main branch,CI/CD; E2E Tests,2021-03-15 12:03:30 +0000 UTC,ashish-amarnath,Opened,,"**Describe the problem/challenge you have**
[A description of the current limitation/problem/challenge that you are experiencing.]


**Describe the solution you'd like**
[A clear and concise description of what you want to happen.]


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`):
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3574,OPEN,Run e2e Tests in PR CI,CI/CD; E2E Tests,2021-03-15 12:03:42 +0000 UTC,ashish-amarnath,Opened,,"**Describe the problem/challenge you have**
[A description of the current limitation/problem/challenge that you are experiencing.]


**Describe the solution you'd like**
[A clear and concise description of what you want to happen.]


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`):
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3572,OPEN,Customizable hook annotation names,Enhancement/User; Needs Product; Needs investigation,2021-03-12 19:22:02 +0000 UTC,lautenbde,Opened,,"**Describe the problem/challenge you have**

Different kind of backups (e.g. including / excluding Volume Snapshots) might require different hooks to apply at backup and restore time. 

While you can apply different set of hooks by specifying them in the Backup / Restore CRD, the names of backup/restore hook annotations are fixed in the code. Hence, only a single set of hooks can be applied through annotations. As hook annotations are considered to be the preferred method it would be great if you could annotate Pods with multiple sets of hooks which then can be selected in the Backup / Restore CRD according to the needs of the given backup or restore. 

**Describe the solution you'd like**
A hook prefix should be definable in the Backup / Restore CRD which is prepended to the default hook names as known today. This will allow to annotate resources with different sets of hooks which can be easily selected for each individual backup.


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3570,OPEN,Non-Running Pod with PV should be skipped from backup,Needs investigation,2021-03-12 19:19:39 +0000 UTC,levindecaro,Opened,,"**Describe the problem/challenge you have**
Completed/Error Cron Pod, BuildConfig Pod in OCP always report backup failed when it has pv/emptydir attached. Restic has no idea to skip those non-running pod. As a result, the backup will always report PartiallyFailed. Although we can bypass  them by annotation, but it is hard to maintain in general.


**Describe the solution you'd like**
Should skip those error/completed pod in restic backup option.


**Environment:**

- Velero version (use `velero version`): 1.5.3
- Kubernetes version (use `kubectl version`): 1.19
- Kubernetes installer & version: OCP 4.6
- Cloud provider or hardware configuration: VMware
- OS (e.g. from `/etc/os-release`): RHCOS 4.6

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3569,OPEN,Multi-arch for gcp plugin,Enhancement/User; P2 - Long-term important,2021-03-12 19:21:08 +0000 UTC,N0Cloud,Opened,,"**Describe the problem/challenge you have**
- Install velero on arm64 cluster with gcp plugin

**Describe the solution you'd like**
- Building multi-arch images for gcp plugin

**Environment:**

- Velero version (use `velero version`): v1.5.3
- Kubernetes version (use `kubectl version`): v1.19.8
- Kubernetes installer & version: k3s
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`): ubuntu 20.04.2 LTS

",,,,,,,,,,,,,,
3566,OPEN,Links with fragments are not rendered on Hugo site,Area/Documentation,2021-03-12 19:20:20 +0000 UTC,zubron,Opened,,"I dont think the links with the # are working correctly, i can't tell if this is bc there is an issue with the preview, or they will also not work on the live site?

_Originally posted by @a-mccarthy in https://github.com/vmware-tanzu/velero/pull/3489#discussion_r592483701_

Our [render hook](https://github.com/vmware-tanzu/velero/blob/main/site/layouts/_default/_markup/render-link.html) to create links drops the fragment when linking to headings within the current page or within other markdown pages on the site.",,,,,,,,,,,,,,
3565,OPEN,Restore of empty backups should have a better failure message,Bug; Enhancement/User; P3 - Wouldn't it be nice if...,2021-03-12 00:06:11 +0000 UTC,dsu-igeek,Opened,,"**What steps did you take and what happened:**
Created a backup with no entries in it

`velero backup create empty-backup-1 --include-cluster-resources=true  --include-namespaces tst-1234-velero --include-resources 'persistentvolumes=pvc-ee198b55-febf-4664-93e2-1e740a99b82e'

The include-resources is incorrect and no resources are selected.  The result is a backup with no items:

```
Name:         empty-backup-1
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  velero.io/source-cluster-k8s-gitversion=v1.19.1+vmware.2
              velero.io/source-cluster-k8s-major-version=1
              velero.io/source-cluster-k8s-minor-version=19

Phase:  Completed

Errors:    0
Warnings:  0

Namespaces:
  Included:  tst-1234-velero
  Excluded:  <none>

Resources:
  Included:        persistentvolumes=pvc-ee198b55-febf-4664-93e2-1e740a99b82e
  Excluded:        <none>
  Cluster-scoped:  included

Label selector:  <none>

Storage Location:  default

Velero-Native Snapshot PVs:  auto

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  1.1.0

Started:    2021-03-11 15:29:21 -0800 PST
Completed:  2021-03-11 15:29:22 -0800 PST

Expiration:  2021-04-10 16:29:21 -0700 PDT

Total items to be backed up:  0
Items backed up:              0

Velero-Native Snapshots: <none included>
```

If you attempt to restore from this backup, it will fail with 'error parsing backup contents: directory ""resources"" does not exist'

`velero restore create empty-restore --from-backup empty-backup-1`

`velero restore describe empty-restore`

```
Name:         empty-restore
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:  PartiallyFailed (run 'velero restore logs empty-restore' for more information)

Started:    2021-03-11 15:31:54 -0800 PST
Completed:  2021-03-11 15:31:54 -0800 PST

Errors:
  Velero:   error parsing backup contents: directory ""resources"" does not exist
  Cluster:    <none>
  Namespaces: <none>

Backup:  empty-backup-1

Namespaces:
  Included:  all namespaces found in the backup
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io
  Cluster-scoped:  auto

Namespace mappings:  <none>

Label selector:  <none>

Restore PVs:  auto
````

**What did you expect to happen:**

Restore should be Failed, not PartiallyFailed
A better error message that explains the restore failed because there were zero items to restore.

**Environment:**

- Velero version (use `velero version`): 1.5.3


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3563,OPEN,Document and test upgrades from v1.5.x and v1.6.x to v1.7.0,Area/Documentation; release-blocker,2021-03-11 19:54:54 +0000 UTC,nrb,Opened,,"**Describe the problem/challenge you have**

Before releasing v1.7.0, we will need upgrade steps for moving from the latest two versions of Velero to v1.7.0


**Describe the solution you'd like**

Documentation should include, but not be limited to, the following:

- Any CRD schema upgrades necessary
- Any plugin compatibility changes
- Any restic impacts",,,,,,,,,,,,,,
3562,OPEN,Document and test upgrades from v1.4.3 and v1.5.3 to v1.6.0,Area/Documentation; release-blocker,2021-03-12 20:30:09 +0000 UTC,nrb,Opened,,"**Describe the problem/challenge you have**

Full upgrade steps are not documented for v1.5.3 and v1.4.3 to v1.6.0. These should be done before final release.


**Describe the solution you'd like**

Documentation for upgrading to v1.6.0 which should include (but not limited to):

- Any CRD upgrades necessary
- Impacts on restic data


**Anything else you would like to add:**

v1.4.3 instructions should be included since requiring uses to move through 2 versions to get a working update is not possible in some circumstances, such as when Velero is included in a product.

---

 - [x] Update v1.4 doc
 - [x] Update v1.5 doc
 - [x] Update v1.6 doc
 - [ ] Test upgrade to v1.6.0",,,,,,,,,,,,,,
3560,OPEN,Labels on backup objects,Enhancement/User; P3 - Wouldn't it be nice if...,2021-03-31 16:53:22 +0000 UTC,DavidDarville,In progress,,"I see that the labels from schedules.velero.io are copied to backups.velero.io, which is not how things are usually done in Kubernetes.

A schedule object like this:
```
apiVersion: velero.io/v1
kind: Schedule
metadata:
  labels:
    foo: bar
  name: test-schedule
  namespace: velero
spec:
  schedule: '0 * * * *'
  template:
    hooks: {}
    includedNamespaces:
    - '*'
    ttl: 48h0m0s
``` 
Will generate backup objects like this:
```
apiVersion: velero.io/v1
kind: Backup
metadata:
  labels:
    foo: bar
    velero.io/backup: test-schedule-20210311110003
    velero.io/pv: ....
    velero.io/schedule-name: test-schedule
    velero.io/storage-location: default
  name: test-schedule-20210311110003
  namespace: velero
......
```
This is not how things are done in Kubernetes. For instance: The pods created by a deploy objects, gets labeled based on '.spec.metadata.labels', NOT '.metadata.labels'.

My problem with this, is that I would like to deploy everything on my Kubernetes clusters automatically - using Argo-CD. And Argo-CD is adding custom labels to all objects that it is managing, to keep track of them. And since Velero is copying those labels from the schedule object, to all backup objects, this tracking will be messed up.

I have tested this using both v1.2.0 and v1.5.3, without seeing any difference in this behaviour.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,arush,"
--
I am working on the issue and the approach to the solution that I am considering is to drop `.metadata.labels` if there's `metadata.spec.labels` provided after the schedule object is `deepcopied` before passing it to the `submitBackupIfDue` function.

But the issue that I am facing with that approach is the `Spec` field received by the controller is basically empty and hence there's no label data available there.
--

--
Got it, sure :)
--

--
@DavidDarville I am glad that it worked out and satisfies your use case. Happy to be of help ❤️
--
",KlavsKlavsen,"
--
Actually - what deployments and others do, is setting: .spec.template.metadata.* (incl. labels) - and then thats copied over to .spec.metadata on the pod object.
Thats what we should follow here - as its the standard, everyone would expect - and its actually whats already being done - only metadata is not part of the .spec.template in velero CRD. So @DavidDarville Would this not be what you'd expect and want?
--
",DavidDarville,"
--
This sounds great!
Having labels originate from .spec.template.metadata.labels in the ""parent"" object, is the behaviour I would expect in Kubernetes.
--

--
I have now tested PR#3641, and everything works for me.
Thank you @arush-sal.
I hope it will be merged soon 😃
--
",,,,,,
3556,OPEN,Expiration date should be calculated from when backup completes; not from when it starts,Bug; P2 - Long-term important,2021-03-10 07:59:40 +0000 UTC,dsu-igeek,Opened,,"**What steps did you take and what happened:**
Backup expiration is calculated by taking the time it is starting and adding the Time To Live (TTL).  If the backup takes a long time to execute and the TTL is short, the backup will expire unexpectedly quickly.

**What did you expect to happen:**
Expiration time should be calculated from the time the backup completes.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3555,OPEN,CLI shows expiration in the past for backups which have not yet executed because operations were stalled,Bug; P3 - Wouldn't it be nice if...,2021-03-10 07:56:34 +0000 UTC,dsu-igeek,Opened,,"**What steps did you take and what happened:**
Retrieved backup info with velero get backups for a backup that had been stalled in New state for a long period of time
~~~
$ velero get backups | grep -e 202101281237 -e NAME
NAME                           STATUS       ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
clusterstate-20210128123759    New          0        0          <nil>                           10d ago                      <none>
~~~
Backup, which hasn't executed yet, is shown as expiring in the past.

**What did you expect to happen:**
The output should have shown either ""N/A"" because the backup is not eligible for GC since it hasn't started yet, or at least a date in the future.




**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3554,OPEN,Scheduler should not create Backups until outstanding backups for the schedule have completed,Bug; P2 - Long-term important,2021-03-10 07:50:54 +0000 UTC,dsu-igeek,Opened,,"**What steps did you take and what happened:**
If a scheduled backup time is reached while a previous backup for the schedule is running, a new Backup resource is created.  This can result in multiple Backups being queued that will all get executed together when the queue is unstalled.

**What did you expect to happen:**
Backups should not be queued until any previous backups for the schedule have completed.  

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3552,OPEN,Restoring Restic volume on multiple pods; but only one pod waits for it (v1.4.0),Needs info,2021-03-10 14:38:58 +0000 UTC,cjmaloof,In progress,,"We have a volume which is mounted on several different pods, annotated with `backup.velero.io/backup-volumes` so that Restic handles its backup and restore. However, upon restore, only one of these pods (the first to start up, I think) receives a `restic-wait` init container. The rest sometimes start up before the volume has been restored, which can cause problems.

In the restore logs, only the pod with the `restic-wait` container displays ""Restic backups for pod found"".

Version: 
```
$ velero version
Client:
        Version: v1.5.1
        Git commit: 87d86a45a6ca66c6c942c7c7f08352e26809426c
Server:
        Version: v1.4.0
```",,,dsu,"
--
**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
- `velero backup logs <backupname>`
- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`):
- Velero features (use `velero client config get features`):
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):


--
",cjmaloof,"
--
@dsu-igeek Thanks for updating. Unfortunately the application with the error is proprietary and also big enough that the logs aren't easy to parse. I will try to create a minimal replication scenario when I have some time (not sure when).

Velero version is in the issue description. I'm running the client in Windows 10 Enterprise on Cygwin.
```
$ velero client config get features
features: <NOT SET>
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.0"", GitCommit:""9e991415386e4cf155a24b1da15becaa390438d8"", GitTreeState:""clean"", BuildDate:""2020-03-25T14:58:59Z"", GoVersion:""go1.13.8"", Compiler:""gc"", Platform:""windows/amd64""}
Server Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.14"", GitCommit:""5de7fd1f9555368a86eb0f8f664dc58055c17269"", GitTreeState:""clean"", BuildDate:""2021-01-18T09:31:01Z"", GoVersion:""go1.13.15"", Compiler:""gc"", Platform:""linux/amd64""}
```
--
",,,,,,,,
3550,OPEN,inconsistent log output format,Enhancement/User; P3 - Wouldn't it be nice if...,2021-03-09 17:16:05 +0000 UTC,seankhliao,Opened,,"**What steps did you take and what happened:**

`k8s.io/client-go` writes out logs through `k8s.io/klog{,/v2}` in a different format than the rest of the application (logfmt or json) making it more difficult to parse, example:

```
time=""2021-03-09T08:12:51Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
I0309 08:13:16.387907       1 request.go:621] Throttling request took 1.058190834s, request: GET:https://10.250.0.1:443/apis/autoscaling.k8s.io/v1?timeout=32s
time=""2021-03-09T08:13:50Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
```

**What did you expect to happen:**

logs output in a consistent format

<!--
**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)
- `kubectl logs deployment/velero -n velero`
- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
- `velero backup logs <backupname>`
- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`
-->

**Anything else you would like to add:**
<!--
[Miscellaneous information that will assist in solving the issue.]
-->

**Environment:**

- Velero version (use `velero version`): 1.5.3
<!--
- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):
-->

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3548,OPEN,Pod volume backup failed,Needs investigation,2021-03-10 02:20:46 +0000 UTC,bayerb,Opened,,"**What steps did you take and what happened:**
After setting up velero on AKS, and running the command:
`velero backup logs balazs-teszt-backup4`

the backup is in PartiallyFailed phase, and the logs show the following error:

`velero backup logs balazs-teszt-backup4|grep error

time=""2021-03-09T08:32:13Z"" level=info msg=""1 errors encountered backup up item"" backup=velero/balazs-teszt-backup4 logSource=""pkg/backup/backup.go:427"" name=ugribugri-nexus-0
time=""2021-03-09T08:32:13Z"" level=error msg=""Error backing up item"" backup=velero/balazs-teszt-backup4 error=""pod volume backup failed: error running restic backup, stderr=Fatal: unable to open config file: blob.GetProperties: storage: service returned error: StatusCode=404, ErrorCode=404 The specified blob does not exist., ErrorMessage=no response body was available for error status code, RequestInitiated=Tue, 09 Mar 2021 08:32:12 GMT, RequestId=4c0d9393-501e-0074-71be-1411e9000000, API Version=2016-05-31, QueryParameterName=, QueryParameterValue=\nIs there a repository at the following location?\nazure:backup:/restic/balazs-teszt\n: exit status 1"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/restic/backupper.go:179"" error.function=""github.com/vmware-tanzu/velero/pkg/restic.(*backupper).BackupPodVolumes"" logSource=""pkg/backup/backup.go:431"" name=ugribugri-nexus-0`

**What did you expect to happen:**
backup is succesful




**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
`kubectl logs deployment/velero -n velero
Found 8 pods, using pod/restic-bgc95
time=""2021-03-09T08:31:44Z"" level=info msg=""Setting log-level to INFO""
time=""2021-03-09T08:31:44Z"" level=info msg=""Starting Velero restic server v1.5.3 (123109a3bcac11dbb6783d2758207bac0d0817cb)"" logSource=""pkg/cmd/cli/restic/server.go:83""
2021-03-09T08:31:45.186Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {""addr"": "":8080""}
time=""2021-03-09T08:31:45Z"" level=info msg=""Starting controllers"" logSource=""pkg/cmd/cli/restic/server.go:208""
time=""2021-03-09T08:31:45Z"" level=info msg=""Starting metric server for restic at address [:8085]"" logSource=""pkg/cmd/cli/restic/server.go:199""
time=""2021-03-09T08:31:45Z"" level=info msg=""Controllers starting..."" logSource=""pkg/cmd/cli/restic/server.go:250""
2021-03-09T08:31:45.240Z        INFO    controller-runtime.manager      starting metrics server {""path"": ""/metrics""}
time=""2021-03-09T08:31:45Z"" level=info msg=""Starting controller"" controller=pod-volume-restore logSource=""pkg/controller/generic_controller.go:76""
time=""2021-03-09T08:31:45Z"" level=info msg=""Waiting for caches to sync"" controller=pod-volume-restore logSource=""pkg/controller/generic_controller.go:81""
time=""2021-03-09T08:31:45Z"" level=info msg=""Starting controller"" controller=pod-volume-backup logSource=""pkg/controller/generic_controller.go:76""
time=""2021-03-09T08:31:45Z"" level=info msg=""Waiting for caches to sync"" controller=pod-volume-backup logSource=""pkg/controller/generic_controller.go:81""
time=""2021-03-09T08:31:45Z"" level=info msg=""Caches are synced"" controller=pod-volume-restore logSource=""pkg/controller/generic_controller.go:85""
time=""2021-03-09T08:31:45Z"" level=info msg=""Caches are synced"" controller=pod-volume-backup logSource=""pkg/controller/generic_controller.go:85""`

- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
`Name:         balazs-teszt-backup4
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  velero.io/source-cluster-k8s-gitversion=v1.16.15
              velero.io/source-cluster-k8s-major-version=1
              velero.io/source-cluster-k8s-minor-version=16

Phase:  PartiallyFailed (run `velero backup logs balazs-teszt-backup4` for more information)

Errors:    1
Warnings:  0

Namespaces:
  Included:  balazs-teszt
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  default

Velero-Native Snapshot PVs:  auto

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  1.1.0

Started:    2021-03-09 09:32:10 +0100 STD
Completed:  2021-03-09 09:32:13 +0100 STD

Expiration:  2021-04-08 10:32:10 +0200 DST

Total items to be backed up:  23
Items backed up:              23

Velero-Native Snapshots: <none included>

Restic Backups (specify --details for more information):
  Failed:  1`

- `velero backup logs <backupname>`
`velero backup logs balazs-teszt-backup4|grep error
time=""2021-03-09T08:32:13Z"" level=info msg=""1 errors encountered backup up item"" backup=velero/balazs-teszt-backup4 logSource=""pkg/backup/backup.go:427"" name=ugribugri-nexus-0
time=""2021-03-09T08:32:13Z"" level=error msg=""Error backing up item"" backup=velero/balazs-teszt-backup4 error=""pod volume backup failed: error running restic backup, stderr=Fatal: unable to open config file: blob.GetProperties: storage: service returned error: StatusCode=404, ErrorCode=404 The specified blob does not exist., ErrorMessage=no response body was available for error status code, RequestInitiated=Tue, 09 Mar 2021 08:32:12 GMT, RequestId=4c0d9393-501e-0074-71be-1411e9000000, API Version=2016-05-31, QueryParameterName=, QueryParameterValue=\nIs there a repository at the following location?\nazure:backup:/restic/balazs-teszt\n: exit status 1"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/restic/backupper.go:179"" error.function=""github.com/vmware-tanzu/velero/pkg/restic.(*backupper).BackupPodVolumes"" logSource=""pkg/backup/backup.go:431"" name=ugribugri-nexus-0`

- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): v1.5.3
- Velero features (use `velero client config get features`): Not set 
- Kubernetes version (use `kubectl version`): Major:""1"", Minor:""19"", GitVersion:""v1.19.3""
- Kubernetes installer & version:
- Cloud provider or hardware configuration: Azure 
- OS (e.g. from `/etc/os-release`): ubuntu 18.04.4 LTS


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3547,OPEN,Error getting server version: timed out waiting for server status request to be processed,Needs info,2021-03-09 21:59:35 +0000 UTC,arunkumarrspl,Opened,,"**What steps did you take and what happened:**
Tried to run `velero version`
I am getting the below error

`
-> velero version -n backup
Client:
	Version: v1.4.0
	Git commit: 5963650c9d64643daaf510ef93ac4a36b6483392
<error getting server version: timed out waiting for server status request to be processed>`

**What did you expect to happen:**
It should show the velero server version.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

**Anything else you would like to add:**
`This is completely new deployment. And tested it in both kubernetes clusters with versions and v1.18.x and v1.19.x`

**Environment:**

- Velero version (use `velero version`): 
- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

`velero client version: v1.4.0
  velero server version: v1.4.0
  kubernetes version: v1.18.8
`

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,dsu,"
--
We would recommend using Velero 1.5.3, the latest.  If you are still having issues, please provide the logs from the Velero pod.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`





--
",,,,,,,,,,
3544,OPEN,Backup CSI Volume Snapshots to S3 (Feature Request),Enhancement/User; P2 - Long-term important,2021-04-02 20:44:26 +0000 UTC,haslersn,In progress,,"We have a bare-metal Kubernetes cluster with in-cluster rook-ceph storage. For our volume backups we'd like to have the following properties:

1. Volume backups must be taken from atomic snapshots.
2. Volume backups must be stored in a durable storage outside of our cluster, preferably S3.
3. Cross-cluster restores of volume backups must be supported.

We can't use velero-plugin-for-csi, because it doesn't provide (2) and (3), and we can't use the Velero Restic integration, because it doesn't provide (1).

**Proposed solution**

We'd like to extend velero-plugin-for-csi by an option to store the snapshot contents in S3. I think this can be realized using Restic as follows: After taking a snapshot, use CSI Volume Cloning in order to create a volume from it. Then mount that volume read-only into a Pod that backs up the data using Restic.

Note that this is widely different from the Velero Restic integration, because it works on a read-only clone of the volume instead of the live volume. This way we maintain property (1) while achieving property (2).

Property (3) can be achieved as follows. When restoring a volume where the snapshot does not exist in the cluster, use Restic in order to restore the volume from S3 instead.

**Question**

Is this something which you would like to see in velero-plugin-for-csi? Or is it more likely that we need to make a fork?

**Environment**

- Kubernetes version: 1.19
- Cloud provider or hardware configuration: bare-metal

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,dsu,"
--
This is something that is going to be handled more generally in Velero as we make data movement a part of Velero proper and integrate it with Astrolabe (https://github.com/vmware-tanzu/astrolabe).  This needs to offer the clone/attach snapshot functionality as an option, but on many storage systems clone of a snapshot is a very expensive operation, so when we can, we will use native data path to extract data from snapshots.  This also needs to handle the data movement independently of the snapshot so that we do not stall the backup while data is being copied (the way Restic is handled currently).  We'd welcome involvement, there will be architecture discussions as we move forward.   
--

--
It will depend on the storage system.  Not all storage systems have native data paths, but EBS has one as well as vSphere.
--
",haslersn,"
--
> so when we can, we will use native data path to extract data from snapshots

Does CSI support extracting data from snapshots? If not, then how do you want to do this without a clone?
--

--
@dsu-igeek 

> We'd welcome involvement, there will be architecture discussions as we move forward.

When and where do these architecture discussions take place?
--
",,,,,,,,
3542,OPEN,Pre/post backup and restore hooks for the entire backup rather than per-pod,Enhancement/User; P2 - Long-term important,2021-03-08 18:51:19 +0000 UTC,dsu-igeek,Opened,,"**Describe the problem/challenge you have**
Add pre/post backup and restore hooks for the entire backup rather than per-pod


**Describe the solution you'd like**

Currently backup and restore hooks are associated with a pod to be executed.  It would be useful to have hooks that were associated with the entire backup/restore and executed before/after the backup/restore, rather than when a pod was encountered.



**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3539,OPEN,velero backup of cluster resources results in a crash of velero pod,Needs investigation,2021-03-08 22:17:52 +0000 UTC,mfiedi,In progress,,"**What steps did you take and what happened:**
I'm trying to run a velero backup request for all cluster scope resources in my Openshift Cluster. The command I'm using is:


Soon after the backup command is issued I see a crash of the velero pod in namespace spp-velero. I enabled debugging in the velero deployment and see that the backups hangs when trying to retrieve the events from the cluster. When I query the amount of event, I see that there are 214558. That's an incredibly high number and honestly I've no idea where the come from. Here is a snippet from the velero log right before the restart:

time=""2021-03-08T10:01:47Z"" level=info msg=""Getting items for resource"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=nodes
time=""2021-03-08T10:01:47Z"" level=info msg=""Listing items"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace= resource=nodes
time=""2021-03-08T10:01:47Z"" level=info msg=""Retrieved 6 items"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace= resource=nodes
time=""2021-03-08T10:01:47Z"" level=info msg=""Getting items for resource"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=podtemplates
time=""2021-03-08T10:01:47Z"" level=info msg=""Listing items"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace= resource=podtemplates
time=""2021-03-08T10:01:47Z"" level=info msg=""Retrieved 0 items"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace= resource=podtemplates
time=""2021-03-08T10:01:47Z"" level=info msg=""Getting items for resource"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=services
time=""2021-03-08T10:01:47Z"" level=info msg=""Listing items"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace= resource=services
time=""2021-03-08T10:01:48Z"" level=info msg=""Retrieved 77 items"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace= resource=services
time=""2021-03-08T10:01:48Z"" level=info msg=""Getting items for resource"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=replicationcontrollers
time=""2021-03-08T10:01:48Z"" level=info msg=""Listing items"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace= resource=replicationcontrollers
time=""2021-03-08T10:01:48Z"" level=info msg=""Retrieved 0 items"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace= resource=replicationcontrollers
time=""2021-03-08T10:01:48Z"" level=info msg=""Getting items for resource"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=limitranges
time=""2021-03-08T10:01:48Z"" level=info msg=""Listing items"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace= resource=limitranges
time=""2021-03-08T10:01:48Z"" level=info msg=""Retrieved 0 items"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace= resource=limitranges
time=""2021-03-08T10:01:48Z"" level=info msg=""Getting items for resource"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=secrets
time=""2021-03-08T10:01:48Z"" level=info msg=""Listing items"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace= resource=secrets
time=""2021-03-08T10:01:50Z"" level=info msg=""Retrieved 1307 items"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace= resource=secrets
time=""2021-03-08T10:01:50Z"" level=info msg=""Getting items for resource"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=events
time=""2021-03-08T10:01:50Z"" level=info msg=""Listing items"" backup=spp-velero/test-mfiedler-debug group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace= resource=events

In an attempt to solve this I raised the memory settings for the velero deployment to these settings:

velero_resource_allocation:
    limits:
      cpu: '1'
      memory: 2Gi
    requests:
      cpu: 500m
      memory: 1Gi

That did not solve the issue. The velero pods still crashes, the backup remains in progress s forever and never comes to an end.

I now deleted all events and restarted the backup. Now it works fine. Questions remains why the backup crashes with many existing events? 

 



**What did you expect to happen:**
Backup completes successfully

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`

- refer to attached log

- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`

 velero describe backup sppbackup-k8s-b7fb5ea4-17f2-4c3c-980e-a74f1b654a0b -n spp-velero
Name:         sppbackup-k8s-b7fb5ea4-17f2-4c3c-980e-a74f1b654a0b
Namespace:    spp-velero
Labels:       velero.io/storage-location=default
Annotations:  velero.io/source-cluster-k8s-gitversion=v1.18.3+fa69cae
              velero.io/source-cluster-k8s-major-version=1
              velero.io/source-cluster-k8s-minor-version=18+

Phase:  InProgress

Errors:    0
Warnings:  0

Namespaces:
  Included:
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  included

Label selector:  <none>

Storage Location:  default

Velero-Native Snapshot PVs:  auto

TTL:  61320h0m0s

Hooks:  <none>

Backup Format Version:  1.1.0

Started:    2021-03-08 12:11:49 +0100 CET
Completed:  <n/a>

Expiration:  2028-03-06 12:11:49 +0100 CET

Velero-Native Snapshots: <none included>

- `velero backup logs <backupname>`

velero backup logs sppbackup-k8s-b7fb5ea4-17f2-4c3c-980e-a74f1b654a0b -n spp-velero
Logs for backup ""sppbackup-k8s-b7fb5ea4-17f2-4c3c-980e-a74f1b654a0b"" are not available until it's finished processing. Please wait until the backup has a phase of Completed or Failed and try again.

- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): 
Client:
        Version: v1.5.2
        Git commit: e115e5a191b1fdb5d379b62a35916115e77124a4
Server:
        Version: v1.5.2-konveyor

- Velero features (use `velero client config get features`): 
features: <NOT SET>

- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.2-0-g52c56ce"", GitCommit:""b66f2d3a6893be729f1b8660309a59c6e0b69196"", GitTreeState:""clean"", BuildDate:""2020-08-10T04:49:09Z"", GoVersion:""go1.13.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""18+"", GitVersion:""v1.18.3+fa69cae"", GitCommit:""fa69cae"", GitTreeState:""clean"", BuildDate:""2020-12-14T23:03:06Z"", GoVersion:""go1.13.15"", Compiler:""gc"", Platform:""linux/amd64""}

- Kubernetes installer & version:

- Cloud provider or hardware configuration:
- 
- OS (e.g. from `/etc/os-release`):
NAME=""Red Hat Enterprise Linux Server""
VERSION=""7.9 (Maipo)""
ID=""rhel""
ID_LIKE=""fedora""
VARIANT=""Server""
VARIANT_ID=""server""
VERSION_ID=""7.9""
PRETTY_NAME=""Red Hat Enterprise Linux Server 7.9 (Maipo)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server""
HOME_URL=""https://www.redhat.com/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""

REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""
REDHAT_BUGZILLA_PRODUCT_VERSION=7.9
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""
REDHAT_SUPPORT_PRODUCT_VERSION=""7.9""


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,mfiedi,"
--
[cluster resource backup failed.txt](https://github.com/vmware-tanzu/velero/files/6101307/cluster.resource.backup.failed.txt)

--
",,,,,,,,,,
3538,OPEN,Schedule is not respecting day of week,Needs investigation,2021-03-08 22:17:33 +0000 UTC,luizgn,Opened,,"**What steps did you take and what happened:**
Hi, congratulations for Velero, it is so much useful and easy to use.

However, I may found a minor bug, it looks like schedule is not respecting day of week. These are my schedules:

```
NAME      STATUS    CREATED                         SCHEDULE           BACKUP TTL   LAST BACKUP   SELECTOR
daily     Enabled   2021-03-04 09:04:12 -0300 -03   45 20 2-31 * 1-6   168h0m0s     14h ago       <none>
monthly   Enabled   2021-03-04 09:04:13 -0300 -03   45 20 1 * *        8784h0m0s    3d ago        <none>
weekly    Enabled   2021-03-04 09:04:12 -0300 -03   45 20 2-31 * 0     744h0m0s     14h ago       <none>
```

I was expecting to run weekly backups on Sundays and daily backups on other days but it is running both, daily and weekly every day (March 7th was Sunday):
```
NAME                     STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
daily-20210307204542     Completed   0        2          2021-03-07 17:45:42 -0300 -03   6d        default            <none>
daily-20210306204542     Completed   0        2          2021-03-06 17:45:42 -0300 -03   5d        default            <none>
daily-20210305204542     Completed   0        2          2021-03-05 17:45:42 -0300 -03   4d        default            <none>
daily-20210304204541     Completed   0        2          2021-03-04 17:45:41 -0300 -03   3d        default            <none>
daily-20210304120412     Completed   0        2          2021-03-04 09:04:12 -0300 -03   3d        default            <none>
monthly-20210304120413   Completed   0        2          2021-03-04 09:05:56 -0300 -03   362d      default            <none>
weekly-20210307204542    Completed   0        2          2021-03-07 17:46:32 -0300 -03   30d       default            <none>
weekly-20210306204542    Completed   0        2          2021-03-06 17:46:34 -0300 -03   29d       default            <none>
weekly-20210305204542    Completed   0        2          2021-03-05 17:46:36 -0300 -03   28d       default            <none>
weekly-20210304204541    Completed   0        2          2021-03-04 17:46:31 -0300 -03   27d       default            <none>
weekly-20210304120412    Completed   0        2          2021-03-04 09:05:03 -0300 -03   27d       default            <none>
```

**What did you expect to happen:**
Configuring `weekly` schedule with 5th field '0', I was expecting to have weekly backups on Sunday only and configuring `daily` schedule with 5th field '1-6', I was expecting to have daily backups from Monday to Saturday.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

```
velero create schedule daily --schedule=""45 20 2-31 * 1-6"" --ttl 168h
velero create schedule weekly --schedule=""45 20 2-31 * 0"" --ttl 744h
velero create schedule monthly --schedule=""45 20 1 * *"" --ttl 8784h
```

**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): 
```
Client:
        Version: v1.5.3
        Git commit: 123109a3bcac11dbb6783d2758207bac0d0817cb
Server:
        Version: v1.5.3
```
- Velero features (use `velero client config get features`): 
```
features: <NOT SET>
```
- Kubernetes version (use `kubectl version`): 
```
Client Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.0"", GitCommit:""9e991415386e4cf155a24b1da15becaa390438d8"", GitTreeState:""clean"", BuildDate:""2020-03-25T14:58:59Z"", GoVersion:""go1.13.8"", Compiler:""gc"", Platform:""windows/amd64""}
Server Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.16+IKS"", GitCommit:""8e07dc697bcf54fc6d4040c16fcd5768cec08e2d"", GitTreeState:""clean"", BuildDate:""2021-02-24T02:18:04Z"", GoVersion:""go1.13.15"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kubernetes installer & version: N/A, installed by cloud provider
- Cloud provider or hardware configuration: IBM Cloud
- OS (e.g. from `/etc/os-release`): Debian (installed by cloud provider)


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3536,OPEN,Add a manifest for backup/restore,Enhancement/Dev; P1 - Important,2021-03-06 01:12:21 +0000 UTC,dsu-igeek,Opened,,"**Describe the problem/challenge you have**
We would like Velero to be able to:

- Have a dry-run option that allows users to see what Velero is going to do before it does a backup or restore
- Execute multiple backups or restores simultaneously
- Restart backups in the event of a server restart
- Order the restore of resources intelligently


**Describe the solution you'd like**
Currently, Velero does not have a complete manifest of everything in the backup, aside from the backup tarball itself.
This change introduces a new data structure to be stored with a backup in object storage which will allow for more efficient operations in reporting of what a backup contains.
Additionally, this manifest should enable advancements in Velero's features and architecture, enabling dry-run support, concurrent backup and restore operations, and reliable restoration of complex applications.


**Environment:**

- Velero version (use `velero version`):
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3535,OPEN,Design doc for multiple cluster support,Area/Design; P1 - Important,2021-03-06 01:39:25 +0000 UTC,dsu-igeek,Opened,,"**Describe the problem/challenge you have**
Velero should be able to handle multiple clusters with a single Velero install. Scheduling and backup management should be centralized. Restore/cloning of clusters should be easy via the Velero API in conjunction with CAPI. It should be possible to backup all clusters managed by a Cluster API Management cluster similar to the way we can back up all namespaces within a single cluster. (https://github.com/vmware-tanzu/velero/issues/3534)

**Describe the solution you'd like**
Design document for how multiple clusters will be handled with subprojects and phases.


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3534,OPEN,Manage backup/restore in multiple clusters from a single Velero instance,Enhancement/User; Epic; P1 - Important,2021-03-06 00:57:36 +0000 UTC,dsu-igeek,Opened,,"**Describe the problem/challenge you have**

Creating multiple Kubernetes clusters has become easier with solutions such as Kubernetes Cluster API (https://cluster-api.sigs.k8s.io/).  Installing and managing multiple Velero instances to deal with the fleet of Kubernetes Clusters is cumbersome.  
 
**Describe the solution you'd like**
Velero should be able to handle multiple clusters with a single Velero install.  Scheduling and backup management should be centralized.  Restore/cloning of clusters should be easy via the Velero API in conjunction with CAPI.  It should be possible to backup all clusters managed by a Cluster API Management cluster similar to the way we can back up all namespaces within a single cluster.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,dsu,"
--
Design doc task - https://github.com/vmware-tanzu/velero/issues/3535

--
",,,,,,,,,,
3533,OPEN,Upload progress monitoring for plugins after backup completion,Area/Plugins; Enhancement/User; Epic; P1 - Important,2021-03-10 01:46:22 +0000 UTC,dsu-igeek,Opened,,"**Describe the problem/challenge you have**

Volume snapshotter plug-in are used by Velero to take snapshots of persistent volume contents. 
Depending on the underlying storage system, those snapshots may be available to use immediately, 
they may be uploaded to stable storage internally by the plug-in or they may need to be uploaded after
the snapshot has been taken. We would like for Velero to continue on to the next part of the backup as quickly
as possible but we would also like the backup to not be marked as complete until it is a usable backup.  We'd also
eventually like to bring the control of upload under the control of Velero and allow the user to make decisions
about the ultimate destination of backup data independent of the storage system they're using.


**Describe the solution you'd like**
Enable monitoring of uploads that are in-progress without blocking backups from starting.


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3532,OPEN,Flaky e2e tests,E2E Tests; P1 - Important,2021-03-12 22:12:32 +0000 UTC,ashish-amarnath,Opened,,"**What steps did you take and what happened:**
Velero E2E tests have non-deterministic waits/sleep that can cause the tests to occasionally produce false negatives.
We should use conditional waiting instead of non-deterministic waits to obtain more consistent test results.


**What did you expect to happen:**


**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
- `velero backup logs <backupname>`
- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): 
- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,ashish,"
--
PR #3527 addresses the flake in the `kibishii_tests`

--
",codegold79,"
--
@ashish-amarnath, PR #3564 that fixes the API group versions e2e tests is ready to review.
--
",,,,,,,,
3531,OPEN,Generate a test plan for Velero,Area/Design,2021-03-05 22:18:58 +0000 UTC,dsu-igeek,Opened,,"Velero should have a test plan that documents how features are tested.  This should include limits for features, expected behavior and inputs/outputs. 


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3530,OPEN,When using Velero w/ vSphere plugin; AWS volume snapshotter is still invoked,Area/Plugins,2021-03-05 17:49:23 +0000 UTC,nrb,Opened,,"**What steps did you take and what happened:**
On clusters with the vSphere plugin, the AWS object storage plugin is required in order to move snapshots out of the cluster.

However, when the AWS plugin image is included, the volume snapshotter plugin is automatically registered, and looks for associated VolumeSnapshotLocations.

In a vSphere deployment scenario, it is unusual that a cluster would have vSphere and EBS volumes; it's often the case that one or the other is used.

This this configuration, the following ignorable error can show up.

```
time=""2021-02-22T03:42:04Z"" level=error msg=""Error getting volume snapshotter for volume snapshot location"" backup=velero/mgtbkbase-full error=""rpc error: code = Unknown desc = missing region in aws configuration"" error.file=""/go/src/github.com/vmware-tanzu/velero-plugin-for-aws/velero-plugin-for-aws/volume_snapshotter.go:76"" error.function=""main.(*VolumeSnapshotter).Init"" logSource=""pkg/backup/item_backupper.go:448"" name=pvc-ae555d7e-3883-4ac7-84aa-cd8967f578aa namespace= persistentVolume=pvc-ae555d7e-3883-4ac7-84aa-cd8967f578aa resource=persistentvolumes volumeSnapshotLocation=default
```

**What did you expect to happen:**
Having a VolumeSnapshot plugin installed should not throw errors if it is unused.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
- `velero backup logs <backupname>`
- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`


**Anything else you would like to add:**

There is a workaround here, which is to add a VolumeSnapshotLocation for the plugin to read but essentially does nothing. This isn't really desirable, though, as it's extra configuration work.

One option for addressing this may be introducing a way for plugins to be de-registered, even if they're included in the same binary like the AWS object store/volume snapshotter are.


**Environment:**

- Velero version (use `velero version`): Velero: 1.4.3
- Kubernetes installer & version: TKGm: 1.2.1 (vSphere6.7u3)
- vsphere plugin version 1.0.2

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3526,OPEN,"Velero installation fails for AKS can't get pass ""checking namespace""",Needs info,2021-03-09 21:59:51 +0000 UTC,luisbtejada,In progress,,"Installing velero in Azure kubernetes service with private cluster enabled.

This is the installation command used:
velero install \
--provider azure \
--image velero/velero:v1.2.0 \
--plugins velero/velero-plugin-for-microsoft-azure:v1.0.1 \
--bucket velero \
--secret-file ./credentials-velero \
--backup-location-config resourceGroup=""css-preprod-1-westus-1"",storageAccount=""csspreprod1westus1st1"",subscriptionId=""52c48814-88e2-4d41-ae63-191e74b07d5b"" \
--snapshot-location-config resourceGroup=""css-preprod-1-westus-1"",subscriptionId=""52c48814-88e2-4d41-ae63-191e74b07d5b"",apiTimeout=15m \
--velero-pod-cpu-limit=""0"" --velero-pod-mem-limit=""0"" \
--velero-pod-mem-request=""0"" --velero-pod-cpu-request=""0""

These are the logs:

(preprod)[super-user@css-suf ~]$ kubectl logs -f deployment/velero -n velero
time=""2021-03-04T22:24:55Z"" level=info msg=""setting log-level to INFO"" logSource=""pkg/cmd/server/server.go:171""
time=""2021-03-04T22:24:55Z"" level=info msg=""Starting Velero server v1.2.0 (5d008491bbf681658d3e372da1a9d3a21ca4c03c)"" logSource=""pkg/cmd/server/server.go:173""
time=""2021-03-04T22:24:55Z"" level=info msg=""1 feature flags enabled []"" logSource=""pkg/cmd/server/server.go:175""
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pod
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pv
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service-account
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/add-pv-from-pvc
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/add-pvc-from-pod
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/change-storage-class
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/cluster-role-bindings
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/job
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pod
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/restic
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/role-bindings
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service-account
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/plugins/velero-plugin-for-microsoft-azure kind=VolumeSnapshotter logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/azure
time=""2021-03-04T22:24:55Z"" level=info msg=""registering plugin"" command=/plugins/velero-plugin-for-microsoft-azure kind=ObjectStore logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/azure
time=""2021-03-04T22:24:55Z"" level=info msg=""Checking existence of namespace"" logSource=""pkg/cmd/server/server.go:337"" namespace=velero
An error occurred: Get https://10.68.248.1:443/api/v1/namespaces/velero: dial tcp 10.68.248.1:443: i/o timeout

It seems that the installation cannot get pass checking if the namespace velero exists, which it does, and starts restarting inmediately.

",,,dsu,"
--
You're installing Velero 1.2.0.  Please try with Velero 1.5.3 and Azure plugin 1.1.2
--

--
Not sure.  Do you know where that 10.68.248.1 was specified?  Is that the Azure endpoint?
--
",luisbtejada,"
--
@dsu-igeek it still fails:

### The command used:
velero install \
--provider azure \
--image velero/velero:v1.5.3 \
--plugins velero/velero-plugin-for-microsoft-azure:v1.1.2 \
--bucket velero \
--secret-file ./credentials-velero \
--backup-location-config resourceGroup=""css-preprod-1-westus-1"",storageAccount=""csspreprod1westus1st1"",subscriptionId=""52c48814-88e2-4d41-ae63-191e74b07d5b"" \
--snapshot-location-config resourceGroup=""css-preprod-1-westus-1"",subscriptionId=""52c48814-88e2-4d41-ae63-191e74b07d5b"",apiTimeout=15m \
--velero-pod-cpu-limit=""0"" --velero-pod-mem-limit=""0"" \
--velero-pod-mem-request=""0"" --velero-pod-cpu-request=""0""

### Output: 

CustomResourceDefinition/backups.velero.io: attempting to create resource
CustomResourceDefinition/backups.velero.io: created
CustomResourceDefinition/backupstoragelocations.velero.io: attempting to create resource
CustomResourceDefinition/backupstoragelocations.velero.io: created
CustomResourceDefinition/deletebackuprequests.velero.io: attempting to create resource
CustomResourceDefinition/deletebackuprequests.velero.io: created
CustomResourceDefinition/downloadrequests.velero.io: attempting to create resource
CustomResourceDefinition/downloadrequests.velero.io: created
CustomResourceDefinition/podvolumebackups.velero.io: attempting to create resource
CustomResourceDefinition/podvolumebackups.velero.io: created
CustomResourceDefinition/podvolumerestores.velero.io: attempting to create resource
CustomResourceDefinition/podvolumerestores.velero.io: created
CustomResourceDefinition/resticrepositories.velero.io: attempting to create resource
CustomResourceDefinition/resticrepositories.velero.io: created
CustomResourceDefinition/restores.velero.io: attempting to create resource
CustomResourceDefinition/restores.velero.io: created
CustomResourceDefinition/schedules.velero.io: attempting to create resource
CustomResourceDefinition/schedules.velero.io: created
CustomResourceDefinition/serverstatusrequests.velero.io: attempting to create resource
CustomResourceDefinition/serverstatusrequests.velero.io: created
CustomResourceDefinition/volumesnapshotlocations.velero.io: attempting to create resource
CustomResourceDefinition/volumesnapshotlocations.velero.io: created
Waiting for resources to be ready in cluster...
Namespace/velero: attempting to create resource
Namespace/velero: created
ClusterRoleBinding/velero: attempting to create resource
ClusterRoleBinding/velero: created
ServiceAccount/velero: attempting to create resource
ServiceAccount/velero: created
Secret/cloud-credentials: attempting to create resource
Secret/cloud-credentials: created
BackupStorageLocation/default: attempting to create resource
BackupStorageLocation/default: created
VolumeSnapshotLocation/default: attempting to create resource
VolumeSnapshotLocation/default: created
Deployment/velero: attempting to create resource
Deployment/velero: created
Velero is installed! ⛵ Use 'kubectl logs deployment/velero -n velero' to view the status.

### Logs:

kubectl logs -f deployment/velero -n velero
time=""2021-03-09T09:37:05Z"" level=info msg=""setting log-level to INFO"" logSource=""pkg/cmd/server/server.go:191""
time=""2021-03-09T09:37:05Z"" level=info msg=""Starting Velero server v1.5.3 (123109a3bcac11dbb6783d2758207bac0d0817cb)"" logSource=""pkg/cmd/server/server.go:193""
time=""2021-03-09T09:37:05Z"" level=info msg=""1 feature flags enabled []"" logSource=""pkg/cmd/server/server.go:195""
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/crd-remap-version
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pod
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pv
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service-account
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/add-pv-from-pvc
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/add-pvc-from-pod
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/change-pvc-node-selector
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/change-storage-class
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/cluster-role-bindings
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/crd-preserve-fields
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/init-restore-hook
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/job
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pod
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/restic
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/role-bindings
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service-account
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/plugins/velero-plugin-for-microsoft-azure kind=VolumeSnapshotter logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/azure
time=""2021-03-09T09:37:05Z"" level=info msg=""registering plugin"" command=/plugins/velero-plugin-for-microsoft-azure kind=ObjectStore logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/azure
An error occurred: Get ""https://10.68.248.1:443/api?timeout=32s"": dial tcp 10.68.248.1:443: i/o timeout


I'm a little lost here, I don't understand why it can't register the plugin or when using a previous version why it can't interact with the API.
--

--
@dsu-igeek Yes, that’s the endpoint of the private AKS.
What I can guess is that the plug-in is having some trouble communicating with the kube api.
--
",,,,,,,,
3521,OPEN,Velero Restic backup stucks indefinitely,Needs info,2021-03-09 20:06:18 +0000 UTC,talha0324,Opened,,"**What steps did you take and what happened:**
I deployed Velero 1.5.3 with helm with restic enabled, deployment looks like this

```
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: velero
  namespace: openshift-velero
spec:
  releaseName: velero
  chart:
    repository: https://vmware-tanzu.github.io/helm-charts
    name: velero
    version: 2.14.12
  values:
    image:
      repository: velero/velero
      tag: v1.5.3
      pullPolicy: Always
    initContainers:
    - name: ""velero-plugin-for-aws""
      image: ""velero/velero-plugin-for-aws:v1.1.0""
      imagePullPolicy: Always
      volumeMounts:
      - name: ""plugins""
        mountPath: ""/target""
    snapshotsEnabled: false
    backupsEnabled: true
    cleanUpCRDs: true
    configuration:
      # Cloud provider being used (e.g. aws, azure, gcp).
      provider: aws
      backupStorageLocation:
        name: default
        bucket: ""velero_backup""
        config:
          # insecureSkipTLSVerify: ""true""
          region: ""europe-se-1""
          # s3ForcePathStyle: true
          s3Url: ""https://object-eu-se-1a.binero.cloud""
      defaultVolumesToRestic: true
      backupSyncPeriod: 5m
      resticTimeout: 6h
    schedules:
    - default-schedule
      mybackup:
        schedule: ""*/5 * * * *""
        template:
          ttl: ""1h""
          includedNamespaces:
          - ""openshift-stakater-system""
          - ""openshift-monitoring""
    credentials:
      secretContents:
        cloud: |
          [default]
          aws_access_key_id = 7049a44a5d5a4c0bb70bd489c6d80924
          aws_secret_access_key = bc5b830f985643c2bb182bb05997ebf1
          
    deployRestic: true
    restic:
      privileged: true
      resources:
        requests:
          memory: ""200Mi""
          cpu: ""200m""
        limits:
          memory: ""200Mi""
          cpu: ""200m""
    configMaps:
      restic-restore-action-config:
        labels:
          velero.io/plugin-config: """"
          velero.io/restic: RestoreItemAction
        data:
          image: velero/velero-restic-restore-helper:v1.5.3
```

**What did you expect to happen:**
A smooth backup with Restic enabled

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero` [logs.txt](https://github.com/vmware-tanzu/velero/files/6084160/logs.txt)

- `velero backup describe <backupname>` 
```
Name:         default-schedule-20210304132959
Namespace:    openshift-velero
Labels:       velero.io/schedule-name=default-schedule
              velero.io/storage-location=default
Annotations:  velero.io/source-cluster-k8s-gitversion=v1.19.0+3b01205
              velero.io/source-cluster-k8s-major-version=1
              velero.io/source-cluster-k8s-minor-version=19

Phase:  InProgress

Errors:    0
Warnings:  0

Namespaces:
  Included:  openshift-stakater-system, openshift-monitoring
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  included

Label selector:  <none>

Storage Location:  default

Velero-Native Snapshot PVs:  auto

TTL:  72h0m0s

Hooks:  <none>

Backup Format Version:  1.1.0

Started:    2021-03-04 18:29:59 +0500 PKT
Completed:  <n/a>

Expiration:  2021-03-07 18:29:59 +0500 PKT

Estimated total items to be backed up:  3565
Items backed up so far:                 7

Velero-Native Snapshots: <none included>

Restic Backups (specify --details for more information):
  Completed:  5
  New:        1
```
- `velero backup logs <backupname>`:not available as backup is stuck
- `velero restore describe <restorename>` : not available as backup is stuck
- `velero restore logs <restorename>`: not available


**Anything else you would like to add:**
Cluster backup without restic works fine


**Environment:**

- Velero version (use `velero version`): 
```
Client:
	Version: v1.5.3
	Git commit: 123109a3bcac11dbb6783d2758207bac0d0817cb
Server:
	Version: v1.5.3
```
- Velero features (use `velero client config get features`): features: <NOT SET>
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.1"", GitCommit:""c4d752765b3bbac2237bf87cf0b1c2e307844666"", GitTreeState:""clean"", BuildDate:""2020-12-18T12:09:25Z"", GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""19"", GitVersion:""v1.19.0+3b01205"", GitCommit:""3b012051f912a6e29337c4087fcfc8161651690b"", GitTreeState:""clean"", BuildDate:""2021-01-16T09:10:53Z"", GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration: Openshift on Openstack


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,dsu,"
--
Restic backups need to finish moving data before the backup completes.  How much data are you backing up?  Did the backup ever complete?

If it hasn't completed, please get the logs from the daemonset with this command:

`kubectl logs -n velero daemonset.apps/restic`
--
",,,,,,,,,,
3520,OPEN,Automatic Backup Rotation for Schedules,Enhancement/User; Needs Product,2021-03-09 21:52:57 +0000 UTC,gmintoco,Opened,,"**Describe the problem/challenge you have**
Currently, you can set a backup TTL that will expire backup after a certain period of time. It would be really nice to be able to setup a rotation ie.
Keep the last 24hrs, 7 days, 4 weeks, 12 months, 3 years of backups

So all backups from the last 24hours, 1 backup for each day, 1 for each week, 1 for each month and 1 for each year 

This would allow long term retention for PVC data while not requiring crazy amounts of storage that would be needed if you kept all backups up to that retention time
**Describe the solution you'd like**
Probably a field in the schedule object like:

rotationSchedule:
  hours: 24
  days: 7
  weeks: 4
  months: 12
  years: 3

That would then set the schedule to automatically rotate backups over this schedule


**Anything else you would like to add:**


**Environment:**

- Velero version (use `velero version`): 1.5.3
- Kubernetes version (use `kubectl version`): 1.18.4
- Kubernetes installer & version: RKE2
- Cloud provider or hardware configuration: Vanilla K8S on Vmware
- OS (e.g. from `/etc/os-release`): Ubuntu 18.04 nodes

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3519,OPEN,Allow not skipping Complete jobs in velero restores.,Needs Product; Needs investigation,2021-03-09 21:52:24 +0000 UTC,daguilarv,Opened,,"**Describe the problem/challenge you have**
Currently we can see in restores logs that Completed Jobs are skipped in restore procedure.

time=""2021-03-04T09:57:31Z"" level=info msg=""baikal-system/elasticsearch-provision-zcsj4 is complete - skipping"" logSource=""pkg/restore/restore.go:853"" restore=baikal-infra/disaster-20210304095607
time=""2021-03-04T09:57:48Z"" level=info msg=""baikal-system/elasticsearch-provision is complete - skipping"" logSource=""pkg/restore/restore.go:853"" restore=baikal-infra/disaster-20210304095607

Just in case you have described some jobs completion checks in other pod init-containers, pods are stuck in init status forever

**Describe the solution you'd like**
[A clear and concise description of what you want to happen.]

I´d like a new flag in restore command to force job reapply.


**Environment:**

- Velero version (use `velero version`): 1.5.3
- Kubernetes version (use `kubectl version`): 1.19.7
- Kubernetes installer & version:
- Cloud provider or hardware configuration: Azure AKS
- OS (e.g. from `/etc/os-release`): Ubuntu

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3518,OPEN,Version compatibility matrix,Area/Documentation; P1 - Important,2021-03-09 21:51:38 +0000 UTC,divbell,Opened,,"**Describe the problem/challenge you have**

The release cycle for both Velero and Kubernetes is a fast-moving target.  Many users are stuck on various older versions of Kubernetes, particularly in a cloud managed kubernetes scenario, and there is ambiguity on what Velero versions are compatible with what Kubernetes versions.

**Describe the solution you'd like**

A clear compatibility matrix or description of Velero compatibility with k8s from 1.15 to present.

**Anything else you would like to add:**

No

**Environment:**

- Velero version (use `velero version`):
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3517,OPEN,Restore issue with Order.certmanager.k8s.io,Needs investigation,2021-03-17 09:57:41 +0000 UTC,daguilarv,In progress,,"**What steps did you take and what happened:**
We use certmanager with letsencrypt in our Kubernetes AKS deployment and we are trying to make a full cluster backup and restore.

Backup procedure works properly, but restore is failing when restoring order.certmanager.k8s.io:
Namespaces:
    disaster-default-ci:  error restoring orders.certmanager.k8s.io/disaster-default-ci/disaster-default-ci-wildcard-2907606739: Order.certmanager.k8s.io ""disaster-default-ci-wildcard-2907606739"" is invalid: status: Required value

**What did you expect to happen:**

Objects must be restored properly.

**The output of the following commands will help us better understand what's going on**:
velero restore describe disaster-20210303090004-20210303101039                                                                                                            
Name:         disaster-20210303090004-20210303101039
Namespace:    baikal-infra
Labels:       <none>
Annotations:  <none>

Phase:  PartiallyFailed (run 'velero restore logs disaster-20210303090004-20210303101039' for more information)

Warnings:
  -Not relevant-
Errors:
  Velero:     <none>
  Cluster:  <none>
  Namespaces:
    disaster-default-ci:  error restoring orders.certmanager.k8s.io/disaster-default-ci/disaster-default-ci-wildcard-2907606739: Order.certmanager.k8s.io ""disaster-default-ci-wildcard-2907606739"" is invalid: status: Required value


**Environment:**

- Velero version (use `velero version`): 1.5.3
- Velero features (use `velero client config get features`): None
- Kubernetes version (use `kubectl version`): 1.19.7
- Kubernetes installer & version:
- Cloud provider or hardware configuration: Azure AKS deployment
- OS (e.g. from `/etc/os-release`): Ubuntu


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,jbouchery,"
--
Hi ! 

Is there a workaround for this issue?
--

--
> > Hi !
> > Is there a workaround for this issue?
> 
> You can exclude orders from restore and everything would work.

Thanks for the quick response @daguilarv :)
--
",daguilarv,"
--
> Hi !
> 
> Is there a workaround for this issue?

You can exclude orders from restore and everything would work.
--
",,,,,,,,
3516,OPEN,Restore issue with MutatingWebhookConfiguration v1beta1 API version,Needs investigation,2021-03-09 21:50:33 +0000 UTC,daguilarv,Opened,,"**What steps did you take and what happened:**
Restoring v1beta1 MutatingWebhookConfiguration into a v1.19.7 cluster is failing with following error description:
Errors:
  Velero:     <none>
  Cluster:  error restoring mutatingwebhookconfigurations.admissionregistration.k8s.io/add-init-containers-to-spark-pods: MutatingWebhookConfiguration.admissionregistration.k8s.io ""add-init-containers-to-spark-pods"" is invalid: webhooks[0].sideEffects: Unsupported value: ""Unknown"": supported values: ""None"", ""NoneOnDryRun""
.
It seems the same bug in described https://github.com/vmware-tanzu/velero/issues/2159 for CRDs but for MutatingWebhooksConfiguration. Objects created with v1beta1 API should be restored with the same apiserversion instead of using v1. 

**What did you expect to happen:**

Objects must be properly restored.

**The output of the following commands will help us better understand what's going on**:
velero restore describe disaster-20210303090004-20210303101039                                                                                                            
Name:         disaster-20210303090004-20210303101039
Namespace:    baikal-infra
Labels:       <none>
Annotations:  <none>

Phase:  PartiallyFailed (run 'velero restore logs disaster-20210303090004-20210303101039' for more information)

Warnings:
  -Not relevant-
Errors:
  Velero:     <none>
  Cluster:  error restoring mutatingwebhookconfigurations.admissionregistration.k8s.io/add-init-containers-to-spark-pods: MutatingWebhookConfiguration.admissionregistration.k8s.io ""add-init-containers-to-spark-pods"" is invalid: webhooks[0].sideEffects: Unsupported value: ""Unknown"": supported values: ""None"", ""NoneOnDryRun""
            error restoring mutatingwebhookconfigurations.admissionregistration.k8s.io/add-tmp-volume-to-spark-pods: MutatingWebhookConfiguration.admissionregistration.k8s.io ""add-tmp-volume-to-spark-pods"" is invalid: webhooks[0].sideEffects: Unsupported value: ""Unknown"": supported values: ""None"", ""NoneOnDryRun""
            error restoring mutatingwebhookconfigurations.admissionregistration.k8s.io/add-toleration-to-spark-pods: MutatingWebhookConfiguration.admissionregistration.k8s.io ""add-toleration-to-spark-pods"" is invalid: webhooks[0].sideEffects: Unsupported value: ""Unknown"": supported values: ""None"", ""NoneOnDryRun""
            error restoring mutatingwebhookconfigurations.admissionregistration.k8s.io/aks-webhook-admission-controller: MutatingWebhookConfiguration.admissionregistration.k8s.io ""aks-webhook-admission-controller"" is invalid: webhooks[0].sideEffects: Unsupported value: ""Unknown"": supported values: ""None"", ""NoneOnDryRun""
            error restoring mutatingwebhookconfigurations.admissionregistration.k8s.io/coredns-deployment-metrics-and-readiness: MutatingWebhookConfiguration.admissionregistration.k8s.io ""coredns-deployment-metrics-and-readiness"" is invalid: webhooks[0].sideEffects: Unsupported value: ""Unknown"": supported values: ""None"", ""NoneOnDryRun""
            error restoring mutatingwebhookconfigurations.admissionregistration.k8s.io/override-node-selector-to-spark-drivers: MutatingWebhookConfiguration.admissionregistration.k8s.io ""override-node-selector-to-spark-drivers"" is invalid: webhooks[0].sideEffects: Unsupported value: ""Unknown"": supported values: ""None"", ""NoneOnDryRun""
            error restoring validatingwebhookconfigurations.admissionregistration.k8s.io/elastic-webhook.k8s.elastic.co: ValidatingWebhookConfiguration.admissionregistration.k8s.io ""elastic-webhook.k8s.elastic.co"" is invalid: [webhooks[0].sideEffects: Unsupported value: ""Unknown"": supported values: ""None"", ""NoneOnDryRun"", webhooks[1].sideEffects: Unsupported value: ""Unknown"": supported values: ""None"", ""NoneOnDryRun""]

Backup:  disaster-20210303090004

Namespaces:
  Included:  *
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io
  Cluster-scoped:  auto

Namespace mappings:  <none>

Label selector:  <none>

Restore PVs:  auto



**Environment:**

- Velero version (use `velero version`): 1.5.3
- Velero features (use `velero client config get features`): None
- Kubernetes version (use `kubectl version`): 1.19.7
- Kubernetes installer & version:
- Cloud provider or hardware configuration: Azure AKS deployment
- OS (e.g. from `/etc/os-release`): Ubuntu


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3513,OPEN,PartiallyFailed Backup because restic issue,Needs info,2021-03-12 12:01:53 +0000 UTC,waldo2188,In progress,,"**What steps did you take and what happened:**
I have an issue on a schedule backup set on Velero.  
This schedule must backup the volume of a specific pod.  
This volume contains approximately 33 7761 files for a weight of 2GB.  

Only 223728 files are backed up.

Restic fail with the error ""signal: killed"".

I've check if the restic pod is OOMKiled, but it's not the case.

**What did you expect to happen:**
The full backup of the volume.

**The output of the following commands will help us better understand what's going on**:
https://gist.github.com/waldo2188/b8840be7b8829fc749881ccd1dba3ae4

**Anything else you would like to add:**
It is possible that during backup some files are still being created on the storage media. I don't know if this is of any importance.

I've other schedule backup on this cluster who save 15Gb of data without any problems.

**Environment:**

- Velero version (use `velero version`):  v1.5.3
- Velero features (use `velero client config get features`): NOT SET
- Kubernetes version (use `kubectl version`): v1.20.1
- Kubernetes installer & version: On premise 
- Hardware configuration: 4 CPU 16GbRAM
- OS (e.g. from `/etc/os-release`):Debian GNU/Linux 10 (buster), kernel : Linux 4.19.0-14-amd64
",,,dsu,"
--
Does it always fail or did only fail once?
--

--
Hmmmm...I will try making some large backups and see if I can reproduce this.  For the other cluster that is successfully backing up 15GB, do you know how many files are involved?

Looks like your target is AWS S3, correct?
--
",waldo2188,"
--
Here are the latest executions of the backup process:
```
$: velero backup get
NAME                                STATUS            ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
some_app-only-20210310050015         Failed            0        0          2021-03-10 06:00:15 +0100 CET   29d       default            app=some_app
some_app-only-20210309050015         PartiallyFailed   2        0          2021-03-09 06:00:15 +0100 CET   28d       default            app=some_app
some_app-only-20210308050015         Completed         0        0          2021-03-08 06:00:15 +0100 CET   27d       default            app=some_app
some_app-only-20210307050010         PartiallyFailed   2        0          2021-03-07 06:00:10 +0100 CET   26d       default            app=some_app
some_app-only-20210306050010         PartiallyFailed   2        0          2021-03-06 06:00:10 +0100 CET   25d       default            app=some_app
some_app-only-20210305050010         Completed         0        0          2021-03-05 06:00:10 +0100 CET   24d       default            app=some_app
some_app-only-20210304050010         PartiallyFailed   2        0          2021-03-04 06:00:10 +0100 CET   23d       default            app=some_app
some_app-only-20210303050009         PartiallyFailed   2        0          2021-03-03 06:00:09 +0100 CET   22d       default            app=some_app
some_app-only-20210302050009         PartiallyFailed   2        0          2021-03-02 06:00:09 +0100 CET   21d       default            app=some_app
some_app-only-20210301092715         PartiallyFailed   2        0          2021-03-01 11:54:14 +0100 CET   21d       default            app=some_app

```
--

--
The target was a S3 API of a Swift infrastructure manage by OVH.
But, hum... The datacenter where data was stored burned to the ground...
So, I can't give you more data about this issue. Sorry.
--
",,,,,,,,
3508,OPEN,Update Azure plugin README to use latest version in install commands,Area/Cloud/Azure; Area/Documentation; Needs investigation,2021-03-11 12:22:04 +0000 UTC,amankohli,In progress,,"We followed the below steps:

i) Created velero credentials:
cat << EOF  > ./credentials-velero
AZURE_SUBSCRIPTION_ID=${AZURE_SUBSCRIPTION_ID}
AZURE_RESOURCE_GROUP=${AZURE_RESOURCE_GROUP}
AZURE_CLOUD_NAME=AzureUsGovenmentCloud
EOF

ii)Installed Velero using the below command:
velero install \
    --provider azure \
    --plugins velero/velero-plugin-for-microsoft-azure:v1.1.0 \
    --bucket $BLOB_CONTAINER \
    --secret-file ./credentials-velero \
    --backup-location-config resourceGroup=$AZURE_BACKUP_RESOURCE_GROUP,storageAccount=$AZURE_STORAGE_ACCOUNT_ID[,subscriptionId=$AZURE_BACKUP_SUBSCRIPTION_ID] \
    --snapshot-location-config apiTimeout=<YOUR_TIMEOUT>[,resourceGroup=$AZURE_BACKUP_RESOURCE_GROUP,subscriptionId=$AZURE_BACKUP_SUBSCRIPTION_ID]


iii) Added the below label to velero deployment:
  labels:
    aadpodidbinding:${IdentityName}
    component: velero

We get the below error:
time=""2021-02-26T06:55:50Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-26T06:55:50Z"" level=error msg=""Error getting a backup store"" backupstoragelocation=default controller=backupstoragelocation error=""rpc error: code = Unknown desc = unable to get all required environment variables: the following keys do not have values: AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET"" error.file=""/go/src/github.com/vmware-tanzu/velero-plugin-for-microsoft-azure/velero-plugin-for-microsoft-azure/object_store.go:178"" error.function=main.getStorageAccountKey logSource=""pkg/controller/backupstoragelocation_controller.go:87""
time=""2021-02-26T06:55:50Z"" level=i


Velero should just use AADPOD Identity and not look for client secret



",,,zubron,"
--
This looks like an issue with the README for the plugin. Support for AAD Pod Identity was released in [v1.1.1](https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure/releases/tag/v1.1.1) but the README gives instructions to install the v1.1.0 version.

Please try using v1.1.1, or the latest [v1.1.2](https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure/releases/tag/v1.1.2) release.

I am going to leave this issue open and change the title to track updating the README to reference the latest version of the plugin.
--
",da1rren,"
--
I can confirm using v1.2 works.  Just shipped my first backups after encountering the same issue.
--
",theok,"
--
I tested the new plugin version (v1.1.2) on a AzureUsGovenmentCloud region and get the following

i) Created velero credentials:
cat << EOF > ./credentials-velero
AZURE_SUBSCRIPTION_ID=${AZURE_SUBSCRIPTION_ID}
AZURE_RESOURCE_GROUP=${AZURE_RESOURCE_GROUP}
AZURE_CLOUD_NAME=AzureUSGovernmentCloud
EOF

ii)Installed Velero using the below command:
velero install
--provider azure
--plugins velero/velero-plugin-for-microsoft-azure:v1.1.2
--bucket $BLOB_CONTAINER
--secret-file ./credentials-velero
--backup-location-config resourceGroup=$AZURE_BACKUP_RESOURCE_GROUP,storageAccount=$AZURE_STORAGE_ACCOUNT_ID[,subscriptionId=$AZURE_BACKUP_SUBSCRIPTION_ID]
--snapshot-location-config apiTimeout=<YOUR_TIMEOUT>[,resourceGroup=$AZURE_BACKUP_RESOURCE_GROUP,subscriptionId=$AZURE_BACKUP_SUBSCRIPTION_ID]

iii) Added the below label to velero deployment:
labels:
aadpodidbinding:${IdentityName}
component: velero

```
time=""2021-03-01T13:47:00Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-03-01T13:47:01Z"" level=error msg=""Error getting a backup store"" backupstoragelocation=default controller=backupstoragelocation error=""rpc error: code = Unknown desc = storage.AccountsClient#ListKeys: Failure responding to request: StatusCode=401 -- Original Error: autorest/azure: Service returned an error. Status=401 Code=\""InvalidAuthenticationTokenAudience\"" Message=\""The access token has been obtained for wrong audience or resource 'https://management.azure.com/'. It should exactly match with one of the allowed audiences 'https://management.core.usgovcloudapi.net/','https://management.core.usgovcloudapi.net','https://management.usgovcloudapi.net/','https://management.usgovcloudapi.net'.\"""" error.file=""/go/src/velero-plugin-for-microsoft-azure/velero-plugin-for-microsoft-azure/object_store.go:213"" error.function=main.getStorageAccountKey logSource=""pkg/controller/backupstoragelocation_controller.go:87""
time=""2021-03-01T13:47:01Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-03-01T13:47:01Z"" level=error msg=""Error getting backup store for this location"" backupLocation=default controller=backup-sync error=""rpc error: code = Unknown desc = storage.AccountsClient#ListKeys: Failure responding to request: StatusCode=401 -- Original Error: autorest/azure: Service returned an error. Status=401 Code=\""InvalidAuthenticationTokenAudience\"" Message=\""The access token has been obtained for wrong audience or resource 'https://management.azure.com/'. It should exactly match with one of the allowed audiences 'https://management.core.usgovcloudapi.net/','https://management.core.usgovcloudapi.net','https://management.usgovcloudapi.net/','https://management.usgovcloudapi.net'.\"""" error.file=""/go/src/velero-plugin-for-microsoft-azure/velero-plugin-for-microsoft-azure/object_store.go:213"" error.function=main.getStorageAccountKey logSource=""pkg/controller/backup_sync_controller.go:168"" 
```

I am not sure if there is bug or there is something more missing from the README for the plugin.
--

--
In line https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure/blob/1f34714a0ea78b1d0a57cc53ad317deb351042cf/velero-plugin-for-microsoft-azure/object_store.go#L207 

env.ResourceManagerEndpoint returns the correct endpoint for `AZURE_CLOUD_NAME=AzureUSGovernmentCloud`. 

I don't understand why code defaults to `resource 'https://management.azure.com/'` in the logs
--
",carlisia,"
--
I think this needs some looking into. I'd start with this: https://stackoverflow.com/questions/34384409/azure-the-access-token-has-been-obtained-from-wrong-audience-or-resource.
--
",amankohli,"
--
@carlisia  Do we have an update on the issue?

We are blocked on our deliverables due to the MSI issue, please let us know if they is a workaround
--
",,
3504,OPEN,Add Jenting to all plugin PR auto-assign GH actions,Area/Plugins; Good first issue,2021-02-25 01:41:05 +0000 UTC,carlisia,In progress,,,,,carlisia,"
--
c/c @jenting 
--
",,,,,,,,,,
3500,OPEN,Move Velero containers to a distroless base image,Enhancement/Dev; P2 - Long-term important; Security,2021-03-29 08:51:50 +0000 UTC,dsu-igeek,Opened,,"**Describe the problem/challenge you have**
Currently the ubunu:focal base image is used for the Velero container.  This contains a number of libraries and other OS related support files that are unnecessary and often include security vulnerabilities.

**Describe the solution you'd like**
Move to a very minimal base image, such as google Distroless.  https://github.com/GoogleContainerTools/distroless/blob/master/examples/go/Dockerfile


**Anything else you would like to add:**
This needs to be tested with e2e tests and all plugins.

Should fix https://github.com/vmware-tanzu/velero/issues/3003


**Environment:**

- Velero version (use `velero version`):
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,SatyKrish,"
--
#3003  is a blocker for us from using velero. Would appreciate if this can be released earlier (1.5.x/1.6.x).
--
",ksandermann,"
--
same here - currently #3003 is blocking us from using velero due to sec-compliance
--
",carsten,"
--
yep, velero blocks the using in a pci-dss environment because of issues in the baseimage
--
",benendboss,"
--
Blocker to use velero in production
--
",,,,
3493,OPEN,Install Velero using Carvel YTT,Area/CLI; Enhancement/User; P1 - Important,2021-03-06 01:36:16 +0000 UTC,dsu-igeek,Opened,,"**Describe the problem/challenge you have**
We would like to be able to install Velero via Carvel

**Describe the solution you'd like**
Carvel template need to be generated that can install Velero in:

- AWS
- Azure
- GCP
- vSphere
- Generic cluster

https://github.com/vmware-tanzu/carvel-ytt


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3472,OPEN,Velero not working without Kubernetes metrics Server,Needs investigation,2021-03-01 19:37:41 +0000 UTC,nschhina,Opened,,"**What steps did you take and what happened:**

velero not able to discover the custom.metrics.k8s.io/v1beta1
evel=warning msg=""Failed to discover group: custom.metrics.k8s.io/v1beta1"" error=""the server is currently unable to handle the request"" logSource=""pkg/discovery/helper.go:154""

**What did you expect to happen:**

Velero to do proper backups without any errors, as metrics were disabled using helm chart installatio


**Anything else you would like to add:**

Why does velero expect kubernetes metrics server to be running if metrics are disabled anyways? Is this a hard requirement for velero backups to happen?

**Environment:**

- Velero version (use `velero version`):  1.5.2
- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`): 1.16
- Kubernetes installer & version:
- Cloud provider or hardware configuration: aws
- OS (e.g. from `/etc/os-release`):


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3471,OPEN,Velero sometimes does not backup API resources installed up to five minutes before backup,Needs investigation,2021-03-01 19:39:29 +0000 UTC,codegold79,Opened,,"I've filed this issue as a bug, but it may just be how Velero is designed to work. If that's the case, I suggest updating the documentation to say that resources need up to five minutes to be recogized by Velero. Suggest ways to make the process faster in the docs as well, such as deleting a Velero pod after installing a resource and Velero will pick it up right away.

@lighting-wings suggested in the Discussions #3295 refreshing discovery just before a backup is performed. It might be better than refreshing every 5 minutes. Only update before a backup.

**What steps did you take and what happened:**
1. install Velero
1. install cert-manager with `kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.0.3/cert-manager.yaml`
1. install CRD with `kubectl apply -f https://raw.githubusercontent.com/brito-rafa/k8s-webhooks/master/examples-for-projectvelero/case-a/source/case-a-source.yaml` and wait for pods to be running.
1. create namespace with `kubectl create namespace rockbands-v1`
1. install a custom resource with `kubectl create -n rockbands-v1 -f https://raw.githubusercontent.com/brito-rafa/k8s-webhooks/master/examples-for-projectvelero/case-a/source/music/config/samples/music_v1_rockband.yaml`
1. run velero backup using `--include-namespaces rockbands-v1` 
1. verify CRs are there with `kubectl get rockbands -A -o yaml`
1. checked backup tarball, and the installed CR wasn't there

**What did you expect to happen:**
I expected the backup tarball to contain the custom resource that was on the cluster during backup.

**The output of the following commands will help us better understand what's going on**:

- `kubectl logs deployment/velero -n velero`
```
time=""2021-02-17T19:36:50Z"" level=info msg=""setting log-level to INFO"" logSource=""pkg/cmd/server/server.go:191""
time=""2021-02-17T19:36:50Z"" level=info msg=""Starting Velero server v1.5.3 (123109a3bcac11dbb6783d2758207bac0d0817cb)"" logSource=""pkg/cmd/server/server.go:193""
time=""2021-02-17T19:36:50Z"" level=info msg=""1 feature flags enabled [EnableAPIGroupVersions]"" logSource=""pkg/cmd/server/server.go:195""
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/crd-remap-version
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pod
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pv
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service-account
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/add-pv-from-pvc
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/add-pvc-from-pod
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/change-pvc-node-selector
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/change-storage-class
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/cluster-role-bindings
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/crd-preserve-fields
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/init-restore-hook
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/job
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pod
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/restic
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/role-bindings
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service-account
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/plugins/velero-plugin-for-aws kind=VolumeSnapshotter logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/aws
time=""2021-02-17T19:36:50Z"" level=info msg=""registering plugin"" command=/plugins/velero-plugin-for-aws kind=ObjectStore logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/aws
time=""2021-02-17T19:36:50Z"" level=info msg=""Checking existence of namespace."" logSource=""pkg/cmd/server/server.go:380"" namespace=velero
time=""2021-02-17T19:36:50Z"" level=info msg=""Namespace exists"" logSource=""pkg/cmd/server/server.go:386"" namespace=velero
I0217 19:36:52.118282       1 request.go:621] Throttling request took 1.0463997s, request: GET:https://10.96.0.1:443/apis/autoscaling/v2beta2?timeout=32s
time=""2021-02-17T19:36:52Z"" level=info msg=""The 'EnableAPIGroupVersions' feature flag was specified, using all API group versions."" logSource=""pkg/discovery/helper.go:159""
time=""2021-02-17T19:36:53Z"" level=info msg=""Checking existence of Velero custom resource definitions"" logSource=""pkg/cmd/server/server.go:415""
time=""2021-02-17T19:36:53Z"" level=info msg=""All Velero custom resource definitions exist"" logSource=""pkg/cmd/server/server.go:449""
time=""2021-02-17T19:36:53Z"" level=warning msg=""Velero restic daemonset not found; restic backups/restores will not work until it's created"" logSource=""pkg/cmd/server/server.go:495""
time=""2021-02-17T19:36:53Z"" level=info msg=""Starting controllers"" logSource=""pkg/cmd/server/server.go:571""
time=""2021-02-17T19:36:53Z"" level=info msg=""Starting metric server at address [:8085]"" logSource=""pkg/cmd/server/server.go:578""
time=""2021-02-17T19:36:53Z"" level=info msg=""Backup sync period is 1m0s"" logSource=""pkg/controller/backup_sync_controller.go:76""
time=""2021-02-17T19:36:53Z"" level=info msg=""Waiting for informer caches to sync"" logSource=""pkg/cmd/server/server.go:836""
time=""2021-02-17T19:36:55Z"" level=info msg=""Done waiting for informer caches to sync"" logSource=""pkg/cmd/server/server.go:839""
time=""2021-02-17T19:36:55Z"" level=info msg=""Informer cache synced"" informer=""*v1.Backup"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-02-17T19:36:55Z"" level=info msg=""Informer cache synced"" informer=""*v1.Restore"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-02-17T19:36:55Z"" level=info msg=""Informer cache synced"" informer=""*v1.PodVolumeBackup"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-02-17T19:36:55Z"" level=info msg=""Informer cache synced"" informer=""*v1.VolumeSnapshotLocation"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-02-17T19:36:55Z"" level=info msg=""Informer cache synced"" informer=""*v1.DownloadRequest"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-02-17T19:36:55Z"" level=info msg=""Informer cache synced"" informer=""*v1.ResticRepository"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-02-17T19:36:55Z"" level=info msg=""Informer cache synced"" informer=""*v1.Schedule"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-02-17T19:36:55Z"" level=info msg=""Informer cache synced"" informer=""*v1.DeleteBackupRequest"" logSource=""pkg/cmd/server/server.go:850""
time=""2021-02-17T19:36:55Z"" level=info msg=""Server starting..."" logSource=""pkg/cmd/server/server.go:896""
time=""2021-02-17T19:36:55Z"" level=info msg=""Starting controller"" controller=schedule logSource=""pkg/controller/generic_controller.go:76""
time=""2021-02-17T19:36:55Z"" level=info msg=""Starting controller"" controller=gc-controller logSource=""pkg/controller/generic_controller.go:76""
time=""2021-02-17T19:36:55Z"" level=info msg=""Starting controller"" controller=backup-deletion logSource=""pkg/controller/generic_controller.go:76""
time=""2021-02-17T19:36:55Z"" level=info msg=""Checking for expired DeleteBackupRequests"" controller=backup-deletion logSource=""pkg/controller/backup_deletion_controller.go:595""
time=""2021-02-17T19:36:55Z"" level=info msg=""Done checking for expired DeleteBackupRequests"" controller=backup-deletion logSource=""pkg/controller/backup_deletion_controller.go:623""
time=""2021-02-17T19:36:55Z"" level=info msg=""Starting controller"" controller=restore logSource=""pkg/controller/generic_controller.go:76""
time=""2021-02-17T19:36:55Z"" level=info msg=""Starting controller"" controller=restic-repository logSource=""pkg/controller/generic_controller.go:76""
time=""2021-02-17T19:36:55Z"" level=info msg=""Starting controller"" controller=downloadrequest logSource=""pkg/controller/generic_controller.go:76""
time=""2021-02-17T19:36:55Z"" level=info msg=""Starting controller"" controller=backup-sync logSource=""pkg/controller/generic_controller.go:76""
time=""2021-02-17T19:36:55Z"" level=info msg=""Starting controller"" controller=backup logSource=""pkg/controller/generic_controller.go:76""
time=""2021-02-17T19:36:55Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:36:55Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:36:55Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:36:55Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:36:55Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:36:55Z"" level=info msg=""Found 30 backups in the backup location that do not exist in the cluster and need to be synced"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:197""
time=""2021-02-17T19:36:55Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-3d808bdb-a491-410d-a585-0fe107e94faa-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:55Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:36:55Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:36:55Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:36:55Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:36:55Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:36:55Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:36:55Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:36:55Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:36:56Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:36:56Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:36:56Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:36:56Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:36:57Z"" level=info msg=""The 'EnableAPIGroupVersions' feature flag was specified, using all API group versions."" logSource=""pkg/discovery/helper.go:159""
time=""2021-02-17T19:36:57Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-3d808bdb-a491-410d-a585-0fe107e94faa-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:57Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-f67c499b-c164-4879-bbb9-8ebe2b533a95-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:57Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-f67c499b-c164-4879-bbb9-8ebe2b533a95-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:57Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-2309b4c0-2c60-4cad-b924-9ce3a606205c-2 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:57Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-2309b4c0-2c60-4cad-b924-9ce3a606205c-2 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:57Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-e9e788c7-37b1-45a6-af7d-8017587e8a83-3 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:57Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-e9e788c7-37b1-45a6-af7d-8017587e8a83-3 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:57Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-5a5287ec-0480-4f74-b6e1-b64bbda50cce-1 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:57Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-5a5287ec-0480-4f74-b6e1-b64bbda50cce-1 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:57Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-3d808bdb-a491-410d-a585-0fe107e94faa-1 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:57Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-3d808bdb-a491-410d-a585-0fe107e94faa-1 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:57Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-3e82f8b1-0177-4858-86c8-6a6c871a516b-2 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:57Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-3e82f8b1-0177-4858-86c8-6a6c871a516b-2 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:57Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-2309b4c0-2c60-4cad-b924-9ce3a606205c-1 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:57Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-2309b4c0-2c60-4cad-b924-9ce3a606205c-1 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:57Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-d502eadb-9e8b-4f14-8e20-30ef286d4096-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:57Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-d502eadb-9e8b-4f14-8e20-30ef286d4096-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:57Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-3fe306a9-b887-4ae0-97cd-6f0d032032ea-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:57Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-3fe306a9-b887-4ae0-97cd-6f0d032032ea-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:57Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-3e82f8b1-0177-4858-86c8-6a6c871a516b-1 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:57Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-3e82f8b1-0177-4858-86c8-6a6c871a516b-1 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:57Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-e9e788c7-37b1-45a6-af7d-8017587e8a83-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:57Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-e9e788c7-37b1-45a6-af7d-8017587e8a83-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:57Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-11de8cc4-9fd2-48b8-b9fa-acdfd9a07a24-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-11de8cc4-9fd2-48b8-b9fa-acdfd9a07a24-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:36:58Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-d422ac93-db91-4e55-a959-18b4e15919e7-2 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-d422ac93-db91-4e55-a959-18b4e15919e7-2 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-8706d2d7-32b1-417e-b2cb-ef041a6b5ddb-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-8706d2d7-32b1-417e-b2cb-ef041a6b5ddb-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-e9e788c7-37b1-45a6-af7d-8017587e8a83-2 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-e9e788c7-37b1-45a6-af7d-8017587e8a83-2 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-d422ac93-db91-4e55-a959-18b4e15919e7-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-d422ac93-db91-4e55-a959-18b4e15919e7-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-e9e788c7-37b1-45a6-af7d-8017587e8a83-1 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-e9e788c7-37b1-45a6-af7d-8017587e8a83-1 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-3e82f8b1-0177-4858-86c8-6a6c871a516b-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-3e82f8b1-0177-4858-86c8-6a6c871a516b-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-2309b4c0-2c60-4cad-b924-9ce3a606205c-3 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-2309b4c0-2c60-4cad-b924-9ce3a606205c-3 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-2309b4c0-2c60-4cad-b924-9ce3a606205c-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-2309b4c0-2c60-4cad-b924-9ce3a606205c-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-f67c499b-c164-4879-bbb9-8ebe2b533a95-2 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-f67c499b-c164-4879-bbb9-8ebe2b533a95-2 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-5a5287ec-0480-4f74-b6e1-b64bbda50cce-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-5a5287ec-0480-4f74-b6e1-b64bbda50cce-0 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=fgold-backup-020201 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=fgold-backup-020201 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-5a5287ec-0480-4f74-b6e1-b64bbda50cce-2 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-5a5287ec-0480-4f74-b6e1-b64bbda50cce-2 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-5a5287ec-0480-4f74-b6e1-b64bbda50cce-3 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-5a5287ec-0480-4f74-b6e1-b64bbda50cce-3 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-f67c499b-c164-4879-bbb9-8ebe2b533a95-1 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-f67c499b-c164-4879-bbb9-8ebe2b533a95-1 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-3e82f8b1-0177-4858-86c8-6a6c871a516b-3 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-3e82f8b1-0177-4858-86c8-6a6c871a516b-3 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-f67c499b-c164-4879-bbb9-8ebe2b533a95-3 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-f67c499b-c164-4879-bbb9-8ebe2b533a95-3 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Attempting to sync backup into cluster"" backup=backup-rockbands-d422ac93-db91-4e55-a959-18b4e15919e7-1 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:205""
time=""2021-02-17T19:36:58Z"" level=info msg=""Successfully synced backup into cluster"" backup=backup-rockbands-d422ac93-db91-4e55-a959-18b4e15919e7-1 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:235""
time=""2021-02-17T19:36:58Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:36:58Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:37:00Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:37:00Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:37:10Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:37:10Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:37:31Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:37:31Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:37:58Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:37:58Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:37:58Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:38:12Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:38:12Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:38:58Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:38:58Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:38:58Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:39:58Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:39:58Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:39:58Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:40:58Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:40:58Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:40:58Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-17T19:41:40Z"" level=info msg=""Setting up backup log"" backup=velero/fgold-backup-021701 controller=backup logSource=""pkg/controller/backup_controller.go:512""
time=""2021-02-17T19:41:40Z"" level=info msg=""Setting up backup temp file"" backup=velero/fgold-backup-021701 logSource=""pkg/controller/backup_controller.go:534""
time=""2021-02-17T19:41:40Z"" level=info msg=""Setting up plugin manager"" backup=velero/fgold-backup-021701 logSource=""pkg/controller/backup_controller.go:541""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting backup item actions"" backup=velero/fgold-backup-021701 logSource=""pkg/controller/backup_controller.go:545""
time=""2021-02-17T19:41:40Z"" level=info msg=""Setting up backup store to check for backup existence"" backup=velero/fgold-backup-021701 logSource=""pkg/controller/backup_controller.go:551""
time=""2021-02-17T19:41:40Z"" level=info msg=""Writing backup version file"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:212""
time=""2021-02-17T19:41:40Z"" level=info msg=""Including namespaces: rockbands-v1"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:218""
time=""2021-02-17T19:41:40Z"" level=info msg=""Excluding namespaces: <none>"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:219""
time=""2021-02-17T19:41:40Z"" level=info msg=""Including resources: *"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:222""
time=""2021-02-17T19:41:40Z"" level=info msg=""Excluding resources: <none>"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:223""
time=""2021-02-17T19:41:40Z"" level=info msg=""Backing up all pod volumes using restic: false"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:224""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=pods
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=pods
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=pods
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=persistentvolumeclaims
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=persistentvolumeclaims
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=persistentvolumeclaims
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=persistentvolumes
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:192"" resource=persistentvolumes
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=configmaps
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=configmaps
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=configmaps
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=endpoints
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=endpoints
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=endpoints
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=events
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=events
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=events
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=limitranges
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=limitranges
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=limitranges
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=namespaces
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting namespace"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:239"" namespace=rockbands-v1 resource=namespaces
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=nodes
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:192"" resource=nodes
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=podtemplates
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=podtemplates
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=podtemplates
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=replicationcontrollers
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=replicationcontrollers
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=replicationcontrollers
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=resourcequotas
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=resourcequotas
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=resourcequotas
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=secrets
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=secrets
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 1 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=secrets
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=serviceaccounts
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=serviceaccounts
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 1 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=serviceaccounts
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=services
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=services
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=services
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=apiregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apiregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=apiservices
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=apiregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=apiservices
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=apiregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apiregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=apiservices
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=apiregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=apiservices
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:165"" resource=controllerrevisions
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=controllerrevisions
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=controllerrevisions
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:165"" resource=daemonsets
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=daemonsets
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=daemonsets
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:165"" resource=deployments
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=deployments
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=deployments
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:165"" resource=replicasets
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=replicasets
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=replicasets
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:165"" resource=statefulsets
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=statefulsets
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=statefulsets
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=events.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=events.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=events
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it cohabitates and we've already processed it"" backup=velero/fgold-backup-021701 cohabitatingResource1=events cohabitatingResource2=events.events.k8s.io group=events.k8s.io/v1 logSource=""pkg/backup/item_collector.go:213"" resource=events
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=events.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=events.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=events
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it cohabitates and we've already processed it"" backup=velero/fgold-backup-021701 cohabitatingResource1=events cohabitatingResource2=events.events.k8s.io group=events.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:213"" resource=events
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=autoscaling/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=autoscaling/v1 logSource=""pkg/backup/item_collector.go:165"" resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=autoscaling/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=autoscaling/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta1 logSource=""pkg/backup/item_collector.go:165"" resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta2 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta2 logSource=""pkg/backup/item_collector.go:165"" resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta2 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta2 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=batch/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=batch/v1 logSource=""pkg/backup/item_collector.go:165"" resource=jobs
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=batch/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=jobs
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=batch/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=jobs
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=batch/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=batch/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=cronjobs
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=batch/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=cronjobs
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=batch/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=cronjobs
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=certificates.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=certificates.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=certificatesigningrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=certificates.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=certificatesigningrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=certificates.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=certificates.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=certificatesigningrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=certificates.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=certificatesigningrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=ingressclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=ingressclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=ingresses
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=ingresses
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=ingresses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=networkpolicies
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=networkpolicies
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=networkpolicies
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=ingressclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=ingressclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=ingresses
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=ingresses
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=ingresses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=policy/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=policy/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=poddisruptionbudgets
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=policy/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=poddisruptionbudgets
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=policy/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=poddisruptionbudgets
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=policy/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=podsecuritypolicies
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=policy/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=podsecuritypolicies
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=clusterrolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=clusterrolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=clusterroles
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=clusterroles
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=rolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=rolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=rolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=roles
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=roles
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=roles
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=clusterrolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=clusterrolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=clusterroles
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=clusterroles
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=rolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=rolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=rolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=roles
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=roles
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=roles
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=csidrivers
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=csidrivers
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=csinodes
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=csinodes
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=storageclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=storageclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=volumeattachments
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=volumeattachments
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=csidrivers
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=csidrivers
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=csinodes
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=csinodes
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=storageclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=storageclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=volumeattachments
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=volumeattachments
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=mutatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=mutatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=validatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=validatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=mutatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=mutatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=validatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=validatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=apiextensions.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apiextensions.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=customresourcedefinitions
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=apiextensions.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=customresourcedefinitions
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=apiextensions.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apiextensions.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=customresourcedefinitions
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=apiextensions.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=customresourcedefinitions
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=scheduling.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=scheduling.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=priorityclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=scheduling.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=priorityclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=scheduling.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=scheduling.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=priorityclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=scheduling.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=priorityclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=leases
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=leases
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=leases
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=leases
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=leases
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=leases
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=node.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=node.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=runtimeclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=node.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=runtimeclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=discovery.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=discovery.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=endpointslices
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=discovery.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=endpointslices
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=discovery.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=endpointslices
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=backupstoragelocations
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=backupstoragelocations
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=backupstoragelocations
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=podvolumebackups
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=podvolumebackups
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=podvolumebackups
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=podvolumerestores
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=podvolumerestores
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=podvolumerestores
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=restores
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=restores
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=restores
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=schedules
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=schedules
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=schedules
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=serverstatusrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=serverstatusrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=serverstatusrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=backups
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=backups
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=backups
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=deletebackuprequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=deletebackuprequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=deletebackuprequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=downloadrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=downloadrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=downloadrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=resticrepositories
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=resticrepositories
time=""2021-02-17T19:41:41Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=resticrepositories
time=""2021-02-17T19:41:41Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=volumesnapshotlocations
time=""2021-02-17T19:41:41Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=volumesnapshotlocations
time=""2021-02-17T19:41:41Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=volumesnapshotlocations
time=""2021-02-17T19:41:41Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=extensions/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:41Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=extensions/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=ingresses
time=""2021-02-17T19:41:41Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=extensions/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=ingresses
time=""2021-02-17T19:41:41Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=extensions/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=ingresses
time=""2021-02-17T19:41:41Z"" level=info msg=""Collected 3 items matching the backup spec from the Kubernetes API (actual number of items backed up may be more or less depending on velero.io/exclude-from-backup annotation, plugins returning additional related items to back up, etc.)"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:278"" progress=
time=""2021-02-17T19:41:41Z"" level=info msg=""Processing item"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:354"" name=rockbands-v1 namespace= progress= resource=namespaces
time=""2021-02-17T19:41:41Z"" level=info msg=""Backing up item"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/item_backupper.go:121"" name=rockbands-v1 namespace= resource=namespaces
time=""2021-02-17T19:41:41Z"" level=info msg=""Backed up 1 items out of an estimated total of 3 (estimate will change throughout the backup)"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:394"" name=rockbands-v1 namespace= progress= resource=namespaces
time=""2021-02-17T19:41:41Z"" level=info msg=""Processing item"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:354"" name=default-token-c84rp namespace=rockbands-v1 progress= resource=secrets
time=""2021-02-17T19:41:41Z"" level=info msg=""Backing up item"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/item_backupper.go:121"" name=default-token-c84rp namespace=rockbands-v1 resource=secrets
time=""2021-02-17T19:41:41Z"" level=info msg=""Backed up 2 items out of an estimated total of 3 (estimate will change throughout the backup)"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:394"" name=default-token-c84rp namespace=rockbands-v1 progress= resource=secrets
time=""2021-02-17T19:41:41Z"" level=info msg=""Processing item"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:354"" name=default namespace=rockbands-v1 progress= resource=serviceaccounts
time=""2021-02-17T19:41:41Z"" level=info msg=""Backing up item"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/item_backupper.go:121"" name=default namespace=rockbands-v1 resource=serviceaccounts
time=""2021-02-17T19:41:41Z"" level=info msg=""Executing custom action"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/item_backupper.go:327"" name=default namespace=rockbands-v1 resource=serviceaccounts
time=""2021-02-17T19:41:41Z"" level=info msg=""Running ServiceAccountAction"" backup=velero/fgold-backup-021701 cmd=/velero logSource=""pkg/backup/service_account_action.go:77"" pluginName=velero
time=""2021-02-17T19:41:41Z"" level=info msg=""Done running ServiceAccountAction"" backup=velero/fgold-backup-021701 cmd=/velero logSource=""pkg/backup/service_account_action.go:120"" pluginName=velero
time=""2021-02-17T19:41:41Z"" level=info msg=""Backed up 3 items out of an estimated total of 3 (estimate will change throughout the backup)"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:394"" name=default namespace=rockbands-v1 progress= resource=serviceaccounts
time=""2021-02-17T19:41:41Z"" level=info msg=""Backed up a total of 3 items"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:419"" progress=
time=""2021-02-17T19:41:41Z"" level=info msg=""Setting up backup store to persist the backup"" backup=velero/fgold-backup-021701 logSource=""pkg/controller/backup_controller.go:632""
time=""2021-02-17T19:41:41Z"" level=info msg=""Backup completed"" controller=backup logSource=""pkg/controller/backup_controller.go:642""
time=""2021-02-17T19:41:58Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:41:58Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-17T19:41:58Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
I0217 19:41:59.228467       1 request.go:621] Throttling request took 1.0478347s, request: GET:https://10.96.0.1:443/apis/music.example.io/v1?timeout=32s
time=""2021-02-17T19:42:00Z"" level=info msg=""The 'EnableAPIGroupVersions' feature flag was specified, using all API group versions."" logSource=""pkg/discovery/helper.go:159""
```
- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
```
Name:         fgold-backup-021701
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  velero.io/source-cluster-k8s-gitversion=v1.19.1
              velero.io/source-cluster-k8s-major-version=1
              velero.io/source-cluster-k8s-minor-version=19

Phase:  Completed

Errors:    0
Warnings:  0

Namespaces:
  Included:  rockbands-v1
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  default

Velero-Native Snapshot PVs:  auto

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  1.1.0

Started:    2021-02-17 11:41:40 -0800 PST
Completed:  2021-02-17 11:41:41 -0800 PST

Expiration:  2021-03-19 12:41:40 -0700 PDT

Total items to be backed up:  3
Items backed up:              3

Velero-Native Snapshots: <none included>
```
- `velero backup logs <backupname>`
```
time=""2021-02-17T19:41:40Z"" level=info msg=""Setting up backup temp file"" backup=velero/fgold-backup-021701 logSource=""pkg/controller/backup_controller.go:534""
time=""2021-02-17T19:41:40Z"" level=info msg=""Setting up plugin manager"" backup=velero/fgold-backup-021701 logSource=""pkg/controller/backup_controller.go:541""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting backup item actions"" backup=velero/fgold-backup-021701 logSource=""pkg/controller/backup_controller.go:545""
time=""2021-02-17T19:41:40Z"" level=info msg=""Setting up backup store to check for backup existence"" backup=velero/fgold-backup-021701 logSource=""pkg/controller/backup_controller.go:551""
time=""2021-02-17T19:41:40Z"" level=info msg=""Writing backup version file"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:212""
time=""2021-02-17T19:41:40Z"" level=info msg=""Including namespaces: rockbands-v1"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:218""
time=""2021-02-17T19:41:40Z"" level=info msg=""Excluding namespaces: <none>"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:219""
time=""2021-02-17T19:41:40Z"" level=info msg=""Including resources: *"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:222""
time=""2021-02-17T19:41:40Z"" level=info msg=""Excluding resources: <none>"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:223""
time=""2021-02-17T19:41:40Z"" level=info msg=""Backing up all pod volumes using restic: false"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:224""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=pods
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=pods
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=pods
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=persistentvolumeclaims
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=persistentvolumeclaims
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=persistentvolumeclaims
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=persistentvolumes
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:192"" resource=persistentvolumes
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=configmaps
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=configmaps
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=configmaps
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=endpoints
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=endpoints
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=endpoints
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=events
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=events
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=events
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=limitranges
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=limitranges
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=limitranges
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=namespaces
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting namespace"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:239"" namespace=rockbands-v1 resource=namespaces
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=nodes
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:192"" resource=nodes
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=podtemplates
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=podtemplates
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=podtemplates
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=replicationcontrollers
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=replicationcontrollers
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=replicationcontrollers
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=resourcequotas
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=resourcequotas
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=resourcequotas
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=secrets
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=secrets
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 1 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=secrets
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=serviceaccounts
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=serviceaccounts
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 1 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=serviceaccounts
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:165"" resource=services
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=services
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=services
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=apiregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apiregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=apiservices
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=apiregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=apiservices
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=apiregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apiregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=apiservices
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=apiregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=apiservices
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:165"" resource=controllerrevisions
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=controllerrevisions
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=controllerrevisions
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:165"" resource=daemonsets
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=daemonsets
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=daemonsets
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:165"" resource=deployments
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=deployments
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=deployments
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:165"" resource=replicasets
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=replicasets
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=replicasets
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:165"" resource=statefulsets
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=statefulsets
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=apps/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=statefulsets
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=events.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=events.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=events
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it cohabitates and we've already processed it"" backup=velero/fgold-backup-021701 cohabitatingResource1=events cohabitatingResource2=events.events.k8s.io group=events.k8s.io/v1 logSource=""pkg/backup/item_collector.go:213"" resource=events
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=events.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=events.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=events
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it cohabitates and we've already processed it"" backup=velero/fgold-backup-021701 cohabitatingResource1=events cohabitatingResource2=events.events.k8s.io group=events.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:213"" resource=events
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=autoscaling/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=autoscaling/v1 logSource=""pkg/backup/item_collector.go:165"" resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=autoscaling/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=autoscaling/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta1 logSource=""pkg/backup/item_collector.go:165"" resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta2 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta2 logSource=""pkg/backup/item_collector.go:165"" resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta2 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=autoscaling/v2beta2 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=horizontalpodautoscalers
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=batch/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=batch/v1 logSource=""pkg/backup/item_collector.go:165"" resource=jobs
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=batch/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=jobs
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=batch/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=jobs
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=batch/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=batch/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=cronjobs
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=batch/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=cronjobs
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=batch/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=cronjobs
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=certificates.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=certificates.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=certificatesigningrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=certificates.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=certificatesigningrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=certificates.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=certificates.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=certificatesigningrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=certificates.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=certificatesigningrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=ingressclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=ingressclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=ingresses
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=ingresses
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=ingresses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=networkpolicies
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=networkpolicies
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=networkpolicies
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=ingressclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=ingressclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=ingresses
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=ingresses
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=networking.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=ingresses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=policy/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=policy/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=poddisruptionbudgets
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=policy/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=poddisruptionbudgets
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=policy/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=poddisruptionbudgets
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=policy/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=podsecuritypolicies
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=policy/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=podsecuritypolicies
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=clusterrolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=clusterrolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=clusterroles
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=clusterroles
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=rolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=rolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=rolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=roles
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=roles
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=roles
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=clusterrolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=clusterrolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=clusterroles
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=clusterroles
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=rolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=rolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=rolebindings
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=roles
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=roles
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=rbac.authorization.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=roles
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=csidrivers
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=csidrivers
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=csinodes
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=csinodes
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=storageclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=storageclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=volumeattachments
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=volumeattachments
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=csidrivers
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=csidrivers
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=csinodes
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=csinodes
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=storageclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=storageclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=volumeattachments
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=storage.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=volumeattachments
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=mutatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=mutatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=validatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=validatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=mutatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=mutatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=validatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=validatingwebhookconfigurations
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=apiextensions.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apiextensions.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=customresourcedefinitions
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=apiextensions.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=customresourcedefinitions
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=apiextensions.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=apiextensions.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=customresourcedefinitions
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=apiextensions.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=customresourcedefinitions
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=scheduling.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=scheduling.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=priorityclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=scheduling.k8s.io/v1 logSource=""pkg/backup/item_collector.go:192"" resource=priorityclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=scheduling.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=scheduling.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=priorityclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=scheduling.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=priorityclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=leases
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=leases
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=leases
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=leases
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=leases
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=coordination.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=leases
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=node.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=node.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=runtimeclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/fgold-backup-021701 group=node.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:192"" resource=runtimeclasses
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=discovery.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=discovery.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=endpointslices
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=discovery.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=endpointslices
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=discovery.k8s.io/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=endpointslices
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=backupstoragelocations
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=backupstoragelocations
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=backupstoragelocations
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=podvolumebackups
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=podvolumebackups
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=podvolumebackups
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=podvolumerestores
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=podvolumerestores
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=podvolumerestores
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=restores
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=restores
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=restores
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=schedules
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=schedules
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=schedules
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=serverstatusrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=serverstatusrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=serverstatusrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=backups
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=backups
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=backups
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=deletebackuprequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=deletebackuprequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=deletebackuprequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=downloadrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=downloadrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=downloadrequests
time=""2021-02-17T19:41:40Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=resticrepositories
time=""2021-02-17T19:41:40Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=resticrepositories
time=""2021-02-17T19:41:41Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=resticrepositories
time=""2021-02-17T19:41:41Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:165"" resource=volumesnapshotlocations
time=""2021-02-17T19:41:41Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=volumesnapshotlocations
time=""2021-02-17T19:41:41Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=velero.io/v1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=volumesnapshotlocations
time=""2021-02-17T19:41:41Z"" level=info msg=""Getting items for group"" backup=velero/fgold-backup-021701 group=extensions/v1beta1 logSource=""pkg/backup/item_collector.go:76""
time=""2021-02-17T19:41:41Z"" level=info msg=""Getting items for resource"" backup=velero/fgold-backup-021701 group=extensions/v1beta1 logSource=""pkg/backup/item_collector.go:165"" resource=ingresses
time=""2021-02-17T19:41:41Z"" level=info msg=""Listing items"" backup=velero/fgold-backup-021701 group=extensions/v1beta1 logSource=""pkg/backup/item_collector.go:291"" namespace=rockbands-v1 resource=ingresses
time=""2021-02-17T19:41:41Z"" level=info msg=""Retrieved 0 items"" backup=velero/fgold-backup-021701 group=extensions/v1beta1 logSource=""pkg/backup/item_collector.go:297"" namespace=rockbands-v1 resource=ingresses
time=""2021-02-17T19:41:41Z"" level=info msg=""Collected 3 items matching the backup spec from the Kubernetes API (actual number of items backed up may be more or less depending on velero.io/exclude-from-backup annotation, plugins returning additional related items to back up, etc.)"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:278"" progress=
time=""2021-02-17T19:41:41Z"" level=info msg=""Processing item"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:354"" name=rockbands-v1 namespace= progress= resource=namespaces
time=""2021-02-17T19:41:41Z"" level=info msg=""Backing up item"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/item_backupper.go:121"" name=rockbands-v1 namespace= resource=namespaces
time=""2021-02-17T19:41:41Z"" level=info msg=""Backed up 1 items out of an estimated total of 3 (estimate will change throughout the backup)"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:394"" name=rockbands-v1 namespace= progress= resource=namespaces
time=""2021-02-17T19:41:41Z"" level=info msg=""Processing item"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:354"" name=default-token-c84rp namespace=rockbands-v1 progress= resource=secrets
time=""2021-02-17T19:41:41Z"" level=info msg=""Backing up item"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/item_backupper.go:121"" name=default-token-c84rp namespace=rockbands-v1 resource=secrets
time=""2021-02-17T19:41:41Z"" level=info msg=""Backed up 2 items out of an estimated total of 3 (estimate will change throughout the backup)"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:394"" name=default-token-c84rp namespace=rockbands-v1 progress= resource=secrets
time=""2021-02-17T19:41:41Z"" level=info msg=""Processing item"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:354"" name=default namespace=rockbands-v1 progress= resource=serviceaccounts
time=""2021-02-17T19:41:41Z"" level=info msg=""Backing up item"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/item_backupper.go:121"" name=default namespace=rockbands-v1 resource=serviceaccounts
time=""2021-02-17T19:41:41Z"" level=info msg=""Executing custom action"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/item_backupper.go:327"" name=default namespace=rockbands-v1 resource=serviceaccounts
time=""2021-02-17T19:41:41Z"" level=info msg=""Running ServiceAccountAction"" backup=velero/fgold-backup-021701 cmd=/velero logSource=""pkg/backup/service_account_action.go:77"" pluginName=velero
time=""2021-02-17T19:41:41Z"" level=info msg=""Done running ServiceAccountAction"" backup=velero/fgold-backup-021701 cmd=/velero logSource=""pkg/backup/service_account_action.go:120"" pluginName=velero
time=""2021-02-17T19:41:41Z"" level=info msg=""Backed up 3 items out of an estimated total of 3 (estimate will change throughout the backup)"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:394"" name=default namespace=rockbands-v1 progress= resource=serviceaccounts
time=""2021-02-17T19:41:41Z"" level=info msg=""Backed up a total of 3 items"" backup=velero/fgold-backup-021701 logSource=""pkg/backup/backup.go:419"" progress=
```
- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
n/a
- `velero restore logs <restorename>`
n/a

**Anything else you would like to add:**
The cause might be that it can take up to five minutes for Velero to call discovery and realize that the new CRDs are there.

See [line 381 in server.go](https://github.com/vmware-tanzu/velero/blob/6bdd4ac1922ebee440257ae5ab21e7b7ce12e60b/pkg/cmd/server/server.go#L381) where discovery is set to look for new objects every five minutes.


**Environment:**

- Velero version (use `velero version`): `v1.5.3`
- Velero features (use `velero client config get features`): `features: <NOT SET>`
- Kubernetes version (use `kubectl version`): 

  ```
  Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.3"", GitCommit:""06ad960bfd03b39c8310aaf92d1e7c12ce618213"", GitTreeState:""clean"", BuildDate:""2020-02-13T18:08:14Z"", GoVersion:""go1.13.8"", Compiler:""gc"", Platform:""darwin/amd64""}
  Server Version: version.Info{Major:""1"", Minor:""19"", GitVersion:""v1.19.1"", GitCommit:""206bcadf021e76c27513500ca24182692aabd17e"", GitTreeState:""clean"", BuildDate:""2020-09-14T07:30:52Z"",   GoVersion:""go1.15"", Compiler:""gc"", Platform:""linux/amd64""}  
  ```
- Kubernetes installer & version: `kind v0.9.0 go1.15.2 darwin/amd64`
- Cloud provider or hardware configuration: `using local instance of Minio`
- OS (e.g. from `/etc/os-release`): `MacOS 10.15.7`


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,dsu,"
--
When this is done test/e2e/enable_api_group_versions_test.go should be fixed to install Velero BEFORE installing the CRDs.  Currently it installs the CRDs first to get around this issue.
--
",,,,,,,,,,
3470,OPEN,PV is not getting cleaned up for the restored volumes,Needs investigation,2021-03-01 19:40:38 +0000 UTC,pawanpraka1,In progress,,"**What steps did you take and what happened:**


Restored the namespace which has 2 pods and corresponding pvcs using openebs/velero-plugin. When I am deleting the restored namespace, pv remains there in the system with the error 

```
  status:
    message: 'Error getting deleter volume plugin for volume ""pvc-b4b70408-49d7-4921-a88e-80c1fcd981aa"":
      no deletable volume plugin matched'
    phase: Failed

```

**What did you expect to happen:**

PV to be cleand up. PVC and pods gets cleaned up but not the PV.



**Anything else you would like to add:**

I think this change (https://github.com/vmware-tanzu/velero/pull/3007) caused the issue. k8s checks the annotation `pv.kubernetes.io/provisioned-by` to find the plugin related information which has been cleaned up in this PR. Now k8s assumes that pv is statically provisioned and throws error.


This works fine with 1.5.1 and lower releases. The issue is there with 1.5.2 and 1.5.3.",,,pawanpraka1,"
--
cc: @nrb 
--
",,,,,,,,,,
3465,OPEN,Velero is not deleting CSI snapshots when it removes the backup after retention period,Area/CSI; Bug,2021-03-10 02:29:42 +0000 UTC,Rahulnz,Opened,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)
We have Velero configured with CSI snapshot feature enabled. We use NetApp trident CSI driver to communicate with storage system. Volumesnapshotclass, CSI driver and CSI plugins are configured as per link https://velero.io/docs/v1.5/csi/#installing-velero-with-csi-support

1) Schedule is created as below
      NAME          STATUS    CREATED                          SCHEDULE     BACKUP TTL   LAST BACKUP   SELECTOR
      icme--daily   Enabled   2020-11-23 10:25:06 +1300 NZDT   @every 24h   720h0m0s     23h ago       <none>

2) Backup is happening as per schedule with errors for couple of PVCs
      NAME                         STATUS            ERRORS   WARNINGS   CREATED                          EXPIRES   STORAGE LOCATION   SELECTOR
icme--daily-20210215212716   InProgress        0        0          2021-02-16 10:27:16 +1300 NZDT   29d       default            <none>
icme--daily-20210214212715   PartiallyFailed   2        0          2021-02-15 10:27:15 +1300 NZDT   28d       default            <none>
icme--daily-20210213212714   PartiallyFailed   4        0          2021-02-14 10:27:14 +1300 NZDT   27d       default            <none>
icme--daily-20210212212714   PartiallyFailed   4        0          2021-02-13 10:27:14 +1300 NZDT   26d       default            <none>
icme--daily-20210211212713   PartiallyFailed   4        0          2021-02-12 10:27:13 +1300 NZDT   25d       default            <none>
icme--daily-20210210212713   PartiallyFailed   4        0          2021-02-11 10:27:13 +1300 NZDT   24d       default            <none>
icme--daily-20210209212712   PartiallyFailed   4        0          2021-02-10 10:27:12 +1300 NZDT   23d       default            <none>
icme--daily-20210208212712   PartiallyFailed   4        0          2021-02-09 10:27:12 +1300 NZDT   22d       default            <none>
icme--daily-20210207212711   PartiallyFailed   4        0          2021-02-08 10:27:11 +1300 NZDT   21d       default            <none>
icme--daily-20210206212710   PartiallyFailed   4        0          2021-02-07 10:27:10 +1300 NZDT   20d       default            <none>
icme--daily-20210205212701   PartiallyFailed   4        0          2021-02-06 10:27:01 +1300 NZDT   19d       default            <none>
icme--daily-20210204212649   PartiallyFailed   4        0          2021-02-05 10:26:49 +1300 NZDT   18d       default            <none>
icme--daily-20210203212648   PartiallyFailed   4        0          2021-02-04 10:26:48 +1300 NZDT   17d       default            <none>
icme--daily-20210202212648   PartiallyFailed   4        0          2021-02-03 10:26:48 +1300 NZDT   16d       default            <none>
icme--daily-20210201212647   PartiallyFailed   4        0          2021-02-02 10:26:47 +1300 NZDT   15d       default            <none>
icme--daily-20210131212609   PartiallyFailed   4        0          2021-02-01 10:26:09 +1300 NZDT   14d       default            <none>
icme--daily-20210130212609   PartiallyFailed   4        0          2021-01-31 10:26:09 +1300 NZDT   13d       default            <none>
icme--daily-20210129212608   PartiallyFailed   4        0          2021-01-30 10:26:08 +1300 NZDT   12d       default            <none>
icme--daily-20210128212608   PartiallyFailed   4        0          2021-01-29 10:26:08 +1300 NZDT   11d       default            <none>
icme--daily-20210127212607   PartiallyFailed   4        0          2021-01-28 10:26:07 +1300 NZDT   10d       default            <none>
icme--daily-20210126212607   PartiallyFailed   4        0          2021-01-27 10:26:07 +1300 NZDT   9d        default            <none>
icme--daily-20210125212606   PartiallyFailed   4        0          2021-01-26 10:26:06 +1300 NZDT   8d        default            <none>
icme--daily-20210124212606   PartiallyFailed   4        0          2021-01-25 10:26:06 +1300 NZDT   7d        default            <none>
icme--daily-20210123212605   PartiallyFailed   4        0          2021-01-24 10:26:05 +1300 NZDT   6d        default            <none>
icme--daily-20210122212605   PartiallyFailed   4        0          2021-01-23 10:26:05 +1300 NZDT   5d        default            <none>
icme--daily-20210121212604   PartiallyFailed   4        0          2021-01-22 10:26:04 +1300 NZDT   4d        default            <none>
icme--daily-20210120212604   PartiallyFailed   4        0          2021-01-21 10:26:04 +1300 NZDT   3d        default            <none>
icme--daily-20210119212603   PartiallyFailed   4        0          2021-01-20 10:26:04 +1300 NZDT   2d        default            <none>
icme--daily-20210118212603   PartiallyFailed   4        0          2021-01-19 10:26:03 +1300 NZDT   1d        default            <none>
icme--daily-20210117212603   PartiallyFailed   4        0          2021-01-18 10:26:03 +1300 NZDT   2m        default            <none>
3) when i run command ""kubectl get volumesnapshot -o wide --all-namespaces"" it gives snapshots with age more than 29 days which are snapshots of old backup jobs that are already removed by velero.

4)volumesnapshotclass and volumesnapshot drivers both are labeled as per requirement to velero.io/csi-volumesnapshot-class: ""true""

5) volumesnapshotclass deletionPolicy is ""Retain""  ( but it should be changed to delete when velero wants to delete the backup, as per documentation)

6) If I delete snapshots using NetApp Trident using ""tridentctl"" command line utility. it deletes snapshot from the storage system as well, so trident is able to delete the snapshots on the storage system



**What did you expect to happen:**
I wanted velero to delete volumesnapshot, volumesnapshotcontent and snapshots from the storage system removed when velero delete the backup 

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
time=""2021-02-16T21:50:38Z"" level=info msg=""Returning from VolumeSnapshotBackupItemAction with 2 additionalItems to backup"" backup=velero/icme--daily-20210216212754 cmd=/plugins/velero-plugin-for-csi logSource=""/go/src/velero-plugin-for-csi/internal/backup/volumesnapshot_action.go:151"" pluginName=velero-plugin-for-csi
time=""2021-02-16T21:50:38Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=csi-snapclass namespace= resource=volumesnapshotclasses.snapshot.storage.k8s.io
time=""2021-02-16T21:50:38Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ecfe32ea-3f16-46cd-8ff2-10043037234e namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-02-16T21:50:38Z"" level=info msg=""Backed up 2605 items out of an estimated total of 2613 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:418"" name=velero-prometheus-metrics-hr-prom-oper-prome-prometheus-dbzs2xv namespace=metrics progress= resource=volumesnapshots.snapshot.storage.k8s.io
time=""2021-02-16T21:50:38Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:378"" name=velero-prometheus-metrics-hr-prom-oper-prome-prometheus-dbzx8v4 namespace=metrics progress= resource=volumesnapshots.snapshot.storage.k8s.io
time=""2021-02-16T21:50:38Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:121"" name=velero-prometheus-metrics-hr-prom-oper-prome-prometheus-dbzx8v4 namespace=metrics resource=volumesnapshots.snapshot.storage.k8s.io
time=""2021-02-16T21:50:38Z"" level=info msg=""Executing custom action"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:327"" name=velero-prometheus-metrics-hr-prom-oper-prome-prometheus-dbzx8v4 namespace=metrics resource=volumesnapshots.snapshot.storage.k8s.io
time=""2021-02-16T21:50:38Z"" level=info msg=""Executing VolumeSnapshotBackupItemAction"" backup=velero/icme--daily-20210216212754 cmd=/plugins/velero-plugin-for-csi logSource=""/go/src/velero-plugin-for-csi/internal/backup/volumesnapshot_action.go:58"" pluginName=velero-plugin-for-csi
time=""2021-02-16T21:50:38Z"" level=info msg=""Getting VolumesnapshotContent for Volumesnapshot metrics/velero-prometheus-metrics-hr-prom-oper-prome-prometheus-dbzx8v4"" backup=velero/icme--daily-20210216212754 cmd=/plugins/velero-plugin-for-csi logSource=""/go/src/velero-plugin-for-csi/internal/backup/volumesnapshot_action.go:91"" pluginName=velero-plugin-for-csi
time=""2021-02-16T21:50:38Z"" level=info msg=""Returning from VolumeSnapshotBackupItemAction with 2 additionalItems to backup"" backup=velero/icme--daily-20210216212754 cmd=/plugins/velero-plugin-for-csi logSource=""/go/src/velero-plugin-for-csi/internal/backup/volumesnapshot_action.go:151"" pluginName=velero-plugin-for-csi
time=""2021-02-16T21:50:38Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=csi-snapclass namespace= resource=volumesnapshotclasses.snapshot.storage.k8s.io
time=""2021-02-16T21:50:38Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-9548bd26-7142-4502-9961-d624a9087f7f namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-02-16T21:50:38Z"" level=info msg=""Backed up 2606 items out of an estimated total of 2613 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:418"" name=velero-prometheus-metrics-hr-prom-oper-prome-prometheus-dbzx8v4 namespace=metrics progress= resource=volumesnapshots.snapshot.storage.k8s.io
time=""2021-02-16T21:50:38Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:378"" name=gitlab-minio namespace=gitlab progress= resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:121"" name=gitlab-minio namespace=gitlab resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Backed up 2607 items out of an estimated total of 2613 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:418"" name=gitlab-minio namespace=gitlab progress= resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:378"" name=gitlab-registry namespace=gitlab progress= resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:121"" name=gitlab-registry namespace=gitlab resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Backed up 2608 items out of an estimated total of 2613 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:418"" name=gitlab-registry namespace=gitlab progress= resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:378"" name=gitlab-webservice namespace=gitlab progress= resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:121"" name=gitlab-webservice namespace=gitlab resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Backed up 2609 items out of an estimated total of 2613 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:418"" name=gitlab-webservice namespace=gitlab progress= resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:378"" name=cm-acme-http-solver-4rwfh namespace=metrics progress= resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:121"" name=cm-acme-http-solver-4rwfh namespace=metrics resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Backed up 2610 items out of an estimated total of 2613 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:418"" name=cm-acme-http-solver-4rwfh namespace=metrics progress= resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:378"" name=cm-acme-http-solver-js6rj namespace=metrics progress= resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:121"" name=cm-acme-http-solver-js6rj namespace=metrics resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Backed up 2611 items out of an estimated total of 2613 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:418"" name=cm-acme-http-solver-js6rj namespace=metrics progress= resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:378"" name=metrics-hr-prom-oper-grafana namespace=metrics progress= resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:121"" name=metrics-hr-prom-oper-grafana namespace=metrics resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Backed up 2612 items out of an estimated total of 2613 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:418"" name=metrics-hr-prom-oper-grafana namespace=metrics progress= resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:378"" name=metrics-hr-prom-oper-prome-prometheus namespace=metrics progress= resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:121"" name=metrics-hr-prom-oper-prome-prometheus namespace=metrics resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Backed up 2613 items out of an estimated total of 2613 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:418"" name=metrics-hr-prom-oper-prome-prometheus namespace=metrics progress= resource=ingresses.extensions
time=""2021-02-16T21:50:38Z"" level=info msg=""Found associated CRD tridentprovisioners.trident.netapp.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:38Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=tridentprovisioners.trident.netapp.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:38Z"" level=info msg=""Found associated CRD volumesnapshotcontents.snapshot.storage.k8s.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:38Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=volumesnapshotcontents.snapshot.storage.k8s.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:38Z"" level=info msg=""Found associated CRD ipamhandles.crd.projectcalico.org to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:38Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=ipamhandles.crd.projectcalico.org namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:38Z"" level=info msg=""Found associated CRD tridentnodes.trident.netapp.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:38Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=tridentnodes.trident.netapp.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:38Z"" level=info msg=""Found associated CRD rules.config.istio.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:38Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=rules.config.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:39Z"" level=info msg=""Found associated CRD certificates.cert-manager.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:39Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=certificates.cert-manager.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:39Z"" level=info msg=""Found associated CRD clusterissuers.cert-manager.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:39Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=clusterissuers.cert-manager.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:39Z"" level=info msg=""Found associated CRD destinationrules.networking.istio.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:39Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=destinationrules.networking.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:39Z"" level=info msg=""Found associated CRD felixconfigurations.crd.projectcalico.org to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:39Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=felixconfigurations.crd.projectcalico.org namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:39Z"" level=info msg=""Found associated CRD instances.config.istio.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:39Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=instances.config.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:39Z"" level=info msg=""Found associated CRD volumesnapshots.snapshot.storage.k8s.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:39Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=volumesnapshots.snapshot.storage.k8s.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:39Z"" level=info msg=""Found associated CRD blockaffinities.crd.projectcalico.org to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:39Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=blockaffinities.crd.projectcalico.org namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:39Z"" level=info msg=""Found associated CRD tridentstorageclasses.trident.netapp.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:39Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=tridentstorageclasses.trident.netapp.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:39Z"" level=info msg=""Found associated CRD backupstoragelocations.velero.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:39Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=backupstoragelocations.velero.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:40Z"" level=info msg=""Found associated CRD ippools.crd.projectcalico.org to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:40Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=ippools.crd.projectcalico.org namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:40Z"" level=info msg=""Found associated CRD handlers.config.istio.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:40Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=handlers.config.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:40Z"" level=info msg=""Found associated CRD challenges.acme.cert-manager.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:40Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=challenges.acme.cert-manager.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:40Z"" level=info msg=""Found associated CRD certificaterequests.cert-manager.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:40Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=certificaterequests.cert-manager.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:40Z"" level=info msg=""Found associated CRD prometheusrules.monitoring.coreos.com to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:40Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=prometheusrules.monitoring.coreos.com namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:40Z"" level=info msg=""Found associated CRD schedules.velero.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:40Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=schedules.velero.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:40Z"" level=info msg=""Found associated CRD tridentversions.trident.netapp.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:40Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=tridentversions.trident.netapp.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:40Z"" level=info msg=""Found associated CRD istiooperators.install.istio.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:40Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=istiooperators.install.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:40Z"" level=info msg=""Found associated CRD volumesnapshotclasses.snapshot.storage.k8s.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:40Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=volumesnapshotclasses.snapshot.storage.k8s.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:40Z"" level=info msg=""Found associated CRD orders.acme.cert-manager.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:40Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=orders.acme.cert-manager.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:40Z"" level=info msg=""Found associated CRD ipamblocks.crd.projectcalico.org to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:40Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=ipamblocks.crd.projectcalico.org namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:40Z"" level=info msg=""Found associated CRD tridentsnapshots.trident.netapp.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:40Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=tridentsnapshots.trident.netapp.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:41Z"" level=info msg=""Found associated CRD prometheuses.monitoring.coreos.com to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:41Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=prometheuses.monitoring.coreos.com namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:41Z"" level=info msg=""Found associated CRD kubecontrollersconfigurations.crd.projectcalico.org to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:41Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=kubecontrollersconfigurations.crd.projectcalico.org namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:41Z"" level=info msg=""Found associated CRD servicemonitors.monitoring.coreos.com to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:41Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=servicemonitors.monitoring.coreos.com namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:41Z"" level=info msg=""Found associated CRD tridentvolumes.trident.netapp.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:41Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=tridentvolumes.trident.netapp.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:41Z"" level=info msg=""Found associated CRD backups.velero.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:41Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=backups.velero.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:41Z"" level=info msg=""Found associated CRD gateways.networking.istio.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:41Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=gateways.networking.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:41Z"" level=info msg=""Found associated CRD downloadrequests.velero.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:41Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=downloadrequests.velero.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:41Z"" level=info msg=""Found associated CRD virtualservices.networking.istio.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:41Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=virtualservices.networking.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:41Z"" level=info msg=""Found associated CRD envoyfilters.networking.istio.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:41Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=envoyfilters.networking.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:41Z"" level=info msg=""Found associated CRD attributemanifests.config.istio.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:41Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=attributemanifests.config.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:41Z"" level=info msg=""Found associated CRD clusterinformations.crd.projectcalico.org to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:41Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=clusterinformations.crd.projectcalico.org namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:41Z"" level=info msg=""Found associated CRD tridentbackends.trident.netapp.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:41Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=tridentbackends.trident.netapp.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:41Z"" level=info msg=""Found associated CRD helmreleases.helm.fluxcd.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:41Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=helmreleases.helm.fluxcd.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:42Z"" level=info msg=""Found associated CRD alertmanagers.monitoring.coreos.com to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=alertmanagers.monitoring.coreos.com namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:42Z"" level=info msg=""Found associated CRD volumesnapshotlocations.velero.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=volumesnapshotlocations.velero.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:42Z"" level=info msg=""Found associated CRD authorizationpolicies.security.istio.io to add to backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:500""
time=""2021-02-16T21:50:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/item_backupper.go:115"" name=authorizationpolicies.security.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-02-16T21:50:42Z"" level=info msg=""Backed up a total of 2613 items"" backup=velero/icme--daily-20210216212754 logSource=""pkg/backup/backup.go:443"" progress=
time=""2021-02-16T21:50:42Z"" level=info msg=""Setting up backup store to persist the backup"" backup=velero/icme--daily-20210216212754 logSource=""pkg/controller/backup_controller.go:632""
time=""2021-02-16T21:50:43Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:50:43Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T21:50:45Z"" level=info msg=""Backup completed"" controller=backup logSource=""pkg/controller/backup_controller.go:642""
time=""2021-02-16T21:50:54Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:50:55Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:50:55Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T21:51:56Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:51:56Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:51:56Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T21:52:57Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:52:58Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:52:58Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T21:53:59Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:53:59Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:53:59Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
I0216 21:54:20.505490       1 request.go:621] Throttling request took 1.043486539s, request: GET:https://10.96.0.1:443/apis/autoscaling/v1?timeout=32s
time=""2021-02-16T21:55:00Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:55:01Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:55:01Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T21:56:02Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:56:02Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:56:02Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T21:57:03Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:57:04Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:57:04Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T21:58:05Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:58:05Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:58:05Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T21:59:06Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:59:07Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T21:59:07Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
I0216 21:59:24.759064       1 request.go:621] Throttling request took 1.029993035s, request: GET:https://10.96.0.1:443/apis/networking.istio.io/v1beta1?timeout=32s
time=""2021-02-16T22:00:08Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:00:08Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:00:08Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T22:01:09Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:01:10Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:01:10Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T22:02:11Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:02:11Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:02:11Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T22:03:12Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:03:13Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:03:13Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T22:04:13Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:04:14Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:04:14Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
I0216 22:04:29.022077       1 request.go:621] Throttling request took 1.028301089s, request: GET:https://10.96.0.1:443/apis/projectcontour.io/v1alpha1?timeout=32s
time=""2021-02-16T22:05:15Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:05:16Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:05:16Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T22:06:16Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:06:17Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:06:17Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T22:07:18Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:07:18Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:07:18Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-02-16T22:07:23Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-02-16T22:07:23Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""


- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
apiVersion: velero.io/v1
kind: Backup
metadata:
  annotations:
    velero.io/source-cluster-k8s-gitversion: v1.18.5
    velero.io/source-cluster-k8s-major-version: ""1""
    velero.io/source-cluster-k8s-minor-version: ""18""
  creationTimestamp: ""2021-01-22T21:26:05Z""
  generation: 210
  labels:
    velero.io/schedule-name: icme--daily
    velero.io/storage-location: default
  managedFields:
  - apiVersion: velero.io/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: {}
          f:velero.io/source-cluster-k8s-gitversion: {}
          f:velero.io/source-cluster-k8s-major-version: {}
          f:velero.io/source-cluster-k8s-minor-version: {}
        f:labels:
          .: {}
          f:velero.io/schedule-name: {}
          f:velero.io/storage-location: {}
      f:spec:
        .: {}
        f:defaultVolumesToRestic: {}
        f:hooks: {}
        f:includedNamespaces: {}
        f:storageLocation: {}
        f:ttl: {}
        f:volumeSnapshotLocations: {}
      f:status:
        .: {}
        f:completionTimestamp: {}
        f:errors: {}
        f:expiration: {}
        f:formatVersion: {}
        f:phase: {}
        f:progress:
          .: {}
          f:itemsBackedUp: {}
          f:totalItems: {}
        f:startTimestamp: {}
        f:version: {}
    manager: velero-server
    operation: Update
    time: ""2021-01-22T21:52:48Z""
  name: icme--daily-20210122212605
  namespace: velero
  resourceVersion: ""72115606""
  selfLink: /apis/velero.io/v1/namespaces/velero/backups/icme--daily-20210122212605
  uid: 53c955d5-f1f1-4c98-a8a4-760bcf338a2d
spec:
  defaultVolumesToRestic: false
  hooks: {}
  includedNamespaces:
  - '*'
  storageLocation: default
  ttl: 720h0m0s
  volumeSnapshotLocations:
  - default
status:
  completionTimestamp: ""2021-01-22T21:52:44Z""
  errors: 4
  expiration: ""2021-02-21T21:26:05Z""
  formatVersion: 1.1.0
  phase: PartiallyFailed
  progress:
    itemsBackedUp: 6853
    totalItems: 6853
  startTimestamp: ""2021-01-22T21:26:05Z""
  version: 1

- `velero backup logs <backupname>`

m_backupper.go:115"" name=snapcontent-ec133de5-feb8-42b6-a8cf-827f8b0fac5f namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6970 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ec133de5-feb8-42b6-a8cf-827f8b0fac5f namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ec47a9c3-e841-419d-a4fd-d0b495a353c3 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ec47a9c3-e841-419d-a4fd-d0b495a353c3 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6969 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ec47a9c3-e841-419d-a4fd-d0b495a353c3 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ec682fdd-862e-494a-b9f3-6385690e2d34 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ec682fdd-862e-494a-b9f3-6385690e2d34 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6968 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ec682fdd-862e-494a-b9f3-6385690e2d34 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ec822284-eb83-42a3-b347-119af6776031 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ec822284-eb83-42a3-b347-119af6776031 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6967 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ec822284-eb83-42a3-b347-119af6776031 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ecac1042-26dc-437a-9995-132411cc0786 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ecac1042-26dc-437a-9995-132411cc0786 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6966 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ecac1042-26dc-437a-9995-132411cc0786 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ed02f68c-d549-48a6-a2c2-c99a22ae7c53 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ed02f68c-d549-48a6-a2c2-c99a22ae7c53 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6965 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ed02f68c-d549-48a6-a2c2-c99a22ae7c53 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ed0c6480-9901-4f35-b1f8-92e8f0ad5363 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ed0c6480-9901-4f35-b1f8-92e8f0ad5363 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6964 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ed0c6480-9901-4f35-b1f8-92e8f0ad5363 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ed1a6f3f-ca53-49fe-a238-db272e6fe43e namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ed1a6f3f-ca53-49fe-a238-db272e6fe43e namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6963 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ed1a6f3f-ca53-49fe-a238-db272e6fe43e namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ed3f8084-099d-4b34-8931-7c2f429510bf namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ed3f8084-099d-4b34-8931-7c2f429510bf namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6962 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ed3f8084-099d-4b34-8931-7c2f429510bf namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ed4d9d4e-9974-4462-91e4-fc6484fff07e namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ed4d9d4e-9974-4462-91e4-fc6484fff07e namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6961 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ed4d9d4e-9974-4462-91e4-fc6484fff07e namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ee0a4318-4e22-429f-99ca-3fda76f4aa5a namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ee0a4318-4e22-429f-99ca-3fda76f4aa5a namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6960 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ee0a4318-4e22-429f-99ca-3fda76f4aa5a namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ee18e2a4-ecf6-401e-b0f8-2091496d5546 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ee18e2a4-ecf6-401e-b0f8-2091496d5546 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6959 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ee18e2a4-ecf6-401e-b0f8-2091496d5546 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ee2fc9dd-82ff-4e0e-ad77-4471d076a2f8 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ee2fc9dd-82ff-4e0e-ad77-4471d076a2f8 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6958 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ee2fc9dd-82ff-4e0e-ad77-4471d076a2f8 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ee71ab20-dbba-42d4-8486-137c913db66a namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ee71ab20-dbba-42d4-8486-137c913db66a namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6957 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ee71ab20-dbba-42d4-8486-137c913db66a namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ee86bb0a-05b6-4efb-be55-0c06c6e7ffea namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ee86bb0a-05b6-4efb-be55-0c06c6e7ffea namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6956 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ee86bb0a-05b6-4efb-be55-0c06c6e7ffea namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ee904086-77e0-472d-a84d-bbf7f3380e81 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ee904086-77e0-472d-a84d-bbf7f3380e81 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6955 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ee904086-77e0-472d-a84d-bbf7f3380e81 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ef0f882c-aff6-4e86-976a-cd6441baea96 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ef0f882c-aff6-4e86-976a-cd6441baea96 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6954 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ef0f882c-aff6-4e86-976a-cd6441baea96 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ef55e191-c033-47d6-adaf-bf4b3534e73b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ef55e191-c033-47d6-adaf-bf4b3534e73b namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6953 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ef55e191-c033-47d6-adaf-bf4b3534e73b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-efb5ee01-0725-4414-ad1a-e322704f0bd3 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-efb5ee01-0725-4414-ad1a-e322704f0bd3 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6952 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-efb5ee01-0725-4414-ad1a-e322704f0bd3 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-efcce726-d04e-428d-b29c-1a36d1e29884 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-efcce726-d04e-428d-b29c-1a36d1e29884 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6951 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-efcce726-d04e-428d-b29c-1a36d1e29884 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-efd0ba0c-3ebd-4822-931a-1e2387e360ee namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-efd0ba0c-3ebd-4822-931a-1e2387e360ee namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6950 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-efd0ba0c-3ebd-4822-931a-1e2387e360ee namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-efdd2007-0b5c-4f08-b3db-d4d686d1c65a namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-efdd2007-0b5c-4f08-b3db-d4d686d1c65a namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6949 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-efdd2007-0b5c-4f08-b3db-d4d686d1c65a namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-efe23049-e5b1-476b-a8e2-bc05deb52bf4 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-efe23049-e5b1-476b-a8e2-bc05deb52bf4 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6948 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-efe23049-e5b1-476b-a8e2-bc05deb52bf4 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-efe8fe35-cc79-4a65-b896-471b2da88185 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-efe8fe35-cc79-4a65-b896-471b2da88185 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6947 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-efe8fe35-cc79-4a65-b896-471b2da88185 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f01e591d-cdb0-4c27-8c2a-f88f835177e2 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f01e591d-cdb0-4c27-8c2a-f88f835177e2 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6946 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f01e591d-cdb0-4c27-8c2a-f88f835177e2 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f025d969-004a-44d8-b484-c1b1fa57ce9b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f025d969-004a-44d8-b484-c1b1fa57ce9b namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6945 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f025d969-004a-44d8-b484-c1b1fa57ce9b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f03d21d3-ce1c-4bea-951c-db685a1f8e16 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f03d21d3-ce1c-4bea-951c-db685a1f8e16 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6944 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f03d21d3-ce1c-4bea-951c-db685a1f8e16 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f07956bc-a7dd-44a9-9fb0-9a62107a5a54 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f07956bc-a7dd-44a9-9fb0-9a62107a5a54 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6943 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f07956bc-a7dd-44a9-9fb0-9a62107a5a54 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f0846b60-687b-4d84-a7e2-11777de9c825 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f0846b60-687b-4d84-a7e2-11777de9c825 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6942 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f0846b60-687b-4d84-a7e2-11777de9c825 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f098d16e-8d49-4bfb-8ebb-e242eec49701 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f098d16e-8d49-4bfb-8ebb-e242eec49701 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6941 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f098d16e-8d49-4bfb-8ebb-e242eec49701 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f0b1ab3d-c8a9-44dc-8ce4-c9156b7bc760 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f0b1ab3d-c8a9-44dc-8ce4-c9156b7bc760 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6940 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f0b1ab3d-c8a9-44dc-8ce4-c9156b7bc760 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f0c3e51b-3452-4d2c-b9fc-2c3bf111b5bf namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f0c3e51b-3452-4d2c-b9fc-2c3bf111b5bf namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6939 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f0c3e51b-3452-4d2c-b9fc-2c3bf111b5bf namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f0cc5ec6-89d2-4d7c-8393-bb3db50d3f13 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f0cc5ec6-89d2-4d7c-8393-bb3db50d3f13 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6938 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f0cc5ec6-89d2-4d7c-8393-bb3db50d3f13 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f0e19284-bae5-4cc2-92b7-905ee16bda43 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f0e19284-bae5-4cc2-92b7-905ee16bda43 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6937 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f0e19284-bae5-4cc2-92b7-905ee16bda43 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f0e21180-1b06-4760-a18f-9671ed593d96 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f0e21180-1b06-4760-a18f-9671ed593d96 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6936 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f0e21180-1b06-4760-a18f-9671ed593d96 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f0f9bb0a-26a3-45ea-aa5c-60edd48cbf75 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f0f9bb0a-26a3-45ea-aa5c-60edd48cbf75 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6935 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f0f9bb0a-26a3-45ea-aa5c-60edd48cbf75 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f11d80e9-af00-484f-b8bf-f28118bf0c53 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f11d80e9-af00-484f-b8bf-f28118bf0c53 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6934 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f11d80e9-af00-484f-b8bf-f28118bf0c53 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f12ef21d-f733-4f7c-ac22-7bb912c66910 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f12ef21d-f733-4f7c-ac22-7bb912c66910 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6933 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f12ef21d-f733-4f7c-ac22-7bb912c66910 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f13a24b5-8242-4ace-a67d-52a01b9ed0b6 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f13a24b5-8242-4ace-a67d-52a01b9ed0b6 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6932 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f13a24b5-8242-4ace-a67d-52a01b9ed0b6 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f1ab9d38-d45d-4569-8c59-c4a474ec1e72 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f1ab9d38-d45d-4569-8c59-c4a474ec1e72 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6931 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f1ab9d38-d45d-4569-8c59-c4a474ec1e72 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f1b8f214-cd48-4f68-9c7b-b5fb2bddb2af namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f1b8f214-cd48-4f68-9c7b-b5fb2bddb2af namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6930 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f1b8f214-cd48-4f68-9c7b-b5fb2bddb2af namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f1e18f82-afc1-47f1-94a8-65a70771adc4 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f1e18f82-afc1-47f1-94a8-65a70771adc4 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6929 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f1e18f82-afc1-47f1-94a8-65a70771adc4 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f222ad97-3a11-45b6-965b-b6c19a4864a6 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f222ad97-3a11-45b6-965b-b6c19a4864a6 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6928 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f222ad97-3a11-45b6-965b-b6c19a4864a6 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f315491b-c250-4654-9b67-06fcee5ecd91 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f315491b-c250-4654-9b67-06fcee5ecd91 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6927 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f315491b-c250-4654-9b67-06fcee5ecd91 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f36609a0-1ddb-4138-aaba-31c95c667edf namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f36609a0-1ddb-4138-aaba-31c95c667edf namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6926 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f36609a0-1ddb-4138-aaba-31c95c667edf namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f36d7596-1287-45ce-ac17-40315149f639 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f36d7596-1287-45ce-ac17-40315149f639 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6925 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f36d7596-1287-45ce-ac17-40315149f639 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f380de02-a5ef-4f7d-90f8-e75ecad0746a namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f380de02-a5ef-4f7d-90f8-e75ecad0746a namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6924 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f380de02-a5ef-4f7d-90f8-e75ecad0746a namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f3912739-020b-4e88-a293-3f2749e1038b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f3912739-020b-4e88-a293-3f2749e1038b namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6923 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f3912739-020b-4e88-a293-3f2749e1038b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f3b9a234-c090-40ba-8b21-062e9c12c9e2 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f3b9a234-c090-40ba-8b21-062e9c12c9e2 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6922 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f3b9a234-c090-40ba-8b21-062e9c12c9e2 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f4006238-692d-4795-93de-c64b54f0da71 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f4006238-692d-4795-93de-c64b54f0da71 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6921 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f4006238-692d-4795-93de-c64b54f0da71 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f4086ff3-335e-456f-8a0f-d65b2fb93487 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f4086ff3-335e-456f-8a0f-d65b2fb93487 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6920 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f4086ff3-335e-456f-8a0f-d65b2fb93487 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f47a307b-f905-4150-a5e2-4d26cb602d45 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f47a307b-f905-4150-a5e2-4d26cb602d45 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6919 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f47a307b-f905-4150-a5e2-4d26cb602d45 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f482b794-1b0d-4d9a-a94a-890ca4b9e4e4 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f482b794-1b0d-4d9a-a94a-890ca4b9e4e4 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6918 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f482b794-1b0d-4d9a-a94a-890ca4b9e4e4 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f4b72488-d173-49cd-b750-b248c72a4fc9 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f4b72488-d173-49cd-b750-b248c72a4fc9 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6917 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f4b72488-d173-49cd-b750-b248c72a4fc9 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f4cab61b-b17e-472f-bb20-3e8ab8ecfeb1 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f4cab61b-b17e-472f-bb20-3e8ab8ecfeb1 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6916 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f4cab61b-b17e-472f-bb20-3e8ab8ecfeb1 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f4f10265-a3c0-4bd5-bcf6-098b2fe12dfc namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f4f10265-a3c0-4bd5-bcf6-098b2fe12dfc namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6915 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f4f10265-a3c0-4bd5-bcf6-098b2fe12dfc namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f510f024-145f-4977-b80d-fdd9880a963c namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f510f024-145f-4977-b80d-fdd9880a963c namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6914 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f510f024-145f-4977-b80d-fdd9880a963c namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f5114f11-20aa-4967-b592-0bcec3115aad namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f5114f11-20aa-4967-b592-0bcec3115aad namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6913 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f5114f11-20aa-4967-b592-0bcec3115aad namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f5157bf9-c38b-40c8-a534-c5bde4564e4b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f5157bf9-c38b-40c8-a534-c5bde4564e4b namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6912 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f5157bf9-c38b-40c8-a534-c5bde4564e4b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f5a4a3dc-34c9-4f29-9b56-fe252942e7c7 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f5a4a3dc-34c9-4f29-9b56-fe252942e7c7 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6911 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f5a4a3dc-34c9-4f29-9b56-fe252942e7c7 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f5b37c3f-2bbb-45fd-9401-0ea20e27dbed namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f5b37c3f-2bbb-45fd-9401-0ea20e27dbed namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6910 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f5b37c3f-2bbb-45fd-9401-0ea20e27dbed namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f5b3fd4e-66ea-425c-94bc-c93b87edb72a namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f5b3fd4e-66ea-425c-94bc-c93b87edb72a namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6909 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f5b3fd4e-66ea-425c-94bc-c93b87edb72a namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f612bcd0-253b-43a7-a37a-b04d63552449 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f612bcd0-253b-43a7-a37a-b04d63552449 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6908 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f612bcd0-253b-43a7-a37a-b04d63552449 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f64a952d-e672-4e4b-9e66-b045e09b0dd3 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f64a952d-e672-4e4b-9e66-b045e09b0dd3 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6907 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f64a952d-e672-4e4b-9e66-b045e09b0dd3 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f6911029-395a-4dc5-8703-e4ce009c5be1 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f6911029-395a-4dc5-8703-e4ce009c5be1 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6906 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f6911029-395a-4dc5-8703-e4ce009c5be1 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f69305e6-fba2-4331-9999-85c58da7c89e namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f69305e6-fba2-4331-9999-85c58da7c89e namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6905 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f69305e6-fba2-4331-9999-85c58da7c89e namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f6bc3d3b-d4b7-4080-8713-c81bc8df7202 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f6bc3d3b-d4b7-4080-8713-c81bc8df7202 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6904 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f6bc3d3b-d4b7-4080-8713-c81bc8df7202 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f70f2d42-0e4a-4f5c-b9c1-5b602cecbb44 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f70f2d42-0e4a-4f5c-b9c1-5b602cecbb44 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6903 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f70f2d42-0e4a-4f5c-b9c1-5b602cecbb44 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f78229c0-b8cc-49a2-a8cd-1a923a65512b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f78229c0-b8cc-49a2-a8cd-1a923a65512b namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6902 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f78229c0-b8cc-49a2-a8cd-1a923a65512b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f7af1d2e-f0a2-404f-94c2-52cb32d19079 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f7af1d2e-f0a2-404f-94c2-52cb32d19079 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6901 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f7af1d2e-f0a2-404f-94c2-52cb32d19079 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f7e12d03-a782-44ae-83ed-19cba53ce4d8 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f7e12d03-a782-44ae-83ed-19cba53ce4d8 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6900 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f7e12d03-a782-44ae-83ed-19cba53ce4d8 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f7e27b2e-4bc7-40b3-b463-222a2df6e5bb namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f7e27b2e-4bc7-40b3-b463-222a2df6e5bb namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6899 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f7e27b2e-4bc7-40b3-b463-222a2df6e5bb namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f7f03ad0-37d6-478a-95d5-1eaafa222a47 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f7f03ad0-37d6-478a-95d5-1eaafa222a47 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6898 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f7f03ad0-37d6-478a-95d5-1eaafa222a47 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f859761a-5e92-4d11-9fb7-70056f5fd28c namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f859761a-5e92-4d11-9fb7-70056f5fd28c namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6897 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f859761a-5e92-4d11-9fb7-70056f5fd28c namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f88fbbee-66aa-4cd7-a917-0ada367978a4 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f88fbbee-66aa-4cd7-a917-0ada367978a4 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6896 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f88fbbee-66aa-4cd7-a917-0ada367978a4 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f941d52b-dcc1-413c-a039-38713c1a1665 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f941d52b-dcc1-413c-a039-38713c1a1665 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6895 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f941d52b-dcc1-413c-a039-38713c1a1665 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f9477998-6efc-4638-be6e-b176c240facf namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f9477998-6efc-4638-be6e-b176c240facf namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6894 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f9477998-6efc-4638-be6e-b176c240facf namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-f972cd29-43ab-45a2-b696-03fde437b948 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-f972cd29-43ab-45a2-b696-03fde437b948 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6893 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-f972cd29-43ab-45a2-b696-03fde437b948 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fa09395c-393b-4497-ab73-4d211ce87356 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fa09395c-393b-4497-ab73-4d211ce87356 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6892 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fa09395c-393b-4497-ab73-4d211ce87356 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fa10761d-78bf-460b-9cd3-7c665eda75ce namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fa10761d-78bf-460b-9cd3-7c665eda75ce namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6891 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fa10761d-78bf-460b-9cd3-7c665eda75ce namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fa4fe49b-d836-472e-a858-79d199dcf6de namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fa4fe49b-d836-472e-a858-79d199dcf6de namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6890 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fa4fe49b-d836-472e-a858-79d199dcf6de namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fa62ba2f-8a10-4312-82dc-c47390b5b7b8 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fa62ba2f-8a10-4312-82dc-c47390b5b7b8 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6889 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fa62ba2f-8a10-4312-82dc-c47390b5b7b8 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fa98f0cd-4f1c-47f2-bb1f-79c83e582d4d namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fa98f0cd-4f1c-47f2-bb1f-79c83e582d4d namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6888 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fa98f0cd-4f1c-47f2-bb1f-79c83e582d4d namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fb17b6ea-4b84-4779-ae14-5dc0840e0269 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fb17b6ea-4b84-4779-ae14-5dc0840e0269 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6887 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fb17b6ea-4b84-4779-ae14-5dc0840e0269 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fb5d95fc-b01e-41bd-a208-b4cccef29feb namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fb5d95fc-b01e-41bd-a208-b4cccef29feb namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6886 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fb5d95fc-b01e-41bd-a208-b4cccef29feb namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fb6d92c2-4fb9-4239-8ce7-554fc855f6f1 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fb6d92c2-4fb9-4239-8ce7-554fc855f6f1 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6885 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fb6d92c2-4fb9-4239-8ce7-554fc855f6f1 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fb74915b-cc6c-42ed-882f-e818a0329af4 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fb74915b-cc6c-42ed-882f-e818a0329af4 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6884 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fb74915b-cc6c-42ed-882f-e818a0329af4 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fbb7a3c2-53c0-4cf4-acd0-8463d25dcd4c namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fbb7a3c2-53c0-4cf4-acd0-8463d25dcd4c namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6883 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fbb7a3c2-53c0-4cf4-acd0-8463d25dcd4c namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fbdb886d-4f1b-41c7-a249-4d997828657b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fbdb886d-4f1b-41c7-a249-4d997828657b namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6882 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fbdb886d-4f1b-41c7-a249-4d997828657b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fbddfaed-11f0-421f-9bab-32d06a372b70 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fbddfaed-11f0-421f-9bab-32d06a372b70 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6881 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fbddfaed-11f0-421f-9bab-32d06a372b70 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fc2c492a-4e49-466e-91a8-8a5823c5d5ea namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fc2c492a-4e49-466e-91a8-8a5823c5d5ea namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6880 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fc2c492a-4e49-466e-91a8-8a5823c5d5ea namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fc2ce35f-89f8-461f-a6f9-5d2975bae652 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fc2ce35f-89f8-461f-a6f9-5d2975bae652 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6879 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fc2ce35f-89f8-461f-a6f9-5d2975bae652 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fc51365b-f672-4f28-b877-e9ecde683813 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fc51365b-f672-4f28-b877-e9ecde683813 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6878 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fc51365b-f672-4f28-b877-e9ecde683813 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fc646ed7-f3e6-4321-bbe0-ff2a9a874e7b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fc646ed7-f3e6-4321-bbe0-ff2a9a874e7b namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6877 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fc646ed7-f3e6-4321-bbe0-ff2a9a874e7b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fc7365a0-9444-40c7-8cc3-3437bd0d881a namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fc7365a0-9444-40c7-8cc3-3437bd0d881a namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6876 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fc7365a0-9444-40c7-8cc3-3437bd0d881a namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fc8e0671-05ea-4a5a-b768-31eda631e268 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fc8e0671-05ea-4a5a-b768-31eda631e268 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6875 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fc8e0671-05ea-4a5a-b768-31eda631e268 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fcbd4914-5b15-409c-a75c-2659762b2aa3 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fcbd4914-5b15-409c-a75c-2659762b2aa3 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6874 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fcbd4914-5b15-409c-a75c-2659762b2aa3 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fcf5f576-18b1-44e3-9103-b97d076661ae namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fcf5f576-18b1-44e3-9103-b97d076661ae namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6873 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fcf5f576-18b1-44e3-9103-b97d076661ae namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fd2820f8-c2b8-4e91-ac63-d678b4ce0c56 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fd2820f8-c2b8-4e91-ac63-d678b4ce0c56 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6872 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fd2820f8-c2b8-4e91-ac63-d678b4ce0c56 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fd559098-c878-4026-99f2-a9d86d483349 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fd559098-c878-4026-99f2-a9d86d483349 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6871 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fd559098-c878-4026-99f2-a9d86d483349 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fd6bdd25-6df2-4d11-8854-fd15f7e91cd4 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fd6bdd25-6df2-4d11-8854-fd15f7e91cd4 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6870 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fd6bdd25-6df2-4d11-8854-fd15f7e91cd4 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fd9304ad-4ac7-401d-85eb-adb1e1c3d348 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fd9304ad-4ac7-401d-85eb-adb1e1c3d348 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6869 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fd9304ad-4ac7-401d-85eb-adb1e1c3d348 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fdc353df-a381-4f0b-b5ac-788ab2d23a59 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fdc353df-a381-4f0b-b5ac-788ab2d23a59 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6868 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fdc353df-a381-4f0b-b5ac-788ab2d23a59 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fdf53637-0740-40bc-9670-6b6806bfe639 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fdf53637-0740-40bc-9670-6b6806bfe639 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6867 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fdf53637-0740-40bc-9670-6b6806bfe639 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fe1efbb5-5875-46d1-9393-3fa255fbf306 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fe1efbb5-5875-46d1-9393-3fa255fbf306 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6866 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fe1efbb5-5875-46d1-9393-3fa255fbf306 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fe347edd-780d-4572-9d5d-59d66c2d652f namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fe347edd-780d-4572-9d5d-59d66c2d652f namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6865 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fe347edd-780d-4572-9d5d-59d66c2d652f namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fe5b4c5b-4c71-4184-8fdd-f61945a5191b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fe5b4c5b-4c71-4184-8fdd-f61945a5191b namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6864 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fe5b4c5b-4c71-4184-8fdd-f61945a5191b namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fe6e110c-3f9a-4bc0-ac3b-54d99899ca3a namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fe6e110c-3f9a-4bc0-ac3b-54d99899ca3a namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6863 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fe6e110c-3f9a-4bc0-ac3b-54d99899ca3a namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fe700eb2-1355-420b-bb12-6cc3d9c3fb59 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fe700eb2-1355-420b-bb12-6cc3d9c3fb59 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6862 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fe700eb2-1355-420b-bb12-6cc3d9c3fb59 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-fec20bcd-0595-4ba9-9953-cddee27152b3 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-fec20bcd-0595-4ba9-9953-cddee27152b3 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6861 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-fec20bcd-0595-4ba9-9953-cddee27152b3 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ff0d49ec-687b-4beb-a21d-45486328afb2 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ff0d49ec-687b-4beb-a21d-45486328afb2 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6860 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ff0d49ec-687b-4beb-a21d-45486328afb2 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ff37e89a-fa0f-4f1a-a236-9a3ebdff5245 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ff37e89a-fa0f-4f1a-a236-9a3ebdff5245 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6859 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ff37e89a-fa0f-4f1a-a236-9a3ebdff5245 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ff52e33a-bf5c-429b-9cb6-012bd0426021 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ff52e33a-bf5c-429b-9cb6-012bd0426021 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6858 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ff52e33a-bf5c-429b-9cb6-012bd0426021 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ff87c124-1e8c-4ecb-bcf0-3aa2069af3f6 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ff87c124-1e8c-4ecb-bcf0-3aa2069af3f6 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6857 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ff87c124-1e8c-4ecb-bcf0-3aa2069af3f6 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=snapcontent-ff9fc00f-9cf1-47bf-924b-03a3fdaa4c26 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=snapcontent-ff9fc00f-9cf1-47bf-924b-03a3fdaa4c26 namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6856 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=snapcontent-ff9fc00f-9cf1-47bf-924b-03a3fdaa4c26 namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=velero-velero-storage-logs-hr-loki-0-fnqn2-vf96h namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=velero-velero-storage-logs-hr-loki-0-fnqn2-vf96h namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6855 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=velero-velero-storage-logs-hr-loki-0-fnqn2-vf96h namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=velero-velero-storage-logs-hr-loki-0-pnc2n-xzv9r namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=velero-velero-storage-logs-hr-loki-0-pnc2n-xzv9r namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6854 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=velero-velero-storage-logs-hr-loki-0-pnc2n-xzv9r namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=velero-velero-storage-logs-hr-loki-0-x9b9x-jln4m namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=velero-velero-storage-logs-hr-loki-0-x9b9x-jln4m namespace= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6846 items out of an estimated total of 6853 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=velero-velero-storage-logs-hr-loki-0-x9b9x-jln4m namespace= progress= resource=volumesnapshotcontents.snapshot.storage.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=gitlab-minio namespace=gitlab progress= resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:121"" name=gitlab-minio namespace=gitlab resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6847 items out of an estimated total of 6853 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=gitlab-minio namespace=gitlab progress= resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=gitlab-registry namespace=gitlab progress= resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:121"" name=gitlab-registry namespace=gitlab resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6848 items out of an estimated total of 6853 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=gitlab-registry namespace=gitlab progress= resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=gitlab-webservice namespace=gitlab progress= resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:121"" name=gitlab-webservice namespace=gitlab resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6849 items out of an estimated total of 6853 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=gitlab-webservice namespace=gitlab progress= resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=cm-acme-http-solver-4rwfh namespace=metrics progress= resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:121"" name=cm-acme-http-solver-4rwfh namespace=metrics resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6850 items out of an estimated total of 6853 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=cm-acme-http-solver-4rwfh namespace=metrics progress= resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=cm-acme-http-solver-js6rj namespace=metrics progress= resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:121"" name=cm-acme-http-solver-js6rj namespace=metrics resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6851 items out of an estimated total of 6853 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=cm-acme-http-solver-js6rj namespace=metrics progress= resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=metrics-hr-prom-oper-grafana namespace=metrics progress= resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:121"" name=metrics-hr-prom-oper-grafana namespace=metrics resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6852 items out of an estimated total of 6853 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=metrics-hr-prom-oper-grafana namespace=metrics progress= resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Processing item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:378"" name=metrics-hr-prom-oper-prome-prometheus namespace=metrics progress= resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Backing up item"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:121"" name=metrics-hr-prom-oper-prome-prometheus namespace=metrics resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Backed up 6853 items out of an estimated total of 6853 (estimate will change throughout the backup)"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:418"" name=metrics-hr-prom-oper-prome-prometheus namespace=metrics progress= resource=ingresses.extensions
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD ipamhandles.crd.projectcalico.org to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=ipamhandles.crd.projectcalico.org namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD alertmanagers.monitoring.coreos.com to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=alertmanagers.monitoring.coreos.com namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD tridentversions.trident.netapp.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=tridentversions.trident.netapp.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD authorizationpolicies.security.istio.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=authorizationpolicies.security.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD prometheuses.monitoring.coreos.com to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=prometheuses.monitoring.coreos.com namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD virtualservices.networking.istio.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=virtualservices.networking.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD backupstoragelocations.velero.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=backupstoragelocations.velero.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD instances.config.istio.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=instances.config.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD prometheusrules.monitoring.coreos.com to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=prometheusrules.monitoring.coreos.com namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD tridentprovisioners.trident.netapp.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=tridentprovisioners.trident.netapp.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD gateways.networking.istio.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=gateways.networking.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD volumesnapshots.snapshot.storage.k8s.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=volumesnapshots.snapshot.storage.k8s.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD kubecontrollersconfigurations.crd.projectcalico.org to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=kubecontrollersconfigurations.crd.projectcalico.org namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD volumesnapshotclasses.snapshot.storage.k8s.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=volumesnapshotclasses.snapshot.storage.k8s.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD ippools.crd.projectcalico.org to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=ippools.crd.projectcalico.org namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD felixconfigurations.crd.projectcalico.org to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=felixconfigurations.crd.projectcalico.org namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD tridentvolumes.trident.netapp.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=tridentvolumes.trident.netapp.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD destinationrules.networking.istio.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=destinationrules.networking.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD envoyfilters.networking.istio.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=envoyfilters.networking.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD certificaterequests.cert-manager.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=certificaterequests.cert-manager.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD blockaffinities.crd.projectcalico.org to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=blockaffinities.crd.projectcalico.org namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD volumesnapshotlocations.velero.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=volumesnapshotlocations.velero.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:42Z"" level=info msg=""Found associated CRD servicemonitors.monitoring.coreos.com to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=servicemonitors.monitoring.coreos.com namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:43Z"" level=info msg=""Found associated CRD tridentstorageclasses.trident.netapp.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:43Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=tridentstorageclasses.trident.netapp.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:43Z"" level=info msg=""Found associated CRD orders.acme.cert-manager.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:43Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=orders.acme.cert-manager.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:43Z"" level=info msg=""Found associated CRD handlers.config.istio.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:43Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=handlers.config.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:43Z"" level=info msg=""Found associated CRD rules.config.istio.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:43Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=rules.config.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:43Z"" level=info msg=""Found associated CRD certificates.cert-manager.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:43Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=certificates.cert-manager.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:43Z"" level=info msg=""Found associated CRD clusterissuers.cert-manager.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:43Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=clusterissuers.cert-manager.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:43Z"" level=info msg=""Found associated CRD helmreleases.helm.fluxcd.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:43Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=helmreleases.helm.fluxcd.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:43Z"" level=info msg=""Found associated CRD schedules.velero.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:43Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=schedules.velero.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:43Z"" level=info msg=""Found associated CRD attributemanifests.config.istio.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:43Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=attributemanifests.config.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:43Z"" level=info msg=""Found associated CRD istiooperators.install.istio.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:43Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=istiooperators.install.istio.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:44Z"" level=info msg=""Found associated CRD volumesnapshotcontents.snapshot.storage.k8s.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:44Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=volumesnapshotcontents.snapshot.storage.k8s.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:44Z"" level=info msg=""Found associated CRD clusterinformations.crd.projectcalico.org to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:44Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=clusterinformations.crd.projectcalico.org namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:44Z"" level=info msg=""Found associated CRD tridentbackends.trident.netapp.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:44Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=tridentbackends.trident.netapp.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:44Z"" level=info msg=""Found associated CRD tridentnodes.trident.netapp.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:44Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=tridentnodes.trident.netapp.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:44Z"" level=info msg=""Found associated CRD tridentsnapshots.trident.netapp.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:44Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=tridentsnapshots.trident.netapp.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:44Z"" level=info msg=""Found associated CRD backups.velero.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:44Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=backups.velero.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:44Z"" level=info msg=""Found associated CRD ipamblocks.crd.projectcalico.org to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:44Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=ipamblocks.crd.projectcalico.org namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:44Z"" level=info msg=""Found associated CRD challenges.acme.cert-manager.io to add to backup"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:500""
time=""2021-01-22T21:52:44Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/item_backupper.go:115"" name=challenges.acme.cert-manager.io namespace= resource=customresourcedefinitions.apiextensions.k8s.io
time=""2021-01-22T21:52:44Z"" level=info msg=""Backed up a total of 6853 items"" backup=velero/icme--daily-20210122212605 logSource=""pkg/backup/backup.go:443"" progress=

- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): 1.5.2
- Velero features (use `velero client config get features`): 
velero.io/csi-pvc-backupper                     BackupItemAction
velero.io/csi-volumesnapshot-backupper          BackupItemAction
velero.io/csi-volumesnapshotclass-backupper     BackupItemAction
velero.io/csi-volumesnapshotcontent-backupper   BackupItemAction
velero.io/csi-pvc-restorer                      RestoreItemAction
velero.io/csi-volumesnapshot-restorer           RestoreItemAction
velero.io/csi-volumesnapshotclass-restorer      RestoreItemAction
velero.io/csi-volumesnapshotcontent-restorer    RestoreItemAction

- Kubernetes version (use `kubectl version`):
Client Version: v1.18.9
Server Version: v1.18.9

- Kubernetes installer & version:
- Cloud provider or hardware configuration:
On prem : vCloud vms are used for master and worker nodes 
- OS (e.g. from `/etc/os-release`):

NAME=""Ubuntu""
VERSION=""20.04.2 LTS (Focal Fossa)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 20.04.2 LTS""
VERSION_ID=""20.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,dsu,"
--
This needs to be fixed as part of https://github.com/vmware-tanzu/velero/issues/2066 as we move CSI snapshots to GA
--
",,,,,,,,,,
3454,OPEN,Convert Pod Volume Backup resource/controller to the Kubebuilder framework,Enhancement/Dev; P1 - Important,2021-02-22 18:29:04 +0000 UTC,codegold79,Opened,,"Convert Pod Volume Backup resource/controller to the Kubebuilder framework

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).
Use the ""reaction smiley face"" up to the right of this comment to vote.

- 👍 for ""The project would be better with this feature added""
- 👎 for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3452,OPEN,Update migration documentation to point out the feature flag isn't required.,Area/Documentation,2021-02-22 12:53:15 +0000 UTC,nrb,In progress,,"Migrating between kubernetes versions doesn't strictly require this feature flag, so long as there were no incompatible changes to the core types. This is a little tricky to provide guidance on, because while I know v1.15.->1.16 did this, not all versions do, and I don't really want to maintain an exhaustive list of version jumps that this is relevant to. 

Could we maybe add some wording to this about Kubernetes upgrades containing incompatible or removed API groups?

_Originally posted by @nrb in https://github.com/vmware-tanzu/velero/pull/3133#discussion_r573971114_",,,nrb,"
--
/area Documentation
--
",a,"
--
The feature flag being referred to in this issue is the EnableAPIGroupVersions, https://velero.io/docs/main/enable-api-group-versions-feature/. 

I'd think the best place to point folks to are the Kubernetes release notes for the cluster versions they are migrating between to see if there are any changes in api versions, https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG.

Also, are there any potentially negative affects of having this flag enabled when you are migrating between clusters where the api group doesn't change? Other than the (i assume) small lift of enabling this flag when maybe you dont need to, would it cause other problems? 

cc: @nrb 
--
",,,,,,,,
3451,OPEN,More examples for using API types could be useful,Area/Documentation,2021-03-01 19:54:17 +0000 UTC,jonasrosland,Opened,,"**Describe the problem/challenge you have**
Based on discussions during the Velero Office Hours today, the [API types](https://velero.io/docs/v1.5/api-types/) examples can be a bit overwhelming

**Describe the solution you'd like**
Having more examples on how to use the [Velero API Types](https://velero.io/docs/v1.5/api-types/) could be useful for those wanting to implement UIs and other methods of controlling Velero. 

**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`):
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,carlisia,"
--
Hi @jonasrosland 👋  !

I am not following what the issue is. Would love to make things less overwhelming. Help me understand, it is perceived now as overwhelming in what way? I don't recall the discussion. More examples would make it less overwhelming?
--
",,,,,,,,,,
3450,OPEN,Velero restore gets stuck restoring an nfs-pv using restic,Needs investigation,2021-03-12 23:37:24 +0000 UTC,fsz285,Opened,,"**What steps did you take and what happened:**
I am trying to backup and restore a container mounting an nfs pv.
I am using the aws plugin 1.1.0. I am also using restic, as I couldn't do the volume backups using just the provider.
The backup goes through without exceptions, but the restore gets stuck when the nfs volume is restored.

Those are the kubernetes resoucres:
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ubuntu-pvc-to-distribute
  namespace: ubuntu
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ubuntu-nfs
spec:
  serviceName: ubuntu-nfs
  replicas: 1
  revisionHistoryLimit: 3
  selector:
    matchLabels:
      role: ubuntu-nfs
  template:
    metadata:
      name: ubuntu-nfs
      labels:
        role: ubuntu-nfs
    spec:
      containers:
      - name: nfs
        image: 'gcr.io/google_containers/volume-nfs:0.8'
        lifecycle:
          postStart:
            exec:
              command: [""/bin/sh"", ""-c"", ""useradd -u 1000 u1000 && mkdir -p /exports/ubuntu && chown -R u1000:u1000 /exports""]
          preStop:
            exec:
              command:
                - sh
                - -c
                - ""sleep 15""
        imagePullPolicy: IfNotPresent
        ports:
          - name: nfs
            containerPort: 2049
          - name: mountd
            containerPort: 20048
          - name: rpcbind
            containerPort: 111
        securityContext:
          privileged: true
        volumeMounts:
          - mountPath: /exports
            name: storage
      volumes:
        - name: storage
          persistentVolumeClaim:
            claimName: ubuntu-pvc-to-distribute
---
apiVersion: v1
kind: Service
metadata:
  name: ubuntu-nfs
spec:
  ports:
    - name: nfs
      port: 2049
    - name: mountd
      port: 20048
    - name: rpcbind
      port: 111
  selector:
    role: ubuntu-nfs
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: ubuntu-nfs
spec:
  capacity:
    storage: '10Gi'
  accessModes:
    - ReadWriteMany
  nfs:
    server: ubuntu-nfs.ubuntu.svc.cluster.local
    path: ""/ubuntu/""
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ubuntu-nfs-claim
  namespace: ubuntu
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: """"
  volumeName: ubuntu-nfs
  resources:
    requests:
      storage: '10Gi'
---
kind: Pod
apiVersion: v1
metadata:
  name: ferdi-ubuntu
spec:
  containers:
    - name: ubuntu
      image: ubuntu
      command: [""/bin/bash"", ""-ec"", ""while :; do echo '.'; sleep 5 ; done""]
      volumeMounts:
      - mountPath: /data
        name: ubuntu-data
  volumes:
  - name: ubuntu-data
    persistentVolumeClaim:
      claimName: ubuntu-nfs-claim
```

**What did you expect to happen:**


**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

Here's an excerpt from the logs that might be related to the issue.
```
time=""2021-02-11T12:04:23Z"" level=debug msg=""Looking for path matching glob"" backup=velero/test12 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:218"" name=test12-2rkb6 namespace=velero pathGlob=""/host_pods/463252ed-4a26-4053-becd-3ee157fc2103/volumes/*/ubuntu-nfs""
time=""2021-02-11T12:04:23Z"" level=debug msg=""Found path matching glob"" backup=velero/test12 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:225"" name=test12-2rkb6 namespace=velero path=""/host_pods/463252ed-4a26-4053-becd-3ee157fc2103/volumes/kubernetes.io~nfs/ubuntu-nfs""
time=""2021-02-11T12:26:34Z"" level=debug msg=""Restore's pod ubuntu/ubuntu-nfs-0 not found, not enqueueing."" controller=pod-volume-restore error=""pod \""ubuntu-nfs-0\"" not found"" logSource=""pkg/controller/pod_volume_restore_controller.go:140"" name=test12-20210211132626-pznzj namespace=velero restore=velero/test12-20210211132626
```
```
 # velero restore describe test12-20210211132626 --details
Name:         test12-20210211132626
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:  InProgress

Started:    2021-02-11 13:26:31 +0100 CET
Completed:  <n/a>

Backup:  test12

Namespaces:
  Included:  all namespaces found in the backup
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io
  Cluster-scoped:  auto

Namespace mappings:  <none>

Label selector:  <none>

Restore PVs:  auto

Restic Restores:
  Completed:
    ubuntu/ubuntu-nfs-0: storage
  New:
    ubuntu/ferdi-ubuntu: ubuntu-data
```
```
 # velero backup describe test12 --details
Name:         test12
Namespace:    velero
Labels:       app.kubernetes.io/instance=velero
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/name=velero
              helm.sh/chart=velero-2.14.8
              velero.io/schedule-name=velero-daily
              velero.io/storage-location=aws
Annotations:  velero.io/source-cluster-k8s-gitversion=v1.17.12-1+36738515228c42
              velero.io/source-cluster-k8s-major-version=1
              velero.io/source-cluster-k8s-minor-version=17+

Phase:  Completed

Errors:    0
Warnings:  0

Namespaces:
  Included:  ubuntu
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  aws

Velero-Native Snapshot PVs:  auto

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  1.1.0

Started:    2021-02-11 13:04:19 +0100 CET
Completed:  2021-02-11 13:04:31 +0100 CET

Expiration:  2021-03-13 13:04:19 +0100 CET

Total items to be backed up:  33
Items backed up:              33

Resource List:
  apps/v1/ControllerRevision:
    - ubuntu/ubuntu-nfs-57784f9f9d
  apps/v1/StatefulSet:
    - ubuntu/ubuntu-nfs
  v1/Endpoints:
    - ubuntu/ubuntu-nfs
  v1/Event:
    - ubuntu/ferdi-ubuntu.1662af50542825f8
    - ubuntu/ferdi-ubuntu.1662af764d43ac92
    - ubuntu/ferdi-ubuntu.1662af764ef4fc54
    - ubuntu/ferdi-ubuntu.1662af9589fa411b
    - ubuntu/ferdi-ubuntu.1662af958c885694
    - ubuntu/ferdi-ubuntu.1662af97a456cbc7
    - ubuntu/ferdi-ubuntu.1662af9847d967a8
    - ubuntu/ferdi-ubuntu.1662af98b17fc4c3
    - ubuntu/ferdi-ubuntu.1662af98bc41403e
    - ubuntu/ferdi-ubuntu.1662af98cb2faa05
    - ubuntu/ubuntu-nfs-0.1662af448b709d44
    - ubuntu/ubuntu-nfs-0.1662af48571b6e63
    - ubuntu/ubuntu-nfs-0.1662af4d7f7578b0
    - ubuntu/ubuntu-nfs-0.1662af4d8768186b
    - ubuntu/ubuntu-nfs-0.1662af4d947d2f41
    - ubuntu/ubuntu-nfs.1662af3fdabbc34c
    - ubuntu/ubuntu-pvc-to-distribute.1662af3fcb4a454c
    - ubuntu/ubuntu-pvc-to-distribute.1662af3fe3305295
    - ubuntu/ubuntu-pvc-to-distribute.1662af3feb1323b3
    - ubuntu/ubuntu-pvc-to-distribute.1662af4471a336ff
  v1/Namespace:
    - ubuntu
  v1/PersistentVolume:
    - pvc-de0f0f15-6818-481b-b317-faa4d9fb8a3d
    - ubuntu-nfs
  v1/PersistentVolumeClaim:
    - ubuntu/ubuntu-nfs-claim
    - ubuntu/ubuntu-pvc-to-distribute
  v1/Pod:
    - ubuntu/ferdi-ubuntu
    - ubuntu/ubuntu-nfs-0
  v1/Secret:
    - ubuntu/default-token-9kh2k
  v1/Service:
    - ubuntu/ubuntu-nfs
  v1/ServiceAccount:
    - ubuntu/default

Velero-Native Snapshots: <none included>

Restic Backups:
  Completed:
    ubuntu/ferdi-ubuntu: ubuntu-data
    ubuntu/ubuntu-nfs-0: storage
```

**Environment:**

- Velero version (use `velero version`):
```
  Client:
        Version: v1.5.2
        Git commit: e115e5a191b1fdb5d379b62a35916115e77124a4
  Server:
        Version: v1.5.3
```
```
velero-plugin-for-aws:v1.1.0
```
The volume backups are made with restic.
- Velero features (use `velero client config get features`): 
not set
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""16"", GitVersion:""v1.16.2"", GitCommit:""c97fe5036ef3df2967d086711e6c0c405941e14b"", GitTreeState:""clean"", BuildDate:""2019-10-15T19:18:23Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17+"", GitVersion:""v1.17.12-1+36738515228c42"", GitCommit:""36738515228c4274bf0cc42b0b15d2bfedabd85f"", GitTreeState:""clean"", BuildDate:""2020-09-17T09:01:41Z"", GoVersion:""go1.13.15"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration:
IONOS
- OS (e.g. from `/etc/os-release`):
Windows 10 running ubuntu 18.04 via WSL1


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,Ahmadre,"
--
👍  I got the same problem
--
",,,,,,,,,,
3449,OPEN,aws s3 BackupStorageLocation behind proxy: Unable to determine bucket's region,Area/Cloud/AWS; Needs info,2021-03-03 21:57:39 +0000 UTC,aayushrangwala,In progress,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)

While running velero from the official [helm chart](https://github.com/vmware-tanzu/helm-charts/tree/main/charts/velero), behind a local (squid) proxy on mac using a self-signed certificate. As per the [docs](https://velero.io/docs/v1.5/self-signed-certificates/#docs) for passing cacert to velero, I am adding it to the BackupStorageLocation, but we are getting continuous errors as below:

```
time=""2021-02-11T11:17:36Z"" level=error msg=""Error getting backup store for this location"" backupLocation=dp-unstable-dev controller=backup-sync error=""rpc error: code = Unknown desc = unable to determine bucket's region"" error.file=""/go/src/github.com/vmware-tanzu/velero-plugin-for-aws/velero-plugin-for-aws/helpers.go:52"" error.function=main.GetBucketRegion logSource=""pkg/controller/backup_sync_controller.go:167""
```

This issue is a part of another issue which was solved ref: #3433

It is found in the code of the plugin that, while determining region, it create an aws session which donot consider the ca certs passed in the BSL or any configuration.
Since the certificates which are needed to be passed for the s3 region api to work are known but its not added in the session config.

**What did you expect to happen:**

velero should be able to get the correct region for the bucket and prefix passed in the BSL

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`: [Here](https://gist.github.com/aayushrangwala/1307ac07dab8397913946a1c9719e36e) are the logs


**Environment:**


  - Velero version: v1.5.3 both client and server
  - Velero features: <NOT SET>
  -  Kubernetes version: kubectl version: v1.20 and server: v1.19
  - Kubernetes installer & version: Kind: kind v0.9.0 go1.15.2 darwin/amd64
  - Cloud provider or hardware configuration: Mac/OSX


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,zubron,"
--
@aayushrangwala - Is there a reason why you are relying on the plugin determining the region and can't add the region to the BSL config as described in the AWS plugin docs? https://github.com/vmware-tanzu/velero-plugin-for-aws#install-and-start-velero

Looking at where this error is generated in the AWS plugin, it should only attempt to find the region if both the `s3Url` and `region` are not specified in config: https://github.com/vmware-tanzu/velero-plugin-for-aws/blob/main/velero-plugin-for-aws/object_store.go#L128-L137
By providing either of those, this check should be skipped. This should unblock you in the meantime.
--

--
@aayushrangwala With regard to the VolumeSnapshot APIs, from looking at the AWS SDK docs, it's not clear whether there is a way to detect the region, and the region is a [required parameter when creating the config for a session](https://github.com/aws/aws-sdk-go/blob/v1.37.17/aws/config.go#L63-L70). We can avoid specifying the region with S3 related calls as the SDK provides the [`GetBucketRegion`](https://pkg.go.dev/github.com/aws/aws-sdk-go/service/s3/s3manager#GetBucketRegion) function but there may not be a similar approach for interacting with snapshots.
--

--
@aayushrangwala Okay, that's good to know. I mentioned it due to your [previous comment](https://github.com/vmware-tanzu/velero/issues/3449#issuecomment-777648677) about the region not always being provided.
--
",aayushrangwala,"
--
> @aayushrangwala - Is there a reason why you are relying on the plugin determining the region and can't add the region to the BSL config as described in the AWS plugin docs? https://github.com/vmware-tanzu/velero-plugin-for-aws#install-and-start-velero
> 
> Looking at where this error is generated in the AWS plugin, it should only attempt to find the region if both the `s3Url` and `region` are not specified in config: https://github.com/vmware-tanzu/velero-plugin-for-aws/blob/main/velero-plugin-for-aws/object_store.go#L128-L137
> By providing either of those, this check should be skipped. This should unblock you in the meantime.

@zubron  Thanks for the response. Yes, I checked that code and got unblocked for now, but raised this bug as we are working on a feature where the user might not always give the region and the cluster workloads must be running behind the proxy. Which makes us blocked by this issue as, if the aws session created in the plugin doesn't use the certs (as discussed in the referred ticket), the BSL wont be in Available state.

So is this something which needs a bit of discussion and more suggestions before even qualifying it as a bug or it can be considered and he actual work is started on it?
--

--
> @aayushrangwala Thanks for getting the PR started!
> 
> I think this might be useful for BSLs behind a proxy, but I'm concerned about the volumesnapshot calls (if any) also being affected. I think if we were to adopt this, then we'd want to make sure _all_ AWS calls were supported.
> 
> Does your use case only use S3, without EBS volumes?

@nrb I think we should support the VolumeSnapshot API calls also along with the BSL ones. Because if we are adopting the certs support it should be more generic rather than a specific use case. But for this we might need more changes
--

--
> @aayushrangwala With regard to the VolumeSnapshot APIs, from looking at the AWS SDK docs, it's not clear whether there is a way to detect the region, and the region is a [required parameter when creating the config for a session](https://github.com/aws/aws-sdk-go/blob/v1.37.17/aws/config.go#L63-L70). We can avoid specifying the region with S3 related calls as the SDK provides the [`GetBucketRegion`](https://pkg.go.dev/github.com/aws/aws-sdk-go/service/s3/s3manager#GetBucketRegion) function but there may not be a similar approach for interacting with snapshots.

@zubron Yes I agree, we can assume that region will always be present in the VolumeSnapshot CR but still volumeSnapshotter needs to communicate outside the proxy, calling aws calls to provision snapshots, for that the aws session used, needs to have the certs. This problem was solved for us in BSL because it already has the caCert field and consumes it


--

--
> @aayushrangwala Okay, that's good to know. I mentioned it due to your [previous comment](https://github.com/vmware-tanzu/velero/issues/3449#issuecomment-777648677) about the region not always being provided.

@zubron To add the same support in VolumeSnapshot, I believe, we need to add a new similar filed of caCert in VolumeSnapshot CRD. The cert file passed to velero install command will propagate the cert data to both the objects, BSL and VSL. And the aws plugin VolSnapshotter implementation will add the certs to the aws sessions.

Please suggest the next steps on the same.
--
",nrb,"
--
@aayushrangwala Thanks for getting the PR started!

I think this might be useful for BSLs behind a proxy, but I'm concerned about the volumesnapshot calls (if any) also being affected. I think if we were to adopt this, then we'd want to make sure _all_ AWS calls were supported.

Does your use case only use S3, without EBS volumes?
--

--
> Yes, I checked that code and got unblocked for now, but raised this bug as we are working on a feature where the user might not always give the region and the cluster workloads must be running behind the proxy. 

I think this should be solved via validation - the region is a required parameter for the call, so we should communicate this to the user.

For volume snapshots, you are correct that the new field would need to be added. This is achievable, certainly, and I don't think the technical challenges for accepting and populating being too difficult.

The complication I see is using an MitM proxy that may cause issues; I am not currently aware of proxy support on AWS EBS calls, but we can research that. I think your use case doesn't require this right now, but it's a scenario we need to consider to get this parity.

Another complication to consider is node-based authentication, such as AWS's IAM AssumeRole support. How does this cert work interact with such a setup?
--
",,,,,,
3440,OPEN,Connection error AzureChinaCloud for velero-plugin-for-microsoft-azure,Area/Cloud/Azure; Needs info; Needs investigation,2021-03-08 09:44:41 +0000 UTC,boehlefeld,In progress,,"**What steps did you take and what happened:**
We already using Velero in AzurePublicCloud for k8s resources successfully, but in AzureChinaCloud we fail to setup. 
The configuration is almost default, but see the detailed configuration attached. 

Scenario:
- What: Redis among other volumes
- Where: The resources are located in a different subscription id from where the storage account is located 
- Permission: service principal auth


Velero pod constantly fails to connect to AzureChinaCloud:


- Subscriptions ids, tenant id, client id triple checked.
- A blob upload test using the service principal within a pod located in AzureChinaCloud was successful

**What did you expect to happen:**
Backups from AzureChinaCloud located resources to be transferred to AzureChinaCloud located Azure Storage Account. 

**The output of the following commands will help us better understand what's going on**:

- `kubectl logs deployment/velero -n velero` [logs](https://pastebin.com/sVcuxaYa)
- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml` [logs](https://pastebin.com/CncEw0zF)
- `velero backup logs <backupname>` no logs


**Anything else you would like to add:**
- [Helm command used](https://pastebin.com/qn4tNhkA)
- [helm get values output](https://pastebin.com/5SHy2iMX) **updated**
- Credential file used:
```
AZURE_SUBSCRIPTION_ID=AAA***
AZURE_TENANT_ID=BBB***
AZURE_CLIENT_ID=CCC***
AZURE_CLIENT_SECRET=***
AZURE_RESOURCE_GROUP=RG1***
AZURE_CLOUD_NAME=AzureChinaCloud
```

**Environment:**

- Velero version (use `velero version`): 
```
Client:
	Version: v1.5.3
	Git commit: -
Server:
	Version: v1.5.3
```
- Velero features (use `velero client config get features`): 
`features: <NOT SET> `
- Kubernetes version (use `kubectl version`):
> Client Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.2"", GitCommit:""faecb196815e248d3ecfb03c680a4507229c2a56"", GitTreeState:""clean"", BuildDate:""2021-01-14T05:14:17Z"", GoVersion:""go1.15.6"", Compiler:""gc"", Platform:""darwin/amd64""} 
> Server Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.14"", GitCommit:""89182bdd065fbcaffefec691908a739d161efc03"", GitTreeState:""clean"", BuildDate:""2020-12-18T16:05:02Z"", GoVersion:""go1.13.15"", Compiler:""gc"", Platform:""linux/amd64""}
- Kubernetes installer & version:
> Azure Portal
- Cloud provider or hardware configuration:
> AzureChinaCloud
- OS (e.g. from `/etc/os-release`):


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,zubron,"
--
Hi @boehlefeld - Sorry you're having issue with this.

I know you've said that you've checked all the IDs, but I have noticed this error in the logs: 
```
time=""2021-02-09T16:56:57Z"" level=error msg=""Error getting backup store for this location"" backupLocation=default controller=backup-sync error=""rpc error: code = Unknown desc = azure.BearerAuthorizer#WithAuthorization: Failed to refresh the Token for request to https://management.chinacloudapi.cn/subscriptions/EEE***/resourceGroups/RG2***/providers/Microsoft.Storage/storageAccounts/***/listKeys?%24expand=kerb&api-version=2019-06-01: StatusCode=400 -- Original Error: adal: Refresh request failed. Status Code = '400'. Response body: {\""error\"":\""invalid_request\"",\""error_description\"":\""AADSTS90002: Tenant 'BBB***' not found. This may happen if there are no active subscriptions for the tenant. Check to make sure you have the correct tenant ID. ***\""error_codes\"":[90002]\""error_uri\"":\""https://login.microsoftonline.com/error?code=90002\""}"" error.file=""/go/src/velero-plugin-for-microsoft-azure/velero-plugin-for-microsoft-azure/object_store.go:213"" error.function=main.getStorageAccountKey logSource=""pkg/controller/backup_sync_controller.go:168""
```

In particular, I notice that the URL that is using `EEE***` as the subscription which doesn't match any of the substituted subscription IDs in the helm values you've used. Is there any way for you to validate, perhaps in the Azure UI, that the given Tenant ID exists and has a valid subscription and that that subscription ID is being used?
--
",boehlefeld,"
--
Hi @zubron,

thank you! I didn't triple check the obfuscated version ;-).
The helm get values output is now correct; I've updated the link.

>Is there any way for you to validate, perhaps in the Azure UI, that the given Tenant ID exists and has a valid subscription and that that subscription ID is being used?


azure.portal.cn
- open subscription AAA***
  - Overview
    - 'Parent management group'
      - BBB***
- open subscription EEE***
  - Overview
    - 'Parent management group'
      - BBB***


I fired up an interactive azure-cli shell in the cluster and logged in using the service principal.
Seems legit to me too:
```
[
  {
    ""cloudName"": ""AzureChinaCloud"",
    ""homeTenantId"": ""BBB***"",
    ""id"": ""AAA***"",
    ""isDefault"": true,
    ""managedByTenants"": [],
    ""name"": ""***"",
    ""state"": ""Enabled"",
    ""tenantId"": ""AAA***"",
    ""user"": {
      ""name"": ""CCC***"",
      ""type"": ""servicePrincipal""
    }
  },
  {
    ""cloudName"": ""AzureChinaCloud"",
    ""homeTenantId"": ""BBB***"",
    ""id"": ""EEE***"",
    ""isDefault"": false,
    ""managedByTenants"": [],
    ""name"": ""***"",
    ""state"": ""Enabled"",
    ""tenantId"": ""AAA***"",
    ""user"": {
      ""name"": ""CCC***"",
      ""type"": ""servicePrincipal""
    }
  }
]
```

Thank you helping me investigating
--

--
Hi @zubron,

any information missing - 
How can I help investigating?
--
",haannnees,"
--
Any updates on that issue? :) 
--
",,,,,,
3419,OPEN,Restoring a LoadBalancer type service gives failed to allocate requested HealthCheck NodePort: provided port is already allocated error,Needs investigation,2021-03-09 21:48:42 +0000 UTC,cihangirbesiktas,Opened,,"**What steps did you take and what happened:**
I created a backup of a namespace which contains a service with type loadbalancer. Then, I deleted everything in the namespace for testing. After that, I run velero restore command, but I am getting `failed to allocate requested HealthCheck NodePort: provided port is already allocated` error. 


**What did you expect to happen:**
The health check node port should be randomly generated by kubernetes in order to avoid this, so `healthCheckNodePort` shouldn't be provided by velero when restoring if I don't give `--preserve-nodeports` in restore command. This is described in https://velero.io/docs/v1.5/restore-reference/#what-happens-to-nodeports-when-restoring-services

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
- `velero backup logs <backupname>`
- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): 
```
$ velero version
Client:
	Version: v1.4.0
	Git commit: -
Server:
	Version: v1.4.0
```
- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`):
```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""19"", GitVersion:""v1.19.0"", GitCommit:""e19964183377d0ec2052d1f1fa930c4d7575bd50"", GitTreeState:""clean"", BuildDate:""2020-08-26T21:54:15Z"", GoVersion:""go1.15"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.14"", GitCommit:""89182bdd065fbcaffefec691908a739d161efc03"", GitTreeState:""clean"", BuildDate:""2020-12-18T16:05:02Z"", GoVersion:""go1.13.15"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
Azure AKS
- OS (e.g. from `/etc/os-release`):


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3407,OPEN,PVC restores completed; however data is missing,Needs investigation; Restic,2021-02-03 15:34:44 +0000 UTC,carlisia,Opened,,"Original: 

https://github.com/vmware-tanzu/velero/discussions/3297

Also related: https://kubernetes.slack.com/archives/C6VCGP4MT/p1612342739112000",,,,,,,,,,,,,,
3392,OPEN,Add a test for large numbers of secrets and other resources that will require chunked listing,Enhancement/Dev; P2 - Long-term important,2021-03-09 00:44:01 +0000 UTC,dsu-igeek,Opened,,"**Describe the problem/challenge you have**
Large numbers of resources are not being handled and our tests are not showing the problem


**Describe the solution you'd like**
Once https://github.com/vmware-tanzu/velero/issues/262 has been addressed, introduce a test that validates it.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3386,OPEN,AzureChinaCloud Restic URL ERROR,Area/Cloud/Azure; Enhancement/User; P2 - Long-term important; Restic,2021-03-09 00:42:18 +0000 UTC,XiaFanGit,In progress,,"when I use velero + restic to backup aks cluster. the restic repo is <blobname>.blob.core.windows.net.
however, in AzureChinaCLoud it should to be <blobname>.blob.core.chinacloudapi.cn

* The output of the following commands

```
cat credentials-velero 
AZURE_STORAGE_ACCOUNT_ACCESS_KEY=<accesskeys>
AZURE_CLOUD_NAME=AzureChinaCloud
```

```
velero install \
    --provider azure \
    --plugins velero/velero-plugin-for-microsoft-azure:v1.1.0 \
    --bucket velero \
    --secret-file ./credentials-velero \
    --backup-location-config resourceGroup=<RG>,storageAccount=<account_name>,storageAccountKeyEnvVar=AZURE_STORAGE_ACCOUNT_ACCESS_KEY,subscriptionId=<subscriptionId>\
    --use-volume-snapshots=false \
    --use-restic
```

```
kubectl logs deployment/velero -n velero
time=""2021-02-01T10:20:01Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
```

```
velero backup logs kube-logging|grep error
time=""2021-02-01T10:13:36Z"" level=error msg=""Error backing up item"" backup=velero/kube-logging error=""restic repository is not ready: error running command=restic init --repo=azure:velero:/restic/kube-logging --password-file=/tmp/velero-restic-credentials-kube-logging392714186 --cache-dir=/scratch/.cache/restic, stdout=, stderr=Fatal: create repository at azure:velero:/restic/kube-logging failed: container.CreateIfNotExists: Put https://velerostaging.blob.core.windows.net/velero?restype=container: dial tcp: lookup velerostaging.blob.core.windows.net on 10.0.0.10:53: no such host\n\n: exit status 1"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/restic/repository_ensurer.go:144"" error.function=""github.com/vmware-tanzu/velero/pkg/restic.(*repositoryEnsurer).EnsureRepo"" logSource=""pkg/backup/backup.go:431"" name=nutstore-log-es-master-nodes-0
```

**Environment:**

- Velero version (use `velero version`): 
```
velero version
Client:
        Version: v1.5.3
        Git commit: 123109a3bcac11dbb6783d2758207bac0d0817cb
Server:
        Version: v1.5.3
```
- Velero features (use `velero client config get features`): none

- Kubernetes version (use `kubectl version`): 1.18.14

- Cloud provider or hardware configuration: AKS
",,,haannnees,"
--
same problem
--
",zubron,"
--
This is an open issue with restic as it does not provide a way to configure the domain to use: https://github.com/restic/restic/issues/2468

The suggested workaround there is to use the `rclone` backend for restic however this is is not currently supported in Velero. We have an open issue to support more backends: #1178.
--
",XiaFanGit,"
--
@zubron Hi, Thanks a lot for your answer. I compiled restic manually and it work fine. Thx 


--
",dsu,"
--
We'll leave this open until we can integrate the fixes to Restic.
--
",,,,
3359,OPEN,Resource filtering while restore,Enhancement/User; P2 - Long-term important,2021-03-09 21:46:12 +0000 UTC,cuttingedge1109,Opened,,"Hi team. Thank you for your effort. We've been enjoying velero for several years.

# Context
We have a scheduled backup for the whole cluster.
I deployed `test-helm` helmrelease in `test-ns` namespace. I try restore from the whole backup by filtering the resources with `--include-namespace`.
```
velero restore create --from-backup SCHEDULED_BACKUP_NAME --include-namespaces test-ns
```
The helmrelease `test-helm` is restored and the status of that is `deployed` and the verbose message is ` Release was successful for Helm release 'test-helm' in 'test-ns'.`

# Challenge
The chart of `test-helm` helmrelease includes one `ClusterRole` and `ClusterRoleBinding`, cluster scope objects.
I noticed that those cluster scope objects are not restored.

# Expectation
I thought the cluster-scope objects should be restored even though I used `--include-namespace` resource filter because those are sub components of the helmrelease which is the namespace-scope object.
So the velero resource filters are filtering the resources which are created by helmreleases?
",,,dsu,"
--
Velero does not restore the cluster level resources if --include-namespaces is used.  We'll track the issue here.  The work-around we have at the moment would be to filter during the backup.
--
",,,,,,,,,,
3358,OPEN,"Restic backup is failing with error ""Error creating temp restic credentials file""",Needs info,2021-02-19 17:12:17 +0000 UTC,email2smohanty,Opened,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)
- Created openshift cluster in aws
- Installed `rabbitmq` sample application which has one persistent storage
- Installed velero on openshift cluster and configured with ibm cloud s3 object storage
- Tried to take backup of `rabbitmq` sample application but backup failed with below error

```
time=""2021-01-28T06:44:24Z"" level=error msg=""Error creating temp restic credentials file"" backup=velero/rabitmq-masterbackup controller=pod-volume-backup error=""open /tmp/velero-restic-credentials-poco389664145: read-only file system"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/restic/common.go:254"" error.function=github.com/vmware-tanzu/velero/pkg/restic.TempCredentialsFile logSource=""pkg/controller/pod_volume_backup_controller.go:230"" name=rabitmq-masterbackup-28j8r namespace=velero 
```

**What did you expect to happen:**
Backup should be taken properly without any error

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
- `velero backup logs <backupname>`
- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`):  v1.5.2
- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`): Server Version: v1.19.0+3b01205, Client Version: v1.18.8
- Kubernetes installer & version:
- Cloud provider or hardware configuration: AWS
- OS (e.g. from `/etc/os-release`): 


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,zubron,"
--
Hi @email2smohanty. The temporary file that is created by Velero for restic authentication is only ever written within the velero pod's filesystem however it looks like the filesystem for your velero pod is read-only: `open /tmp/velero-restic-credentials-poco389664145: read-only file system` . Have you applied any security constraints that would make the pod's filesystem read-only?
--
",xzlin,"
--
I have the same issue. My openshift version is 4.5. Velero pod is created by velero install. I applied requirements from restic document https://velero.io/docs/v1.5/restic/
$ oc adm policy add-scc-to-user privileged -z velero -n velero
oc patch ds/restic \
  --namespace velero \
  --type json \
  -p '[{""op"":""add"",""path"":""/spec/template/spec/containers/0/securityContext"",""value"": { ""privileged"": true}}]'
I didn't do anything else.
--
",,,,,,,,
3353,OPEN,Use manager context in controller `Reconcile` functions,Enhancement/Dev,2021-01-26 16:05:57 +0000 UTC,zubron,Opened,,"With #3202, we are upgrading the version of controller-runtime. This includes a change (https://github.com/kubernetes-sigs/controller-runtime/pull/1054) to add a context to the `Reconciler` interface. This is the context that is now passed to the [Manager on Start](https://github.com/vmware-tanzu/velero/pull/3202/files#diff-7a8f33109587a2bd51be299c9d2b3849a38ce9d19715e2ea04449b4602fbba54R862).

The existing Reconcilers ([BSL](https://github.com/vmware-tanzu/velero/blob/main/pkg/cmd/server/server.go#L820) and [ServerStatusRequest](https://github.com/vmware-tanzu/velero/blob/main/pkg/cmd/server/server.go#L839)) already store a reference to this context, so we can instead remove these contexts from the types and instead use the one that is now passed through in [`Redoncile`](https://github.com/vmware-tanzu/velero/pull/3202/files#diff-392430c1e526e2d5a98e4a69deb90350eabcb7a77e58e2f967b4c806568dda2aR55).",,,,,,,,,,,,,,
3349,OPEN,ERR_CONNECTION_TIMED_OUT upon restore of app with PVC,Needs investigation,2021-01-25 21:30:01 +0000 UTC,carlisia,Opened,,"@jc0705 apologies for moving this issue around. We concluded that this one is best kept as an issue vs. discussion. Please subscribe to this issue to get updates.

Link to original: https://github.com/vmware-tanzu/velero/discussions/3261.",,,,,,,,,,,,,,
3347,OPEN,Velero backup do not restore Deployment volumes [aws-efs],Area/Cloud/AWS; Needs investigation,2021-03-18 14:19:28 +0000 UTC,carlisia,Opened,,"@fabriziogaliano apologies for moving this issue around. We concluded that this one is best kept as an issue vs. discussion. Please subscribe to this issue to get updates.

Link to original: https://github.com/vmware-tanzu/velero/discussions/3245.",,,dsu,"
--
velero restic backup
steps to reproduce:

create a POD and a PVC in a specific namespace called velero-restic-test:
POD YAML FILE

kind: Pod
metadata:
  name: sample
  namespace: velero-restic-test
  annotations:
    backup.velero.io/backup-volumes: sample-volume
spec:
  tolerations:
   - effect: NoSchedule
     operator: Exists
   - effect: NoExecute
     operator: Exists
  containers:
  - image: busybox
    name: test-webserver
    command: [""tail""]
    args: [""-f"", ""/dev/null""]
    volumeMounts:
    - name: sample-volume
      mountPath: /volume-1
    - name: emptydir-volume
      mountPath: /volume-2
  volumes:
  - name: sample-volume
    persistentVolumeClaim:
      claimName: sample-volume
  - name: emptydir-volume
    emptyDir: {}
POD PVC YAML FILE

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: sample-volume
  namespace: velero-restic-test
  annotations:
    volume.beta.kubernetes.io/storage-class: ""aws-efs""
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10G
Apply resources:
kubectl apply -f pvc-pod.yaml
kubectl apply -f pod.yaml

DEPLOYMENT YAML FILE

apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample
  namespace: velero-restic-test
  annotations:
    backup.velero.io/backup-volumes: sample-volume2
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      tolerations:
       - effect: NoSchedule
         operator: Exists
       - effect: NoExecute
         operator: Exists
      containers:
      - image: busybox
        name: test-webserver
        command: [""tail""]
        args: [""-f"", ""/dev/null""]
        volumeMounts:
        - name: sample-volume2
          mountPath: /volume-1
        - name: emptydir-volume2
          mountPath: /volume-2
      volumes:
      - name: sample-volume2
        persistentVolumeClaim:
          claimName: sample-volume2
      - name: emptydir-volume2
        emptyDir: {}
DEPLOYMENT PVC YAML FILE

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: sample-volume2
  namespace: velero-restic-test
  annotations:
    volume.beta.kubernetes.io/storage-class: ""aws-efs""
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10G
Apply resources:
kubectl apply -f pvc-deployment.yaml
kubectl apply -f deployment.yaml

at this moment every pods is running with their volumes attached

just to do not have an empty folder, i've created a file with cat /dev/urandom > file in each volumes.

MAKE A VELERO BACKUP

velero backup create test-001 --include-namespaces velero-restic-test

this is the result of the backup:

Name:         test-001
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  velero.io/source-cluster-k8s-gitversion=v1.11.10
              velero.io/source-cluster-k8s-major-version=1
              velero.io/source-cluster-k8s-minor-version=11

Phase:  Completed

Errors:    0
Warnings:  0

Namespaces:
  Included:  velero-restic-test
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  default

Velero-Native Snapshot PVs:  auto

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  1

Started:    2020-12-18 13:30:36 +0000 UTC
Completed:  2020-12-18 13:30:42 +0000 UTC

Expiration:  2021-01-17 13:30:36 +0000 UTC

Total items to be backed up:  41
Items backed up:              41

Velero-Native Snapshots: <none included>

Restic Backups (specify --details for more information):
  Completed:  1
What we see in AWS S3?

in the Bucket i've 2 folder, like this:

 backups/
    ----- <VELERO_BACKUP_NAME>/
 restic/
    ----- <NAMESPACE_NAME>/
           ----- config/
           ----- data/
           ----- etc ...
TRY TO DO A RESTORE

kubectl delete -f pod.yaml
kubectl delete -f pvc-pod.yaml
kubectl delete -f deployment.yaml
kubectl delete -f pvc-deployment.yaml

Launch velero RESTORE:

velero restore create --from-backup test-001

everything seems recreated by velero, IF i go inside the POD to check if the data into the pvc volume it was restored i find the file i've created, in the Deployment every object was created but the file/s inside the volume NOT.

have you an idea why?

[sample-deployment-pvc-data.log](https:
restore-sample-deployment-pvc-data-20201218133434.log
//github.com/vmware-tanzu/velero/files/5716140/sample-deployment-pvc-data.log)

Environment:

Velero version (1.4.3):
Velero features (features: <NOT SET>):
Kubernetes version (1.11.10):
Kubernetes installer & version: Kops on AWS
Cloud provider (AWS):
Vote on this issue!

This is an invitation to the Velero community to vote on issues, you can see the project's top voted issues listed here.
Use the ""reaction smiley face"" up to the right of this comment to vote.

👍 for ""I would like to see this bug fixed as soon as possible""
👎 for ""There are more important bugs to focus on right now""
--
",sseago,"
--
I'm not sure this is the entire issue, but at least part of the problem is the use of the (deprecated) beta storageclass annotation.
```
  annotations:
    volume.beta.kubernetes.io/storage-class: ""aws-efs""
```
The recommended way is to set `spec.storageClassName` instead. While this could probably be fixed with a relatively simple change to the csi plugin, when I came across a similar problem in the velero core repo related to this annotation a while back, at the time the general consensus was that since this is deprecated usage, there was little interest in fixing it upstream. I'm not sure whether the feeling has changed since then or not. In any case, it might be worth trying the above again with the modified PVC to see whether this resolves the problem here.
--
",,,,,,,,
3346,OPEN,Calico network policies being skipped in backups,Needs investigation,2021-03-08 23:36:39 +0000 UTC,carlisia,Opened,,"@colinliddlemyob apologies for moving this issue around. We concluded that this one is best kept as an issue vs. discussion. Please subscribe to this issue to get updates.

Link to original: https://github.com/vmware-tanzu/velero/discussions/3270.",,,dsu,"
--
What steps did you take and what happened:
Backing up and restoring a cluster with calico network policies

What did you expect to happen:
Both k8s network policies and calico network policies should be backed up and restored

The output of the following commands will help us better understand what's going on:

Creating a backup and just specifying the calico network policies will create a backup (but just with those resources)
velero create backup --include-resources networkpolicies.crd.projectcalico.org ...

Creating a backup without specifying resources (eg all resources) will skip over the calico network policies because it thinks it has already backed them up, but was actually just the k8s native network policies
velero create backup ...
time=""2021-01-06T23:32:50Z"" level=info msg=""Skipping resource because it cohabitates and we've already processed it"" backup=kube-system/mf-backup-9 cohabitatingResource1=networkpolicies.extensions cohabitatingResource2=networkpolicies. networking.k8s.io group=crd.projectcalico.org/v1 logSource=""pkg/backup/resource_backupper.go:148"" resource=networkpolicies

Anything else you would like to add:

If I try --include-resources *,networkpolicies.crd.projectcalico.org I get the error Validation errors: Invalid included/excluded resource lists: includes list must either contain '*' only, or a non-empty list of items

Environment:

Velero version (use velero version): 1.3.1 (have tried 1.5.2 as well with same issues)
Velero features (use velero client config get features): None
Kubernetes version (use kubectl version): 1.17.12
Kubernetes installer & version: Self managed
Cloud provider or hardware configuration: AWS EC2
OS (e.g. from /etc/os-release): Amazon linux 2
Vote on this issue!

This is an invitation to the Velero community to vote on issues, you can see the project's top voted issues listed here.
Use the ""reaction smiley face"" up to the right of this comment to vote.

👍 for ""I would like to see this bug fixed as soon as possible""
👎 for ""There are more important bugs to focus on right now""
--
",,,,,,,,,,
3344,OPEN,Velero backed up PV Data to Minio and reported an error,Needs info; Needs investigation; Restic,2021-03-08 23:33:58 +0000 UTC,carlisia,Opened,,"@misteruly apologies for moving this issue around. We concluded that this one is best kept as an issue vs. discussion. Please subscribe to this issue to get updates.

Link to original: https://github.com/vmware-tanzu/velero/discussions/3244.",,,dsu,"
--
@misteruly If you're still having difficulties, we need more information.  Apologies for the move from discussion to issue, this was being asked:

--use-volume-snapshots=true Is that necessary?

No, it is not necessary. Using volume snapshots is only applicable when running on a provider that provides volume snapshotting capabilities, such as on AWS where you are using EBS for your volumes. If you are running on AWS and wish to use volume snapshots, you will also need to configure a VolumeSnapshotLocation. From your install command however, you are using MinIO and restic so volume snapshots will not apply.

From the logs, it looks like it is attempting to take a volume snapshot, did you create a volume snapshot location? It looks like some volumes are being backed up with restic and some are attempting to use snapshots.

Can you retry taking a backup without specifying --snapshot-volumes, e.g velero backup create velero-test-<name> --include-namespaces velero-test.

Also, can you provide the output from the following commands:

velero backup describe velero-test-pvc4 --details
velero restic repo get
velero snapshot-location get
Thanks!
--
",,,,,,,,,,
3342,OPEN,can't restore the ebs volume properly,Area/Cloud/AWS; Needs investigation,2021-03-11 23:18:40 +0000 UTC,carlisia,Opened,,"@ozbillwang apologies for moving this issue around. We concluded that this one is best kept as an issue vs. discussion. Please subscribe to this issue to get updates.

Link to original: https://github.com/vmware-tanzu/velero/discussions/3247.
",,,nrb,"
--
https://github.com/vmware-tanzu/velero/discussions/3247#discussioncomment-280104

@ozbillwang When you say ""By the way, why I can't keep the exist EBS to get restore successfully?,"" are you referring to the EBS volume?
--

--
This may be an issue with the original PV being a `retain` policy, which means the first EBS volume would not be deleted when the Kubernetes PV is deleted.
--
",ozbillwang,"
--
Yes, it is set with **retain** policy

I found, I have to manually delete the original EBS, then I can successfully restore. 
--
",,,,,,,,
3340,OPEN,Upgrade Kube dependencies,Enhancement/Dev,2021-01-25 21:31:08 +0000 UTC,carlisia,Opened,,"- Investigate which Kubernetes dependencies we should update for 1.7.
- Update dependencies.",,,,,,,,,,,,,,
3338,OPEN,Upgrade AWS Go sdk to v2,Enhancement/Dev,2021-01-25 18:22:40 +0000 UTC,carlisia,Opened,,https://github.com/aws/aws-sdk-go-v2/releases,,,,,,,,,,,,,,
3328,OPEN,Use smaller footprint image for Velero operator and plugins,Enhancement/User; P1 - Important,2021-02-03 17:46:05 +0000 UTC,sancyx,In progress,,"**Describe the problem/challenge you have**
Latest Velero operator and plugin images based on Ubuntu:focal has some security vulnerability issues.


**Describe the solution you'd like**
[A clear and concise description of what you want to happen.]
Provide Dockerfile for an alpine based images. We may eventually contribute the PR for alpine based version. 

**Environment:**

- Velero version (use `velero version`): 
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,nrb,"
--
@sancyx We're looking at updating Velero's image to use `scratch`, which should alleviate these issues.

We won't be changing to alpine for compliance reasons within VMware, but if `scratch` works, we will pursue that avenue. If not, we'll looking at something like `debian-slim`.
--
",sancyx,"
--
Thanks @nrb, that sound good, scratch based image should be fine.
--

--
@nrb BTW do you have an estimated timeline for v1.7.0?
--
",,,,,,,,
3315,OPEN,GKE PVC snapshots are not being taken,Needs investigation,2021-03-15 07:20:39 +0000 UTC,yellowmegaman,In progress,,"**What steps did you take and what happened:**
Installed Velero to GKE cluster, created a backup of namespace with PVC's in it.
PVC snapshots are not being created during backup, with no errors.


**What did you expect to happen:**
Expected to have backup with PVC snapshots, or at least an error.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
https://pastebin.pl/view/raw/40aa1002
- `kubectl get backup/web0 -n velero -o yaml`
https://pastebin.pl/view/raw/7e1c1c23
- `velero backup logs web0`
https://pastebin.pl/view/raw/2b0cbda5
- `kubectl -n velero get volumesnapshotlocations.velero.io default  -o yaml`
https://pastebin.pl/view/raw/6cea0af7
- most interesting log lines:
```
time=""2021-01-21T10:24:30Z"" level=info msg=""No volume ID returned by volume snapshotter for persistent volume"" backup=velero/pvconly logSource=""pkg/backup/item_backupper.go:458"" name=pvc-5bf6c9c5-b913-4854-aab9-d7f584810642 namespace= persistentVolume=pvc-5bf6c9c5-b913-4854-aab9-d7f584810642 resource=persistentvolumes volumeSnapshotLocation=default
time=""2021-01-21T10:24:30Z"" level=info msg=""Persistent volume is not a supported volume type for snapshots, skipping."" backup=velero/pvconly logSource=""pkg/backup/item_backupper.go:469"" name=pvc-5bf6c9c5-b913-4854-aab9-d7f584810642 namespace= persistentVolume=pvc-5bf6c9c5-b913-4854-aab9-d7f584810642 resource=persistentvolumes
```

**Anything else you would like to add:**
Velero was installed this way:
```
velero install \
    --provider velero.io/gcp \
    --plugins velero/velero-plugin-for-gcp:v1.1.0 \
    --bucket $BUCKET \
    --secret-file ./credentials-velero \
    --snapshot-location-config snapshotLocation=europe-west1,project=someproject
```
All other steps for service account were taken from https://github.com/vmware-tanzu/velero-plugin-for-gcp


**Environment:**

- Velero version (use `velero version`): v1.5.2
- Velero features (use `velero client config get features`): features: <NOT SET>
- Kubernetes version (use `kubectl version`): ""v1.17.14-gke.400""
- Cloud provider or hardware configuration: GKE on GCP
- OS (e.g. from `/etc/os-release`): Container-Optimized OS from Google


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,yellowmegaman,"
--
Retried again using this as an example: https://medium.com/@ct.ics2009/install-velero-with-helm-in-gcp-15b7a0186ae
Got same result deploying it as a helm chart
--
",wi1dcard,"
--
Having the same issue here, I guess it might be related to the CSI plugin since we're not using the built-in `gcePersistentDisk`?

```
time=""2021-03-15T07:13:22Z"" level=info msg=""Persistent volume is not a supported volume type for snapshots, skipping."" ...
```


--
",,,,,,,,
3305,OPEN,capi-controller-manager in capi-system namespace crashes after restoring the management cluster in TKG 1.2.1,Needs investigation,2021-01-25 20:23:04 +0000 UTC,McCoyAle,Opened,,"**What steps did you take and what happened:**

Steps to create issue are below. In addition, the following document was followed by another user to reproduce the same outcome (https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/1.2/vmware-tanzu-kubernetes-grid-12/GUID-mgmt-clusters-backup-restore-mgmt-cluster.html?hWord=N4IghgNiBcIG4FMIIE4HsQF8g):

Steps to recreate the issue:
1. Create a management cluster.
2. Set context for the management cluster
3. Create a workload cluster.
4. Install velero on the management cluster using the below command:
velero install --provider aws --plugins oss-harbor.test.local/tkg/velero/velero-plugin-for-aws:v1.1.0_vmware.1 --bucket velero --secret-file ~/velero/credentials-velero --use-restic --backup-location-config region=minio,s3ForcePathStyle=""true"",s3Url=http://10.92.221.234:9000 --image=oss-harbor.test.local/tkg/velero/velero:v1.4.2_vmware.1
5. Install the vSphere Velero plugin
velero plugin add oss-harbor.test.local/tkg/velero/velero-plugin-for-vsphere:v1.0.2_vmware.1
6. Take a backup of the management cluster.
velero snapshot-location create vsl-vsphere --provider velero.io/vsphere
kubectl patch cluster restworker --type='merge' -p '{""spec"":{""paused"": true}}'
velero backup create mgmtwork --exclude-namespaces=tkg-system
7. Create a second management cluster to run the restore process.
8. Change your context to the new management cluster.
9. Install the Velero plugin and the plugin for vSphere (steps 4 and 5)
10. Restore the second management cluster, using the backup created in step 6. 
velero restore create mgmt1635 --from-backup mgmtwork
kubectl patch cluster restworker --type='merge' -p '{""spec"":{""paused"": false}}'

**What did you expect to happen:**
Expect the second cluster restored in step 10 to accept the changes and all resources in a healthy state. 

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
- `velero backup logs <backupname>`
- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`

The above commands from a lab environment of the recreation are here: https://gist.github.com/McCoyAle/1f999e4fe8c5de760a528d2beee6b25b

**Second environment where issue was replicated in a lab:**
backup and restore commands
https://gist.github.com/McCoyAle/58e6199c3a1478a64725803230169e8e

backup and restore logs
https://gist.github.com/McCoyAle/f2e35de1f71b610d6a061126c6904ad0
https://gist.github.com/McCoyAle/3186de00ba0b915587c304e54b327e81

**Anything else you would like to add:**
One thing I would like to include is that the backup in the lab to recreate this issue is in a partially failed state. 


**Environment:**

- Velero version (use `velero version`): 
- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3298,OPEN,velero-plugin-gcp uses the Google API incorrectly leading to failures/bugs in snapshot creation / restores,Area/Cloud/GCP; Needs investigation; Volumes,2021-01-25 20:22:42 +0000 UTC,arianitu,Opened,,"**What steps did you take and what happened:**
velero-plugin-gcp uses the Google API incorrectly leading to issues such as snapshots being created without errors when in fact they have not been created. 

The usage of the Google API inside velero-plugin-gcp does not check for the running operation to complete before returning a success.

See the bug report I made at https://github.com/googleapis/google-api-go-client/issues/832 where the authors show how to correctly use the API by using the `sv.operation` API to watch for operation to finish.

A good idea would be copying what gcp-csi driver does for watching the operation here:

https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/blob/95ffc203e9aabb8d11f3d7d1572d82ea5e5d3b71/pkg/gce-cloud-provider/compute/gce-compute.go#L680

```
func (cloud *CloudProvider) waitForZonalOp(ctx context.Context, opName string, zone string) error {
	// The v1 API can query for v1, alpha, or beta operations.
	svc := cloud.service
	project := cloud.project
	return wait.Poll(3*time.Second, 5*time.Minute, func() (bool, error) {
		pollOp, err := svc.ZoneOperations.Get(project, zone, opName).Context(ctx).Do()
		if err != nil {
			klog.Errorf(""WaitForOp(op: %s, zone: %#v) failed to poll the operation"", opName, zone)
			return false, err
		}
		done, err := opIsDone(pollOp)
		return done, err
	})
}
```


Here's an example of where it's done incorrectly when making a volume snapshot (at https://github.com/vmware-tanzu/velero-plugin-for-gcp/blob/main/velero-plugin-for-gcp/volume_snapshotter.go)

```
	gceSnap := compute.Snapshot{
		Name:        snapshotName,
		Description: getSnapshotTags(tags, disk.Description, b.log),
	}

	if b.snapshotLocation != """" {
		gceSnap.StorageLocations = []string{b.snapshotLocation}
	}

	_, err = b.gce.Disks.CreateSnapshot(b.snapshotProject, volumeAZ, volumeID, &gceSnap).Do()
	if err != nil {
		return """", errors.WithStack(err)
	}
```

This function assumes that CreateSnapshot succeeded, but in fact, it may have failed. In our case, we hit a Quota limit and this function succeeded.

**What did you expect to happen:**
velero-plugin-gcp should use  `svc.ZoneOperations.Get` to see if GCP API operations have in fact completed correctly or else the API could have returned a failure which leads to snapshot failures that look like successes. 


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,nrb,"
--
@arianitu Thanks; right now, Velero doesn't follow up on snapshot API calls due to the time taken to upload to object storage. Our AWS and Azure functionality is the same.

However, since this is a recurring issue, we'll take a look to see how long this might pause operations to wait.
--
",,,,,,,,,,
3285,OPEN,Support Velero plugin versioning,Area/Plugins; Enhancement/Dev,2021-02-16 17:34:10 +0000 UTC,nrb,In progress,,"**Describe the problem/challenge you have**
Right now, when Velero makes changes to its plugin APIs, there's no mechanism for the Velero server to communicate the version of the API that is supported, or for the plugins to communicate what version they implement.

This leads to restrictions on what can be changed in the API, or breaking everyone's code with major releases. Backwards-compatible changes either lead to API expansion, or implementations that have to check for the presence of specific functions, which is a runtime check and prone to error.


**Describe the solution you'd like**
Ideally, there would be a version negotiation system that allows Velero's server to support some level of version skew, decoupling Velero and plugins from having to move their API in lock step. This way, Velero core can support some number of API versions while iterating on new API features, and plugin authors are not forced to upgrade right away.

The [go-plugin](https://github.com/hashicorp/go-plugin) library we use has support for this, but I don't know how sophisticated their implementation is.


**Anything else you would like to add:**
 https://github.com/vmware-tanzu/velero/pull/1303 updated go-plugin to a version that supported plugin version negotiation, but I think there's still work to be done on top of that.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,nrb,"
--
@phuongatemc I know you're interested in this, and @sseago has some work that would greatly benefit from having this in place.
--
",,,,,,,,,,
3284,OPEN,Plugins `log.Debugln` does not print debug lines when in debug mode.,Needs info; Needs investigation,2021-03-26 08:46:08 +0000 UTC,arianitu,Opened,,"**What steps did you take and what happened:**
When debugging our plugin, we attempted to print logs like so:

```
b.log.Debugln(""Testing debug"")
```

These debug lines would not get printed out, even though the server is in debug mode. We can see that in the logs here:

```
time=""2021-01-15T16:39:34Z"" level=debug msg=""starting plugin"" args=""[/velero run-plugins --log-level debug]"" cmd=/velero logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/velero restore=velero/c6f6b8fc-ebd6-4300-bff9-10decebe213b
```

In this case, the `b.log` refers to:

```
type VolumeSnapshotter struct {
	log                     logrus.FieldLogger
```


**What did you expect to happen:**
`b.log.Debugln` should print out debug lines. We had to use `b.log.Info` to print them out.

**Environment:**

- Velero version (use `velero version`): 

```
Client:
	Version: v1.3.1
	Git commit: -
Server:
	Version: v1.3.1
```

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,carlisia,"
--
We'll take a look. In the mean time: does `b.log.Debug` work for you?
--
",nrb,"
--
@arianitu Have you tried with v1.4.0? I know we fixed issues with passing feature flags down, but we may need to be sure that the debug flag is passed down, too.
--
",,,,,,,,
3282,OPEN,"Velero 1.5.1 error=""backupstoragelocations.velero.io \""default\"" not found""",Needs info,2021-03-12 00:29:19 +0000 UTC,alexcpn,Opened,,"**What steps did you take and what happened:**

Installed  Velero 1.5.1 in a baremetal cluster with S3 hosted on Rook-ceph object storage

```
velero install  --provider aws  --plugins velero/velero-plugin-for-aws:v1.0.0  --bucket velero-bucket   --secret-file ./credentials-velero-k8sstorage  --use-volume-snapshots=true  --backup-location-config region=default,s3ForcePathStyle=""true"",s3Url=http://rook-s3.10.131.228.112.nip.io  --image velero/velero:v1.5.1   --snapshot-location-config region=""default""  --use-restic
```

Getting this error logs

```
time=""2021-01-13T14:04:49Z"" level=error msg=""Error patching backup location's last-synced time"" backupLocation=default controller=backup-sync error=""backupstoragelocations.velero.io \""default\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/backup_sync_controller.go:311"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*backupSyncController).run"" logSource=""pkg/controller/backup_sync_controller.go:311""
time=""2021-01-13T14:04:58Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-01-13T14:04:58Z"" level=error msg=""Error updating backup location phase"" backupstoragelocation=default controller=backupstoragelocation error=""backupstoragelocations.velero.io \""default\"" not found"" logSource=""pkg/controller/backupstoragelocation_controller.go:115""
time=""2021-01-13T14:05:20Z"" level=error msg=""Error patching backup location's last-synced time"" backupLocation=default controller=backup-sync error=""backupstoragelocations.velero.io \""default\"" not found"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/backup_sync_controller.go:311"" error.function=""github.com/vmware-tanzu/velero/pkg/controller.(*backupSyncController).run"" logSource=""pkg/controller/backup_sync_controller.go:311
```
Try to take backup - It does not complete in time
```
 velero backup create nginx-test --include-namespaces test-nginx --wait
Backup request ""nginx-test"" submitted successfully.
Waiting for backup to complete. You may safely press ctrl-c to stop waiting - your backup will continue in the background.
............................................................................................................................................^C
````

```
 velero backup-location get
NAME      PROVIDER   BUCKET/PREFIX   PHASE     LAST VALIDATED   ACCESS MODE
default   aws        velero-bucket   Unknown   Unknown          ReadWrite
```

In the same testbed , deleted Velero namespace and installed v1.4.0 version

```
velero install  --provider aws  --plugins velero/velero-plugin-for-aws:v1.0.0  --bucket velero-bucket   --secret-file ./credentials-velero-k8sstorage  --use-volume-snapshots=true  --backup-location-config region=default,s3ForcePathStyle=""true"",s3Url=http://rook-s3.10.131.228.112.nip.io  --image velero/velero:v1.4.0   --snapshot-location-config region=""default""  --use-restic
```

Took backup, everything works; No change between these two related to anything - S3 URL, access keys etc; Only version change

```
velero backup create nginx-test --include-namespaces test-nginx --wait
Backup request ""nginx-test"" submitted successfully.
Waiting for backup to complete. You may safely press ctrl-c to stop waiting - your backup will continue in the background.

```
Backup completed with status: Completed. You may check for more information using the commands `velero backup describe nginx-test` and `velero backup logs nginx-test`.

```
alex@N-20HEPF0ZU9PR:/mnt/c/Users/acp/Documents/certs3/ee$ velero backup-location get
NAME      PROVIDER   BUCKET/PREFIX   PHASE     LAST VALIDATED   ACCESS MODE
default   aws        velero-bucket   Unknown   Unknown          ReadWrite


velero get backup
NAME         STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
nginx-test   Completed   0        0          2021-01-15 15:36:17 +0530 DST   29d       default            <none>

Backup is proper in S3 
$ mc ls  k8sstorage/velero-bucket/backups
[2021-01-15 15:46:08 DST]     0B nginx-test/
```
**Expected behaviour **

Velero installation with v1.5.1  should work
",,,rflorenc,"
--
I am seeing the same on `v1.5.2`.

```
time=""2021-01-18T11:34:19Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2021-01-18T11:34:19Z"" level=error msg=""Error updating backup location phase"" backupstoragelocation=default controller=backupstoragelocation error=""backupstoragelocations.velero.io \""default\"" not found"" logSource=""pkg/controller/backupstoragelocation_controller.go:115""
```

The same BSL configuration works fine on v1.4.0.

```
- apiVersion: velero.io/v1
  kind: BackupStorageLocation
  metadata:
    creationTimestamp: ""2021-01-18T11:33:55Z""
    generation: 1
    labels:
      component: velero
    managedFields:
    - apiVersion: velero.io/v1
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:labels:
            .: {}
            f:component: {}
        f:spec:
          .: {}
          f:config:
            .: {}
            f:insecureSkipTLSVerify: {}
            f:region: {}
            f:s3ForcePathStyle: {}
            f:s3Url: {}
          f:objectStorage:
            .: {}
            f:bucket: {}
          f:provider: {}
      manager: OpenAPI-Generator
      operation: Update
      time: ""2021-01-18T11:33:55Z""
    name: default
    namespace: velero
    resourceVersion: ""142838293""
    selfLink: /apis/velero.io/v1/namespaces/velero/backupstoragelocations/default
    uid: e5c05c5d-7e8d-4652-95e9-7f8d7c55eef3
  spec:
    config:
      insecureSkipTLSVerify: ""true""
      region: minio
      s3ForcePathStyle: ""true""
      s3Url: https://s3-noobaa.apps.example.domain.net
    objectStorage:
      bucket: first.bucket
    provider: aws
```



--
",zubron,"
--
Hi! This looks like it is due to incorrect versions of the CRDs on your cluster. Given that you had no issues with 1.4.0, I'm going to assume that you either had a previous installation of 1.4 on the cluster, or are using a 1.4 version of the CLI. You can check this by running `velero version`.

In order to use 1.5, you will need to follow our instructions here: https://velero.io/docs/v1.5/upgrade-to-1.5/
particularly ensuring that you are using a [1.5 version of the CLI](https://github.com/vmware-tanzu/velero/releases/tag/v1.5.3) and then performing the instructions to upgrade the CRDs in the cluster.
--

--
Hi @Reinkaos - to confirm, were you using a 1.5 version of the CLI to reinstall the CRDs?
--
",Reinkaos,"
--
Hey, we are experiencing the same issue. Downgrading to 1.4.3 fixes the problem. We have tried reinstalling the CRDs as described in the link but still no luck. 
--

--
Hey @zubron, yes we are using 1.5.2.
--
",supasteev0,"
--
FYI, I'm also seing the following logs after upgrading to 1.5.2: 
```
time=""2021-01-19T07:06:25Z"" level=error msg=""Current backup storage locations available/unavailable/unknown: 0/0/1)"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:154""
time=""2021-01-19T07:06:25Z"" level=info msg=""No backup locations were ready to be verified"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:120""
time=""2021-01-19T07:06:25Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
```
Using AWS S3 bucket for BackupStorageLocation with this config: 
```
spec:
  backupSyncPeriod: 0s
  config:
    region: eu-west-1
  objectStorage:
    bucket: mybucket
    prefix: staging
  provider: aws
  validationFrequency: 0s
  ```
  
I updated the CRD with velero 1.5.2 CLI.
--
",giordyb,"
--
I have the same issue with 1.5.3 installed from scratch, although the backup seems to be running fine.
I'm using the aws provider with minio.


--

--
@nrb thank you for the quick reply.

I was following this issue because I was getting the same error as in the title of the issue.
After reinstalling velero from scratch that specific message went away but I kept seeing the same messages as @supasteev0 mentioned (Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available) every minute so I figured it might be related.

If you are saying they are expected then it's fine, sorry if I created some confusion... 

--
",nrb,"
--
@supasteev0 Your logs don't look like an error - those are expected log messages.

@giordyb Can you be more specific on which issue you're having? I think folks are reporting multiple issues here.

@alexcpn and @rflorenc - can you provide the output of `kubectl get crd/backupstoragelocation.velero.io -o yaml` - we should see a `Status.Phase` field in it. Otherwise, the CRD needs to be upgraded.

See step 2 in https://velero.io/docs/v1.5/upgrade-to-1.5/#docs
--
"
3278,OPEN,[Feature Request]: Add the feature in Velero to take the backup of S3(Object storage) buckets,Enhancement/User; Needs Product; P3 - Wouldn't it be nice if...,2021-03-12 00:23:27 +0000 UTC,rahulgit1988,In progress,,"**Describe the problem/challenge you have**
Other than AWS S3, the object storage bucket data is not safe, there should be a way in Velero to take the backup of those S3 bucket which are used as PV storage options in K8s enviorment.


**Describe the solution you'd like**
There should be some hook or API kind of facility like CSI snapshot kicking can be implmented, for example kick the S3 CLI from API and copy the bucket data to another S3 or at safe place.



**Anything else you would like to add:**
S3 backup way need to implement


**Environment:**

- Velero version (use `velero version`):1.5.1
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,nrb,"
--
@rahulgit1988 Are you asking about the S3 storage that Velero uses, or for any S3 storage that is linked to the cluster?
--
",rahulgit1988,"
--
> @rahulgit1988 Are you asking about the S3 storage that Velero uses, or for any S3 storage that is linked to the cluster?

Velero to perform the backup of any S3 storage that is linked to the cluster
--
",dsu,"
--
This would be something that should be integrated with COSI https://pkg.go.dev/github.com/kubernetes-sigs/container-object-storage-interface-spec.  We will look at this as a potential type for Astrolabe (https://github.com/vmware-tanzu/astrolabe)
--
",,,,,,
3276,OPEN,Backup fails for PVC with storage provisioner `file.csi.azure.com`,Needs info; Needs investigation,2021-03-12 00:22:44 +0000 UTC,vaishali-prophecy,Opened,,"I followed the steps described here to create backup of azure disks with CSI enabled - https://gist.github.com/ashish-amarnath/b4162d2836a63516540bfa1d24783120

Backup and restore worked perfectly fine for me. Then in this example, I changed PVC storage class to azurefile-csi whose provisioner is 'file.csi.azure.com' and created new volumesnapshotclass corresponding to this provisioner. Creating backup fails with the error - 

`time=""2021-01-14T16:58:17Z"" level=info msg=""Waiting for volumesnapshotcontents snapcontent-982200b8-442b-4748-8d95-edc67502424c to have snapshot handle. Retrying in 5s"" backup=velero/csi-b3 cmd=/plugins/velero-plugin-for-csi logSource=""/go/src/velero-plugin-for-csi/internal/util/util.go:179"" pluginName=velero-plugin-for-csi
time=""2021-01-14T16:58:17Z"" level=info msg=""Waiting for volumesnapshotcontents snapcontent-982200b8-442b-4748-8d95-edc67502424c to have snapshot handle. Retrying in 5s"" backup=velero/csi-b3 cmd=/plugins/velero-plugin-for-csi logSource=""/go/src/velero-plugin-for-csi/internal/util/util.go:179"" pluginName=velero-plugin-for-csi
time=""2021-01-14T16:58:17Z"" level=error msg=""Timed out awaiting reconciliation of volumesnapshot csi-app/velero-pvc-azuredisk-kcp9f"" backup=velero/csi-b3 cmd=/plugins/velero-plugin-for-csi logSource=""/go/src/velero-plugin-for-csi/internal/util/util.go:188"" pluginName=velero-plugin-for-csi
time=""2021-01-14T16:58:17Z"" level=info msg=""1 errors encountered backup up item"" backup=velero/csi-b3 logSource=""pkg/backup/backup.go:451"" name=csi-nginx
time=""2021-01-14T16:58:17Z"" level=error msg=""Error backing up item"" backup=velero/csi-b3 error=""error executing custom action (groupResource=volumesnapshots.snapshot.storage.k8s.io, namespace=csi-app, name=velero-pvc-azuredisk-kcp9f): rpc error: code = Unknown desc = timed out waiting for the condition"" logSource=""pkg/backup/backup.go:455"" name=csi-nginx
time=""2021-01-14T16:58:17Z"" level=info msg=""Backed up 4 items out of an estimated total of 20 (estimate will change throughout the backup)"" backup=velero/csi-b3 logSource=""pkg/backup/backup.go:418"" name=csi-nginx namespace=csi-app progress= resource=pods`",,,vaishali,"
--
@ashish-amarnath  Can you please help in resolving this?
--
",nrb,"
--
@vaishali-prophecy Do you know if `files.csi.azure.com` has CSI snapshotting capabilities? If not, you'd have to use restic.

Based on the error we never get an actual CSI snapshot generated for Velero to use.
--
",dsu,"
--
According to this (https://docs.microsoft.com/en-us/azure/aks/azure-files-csi) it looks like Azure Files supports CSI snapshots
--
",,,,,,
3264,OPEN,If you are seeking support; or have a question; or something is not working: please read this,Enhancement/Dev,2021-01-25 17:54:10 +0000 UTC,carlisia,Opened,,"Hello everyone, thanks for stopping by to try and resolve your Velero issues.

We ask that you add your question to our Q&A Discussion forum here:

https://github.com/vmware-tanzu/velero/discussions/categories/community-support-q-a

Going forward, support questions added as issues will be converted to our Q&A Discussion forum.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
3239,OPEN,Backup would always be phase of “InProgress” if velero failed to patchBackup,Bug; P3 - Wouldn't it be nice if...,2021-03-11 18:30:19 +0000 UTC,Lighting-wings,Opened,,"velero version : 1.5.2

The function `processBackup` in `backup_controller` just log the error and return nil at the end. Sobackup would always be phase of “InProgress” if velero failed to patchBackup. Maybe there need enhancement about the consistency of backup. Of course, it's just  my personal thought. Do you think about it ?

![image](https://user-images.githubusercontent.com/24741946/104423598-42070880-55b9-11eb-86f4-5925f2400438.png)
",,,dsu,"
--
If the patch fails it probably means we've got some kind of trouble talking to the API server and there's not much we can do to persist the state.  We should be failing InProgress backups on a server restart and there's probably something else we can do to cleanup without a restart.  We'll log this as a bug, thanks.
--
",,,,,,,,,,
3238,OPEN,Why don't turn up logging verbosity when backuping additionalItemIdentifiers generated from plugins ?,Bug; Enhancement/User,2021-03-11 18:27:21 +0000 UTC,Lighting-wings,Opened,,"velero version : 1.5.2
Hello, I found that logging verbosity is `warn` when velero failed to backup additionalItemIdentifiers and the error is `NotFound` .The function is `executeActions` in item_backupper.go. 

![image](https://user-images.githubusercontent.com/24741946/104421467-572e6800-55b6-11eb-9b80-7423ea047128.png)

I think it maybe leave some resource that are important for restore and mislead users that the backup is `completed`. Do you think it is better to log error for this ? Then the backup is `PartiallyFailed`. 
",,,,,,,,,,,,,,
3236,OPEN,All backup can't be handled if you set wrong format about ttl using kubectl,Enhancement/User,2021-01-14 01:23:27 +0000 UTC,Lighting-wings,Opened,,"velero version：1.5.2

I used the command `kubectl apply -f` to create a backup named test01. And I made a mistake when I set ttl. The ttl was set `1h0m0s0`. Then the phase of the backup is always `InProgress`. I tried to use velero client to create another backup named test02. Unfortunately, the second backup can't be handled and it didn't even have the field of phase. I tried third backup and so on. The result was all backup can't be handled.

I think the reason is that velero uses `kubebuilder` to create the yaml file of crds. The type of ttl is `metav1.Duration` in `golanguage` struct. But it is `string` in generated yaml file. 

![image](https://user-images.githubusercontent.com/24741946/104417850-02d4b980-55b1-11eb-8dd7-f33f4fc3d592.png)

![image](https://user-images.githubusercontent.com/24741946/104417914-17b14d00-55b1-11eb-8af3-0007e3c5aa5e.png)

So kubernetes will accept all expression as ttl because it is `string`. However, the informer of k8s tried to unmarshal it as `metav1.Duration` in node's cache. The process of backup will be blocked there because informer tries to unmarshal it all the time.

Do you think velero needs webhooks to validate those problems like this ?

",,,carlisia,"
--
Hi @Lighting-wings, thanks for reporting this. Yes, we should have a validation, we'll look into adding that.

Just to let you know about your other issue: Velero processes backups sequentially (we are working on changing this), so while a backup is `InProgress`, no other backup created after will be processed.
--
",Lighting,"
--
Thanks for replying. I'm sorry for that there is a mistake about my description. The first backup which I set wrong format about ttl is not in the phase of `InProgress`. The phase of it is nothing just like subsequent backups. 
--
",,,,,,,,
3229,OPEN,Add a new data movement layer to Velero's architecture (Velero 2.0 Epic),Enhancement/User; Needs Product; Product Requirements; Size/XXXL; Volumes,2021-01-11 20:46:43 +0000 UTC,nrb,Opened,,"**Describe the problem/challenge you have**

Velero has restic integration as part of it's core, which is useful for moving filesystem level data between storage providers or when a Velero volume snapshot plugin isn't available for a storage platform. However, since the restic integration is part of Velero's core, this is also challenging should someone like to use an alternative solution such as Borg, Duplicity, or Kopia.


**Describe the solution you'd like**

Velero should provide a more abstract, swappable layer for allowing filesystem data transfer out of persistent volume. This should abstract the following things:

 * the tool used to send the data (for example, replace restic with some other executable)
 * the destination for filesystem data (instead of being forced to use restic repositories, allow using something like Borg scp endpoints)



**Anything else you would like to add:**


An implementation of a data mover already exists in the vSphere plugin. It can be used as a reference for bringing this into Velero.

See issues #3223 and #3226 for requests to add replacement tools to Velero.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3222,OPEN,"Velero will crash if your backup's excludeNamespaces is ""**""",Bug; Help wanted,2021-01-13 18:31:48 +0000 UTC,Lighting-wings,Opened,,"Version is v1.5.2.
I noticed that velero supports regular expression when you set `includeNamespaces` and `excludeNamespaces ` of bakcup from code. And then velero will happen to panic if a backup's excludeNamespaces is ""\*\*"" and `includeNamespaces` is empty. I think this is serious.
The reson is that the function `ValidateIncludesExcludes` use normal way to check these settings and ""**"" will escape from it. And then the function `getNamespacesToList` will return empty slice. Velero handle namespace first in order to resolve particular case. There is the location where velero will happen to panic.

The file is item_collector.go
![image](https://user-images.githubusercontent.com/24741946/103598434-8f420500-4f3d-11eb-9132-38502901b797.png)

Is that neccessary  to use regular expression in code ? I have doubt with it because velero can't handle it when the setting of `includeNamespaces` is like `default* ` and so on. Velero will log error when handle backup like that.


",,,zubron,"
--
Hi @Lighting-wings. Thanks for reporting this!

Velero doesn't currently wildcard matching for namespaces (see https://github.com/vmware-tanzu/velero/issues/1874). With the current implementation it expects either a list of complete namespaces or a single `*` to indicate that it should include all namespaces. If you specify an excludes list with `*` it will fail validation. It definitely shouldn't be panicking with `**` as the input, so we need to add more validation of the arguments here.
--
",Lighting,"
--
So is there a plan to enhance the ability of wildcard ? In fact, I don't think wildcard matching is neccessary for velero. Maybe velero could abandon this feature in order to higher reliability.
--

--
Hello,it seems that the process of backup doesn't adapt to wildcard matching so far, especially execute the backup of namespace. When can this feature be supported if there is a plan about it? Or would this feature be considered to quit? Thank you for your reply.
@zubron 
--
",,,,,,,,
3220,OPEN,Should backup deletion proceed if there is an error getting the backup store?,Enhancement/User,2021-01-13 22:52:04 +0000 UTC,zubron,Opened,,"> Why doesn't velero return error when it can't get `BackStore` in `backupdeletioncontroller` ? I think it will cause panic because of nil pointer. By the way, I met this one time when I restarted velero and offered wrong `sk`. Remaining `deletebackuprequest` is processed by the new velero container.

_Originally posted by @Lighting-wings in https://github.com/vmware-tanzu/velero/issues/3206_

In [`backupDeletionController.processRequest`](https://github.com/vmware-tanzu/velero/blob/main/pkg/controller/backup_deletion_controller.go#L293-L296), if an error is returned from `newBackupStore`, we append it to the slice of errors rather than returning it. In this case, the returned `backupStore` could be `nil` and we use it [later in the function](https://github.com/vmware-tanzu/velero/blob/main/pkg/controller/backup_deletion_controller.go#L307). We do check that it's not nil before a different use.

We should either allow it to be nil and check for it in both uses, or return from the function early if there was an error getting the backup store. In the case where we allow it to be nil, we should add some additional logging to state what the impact of not being able to get the backup is.",,,,,,,,,,,,,,
3218,OPEN,S3 client-side encryption support,Enhancement/User; Needs Product,2021-01-13 18:36:06 +0000 UTC,invidian,Opened,,"**Describe the problem/challenge you have**
In addition to server-side encryption requested in #1782, it would be great to also support client-side encryption with client-provided master key.

This would allow using not fully trusted Minio instance for example. As far as I know, right now data is encrypted using static key which does not add any protection.

**Describe the solution you'd like**
Add support for specifying client-side encryption master key for S3 backup storage location.

**Anything else you would like to add:**

S3 documentation: https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html

I guess this could be workaround using some sort of S3 proxy, which would handle the encryption independent from Velero.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3210,OPEN,Display which plugins are mandatory and can't be removed,Area/CLI; Enhancement/User,2021-01-04 20:16:45 +0000 UTC,jenting,Opened,,"**Describe the problem/challenge you have**

```
$ velero plugin get
NAME                                                          KIND
velero.io/crd-remap-version                                   BackupItemAction
velero.io/pod                                                 BackupItemAction
velero.io/pv                                                  BackupItemAction
velero.io/service-account                                     BackupItemAction
velero.io/aws                                                 ObjectStore
velero.io/add-pv-from-pvc                                     RestoreItemAction
velero.io/add-pvc-from-pod                                    RestoreItemAction
velero.io/change-pvc-node-selector                            RestoreItemAction
velero.io/change-storage-class                                RestoreItemAction
velero.io/cluster-role-bindings                               RestoreItemAction
velero.io/crd-preserve-fields                                 RestoreItemAction
velero.io/init-restore-hook                                   RestoreItemAction
velero.io/job                                                 RestoreItemAction
velero.io/pod                                                 RestoreItemAction
velero.io/restic                                              RestoreItemAction
velero.io/role-bindings                                       RestoreItemAction
velero.io/service                                             RestoreItemAction
velero.io/service-account                                     RestoreItemAction
velero.io/aws                                                 VolumeSnapshotter
```

```
$ velero plugin remove velero.io/service-account
An error occurred: init container velero.io/service-account not found in Velero server deployment

$ velero plugin remove velero.io/aws
An error occurred: init container velero.io/aws not found in Velero server deployment
```

**Describe the solution you'd like**

The `velero plugin remove` command can't remove the system default plugins, it'd be good the UI displays which plugins are mandatory and can't be removed.
Furthermore, it'd be good to make the `velero plugin remove velero.io/aws` work instead `velero plugin remove velero/velero-plugin-for-aws:v1.1.0`.

**Anything else you would like to add:**

N/A

**Environment:**

- Velero version (use `velero version`): v1.5.2
- Kubernetes version (use `kubectl version`): v1.19.1
- Kubernetes installer & version: kind
- Cloud provider or hardware configuration: N/A
- OS (e.g. from `/etc/os-release`): Ubuntu

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3204,OPEN,Cleanup residual temporary files,Good first issue,2021-01-08 13:16:41 +0000 UTC,Lighting-wings,Opened,,"Hello, I noticed changes about deletebackupcontroller. The function closeAndRemoveFile is put in front of the function return.
![image](https://user-images.githubusercontent.com/24741946/103073326-68610600-4602-11eb-9853-d06656e34cb4.png)

But I don't think this way can be enough. Because the function  downloadToTempFile is used in two places，restorecontroller and deletebackuprequestcontroller.  I found that the function downloadToTempFile can't return error in time if network interruption occurs when we try to download temporary file. The reason is that velero just sendMsg but not recevieMsg when it invokes plugin's function getObject.
![image](https://user-images.githubusercontent.com/24741946/103073932-782d1a00-4603-11eb-89e0-d6c3d1410cc6.png)
 
We can't know what has been happened until try to copy data from stream. At this time temporary file has been created and maybe received some data. So why not try to delete temporary file when an error occurs during copying. There is no need to judge if the file is nil pointer in closeAndRemoveFile function.
![image](https://user-images.githubusercontent.com/24741946/103074315-4ff1eb00-4604-11eb-86cd-c44f6952803f.png)

That's my personal idea. Please correct it if there are some problems. Thank you. ",,,Lighting,"
--
@ncdc 
--

--
Thanks a lot. I have some new ideas with it and will open a PR recently. Would you like to discuss with me when I pull a PR ? 
--
",nevermore,"
--
Same with me. Maybe velero needs enhance the feature. 
--
",neujie,"
--
yeah，I think it’s better to put it in downloadToTempFile.
--
",zubron,"
--
For reference, the code in the BackupDeletionController and the change to check if the file is `nil` in `closeAndRemoveFile` were made in #2993.

You're right in that we don't know about certain errors until we try to copy from the stream and was discussed in https://github.com/vmware-tanzu/velero/pull/2993#discussion_r518118234. We could clean up the file in `downloadToTempFile` in the case where an error occurred, however, we will still need to clean up the file in both the BackupDeletionController and in the RestoreController in the case where the download was successful. It was discussed (https://github.com/vmware-tanzu/velero/pull/2993#discussion_r527268051) and decided that we would leave the call to `closeAndRemoveFile` in the outer scope and instead change the function to handle when a nil file pointer is provided.

If you would like to open a PR to update `downloadToTempFile` in the case where an error occurs, that would be great. Thanks!
--

--
Great - thanks for taking this on! I will assign this issue to you.

When submitting a PR, the maintainers of the project will be assigned to review it so we will take a look once it has been created. Please see our documentation on [getting started with contributing](https://velero.io/docs/v1.5/start-contributing/) and our [code standards](https://velero.io/docs/v1.5/code-standards/) which includes more details of the requirements when creating PRs. Thanks! :smile: 
--
",,,,
3194,OPEN,Have the Velero CLI use the active namespace in the current context,Area/CLI; Enhancement/User; Good first issue; Help wanted,2021-02-09 17:37:07 +0000 UTC,carlisia,In progress,,"I thought we had an issue for this but couldn't find it.

Currently, the Velero CLI uses the `velero` namespace in the current context, unless otherwise specified. This means that if you are operating under a namespace other than `velero`, you have to specify the namespace with every single command.

As it is common with other tools, invert things and make it so Velero recognizes and uses the active namespace, unless otherwise specified.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,nrb,"
--
You can use `velero client config set namespace <namespace>`, but this doesn't tie it to a Kubeconfig context currently.

We could perhaps expand the config to take a context -> namespace mapping.
--

--
Yep, I agree with @carlisia!
--

--
I will link this to our multitenancy epic, as there are assumptions for that tied up in this issue.
--
",shawn,"
--
Hello all,

I think I am interested in taking this issue on if no one is working on it? 

--

--
I just want to make sure that I am doing the right thing here because I am wondering if this is breaking change.

This means that unless otherwise specified we will use the context name, and will never use the `velero` namespace by default or the namespace defined in `VELERO_NAMESPACE` env var?

--

--
I have often found that for `velero backup` / `velero restore` commands, I wish it understood my context.

The velero install is an interesting case, though.

I wonder if we should talk through this in a community meeting? 
--
",carlisia,"
--
It's yours! Thank you. Just make sure that users can override that with the existing global flag `--namespace`.
--

--
Ok, great question. This is what I think the order of precedence should be:

- use the namespace given in the command line 
- if the above is not passed in, use the env var
- if not those, use whatever namespace is in context

I'd like to get a sanity check from @nrb.
--
",dsu,"
--
So if I don't do anything my active namespace is ""default"", right?

If I then do ""velero install"" without specifying a namespace, where will it go?  ""default"" or ""velero""?

Afterwards, if I don't set an active namespace, how will this work?  If I'm doing something in another namespace and then want to do a backup using Velero, I have to either switch namespaces or specify the velero namespace on the command line?

This doesn't sound right.  My active namespace is where I want my kubectl commands to go to.  The velero CLI is a different app, it shouldn't be going to the same place my kubectl commands go, because I'm not using kubectl, in all likelihood I'm working on a namespace that doesn't have anything to do with Velero and then I want to back something up.

Am I missing something here?
--

--
I put this on the Agenda for Feb 9, if that date doesn't work, please modify the agenda and put it when it's convenient, thanks!
--
",,,,
3191,OPEN,Velero : Throttling request errors,Enhancement/User; P3 - Wouldn't it be nice if...,2021-03-10 02:25:27 +0000 UTC,srajput1991,Opened,,"[carlisia] Update on 1/13: this became a task to update our server defaults.

--------
Hi,

I am using velero to take backup of my k8s resources. I am seeing a lot errors in datadog from velero as below:

    1 request.go:621] **Throttling request took** 1.047035991s, request: GET:https://10.0.0.1:443/apis/autoscaling/v2beta2?timeout=32s",,,zubron,"
--
Hi @srajput1991 - Apologies for the delay in getting back to you on this.

These messages are coming from [`client-go`](https://github.com/kubernetes/client-go/blob/v0.18.4/rest/request.go#L560-L562). After looking through some [comments on the Kubernetes slack](https://kubernetes.slack.com/archives/C76GB48RK/p1606770235088500), it appears to be affecting other projects. It's not clear if it's an issue with the API server being slow to respond and so the client reports that the request took over the maximum time (1s) or whether this is an indication to the client to slow down the rate of requests.

My understanding is that this shouldn't impact the functionality of Velero. We should investigate though to see if there are better default settings for `QPS` and `Burst` that we could be using in the config for the client.

See the following settings for the velero server:
```
--client-burst int      Maximum number of requests by the server to the Kubernetes API in a short period of time. (default 30)
--client-qps float32    Maximum number of requests per second by the server to the Kubernetes API once the burst limit has been reached. (default 20)
```
--
",nrb,"
--
After doing some digging in the code, the underlying struct used is [this Golang rate limiter](https://godoc.org/golang.org/x/time/rate#Limiter) and its [`Wait`](https://godoc.org/golang.org/x/time/rate#Limiter.Wait) method.

Here's the key sentence:

> If no token is available, Wait blocks until one [token] can be obtained or its associated context.Context is canceled. 

So what I believe is happening is that the `Wait` call is blocking, waiting for a token to be re-added to the bucket, and logging these messages when it takes too long (according to the client-go code, over 1 second).

As @zubron said, the size of the bucket can be increased with the `--client-burst` value (default 30), and the rate of refilling tokens can be changed with `--client-qps` (default 20). That said, I think we should look at altering the defaults here, because we added more controllers with more things to watch.

I don't have a good handle on what values to set at the moment. I'm going to do some experimenting, but I think moving up to 50 burst/40 QPS would be a good start.
--

--
Looking around, [Prometheus uses 100/100](https://github.com/prometheus-operator/prometheus-operator/blob/master/pkg/k8sutil/k8sutil.go#L91-L92) and [ingress-nginx used to use 1,000,000 for both](https://github.com/kubernetes/ingress-nginx/blob/e4d53786e771cc6bdd55f180674b79f5b692e552/pkg/ingress/controller/launch.go#L252-L259), though it appears they've since removed any default values.

We don't want Velero to spam the API server, so I think a value of 1,000,000 is too much, but moving up into the order of 100 or so is a reasonable start.
--

--
@srajput1991 the following values made the throttling messages go away in a test cluster. Highlighted inline.

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: ""3""
  creationTimestamp: ""2021-01-11T23:19:08Z""
  generation: 3
  labels:
    component: velero
  name: velero
  namespace: velero
  resourceVersion: ""3678""
  selfLink: /apis/apps/v1/namespaces/velero/deployments/velero
  uid: 2ccad69e-f561-4497-98d8-9b4604073580
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      deploy: velero
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: ""8085""
        prometheus.io/scrape: ""true""
      creationTimestamp: null
      labels:
        component: velero
        deploy: velero
    spec:
      containers:
      - args:
        - server
        - --client-qps=75.0 <----- HERE
        - --client-burst=100 <----- HERE
        - --features=
        command:
        - /velero
        env:
        - name: VELERO_SCRATCH_DIR
          value: /scratch
        - name: VELERO_NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        - name: LD_LIBRARY_PATH
          value: /plugins
        - name: GOOGLE_APPLICATION_CREDENTIALS
          value: /credentials/cloud
        - name: AWS_SHARED_CREDENTIALS_FILE
          value: /credentials/cloud
        - name: AZURE_CREDENTIALS_FILE
          value: /credentials/cloud
        - name: ALIBABA_CLOUD_CREDENTIALS_FILE
          value: /credentials/cloud
        image: velero/velero:main
        imagePullPolicy: IfNotPresent
        name: velero
        ports:
        - containerPort: 8085
          name: metrics
          protocol: TCP
        resources:
          limits:
            cpu: ""1""
            memory: 256Mi
          requests:
            cpu: 500m
            memory: 128Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /plugins
          name: plugins
        - mountPath: /scratch
          name: scratch
        - mountPath: /credentials
          name: cloud-credentials
      dnsPolicy: ClusterFirst
      initContainers:
      - image: velero/velero-plugin-for-gcp:v1.1.0
        imagePullPolicy: IfNotPresent
        name: velero-plugin-for-gcp
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /target
          name: plugins
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: velero
      serviceAccountName: velero
      terminationGracePeriodSeconds: 30
      volumes:
      - emptyDir: {}
        name: plugins
      - emptyDir: {}
        name: scratch
      - name: cloud-credentials
        secret:
          defaultMode: 420
          secretName: cloud-credentials
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: ""2021-01-11T23:19:26Z""
    lastUpdateTime: ""2021-01-11T23:19:26Z""
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: ""True""
    type: Available
  - lastTransitionTime: ""2021-01-11T23:19:08Z""
    lastUpdateTime: ""2021-01-11T23:21:40Z""
    message: ReplicaSet ""velero-6d9c7fc787"" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: ""True""
    type: Progressing
  observedGeneration: 3
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
```

This is something that you can change in your own deployment, though I think we'll get the defaults updated within Velero's code, too.
--
",tareqhs,"
--
Running into this issue too. I've created https://github.com/vmware-tanzu/helm-charts/pull/222 to allow each user to customize the client's qps and burst rate from chart values conveniently.
--
",,,,,,
3173,OPEN,Reconciling the server default BSL with the user configured default BSL,Area/CLI,2021-03-12 22:13:46 +0000 UTC,carlisia,In progress,,"I wanted to surface this and get a sanity check if this has been addressed best we can, or if there's still a use case we need to cover. This is in regards to PR https://github.com/vmware-tanzu/velero/pull/3092. More specifically, this is in regards to how we pick the default BSL to use when the Velero server starts up with its configured default BSL when at the same time there is  a BSL marked as default (in the CRD).

For context/recall:

> > It means that every time the velero pod start/restart, it might change the default BSL setting.

> Yeah, that's a very good point. That's definitely not something we want, because that will send data to the wrong place based on other configuration changes. Perhaps the server-level option sets the default BSL once and only once, for migration purposes, then it doesn't work anymore?
Source: https://github.com/vmware-tanzu/velero/pull/3092#discussion_r534424990

> I like this functionality a lot, but I would like to see a follow up PR that makes the --default-storage-location argument on velero server set the Default field.
Source: https://github.com/vmware-tanzu/velero/pull/3092#pullrequestreview-543169661

Currently, this is how we pick the default BSL to use:

1) Did the user specify the default at backup creation time? If so, use that.

2) Is there a BSL marked as default (in the CRD)? Either the user did that manually or the BSL is from a new installation that automatically configured that first BSL as default. If so, use that (refs: https://github.com/vmware-tanzu/velero/blob/844cc16803c010635f2faa030ce882e8ccb3adbb/pkg/controller/backup_storage_location_controller.go#L72 and pending PR to automatically make BSL the default on install: https://github.com/vmware-tanzu/velero/pull/3172/files#diff-ff4ac83c63c469932ea321327ca5368dbf304c8ea2affe85539d9225aea93247R163)

3) If none of the above cases are true, use the server configuration to set a BSL as the default, IF AND ONLY IF there exists a BSL with the same name. If no BSL with that name exists, nothing can be done, except warn. Refs: https://github.com/vmware-tanzu/velero/blob/844cc16803c010635f2faa030ce882e8ccb3adbb/pkg/controller/backup_storage_location_controller.go#L84 and pending PR (with better warning): https://github.com/vmware-tanzu/velero/pull/3172/files#diff-392430c1e526e2d5a98e4a69deb90350eabcb7a77e58e2f967b4c806568dda2aR173.

4) If none of the above cases are possible, either there is no existing BSL, or none of the existing BSL is marked as ""default"". Likely these are legacy BSLs. In this case, the BSL controller will continue to throw the warning (see right above), and the failed backup will contain the error message explaining why and how to fix it. Ref: see pending PR for better error message: https://github.com/vmware-tanzu/velero/pull/3172/files#diff-f0334755c17add3306aecd57a34bc24dfa20a66f617dbaf541042dfd8e0887deR380. 

❓ This is the question to answer:

**Do we want the user configured default BSL to take precedence over the server configured default?**

If so, this is handled. This is my preference, especially since the server config will be deprecated.

If not, we need to invert the existing logic to give preference to the server configuration.

Let me know if I missed anything.

c/c @nrb and @jenting 








",,,jenting,"
--
If we want to cover both user-configured and server-configured at the same time, the only way I could though now is to have a new field in BSL CRD like `.spec.serverDefaultBSLName` to indicates the server-side default BSL name.

Once the Velero pod restart, it compare the `.spec.serverDefaultBSLName` equals to current server-side BSL name.
If mismatch, use the server-side BSL name as the default BSL and updates the `.spec.serverDefaultBSLName`.
--

--
> But this would not translate into covering both at the same time. This would mean giving precedence to the server config over the user config. In this proposed way, it would also break the principle of ""do the least surprising thing"", since the user would probably be surprised something they intentionally set was changed to something else they might not even be aware of (the server config, which will always have a default value of ""default"").

Let me recap the scenario I've thought, for example, we have 2 BSLs, one is `default` and the other one is `secondary`.

1a. During the fresh install, the velero CLI configures a BSL named ""default"" as the default BSL. (`.spec.serverDefaultBSLName: default`)
1a. During the upgrade, the BSL controller sets the defaulted BSL according to the BSL name ""default"". (`.spec.serverDefaultBSLName: default`)
2. The user changes the defaulted BSL to `secondary` by Velero CLI, so the configuration `.spec.serverDefaultBSLName: default` keeps the same.
3. The Velero pod restarts due to pod crash _or_ delete, the Velero server start and the BSL controller checks the `.spec.serverDefaultBSLName`. Since it's unchanged, so the BSL controller won't update the defaulted BSL from the server-side configuration.
4. The user changes velero-server side `--default-backup-storage-location=secondary`. The `.spec.serverDefaultBSLName: default` mismatch the velero-server side configuration, change to use server-side configuration and update the `.spec.serverDefaultBSLName: secondary`.
--
",carlisia,"
--
But this would not translate into covering both at the same time. This would mean giving precedence to the server config over the user config. In this proposed way, it would also break the principle of ""do the least surprising thing"", since the user would probably be surprised something they intentionally set was changed to something else they might not even be aware of (the server config, which will always have a default value of ""default"").
--

--
https://github.com/vmware-tanzu/velero/pull/3172/files#diff-ff4ac83c63c469932ea321327ca5368dbf304c8ea2affe85539d9225aea93247R163

The PR above marks the BSL to be created as the default, since at this time there's only 1.

Another point where to handle this is when looping over the controller (BSL or Sync, doesn't matter), if there's only 1 BSL then mark it as default. Either this way or by just using the BSL if there's no other (it'd be odd for the backup to be doing this check but let's roll with it), the question of what if the server configuration specifies a different default?
--

--
I just realized we have a user (cli) and a global (server)l level config for the ttl. Let me look at how the precedence is handled with that one.
--
",dsu,"
--
Can I add one more case here?  I think the majority of installations only have 1 BSL configured, so if that's the case can we use it irregardless of whether it's marked as default or not?
--
",,,,,,
3169,OPEN,Collect installed plugin information in a central place,Area/Plugins; Enhancement/User,2021-01-13 22:53:13 +0000 UTC,nrb,Opened,,"**Describe the problem/challenge you have**
Right now, backups don't report what plugins were installed at backup time. An administrator or plugin developer has to know what was there. While this isn't a problem in many cases, having the information about the Velero executing environment at backup time allows for more accurate restore time actions.


**Describe the solution you'd like**
* List installed plugins and their versions (see #3168) within a backup.
  * Could be on the `Backup` CR or a separate file
* Provide a way to read the plugin names and versions to plugins at restore time. This is particularly useful for plugins that might be migrating their own data formats/architectures
* Longer term: validate whether the plugins within a backup are installed prior to running a restore.
  * Do we want to do full version compatibility checking? Or just name-based checks?
  * Some of these will be assumed; if you can get to the BackupStorageLocation, you likely have the right plugin.


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3168,OPEN,Add plugin version reporting API,Area/Plugins; Enhancement/User,2021-01-13 22:53:44 +0000 UTC,nrb,Opened,,"**Describe the problem/challenge you have**
Right now, plugins are on their own for version tracking information; they can put it in an annotation for objects they manipulate, but this is hacky and not really effective long term.


**Describe the solution you'd like**
The plugin API should have a method that the Velero server can call if implemented that reports the installed plugin's current version.

This method should be available for any type of Velero plugin.


**Anything else you would like to add:**
Once this information is available to Velero, it should be captured into a central location, whether on the `Backup` object or some other file in the backup.



**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3136,OPEN,Document resticRepoPrefix,Area/Documentation; Good first issue; Help wanted,2021-02-19 16:55:26 +0000 UTC,a-mccarthy,Opened,,"**Describe the problem/challenge you have**
We should document the `resticRepoPrefix` and how it should be used.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,a,"
--
@nrb can you provide more context about the `resticRepoPrefix`? or point me in the direction of where to look for more info?
--
",,,,,,,,,,
3132,OPEN,Add restic-restore-helper to the Tilt setup,Enhancement/Dev,2020-12-04 18:20:21 +0000 UTC,carlisia,Opened,,For reference: https://github.com/vmware-tanzu/velero/pull/3119#discussion_r535529605,,,,,,,,,,,,,,
3130,OPEN,Velero restore with restic not working for statefulsets with (multiple) initContainers,Enhancement/User; Restic,2021-01-13 18:47:13 +0000 UTC,sgyuris,Opened,,"[carlisia] Update on 1/13: this became a task.

-----
**What steps did you take and what happened:**
I have the latest azure AKS cluster 1.17.9 and 1.18.10 and azurefile as pv.

In the source cluster I run:
`velero backup create full-backup2 --default-volumes-to-restic --ttl ""48h00m00s""`

The restic and velero backup part is done correctly. I can see it in the backup storage account. Everything is fine. The PV is backed up.

In the destination cluster (this should be a migration from one cluster to another) I run:
`velero restore create --from-backup full-backup2 --include-namespaces test-ns`

There are two statefulset in the very same test-ns namespace lets call it sts-test1 and sts-test2.

sts-test2 has no initcontainer.
sts-test1 need to wait for sts-test2 to be live. So we did introduced an initcontainer to check sts-test2's state and wait until sts-test2 is in working state. Then there is an another initcontainer in sts-test1 which do some copying and initial setup before the main container could start.

The thing is that sts-test2 (no initcontainer) got it's data back from the backup.
[time=""2020-12-04T10:12:38Z"" level=info msg=""Restic backups for pod found"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:97"" pluginName=velero pod=test-ns/sts-test2 restore=velero/full-backup-20201204101233]

sts-test1 has no log entry like you can see above and there is no backup restored. (backup is present in the blob store just like for sts-test2) but I see this in restic's log:
[time=""2020-12-04T10:13:51Z"" level=debug msg=""Pod is not running restic-wait init container, not enqueuing restores for pod"" controller=pod-volume-restore key=test-ns/sts-test1 logSource=""pkg/controller/pod_volume_restore_controller.go:173""]

multiple times.

**What did you expect to happen:**

I expect that the pv content would be restored also for sts-test1.

**Environment:**

- Velero version (use `velero version`): 
Client:
        Version: v1.5.2
        Git commit: e115e5a191b1fdb5d379b62a35916115e77124a4
Server:
        Version: v1.5.2 
- Velero features (use `velero client config get features`):
 features: <NOT SET>
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.3"", GitCommit:""2e7996e3e2712684bc73f0dec0200d64eec7fe40"", GitTreeState:""clean"", BuildDate:""2020-05-20T12:52:00Z"", GoVersion:""go1.13.9"", Compiler:""gc"", Platform:""linux/amd64""}
SRC cluster:
Server Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.9"", GitCommit:""e3808385c7b3a3b86db714d67bdd266dc2b6ab62"", GitTreeState:""clean"", BuildDate:""2020-07-15T20:50:36Z"", GoVersion:""go1.13.6"", Compiler:""gc"", Platform:""linux/amd64""}
DEST cluster:
Server Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.10"", GitCommit:""62876fc6d93e891aa7fbe19771e6a6c03773b0f7"", GitTreeState:""clean"", BuildDate:""2020-10-16T20:43:34Z"", GoVersion:""go1.13.15"", Compiler:""gc"", Platform:""linux/amd64""}
- Kubernetes installer & version:
AKS 1.17.9 and 1.18.10
- Cloud provider or hardware configuration:
MS Azure
- OS (e.g. from `/etc/os-release`):
Ubuntu 16.04

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,nrb,"
--
@sgyuris Thanks for this issue! It's definitely a problem with our restic implementation that the init container is bound to the pod and doesn't bubble up to the statefulset, which results in the statefulset controller killing the pods in order to force the templates to match.

I thought we already had an issue for this, but I can't find one that represents it as well as what you've described, so I'm going to use this one and mark it for v1.7.0.
--
",,,,,,,,,,
3150,OPEN,Support encryption of Volume Snapshots using KMSKeys,Area/Cloud/GCP; Enhancement/User; Volumes,2021-02-01 18:25:53 +0000 UTC,nitishkrishna,In progress,,"Currently Velero supports the encryption of backups with [customer-provided KMSKey](https://github.com/vmware-tanzu/velero-plugin-for-gcp/blob/main/backupstoragelocation.md) but doesn't support the same for [Volume Snapshots ](https://github.com/vmware-tanzu/velero-plugin-for-gcp/blob/main/volumesnapshotlocation.md)

The GCP API to create a Snapshot accepts a [KMSKey name](https://cloud.google.com/compute/docs/reference/rest/v1/disks/createSnapshot) and this is the API used [here](https://github.com/vmware-tanzu/velero-plugin-for-gcp/blob/b10604414efa87be054d5e64b9ffbf8a98db7924/velero-plugin-for-gcp/volume_snapshotter.go#L241) as well.
",,,nitishkrishna,"
--
@nrb Any updates on when we can expect this change in Velero?
--
",,,,,,,,,,
3126,OPEN,Remove check for if `updated.Name != created.Name` on downloadrequest code,Area/CLI,2021-02-03 17:44:28 +0000 UTC,carlisia,Opened,,As per this comment: https://github.com/vmware-tanzu/velero/pull/3004#discussion_r533720827,,,,,,,,,,,,,,
3118,OPEN,Substitute environment variables in pre/post hook not working,Enhancement/User; Restore Hooks,2021-03-12 12:55:54 +0000 UTC,d3473r,Opened,,"[carlisia] Update on 1/13: this became a task.

----
**What steps did you take and what happened:**
Create a backup of a mysql container with the hooks:
`pre.hook.backup.velero.io/command=[""mysql"", ""--password=$MYSQL_ROOT_PASSWORD"", ""-e"", ""FLUSH TABLES WITH READ LOCK""]`
and
`post.hook.backup.velero.io/command=[""mysql"", ""--password=$MYSQL_ROOT_PASSWORD"", ""-e"", ""UNLOCK TABLES""]`

velero describe gives me a PartiallyFailed backup and the logs are showing a wrong password:
` msg=""stderr: mysql: [Warning] Using a password on the command line interface can be insecure.\nERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: YES)\n""`
`hookCommand=""[mysql --password=$MYSQL_ROOT_PASSWORD -e FLUSH TABLES WITH READ LOCK]""`

If I am running the hook with the password directly it works:
`pre.hook.backup.velero.io/command=[""mysql"", ""--password=test123"", ""-e"", ""FLUSH TABLES WITH READ LOCK""]`
`msg=""stderr: mysql: [Warning] Using a password on the command line interface can be insecure.\n""`
`hookCommand=""[mysql --password=test123 -e FLUSH TABLES WITH READ LOCK]""`


**Anything else you would like to add:**


`MYSQL_ROOT_PASSWORD` is a required env in the mysql container (https://hub.docker.com/_/mysql) and is set in the container

```
root@mysql-84f87c85d6-skvfn:/# echo $MYSQL_ROOT_PASSWORD
test123
```


**Environment:**

- Velero version (use `velero version`): 
```
λ velero version
Client:
        Version: v1.5.2
        Git commit: e115e5a191b1fdb5d379b62a35916115e77124a4
Server:
        Version: v1.5.2
```
- Velero features (use `velero client config get features`): 
```
λ velero client config get features
features: <NOT SET>
```
- Kubernetes version (use `kubectl version`):
```
λ kubectl version
Client Version: version.Info{Major:""1"", Minor:""19"", GitVersion:""v1.19.3"", GitCommit:""1e11e4a2108024935ecfcb2912226cedeafd99df"", GitTreeState:""clean"", BuildDate:""2020-10-14T12:50:19Z"", GoVersion:""go1.15.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.9"", GitCommit:""94f372e501c973a7fa9eb40ec9ebd2fe7ca69848"", GitTreeState:""clean"", BuildDate:""2020-09-16T13:47:43Z"", GoVersion:""go1.13.15"", Compiler:""gc"", Platform:""linux/amd64""}
```


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,paddy667,"
--
While this may not be a resolution to the underlying issue of variable expansion. Here is a workaround for the specific issue regarding Mysql if some environment variables already exist in the pod that contain the password (very specfic I know but it might help someone on the right track).
```
    hooks:
      resources:
        <omitted>
        pre:
        - exec:
            command:
            - /bin/bash
            - -c
            - export MYSQL_PWD=${MYSQL_ROOT_PASSWORD} && /opt/rh/rh-mysql57/root/usr/bin/mysqldump
              --single-transaction -hsystem-mysql -uroot system > /var/lib/mysql/data/mysqldump
            container: system-mysql
            onError: Fail
            timeout: 30s
```

Basically: `export MYSQL_PWD=${MYSQL_ROOT_PASSWORD} && <cmd>`
--
",marrau,"
--
Hi, i want to confirm the solution posted by @paddy667. It actually works that way.
I tested it with a mongodump, but instead of exporting the environment variable i just put the ```/bin/bash -c``` in front of it.

Before (substitution not working):
```
<omitted>
         pre:
          - exec:
              container: mongodb
              command:
                - mongodump 
                - -u 
                - root 
                - -p 
                - ${MONGODB_ROOT_PASSWORD} 
                - --gzip 
                - -o 
                - /bitnami/mongodb/backup
              onError: Fail
              timeout: 300s
```

After (substitution works):
```
<omitted>
         pre:
          - exec:
              container: mongodb
              command:
                - /bin/bash
                - -c
                - mongodump -u root -p ${MONGODB_ROOT_PASSWORD} --gzip -o /bitnami/mongodb/backup
              onError: Fail
              timeout: 300s
```

As explanation:
The command is executed as is, the same way kubernetes itself does it. So it does not run a shell but mysqldump (or mongodump in my case) directly. The argument with the environment variable is just an argument and the commands are not able to substitute it.
Instead of the mysqldump a shell must be started. The shell is capable of variable substitution. As the argument for the shell is the command we want to call and that´s what the shell is interpreting, it works.

--
",,,,,,,,
3116,OPEN,ordered-resources has no effect on schedule,Bug; Help wanted,2020-12-14 21:35:00 +0000 UTC,jemag,Opened,,"**What steps did you take and what happened:**
Create backup using command:
```
velero create backup bs --ttl 72h --ordered-resources 'pods=monitoring/kube-prometheus-stack-grafana-6dc6f9ccdf-rth4x,monitoring/kube-prometheus-stack-kube-state-metrics-66789f8885-rd6wv,monitoring/kube-prometheus-stack-operator-6f65f78c4c-4ckcx,monitoring/kube-prometheus-stack-promet
heus-node-exporter-6l2kp,monitoring/prometheus-kube-prometheus-stack-prometheus-0,monitoring/prometheus-msteams-68dffb4b4-zzwtz,monitoring/alertmanager-kube-prometheus-stack-alertmanager-0' --include-namespaces=monitoring
```
Notice backup properly contains orderedResources:
``` yaml
OrderedResources:
  pods: monitoring/kube-prometheus-stack-grafana-6dc6f9ccdf-rth4x,monitoring/kube-prometheus-stack-kube-state-metrics-66789f8885-rd6wv,monitoring/kube-prometheus-stack-operator-6f65f78c4c-4ckcx,monitoring/kube-prometheus-stack-prometheus-node-exporter-6l2kp,monitoring/prometheus-kube-pr
ometheus-stack-prometheus-0,monitoring/prometheus-msteams-68dffb4b4-zzwtz,monitoring/alertmanager-kube-prometheus-stack-alertmanager-0

```

Create schedule with similar command
```
velero create schedule bs --schedule=""@every 1h"" --ttl 72h --ordered-resources 'pods=monitoring/kube-prometheus-stack-grafana-6dc6f9ccdf-rth4x,monitoring/kube-prometheus-stack-kube-state-metrics-66789f8885-rd6wv,monitoring/kube-prometheus-stack-operator-6f65f78c4c-4ckcx,monitoring/kub
e-prometheus-stack-prometheus-node-exporter-6l2kp,monitoring/prometheus-kube-prometheus-stack-prometheus-0,monitoring/prometheus-msteams-68dffb4b4-zzwtz,monitoring/alertmanager-kube-prometheus-stack-alertmanager-0' --include-namespaces=monitoring
```
Notice schedule has no mention of ordered resources
``` yaml
Name:         bs
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:  Enabled

Schedule:  @every 1h

Backup Template:
  Namespaces:
    Included:  monitoring
    Excluded:  <none>

  Resources:
    Included:        *
    Excluded:        <none>
    Cluster-scoped:  auto

  Label selector:  <none>

  Storage Location:

  Velero-Native Snapshot PVs:  auto

  TTL:  72h0m0s

  Hooks:  <none>

Last Backup:  2020-11-26 12:02:35 -0500 EST
```
and 
``` yaml
Name:         bs
Namespace:    velero
Labels:       <none>
Annotations:  <none>
API Version:  velero.io/v1
Kind:         Schedule
Metadata:
  Creation Timestamp:  2020-11-26T17:02:35Z
  Generation:          3
  Managed Fields:
    API Version:  velero.io/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:spec:
        .:
        f:schedule:
        f:template:
          .:
          f:hooks:
          f:includedNamespaces:
          f:ttl:
      f:status:
    Manager:      velero
    Operation:    Update
    Time:         2020-11-26T17:02:35Z
    API Version:  velero.io/v1
    Fields Type:  FieldsV1
    fieldsV1:
      f:status:
        f:lastBackup:
        f:phase:
    Manager:         velero-server
    Operation:       Update
    Time:            2020-11-26T17:02:35Z
  Resource Version:  18170
  Self Link:         /apis/velero.io/v1/namespaces/velero/schedules/bs
  UID:               c0222a4e-b08b-4ed6-af60-cee12c6ce3f7
Spec:
  Schedule:  @every 1h
  Template:
    Hooks:
    Included Namespaces:
      monitoring
    Ttl:  72h0m0s
Status:
  Last Backup:  2020-11-26T17:02:35Z
  Phase:        Enabled
Events:         <none>
```
Backups created from schedule also do not mention ordered-resources
``` yaml
Name:         bs-20201126170235
Namespace:    velero
Labels:       velero.io/schedule-name=bs
              velero.io/storage-location=default
Annotations:  velero.io/source-cluster-k8s-gitversion=v1.18.10
              velero.io/source-cluster-k8s-major-version=1
              velero.io/source-cluster-k8s-minor-version=18

Phase:  Completed

Errors:    0
Warnings:  0

Namespaces:
  Included:  monitoring
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  default

Velero-Native Snapshot PVs:  auto

TTL:  72h0m0s

Hooks:  <none>

Backup Format Version:  1.1.0

Started:    2020-11-26 12:02:35 -0500 EST
Completed:  2020-11-26 12:03:13 -0500 EST

Expiration:  2020-11-29 12:02:35 -0500 EST

Total items to be backed up:  167
Items backed up:              167

Velero-Native Snapshots:  3 of 3 snapshots completed successfully (specify --details for more information)

```
**What did you expect to happen:**
Schedule will create backups that consider the ordered-resources provided



**Environment:**

- Velero version (use `velero version`): 1.5.2
- Velero features (use `velero client config get features`): NOT SET
- Kubernetes version (use `kubectl version`): 1.18.10
- Cloud provider or hardware configuration: azure AKS



**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,zubron,"
--
Thanks for reporting this! There is a bug here where we apply the ordered resources when [creating a backup](https://github.com/vmware-tanzu/velero/blob/main/pkg/cmd/cli/backup/create.go#L332-L338) but do not apply them when [creating a schedule](https://github.com/vmware-tanzu/velero/blob/main/pkg/cmd/cli/schedule/create.go#L125-L137). The options flags for creating a backup are included in `schedule create` command so they should processed and applied in the same way.
--
",nrb,"
--
@phuongatemc Would you have bandwidth to help with this issue?
--

--
@phuongatemc That's totally fine with me!
--
",phuongatemc,"
--
How urgent this bug?  I can get to it after the New Year.
--

--
OK.  You can assign to me.

--
",,,,,,
3099,OPEN,Add a column to volume snapshot location to indicate which one is the default,Area/CLI; Enhancement/User,2020-11-19 16:36:11 +0000 UTC,carlisia,Opened,,,,,,,,,,,,,,,,
3098,OPEN,Add a column to BSL to indicate the validation frequency,Area/CLI; Enhancement/User,2020-12-09 02:07:46 +0000 UTC,carlisia,Opened,,,,,jenting,"
--
@carlisia 
Will the flag `--store-validation-frequency` be deprecated under `velero server`?
Because there is a flag `--validation-frequency` under `velero backup-location create` already.

From the design doc https://github.com/vmware-tanzu/velero/blob/master/design/cli-install-changes.md, I did not see something related to it.
--
",,,,,,,,,,
3094,OPEN,Removal of expired backups does not work,Bug; Duplicate,2021-03-10 10:39:04 +0000 UTC,djablonski-moia,Opened,,"**What steps did you take and what happened:**

In our AWS-based setup, when scheduled backup reach their TTL, the deletion process is started but gets stuck in status `Deleting`. The contents in S3 for the bucket are properly deleted, while volume snapshots stay (causing significant extra cost).

**What did you expect to happen:**

I expect backups to be cleanly removed when their TTL expires, including all backed up data, such as volume snapshots.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`

```
time=""2020-11-18T08:23:47Z"" level=info msg=""Removing existing deletion requests for backup"" backup=velero-nightly-backup-20201103034355 controller=backup-deletion logSource=""pkg/controller/backup_deletion_controller.go:469"" name=velero-nightly-backup-20201103034355-gt6b9 namespace=velero
time=""2020-11-18T08:23:50Z"" level=error msg=""Error in syncHandler, re-adding item to queue"" controller=backup-deletion error=""error downloading backup: error copying Backup to temp file: rpc error: code = Unknown desc = error getting object backups/velero-nightly-backup-20201103034355/velero-nightly-backup-20201103034355.tar.gz: NoSuchKey: The specified key does not exist.\n\tstatus code: 404, request id: 01DBEB5FABBF40BD, host id: HKe3B0heM0NpUhxXbLEZp7THCXtsfDKJkYdR6Sg0bS3+j0ywshitElmEnG7mPdDNmq6ASEtKT6w="" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/controller/restore_controller.go:558"" error.function=github.com/vmware-tanzu/velero/pkg/controller.downloadToTempFile key=velero/velero-nightly-backup-20201103034355-gt6b9 logSource=""pkg/controller/generic_controller.go:140""
```

- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`

```
Name:         velero-nightly-backup-20201103034355
Namespace:    velero
Labels:       app.kubernetes.io/instance=velero
              app.kubernetes.io/managed-by=Tiller
              app.kubernetes.io/name=velero
              helm.sh/chart=velero-2.0.3
              velero.io/schedule-name=velero-nightly-backup
              velero.io/storage-location=aws
Annotations:  <none>

Phase:  Deleting

Errors:    0
Warnings:  0

Namespaces:
  Included:  *
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  aws

Velero-Native Snapshot PVs:  auto

TTL:  360h0m0s

Hooks:  <none>

Backup Format Version:

Started:    2020-11-03 04:43:55 +0100 CET
Completed:  2020-11-03 04:52:17 +0100 CET

Expiration:  2020-11-18 04:43:55 +0100 CET

Velero-Native Snapshots:  2 of 2 snapshots completed successfully (specify --details for more information)

Deletion Attempts:
  2020-11-18 06:30:25 +0100 CET: InProgress
```

- `velero backup logs <backupname>`

```
Logs for backup ""velero-nightly-backup-20201103034355"" are not available until it's finished processing. Please wait until the backup has a phase of Completed or Failed and try again.
```

**Anything else you would like to add:**

My guess is that this at least loosely related to https://github.com/vmware-tanzu/velero/pull/2993.

**Environment:**

Velero version:
```
Client:
	Version: v1.5.2
	Git commit: -
Server:
	Version: v1.5.2
```

Velero features:
```
features: <NOT SET>
```

Kubernetes version:
1.18.9

Kubernetes installer & version:
kops 1.18.1

Cloud provider or hardware configuration:
AWS (with aws plugin)

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,ashish,"
--
This is definitely a bug.
The correct fix is to not delete the Backup from the object store when there are errors.
In this case, there might have been errors deleting volumesnpshots at https://github.com/vmware-tanzu/velero/blob/main/pkg/controller/backup_deletion_controller.go#L349
And these errors are ignored to eventually delete the backup from the object store at https://github.com/vmware-tanzu/velero/blob/main/pkg/controller/backup_deletion_controller.go#L362

The correct fix should be to move the 
```
	if backupStore != nil {
		log.Info(""Removing backup from backup storage"")
		if err := backupStore.DeleteBackup(backup.Name); err != nil {
			errs = append(errs, err.Error())
		}
	}
```
into the
```
	if len(errs) == 0 {
		// Only try to delete the backup object from kube if everything preceding went smoothly
		err = c.backupClient.Backups(backup.Namespace).Delete(context.TODO(), backup.Name, metav1.DeleteOptions{})
		if err != nil {
			errs = append(errs, errors.Wrapf(err, ""error deleting backup %s"", kube.NamespaceAndName(backup)).Error())
		}
	}
```
at https://github.com/vmware-tanzu/velero/blob/main/pkg/controller/backup_deletion_controller.go#L410

cc @zubron because you are authoring PR #2993
--

--
I am closing this as a duplicate of #2980
I've copied a link to my above comment into my code review of #2993 
--

--
https://github.com/vmware-tanzu/velero/pull/2993#issuecomment-730440597
Based on this comment reopening this and work on a fix for this.
--
",billimek,"
--
I think that I'm experiencing this as well.  Is it 'safe' to manually delete the backups.velero.io objects that seem to be 'stuck' deleting (e.g. `k delete backups.velero.io -n velero velero-daily-backup-20201212060042`)

![image](https://user-images.githubusercontent.com/6393612/104255493-31cf1c00-5447-11eb-9930-9795b0188519.png)

--
",djablonski,"
--
@billimek For me it was the only way to clean this up, and I did not encounter any problems afterwards so far. But still, it's just guessing 😉 
--
",kong62,"
--
please add  ```velero backup delete --force``` parameter.
I do not think ```kubeclt delete backup``` is a good idea.
--
",,,,
3161,OPEN,Support for Customer Managed Keys (CMK),Area/Cloud/Azure; Needs Product,2021-01-13 18:55:23 +0000 UTC,akcrisp,In progress,,"Hi,

Can you confirm if there are any plans to support cmk ? 

https://docs.microsoft.com/en-us/azure/aks/azure-disk-customer-managed-keys

Thanks

Andy",,,akcrisp,"
--
Hi can I ask what the outcome was of the triage ? 
--
",nrb,"
--
@akcrisp This is not on our roadmap and is unlikely to be added soon. The 'Needs Product' label means we need to discuss this with our product management team to prioritize it.
--
",,,,,,,,
3083,OPEN,Need velero plugin [delete update] command,Area/CLI; Enhancement/User,2020-12-28 03:07:14 +0000 UTC,ashish-amarnath,Opened,,"**Describe the problem/challenge you have**
Users of Velero need to be able to delete and update plugins without having to hand-edit the Velero deployment.


**Describe the solution you'd like**
Implement `velero plugin delete` and `velero plugin update` command

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,survivant,"
--
real usecase : 

I have openebs/velero-plugin:2.2.0  installed, but the version 2.3.0 was released yesterday.  How can I update it ?

#1 - Velero need a way to easily remove any plugin installed
#2 - Velero need a way to update a plugin  (Could call : delete followed by install).  I don't know if the plugin have configuration, but if it's the case, the configuration need to be kept for a update
--
",jenting,"
--
related to #3039 
--
",,,,,,,,
3072,OPEN,"Support multiple selectors (""OR"" logic) when creating a backup",Area/CLI; Enhancement/User,2020-11-12 01:04:30 +0000 UTC,tcdowney,Opened,,"**Describe the problem/challenge you have**
I have a usecase where I'd like to take a targeted backup of several components, but they are not consistently labeled. One is labeled with `app: postgres` and the other with `app.kubernetes.io/name: cf-api-controllers`.

Since a single Kubernetes label selector only supports logical `AND`s of different labels, I have to change the labels on one of the Pods to get this to work. In this case I have control of the `cf-api-controllers` Deployment so I labeled it with `app: cf-api-controllers` as a workaround, but it would be nice if Velero supported a logical `OR` for cases where I didn't have the ability to change the Deployment.

**Describe the solution you'd like**
One option would be to support multiple `--selector` flags. For example:

```console
velero create backup ""${backup_name}"" \
       --include-namespaces cf-system,cf-db \
       --selector 'app in (postgres)' \
       --selector 'app.kubernetes.io/name in (cf-api-controllers)' \
       --wait
```

**Environment:**
I don't think environment matters much here, but will include some anyway:

- Velero version (use `velero version`):
```
Client:
	Version: v1.5.2
	Git commit: -
Server:
	Version: v1.3.2
```

- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.8"", GitCommit:""9f2892aab98fe339f3bd70e3c470144299398ace"", GitTreeState:""clean"", BuildDate:""2020-08-14T11:09:22Z"", GoVersion:""go1.14.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17+"", GitVersion:""v1.17.12-gke.1504"", GitCommit:""17061f5bd4ee34f72c9281d49f94b4f3ac31ac25"", GitTreeState:""clean"", BuildDate:""2020-10-19T17:00:22Z"", GoVersion:""go1.13.15b4"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Kubernetes installer & version: GKE
- Cloud provider or hardware configuration: GKE
- OS (e.g. from `/etc/os-release`): OS X

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3066,OPEN,Add info about compression to docs,Area/Documentation; Good first issue; Help wanted,2021-02-19 14:29:13 +0000 UTC,a-mccarthy,Opened,,"**Describe the problem/challenge you have**
Does Velero provides compression? What is the percentage of data it compresses?


**Describe the solution you'd like**
We should improve the docs to cover how Velero handles compression.

Velero's compression for object metadata is limited, using Golang's tar implementation. However, in most instances, Kubernetes objects are limited to 1.5MB in size, and many don't approach that. For restic data, they have not yet implemented compression, though they do have de-deduplication.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3062,OPEN,--default-restic-prune-frequency does not have any effect ; resticrepo is always created with default value (168h0m0s) even after passing different value,Bug; Restic,2021-01-13 18:59:19 +0000 UTC,noeljose2020,Opened,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)
installed velero using helm chart in kubernetes enviornment and passed ---default-restic-prune-frequency=24h0m0s , 
created a schedule , resticrepo got created with default maintenance frequency it did not pick  the value passed 

output of resticrepo
Name:         test-default-ljppp
Namespace:    test
Labels:       velero.io/storage-location=default
              velero.io/volume-namespace=test
Annotations:  <none>
API Version:  velero.io/v1
Kind:         ResticRepository
Metadata:
  Creation Timestamp:  2020-11-10T13:41:34Z
  Generate Name:       test-default-
  Generation:          3
  Resource Version:    12812120
  Self Link:           /apis/velero.io/v1/namespaces/test/resticrepositories/test-default-ljppp
  UID:                 a35707ad-8e2c-4dc1-adc9-47c207fe082e
Spec:
  Backup Storage Location:  default
  Maintenance Frequency:    168h0m0s
  Restic Identifier:        s3:
  Volume Namespace:         test
Status:
  Last Maintenance Time:  2020-11-10T13:41:39Z
  Phase:                  Ready
Events:                   <none>
**What did you expect to happen:**
expected resticrepo will get created with the passed value

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
- `velero backup logs <backupname>`
- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): 1.51.
- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,nrb,"
--
@noeljose2020 Thanks for the report - we've heard about this from other people and I think the argument isn't be passed from the command line into the configuration properly.
--
",,,,,,,,,,
3053,OPEN,Support hostpath volumes to be backed up and restored by restic in order to running / testing / developing in kind cluster.,Needs Product,2021-01-14 14:44:20 +0000 UTC,VolkerKozlowski,Opened,,"**Describe the problem/challenge you have**
We are running e2e tests inside of a kind cluster. Kind only provides hostpath mounted volumes provisioned by rancher.io/local-path. 
Unfortunately velero cannot handle to backup and restore such volumes with restic.  


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**
Kind kind v0.9.0-alpha
Velero 1.4

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,jppitout,"
--
This would definitely help for training and home labs using Kind clusters :) 
--
",,,,,,,,,,
3052,OPEN,Have a way to modify nodeSelector when restoring,Enhancement/User; Needs Product,2021-04-09 08:59:25 +0000 UTC,sagor999,Opened,,"**Describe the problem/challenge you have**
I have Cluster A, that uses `nodeSelector`.
I want to restore into Cluster B, that doesn't use nodeSelector or doesn't have similar labels as Cluster A has.

**Describe the solution you'd like**
Similar to `velero.io/change-storage-class: RestoreItemAction` I would like to be able to remap nodeSelector labels when restoring, or have a way to completely delete any labels in nodeSelector.

**Anything else you would like to add:**


**Environment:**

- Velero version (use `velero version`):
1.5.1
- Kubernetes version (use `kubectl version`):
1.17.3
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,brian,"
--
I also need this feature, actually I am trying to find a workaround.
--
",,,,,,,,,,
3043,OPEN,Generate RPM & Deb packages for Velero CLI,Enhancement/User,2020-11-26 09:48:48 +0000 UTC,dsu-igeek,Opened,,"**Describe the problem/challenge you have**
Installing the Velero CLI involves downloading the binary and putting it in the appropriate directory (/usr/local/bin recommended).  There is not automatic update and installation requires multiple steps.

**Describe the solution you'd like**
Generate RPM and Deb packages to install the CLI in a consistent manner and manage versions.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
3214,OPEN,Problems with configuring incremental snapshots with the Helm chart,Area/Cloud/Azure; Bug; Helm,2021-03-16 16:04:35 +0000 UTC,aristosvo,In progress,,"**What steps did you take and what happened:**

Helm install of Velero (Azure plugin) with Terraform with option `configuration.volumeSnapshotLocation.config.incremental=true`

After application of this an error showed up:
```bash
Error: post-upgrade hooks failed: warning: Hook post-upgrade velero/templates/volumesnapshotlocation.yaml failed: VolumeSnapshotLocation.velero.io ""default"" is invalid: spec.config.incremental: Invalid value: ""boolean"": spec.config.incremental in body must be of type string: ""boolean""
```

**What else did you try:**

Helm install of Velero (Azure plugin) with Terraform with option `configuration.volumeSnapshotLocation.config.incremental=""\""true\""""`, but this gives problems in de controller:
```
time=""2020-10-23T07:25:26Z"" level=error msg=""Error getting volume snapshotter for volume snapshot location"" backup=velero/mongodb-20201023072522 error=""rpc error: code = Unknown desc = unable to parse value \""\\\""true\\\""\"" for config key \""incremental\"" (expected a boolean value): strconv.ParseBool: parsing \""\\\""true\\\""\"": invalid syntax"" error.file=""/go/src/velero-plugin-for-microsoft-azure/velero-plugin-for-microsoft-azure/volume_snapshotter.go:142"" error.function=""main.(*VolumeSnapshotter).Init"" logSource=""pkg/backup/item_backupper.go:449"" name=pvc-051b880a-6fd5-4e6f-99a0-be96b211c18e namespace= persistentVolume=pvc-051b880a-6fd5-4e6f-99a0-be96b211c18e resource=persistentvolumes volumeSnapshotLocation=default
```

**What did you expect to happen:**

I expect it to work, as it is given as a boolean in the possible values.





**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- helm version (use `helm version`): 
v3
- helm chart version and app version:
`velero  velero          5               2020-10-23 11:01:23.127373 +0200 CEST   failed  velero-2.13.3   1.5.1   `
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""19"", GitVersion:""v1.19.2"", GitCommit:""f5743093fd1c663cb0cbc89748f730662345d44d"", GitTreeState:""clean"", BuildDate:""2020-09-16T21:51:49Z"", GoVersion:""go1.15.2"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.8"", GitCommit:""73ec19bdfc6008cd3ce6de96c663f70a69e2b8fc"", GitTreeState:""clean"", BuildDate:""2020-09-17T04:17:08Z"", GoVersion:""go1.13.15"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Cloud provider or hardware configuration: `azure`
",,,carlisia,"
--
This looks like it could be a bug in the chart.
--
",aristosvo,"
--
True. Specifying optional parameters in `/crds/volumesnapshotlocations.yaml` with the correct type or adding multiple types might fix it:
```
spec:
  validation:
    openAPIV3Schema:
      properties:
        spec:
          properties:
            config:
              additionalProperties:
                type: string <---- Cause of the problem?
```

Possible solutions:
```
...
          properties:
            config:
              additionalProperties: {}
```

```
...
              properties:
                incremental:
                  type: boolean
              additionalProperties:
                type: string
```

Any preference?

--
",jenting,"
--
Related to https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure/pull/52

According to https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure/blob/3d038eb/volumesnapshotlocation.md, it's a string type
```
incremental: ""<false|true>""
```

However, the code expects it's a boolean type https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure/blob/3d038eb/velero-plugin-for-microsoft-azure/volume_snapshotter.go#L137-L147 
--

--
> This looks like it could be a bug in the chart.

I think it's a bug in the microsoft-azure plugin.

/cc @carlisia 
--

--
@aristosvo,
I file a [PR](https://github.com/vmware-tanzu/helm-charts/pull/229) to address this issue, could you please take a look if you are free? Thank you.
--
",,,,,,
3035,OPEN,Allow backups with same name to be imported when adding a new BSL with existing backups,Bug; Needs Product,2020-10-22 21:40:16 +0000 UTC,zubron,In progress,,"Backups are considered unique based on their name, not a combination of the name and the storage location. When you add a new backup storage location where a backup name matches a backup in the existing storage location, it will not be imported into the new cluster and currently we don't have a way to work around that.

_Originally posted by @zubron in https://github.com/vmware-tanzu/velero/issues/3026#issuecomment-714772975_

Users may not know the names of all backups being imported when adding a BSL with existing backups. Currently we don't import those and users don't have a way to access them.",,,zubron,"
--
There are different ways that this could be solved, e.g. modifying velero to identify backups based not on the name from the CRD but from a combination of backup and bucket name, or allowing the backups to be imported from the BSL with a different name or prefix/suffix that helps to distinguish them.

There are likely UX implications for adding something like this so although the current behaviour could be considered a bug, it may need some product team input about the best way to solve this for users.
--
",,,,,,,,,,
3159,OPEN,Enable use with Azure Stack,Area/Cloud/Azure; Enhancement/User; Needs Product,2020-12-08 00:08:52 +0000 UTC,dsu-igeek,Opened,,Azure Stack provides a hybrid Azure platform that does is separate from the Azure Public Cloud.  Currently we use the cloudName to generate the Azure environment (https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure/blob/22b6776fba44b7f521afce56de6e20f1b7373855/velero-plugin-for-microsoft-azure/common.go#L50).  Azure Stack appears to need a different path to generate the environment (https://raw.githubusercontent.com/Azure/go-autorest/master/autorest/azure/environments.go).  There may be other issues as well.,,,,,,,,,,,,,,
3033,OPEN,Restic configuration CLI new flags,Area/CLI; Needs Product; Restic,2020-10-22 21:43:13 +0000 UTC,AhmedBenAbid,Opened,,"**Describe the problem/challenge you have**
Hello,
We are using PKS as a Kubernetes installer on vSphere in a air-gapped environnement, so all Docker images need to be imported to a local registry.
The vSphere version we're currently using doesn't support Volume Snaphot, so we choose Restic integration for PV backups.

We are using the CLI to install the velero, which allow us to choose velero's and restic's images, but not the **restic-restore-helper** image (which can be configured later using a ConfigMap)
Also, we cannot choose hostPath path on installation

**Describe the solution you'd like**
It would be practical to add these two parameters as cli flags:
--restic-helper-image
--restic-hostpath

**Environment:**

- Velero version: Version: v1.5.1
- Kubernetes version: v1.17.0
- Kubernetes installer & version: PKS 1.7
- Cloud provider or hardware configuration: vSphere
- OS (e.g. from `/etc/os-release`):

Thank you for your attention.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,nrb,"
--
I think we can reliably detect TKGI's preferred restic hostpath with information provided in https://github.com/vmware-tanzu/velero/issues/2767. The helper image would definitely be useful to add, though.
--
",,,,,,,,,,
3026,OPEN,Add options --storage-location and --from-storage-location to the velero restore command,Needs Product,2021-03-11 06:33:41 +0000 UTC,konvergence,In progress,,"**Describe the problem/challenge you have**

  1.  Velero allow to add backup-location in ReadOnly to be able to restore from antoher cluster. But if you have already a backupName on the ""default"" backup-location you can not see the other backupName on the alternative backup-location. (This for me a bug)
  2. for a `velero restore` we should be able to choose the backupName `--from-storage-location` and be able to choose on which `--storage-location` we want to write the  restic `/restore` information (see #3019)
 

**Describe the solution you'd like**
  1. Add options `--storage-location` and `--from-storage-location` to the velero restore command
  2.  Update the sync controller to allow to have same backupName but on different backup-location

**Environment:**

- Velero version (use `velero version`): 

```
Client:
        Version: v1.5.1
        Git commit: 87d86a45a6ca66c6c942c7c7f08352e26809426c
Server:
        Version: v1.5.1
```
- Velero features (use `velero client config get features`): 

```
features: <NOT SET>
```

- Kubernetes version (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.6"", GitCommit:""dff82dc0de47299ab66c83c626e08b245ab19037"", GitTreeState:""clean"", BuildDate:""2020-07-15T16:58:53Z"", GoVersion:""go1.13.9"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.6"", GitCommit:""dff82dc0de47299ab66c83c626e08b245ab19037"", GitTreeState:""clean"", BuildDate:""2020-07-15T16:51:04Z"", GoVersion:""go1.13.9"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Kubernetes installer & version:` rancher 2.4.5 rke


- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

```
NAME=""Ubuntu""
VERSION=""18.04.4 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.4 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
````
**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,zubron,"
--
Hi @konvergence,

> Velero allow to add backup-location in ReadOnly to be able to restore from antoher cluster. But if you have already a backupName on the ""default"" backup-location you can not see the other backupName on the alternative backup-location. (This for me a bug)

Just to clarify, do you have backups with the same name in both buckets, but you are only able to see the backup from the default bucket?
--

--
Hi again @konvergence. Unfortunately the backups are considered unique based on their name, not a combination of the name and the storage location. When you add a new backup storage location where a backup name matches a backup in the existing storage location, it will not be imported into the new cluster and currently we don't have a way to work around that.

Given that this is a distinct issue from point 2 in the description above, I am going to open a separate issue to address this and leave this issue to solely address the redirecting where the restore information is written to.
--
",konvergence,"
--
Thanks 
Yes a had the same backupName on the 2 locations. 

Is there a way to configure a prefix for backupName on velero server side ?
--
",zzmg,"
--
hi
i face the same problem with you
I cannot migrate from one cluster to another cluster right now，because new cluster do not get  backup object from a new backup location，and old cluster' s backup in the new backup location 
--
",,,,,,
3013,OPEN,Restore is not failing when trying to restore a namespace which does not exist in backup,Enhancement/User,2021-01-13 22:59:21 +0000 UTC,email2smohanty,In progress,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)
- Created a velero backup which consists of 2 namespaces named `default` and `openshift`
- By mistake when I am restoring namespace `openldap`  which does not exist in backup then the restore is getting complete without any error

**What did you expect to happen:**
- I am expecting as I am trying to restore a namespace `openldap` which does not exist in the backup then the restore should fail with proper error

**Environment:**

- Velero version (use `velero version`): 
Client:
        Version: v1.4.2
Server:
        Version: v1.5.1
- Velero features (use `velero client config get features`): 
",,,carlisia,"
--
Hey @email2smohanty, thanks for the report.

Can you please send me the command you used to do the restore? Thank you.
--
",email2smohanty,"
--
@carlisia please find my restore command below

```
velero create restore openshift-operators-restore --from-backup openshift-operators-backup --include-namespaces openshift-operators --include-cluster-resources=true
```
--
",nrb,"
--
@email2smohanty Thanks for the information!

I think this is a reasonable request to enhance Velero. I'll get it moved into the backlog.
--
",ashish,"
--
I fully understand why this is happening. Considering that this has not been moved into our backlog it requires no further investigation. Hence removing ""Needs Investigation"".
--
",,,,
3009,OPEN,Update hashicorp/go-plugin dependency,Enhancement/User,2020-11-25 08:55:52 +0000 UTC,sshende-catalogicsoftware,Opened,,"**What steps did you take and what happened:**
Installed Velero and triggered the backup for all namespaces. The backup failed with error messages in log files.

**What did you expect to happen:**
For the backup to finish with phase ""Completed""

**Anything else you would like to add:**

- The cluster has 1000's of Custom resources which we call ""kcas"" belonging to group version ""cc/v1"".

- The backup logs show this error message:

time=""2020-10-12T14:29:20Z"" level=info msg=""Getting items for group"" backup=cio/5f84683622c8a8f3fd63b645 group=cc/v1 logSource=""pkg/backup/item_collector.go:76""
time=""2020-10-12T14:29:20Z"" level=info msg=""Getting items for resource"" backup=cio/5f84683622c8a8f3fd63b645 group=cc/v1 logSource=""pkg/backup/item_collector.go:165"" resource=kcas
time=""2020-10-12T14:29:20Z"" level=info msg=""Listing items"" backup=cio/5f84683622c8a8f3fd63b645 group=cc/v1 logSource=""pkg/backup/item_collector.go:291"" namespace= resource=kcas
time=""2020-10-12T14:29:23Z"" level=error msg=""Error listing items"" backup=cio/5f84683622c8a8f3fd63b645 error=""rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (3939450546 vs. 2147483647)"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/backup/item_collector.go:294"" error.function=""github.com/vmware-tanzu/velero/pkg/backup.(*itemCollector).getResourceItems"" group=cc/v1 logSource=""pkg/backup/item_collector.go:294"" namespace= resource=kcas

- Is there a configurable parameter to set the maxReceiveMessageSize for gRPC calls? Any other way?

**Environment:**

- Velero version: 1.5.1 
- Kubernetes version 1.18

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,carlisia,"
--
Hey @sshende-catalogicsoftware, I would try increasing the memory limit on the Velero deployment. There may be a couple of defaults that aren't playing nice together. Let us know if that fixes things.

Instructions: https://velero.io/docs/v1.5/customize-installation/#customize-resource-requests-and-limits
--
",ashish,"
--
@sshende-catalogicsoftware We currently don't offer a way to customize the grpc message size for the plugin client.
This is not a limit that many of our users have hit.

However, we should consider updating our [hashicorp/go-plugin](/hashicorp/go-plugin) dependency from
`github.com/hashicorp/go-plugin v0.0.0-20190610192547-a1bc61569a26` to the latest release because there is a fix that might be useful for this. https://github.com/hashicorp/go-plugin/commit/6d2a5564f60a0ff22a42428aa9fdfe48dfeac8df

--

--
I am going to add this to our backlog with the ""Enhancement/User"" label and remove the ""Needs Investigation"" label.
--
",,,,,,,,
3158,OPEN,Network restrictions on Azure Storage Account not working,Area/Cloud/Azure; Area/Documentation; Good first issue; Help wanted,2021-01-22 15:48:20 +0000 UTC,aristosvo,In progress,,"When we installed Velero with the Velero plugin for Azure on our AKS cluster in the first place, we didn't restrict our storage on AKS outbound IP, as it was just an experiment.

After this experiment we tried to improve security by restricting to only the outbound IP of our AKS cluster. This doesn't seem to work. AKS documentation refers to account access [here](https://docs.microsoft.com/en-us/azure/aks/troubleshooting#error-when-enabling-allow-access-allow-access-from-selected-network-setting-on-storage-account), but I didn't expect Velero to have the same problem as it runs on the worker nodes.

Any other experiences and/or how to mitigate it?

",,,ashish,"
--
@aristosvo I am not super familiar with the network restrictions here. But to troubleshoot this, can you please share more details on the error that you saw? Also, any documentation that you can point us to try this ourselves will be useful to make suggestions.
--
",aristosvo,"
--
Hi @ashish-amarnath ! I have found the problem and will try to find the time to write down the possible solutions for users of Velero with AKS to secure the storage account in the best possible ways after I've implemented it myself.

If you prefer to investigate it yourself, this is the [issue (with config](https://github.com/Azure/AKS/issues/1899#issue-720462843)) on AKS and the provided [solution](https://github.com/Azure/AKS/issues/1899#issuecomment-708007553)
--

--
@ashish-amarnath Where should I put this kind of information? Is there a `docs` section where I can put this?
--

--
@a-mccarthy Thanks for notifying! I'm on it, will open an PR in a minute.

The only update on your summary would be: 
> if you are using AKS and an Azure Storage account **with no public access enabled** in the same region, you need to use VNET Service Endpoints
--

--
@a-mccarthy If you have any feedback: thanks in advance!

I'm not sure whether I should include step-by-step instructions or not.
--
",nrb,"
--
@aristosvo I think the best place for that information right now is in the Azure plugin's README.
--
",a,"
--
To try and summarize here, if you are using AKS and an Azure Storage account in the same region, you need to use [VNET Service Endpoints](https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-service-endpoints-overview), as noted in this [issue comment](https://github.com/Azure/AKS/issues/1899#issuecomment-708007553)

> The network restriction option where you whitelist the public IP isn't working, when both resources are in the same region. As the traffic is handled internally in the region itself and never leaves the network via the outbound public IP.

As Nolan mentioned, the best place for this information is on our Azure Plugin's [README](https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure/blob/main/README.md)

@aristosvo are you still able to help update the docs for this?
--
",,,,
3003,OPEN,sysdig report shows vulnerabilities with velero image,Security,2021-01-13 19:05:57 +0000 UTC,harsh2202,In progress,,"Hi Team,

We can see list of vulnerabilities with velero image (veleor:v1.4.0) while having sysdig image scan, though most of medium level and seems fix available not for all. Can you please suggest & help us with latest velero image.

Thank You
Kind Regards

Harish",,,harsh2202,"
--
Vulnerabilities Critical High Medium Low Negligible Unknown
Operating System 0 0 14 (10 fixes) 33 (9 fixes) 28 (8 fixes) 0

Operating System
Vulnerability ID    Severity Package Type Package                              Fix
CVE-2020-13844 Medium dpkg libstdc++6-8.4.0-1ubuntu1~18.04 None
CVE-2020-13844 Medium dpkg gcc-8-base-8.4.0-1ubuntu1~18.04 None
CVE-2018-20839 Medium dpkg libudev1-237-3ubuntu10.39 None
CVE-2018-20839 Medium dpkg libsystemd0-237-3ubuntu10.39 None
CVE-2020-1751 Medium dpkg libc6-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2020-1751 Medium dpkg libc-bin-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2018-19591 Medium dpkg libc6-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2018-19591 Medium dpkg libc-bin-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2018-11237 Medium dpkg libc6-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2018-11237 Medium dpkg libc-bin-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2018-11236 Medium dpkg libc6-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2018-11236 Medium dpkg libc-bin-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2020-3810 Medium dpkg libapt-pkg5.0-1.6.12 1.6.12ubuntu0.1
CVE-2020-3810 Medium dpkg apt-1.6.12 1.6.12ubuntu0.1
CVE-2020-6096 Low dpkg libc6-2.27-3ubuntu1 None
CVE-2020-6096 Low dpkg libc-bin-2.27-3ubuntu1 None
CVE-2020-13776 Low dpkg libudev1-237-3ubuntu10.39 None
CVE-2020-13776 Low dpkg libsystemd0-237-3ubuntu10.39 None
CVE-2020-12723 Low dpkg perl-base-5.26.1-6ubuntu0.3 None
CVE-2020-10878 Low dpkg perl-base-5.26.1-6ubuntu0.3 None
CVE-2020-10543 Low dpkg perl-base-5.26.1-6ubuntu0.3 None
CVE-2019-9923 Low dpkg tar-1.29b-2ubuntu0.1 None
CVE-2019-20838 Low dpkg libpcre3-2:8.39-9 None
CVE-2019-18276 Low dpkg bash-4.4.18-2ubuntu1.2 None
CVE-2019-17543 Low dpkg liblz4-1-0.0~r131-2ubuntu3 None
CVE-2019-13050 Low dpkg gpgv-2.2.4-1ubuntu1.2 None
CVE-2019-12904 Low dpkg libgcrypt20-1.8.1-4ubuntu1.2 None
CVE-2018-7169 Low dpkg passwd-1:4.5-1ubuntu2 None
CVE-2018-7169 Low dpkg login-1:4.5-1ubuntu2 None
CVE-2018-20482 Low dpkg tar-1.29b-2ubuntu0.1 None
CVE-2018-16869 Low dpkg libnettle6-3.4-1 None
CVE-2018-16869 Low dpkg libhogweed4-3.4-1 None
CVE-2018-16868 Low dpkg libgnutls30-3.5.18-1ubuntu1.3 None
CVE-2016-2781 Low dpkg coreutils-8.28-1ubuntu1 None
CVE-2016-10739 Low dpkg libc6-2.27-3ubuntu1 None
CVE-2016-10739 Low dpkg libc-bin-2.27-3ubuntu1 None
CVE-2013-4235 Low dpkg passwd-1:4.5-1ubuntu2 None
CVE-2013-4235 Low dpkg login-1:4.5-1ubuntu2 None
CVE-2020-1752 Low dpkg libc6-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2020-1752 Low dpkg libc-bin-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2020-10029 Low dpkg libc6-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2020-10029 Low dpkg libc-bin-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2019-9169 Low dpkg libc6-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2019-9169 Low dpkg libc-bin-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2019-19126 Low dpkg libc6-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2019-19126 Low dpkg libc-bin-2.27-3ubuntu1 2.27-3ubuntu1.2
CVE-2019-14855 Low dpkg gpgv-2.2.4-1ubuntu1.2 2.2.4-1ubuntu1.3
CVE-2020-14155 Negligible dpkg libpcre3-2:8.39-9 None
CVE-2019-17595 Negligible dpkg ncurses-bin-6.1-1ubuntu1.18.04 None
CVE-2019-17595 Negligible dpkg ncurses-base-6.1-1ubuntu1.18.04 None
CVE-2019-17595 Negligible dpkg libtinfo5-6.1-1ubuntu1.18.04 None
CVE-2019-17595 Negligible dpkg libncursesw5-6.1-1ubuntu1.18.04 None
CVE-2019-17595 Negligible dpkg libncurses5-6.1-1ubuntu1.18.04 None
CVE-2019-17594 Negligible dpkg ncurses-bin-6.1-1ubuntu1.18.04 None
CVE-2019-17594 Negligible dpkg ncurses-base-6.1-1ubuntu1.18.04 None
CVE-2019-17594 Negligible dpkg libtinfo5-6.1-1ubuntu1.18.04 None
CVE-2019-17594 Negligible dpkg libncursesw5-6.1-1ubuntu1.18.04 None
CVE-2019-17594 Negligible dpkg libncurses5-6.1-1ubuntu1.18.04 None
CVE-2018-1000654 Negligible dpkg libtasn1-6-4.13-2 None
CVE-2017-8283 Negligible dpkg dpkg-1.19.0.5ubuntu2.3 None
CVE-2017-11164 Negligible dpkg libpcre3-2:8.39-9 None
CVE-2016-10228 Negligible dpkg libc6-2.27-3ubuntu1 None
CVE-2016-10228 Negligible dpkg libc-bin-2.27-3ubuntu1 None
CVE-2015-8985 Negligible dpkg libc6-2.27-3ubuntu1 None
CVE-2015-8985 Negligible dpkg libc-bin-2.27-3ubuntu1 None
CVE-2009-5155 Negligible dpkg libc6-2.27-3ubuntu1 None
CVE-2009-5155 Negligible dpkg libc-bin-2.27-3ubuntu1 None
CVE-2018-7738 Negligible dpkg util-linux-2.31.1-0.4ubuntu3.6 2.31.1-0.4ubuntu3.7
CVE-2018-7738 Negligible dpkg mount-2.31.1-0.4ubuntu3.6 2.31.1-0.4ubuntu3.7
CVE-2018-7738 Negligible dpkg libuuid1-2.31.1-0.4ubuntu3.6 2.31.1-0.4ubuntu3.7
CVE-2018-7738 Negligible dpkg libsmartcols1-2.31.1-0.4ubuntu3.6 2.31.1-0.4ubuntu3.7
CVE-2018-7738 Negligible dpkg libmount1-2.31.1-0.4ubuntu3.6 2.31.1-0.4ubuntu3.7
CVE-2018-7738 Negligible dpkg libfdisk1-2.31.1-0.4ubuntu3.6 2.31.1-0.4ubuntu3.7
CVE-2018-7738 Negligible dpkg libblkid1-2.31.1-0.4ubuntu3.6 2.31.1-0.4ubuntu3.7
CVE-2018-7738 Negligible dpkg fdisk-2.31.1-0.4ubuntu3.6 2.31.1-0.4ubuntu3.7
--

--
@carlisia Can you please let us know if there is some progress towards resolutions?
--
",carlisia,"
--
Thank you @harsh2202, looking into it.
--
",SatyKrish,"
--
@carlisia please add support for distroless images. This is can reduce/remove most of the OS vulnerabilities.
--
",,,,,,
2999,OPEN,Velero cannot backup one StorageClass with plugins and another StorageClass with restic,Area/CSI; Enhancement/User; Restic; Volumes,2021-01-13 22:59:50 +0000 UTC,sagor999,In progress,,"**What steps did you take and what happened:**
Upgraded to velero 1.5.1 to take advantage of `--default-volumes-to-restic` and not having to annotate every single volume.
Unfortunately it doesn't work. Fails with `PartiallyFailed`. 
Here is the command that I run:
`velero backup create lvm-test6 --default-volumes-to-restic --include-namespaces default --selector app=lvm-test`
Here is log:
```
time=""2020-10-09T21:51:57Z"" level=info msg=""Executing custom action"" backup=velero/lvm-test6 logSource=""pkg/backup/item_backupper.go:327"" name=lvm-test-pvc namespace=default resource=persistentvolumeclaims
time=""2020-10-09T21:51:57Z"" level=info msg=""Starting PVCBackupItemAction"" backup=velero/lvm-test6 cmd=/plugins/velero-plugin-for-csi logSource=""/go/src/velero-plugin-for-csi/internal/backup/pvc_action.go:57"" pluginName=velero-plugin-for-csi
time=""2020-10-09T21:51:57Z"" level=info msg=""Fetching storage class for PV lvm-local-disk"" backup=velero/lvm-test6 cmd=/plugins/velero-plugin-for-csi logSource=""/go/src/velero-plugin-for-csi/internal/backup/pvc_action.go:101"" pluginName=velero-plugin-for-csi
time=""2020-10-09T21:51:57Z"" level=info msg=""1 errors encountered backup up item"" backup=velero/lvm-test6 logSource=""pkg/backup/backup.go:451"" name=lvm-test-5ff8bbd8f5-r7gtc
time=""2020-10-09T21:51:57Z"" level=error msg=""Error backing up item"" backup=velero/lvm-test6 error=""error executing custom action (groupResource=persistentvolumeclaims, namespace=default, name=lvm-test-pvc): rpc error: code = Unknown desc = failed to get volumesnapshotclass for storageclass lvm-local-disk: failed to get volumesnapshotclass for provisioner topolvm.cybozu.com"" logSource=""pkg/backup/backup.go:455"" name=lvm-test-5ff8bbd8f5-r7gtc
time=""2020-10-09T21:51:57Z"" level=info msg=""Backed up 3 items out of an estimated total of 4 (estimate will change throughout the backup)"" backup=velero/lvm-test6 logSource=""pkg/backup/backup.go:418"" name=lvm-test-5ff8bbd8f5-r7gtc namespace=default progress= resource=pods
```

Velero is installed using helm:
```
    initContainers: 
      - name: velero-plugin-for-aws
        image: velero/velero-plugin-for-aws:v1.1.0
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - mountPath: /target
            name: plugins
      - name: velero-plugin-for-csi
        image: velero/velero-plugin-for-csi:v0.1.1
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - mountPath: /target
            name: plugins
...
      # Comma separated list of velero feature flags. default: empty
      features: EnableCSI
...
      # Set true for backup all pod volumes without having to apply annotation on the pod when used restic Default: false. Other option: false.
      defaultVolumesToRestic: true
```

We have two storage providers in our on-prem cluster: CephCSI, and TopolVM. 
CephCSI supports snapshotting and works fine.
TopolVM volumes do not support snapshotting and should be using restic.

Annotating volume for opt-in works fine though:
```
      annotations:
        backup.velero.io/backup-volumes: lvm-volume
```

Is there a way to make it so that velero automatically backs up volumes that support snapshotting via snapshot feature, and for the rest to default to restic?


**What did you expect to happen:**
Expected it to work.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
- `velero backup logs <backupname>`
- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`

**Anything else you would like to add:**


**Environment:**

- Velero version (use `velero version`): 
```
Client:
        Version: v1.5.1
        Git commit: -
Server:
        Version: v1.5.1
```

- Velero features (use `velero client config get features`): 
```
velero client config get features
features: <NOT SET>
```

- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""19"", GitVersion:""v1.19.2"", GitCommit:""f5743093fd1c663cb0cbc89748f730662345d44d"", GitTreeState:""clean"", BuildDate:""2020-09-19T09:16:25Z"", GoVersion:""go1.15.2"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.3"", GitCommit:""06ad960bfd03b39c8310aaf92d1e7c12ce618213"", GitTreeState:""clean"", BuildDate:""2020-02-11T18:07:13Z"", GoVersion:""go1.13.6"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Kubernetes installer & version:
- Cloud provider or hardware configuration:
on prem cluster.
- OS (e.g. from `/etc/os-release`):


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,carlisia,"
--
I'm double checking if restic is supposed to work when CSI is enabled. 
--
",nrb,"
--
> Is there a way to make it so that velero automatically backs up volumes that support snapshotting via snapshot feature, and for the rest to default to restic?

Not right now, no. There's not a way to allow Velero to use restic for one storage class (TopolVM in your case) and a plugin for another. 

We have had similar requests, and this is likely a feature we need to look into, as mixed environments seem to becoming more prevalent.

I'm going to update the issue title to reflect that this would be a feature request, and I can't find a pre-existing issue.
--

--
@betta1 I think you were requesting this too at one point, if I'm not mistaken.
--
",ashish,"
--
@sagor999 It looks to me like you wanted to use restic to backup volumes that were backed by a CSI provider using the `--default-volumes-to-restic` feature.
The `v0.1.1` release of the `velero-plugin-for-csi` is incompatible with the `v1.5.x` as it uses an old API to determine whether or not a volume is being backed up using restic. Specifically, [this change](https://github.com/vmware-tanzu/velero-plugin-for-csi/pull/70).

Please update your `velero-plugin-for-csi` to the latest release [`v0.1.2`](https://github.com/vmware-tanzu/velero-plugin-for-csi/releases/tag/v0.1.2)
--

--
The reported issue is fixed in the [`v0.1.2`](https://github.com/vmware-tanzu/velero-plugin-for-csi/releases/tag/v0.1.2) release of the CSI plugin.
This can be closed once this is confirmed.
Based on this I am removing the ""Needs Investigation"" label and adding the ""Needs Info"" label.

I think the ""Enhancement/User"" label was added for
>Is there a way to make it so that velero automatically backs up volumes that support snapshotting via snapshot feature, and for the rest to default to restic?

So preserving that label.
--

--
@sagor999 Can you please share a sample workload that you are trying to backup and restore?
If you already have done that, please point me to it. I will try this out to see what might be happening.
--
",sagor999,"
--
@ashish-amarnath I just tried to use this again. For some reason, now it doesn't use restic at all. 
It just seems to skip the actual backup phase of the volume.
```
time=""2020-11-03T01:31:56Z"" level=info msg=""Backing up item"" backup=velero/sentry-kafka0 logSource=""pkg/backup/item_backupper.go:121"" name=data-sentry-kafka-0 namespace=ctla resource=persistentvolumeclaims
time=""2020-11-03T01:31:56Z"" level=info msg=""Executing custom action"" backup=velero/sentry-kafka0 logSource=""pkg/backup/item_backupper.go:327"" name=data-sentry-kafka-0 namespace=ctla resource=persistentvolumeclaims
time=""2020-11-03T01:31:56Z"" level=info msg=""Executing PVCAction"" backup=velero/sentry-kafka0 cmd=/velero logSource=""pkg/backup/backup_pv_action.go:49"" pluginName=velero
time=""2020-11-03T01:31:56Z"" level=info msg=""Backing up item"" backup=velero/sentry-kafka0 logSource=""pkg/backup/item_backupper.go:121"" name=pvc-1e4bfb1c-ec3e-4c88-a09d-dc2867276d58 namespace= resource=persistentvolumes
time=""2020-11-03T01:31:56Z"" level=info msg=""Executing takePVSnapshot"" backup=velero/sentry-kafka0 logSource=""pkg/backup/item_backupper.go:405"" name=pvc-1e4bfb1c-ec3e-4c88-a09d-dc2867276d58 namespace= resource=persistentvolumes
time=""2020-11-03T01:31:56Z"" level=info msg=""Backup has volume snapshots disabled; skipping volume snapshot action."" backup=velero/sentry-kafka0 logSource=""pkg/backup/item_backupper.go:408"" name=pvc-1e4bfb1c-ec3e-4c88-a09d-dc2867276d58 namespace= resource=persistentvolumes
time=""2020-11-03T01:31:56Z"" level=info msg=""Executing custom action"" backup=velero/sentry-kafka0 logSource=""pkg/backup/item_backupper.go:327"" name=data-sentry-kafka-0 namespace=ctla resource=persistentvolumeclaims
time=""2020-11-03T01:31:56Z"" level=info msg=""Starting PVCBackupItemAction"" backup=velero/sentry-kafka0 cmd=/plugins/velero-plugin-for-csi logSource=""/go/src/velero-plugin-for-csi/internal/backup/pvc_action.go:58"" pluginName=velero-plugin-for-csi
time=""2020-11-03T01:31:56Z"" level=info msg=""Volume snapshotting not requested for backup velero/sentry-kafka0"" backup=velero/sentry-kafka0 cmd=/plugins/velero-plugin-for-csi logSource=""/go/src/velero-plugin-for-csi/internal/backup/pvc_action.go:62"" pluginName=velero-plugin-for-csi
```
I looked at storage, and couldn't find actual restic data in there.
I tried to restore from that backup, and yes, it restores PV and PVC, but doesn't perform restic restore init container. So no actual data is being restored\backed up. 

I also tried adding annotation:
`backup.velero.io/backup-volumes: kafka0-volume` but that did not do anything. 

--

--
@ashish-amarnath Sure, here it is:
```
kind: Deployment
apiVersion: apps/v1
metadata:
  name: velero-test-backup-pod
  namespace: ctla
spec:
  replicas: 1
  selector:
    matchLabels:
      app: velero-test-backup-pod
  template:
    metadata:
      annotations:
        backup.velero.io/backup-volumes: kafka0-volume
      labels:
        app: velero-test-backup-pod
    spec:
      volumes:
        - name: kafka0-volume
          persistentVolumeClaim:
            claimName: data-sentry-kafka-0
      containers:
        - name: app
          image: ubuntu:focal
          args:
            - /bin/sh
            - -c
            - touch /tmp/healthy; sleep 50000000;
          volumeMounts:
            - mountPath: ""/backup""
              name: kafka0-volume
```
And PVC that is referenced is backed by TopolVM (https://github.com/topolvm/topolvm). 

Velero itself is installed via helm chart, using two plugins: 
`velero/velero-plugin-for-gcp:v1.1.0`
`velero/velero-plugin-for-csi:v0.1.2`
`features: EnableCSI`

Thank you!
--

--
@ashish-amarnath I found the cause I think. in helm chart I had this set:
`configuration.defaultVolumesToRestic: true`
setting it to false fixed restic backup\restore and now during restore it properly adds restic-restore init container.
So something is wrong\bugged with that option I think. 
--
",,,,
2997,OPEN,Restoring pods in an Istio enabled namespace causes them to error,Restore; Size/M,2021-03-31 10:30:58 +0000 UTC,Samze,Opened,,"**What steps did you take and what happened:**
1. Install Istio `istioctl install --set profile=default`
1. Install Velero `velero install`
1. Create namespace `kubectl create namespace nginx-example`
1. Enabled istio on namespace `kubectl label namespace nginx-example istio-injection=enabled`
1. Apply the Velero non-pvc example. `kubectl apply -f https://raw.githubusercontent.com/vmware-tanzu/velero/main/examples/nginx-app/base.yaml`
1. Back up the namespace `velero backup create nginx-backup --include-namespaces nginx-example`
1. Velero reports that the backup was succesfull.
1. Simulate a disaster `kubectl delete namespaces nginx-example`
1. Restore `velero restore create --from-backup nginx-backup`
1. Velero reports that the restore was succesfull.
1. Check on pods in namespace and note they are crashing
1. Check pod logs and note an error in the istio container.

**What did you expect to happen:**
The pods to be restored successfully.

**Anything else you would like to add:**
This was also raised on the istio repo here: https://github.com/istio/istio/issues/27675

It appears as though injection of istio is happening twice on a restore, once from velero once from istio.

(You can potentially work around this by excluding the pod resource and let the deployment recreate the pods, however this will mean that volumes for pods won't be backedup)

**Environment:**
- Istio version: (istioctl version) client version: 1.7.2, control plane version: 1.7.3, data plane version: 1.7.3 (9 proxies)
- Velero version (use `velero version`): v1.5.1
- Velero features (use `velero client config get features`): <NOT_SET>
- Kubernetes version (use `kubectl version`): Server: 1.16, Client: 1.19
- Kubernetes installer & version: GKE 1.16
- Cloud provider or hardware configuration: GKE
- OS (e.g. from `/etc/os-release`): 


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""

Thanks Sam and @teddyking
",,,carlisia,"
--
@Samze thank you for reporting this.

This issue seems to have enough information to try to replicate and evaluate an outcome.
--
",nrb,"
--
Thanks for the detailed production steps @Samze. I'm able to confirm this with Istio 1.7.3, and in reading the Istio issue, I think it's definitely a result of Velero applying the object ""as-is"" (which has the configuration once) and then Istio seeing the object being recreated and trying to configure it too.

This isn't necessarily unique to Istio and Velero. There are a number of operators/controllers that don't expect Velero to restore resources with their labels and annotations (though we do remove status), so they re-process the restored objects.

One potential fix from our side: remove specific Istio annotations via a Pod `RestoreItemAction` plugin.

I do think https://github.com/istio/istio/issues/25931 is the more correct fix, though.
--
",venksel,"
--
Facing the exact same issue, currently using the workaround of excluding POD resources at the time of restore. Did upgrade to latest version 1.5.2.  Any resolution to this problem will be of great help.
--
",MRostanski,"
--
Could you provide the example of command to exclude the pod resources that work for you @venksel ?
Also, this issue requires the specific documentation on how to proceed with istio-controlled cluster (because istio is an operator and we also experience racing conditions with an operator.
The one solution/workaround is to scale dwon the operator to 0 as suggested for example in https://banzaicloud.com/blog/vault-backup-velero/ or to set up the restore order policy so that operators are restored last - but again, I don;t have the working example.
--
",,,,
2994,OPEN,Add the ability to add the docker images to the backup as well,Needs Product,2020-12-01 10:58:07 +0000 UTC,mfuxi,Opened,,"**Describe the problem/challenge you have**
[A description of the current limitation/problem/challenge that you are experiencing.]
I would like to be able to backup and restore an air-gap cluster from one data center to another cluster in a different data center
For that I must have the docker images together with all the other Kubernetes objects backup

**Describe the solution you'd like**
[A clear and concise description of what you want to happen.]
The user should be able to add in his backup command another flag something like --images which would then also backup the images into the object storage which he configured.

**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]
This is an addition to this issue which was closed a year ago:
https://github.com/vmware-tanzu/velero/issues/464

**Environment:**

- Velero version (use `velero version`):
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,devops,"
--
I'd appreciate (and up-vote) if such a feature would be implemented :)
--
",,,,,,,,,,
2990,OPEN,Velero to support active failover to reduce the downtime,Needs Product,2020-10-19 20:25:33 +0000 UTC,gowrisankar22,Opened,,"It would be certainly very nice to mitigate the downtime via an active failover approach. This will be a worth feature add for velero.
https://kubernetes.slack.com/archives/C6VCGP4MT/p1602004864147000",,,,,,,,,,,,,,
2989,OPEN,Make printer column titles consistent across all types,Area/CLI,2020-10-19 20:23:55 +0000 UTC,zubron,Opened,,"> That does, however, put us in an awkward spot since I think all other `.status.startTimestamp` fields should match, due to the existence of the `creationTimestamp` field. Ideally, we'd have clean mappings between the field name in the YAML and printer column, but I think due to historical reasons we're a little stuck.

> I think we could put that on the docket for v2.0 to update those velero commands so we don't break any scripts.

_Originally posted by @nrb in https://github.com/vmware-tanzu/velero/pull/2881#discussion_r496236736_

In adding the additional printer columns for CRDs, it was noted that the columns for the `get` commands were named inconsistently from the fields being displayed, but also that they were inconsistent across types. For example, `.status.startTimestamp` is displayed under the column `CREATED` when running `velero get backups` but is displayed under the column `STARTED` when running `velero get restores`.

We should revisit each of these types and determine how they should be printed and ensure they are all consistent with each other.",,,,,,,,,,,,,,
3039,OPEN,How to Upgrade Plugin and Velero version?,Area/CLI; Area/Plugins,2020-10-23 16:47:09 +0000 UTC,relgisri,In progress,,"Hello lovely team,

I am trying to upgrade our Velero and GCP Plugin versions but struggle a bit understanding on how this process is done.
For Velero itself, there is a comprehensive guide which leads through the upgrade procedure for each version. Here: https://velero.io/docs/v1.4/upgrade-to-1.4/

Currently we run Velero on version 1.3 and the GCP Plugin on version 1.0.1
Following the [install documentation ](https://github.com/vmware-tanzu/velero-plugin-for-gcp#install-and-start-velero) I just exchanged the GCP Plugin version from 1.0.1 to 1.1.0 which doesn't lead to any change.

Therefore I used the way Velero described it and added a step to update the init-container image for the GCP Plugin like so:
`kubectl set image deployment/velero \
    velero-plugin-for-gcp=velero/velero-plugin-for-gcp:v1.1.0 \
    --namespace velero`

`kubectl set image deployment/velero \
    velero=velero/velero:v1.4.0 \
    --namespace velero`

Could somebody please enlighten me on how the upgrade process should look like?

With best regards",,,relgisri,"
--
Am I the only one with this problem :( ?
--
",jenting,"
--
I update the Velero by helm chart. About install Velero by CLI, I think the upgrade progress would be:
1. download the updated Velero CLI
2. upgrade the Velero CRDs by Velero CLI
3. updates the Velero server images, Restic DaemonSet images if present, and updates the plugins images
--
",nrb,"
--
@relgisri Hi there - sorry for the trouble this the plugins. I can definitely see where this is a gap in our documentation and tooling.

I don't think `kubectl set image` updates the init containers. Instead, the YAML has to be edited for the new version tag, or use `velero plugin add velero/velero-plugin-for-gcp:v1.1.0` to get the new version. We're lacking a `velero plugin remove` command though, so I think we need to get this scheduled as a feature against Velero core.
--
",,,,,,
2982,OPEN,velero restore failed with volume token not found,Bug; Volumes,2020-11-02 14:45:39 +0000 UTC,AkhilaDamera,In progress,,"**What steps did you take and what happened:**
I have backed up a vault cluster namespace which has AzureFileshare pvc's with one of the pod volumes as below:

```
$ kubectl get pod/vault-0 -n hashicorp-vault -o yaml

    volumeMounts:
    - mountPath: /vault/data
      name: data
    - mountPath: /vault/config
      name: config
    - mountPath: /home/vault
      name: home
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: vault-token-7c4br

  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: data-vault-0
  - configMap:
      defaultMode: 420
      name: vault-config
    name: config
  - emptyDir: {}
    name: home
  - name: vault-token-7c4br
    secret:
      defaultMode: 420
      secretName: vault-token-7c4br
```
I have used velero with restic and have annotated the pods before creating the backup: 
```
kubectl -n hashicorp-vault annotate pod/vault-0 backup.velero.io/backup-volumes=data,config,home,vault-token-7c4br
```
After creating the backup, deleting the namespace and while restoring the error stated as below:

```
error: 
$ velero restore logs vault-backup-20200922222147 | grep error
time=""2020-09-23T02:21:49Z"" level=info msg=""error restoring vault-0: Pod \""vault-0\"" is invalid: spec.initContainers[0].volumeMounts[1].name: Not found: \""vault-token-7c4br\"""" logSource=""pkg/restore/restore.go:1152"" restore=velero/vault-backup-20200922222147
```
When i looked into the pod information the token has changed to vault-token-cgchc and the pods were failed to initialize.
How to backup the persistent volumes with secret token values which changes after the backup and failing during restore?

**What did you expect to happen:**
Velero should restore the pvc volumes(vault-token-7c4br) successfully.


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`):  1.5.1
- Velero features (use `velero client config get features`): Not Set
- Kubernetes version (use `kubectl version`): 1.17.9
- Cloud provider or hardware configuration: azure aks


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,carlisia,"
--
This seems to have enough information to investigate. Might come down to answering this question:

> How to backup the persistent volumes with secret token values which changes after the backup and failing during restore?
--
",nrb,"
--
@AkhilaDamera Thanks for the report! The cause for this is because Velero ignores any volumes with `-token-` in their name under the assumption that they are Kubernetes tokens.

However, this is a demonstration of a case where that assumption is incorrect.

I'm not super familiar with Vault, do you know if it's possible to name the volume mounts something else as a workaround?
--
",AkhilaDamera,"
--
@nrb The `vault-token-` is created by the kubernetes and automatically mounted within the pod using the kubernetes service account for authentication. This sa token is createdafter the deployment of vault and the code I have will auto-initialize the vault.

I have tried the workaround but it doesn't seems working as the velero is not able to restore the old token pod volume config and the restore failing with vault pods installed but not initialized as it requires the same token which it has auto initialize configured for.

--
",,,,,,
3143,OPEN,Support for IAM roles for service accounts in EKS,Area/Cloud/AWS; Enhancement/User; Help wanted,2020-12-07 23:37:01 +0000 UTC,mihail-sky,Opened,,"Hi, I'm wondering if velero supports [IAM roles for service accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html). In the readme the only two options mentioned are an AWS user and kube2iam. ",,,mihail,"
--
I configured velero with IAM roles for service accounts: 
```
➜  ~ k -n velero exec -it velero-7bb5c888c4-jv8dj -- env | grep AWS
AWS_ROLE_ARN=arn:aws:iam::[ACCOUNT_ID]:role/pre-dev-velero-role
AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token
```
However, it looks like the plugin does not support this option. 
```
➜  ~ k -n velero logs -f velero-7bb5c888c4-jv8dj
time=""2020-09-29T11:35:57Z"" level=info msg=""Checking for existing backup locations ready to be verified; there needs to be at least 1 backup location available"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:58""
time=""2020-09-29T11:37:27Z"" level=error msg=""Error listing backups in backup store"" backupLocation=default controller=backup-sync error=""rpc error: code = Unknown desc = AccessDenied: Access Denied\n\tstatus code: 403, request id: D2D87E673B947D97, host id: zbFi7FwLkfKYaAaXboowwkNBxXU8A2z1SA1ofCJ5S2Wwqs1v7AvHQCjFdXgBa3HQq27UU5ufqlw="" error.file=""/go/src/github.com/vmware-tanzu/velero-plugin-for-aws/velero-plugin-for-aws/object_store.go:331"" error.function=""main.(*ObjectStore).ListCommonPrefixes"" logSource=""pkg/controller/backup_sync_controller.go:175""
time=""2020-09-29T11:38:00Z"" level=warning msg=""The specified default backup location named \""default\"" is unavailable; for convenience, be sure to configure it properly or make another backup location that is available the default"" backupstoragelocation=default controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:105""
time=""2020-09-29T11:38:00Z"" level=error msg=""Current backup storage locations available/unavailable/unknown: 0/1/0, Backup location \""default\"" is unavailable: rpc error: code = Unknown desc = AccessDenied: Access Denied\n\tstatus code: 403, request id: E3B416F2C83195E1, host id: BWLKry8uptDp300YEgU8xCd51qu6BpFBi4QvrQ41QfqOTuGDpxa/SZjx0Wzwdzs6qk+638rqYxc=)"" controller=backupstoragelocation logSource=""pkg/controller/backupstoragelocation_controller.go:152""
```

And backup storage output:
```
➜  ~ k -n velero get backupstoragelocation default
NAME      PHASE         LAST VALIDATED   AGE
default   Unavailable   6s               20h
```

It would be nice to add support for it as this is a newer solution than kube2iam. 
--
",,,,,,,,,,
3157,OPEN,Backup Azure Managed Disks from multiple Resource Groups,Area/Cloud/Azure; Enhancement/User; Volumes,2020-12-08 00:06:08 +0000 UTC,cholm321,In progress,,"We have an AKS cluster with velero installed in an enterprise setup.

On the cluster we have 
app1     with managed disk in ResourceGroup 1
app2     with managed disk in ResourceGroup 2
app3     with managed disk in ResourceGroup 3

All Resource Groups has the AKS spn as contributor.

We have been poking a little around, but it seem that the ResourceGroup must be specified on velero install.

Have we missed something - is it possible to do all configuration at backup stage?
Or better, let the plugin lookup which ResourceGroup the disks are located by inspecting the pv objects.
",,,ashish,"
--
@cholm321 If I am following this correctly, the issue you are facing is an error during disk information lookup which uses the resource group from the envvars, derived from the `cloud-credentials` secret, as the resource group for all disks.
https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure/blob/main/velero-plugin-for-microsoft-azure/volume_snapshotter.go#L163

This is, currently, an unsupported capability in Velero. We do have plans of adding support for multiple cloud credentials and I am expecting this to be addressed as a side effect of that.
--

--
This is the design got supporting multiple credentials.
https://github.com/vmware-tanzu/velero/pull/2403
--

--
That metadata is specific to Azure.
The volumesnapshotter interface is designed to be a generic volume snapshotter and keeping it agnostic of kubernetes volumes. 
```
func (c *VolumeSnapshotterGRPCClient) CreateSnapshot(volumeID, volumeAZ string, tags map[string]string) (string, error) {
```
So the volumesnapshotter is given the necessary info to locate the volume and call the volume provider's snapshot API.

This, unfortunately, makes changing the interface a breaking change across all providers.
--
",cholm321,"
--
@ashish-amarnath I would think that when Azure Managed Disks are placed across several Azure Resource Groups, Velero should just lookup the `DiskURI` in the k8s meta data - no need to expose that to the velero cli.

When I do `kubectl get pvc` I can locate associated pv's by `kubectl get pv`.

Assume I have a `pv` named `pv-something`.
Then `kubectl describe pv pv-something` gives me 
```
DiskURI:      /subscriptions/xxx/resourceGroups/xxx/providers/Microsoft.Compute/disks/kubernetes-dynamic-pv-something
```

Why does Velero just not do that lookup?
--
",,,,,,,,
2973,OPEN,Custom metadata from Velero hooks,Enhancement/Dev,2021-02-22 21:34:12 +0000 UTC,yaronp68,Opened,,"**Describe the problem/challenge you have**
[A description of the current limitation/problem/challenge that you are experiencing.]
This ER is about adding the ability to write custom metadata to the backup file from hooks and plugins using an additional argument or method.

### The use case
When Velero backs-up a pod with its data it treats it as a blackbox. It only knows what K8s knows about the pod. This is a good approach that makes Velero very generic and effective. However, in the same manner that users need to perform some custom actions on a pod before backing it up (done via hooks), they sometimes need to have additional information in the backup that will help them understand what they are about to restore and what will be the implications of restoring a stateful pod.

### Here's an example: 
Our team is in charge of using Velero to backup a product named Tanzu Application Service (aka TAS)
As a central part of this task we are backing up a relational database that contains the TAS Cloud Controller data.
Kubernetes (and hence Velero) knows nothing about the database data or structure. For Velero the Cloud Controller DB is a pod with a PV attached to it.
For the user on the other hand, the information in the DB is crucial. When the user restores a db PV, the data going back into the DB will be the state of the application.
In our case the user is a platform operator providing services to application developers. In the database there is information about the names & the ownership of the applications that can help the operator understand what is about to be restored.
We wanted to have this information inside the Velero backup in order to allow for the operator to be able to know what kind of state is inside the backup of the data and once restored, to diff between the old system and the new system and see if everything had been fully restored. Because we could not do that, we wrote the information from a hook into the Velero log file which is also stored in the backup location. We had to add some user steps that are external to velero to help our user to extract the information from Velero log file.



**Describe the solution you'd like**
[A clear and concise description of what you want to happen.]
### The proposed solution
We would like to propose the following changes to Velero:

1. An addition to the backup file structure for storing custom metadata
2. Additional mechanisms for writing the custom metadata
3. Additional mechanisms for reading the custom metadata

### Storing the metadata: Changes to the Velero Backup File 
Velero backup file format is described  [here](https://velero.io/docs/v1.4/output-file-format/)

Currently, each backup file is stored in its own subdirectory in the bucket specified in the Velero server configuration. 
This subdirectory includes an additional file called velero-backup.json. 
The JSON file lists all information about your associated Backup resource, including any default values.

We propose one of the following options:

- Adding a custom section to the JSON file
- Alternatively adding a custom text file with structure that is known for the parsing client for each resource that is backed up and wants to add custom metadata 

### Option 1: Additional section to the JSON (velero-backup.json)
**Pros:**

- No additional files
- Small change to the velero codebase to get this to work

**Cons:**

- The metadata should probably be limited in size and escaped so it does not break the JSON
- Metadata is not local to particular resources. Instead, metadata is always stored at the granularity of a whole backup.
- Without context, this additional key in the json file with seemingly arbitrary data may not make a lot of sense and be confusing.


### Option 2: Separate text file per resource
**Pros:**

- Can have any content (won’t be parsed by anything)
- No size limitation 	
- Metadata can be stored at the same granularity as hooks

**Cons:**

- Slightly more complex logic 

### Writing the metadata
**Changes to Velero Hooks**
Velero backup hooks are described [here](https://velero.io/docs/v1.4/hooks/). In particular, note the pre.hook.backup.velero.io/command and post.hook.backup.velero.io/command hooks. These can be extended to offer the hook author the opportunity to write metadata into a file.

Velero should set the environment variable `VELERO_BACKUP_METADATA_FILE` to the full path to a file into which the hook command can choose to write metadata. If the hook command terminates successfully, and if the file has been written to, then the contents of that file should be written to the metadata storage for this resource.

**Changes to Velero Plugins**
[This github issue](https://github.com/vmware-tanzu/velero/issues/2483) suggests one option for updating Velero plugins -- but makes some assumptions about where the data will ultimately live. If the data lives in the backup object, then we only need to make that object writable from within the plugin.

If the data lives elsewhere, we may need to introduce a new (optional?) parameter to the backup item plugin method

### Reading the metadata: changes to Velero Describe
We propose that `velero describe backups` gain an additional flag `-c`, which will show custom metadata for whichever resource is being described.

This could be used in conjunction with the existing `-l` selector flag as follows:

`velero describe backups -c -l my-deployment`






**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`):
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,nrb,"
--
Hi @yaronp68, I think this is a reasonable request and certainly something we need to work with your team on.

Right now, we do have a `plugins` directory in the backup storage location that plugins can write to that the vSphere plugin is leveraging. I don't think this will be the entirety of the solution, but perhaps at least part of it.

I did some searching through our docs and it looks like while we implemented this, we didn't document it, so I'll file an issue to get it documented in our custom plugins page.
--

--
Re-reading the issue, the `plugins` directory isn't sufficient for what you're asking.

Changes to the `backup.json` file will be at the CRD level - so we'll have to se if we can put arbitrary data there.
--

--
@xing-yang and @lintongj FYI, since I know you're interested in the output for `velero describe` too. We should make sure whatever we implement for both the vSphere plugin and TAS.
--
",michmike,"
--
we are prioritizing this work for 1.7 release of Velero.
after 1.6 is tied down, @nrb will reply with some of the guard rails of this implementation for feedback.
--
",dsu,"
--
Moving this forward to 2.0.  I think this should be handled by TAS presenting an Astrolabe snapshot API.  Diffs between snapshots is something that is TAS specific and could be handled by a TAS utility, this is not a core Velero function.  We'll be able to add any custom data that you like as part of the Astrolabe snapshot metadata (https://github.com/vmware-tanzu/astrolabe)
--
",,,,,,
2971,OPEN,"Restic restore stuck InProcess while restoring PV with ""volumeBindingMode: WaitForFirstConsumer"" storage class",Bug; Restic; Restic - GA,2021-02-03 17:40:55 +0000 UTC,invidian,Opened,,"**What steps did you take and what happened:**
1. Create storage class with `volumeBindingMode: Immediate`.
2. Create sample workload with PV using this storage class.
3. Write some data to it.
4. Take a backup.
5. Remove namespace with sample workload.
6. Restore a backup.

What's happening then:
- `Restore` object is stuck forever (or very long) in `InProgress` state
- workload starts, but no init container gets injected, so no data has been restored.
- Velero logs says restic restore action has run.
- `PodVolumeRestore` object gets created, but it's state is never updated.
- `Restic` logs does not say anything on `info` log level.

**What did you expect to happen:**
Data in PV should be restored from backup.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

- `kubectl logs deployment/velero -n velero`
- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
- `velero backup logs <backupname>`
- `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
- `velero restore logs <restorename>`


**Anything else you would like to add:**

As a workaround, one can create a copy of used storageclass and use https://velero.io/docs/v1.5/restore-reference/#changing-pvpvc-storage-classes feature to use modified one, which has `volumeBindingMode: Immediate`. 

Volumes are being provisioned using https://github.com/hetznercloud/csi-driver.

Restoring volumes with `volumeBindingMode: Immediate` works well.

**Environment:**

- Velero version (use `velero version`): Tried `1.4.2` and `1.5.1` with the same result
- Velero features (use `velero client config get features`): `features: <NOT SET>`
- Kubernetes version (use `kubectl version`):
  ```
  Client Version: version.Info{Major:""1"", Minor:""19"", GitVersion:""v1.19.2"", GitCommit:""f5743093fd1c663cb0cbc89748f730662345d44d"", GitTreeState:""archive"", BuildDate:""2020-09-18T18:46:38Z"", 
  GoVersion:""go1.15.2"", Compiler:""gc"", Platform:""linux/amd64""}
  Server Version: version.Info{Major:""1"", Minor:""19"", GitVersion:""v1.19.2"", GitCommit:""f5743093fd1c663cb0cbc89748f730662345d44d"", GitTreeState:""clean"", BuildDate:""2020-09-16T13:32:58Z"", 
  GoVersion:""go1.15"", Compiler:""gc"", Platform:""linux/amd64""}
  ```
- Kubernetes installer & version: Flexkube v0.4.3
- Cloud provider or hardware configuration: hcloud
- OS (e.g. from `/etc/os-release`): Flatcar stable


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,nrb,"
--
Thanks for this report. I think I see the issue, and it's an order of operations one - Velero is trying to recreate the PV, PVC, and Pod (in that order), but when in a `WaitForFirstConsumer` binding mode, this isn't sufficient.

I'm going to log this as a high priority bug, because it's not a unique use case, but I don't have an answer for it at the moment.
--
",dsu,"
--
We've been looking at this as well for other use cases.  A long term solution would be to use the proposed Data Populators (https://github.com/kubernetes/enhancements/issues/1495) but this will require changes in how Restic is handled.
--
",,,,,,,,
2963,OPEN,Random restic failures,Restic,2020-09-28 09:04:31 +0000 UTC,foosinn,In progress,,"**What steps did you take and what happened:**

We have setup Velero 1.4 in our Openshift Cluster. The majority of our Backups work fine. Sometimes the restic backup fails taking almost no time. This happens randomly over all our PVCs.

**What did you expect to happen:**

Consistent backup results


**The output of the following commands will help us better understand what's going on**:

Velero Backup Logs
https://paste.kube.f2o.io/raw/F6ko42DoVWLMo6LjTqUog

**Anything else you would like to add:**

Is there a way to see the full output of the restic command?

**Environment:**

- Velero version (use `velero version`): 
```
$ velero version
Client:
	Version: v1.4.2
	Git commit: 56a08a4d695d893f0863f697c2f926e27d70c0c5
Server:
	Version: v1.4.0
```
- Velero features (use `velero client config get features`): 
```
$ velero client config get features
features: <NOT SET>
```
- Kubernetes version (use `kubectl version`):
```
$ oc version
oc v3.11.0+0cbc58b
kubernetes v1.11.0+d4cacc0
```



**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,nrb,"
--
More detailed output for the restic logs would be in the pods on the node that the restic job ran on.

Velero's implementation of restic support is in a DaemonSet, with pods on each node so that the restic pods can access all workload volumes on that node. Right now, the logs for each of those pods is on the node, and not aggregated. So finding the node is a matter of looking for the failed `PodVolumeBackups` and the relevant pod, and inspecting the pod for it's node name, then finding the restic pod on that node and looking at the `kubectl logs <restic pod name> -n velero`.
--
",foosinn,"
--
I was unable to find any hints why the backup failed in the output. Any idears?

Thanks a lot for your help!

Restic pod log:

```
time=""2020-09-26T22:08:25Z"" level=error msg=""Error running command=restic backup --repo=***HIDDEN*** --password-file=/tmp/velero-restic-credentials-ot-richcoon666888759 --cache-dir=/scratch/.cache/restic . --tag=ns=ot-richcoon --tag=pod=ot-richcoon-mariadb-0 --tag=pod-uid=6c04f2a8-fc15-11ea-a0e6-9cdc71c0dd8e --tag=pvc-uid=8e0fd405-adf2-11e9-bacf-9cdc71c0df4c --tag=volume=data --tag=backup=daily-20200926220020 --tag=backup-uid=ab6e46ab-0043-11eb-a0e6-9cdc71c0dd8e --host=velero --json --parent=66d571c2, stdout={\""message_type\"":\""status\"",\""percent_done\"":0,\""total_files\"":1,\""total_bytes\"":2752740}\n{\""message_type\"":\""status\"",\""percent_done\"":0.029610298414193936,\""total_files\"":101,\""files_done\"":4,\""total_bytes\"":129750229,\""bytes_done\"":3841943,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""percent_done\"":0.07830972577420471,\""total_files\"":129,\""files_done\"":4,\""total_bytes\"":153017443,\""bytes_done\"":11982754,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.1062338167551264,\""total_files\"":129,\""files_done\"":4,\""total_bytes\"":153017443,\""bytes_done\"":16255627,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.15110612585520725,\""total_files\"":129,\""files_done\"":4,\""total_bytes\"":153017443,\""bytes_done\"":23121873,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.172560523050957,\""total_files\"":129,\""files_done\"":4,\""total_bytes\"":153017443,\""bytes_done\"":26404770,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.2800502031653999,\""total_files\"":129,\""files_done\"":4,\""total_bytes\"":153017443,\""bytes_done\"":42852566,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.30185872338750297,\""total_files\"":129,\""files_done\"":4,\""total_bytes\"":153017443,\""bytes_done\"":46189650,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.3775959581287736,\""total_files\"":129,\""files_done\"":4,\""total_bytes\"":153017443,\""bytes_done\"":57778768,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.41558080407865655,\""total_files\"":129,\""files_done\"":4,\""total_bytes\"":153017443,\""bytes_done\"":63591112,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.4479426897755702,\""total_files\"":129,\""files_done\"":4,\""total_bytes\"":153017443,\""bytes_done\"":68543045,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.5027193860506478,\""total_files\"":129,\""files_done\"":4,\""total_bytes\"":153017443,\""bytes_done\"":76924835,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.5354016469873961,\""total_files\"":129,\""files_done\"":4,\""total_bytes\"":153017443,\""bytes_done\"":81925791,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.5838848581465317,\""total_files\"":129,\""files_done\"":4,\""total_bytes\"":153017443,\""bytes_done\"":89344568,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.6193810139671462,\""total_files\"":129,\""files_done\"":4,\""total_bytes\"":153017443,\""bytes_done\"":94776099,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.673913692310229,\""total_files\"":129,\""files_done\"":4,\""total_bytes\"":153017443,\""bytes_done\"":103120550,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.6759880963374875,\""total_files\"":129,\""files_done\"":5,\""total_bytes\"":153017443,\""bytes_done\"":103437970,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ib_logfile1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":1,\""seconds_remaining\"":8,\""percent_done\"":0.8421503553683092,\""total_files\"":129,\""files_done\"":36,\""total_bytes\"":153017443,\""bytes_done\"":128863694,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ibdata1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":2,\""percent_done\"":0.8451642862702914,\""total_files\"":129,\""files_done\"":47,\""total_bytes\"":153017443,\""bytes_done\"":129324878,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ibdata1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":2,\""percent_done\"":0.8451642862702914,\""total_files\"":129,\""files_done\"":48,\""total_bytes\"":153017443,\""bytes_done\"":129324878,\""current_files\"":[\""/data/ib_logfile0\"",\""/data/ibdata1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":2,\""percent_done\"":0.8489080947457736,\""total_files\"":129,\""files_done\"":103,\""total_bytes\"":153017443,\""bytes_done\"":129897746,\""current_files\"":[\""/data/ibdata1\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":3,\""percent_done\"":0.9927619166920728,\""total_files\"":129,\""files_done\"":125,\""total_bytes\"":153017443,\""bytes_done\"":151909890,\""current_files\"":[\""/data/ibdata1\"",\""/data/richcoon/sessions.ibd\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":3,\""percent_done\"":0.9927619166920728,\""total_files\"":129,\""files_done\"":126,\""total_bytes\"":153017443,\""bytes_done\"":151909890,\""current_files\"":[\""/data/ibdata1\"",\""/data/richcoon/sessions.ibd\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":3,\""percent_done\"":1,\""total_files\"":129,\""files_done\"":129,\""total_bytes\"":153017443,\""bytes_done\"":153017443,\""current_files\"":[\""/full_dump.gz\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":4,\""percent_done\"":1,\""total_files\"":129,\""files_done\"":129,\""total_bytes\"":153017443,\""bytes_done\"":153017443}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":5,\""percent_done\"":1,\""total_files\"":129,\""files_done\"":129,\""total_bytes\"":153017443,\""bytes_done\"":153017443}\n{\""message_type\"":\""status\"",\""second_new\"":0,\""files_changed\"":7,\""filesotal_files\"":129,\""files_done\"":129,\""total_bytes\"":153017443,\""bytes_done\"":153017443}\ncurrent_files\"":[\""/full_dump.gz\""]}\n{\""message_type\"":\""status\"",\""second_new\"":0,\""files_changed\"":7,\""filesotal_files\"":129,\""files_done\"":129,\""total_bytes\"":153017443,\""bytes_done\"":153017443}\ncurrent_files\"":[\""/full_dump.gz\""]}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":4,\""percent_done\"":1,\""total_files\"":129,\""files_done\"":129,\""total_bytes\"":153017443,\""{\""message_type\"":\""status\"",\""seconds_elapsed\"":5,\""percent_done\"":1,\""total_files\"":129,\""files_done\"":129,\""total_bytes\"":153017443,\""bytes_done\"":153017443}\n{\""message_type\"":\""status\"",\""seconds_elapsed\"":4,\""percent_done\"":1,\""total_files\"":129,\""files_done\"":129,\""total_bytes\"":153017443,\""{\""message_type\"":\""status\"",\""seconds_elapsed\"":5,\""percent_done\"":1,\""total_files\"":129,\""files_done\"":129,\""total_bytes\"":153017443,\""bytes_done\"":153017443}\n, stderr="" backup=velero/daily-20200926220020 controller=pod-volume-backup error=""unable to find summary in restic backup command output"" error.file=""/github.com/vmware-tanzu/velero/pkg/restic/exec_commands.go:175"" error.function=github.com/vmware-tanzu/velero/pkg/restic.getSummaryLine logSource=""pkg/controller/pod_volume_backup_controller.go:283"" name=daily-20200926220020-nwgvb namespace=velero
```

Pretty stdout:

```json
{
  ""message_type"": ""status"",
  ""percent_done"": 0,
  ""total_files"": 1,
  ""total_bytes"": 2752740
}
{
  ""message_type"": ""status"",
  ""percent_done"": 0.029610298414193936,
  ""total_files"": 101,
  ""files_done"": 4,
  ""total_bytes"": 129750229,
  ""bytes_done"": 3841943,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""percent_done"": 0.07830972577420471,
  ""total_files"": 129,
  ""files_done"": 4,
  ""total_bytes"": 153017443,
  ""bytes_done"": 11982754,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.1062338167551264,
  ""total_files"": 129,
  ""files_done"": 4,
  ""total_bytes"": 153017443,
  ""bytes_done"": 16255627,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.15110612585520725,
  ""total_files"": 129,
  ""files_done"": 4,
  ""total_bytes"": 153017443,
  ""bytes_done"": 23121873,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.172560523050957,
  ""total_files"": 129,
  ""files_done"": 4,
  ""total_bytes"": 153017443,
  ""bytes_done"": 26404770,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.2800502031653999,
  ""total_files"": 129,
  ""files_done"": 4,
  ""total_bytes"": 153017443,
  ""bytes_done"": 42852566,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.30185872338750297,
  ""total_files"": 129,
  ""files_done"": 4,
  ""total_bytes"": 153017443,
  ""bytes_done"": 46189650,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.3775959581287736,
  ""total_files"": 129,
  ""files_done"": 4,
  ""total_bytes"": 153017443,
  ""bytes_done"": 57778768,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.41558080407865655,
  ""total_files"": 129,
  ""files_done"": 4,
  ""total_bytes"": 153017443,
  ""bytes_done"": 63591112,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.4479426897755702,
  ""total_files"": 129,
  ""files_done"": 4,
  ""total_bytes"": 153017443,
  ""bytes_done"": 68543045,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.5027193860506478,
  ""total_files"": 129,
  ""files_done"": 4,
  ""total_bytes"": 153017443,
  ""bytes_done"": 76924835,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.5354016469873961,
  ""total_files"": 129,
  ""files_done"": 4,
  ""total_bytes"": 153017443,
  ""bytes_done"": 81925791,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.5838848581465317,
  ""total_files"": 129,
  ""files_done"": 4,
  ""total_bytes"": 153017443,
  ""bytes_done"": 89344568,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.6193810139671462,
  ""total_files"": 129,
  ""files_done"": 4,
  ""total_bytes"": 153017443,
  ""bytes_done"": 94776099,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsedparse error: Invalid numeric literal at line 28, column 14
"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.673913692310229,
  ""total_files"": 129,
  ""files_done"": 4,
  ""total_bytes"": 153017443,
  ""bytes_done"": 103120550,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.6759880963374875,
  ""total_files"": 129,
  ""files_done"": 5,
  ""total_bytes"": 153017443,
  ""bytes_done"": 103437970,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ib_logfile1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 1,
  ""seconds_remaining"": 8,
  ""percent_done"": 0.8421503553683092,
  ""total_files"": 129,
  ""files_done"": 36,
  ""total_bytes"": 153017443,
  ""bytes_done"": 128863694,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ibdata1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 2,
  ""percent_done"": 0.8451642862702914,
  ""total_files"": 129,
  ""files_done"": 47,
  ""total_bytes"": 153017443,
  ""bytes_done"": 129324878,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ibdata1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 2,
  ""percent_done"": 0.8451642862702914,
  ""total_files"": 129,
  ""files_done"": 48,
  ""total_bytes"": 153017443,
  ""bytes_done"": 129324878,
  ""current_files"": [
    ""/data/ib_logfile0"",
    ""/data/ibdata1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 2,
  ""percent_done"": 0.8489080947457736,
  ""total_files"": 129,
  ""files_done"": 103,
  ""total_bytes"": 153017443,
  ""bytes_done"": 129897746,
  ""current_files"": [
    ""/data/ibdata1""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 3,
  ""percent_done"": 0.9927619166920728,
  ""total_files"": 129,
  ""files_done"": 125,
  ""total_bytes"": 153017443,
  ""bytes_done"": 151909890,
  ""current_files"": [
    ""/data/ibdata1"",
    ""/data/richcoon/sessions.ibd""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 3,
  ""percent_done"": 0.9927619166920728,
  ""total_files"": 129,
  ""files_done"": 126,
  ""total_bytes"": 153017443,
  ""bytes_done"": 151909890,
  ""current_files"": [
    ""/data/ibdata1"",
    ""/data/richcoon/sessions.ibd""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 3,
  ""percent_done"": 1,
  ""total_files"": 129,
  ""files_done"": 129,
  ""total_bytes"": 153017443,
  ""bytes_done"": 153017443,
  ""current_files"": [
    ""/full_dump.gz""
  ]
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 4,
  ""percent_done"": 1,
  ""total_files"": 129,
  ""files_done"": 129,
  ""total_bytes"": 153017443,
  ""bytes_done"": 153017443
}
{
  ""message_type"": ""status"",
  ""seconds_elapsed"": 5,
  ""percent_done"": 1,
  ""total_files"": 129,
  ""files_done"": 129,
  ""total_bytes"": 153017443,
  ""bytes_done"": 153017443
}
{
  ""message_type"": ""status"",
  ""second_new"": 0,
  ""files_changed"": 7,
  ""filesotal_files"": 129,
  ""files_done"": 129,
  ""total_bytes"": 153017443,
  ""bytes_done"": 153017443
}
```
--
",,,,,,,,
2958,OPEN,Not able to restore EFS volumes using restic,Area/Cloud/AWS; Bug; Restic - GA; Volumes,2021-03-26 13:02:10 +0000 UTC,vaishali-prophecy,Opened,,"Backup is completed successfully as can be seen here - 
```Restic Backups:
  Completed:
    xyz/app-xyz-64b9cdb6cb-tbbn8: app-common-volume, app-log-volume```

But while restore, the PVCs are stuck in pending state - 

`Name:          app-common-volume-pvc
Namespace:     xyz
StorageClass:  efs-sc
Status:        Pending
Volume:
Labels:        prophecy.io/cluster=prophecydev
               prophecy.io/component=app
               prophecy.io/controlplane=xyz
               prophecy.io/logconfmapversion=1659308
               velero.io/backup-name=xyz-backup2
               velero.io/restore-name=xyz-backup2-20200921165054
Annotations:   volume.beta.kubernetes.io/storage-provisioner: efs.csi.aws.com
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:
Access Modes:
VolumeMode:    Filesystem
Mounted By:    app-xyz-64b9cdb6cb-tbbn8
Events:
  Type    Reason                Age                  From                         Message
  ----    ------                ----                 ----                         -------
  Normal  ExternalProvisioning  64s (x202 over 51m)  persistentvolume-controller  waiting for a volume to be created, either by external provisioner ""efs.csi.aws.com"" or manually created by system administrator`

Not sure why it doesnt get created on its own when PV  resource itself is part of backup",,,nrb,"
--
@vaishali-prophecy Thanks for the report, that is definitely unusual.

Can you provide the YAML of the PV resource and the PV's storage class?

Also, what version of Velero and Kubernetes are you using?
--

--
@vaikas Please realize that we're trying to get to as many requests as we can, but we're not able to get to them all immediately.

Also, I'm not sure what `RCA'd` means in this context.

Are you using restic to back up this volume? If using restic on a volume with a `delete` ReclaimPolicy, Velero will not restore the PV object itself, but rather dynamically provision an empty volume to restore the data into. Then, the pod is restored and Velero uses an initContainer inside of the pod to do the restic restore.

We may be seeing an order of operations issue here if it's restic, and this is probably something we need to look into.
--

--
Based on the message `waiting for a volume to be created, either by external provisioner ""efs.csi.aws.com""`, the EFS CSI driver hasn't yet created the volume.

Doing some Googling, as I'm not familiar with the driver, I came across [their documentation](https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html) which states

> Only static volume provisioning is supported. This means that an Amazon EFS file system needs to be created outside of Amazon EKS before being used by pods in your cluster.

Docs in https://github.com/kubernetes-sigs/aws-efs-csi-driver state this, too. So it appears AWS has changed how EFS is provisioned and we need to revisit how it's approached in Velero; this is going to require some research on our part.
--
",vaishali,"
--
Hi @nrb ,

Yaml for the PV resource - 

```
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    pv.kubernetes.io/bound-by-controller: ""yes""
  finalizers:
  - kubernetes.io/pv-protection
  labels:
    prophecy.io/cluster: prophecydev
    prophecy.io/component: efs
    pvselector: app-common-volume-aaa
    velero.io/backup-name: aaa-backup
  name: app-app-common-volume-aaa
spec:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 5Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: app-common-volume-pvc
    namespace: aaa
  csi:
    driver: efs.csi.aws.com
    volumeHandle: fs-e2900360:/aaa/app/app/common
  persistentVolumeReclaimPolicy: Delete
  storageClassName: efs-sc
  volumeMode: Filesystem
```

Yaml for storage class - 

```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    meta.helm.sh/release-name: vish
    meta.helm.sh/release-namespace: prophecy
  creationTimestamp: ""2020-09-17T15:35:23Z""
  labels:
    app.kubernetes.io/managed-by: Helm
  name: efs-sc
  resourceVersion: ""388862""
  selfLink: /apis/storage.k8s.io/v1/storageclasses/efs-sc
  uid: df3f7a5b-55e0-4e73-ba35-236c6b9d06d2
provisioner: efs.csi.aws.com
reclaimPolicy: Delete
volumeBindingMode: Immediate
```

Version of velero - 1.4
K8s version - 1.16

Note that PV resource itself isn't restored on restore action  for the PVCs which are marked in pod annotation.
Also just shared PV yaml corresponding to some other deployment as the previous one doesnt exist as of now.
--

--
@nrb Hey could you figure out the issue or any other information is needed?
--

--
@nrb Any updates on this?
--

--
@nrb  Could it be RCA'ed?
--

--
@nrb @ncdc @carlisia @jwhitcraft ?
--

--
@nrb Yes I am using restic. I dont see any dynamically provisioned volume and with ReclaimPolicy as Retain, I see other issues being reported. So what kind of EFS volumes can we actually restore with restic?

I think this is the one where Retain policy also has issues - https://github.com/vmware-tanzu/velero/issues/1151

--

--
@nrb So to fix problem for now, in case the policy is Retain, how can i go about it? Or you are saying that same is problem with PVs with Retain policy. I am not sure how those volumes are handled by restic.
--

--
Looks like as per https://github.com/vmware-tanzu/velero/pull/1713, PVs are dynamically reprovisioned irrespective of ReclaimPolicy. Am i right?
--
",vaikas,"
--
@nrb I assume I was mentioned here by accident? If not, is there something that I need to do here?

https://github.com/vmware-tanzu/velero/issues/2958#issuecomment-701496524
--
",headyj,"
--
I'm also facing the same problem. @vaishali-prophecy as a workaround, you can create the PV by hand, just be sure to correctly setup the PV name, PVC namespace and name, as well as volumeHandle parts (for example using aws efs):
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: <pv-name>
spec:
  capacity:
    storage: 1Gi
  csi:
    driver: efs.csi.aws.com
    volumeHandle: 'fs-XXXXXXXX::fsap-XXXXXXXXXXXXXXXXX'
  accessModes:
    - ReadWriteMany
  claimRef:
    kind: PersistentVolumeClaim
    namespace: <claim-namespace>
    name: <claim-name>
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-storage-class
  volumeMode: Filesystem
```

Then you can run the restore process and everything should work fine.
--
",,,,
2957,OPEN,Add option to define custom tags in Schedule/Backup.,Enhancement/User; Needs Product,2021-02-26 14:57:50 +0000 UTC,BartoszZawadzki,Opened,,"**Describe the problem/challenge you have**
[A description of the current limitation/problem/challenge that you are experiencing.]

Currently velero adds predefined tags to EBS snapshots. These cannot be changed and no user-specific tags can be added.

**Describe the solution you'd like**
[A clear and concise description of what you want to happen.]

We would like to have the ability to specify tags in Schedule/Backup objects that would then be added to EBS snapshots upon creation:

`apiVersion: velero.io/v1
  kind: Schedule
  metadata:
    name: a
    namespace: velero
spec:
  schedule: 0 7 * * *
  template:
    additionalTags:
      - 'KEY1: VALUE1'
      - 'KEY2: VALUE2'`

**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]

Discussed (briefly) https://kubernetes.slack.com/archives/C6VCGP4MT/p1544209824358200

**Environment:**

- Velero version (use `velero version`): Version: v1.3.2
- Kubernetes version (use `kubectl version`): v1.18.8
- Kubernetes installer & version: kops 1.18.0
- Cloud provider or hardware configuration: AWS
- OS (e.g. from `/etc/os-release`): Flatcar Linux

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,nrb,"
--
@BartoszZawadzki Thanks for this request - I think this is definitely an interesting feature that we should look into supporting more generally for any `VolumeSnapshotter` plugin. I don't know that tags are universal across all snapshots on all platforms, but based on how Velero is architected, this would be best implemented as a plugin interface feature.

I've marked it as a feature enhancement and for consideration by our product team, as well as moved it into our backlog.
--
",jinoa,"
--
Is there by any chance any traction on this? 
--
",,,,,,,,
2938,OPEN,Log level inconsistency in v1.5,Area/CLI,2020-10-22 15:24:22 +0000 UTC,davemazur,In progress,,"Previous versions (<v1.5) used debug, info, warning and error  strings when setting the log level via:
% velero version -v {debug, info, warning, error}

This is also the understanding from the team - see below.

![Velero-log-error-slack](https://user-images.githubusercontent.com/16963783/93343137-24d64f00-f7fe-11ea-9b46-ef1150f2febb.png)

In 1.5, the help is looking for a number:
% velero help
  -v, --v Level                          number for the log level verbosity

If I enter:
% velero version -v debug

I get the following error which shows it is trying to convert from string to int:
An error occurred: invalid argument ""debug"" for ""-v, --v"" flag: strconv.ParseInt: parsing ""debug"": invalid syntax

My thoughts on the issue are:
1) revert back to debug, info, warning, error strings in the code and modify help
2) modify help to define a  min/max/default integer log level and what level corresponds to debug, info, warning, error (i.e. level 0=info, level 10 = debug, default level = x, etc)

I did the following and did not notice a change in log level from 0-5.  I noticed an increase at level 6 and also at level 10.

![Velero-log-level-error](https://user-images.githubusercontent.com/16963783/93345560-f73ed500-f800-11ea-8612-36ff0c343433.png)


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,nrb,"
--
Thanks for the report!

To change the log level, you need to edit the Kubernetes deployment containing the Velero server; it can't be directly changed from the velero client.

See https://velero.io/docs/latest/troubleshooting/#getting-velero-debug-logs for changing it.

Still, I think we need to change the message that the help displays to be more accurate.
--

--
Oh, I read this too quickly; this is about `-v`, not `--log-level`...hmm.

Yeah, we should still be more explicit.
--
",ashish,"
--
@davemazur In Velero we use the `""github.com/sirupsen/logrus""` logging package and not something like `github.com/golang/glog` where the logging verbosity is controlled by the `--v NUMBER` flag. In the `logrus` package, the logging verbosity is controlled using the `--log-level` flag.
```
      --log-level                                           the level at which to log. Valid values are trace, debug, info, warning, error, fatal, panic. (default info)
```
Also, this flag is available only for the `velero server` sub-command.

We don't currently have flags to control the logging verbosity from the Velero client, i.e. all the log messages are printed.
The `--v` or the `-v` flag that you are referring to may be used by the packages that Velero calls into, for example the kubernetes client-go package. The logs that are seen in the screenshot is from the client-go package.
The logging levels controlled by the `--v` using numbers 1-10, don't directly map to the logging levels used by `logrus`.

Also it is worth noting that this is not a change introduced in Velero `v1.5.1`. For that reason, I don't think there is any inconsistency here from Velero's per se. It is possible that some of our dependencies may have tuned their logging verbosity. 

Can you please tell us what is the problem you are trying to troubleshoot? This will help us in better assisting you.
--
",davemazur,"
--
@ashish-amarnath Thanks for the explanation.  It is confusing that when you do a ""velero help"", you see the option for ""-v Level"" and as you stated, this does not control the logging for velero but for other packages that velero call into.  Maybe remove the ""-v"" option from the help?  Or leave it and modify the help to state something like ""number for the log level verbosity (1-10) of non-velero packages"".  
--
",,,,,,
2935,OPEN,General timeline for a release,Area/Documentation,2020-09-25 17:29:47 +0000 UTC,ashish-amarnath,Opened,,"Following up on the discussion on the community meeting on 9/15.

Asking the below questions:
1. What is the general timeline for a release, from a pre-release to a GA release
1. Should we continue shipping pre-release beta (one or more as needed) followed by a GA release or should we ship pre-release versions, followed by a release candidate, and eventually a GA release?
1. As a Velero user, can I deduce the release date for a GA release from the release date of the pre-release, or the release candidate release?
1. Will this cadence and process be applicable to the plugins as well? In the past, there have not been pre-releases or release candidates for plugins.


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,nrb,"
--
I'd like to timebox the decision for v1.6 to Friday, September 25th so we have something and go with it. That's not to say it will be perfect or can't change in the future, I just would like to see things for that release decided by then so we can communicate it out.

My thoughts:

1. Generally, releases will be on a 3 month cycle. Pre-releases will start coming out when features are roughly complete, and are largely discretionary, but probably 1-2 months in.
2. We should release betas (and if the team deems necessary, alphas) to get early feedback from the community about features and how they work. Release candidates offer a ""feature complete"" package and would be used primarily to look for bugs. In my opinion, release candidates should not be used in production.
3. I don't think there should be only 1 pre-release, which the wording the question suggests. Pre-releases should be tools, and limiting ourselves to one is too restrictive; it becomes much harder to respond to feedback that way. However, I think our main goal for release candidates should be 1-2 weeks from RC to GA, in order to provide both time to test/react and for people to accurately predict when GA will happen.
4. Yes, I think the process should also apply to plugins to be consistent.

So my proposed timelines:

Betas when features are roughly complete, and 1 week between them.
Once an RC is created, 1-2 weeks (open to discussion here) until cutting the GA release.
--

--
Decision (to be documented):

* Betas will be cut at feature completeness, with 1 week between releases for bug fixes. The betas should be tested on a rolling basis.
* RC is created when all release blocking issues are closed.
* After RC, 1 week til GA.
* This applies to the plugins we maintain too (not the vSphere plugin)
--
",dsu,"
--
I think this is good.  Our goal should be for beta builds to be passing all of our existing tests and to add tests for new functionality prior to RC.  By the time we get to RC we should be very confident it will pass final test.  Having bugs found in the RC should not be expected, so we don't build slack into the schedule for bug fixes post-RC, but we leave ourselves the option of postponing the release if bugs are found in the RC build.  I think 1 week RC->GA is reasonable.
--
",carlisia,"
--
> Betas when features are roughly complete, and 1 week between them.
Once an RC is created, 1-2 weeks (open to discussion here) until cutting the GA release.

I think this is a good cadence.
--
",,,,,,
2928,OPEN,Add -w; --wait to velero delete backup,Enhancement/User,2020-09-15 21:32:21 +0000 UTC,ntalekt,Opened,,"**Describe the problem/challenge you have**
Working on pipelining some units tests for a Velero install. `velero create backup` has a `--wait` flag however the `velero delete backup` does not. 


**Describe the solution you'd like**
We can loop over the delete task to workaround but it would be a lot cleaner if the CLI handed a wait condition on the delete task itself. 


**Anything else you would like to add:**
Thank you.


**Environment:**

- Velero version (use `velero version`): 1.4.2
- Kubernetes version (use `kubectl version`): 1.17.8
- Kubernetes installer & version: PKS 1.7
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
2927,OPEN,Change Image Name while Restoring Deployments In Target Cluster,Needs Product,2020-10-21 17:26:17 +0000 UTC,rajivml,Opened,,"HI,

I have a use case where I have migrate an pretty big kubernetes cluster from GKE to AKS. 

The thing am not able to figure out is, on GCP we are using google container registry and all the GKE deployments points to the images located in google cloud registry.

Now before the restore, I will sync all the docker images present under GCS to Azure container registry. But at the time of restoring the backup onto AKS cluster , I have to change the image Names under deployments to point to Azure container registry instead of google container registry.

Any idea how can i achieve this ?
",,,zubron,"
--
Hi :wave: One option is to create your own [`RestoreItemAction` plugin](https://velero.io/docs/v1.5/custom-plugins/) which allows you to execute your own logic on resources as they are being restored. In this case you could modify the container registry being used on each pod/container before it is restored in your new cluster. We also have an [example repo](https://github.com/vmware-tanzu/velero-plugin-example) which shows you how to create your own plugins.

We have had some other requests though for a more general purpose way of how to easily edit resources on restore but we don't have that built into the project.
--
",,,,,,,,,,
2922,OPEN,Request to enhance plugin with Timeout,Area/Plugins; Enhancement/Dev,2020-10-19 21:15:31 +0000 UTC,phuongatemc,In progress,,"For App Consistent backup, the database application pod will be quiesce during the time Velero execute plugin (on PVC) to make snapshot of the PVC.  If for some reason, the plugin takes a long time to finish (i.e. fail to create snapshot and retry, network failure), the application pod will be in quiesce state which will block user from using the application.  This is undesirable condition.

We would like to enhance the plugin to have a timeout so that the plugin will fail if it takes more than specified timeout to execute.

- Velero version: v1.3.2
- Kubernetes version: v1.17.2 and v1.18.8
",,,nrb,"
--
Which plugin type are you referring to? VolumeSnapshotter?
--

--
This may also be relevant to https://github.com/vmware-tanzu/velero/pull/2867
--
",phuongatemc,"
--
Yes, we use VMWare VolumeSnapshotter for First Class Disk (FCD) PV.
--

--
We also plan to use BackupItem plugin in the near future.

--

--
I propose we enhance the BackupItemActionPlugin struct to contain a timeout (in seconds) and enhance BackupItemActionGRPCClient.Execute as following:
if timeout is not specify, call grpcClient.Execute directly
  res, err := c.grpcClient.Execute(context.Background(), req)
if timeout is specify, execute it in a goroutine (using context.WithCancel to terminate the goroutine if timeout occurs) and select timeout.  Roughly like the code below

ctx, cancel := context.WithCancel(context.Background())
defer cancel()

ch := make(chan Response, 1)

go func() {
   res, err := c.grpcClient.Execute(context.Background(), req)   
    select {
    default:
        ch <- res
    case <-ctx.Done():
        fmt.Println(""Canceled by timeout"")
        return
    }
}()

select {
case res <-ch:
    //handle res
case <-time.After(timeout):
    // print timeout to log
}

--

--
The Restore datapath is much more complicated because of behavior described in that #2867  I think we should consider it separately.  This should focus on the Backup datapath.
--
",,,,,,,,
2912,OPEN,The latest backup and schedule are not equal：schedule.status.lastBackup and backup.metadata.creationTimestamp,Bug,2020-09-16 01:42:55 +0000 UTC,Cweiping,In progress,,"The latest backup and schedule are not equal：

schedule.status.lastBackup and backup.metadata.creationTimestamp

I want to use this method to confirm that the current backup is up to date.
",,,nrb,"
--
I see why this is happening, and it makes sense, though I'm not 100% sure how to fix it at the moment.

The Schedule controller sets the `LastBackup` value when it submits the request for a Backup to be made. The Backup's `CreationTimestamp` is populated by the Kubernetes API server when it receives the request over the network. Thus, there is going to be some skew.

I think we need to address the goal of confirming that the latest backup from a schedule is up-to-date, though I don't know that it will necessarily be tied directly to these fields.
--
",Cweiping,"
--
> I see why this is happening, and it makes sense, though I'm not 100% sure how to fix it at the moment.
> 
> The Schedule controller sets the `LastBackup` value when it submits the request for a Backup to be made. The Backup's `CreationTimestamp` is populated by the Kubernetes API server when it receives the request over the network. Thus, there is going to be some skew.
> 
> I think we need to address the goal of confirming that the latest backup from a schedule is up-to-date, though I don't know that it will necessarily be tied directly to these fields.

Thank you for your reply. Yes, as I know. I think it's reasonable
--
",,,,,,,,
2908,OPEN,Support subPath for volume backups,Restic,2020-09-28 09:53:14 +0000 UTC,beegee1,In progress,,"**Describe the problem/challenge you have**
Velero can backup Pod volumes by specifying annotations (backup.velero.io/backup-volumes) in the Pod spec. We're using the same PV/PVC for different services except the subPath is different for each service when mounting the volume.  https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath We cannot simply specify this volume because it holds many other data.
This means, we need to copy the files which were specific to the current service to a separate backup volume (type EmptyDir) first. The backup volume is specified in the annotation. In the restore case we need to copy the data back from the backup volume.


**Describe the solution you'd like**
The possibility to specify the subPath using a Pod annotation what exactly of the volume should be included in the backup.

**Environment:**

- Velero version (use `velero version`): 1.4
- Kubernetes version (use `kubectl version`): 1.16

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,beegee1,"
--
@nrb  Can I work on this during the Hacktoberfest event (in October)? Do you think such a feature would be OK from your side? Would be great if someone could give me some hints how it should be implemented?
--
",,,,,,,,,,
2906,OPEN,Support configuring client namespace based on kubecontext,Area/CLI; Enhancement/User,2020-10-22 17:56:53 +0000 UTC,runzexia,Opened,,"**Describe the problem/challenge you have**
In our usage scenario, the name of the namespace used by each cluster to deploy velero is different. So when I switch the context, I need to manually process the configuration of velero to switch namesapce.
**Describe the solution you'd like**
[A clear and concise description of what you want to happen.]
 If velero support context-based default namespace like `map[contextName]namspaces`, we can reduce a lot of configuration


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`):
Client:
	Version: v1.4.2
- Kubernetes version (use `kubectl version`):
Server Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.5"", GitCommit:""e6503f8d8f769ace2f338794c914a96fc335df0f"", GitTreeState:""clean"", BuildDate:""2020-06-26T03:39:24Z"", GoVersion:""go1.13.9"", Compiler:""gc"", Platform:""linux/amd64""}


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,nrb,"
--
@runzexia Sorry for the long delay in response.

I can think of 2 ways to handle this as of today.

1) The Velero client acts according the current kubecontext. Using any sort of tool that manages these could help.
2) The Velero client has a configuration file that is not well documented. Using `velero client config set namespace <namespace name>` will allow you to set the namespace.

However, in terms of combining the two I could see this being useful and view it as a product request. We have the pieces to set up something like this, though.
--
",,,,,,,,,,
2900,OPEN,Improve the use of Restic in Velero and offer stable support,Epic; Restic - GA,2021-02-22 20:02:51 +0000 UTC,stephbman,Opened,,"**Background and Strategic Fit**
Restic is a third party tool open-source tool integrated with Velero that allows users to have an out-of-the box solution for backing up nearly any type of Kubernetes volume.

Because Restic is not tied to any specific storage platform, it can also be used to enable cross-volume-type data migrations. 

**Epic Description**
Enable quality of life user experience improvements for Restic and offer users more stable restic support for backup, restore, disaster recovery and data protection use cases. 

**Vote on this epic!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
2898,OPEN,Allow `velero install` to specify tolerations for restic daemonset,Needs Product,2021-03-10 15:51:01 +0000 UTC,jbmassicotte,In progress,,"Edited this earlier post of mine given the more recent info I’ve gathered.

**What I did**
- Installed velero client 1.4.2
- Created a credential file, following instructions from [velero-plugin-for-azure](https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure)
- Deployed velero server in Azure cluster using:
```
CREDENTIAL_FILE=${HOME}/credentials-velero
BLOB_CONTAINER=dcieastus2dev03cont
AZURE_BACKUP_RESOURCE_GROUP=dci-eastus2-dev-gen-rg-03
AZURE_STORAGE_ACCOUNT=dcieastus2dev03st
API_TIMEOUT=5m

velero install \
    --provider azure \
    --plugins velero/velero-plugin-for-microsoft-azure:v1.1.0 \
    --bucket $BLOB_CONTAINER \
    --secret-file $CREDENTIAL_FILE \
    --backup-location-config resourceGroup=$AZURE_BACKUP_RESOURCE_GROUP,storageAccount=$AZURE_STORAGE_ACCOUNT \
    --snapshot-location-config apiTimeout=$API_TIMEOUT,resourceGroup=$AZURE_BACKUP_RESOURCE_GROUP \
    --use-restic
```
- I am using restic because my app mounts an AzureFile volume (it also mounts 3 ManagedDisk volumes but these are supported natively by velero)
- I added the AzureFile volume name to the app pod annotation, as required by restic (`backup.velero.io/backup-volumes: <volumename>`)
- I also added the mountOptions nouser_xattr to the AzureFile storageclass, again, as required by restic
- Attempted to create a backup: `velero backup create backup1 --include-namespaces mynamespace`

**The problem**
- `velero backup describe backup1 –details` shows process stuck InProgress, no error, no warning. See attached file.
- last log from `kubectl logs deployment/velero -n velero` says 'Initializing restic repository'

**What did you expect to happen:**
The backup to complete

**Anything else you would like to add:**

- I can see in Azure Portal that velero created a folder called restic under the Azure container, so I know the container location is valid
- I tried removing the AzureFile volume name from the pod annotation, restarted velero, with the use-restic flag still on, and the backup succeeded this time, which points to restic as the culprit.
**BUT**: I also tried removing the use-restic flag (checked that the restic daemonset was not started), added the pod annotation back, and check that: the backup failed with the same ""Initializing restic repo"" condition. What's up with that!?
- I am starting to believe this is a bug, so please prove me wrong

**Environment:**

```
$ kubectl version --short
Client Version: v1.15.10
Server Version: v1.17.9
$ velero client config get features
features: <NOT SET>
$ velero version
Client:
        Version: v1.4.2
        Git commit: 56a08a4d695d893f0863f697c2f926e27d70c0c5
Server:
        Version: v1.4.2
```
[create-backup.txt](https://github.com/vmware-tanzu/velero/files/5210944/create-backup.txt)
",,,jbmassicotte,"
--
We figured out our problem: our cluster is composed of 3 nodepools, the default, plus let’s say pool A and B. We have 2 applications, let’s say X and Y, and use ‘tolerations’ to force app X on nodepool A, and app Y on nodepool B. Because restic uses no toleration, it runs on default nodepool and fails to backup volumes from applications running on pool A and B. 

To fix the problem (temporarily), I used `kubectl edit daemonset/restic -n velero` to add the needed toleration, which forced restic to run on all cluster nodes. Subsequent backups worked.

**Questions to the Velero experts**: I need to make these changes permanent. How can I provide these changes to the ‘velero install’ command? Is there a way to provide a daemonset-restic.yaml file to ‘velero install’, and if so, where can I find the default file which I will use to add the toleration config?

--

--
I ended up writing a script to capture the daemonset yaml config, to add the toleration to this config via a sequence of sed updates, and to invoke ‘kubectl replace’ with the updated config. It does the trick but I find that somewhat cheesy. Any solution deemed more elegant and reliable would be appreciated.
--
",JarnoRFB,"
--
@jbmassicotte In case you can use the velero helm chart instead, it is possible to specify tolerations for the daemonset there https://github.com/vmware-tanzu/helm-charts/blob/main/charts/velero/values.yaml#L269. 

As this tripped me off a bit, when trying to do a restic backup on a pod that was running on a node where no restic daemon was running I think it would be good behavior if the backup would raise an error or at least show a warning in the velero logs in this situation.
--
",,,,,,,,
2888,OPEN,Backing up resources in parallel,Needs Product; Performance,2020-10-21 18:34:16 +0000 UTC,phuongatemc,In progress,,"Currently Velero Backup processes resources in serial.  In some scenarios, we would like to back up resources in parallel, not only to increase performance but also help reduce the time gap between backup time of the items.  For example, to backup a Cassandra cluster of 20 pods (each has 1 PVC).  The Backup of such cluster woud take snapshots of PVCs belong to these Pods and to help application consistency, these PVCs should be snapshotted as close to each other as possible (either in parallel or in a single volume group, depending on what storage back-end supported).

So the enhancement request is to allow users to specify the resource types (Kind) to be backed up in parallel.  For example, we can enhance an option say ""ConcurrentResources"" and users can specify ConcurrentResources: ""pods"".  Then during backup, we will create goroutine to backup all Pods in parallel.

This feature may conflict with the ""OrderedResources"" feature which we Backup resources of specific Kind in specific ordered.  So these two ""OrderedResources"" and ""ConcurrentResources"" cannot specify the same Kind.

Another aspect can also be considered here is the level of concurrency allowed.  For example if the back end system can only allow up to 10 PVC snapshots being taken in parallel or the backup storage device can allow 10 write streams in parallel then Backup cannot create more backup goroutines than such limit.  This also raise the issue of multiple Backups in parallels and we need to factor in the limitation mentioned above when creating goroutines.

An alternative solution would be VolumeGroup that is currently proposed in the Kubernetes Data Protection Working Group.  This VolumeGroup allows grouping together related PV (so they can be snapshotted together...). ",,,phuongatemc,"
--
We ultimately want all the PVCs belong to the pods of the same application (in the same namespace) being snapshotted in parallel.  However in Velero current implementation, backup item will backup pod and its PVC, PV together before moving to the next pod, we can make it parallel at the level of pod which should be good enough because each pod usually have 1 PVC-PV.
--
",,,,,,,,,,
2883,OPEN,Improve BSL controller performance,Enhancement/User,2020-09-01 15:11:05 +0000 UTC,carlisia,Opened,,"To prevent delays in the reconciliation of BSLs, we should be running iterations of the `for i := range locationList.Items ` loop concurrently. 

For reference:
https://github.com/vmware-tanzu/velero/pull/2870#discussion_r480272792
",,,,,,,,,,,,,,
2878,OPEN,Windows container,Needs Product,2021-01-19 15:42:52 +0000 UTC,cloudcafetech,Opened,,"Now a windows container becoming popular, any plan for using in windows container ?

I assume restic has already released windows binary. 

Found some nice work for windows using restic 👍 https://github.com/kmwoley/restic-windows-backup ",,,perithompson,"
--
Has there been any progress on this?
--
",,,,,,,,,,
2877,OPEN,Backups and restores not adopted when the velero pod is restarted,Bug; P1 - Important,2020-08-28 20:18:33 +0000 UTC,nrb,Opened,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)

When the `velero` deployment pod restarts, any backup or restore action that was in progress is effectively abandoned, because the controller loops only look for `Phase: New` custom resources.

The challenge here is knowing where a backup was in its process in order to resume.

Moving to a worker pod design may help here, as the controller pods restarting will only affect grabbing work off the queue rather than the running backup/restore jobs.

**What did you expect to happen:**

The `velero` deployment restarting should not interrupt running backup and restore operations.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,,,,,,,,,,,,
2876,OPEN,Sync `Schedule` CRs into clusters,Enhancement/User,2020-09-02 04:29:16 +0000 UTC,nrb,Opened,,"**Describe the problem/challenge you have**
Currently, Velero doesn't sync `Schedule` custom resources across clusters, only the `Backup` custom resources that were generated by that `Schedule`. This can be somewhat surprising to users, and is also lossy in the event of a total cluster loss - the original schedule has to be recreated from scratch.


**Describe the solution you'd like**
The `Schedule` objects should be synced into clusters via the `BackupStorageLocation` so that users can observe them with `velero schedule get` on clusters other than the ones where the `Schedule` was original defined.


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,ashish,"
--
We should also sync `Restore` CRs.
see https://github.com/vmware-tanzu/velero/issues/2697 for more details
--
",,,,,,,,,,
2873,OPEN,Expose controller metrics,Metrics; P1 - Important,2020-08-27 16:59:59 +0000 UTC,bgagnon,Opened,,"**Describe the problem/challenge you have**
We have log entries indicating that an `Backup` object references a `BackupStorageLocation` that no longer exists:

```json
{
  ""controller"":""gc-controller"",
  ""error"":""error getting backup storage location: backupstoragelocation.velero.io \""default\"" not found"", 
  ""error.file"":""/go/src/github.com/vmware-tanzu/velero/pkg/controller/gc_controller.go:144"", 
  ""error.function"":""github.com/vmware-tanzu/velero/pkg/controller.(*gcController).processQueueItem"",
  ""key"":""runway-backup/default-20200718080037"",
  ""level"":""error"",
  ""logSource"":""pkg/controller/generic_controller.go:137"",
  ""msg"":""Error in syncHandler, re-adding item to queue"",
  ""time"":""2020-08-27T15:59:44Z""
}
```

There is no corresponding counter metric that would allow us to alert on this condition. This issue is not so much about this particular error conditions above, but a general point about the lack of metrics for internal controller behavior.

**Describe the solution you'd like**
Standard controller metrics (HTTP calls and status codes made to the K8S API) could suffice, otherwise some specialized `velero_` time series for each handled error:

- attempts
- failures

That way, a rate can be computed and a threshold can be set for alerting.",,,ashish,"
--
@bgagnon Thanks for reporting this. We need to expose the metrics of BSL availability from the BSL controller.

--
",,,,,,,,,,
2858,OPEN,Restic Microk8s Compatibility,P2 - Long-term important; Restic,2021-01-01 14:02:23 +0000 UTC,sstubbs,Opened,,"**Describe the problem/challenge you have**
I running velero on microk8s. However the restic daemonset does not work as it cannot find the kubelet directory specified as microk8s has a different kubelet directory. 


**Describe the solution you'd like**
A way to configure the kubelet directory. 


**Anything else you would like to add:**
Currently after installation I'm running:
```
kubectl edit daemonset -n velero restic
```
and then changing the path manually like the following:
```
 spec:
      volumes:
        - name: host-pods
          hostPath:
            path: /var/snap/microk8s/common/var/lib/kubelet/pods
            type: ''
```


**Environment:**

- Velero version (use `velero version`):
```
Client:
	Version: v1.4.2
	Git commit: -
Server:
	Version: v1.4.2
```
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.8"", GitCommit:""9f2892aab98fe339f3bd70e3c470144299398ace"", GitTreeState:""clean"", BuildDate:""2020-08-14T11:09:22Z"", GoVersion:""go1.14.7"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""19+"", GitVersion:""v1.19.0-rc.4"", GitCommit:""1afc53514032a44d091ae4a9f6e092171db9fe10"", GitTreeState:""clean"", BuildDate:""2020-08-04T23:09:31Z"", GoVersion:""go1.14.6"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kubernetes installer & version:
```
microk8s
```
- Cloud provider or hardware configuration:
```
On Premise Server
```
- OS (e.g. from `/etc/os-release`):
```
Ubuntu 20.04
```
**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,spasche,"
--
Something else that will prevent MicroK8s compatibility is that the storage add-on [uses hostPath volumes](https://github.com/ubuntu/microk8s/blob/83f24a689c57b1c729c72cf0d4d6df82906e8be6/microk8s-resources/actions/storage.yaml#L13) which are [not currently compatible with restic](https://github.com/vmware-tanzu/velero/blame/be5fbb00ead1999345d7da559473edf94eaf1d05/site/content/docs/v1.5/restic.md#L19).

I believe it should be possible to use the [local-path-provisioner](https://github.com/rancher/local-path-provisioner) instead, but I haven't tried yet.

--
",,,,,,,,,,
2856,OPEN,v1.5 CRDs are not compatible with Kubernetes v1.11 and earlier,Breaking change; Bug,2021-01-29 15:18:23 +0000 UTC,albertollamaso,In progress,,"**What steps did you take and what happened:**
Installed velero version  v1.5.0-beta.1

tried to install with:


```
velero install \
    --provider aws \
    --use-restic \
    --plugins velero/velero-plugin-for-aws:v1.1.0 \
    --bucket kubernetes.test.velero \
    --backup-location-config region=us-east-1 \
    --snapshot-location-config region=us-east-1 \
    --secret-file ./credentials-velero
```

it failed with:

`
CustomResourceDefinition/backups.velero.io: attempting to create resource
CustomResourceDefinition/backups.velero.io: already exists, proceeding
CustomResourceDefinition/backups.velero.io: created
CustomResourceDefinition/backupstoragelocations.velero.io: attempting to create resource
An error occurred:

Error installing Velero.Use `kubectl logs deploy/velero -n velero`
to check the deploy logs: Error creating resource CustomResourceDefinition / backupstoragelocations.velero.io: CustomResourceDefinition.apiextensions.k8s.io ""backupstoragelocations.velero.io""
is invalid: spec.validation.openAPIV3Schema: Invalid value: apiextensions.JSONSchemaProps {
	ID: """",
	Schema: """",
	Ref: ( * string)(nil),
	Description: ""BackupStorageLocation is a location where Velero stores backup objects"",
	Type: ""object"",
	Format: """",
	Title: """",
	Default: ( * apiextensions.JSON)(nil),
	Maximum: ( * float64)(nil),
	ExclusiveMaximum: false,
	Minimum: ( * float64)(nil),
	ExclusiveMinimum: false,
	MaxLength: ( * int64)(nil),
	MinLength: ( * int64)(nil),
	Pattern: """",
	MaxItems: ( * int64)(nil),
	MinItems: ( * int64)(nil),
	UniqueItems: false,
	MultipleOf: ( * float64)(nil),
	Enum: [] apiextensions.JSON(nil),
	MaxProperties: ( * int64)(nil),
	MinProperties: ( * int64)(nil),
	Required: [] string(nil),
	Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
	AllOf: [] apiextensions.JSONSchemaProps(nil),
	OneOf: [] apiextensions.JSONSchemaProps(nil),
	AnyOf: [] apiextensions.JSONSchemaProps(nil),
	Not: ( * apiextensions.JSONSchemaProps)(nil),
	Properties: map[string] apiextensions.JSONSchemaProps {
		""status"": apiextensions.JSONSchemaProps {
			ID: """",
			Schema: """",
			Ref: ( * string)(nil),
			Description: ""BackupStorageLocationStatus defines the observed state of BackupStorageLocation"",
			Type: ""object"",
			Format: """",
			Title: """",
			Default: ( * apiextensions.JSON)(nil),
			Maximum: ( * float64)(nil),
			ExclusiveMaximum: false,
			Minimum: ( * float64)(nil),
			ExclusiveMinimum: false,
			MaxLength: ( * int64)(nil),
			MinLength: ( * int64)(nil),
			Pattern: """",
			MaxItems: ( * int64)(nil),
			MinItems: ( * int64)(nil),
			UniqueItems: false,
			MultipleOf: ( * float64)(nil),
			Enum: [] apiextensions.JSON(nil),
			MaxProperties: ( * int64)(nil),
			MinProperties: ( * int64)(nil),
			Required: [] string(nil),
			Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
			AllOf: [] apiextensions.JSONSchemaProps(nil),
			OneOf: [] apiextensions.JSONSchemaProps(nil),
			AnyOf: [] apiextensions.JSONSchemaProps(nil),
			Not: ( * apiextensions.JSONSchemaProps)(nil),
			Properties: map[string] apiextensions.JSONSchemaProps {
				""accessMode"": apiextensions.JSONSchemaProps {
					ID: """",
					Schema: """",
					Ref: ( * string)(nil),
					Description: ""AccessMode is an unused field. \n Deprecated: there is now an AccessMode field on the Spec and this field will be removed entirely as of v2.0."",
					Type: ""string"",
					Format: """",
					Title: """",
					Default: ( * apiextensions.JSON)(nil),
					Maximum: ( * float64)(nil),
					ExclusiveMaximum: false,
					Minimum: ( * float64)(nil),
					ExclusiveMinimum: false,
					MaxLength: ( * int64)(nil),
					MinLength: ( * int64)(nil),
					Pattern: """",
					MaxItems: ( * int64)(nil),
					MinItems: ( * int64)(nil),
					UniqueItems: false,
					MultipleOf: ( * float64)(nil),
					Enum: [] apiextensions.JSON {
						""ReadOnly"",
						""ReadWrite""
					},
					MaxProperties: ( * int64)(nil),
					MinProperties: ( * int64)(nil),
					Required: [] string(nil),
					Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
					AllOf: [] apiextensions.JSONSchemaProps(nil),
					OneOf: [] apiextensions.JSONSchemaProps(nil),
					AnyOf: [] apiextensions.JSONSchemaProps(nil),
					Not: ( * apiextensions.JSONSchemaProps)(nil),
					Properties: map[string] apiextensions.JSONSchemaProps(nil),
					AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
					Dependencies: apiextensions.JSONSchemaDependencies(nil),
					AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					Definitions: apiextensions.JSONSchemaDefinitions(nil),
					ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
					Example: ( * apiextensions.JSON)(nil)
				},
				""lastSyncedRevision"": apiextensions.JSONSchemaProps {
					ID: """",
					Schema: """",
					Ref: ( * string)(nil),
					Description: ""LastSyncedRevision is the value of the `metadata/revision` file in the backup storage location the last time the BSL's contents were synced into the cluster. \n Deprecated: this field is no longer updated or used for detecting changes to the location's contents and will be removed entirely in v2.0."",
					Type: ""string"",
					Format: """",
					Title: """",
					Default: ( * apiextensions.JSON)(nil),
					Maximum: ( * float64)(nil),
					ExclusiveMaximum: false,
					Minimum: ( * float64)(nil),
					ExclusiveMinimum: false,
					MaxLength: ( * int64)(nil),
					MinLength: ( * int64)(nil),
					Pattern: """",
					MaxItems: ( * int64)(nil),
					MinItems: ( * int64)(nil),
					UniqueItems: false,
					MultipleOf: ( * float64)(nil),
					Enum: [] apiextensions.JSON(nil),
					MaxProperties: ( * int64)(nil),
					MinProperties: ( * int64)(nil),
					Required: [] string(nil),
					Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
					AllOf: [] apiextensions.JSONSchemaProps(nil),
					OneOf: [] apiextensions.JSONSchemaProps(nil),
					AnyOf: [] apiextensions.JSONSchemaProps(nil),
					Not: ( * apiextensions.JSONSchemaProps)(nil),
					Properties: map[string] apiextensions.JSONSchemaProps(nil),
					AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
					Dependencies: apiextensions.JSONSchemaDependencies(nil),
					AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					Definitions: apiextensions.JSONSchemaDefinitions(nil),
					ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
					Example: ( * apiextensions.JSON)(nil)
				},
				""lastSyncedTime"": apiextensions.JSONSchemaProps {
					ID: """",
					Schema: """",
					Ref: ( * string)(nil),
					Description: ""LastSyncedTime is the last time the contents of the location were synced into the cluster."",
					Type: ""string"",
					Format: ""date-time"",
					Title: """",
					Default: ( * apiextensions.JSON)(nil),
					Maximum: ( * float64)(nil),
					ExclusiveMaximum: false,
					Minimum: ( * float64)(nil),
					ExclusiveMinimum: false,
					MaxLength: ( * int64)(nil),
					MinLength: ( * int64)(nil),
					Pattern: """",
					MaxItems: ( * int64)(nil),
					MinItems: ( * int64)(nil),
					UniqueItems: false,
					MultipleOf: ( * float64)(nil),
					Enum: [] apiextensions.JSON(nil),
					MaxProperties: ( * int64)(nil),
					MinProperties: ( * int64)(nil),
					Required: [] string(nil),
					Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
					AllOf: [] apiextensions.JSONSchemaProps(nil),
					OneOf: [] apiextensions.JSONSchemaProps(nil),
					AnyOf: [] apiextensions.JSONSchemaProps(nil),
					Not: ( * apiextensions.JSONSchemaProps)(nil),
					Properties: map[string] apiextensions.JSONSchemaProps(nil),
					AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
					Dependencies: apiextensions.JSONSchemaDependencies(nil),
					AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					Definitions: apiextensions.JSONSchemaDefinitions(nil),
					ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
					Example: ( * apiextensions.JSON)(nil)
				},
				""lastValidationTime"": apiextensions.JSONSchemaProps {
					ID: """",
					Schema: """",
					Ref: ( * string)(nil),
					Description: ""LastValidationTime is the last time the backup store location was validated the cluster."",
					Type: ""string"",
					Format: ""date-time"",
					Title: """",
					Default: ( * apiextensions.JSON)(nil),
					Maximum: ( * float64)(nil),
					ExclusiveMaximum: false,
					Minimum: ( * float64)(nil),
					ExclusiveMinimum: false,
					MaxLength: ( * int64)(nil),
					MinLength: ( * int64)(nil),
					Pattern: """",
					MaxItems: ( * int64)(nil),
					MinItems: ( * int64)(nil),
					UniqueItems: false,
					MultipleOf: ( * float64)(nil),
					Enum: [] apiextensions.JSON(nil),
					MaxProperties: ( * int64)(nil),
					MinProperties: ( * int64)(nil),
					Required: [] string(nil),
					Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
					AllOf: [] apiextensions.JSONSchemaProps(nil),
					OneOf: [] apiextensions.JSONSchemaProps(nil),
					AnyOf: [] apiextensions.JSONSchemaProps(nil),
					Not: ( * apiextensions.JSONSchemaProps)(nil),
					Properties: map[string] apiextensions.JSONSchemaProps(nil),
					AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
					Dependencies: apiextensions.JSONSchemaDependencies(nil),
					AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					Definitions: apiextensions.JSONSchemaDefinitions(nil),
					ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
					Example: ( * apiextensions.JSON)(nil)
				},
				""phase"": apiextensions.JSONSchemaProps {
					ID: """",
					Schema: """",
					Ref: ( * string)(nil),
					Description: ""Phase is the current state of the BackupStorageLocation."",
					Type: ""string"",
					Format: """",
					Title: """",
					Default: ( * apiextensions.JSON)(nil),
					Maximum: ( * float64)(nil),
					ExclusiveMaximum: false,
					Minimum: ( * float64)(nil),
					ExclusiveMinimum: false,
					MaxLength: ( * int64)(nil),
					MinLength: ( * int64)(nil),
					Pattern: """",
					MaxItems: ( * int64)(nil),
					MinItems: ( * int64)(nil),
					UniqueItems: false,
					MultipleOf: ( * float64)(nil),
					Enum: [] apiextensions.JSON {
						""Available"",
						""Unavailable""
					},
					MaxProperties: ( * int64)(nil),
					MinProperties: ( * int64)(nil),
					Required: [] string(nil),
					Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
					AllOf: [] apiextensions.JSONSchemaProps(nil),
					OneOf: [] apiextensions.JSONSchemaProps(nil),
					AnyOf: [] apiextensions.JSONSchemaProps(nil),
					Not: ( * apiextensions.JSONSchemaProps)(nil),
					Properties: map[string] apiextensions.JSONSchemaProps(nil),
					AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
					Dependencies: apiextensions.JSONSchemaDependencies(nil),
					AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					Definitions: apiextensions.JSONSchemaDefinitions(nil),
					ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
					Example: ( * apiextensions.JSON)(nil)
				}
			},
			AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
			PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
			Dependencies: apiextensions.JSONSchemaDependencies(nil),
			AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
			Definitions: apiextensions.JSONSchemaDefinitions(nil),
			ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
			Example: ( * apiextensions.JSON)(nil)
		},
		""apiVersion"": apiextensions.JSONSchemaProps {
			ID: """",
			Schema: """",
			Ref: ( * string)(nil),
			Description: ""APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources"",
			Type: ""string"",
			Format: """",
			Title: """",
			Default: ( * apiextensions.JSON)(nil),
			Maximum: ( * float64)(nil),
			ExclusiveMaximum: false,
			Minimum: ( * float64)(nil),
			ExclusiveMinimum: false,
			MaxLength: ( * int64)(nil),
			MinLength: ( * int64)(nil),
			Pattern: """",
			MaxItems: ( * int64)(nil),
			MinItems: ( * int64)(nil),
			UniqueItems: false,
			MultipleOf: ( * float64)(nil),
			Enum: [] apiextensions.JSON(nil),
			MaxProperties: ( * int64)(nil),
			MinProperties: ( * int64)(nil),
			Required: [] string(nil),
			Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
			AllOf: [] apiextensions.JSONSchemaProps(nil),
			OneOf: [] apiextensions.JSONSchemaProps(nil),
			AnyOf: [] apiextensions.JSONSchemaProps(nil),
			Not: ( * apiextensions.JSONSchemaProps)(nil),
			Properties: map[string] apiextensions.JSONSchemaProps(nil),
			AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
			PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
			Dependencies: apiextensions.JSONSchemaDependencies(nil),
			AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
			Definitions: apiextensions.JSONSchemaDefinitions(nil),
			ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
			Example: ( * apiextensions.JSON)(nil)
		},
		""kind"": apiextensions.JSONSchemaProps {
			ID: """",
			Schema: """",
			Ref: ( * string)(nil),
			Description: ""Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"",
			Type: ""string"",
			Format: """",
			Title: """",
			Default: ( * apiextensions.JSON)(nil),
			Maximum: ( * float64)(nil),
			ExclusiveMaximum: false,
			Minimum: ( * float64)(nil),
			ExclusiveMinimum: false,
			MaxLength: ( * int64)(nil),
			MinLength: ( * int64)(nil),
			Pattern: """",
			MaxItems: ( * int64)(nil),
			MinItems: ( * int64)(nil),
			UniqueItems: false,
			MultipleOf: ( * float64)(nil),
			Enum: [] apiextensions.JSON(nil),
			MaxProperties: ( * int64)(nil),
			MinProperties: ( * int64)(nil),
			Required: [] string(nil),
			Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
			AllOf: [] apiextensions.JSONSchemaProps(nil),
			OneOf: [] apiextensions.JSONSchemaProps(nil),
			AnyOf: [] apiextensions.JSONSchemaProps(nil),
			Not: ( * apiextensions.JSONSchemaProps)(nil),
			Properties: map[string] apiextensions.JSONSchemaProps(nil),
			AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
			PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
			Dependencies: apiextensions.JSONSchemaDependencies(nil),
			AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
			Definitions: apiextensions.JSONSchemaDefinitions(nil),
			ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
			Example: ( * apiextensions.JSON)(nil)
		},
		""metadata"": apiextensions.JSONSchemaProps {
			ID: """",
			Schema: """",
			Ref: ( * string)(nil),
			Description: """",
			Type: ""object"",
			Format: """",
			Title: """",
			Default: ( * apiextensions.JSON)(nil),
			Maximum: ( * float64)(nil),
			ExclusiveMaximum: false,
			Minimum: ( * float64)(nil),
			ExclusiveMinimum: false,
			MaxLength: ( * int64)(nil),
			MinLength: ( * int64)(nil),
			Pattern: """",
			MaxItems: ( * int64)(nil),
			MinItems: ( * int64)(nil),
			UniqueItems: false,
			MultipleOf: ( * float64)(nil),
			Enum: [] apiextensions.JSON(nil),
			MaxProperties: ( * int64)(nil),
			MinProperties: ( * int64)(nil),
			Required: [] string(nil),
			Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
			AllOf: [] apiextensions.JSONSchemaProps(nil),
			OneOf: [] apiextensions.JSONSchemaProps(nil),
			AnyOf: [] apiextensions.JSONSchemaProps(nil),
			Not: ( * apiextensions.JSONSchemaProps)(nil),
			Properties: map[string] apiextensions.JSONSchemaProps(nil),
			AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
			PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
			Dependencies: apiextensions.JSONSchemaDependencies(nil),
			AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
			Definitions: apiextensions.JSONSchemaDefinitions(nil),
			ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
			Example: ( * apiextensions.JSON)(nil)
		},
		""spec"": apiextensions.JSONSchemaProps {
			ID: """",
			Schema: """",
			Ref: ( * string)(nil),
			Description: ""BackupStorageLocationSpec defines the desired state of a Velero BackupStorageLocation"",
			Type: ""object"",
			Format: """",
			Title: """",
			Default: ( * apiextensions.JSON)(nil),
			Maximum: ( * float64)(nil),
			ExclusiveMaximum: false,
			Minimum: ( * float64)(nil),
			ExclusiveMinimum: false,
			MaxLength: ( * int64)(nil),
			MinLength: ( * int64)(nil),
			Pattern: """",
			MaxItems: ( * int64)(nil),
			MinItems: ( * int64)(nil),
			UniqueItems: false,
			MultipleOf: ( * float64)(nil),
			Enum: [] apiextensions.JSON(nil),
			MaxProperties: ( * int64)(nil),
			MinProperties: ( * int64)(nil),
			Required: [] string {
				""objectStorage"",
				""provider""
			},
			Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
			AllOf: [] apiextensions.JSONSchemaProps(nil),
			OneOf: [] apiextensions.JSONSchemaProps(nil),
			AnyOf: [] apiextensions.JSONSchemaProps(nil),
			Not: ( * apiextensions.JSONSchemaProps)(nil),
			Properties: map[string] apiextensions.JSONSchemaProps {
				""provider"": apiextensions.JSONSchemaProps {
					ID: """",
					Schema: """",
					Ref: ( * string)(nil),
					Description: ""Provider is the provider of the backup storage."",
					Type: ""string"",
					Format: """",
					Title: """",
					Default: ( * apiextensions.JSON)(nil),
					Maximum: ( * float64)(nil),
					ExclusiveMaximum: false,
					Minimum: ( * float64)(nil),
					ExclusiveMinimum: false,
					MaxLength: ( * int64)(nil),
					MinLength: ( * int64)(nil),
					Pattern: """",
					MaxItems: ( * int64)(nil),
					MinItems: ( * int64)(nil),
					UniqueItems: false,
					MultipleOf: ( * float64)(nil),
					Enum: [] apiextensions.JSON(nil),
					MaxProperties: ( * int64)(nil),
					MinProperties: ( * int64)(nil),
					Required: [] string(nil),
					Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
					AllOf: [] apiextensions.JSONSchemaProps(nil),
					OneOf: [] apiextensions.JSONSchemaProps(nil),
					AnyOf: [] apiextensions.JSONSchemaProps(nil),
					Not: ( * apiextensions.JSONSchemaProps)(nil),
					Properties: map[string] apiextensions.JSONSchemaProps(nil),
					AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
					Dependencies: apiextensions.JSONSchemaDependencies(nil),
					AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					Definitions: apiextensions.JSONSchemaDefinitions(nil),
					ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
					Example: ( * apiextensions.JSON)(nil)
				},
				""validationFrequency"": apiextensions.JSONSchemaProps {
					ID: """",
					Schema: """",
					Ref: ( * string)(nil),
					Description: ""ValidationFrequency defines how frequently to validate the corresponding object storage. A value of 0 disables validation."",
					Type: ""string"",
					Format: """",
					Title: """",
					Default: ( * apiextensions.JSON)(nil),
					Maximum: ( * float64)(nil),
					ExclusiveMaximum: false,
					Minimum: ( * float64)(nil),
					ExclusiveMinimum: false,
					MaxLength: ( * int64)(nil),
					MinLength: ( * int64)(nil),
					Pattern: """",
					MaxItems: ( * int64)(nil),
					MinItems: ( * int64)(nil),
					UniqueItems: false,
					MultipleOf: ( * float64)(nil),
					Enum: [] apiextensions.JSON(nil),
					MaxProperties: ( * int64)(nil),
					MinProperties: ( * int64)(nil),
					Required: [] string(nil),
					Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
					AllOf: [] apiextensions.JSONSchemaProps(nil),
					OneOf: [] apiextensions.JSONSchemaProps(nil),
					AnyOf: [] apiextensions.JSONSchemaProps(nil),
					Not: ( * apiextensions.JSONSchemaProps)(nil),
					Properties: map[string] apiextensions.JSONSchemaProps(nil),
					AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
					Dependencies: apiextensions.JSONSchemaDependencies(nil),
					AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					Definitions: apiextensions.JSONSchemaDefinitions(nil),
					ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
					Example: ( * apiextensions.JSON)(nil)
				},
				""accessMode"": apiextensions.JSONSchemaProps {
					ID: """",
					Schema: """",
					Ref: ( * string)(nil),
					Description: ""AccessMode defines the permissions for the backup storage location."",
					Type: ""string"",
					Format: """",
					Title: """",
					Default: ( * apiextensions.JSON)(nil),
					Maximum: ( * float64)(nil),
					ExclusiveMaximum: false,
					Minimum: ( * float64)(nil),
					ExclusiveMinimum: false,
					MaxLength: ( * int64)(nil),
					MinLength: ( * int64)(nil),
					Pattern: """",
					MaxItems: ( * int64)(nil),
					MinItems: ( * int64)(nil),
					UniqueItems: false,
					MultipleOf: ( * float64)(nil),
					Enum: [] apiextensions.JSON {
						""ReadOnly"",
						""ReadWrite""
					},
					MaxProperties: ( * int64)(nil),
					MinProperties: ( * int64)(nil),
					Required: [] string(nil),
					Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
					AllOf: [] apiextensions.JSONSchemaProps(nil),
					OneOf: [] apiextensions.JSONSchemaProps(nil),
					AnyOf: [] apiextensions.JSONSchemaProps(nil),
					Not: ( * apiextensions.JSONSchemaProps)(nil),
					Properties: map[string] apiextensions.JSONSchemaProps(nil),
					AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
					Dependencies: apiextensions.JSONSchemaDependencies(nil),
					AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					Definitions: apiextensions.JSONSchemaDefinitions(nil),
					ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
					Example: ( * apiextensions.JSON)(nil)
				},
				""backupSyncPeriod"": apiextensions.JSONSchemaProps {
					ID: """",
					Schema: """",
					Ref: ( * string)(nil),
					Description: ""BackupSyncPeriod defines how frequently to sync backup API objects from object storage. A value of 0 disables sync."",
					Type: ""string"",
					Format: """",
					Title: """",
					Default: ( * apiextensions.JSON)(nil),
					Maximum: ( * float64)(nil),
					ExclusiveMaximum: false,
					Minimum: ( * float64)(nil),
					ExclusiveMinimum: false,
					MaxLength: ( * int64)(nil),
					MinLength: ( * int64)(nil),
					Pattern: """",
					MaxItems: ( * int64)(nil),
					MinItems: ( * int64)(nil),
					UniqueItems: false,
					MultipleOf: ( * float64)(nil),
					Enum: [] apiextensions.JSON(nil),
					MaxProperties: ( * int64)(nil),
					MinProperties: ( * int64)(nil),
					Required: [] string(nil),
					Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
					AllOf: [] apiextensions.JSONSchemaProps(nil),
					OneOf: [] apiextensions.JSONSchemaProps(nil),
					AnyOf: [] apiextensions.JSONSchemaProps(nil),
					Not: ( * apiextensions.JSONSchemaProps)(nil),
					Properties: map[string] apiextensions.JSONSchemaProps(nil),
					AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
					Dependencies: apiextensions.JSONSchemaDependencies(nil),
					AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					Definitions: apiextensions.JSONSchemaDefinitions(nil),
					ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
					Example: ( * apiextensions.JSON)(nil)
				},
				""config"": apiextensions.JSONSchemaProps {
					ID: """",
					Schema: """",
					Ref: ( * string)(nil),
					Description: ""Config is for provider-specific configuration fields."",
					Type: ""object"",
					Format: """",
					Title: """",
					Default: ( * apiextensions.JSON)(nil),
					Maximum: ( * float64)(nil),
					ExclusiveMaximum: false,
					Minimum: ( * float64)(nil),
					ExclusiveMinimum: false,
					MaxLength: ( * int64)(nil),
					MinLength: ( * int64)(nil),
					Pattern: """",
					MaxItems: ( * int64)(nil),
					MinItems: ( * int64)(nil),
					UniqueItems: false,
					MultipleOf: ( * float64)(nil),
					Enum: [] apiextensions.JSON(nil),
					MaxProperties: ( * int64)(nil),
					MinProperties: ( * int64)(nil),
					Required: [] string(nil),
					Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
					AllOf: [] apiextensions.JSONSchemaProps(nil),
					OneOf: [] apiextensions.JSONSchemaProps(nil),
					AnyOf: [] apiextensions.JSONSchemaProps(nil),
					Not: ( * apiextensions.JSONSchemaProps)(nil),
					Properties: map[string] apiextensions.JSONSchemaProps(nil),
					AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(0xc43ad89c00),
					PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
					Dependencies: apiextensions.JSONSchemaDependencies(nil),
					AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					Definitions: apiextensions.JSONSchemaDefinitions(nil),
					ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
					Example: ( * apiextensions.JSON)(nil)
				},
				""objectStorage"": apiextensions.JSONSchemaProps {
					ID: """",
					Schema: """",
					Ref: ( * string)(nil),
					Description: ""ObjectStorageLocation specifies the settings necessary to connect to a provider's object storage."",
					Type: ""object"",
					Format: """",
					Title: """",
					Default: ( * apiextensions.JSON)(nil),
					Maximum: ( * float64)(nil),
					ExclusiveMaximum: false,
					Minimum: ( * float64)(nil),
					ExclusiveMinimum: false,
					MaxLength: ( * int64)(nil),
					MinLength: ( * int64)(nil),
					Pattern: """",
					MaxItems: ( * int64)(nil),
					MinItems: ( * int64)(nil),
					UniqueItems: false,
					MultipleOf: ( * float64)(nil),
					Enum: [] apiextensions.JSON(nil),
					MaxProperties: ( * int64)(nil),
					MinProperties: ( * int64)(nil),
					Required: [] string {
						""bucket""
					},
					Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
					AllOf: [] apiextensions.JSONSchemaProps(nil),
					OneOf: [] apiextensions.JSONSchemaProps(nil),
					AnyOf: [] apiextensions.JSONSchemaProps(nil),
					Not: ( * apiextensions.JSONSchemaProps)(nil),
					Properties: map[string] apiextensions.JSONSchemaProps {
						""bucket"": apiextensions.JSONSchemaProps {
							ID: """",
							Schema: """",
							Ref: ( * string)(nil),
							Description: ""Bucket is the bucket to use for object storage."",
							Type: ""string"",
							Format: """",
							Title: """",
							Default: ( * apiextensions.JSON)(nil),
							Maximum: ( * float64)(nil),
							ExclusiveMaximum: false,
							Minimum: ( * float64)(nil),
							ExclusiveMinimum: false,
							MaxLength: ( * int64)(nil),
							MinLength: ( * int64)(nil),
							Pattern: """",
							MaxItems: ( * int64)(nil),
							MinItems: ( * int64)(nil),
							UniqueItems: false,
							MultipleOf: ( * float64)(nil),
							Enum: [] apiextensions.JSON(nil),
							MaxProperties: ( * int64)(nil),
							MinProperties: ( * int64)(nil),
							Required: [] string(nil),
							Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
							AllOf: [] apiextensions.JSONSchemaProps(nil),
							OneOf: [] apiextensions.JSONSchemaProps(nil),
							AnyOf: [] apiextensions.JSONSchemaProps(nil),
							Not: ( * apiextensions.JSONSchemaProps)(nil),
							Properties: map[string] apiextensions.JSONSchemaProps(nil),
							AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
							PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
							Dependencies: apiextensions.JSONSchemaDependencies(nil),
							AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
							Definitions: apiextensions.JSONSchemaDefinitions(nil),
							ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
							Example: ( * apiextensions.JSON)(nil)
						},
						""caCert"": apiextensions.JSONSchemaProps {
							ID: """",
							Schema: """",
							Ref: ( * string)(nil),
							Description: ""CACert defines a CA bundle to use when verifying TLS connections to the provider."",
							Type: ""string"",
							Format: ""byte"",
							Title: """",
							Default: ( * apiextensions.JSON)(nil),
							Maximum: ( * float64)(nil),
							ExclusiveMaximum: false,
							Minimum: ( * float64)(nil),
							ExclusiveMinimum: false,
							MaxLength: ( * int64)(nil),
							MinLength: ( * int64)(nil),
							Pattern: """",
							MaxItems: ( * int64)(nil),
							MinItems: ( * int64)(nil),
							UniqueItems: false,
							MultipleOf: ( * float64)(nil),
							Enum: [] apiextensions.JSON(nil),
							MaxProperties: ( * int64)(nil),
							MinProperties: ( * int64)(nil),
							Required: [] string(nil),
							Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
							AllOf: [] apiextensions.JSONSchemaProps(nil),
							OneOf: [] apiextensions.JSONSchemaProps(nil),
							AnyOf: [] apiextensions.JSONSchemaProps(nil),
							Not: ( * apiextensions.JSONSchemaProps)(nil),
							Properties: map[string] apiextensions.JSONSchemaProps(nil),
							AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
							PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
							Dependencies: apiextensions.JSONSchemaDependencies(nil),
							AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
							Definitions: apiextensions.JSONSchemaDefinitions(nil),
							ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
							Example: ( * apiextensions.JSON)(nil)
						},
						""prefix"": apiextensions.JSONSchemaProps {
							ID: """",
							Schema: """",
							Ref: ( * string)(nil),
							Description: ""Prefix is the path inside a bucket to use for Velero storage. Optional."",
							Type: ""string"",
							Format: """",
							Title: """",
							Default: ( * apiextensions.JSON)(nil),
							Maximum: ( * float64)(nil),
							ExclusiveMaximum: false,
							Minimum: ( * float64)(nil),
							ExclusiveMinimum: false,
							MaxLength: ( * int64)(nil),
							MinLength: ( * int64)(nil),
							Pattern: """",
							MaxItems: ( * int64)(nil),
							MinItems: ( * int64)(nil),
							UniqueItems: false,
							MultipleOf: ( * float64)(nil),
							Enum: [] apiextensions.JSON(nil),
							MaxProperties: ( * int64)(nil),
							MinProperties: ( * int64)(nil),
							Required: [] string(nil),
							Items: ( * apiextensions.JSONSchemaPropsOrArray)(nil),
							AllOf: [] apiextensions.JSONSchemaProps(nil),
							OneOf: [] apiextensions.JSONSchemaProps(nil),
							AnyOf: [] apiextensions.JSONSchemaProps(nil),
							Not: ( * apiextensions.JSONSchemaProps)(nil),
							Properties: map[string] apiextensions.JSONSchemaProps(nil),
							AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
							PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
							Dependencies: apiextensions.JSONSchemaDependencies(nil),
							AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
							Definitions: apiextensions.JSONSchemaDefinitions(nil),
							ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
							Example: ( * apiextensions.JSON)(nil)
						}
					},
					AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
					Dependencies: apiextensions.JSONSchemaDependencies(nil),
					AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
					Definitions: apiextensions.JSONSchemaDefinitions(nil),
					ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
					Example: ( * apiextensions.JSON)(nil)
				}
			},
			AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
			PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
			Dependencies: apiextensions.JSONSchemaDependencies(nil),
			AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
			Definitions: apiextensions.JSONSchemaDefinitions(nil),
			ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
			Example: ( * apiextensions.JSON)(nil)
		}
	},
	AdditionalProperties: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
	PatternProperties: map[string] apiextensions.JSONSchemaProps(nil),
	Dependencies: apiextensions.JSONSchemaDependencies(nil),
	AdditionalItems: ( * apiextensions.JSONSchemaPropsOrBool)(nil),
	Definitions: apiextensions.JSONSchemaDefinitions(nil),
	ExternalDocs: ( * apiextensions.ExternalDocumentation)(nil),
	Example: ( * apiextensions.JSON)(nil)
}: must only have ""properties"", ""required""
or ""description""
at the root
if the status subresource is enabled`



**Environment:**

- Velero version (use `velero version`): v1.5.0-beta.1
- Kubernetes version (use `kubectl version`): v1.11.10
- Kubernetes installer & version: v1.11.10
- OS (e.g. from `/etc/os-release`): 

Distributor ID:	Linuxmint
Description:	Linux Mint 20
Release:	20
Codename:	ulyana



**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,albertollamaso,"
--
I was able to install just downgrading to velero version: **v1.4.2** thought it is a bug on **v1.5.0-beta.1**
--
",carlisia,"
--
> CustomResourceDefinition/backups.velero.io: attempting to create resource
CustomResourceDefinition/backups.velero.io: already exists, proceeding

This makes me think this is not a fresh install, you already had some version there before? If so, please run:

`velero install --crds-only --dry-run -o yaml | kubectl apply -f -`

Let me know if it works or not and we'll go from there, and thank you for testing the beta!
--
",dsu,"
--
Closing this, please re-open if the issue continues.
--
",zubron,"
--
Following up from [a discussion](https://github.com/vmware-tanzu/velero/discussions/3293#discussioncomment-320948), our CRDs are no longer compatible with Kubernetes v1.11 and earlier. We state that we support v1.10 and later, so we should revisit our list of supported Kubernetes versions.

To summarise the issue, support for the field ""type"" when using the status subresource in the CRD validation schema was [only introduced in Kubernetes v1.12](https://github.com/kubernetes/kubernetes/commit/b38e1a9e42777103fdd491a9f76d1aa73bb32454). This means that velero 1.5 and later cannot be installed on Kubernetes 1.11 or earlier.
--
",,,,
2841,OPEN,Removing Dead Code,Enhancement/Dev,2020-11-30 19:35:52 +0000 UTC,RobbieJVMW,In progress,,"Running the code though linter to clean up redundant & dead code.  
First pass as part of general code clean up activities. 
",,,ashish,"
--
@RobbieJVMW Thank you for reporting this.
For this to be actionable can you please provide more information:
1. What linting software did you use? How to get it? How to run it?
1. What was the output of the linter?
--

--
should we close this issue and as this will be addressed in #2397 
--
",nrb,"
--
Is this a duplicate of #2397?
--
",carlisia,"
--
I'll answer, yes it is! @RobbieJVMW please list which package(s) you will be cleaning up when you have a chance.
--

--
Two people were interested in tackling #2397 so depending on how this develops it'd be best to have two separate issues. Ideally each issue would list what packages are being cleaned up.
--
",RobbieJVMW,"
--
Happy for you to map it over to #2397, I'll get to listing the package next week I'm away home this coming week. As this is my first contrib to the project I'll need some guidance following your guidelines I expect.  
--

--
I'll start with : 

pkg/controller/
pkg/metrics/
pkg/restic/

quickly clean up dead code - mostly as a learning exercise for myself to learn how you folks like to do things / learn the processes.  Then move onto more investing bits of code. 

@carlisia do you which issue to you want me to associate with ?   

--
",,,,
2836,OPEN,Change PVC Size on Restore,Needs Product,2020-08-24 17:04:37 +0000 UTC,dbpolito,Opened,,"**Describe the problem/challenge you have**

Currently we can change Node, Namespace and Storage Class as far as i can tell looking at docs: https://velero.io/docs/v1.4/restore-reference/

Would be very useful if we could change the request size, my use case is i want to decrease a PVC size as i allocated too much space on it, so my idea is:

* Create Backup
* Delete PVC
* Restore Backup with smaller size

I'm already doing this approach for migrating storage class.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
2825,OPEN,Velero does not send client certificate when using custom cert option,Area/Documentation; Good first issue; Help wanted; Security,2021-02-23 19:46:05 +0000 UTC,hmehra,In progress,,"**Describe the problem/challenge you have**
When a self signed certificate is provided to Velero for the S3 object store with cacert option, it uses SSL with TLSv1.0 for the security handshake. TLS v1.0 is very old and the server rejects the handshake. This was done using Velero v1.4 and aws-plugin v1.1.0 

**Describe the solution you'd like**
Velero should use TLSv1.2 for SSL handshakes and connections.

**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`):

velero version
Client:
        Version: v1.4.0
        Git commit: 5963650c9d64643daaf510ef93ac4a36b6483392
Server:
        Version: v1.4.0

- Kubernetes version (use `kubectl version`):
kubectl version
Client Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.3"", GitCommit:""2e7996e3e2712684bc73f0dec0200d64eec7fe40"", GitTreeState:""clean"", BuildDate:""2020-05-20T12:52:00Z"", GoVersion:""go1.13.9"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""16"", GitVersion:""v1.16.0"", GitCommit:""2bd9643cee5b3b3a5ecbd3af49d09018f0773c77"", GitTreeState:""clean"", BuildDate:""2019-09-18T14:27:17Z"", GoVersion:""go1.12.9"", Compiler:""gc"", Platform:""linux/amd64""}

- Kubernetes installer & version:

minikube version
minikube version: v1.4.0
commit: 7969c25a98a018b94ea87d949350f3271e9d64b6

- Cloud provider or hardware configuration:

- OS (e.g. from `/etc/os-release`):
cat /etc/os-release
NAME=""Ubuntu""
VERSION=""18.04.4 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.4 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic

- Object store
Proprietary object store which is compatible with S3 protocol. The server enforces strict TLS check. Any version below v1.2 is rejected. 

- Repro steps

Install Velero using self signed certificate 
velero install --use-restic --provider aws --bucket k8s-backup-view --secret-file ./secret --cacert ./ssl_cert.pem --use-volume-snapshots=false --backup-location-config region=default,s3ForcePathStyle=""true"",s3Url=https://sv4-dell87-c3-ve02.com:3000 --plugins velero/velero-plugin-for-aws:v1.1.0

Error seen -

kubectl logs deployment/velero -n velero
time=""2020-08-14T20:51:13Z"" level=info msg=""setting log-level to INFO"" logSource=""pkg/cmd/server/server.go:177""
time=""2020-08-14T20:51:13Z"" level=info msg=""Starting Velero server v1.4.0 (5963650c9d64643daaf510ef93ac4a36b6483392)"" logSource=""pkg/cmd/server/server.go:179""
time=""2020-08-14T20:51:13Z"" level=info msg=""1 feature flags enabled []"" logSource=""pkg/cmd/server/server.go:181""
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/crd-remap-version
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pod
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pv
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=BackupItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service-account
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/add-pv-from-pvc
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/add-pvc-from-pod
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/change-pvc-node-selector
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/change-storage-class
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/cluster-role-bindings
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/crd-preserve-fields
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/job
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/pod
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/restic
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/role-bindings
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/velero kind=RestoreItemAction logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/service-account
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/plugins/velero-plugin-for-aws kind=VolumeSnapshotter logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/aws
time=""2020-08-14T20:51:14Z"" level=info msg=""registering plugin"" command=/plugins/velero-plugin-for-aws kind=ObjectStore logSource=""pkg/plugin/clientmgmt/registry.go:100"" name=velero.io/aws
time=""2020-08-14T20:51:14Z"" level=info msg=""Checking existence of namespace"" logSource=""pkg/cmd/server/server.go:361"" namespace=velero
time=""2020-08-14T20:51:14Z"" level=info msg=""Namespace exists"" logSource=""pkg/cmd/server/server.go:367"" namespace=velero
time=""2020-08-14T20:51:16Z"" level=info msg=""Checking existence of Velero custom resource definitions"" logSource=""pkg/cmd/server/server.go:396""
time=""2020-08-14T20:51:16Z"" level=info msg=""All Velero custom resource definitions exist"" logSource=""pkg/cmd/server/server.go:430""
time=""2020-08-14T20:51:16Z"" level=info msg=""Checking that all backup storage locations are valid"" logSource=""pkg/cmd/server/server.go:437""
An error occurred: some backup storage locations are invalid: backup store for location ""default"" is invalid: rpc error: code = Unknown desc = RequestError: send request failed
caused by: Get https://sv4-dell87-c3-ve02.com:3000/k8s-backup-view?delimiter=%2F&list-type=2&prefix=: remote error: tls: alert(116)


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,ashish,"
--
Thank you for reporting this issue.
Velero doesn't initiate the SSL handshake to negotiate the TLS versions. This is handled by the aws sdk that Velero uses.
We'll investigate how this can be configured.
--

--
@hmehra thanks for the update. Is there any information indicative of why the cert wasn't used?
Also, FYI, Velero, doesn't handle the ssl handshake and negotiation directly. We use the AWS sdk that handles the ssl connection setup.
--

--
@hmehra thanks for figuring this out. I am not sure what changes are required in Velero for this. The SSL connection setup is handled by the AWS SDK.
At the very least, we would like this to be documented. Can you please open a docs PR for this?
--
",hmehra,"
--
@ashish-amarnath  On digging deep within the S3 server, we have determined the TLS version is negotiated correctly. But the client does not send its certificate to the server. This causes the server to reset the connection.
--

--
@ashish-amarnath  I am not sure why Velero as a client did not include its certificate while the SSL handshake was done. From TLS 1.3 spec https://tools.ietf.org/html/rfc8446, verifying client certificate could be optional. Changing this on the server made it work. I am not sure if you guys want to fix this in Velero, where it would send a certificate for the handshake.
It would be better to edit the issue title as well. Please let me know.
--

--
I have edited the issue title. Will open a docs PR.
--

--
@a-mccarthy This slipped my to do list. Will send a docs PR in the weekend.
--
",jenting,"
--
@hmehra,
maybe you could change the https://github.com/vmware-tanzu/velero-plugin-for-aws to add the TLS min version to 1.2 on the HTTP client default transport when caCert provided.
```
	if len(caCert) > 0 {
		serverConfig.HTTPClient = &http.Client{
			Transport: &http.Transport{
				TLSClientConfig: &tls.Config{
					PreferServerCipherSuites: true,
					MinVersion:               tls.VersionTLS12,
				},
			},
		}
	}
```
--
",carlisia,"
--
Hey @hmehra, are you still able to help out by adding documentation for this? It'd be great, thank you!
--
",a,"
--
We should probably this call out on the https://velero.io/docs/main/self-signed-certificates/

@hmehra are you still able to work on this? thanks!
--
",,
2821,OPEN,Ability to set/get logs from restic restore commands,Restic - GA,2020-08-18 18:20:17 +0000 UTC,galindro,In progress,,"**Describe the problem/challenge you have**
Today I executed a velero restore of a jenkins application and it took almost 1h to finish. The backup files are placed in a S3 bucket located in the same region of the EKS cluster (eu-west-1). The restore's storage class destination is AWS EFS. The data size is only 1.3GB. 

Here is the velero restore log snippet that states the start and end times of restic restore:

```bash
time=""2020-08-14T07:17:14Z"" level=info msg=""Waiting for all restic restores to complete"" logSource=""pkg/restore/restore.go:470"" restore=velero/jenkins-final-1
time=""2020-08-14T08:08:22Z"" level=info msg=""Done waiting for all restic restores to complete"" logSource=""pkg/restore/restore.go:486"" restore=velero/jenkins-final-1
```

Here is the restic controller logs which was responsible to execute the restore:
```bash
time=""2020-08-14T07:17:16Z"" level=debug msg=Enqueuing controller=pod-volume-restore key=jenkins/jenkins-68bdbd997f-9p2kl logSource=""pkg/controller/pod_volume_restore_controller.go:194"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:17:16Z"" level=debug msg=""Running processQueueItem"" controller=pod-volume-restore key=velero/jenkins-final-1-bp8pb logSource=""pkg/controller/pod_volume_restore_controller.go:224""
time=""2020-08-14T07:17:16Z"" level=info msg=""Restore starting"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:262"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:17:16Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:17:20Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:18:20Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:18:28Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:18:38Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:18:48Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:18:58Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:19:12Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:19:24Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:19:32Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:19:41Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:19:51Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:20:00Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:20:11Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:20:28Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:20:45Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:21:02Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:21:21Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:21:33Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:21:48Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:22:01Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:22:11Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:22:19Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:22:31Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:22:48Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:23:05Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:23:24Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:23:35Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:23:49Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:24:08Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:24:23Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:24:37Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:25:01Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:25:16Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:25:42Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:26:25Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:26:51Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:27:07Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:27:20Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:27:32Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:27:47Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:28:03Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:28:38Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:29:01Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:29:20Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:29:33Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:29:52Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:30:39Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:31:52Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:32:17Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:32:50Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:33:06Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:33:49Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:34:38Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:35:04Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:36:39Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:37:36Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:38:05Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T07:39:05Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T08:08:22Z"" level=debug msg=""Ran command=restic restore --repo=s3:s3-eu-west-1.amazonaws.com/nie-backup-ict-prod/restic/jenkins --password-file=/tmp/velero-restic-credentials-jenkins284039206 --cache-dir=/scratch/.cache/restic 86b9508c --target=., stdout=restoring <Snapshot 86b9508c of [/host_pods/6949bd49-beb5-4051-8486-9891f159f61d/volumes/kubernetes.io~csi/pvc-6ead4d19-7eb8-4e51-b579-7a7e12b6799b/mount] at 2020-08-13 11:57:34.522448606 +0000 UTC by root@velero> to .\n, stderr="" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:368"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T08:08:22Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
time=""2020-08-14T08:08:22Z"" level=info msg=""Restore completed"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:326"" name=jenkins-final-1-bp8pb namespace=velero restore=velero/jenkins-final-1
```

If I could add a verbose log to `restic restore` command and retrieve them, maybe I could find out the times where each file is taken to be written to the EFS share or other useful info for debugging.

**Describe the solution you'd like**
Ability to set/get logs from restic restore commands via configmap or other method (like command line argument).


**Anything else you would like to add:**
Quantity of files restored: *102886*
Average file size: *9.5K*


**Environment:**

- Velero version (use `velero version`):
```bash
Client:
	Version: v1.4.2
	Git commit: 56a08a4d695d893f0863f697c2f926e27d70c0c5
Server:
	Version: v1.4.2
```
- Kubernetes version (use `kubectl version`):
```bash
Client Version: version.Info{Major:""1"", Minor:""15"", GitVersion:""v1.15.12"", GitCommit:""e2a822d9f3c2fdb5c9bfbe64313cf9f657f0a725"", GitTreeState:""clean"", BuildDate:""2020-05-06T05:17:59Z"", GoVersion:""go1.12.17"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""16+"", GitVersion:""v1.16.8-eks-e16311"", GitCommit:""e163110a04dcb2f39c3325af96d019b4925419eb"", GitTreeState:""clean"", BuildDate:""2020-03-27T22:37:12Z"", GoVersion:""go1.13.8"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kubernetes installer & version: `1.16 eks.1`
- Cloud provider or hardware configuration:
```bash
AWS EKS
t3.medium nodes
```
- OS (e.g. from `/etc/os-release`):
```bash
NAME=""Amazon Linux""
VERSION=""2""
ID=""amzn""
ID_LIKE=""centos rhel fedora""
VERSION_ID=""2""
PRETTY_NAME=""Amazon Linux 2""
ANSI_COLOR=""0;33""
CPE_NAME=""cpe:2.3:o:amazon:amazon_linux:2""
HOME_URL=""https://amazonlinux.com/""
```

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,galindro,"
--
With these logs in hands I could identify this situation:

I started a backup at 10:35:13. According to velero describe restore command, restic took approximately 24 minutes to reach the 100%. But it is still in progress!!

```bash
velero restore describe jenkins --details
Name:         jenkins
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:  InProgress

Backup:  jenkins

Namespaces:
  Included:  all namespaces found in the backup
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io
  Cluster-scoped:  auto

Namespace mappings:  <none>

Label selector:  <none>

Restore PVs:  auto

Restic Restores:
  In Progress:
    jenkins/jenkins-75bcd45645-hfgx6: jenkins-home (100.00%)
```

It will stay In Progress for at least 30 minutes more, then it will complete without errors. 

--
",,,,,,,,,,
2820,OPEN,Checking restic repository for stale locks makes a lot of s3 api calls,Restic,2020-08-18 18:05:27 +0000 UTC,tsufeki,Opened,,"**Describe the problem/challenge you have**
Checking restic repository for stale locks runs `restic unlock` every [5 minutes](https://github.com/vmware-tanzu/velero/blob/c663ce15ab468b21a19336dcc38acf3280853361/pkg/controller/restic_repository_controller.go#L89) for each ResticRepository. This makes ~100 s3 api calls per hour per repository, essentially doing nothing of value.

I'm using velero for a toy project with Backblaze B2 and I want to stay in their free plan, but my 9 repositories burn through the daily api calls cap in few hours.


**Describe the solution you'd like**
Please consider making stale lock check interval configurable (something like `--restic-stale-lock-check-period=12h` option) or changing the approach to something less api calls-heavy.


**Anything else you would like to add:**


**Environment:**

- Velero version (use `velero version`):
```
Client:
        Version: v1.4.2
        Git commit: 56a08a4d695d893f0863f697c2f926e27d70c0c5
Server:
        Version: v1.4.2
```
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.6"", GitCommit:""dff82dc0de47299ab66c83c626e08b245ab19037"", GitTreeState:""archive"", BuildDate:""2020-07-16T09:08:46Z"", GoVersion:""go1.14.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.6+k3s1"", GitCommit:""6f56fa1d68a5a48b8b6fdefa8eb7ead2015a4b3a"", GitTreeState:""clean"", BuildDate:""2020-07-16T20:46:15Z"", GoVersion:""go1.13.11"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kubernetes installer & version: k3s v1.18.6+k3s1
- Cloud provider or hardware configuration: bare metal
- OS (e.g. from `/etc/os-release`): Debian GNU/Linux bullseye/sid

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
2819,OPEN,create an alias like ‘s3’ so --provider s3 would be similar than --provider aws,Area/Plugins,2020-08-18 18:02:50 +0000 UTC,guillierf,Opened,,"When deploying Velero in a vSphere environmnet, ""--provider aws"" option in the Velero install command may be misleading.
Creating an alias like 's3' instead of 'aws' can improve user experience in a vSphere environment.",,,,,,,,,,,,,,
2818,OPEN,doc: describes how to install Velero on a vSphere environment using non-vSphere plugin,Area/Documentation,2020-08-18 18:04:53 +0000 UTC,guillierf,Opened,,"hello,

having a doc page that describes how to install Velero on a vSphere environment using non-vSphere plugin would be helpful. In that case, Restic needs to be installed for both vCP and CSI storage plugin
",,,nrb,"
--
@guillierf To my knowledge vSphere does not use the Velero CSI plugin. It uses its own plugin based on the CNS stack, which doesn't support CSI's snapshot APIs.

I just want to clarify that language because the state of CSI support on vSphere right now is a little confusing.
--
",,,,,,,,,,
2812,OPEN,"Restore fails with error ""no space left on device"" if PVCs are 100% utilised during backup",Bug; P1 - Important; Restic - GA,2021-02-03 17:40:47 +0000 UTC,nrb,In progress,,"**What steps did you take and what happened:**
From an internal bug report:

Using Restic, restore is failing with error ""no space left on device"" if PVC was fully occupied by the user data while the backup is taken.

for example, PVC of size 1Gi and user data in the PVC is ~upto 100% which is of 1Gi

**What did you expect to happen:**

Velero restic restores should be successful, even if the PV is full.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

```
root@k8-master-277:~# velero restore describe restore001 --details
Name:         restore001
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:  PartiallyFailed (run 'velero restore logs restore001' for more information)

Errors:
  Velero:   pod volume restore failed: error restoring volume: error creating .velero directory for done file: mkdir /host_pods/60880bf2-9d1c-47cf-ba8f-5f8db43b385b/volumes/kubernetes.io~csi/pvc-ee79e0f0-ed62-44f7-987e-16efca1d1cd5/mount/.velero: no space left on device
  Cluster:    <none>
  Namespaces: <none>
```


**Anything else you would like to add:**

Since this is related to creating the `.velero` donefile, the fix might be somewhat relate to #2722.


**Environment:**

- Velero version (use `velero version`): v1.4.0
- Cloud provider or hardware configuration: vSphere


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,nrb,"
--
Scoping this for v1.6 currently; I'm unsure if the fix will be backport-able at this time.
--
",,,,,,,,,,
2809,OPEN,Remove the necessity of object storage as primary backup location,Duplicate; Enhancement/User; Needs Product,2020-10-22 17:46:09 +0000 UTC,rahulgit1988,Opened,,"**Describe the problem/challenge you have**
Nowadays, object storage solution is very pupular and common in OSS forums.
I think that is the reasone Velero backup supports only object storage as backup location.
But for users on-premises or private enviorment, object storage is not very common soultion. and that is the major problem/limitation of Velero.

**Describe the solution you'd like**
- A Object Storage(for example: MINIO) as cache  ==> pod solution ""Velero"" with ""MINIO"" as side car in the containerized cluster enviorment.
- Download objects from ""Object storage cache"" and save them as file in Pod's PV, which is backup with snapshot or safer separated storage in user's enviorment.

**Anything else you would like to add:**
Here we can automate the process of downloading the object from Cache to File storage.

**Environment:**
Openshift Enviorment, K8S Enviorment

- Velero version (use `velero version`): 1.4
- Kubernetes version (use `kubectl version`):1.17
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,nrb,"
--
Closing this as a duplicate of #1229
--
",,,,,,,,,,
2800,OPEN,Backup process should have a time out so that long running backups don't block the backup pipeline,Enhancement/User; Needs Product,2020-12-04 11:57:02 +0000 UTC,ashish-amarnath,Opened,,"https://github.com/vmware-tanzu/velero/issues/2738#issuecomment-662386328


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,nrb,"
--
In thinking about this, I think I'd label this as a P1.

Even once we have multiple backups running at once, this would be useful in preventing a backup from getting stuck.

What I don't know is what a reasonable default is, but I think this requires a short design document with some experimentation.
--

--
@survivant That's a reasonable request, but also a different issue than this one.
--
",survivant,"
--
and also.. if a backup is stalled at a particular state.. we should be able to kill it. 

````
 have a issue.  I don't know how to delete a backup that it's stuck.
if I see velero logs
time=""2020-12-03T13:23:23Z"" level=warning msg=""Epoll wait failed : interrupted system call"" backup=velero/initial cmd=/plugins/velero-blockstore-openebs logSource=""/go/src/github.com/openebs/velero-plugin/pkg/clouduploader/server.go:302"" pluginName=velero-blockstore-openebs
time=""2020-12-03T13:23:23Z"" level=warning msg=""Epoll wait failed : interrupted system call"" backup=velero/initial cmd=/plugins/velero-blockstore-openebs logSource=""/go/src/github.com/openebs/velero-plugin/pkg/clouduploader/server.go:302"" pluginName=velero-blockstore-openebs
time=""2020-12-03T13:23:24Z"" level=warning msg=""Epoll wait failed : interrupted system call"" backup=velero/initial cmd=/plugins/velero-blockstore-openebs logSource=""/go/src/github.com/openebs/velero-plugin/pkg/clouduploader/server.go:302"" pluginName=velero-blockstore-openebs
time=""2020-12-03T13:23:24Z"" level=warning msg=""Epoll wait failed : interrupted system call"" backup=velero/initial cmd=/plugins/velero-blockstore-openebs logSource=""/go/src/github.com/openebs/velero-plugin/pkg/clouduploader/server.go:302"" pluginName=velero-blockstore-openebs
The backup stop after 89/1659 items backup
I try to stop the backup process by :
velero backup delete initial
root@test-pcl109:/tmp/velero-v1.5.2-linux-amd64# velero backup get
NAME                 STATUS       ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
initial              InProgress   0        0          2020-12-03 13:01:28 +0000 UTC   29d       aws                <none>
initial-without-pv   New          0        0          <nil>                           29d       aws                <none>
root@test-pcl109:/tmp/velero-v1.5.2-linux-amd64#
How can I stop that backup ?   (initial)
````
--

--
yes, but I wanted to point that the timeout could be absolute.. like after 10 min stop the backup.  But if the backup is just long because there are a lot of resources to backup, maybe the backup shouldn't timeout if there are still activity.  Like if there are 2000 items to backup and it's only slow.. the backup could continue. 
--
",,,,,,,,
2798,OPEN,"Question/documentation: migration -- how to ""point to location used by cluster 1"" on EKS w/ service role",Question,2020-08-24 18:03:22 +0000 UTC,shaunc,In progress,,"I have two clusters, and would like to migrate from one to the other. They are setup using velero EKS config with service roles.

The documentation states:

> (Cluster 2) Configure BackupStorageLocations and VolumeSnapshotLocations, pointing to the locations used by Cluster 1, using velero backup-location create and velero snapshot-location create. 

How do I do this? The clusters have separate service roles, and the snapshot location of cluster 1 is simply listed as ""default"" by `velero snapshot-location get`. Perhaps I can hack the S3 permissions and/or the service role setup so that cluster 2 can read the cluster 1 bucket, but how do I ""point to"" the snapshot location (which is where??) and give cluster 2 permissions?

More extensive documentation would be useful.",,,ashish,"
--
@shaunc 
The instruction you are referring to is asking you to create backup storage location and volume snapshot locations in cluster 2 with the same data as those in cluster 1, where you want to migrate from.

The idea is that cluster 2 should be allowed/able to read backups taken from cluster 1. This will let Velero in cluster 2, read the backups and volumesnapshots taken as part of backups from cluster 1 as part of restoring backup from cluster 1 into cluster 2.

I hope this answers your question.
--

--
@shaunc From my, rather limited, understanding of service roles and ISRA. It seems like a feature to associate Kubernetes service accounts with IAM role policies. You might find this link useful https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-cni-walkthrough.html

You can use the velero CLI to create new backup storage location and volume snapshot location
```bash
$ velero create --help | grep ""location""
  backup-location   Create a backup storage location
  snapshot-location Create a volume snapshot location
```
We have some of what you are looking for documented at https://velero.io/docs/v1.4/locations/#additional-use-cases and https://velero.io/docs/v1.4/disaster-case/.
--

--
@shaunc Glad, you were able to figure this out!
We'd appreciate a PR with the docs.
If you are unable to submit a PR atm, please add some pointers, to resources you found helpful, into this issue that will help us update our documentation or can help another user.
--
",shaunc,"
--
@ashish-amarnath -- in fact, that much I figured out. The question is how to set this up for EKS with service roles & ISRA. In fact, I ended up reconfiguring cluster 2 to have the same service account and S3 bucket as cluster 1. However, in general it would be good to have instructions on how to add an additional location, as often when you migrate you don't want to affect the target cluster's own backups. 

It would seem that the snapshot location works by ""magic"" ... no setup seems necessary. I wonder how this would work if I wanted a permission boundary between the two clusters.
--

--
Thanks! ... If I understood this stuff better myself, I'd open a PR. Unfortunately, documentation is a bit either thin or not necessarily relevant.
--

--
So -- as I indicated above, my main workaround was to change the 2nd cluster service account to be the same as the first, so I didn't have to figure out the IRSA reconfigurations. I didn't know about `velar create snapshot-location` which might have helped out with the issues I was running into. (Though really I need ""set snapshot-location"") But I like to provision with terraform, so would prefer a terraform-provider-compatible way of specifying snapshot locations and their linked roles (and not just use blanket EC2 permissions).

Next time I ""cycle"" on this (when I set up a new cluster), I'll see if I can do this more systematically.
--
",,,,,,,,
2794,OPEN,Create a way to do only cluster-scoped resource backup,Enhancement/User,2020-10-22 17:46:43 +0000 UTC,henkypunky,In progress,,"I am about to create a backup of all cluster-scoped resources only using these options:

--incl_namespaces="""" --incl_clusterresources=True

Seems to work except for the fact that the namespace resources which are actually cluster-scoped resources are excluded from the backup. Is there an alternative way to get a backup of cluster-scoped resources including the namespaces (w/o its content)? 
I guess you can't omit the incl_namespaces="""" clause as it defaults to include all namespaces (and its resources).

Thanks!",,,ashish,"
--
@henkypunky I don't see a straightforward way to do this.
The best I could come up with was to use the `--exclude-resources` and `--include-resources` options for the `velero backup create` command.
I am going to mark this as an enhancement request rather than a question so that there is a more straightforward way to do this. 
--
",henkypunky,"
--
@ashish-amarnath Thanks for quick response and action!
--
",,,,,,,,
2780,OPEN,restic: support backup on volumeMode block,Enhancement/User; Needs Product; Restic,2020-08-11 17:42:06 +0000 UTC,shellwedance,Opened,,"**Describe the problem/challenge you have**

With using restic, velero does backup and restore only PVs whose **volumeMode is Filesystem**.
When it creates backup for volumeMode Block PV, partial fail occurs.

**Describe the solution you'd like**

Make restic-plugin-velero support Backup and Restore on **volumeMode Block PV**.

**TODO**
  - Detects the volumeMode of annotated volume in backup controller
  - Change the volume's path if the volumeMode is Block
    - Currently, velero uses only Filesystem PV path
  - Change the restic command if the volumeMode is Block
    - To support block device backup in restic, we should pass device as stdin to restic
    - Likewise, when restoring, we should redirect restic dump result to target device

**Anything else you would like to add:**
```shell
k8s@master1:~$ velero backup get
NAME                                  STATUS            ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
backup-test-with-volumes              Completed         0        0          2020-07-30 05:58:23 +0000 UTC   28d       default            <none>
backup-test-with-volumes-block-mode   PartiallyFailed   1        0          2020-07-30 06:23:26 +0000 UTC   28d       default            <none>
```

```shell
k8s@master1:~$ velero backup logs backup-test-with-volumes-block-mode
time=""2020-07-30T06:23:26Z"" level=info msg=""Setting up backup temp file"" backup=velero/backup-test-with-volumes-block-mode logSource=""pkg/controller/backup_controller.go:519""
time=""2020-07-30T06:23:26Z"" level=info msg=""Setting up plugin manager"" backup=velero/backup-test-with-volumes-block-mode logSource=""pkg/controller/backup_controller.go:526""
time=""2020-07-30T06:23:26Z"" level=info msg=""Getting backup item actions"" backup=velero/backup-test-with-volumes-block-mode logSource=""pkg/controller/backup_controller.go:530""
time=""2020-07-30T06:23:26Z"" level=info msg=""Setting up backup store"" backup=velero/backup-test-with-volumes-block-mode logSource=""pkg/controller/backup_controller.go:536""

...

time=""2020-07-30T06:23:27Z"" level=info msg=""1 errors encountered backup up item"" backup=velero/backup-test-with-volumes-block-mode logSource=""pkg/backup/backup.go:444"" name=wordpress-mysql-85d8c4c78d-cg5nd
time=""2020-07-30T06:23:27Z"" level=error msg=""Error backing up item"" backup=velero/backup-test-with-volumes-block-mode error=""pod volume backup failed: error getting volume path on host: expected one matching path, got 0"" error.file=""/github.com/vmware-tanzu/velero/pkg/restic/backupper.go:179"" error.function=""github.com/vmware-tanzu/velero/pkg/restic.(*backupper).BackupPodVolumes"" logSource=""pkg/backup/backup.go:448"" name=wordpress-mysql-85d8c4c78d-cg5nd
...
```


Some K8S users may use Block mode volumes which are not provided by AWS, GCP, Azure, or CSI. They will try to backup the volumes by restic-plugin.

Thus I think restic-plugin-velero should support Backup/Restore regardless whether target volume is Filesystem or Block.

**Environment:**

- Velero version (use `velero version`): v1.4.2
- Kubernetes version (use `kubectl version`): 1.18.6
- Kubernetes installer & version: kubeadm 1.18.6
- Cloud provider or hardware configuration: 
- OS (e.g. from `/etc/os-release`): Ubuntu 18.04

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
2770,OPEN,Customize defaulting volume backups to restic,Needs Product; Restic,2021-01-21 16:10:49 +0000 UTC,ashish-amarnath,Opened,,"Tracking some of the suggestions that were received on this feature:
- Support for label selectors to choose pods to backup volumes using restic. Using label selection to get pods, return a list of all volumes attached to those pods and backup those volumes using restic
- Exclude pod volumes that share the pods's lifetime. There are certain VolumeSources that share the pod's lifetime. E.g. `emptyDir`, `NFS`, `GlusterFS`, etc https://pkg.go.dev/k8s.io/api@v0.18.4/core/v1?tab=doc#VolumeSource. Should these be included while defaulting pod volume backups to restic?



**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,quentinbadoud,"
--
I agree beacause with Velero 1.5 you can backup all pod volumes by default. Restic tries to backup EmptyDir volume and fails. Then you have partially failed backups even if they are OK.
--
",,,,,,,,,,
2767,OPEN,Velero Restic plugin install for TKGI,Area/CLI; Enhancement/User; Needs Product; Restic,2020-11-03 23:44:47 +0000 UTC,guillierf,In progress,,"When installing Velero/Restic on TKGI, we have to initiate this procedure (at this point in time, the restic pods are in crashloopback mode):
 
https://velero.io/docs/main/restic/:
“The hostPath should be changed from /var/lib/kubelet/pods to /var/vcap/data/kubelet/pods

hostPath:
  path: /var/vcap/data/kubelet/pods
“
 
 
Will there be an elegant way to streamline the installation in the near future?
(maybe a flag during the install to customize the hostpath?)
This will improve customer experience with both products
 ",,,carlisia,"
--
@guillierf thank you, it's a good suggestion, we will consider it.
--
",nrb,"
--
@guillierf Is there a way we can programmatically identify TKGI clusters to differentiate this that you're aware of? Something we can query the Kube API server for?
--

--
@guillierf Ok, thanks. So for TKGI at least, we can detect this label key and set the path appropriately, which seems easy enough.

@dymurray or @sseago Do you know if there's something similar we can do for OpenShift?

I _think_ if we can do this for TKGI, OpenShift, and Rancher, we're covered for the vast majority of cases.
--

--
We can also see if there's a way to plumb this through sig-node.
--
",guillierf,"
--
""kubectl get nodes -o wide --show-labels"" would help

in case of TKGI, you will see the keyword 'bosh.id'

example below:

```
root@PKS-client-VM-WA:~# kubectl get nodes -o wide --show-labels
NAME                                   STATUS   ROLES    AGE   VERSION            INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME   LABELS
9bd2cd0b-c532-490f-bb02-388d1f286080   Ready    <none>   28d   v1.18.8+vmware.1   192.168.3.5   192.168.3.5   Ubuntu 16.04.7 LTS   4.15.0-117-generic   docker://19.3.5     beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,bosh.id=672f827a-11bb-40b9-ac71-7fcd3219bf3b,bosh.zone=PKS-AZ-1,failure-domain.beta.kubernetes.io/zone=PKS-AZ-1,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.3.5,kubernetes.io/os=linux,pks-system/cluster.name=pks-cluster-1-shared-t1,pks-system/cluster.uuid=service-instance_743c09e8-ac84-4f69-95f2-867afbbf8af2,spec.ip=192.168.3.5
aec15c5c-9b86-4acd-8163-617d19a99ac0   Ready    <none>   28d   v1.18.8+vmware.1   192.168.3.7   192.168.3.7   Ubuntu 16.04.7 LTS   4.15.0-117-generic   docker://19.3.5     beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,bosh.id=f4ff3448-9ccb-4707-901d-e6414bb945fb,bosh.zone=PKS-AZ-3,failure-domain.beta.kubernetes.io/zone=PKS-AZ-3,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.3.7,kubernetes.io/os=linux,pks-system/cluster.name=pks-cluster-1-shared-t1,pks-system/cluster.uuid=service-instance_743c09e8-ac84-4f69-95f2-867afbbf8af2,spec.ip=192.168.3.7
e81f77ef-fe1a-45b2-971f-6e6dd5679be8   Ready    <none>   28d   v1.18.8+vmware.1   192.168.3.6   192.168.3.6   Ubuntu 16.04.7 LTS   4.15.0-117-generic   docker://19.3.5     beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,bosh.id=95c6e42c-49e2-4fe2-8c32-5ee0dc421556,bosh.zone=PKS-AZ-2,failure-domain.beta.kubernetes.io/zone=PKS-AZ-2,kubernetes.io/arch=amd64,kubernetes.io/hostname=192.168.3.6,kubernetes.io/os=linux,pks-system/cluster.name=pks-cluster-1-shared-t1,pks-system/cluster.uuid=service-instance_743c09e8-ac84-4f69-95f2-867afbbf8af2,spec.ip=192.168.3.6

```
--
",ashish,"
--
Here are our options for this:

1. Lookup/query kubelet configuration to get the value of `--root-dir`:
    The directory path for the mount point of the kubelet files is controlled by the `--root-dir` flag to the kubelet and is not directly exposed by an API. https://github.com/kubernetes/kubernetes/blob/master/cmd/kubelet/app/options/options.go#L340. This is not feasible.

1. Infer the path based on the distro of Kubernetes:
    - Maintain a map of distros to kubelet root directory.
    - Detect the kubernetes distro. (Not sure how, but assuming this is possible for now)
    - Lookup the kubelet root directory based on the detected distro

1. Offer a `--distro` and `--kubelet-pod-dir` as flags to Velero server.
    - Maintain a map of distros to kubelet root directory.
    - Detect the kubernetes distro. (Not sure how, but assuming this is possible for now)
    - Lookup the kubelet root directory based on the detected distro
    - If the distro cannot be detected or if the map has no entry for the value supplied in the `distro` flag, then use the `--kubelet-pod-dir` flag to get this value.

Because detecting the distro of kubernetes might not be feasible, option 3 above would be most feasible.

--
",,,,
2760,OPEN,Create GitHub Action to auto-update copyright year,CI/CD,2020-07-27 20:48:51 +0000 UTC,nrb,In progress,,"Right now, asking contributors to update the copyright year on file modifications results in a lot of lost time. It would be helpful to automate updating the copyright year when a file is modified.

Investigate using https://github.com/stefanzweifel/git-auto-commit-action to keep copyright year up-to-date on relevant files.

Some things to note:

* The year only needs to be the latest year
* We may need to relax branch protections for it, see docs
* We'll need to make sure we get DCO signatures set up correctly for it
* Which branch gets pushed to might be an issue, so it may not update the actual PR in quest, but happen after a merge.

https://github.com/kubernetes-sigs/cluster-api/blob/master/hack/verify-boilerplate.sh may be a useful reference.",,,nrb,"
--
After doing some inspection on the repo, we have a few valid copyright formats:

`Copyright [latest year] the Velero contributors.` (for files originating from our community)

 `Copyright [year] The Kubernetes Authors.` (for files borrowed from upstream Kubernetes projects without modification)

```
Copyright [year] The Kubernetes Authors.
Modifications copyright [year] the Velero contributors.
```
(for files borrowed from upstream Kubernetes projects with modifications)

---

For generated code such as with protobuffers and generated Kubernetes packages the year is excluded, like so:

`Copyright the Velero contributors.`

For generated Kubernetes files, the copyright is found in `hack/boilerplate.go.txt`. Protobuffers use their own generation tool that doesn't use this file for its header.
--
",,,,,,,,,,
2747,OPEN,⚠️ If you are running against the main branch,Area/Documentation; Public Announcement,2021-02-22 20:05:55 +0000 UTC,carlisia,In progress,,"There has been some changes to the Velero CRDs. Depending on when you installed Velero, you might not have picked up the changes. 

You may run the command below to update without any concern for messing up your install:

`velero install --crds-only --dry-run -o yaml | kubectl apply -f -`",,,nrb,"
--
Is this a duplicate of #2717?
--

--
This seems like it would be an ongoing concern against `main` whenever fields are added to the CRDs.

I think it's something we should add to developer documentation, but maybe not be a pinned issue. What do others think of that?
--

--
@ashish-amarnath I'm not sure I understand what you mean about the background - how does it relate to running this command?

I'm not sure about adding specific commands to the `main` banner...I was more thinking the https://velero.io/docs/v1.4/development/ page. My fear is we'll stuff the warning banner full of messages, and it will get super long.
--

--
Echoing my previous comment, this is something I would like to get in developer documentation and then close out this issue. I'm going to put a v1.6.0 milestone on it.
--
",carlisia,"
--
I didn't think so, this is meant as a temporary warning with a specific instruction of what to do until 1.5 is released, or maybe even longer than that for people who didn't upgrade.
--

--
Yes, good idea. I think adding this warning to the end of the text in the banner for ""main"" will be perfect.
--
",ashish,"
--
Now that we've shipped the 1.5 beta. Should we close this out?
--

--
+1 for adding doc and not having it be a pinned issue.
Maybe just changing the background of this banner on our website should do?
![image](https://user-images.githubusercontent.com/22872138/91336399-ae0ddf00-e786-11ea-9da6-98379f9c9764.png)

--

--
My intention was to draw attention to the warning to convey to our users using the `main` version of the docs can result in a bumpy ride.
I believe your intention is to update the development doc w/ instructions on how to deploy from main.
I am +1 on updating the development docs too.
--
",,,,,,
2745,OPEN,Question about pvc migration,Enhancement/User; Needs Product,2020-07-22 18:15:39 +0000 UTC,amehub,Opened,,"**Describe the problem/challenge you have**
Existing k8s cluster
Gluster storage - servers external to K8S
K8s -> Gluster using heketi 

Want to migrate the pv data to a new gluster

Have built a new gluster cluster

Need to recreate storage classes to point to new gluster
Manually copy data off pods
Stop pods
Delete pvc
Recreate pvc
Start pod
Copy data back into pv


**Describe the solution you'd like**
An automated solution to backup named pod or all pod pv data and migrate the pvc to a new gluster.

**Anything else you would like to add:**
Want to keep the existing pvc names so no changes are required to pods.  
Hence delete pvc, recreate pvc to new storage class pointing to new gluster


**Environment:**

- Velero version - na
- Kubernetes version - 1.18 
- Kubernetes installer & version: Kubespray 
- Cloud provider or hardware configuration: Hardware
- OS (e.g. from `/etc/os-release`):  Ububntu 18.04.03

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,nrb,"
--
I think our [storage class remapping support](https://velero.io/docs/v1.4/restore-reference/#changing-pvpvc-storage-classes) and [restic integration](https://velero.io/docs/v1.4/restic/) may be helpful to you. To my knowledge, there's no Gluster plugins for Velero.

If those two don't fit your needs, then I think this will be a feature request that we'd need to slot into our roadmap at some point.
--
",,,,,,,,,,
2722,OPEN,remove .velero folder after restic restore,Bug; Restic; Restic - GA,2020-10-22 17:47:55 +0000 UTC,vinayus,Opened,,"i have been using velero(with restic) for my local backup and restore purposes. I did observe that during restore a folder named **.velero** is created in the volumes of every statefulset that is restored back.

can we have an enhancement where we can remove **.velero** as a cleanup?

why would this cause an issue even though it doesn't contain any data is because statefulset like kafka would parse the folder and complain them if it is not part of their topic partition table.

as a workaround i manually delete it and restart the pod but would love to have this minor enhancement included in newer release

```
[supreme@worker-16070 kafka-data]$ ls -a
----trimmed----
__consumer_offsets-8
__consumer_offsets-9
.lock
log-start-offset-checkpoint
meta.properties
recovery-point-offset-checkpoint
replication-offset-checkpoint
.velero
```
",,,nrb,"
--
Thanks for the report @vinayus! This seems like a fairly low effort fix, so I'll add it to the backlog.
--
",ashish,"
--
There are a couple of things that make this change non-trivial.
- The `doneFile` is created as [ioutil.WriteFile(filepath.Join(volumePath, "".velero"", string(restoreUID)), nil, 0644)](https://github.com/vmware-tanzu/velero/blob/main/pkg/controller/pod_volume_restore_controller.go#L399)
- Updating the permissions on the `doneFile` to `0666` in the `WriteFile`  call doesn't work because 
```bash
root@restic-jq4rh:/host_pods# hostname
restic-jq4rh
root@restic-jq4rh:/host_pods# umask
0022
root@restic-jq4rh:/host_pods#
```
- This can be solved by `os.Chmod` to `0777` on the `.velero` and `0666` on the `doneFile`. This will let the `restic-wait` init container to delete the `doneFile`
- However, the `restic-wait` init container cannot delete the `.velero` directory even though the permissions on them are `0777`
From the `velero-restic-restore-helper` code
```bash
Permissions on /restores/pvc2-vm/.velero [drwxrwxrwx]
Failed to delete directory /restores/pvc1-vm/.velero: unlinkat /restores/pvc1-vm/.velero: permission denied
```
From exec-ing into the `restic-wait` container
```bash
$ kcn csi-app exec -it csi-app1 -c restic-wait bash
nobody@csi-app1:/$ hostname
csi-app1
nobody@csi-app1:/$ rm -rf /restores/pvc1-vm/.velero/
rm: cannot remove '/restores/pvc1-vm/.velero/': Permission denied
nobody@csi-app1:/$ rm -rf /restores/pvc2-vm/.velero/
rm: cannot remove '/restores/pvc2-vm/.velero/': Permission denied
```
this is most likely because of how the volume is shared by the restic daemon set with the restic-wait container.

The solution to this is to have the restic daemon-set pod to delete the `.velero` directory once `restic-wait` init container has completed. But this requires the `restic-wait` init container to communicate w/ the restic daemon set pod that it has ""finished"" waiting and has completed. The deletion of the `doneFile` can be used to signal this.


--

--
@nrb removing this from the 1.5 milestone.
--
",,,,,,,,
2701,OPEN,"Velero backup loops every minute when ""ETCD storage quota exceeded"" is meet",Enhancement/User; P2 - Long-term important,2020-11-24 06:45:36 +0000 UTC,fredleger,In progress,,"**Describe the problem/challenge you have**
One of the cluster i run is on a managed k8s offer. The provider defines quota for ETCD and we reached it out at somme point.
This make the velero backups to fail every minutes (1440 backups of each schedule by day) making the situation even worst, acting like a snowball.
The error message in the backup is ""ETCD storage quota exceeded"".

**Describe the solution you'd like**
I think it can be wise to define a maximum retries threeshold for a backup in the schedule something like `maxRetries` with a descent default value (IMHO 1440/day is not).

**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]

**Environment:**

- Velero version (use `velero version`): v1.3.2
- Kubernetes version (use `kubectl version`):  Client Version: v1.15.12 / Server Version: v1.15.11
- Kubernetes installer & version: managed by OVH / v1.15.11
- Cloud provider or hardware configuration: OVH Public Cloud (openstack based)
- OS (e.g. from `/etc/os-release`): Ubuntu 18.04 LTS

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,nrb,"
--
@fredleger Thanks for the report! That's definitely frustrating.

Can you clarify your Velero schedule frequency for me? You're taking a backup every minute?
--

--
Ahh, got it. Thanks for that clarification.
--
",fredleger,"
--
@nrb not at all ! this is which is the most suprising.
Backups were scheduled once a day and when the ETCD quota issue arise they started to be tried every minutes (all with status new)
--

--
exemple of it:

```sh
kubectl get backups -l app=solr | awk '{print $1}' | perl -ne '/(.*-2020\d{4})/ && print ""$1\n""' | sort | uniq -c
   1 solr-cloud-solr-20200511
   1 solr-cloud-solr-20200512
   1 solr-cloud-solr-20200513
   1 solr-cloud-solr-20200514
   1 solr-cloud-solr-20200515
   1 solr-cloud-solr-20200516
   1 solr-cloud-solr-20200517
   1 solr-cloud-solr-20200518
   1 solr-cloud-solr-20200519
   1 solr-cloud-solr-20200520
   1 solr-cloud-solr-20200521
   1 solr-cloud-solr-20200522
   1 solr-cloud-solr-20200523
   1 solr-cloud-solr-20200524
   1 solr-cloud-solr-20200525
   1 solr-cloud-solr-20200623
   1 solr-cloud-solr-20200624
   1 solr-cloud-solr-20200625
   5 solr-cloud-solr-20200720
  10 solr-cloud-solr-20200721
```

As you can see starting at the 2020-07-20 a lot of backups started to be created (i cleaned it a bit since). Bu still we can see the raise in the number of backups
--

--
Any update on this ?
Thsi was really a major issue and velero has been disabled since that. So the infra has no backups at all !
--

--
Thanks for this feedback @sylu. On our side there was no crd upgrade at the time of the error if i remember well. But might be différent conditions but similar behavior. AMHA the concern is not that the backup is failing but that it's not secured enough to avoid the overcost (almost infinite if you don't catch it on the Bill)
--

--
@nrb at least this give à chance to reproduce and fix the potential risk
--
",sylus,"
--
@fredleger 

I think we are seeing this too, where the backups kept looping even though the backup was set every day though we are on 1.5.2. We are unsure if is related to ETCD though.

A bit concerning as resulted in a significant cost overrun before was caught and backups removed.
 
--

--
Think me and @zachomedia found the problem. Seems that during the crd upgrades, either a field was added and it’s set to `null` on all the old schedules. So velero was failing to update the last backup time on the schedule objects.

```sh
Error in syncHandler, re-adding item to queue” controller=schedule error=“error updating Schedule’s LastBackup time to 2020-11-21 04:00:02.07616686 +0000 UTC m=+259464.959276461: error patching schedule: Schedule.velero.io \“octopus-backup\” is invalid: [status.validationErrors: Invalid value: \“null\“: status.validationErrors in body must be of type array: \“null\“, spec.template.volumeSnapshotLocations: Invalid value: \“null\“: spec.template.volumeSnapshotLocations in body must be of type array: \“null\”
```
--
",,,,,,
2697,OPEN,unable to delete velero restore objects from the cloud storage,Bug; P1 - Important,2020-08-05 17:05:13 +0000 UTC,sawansharma005,Opened,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)

I was trying to delete the restore objects using the below command but it is not deleting the restore objects from cloud storage.

`velero restore delete ${restore_name}`


**What did you expect to happen:**
It should delete the restore from velero as well as from the cloud storage

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

**Server Side logs**
```
root@testvm:~# velero restore get
NAME                        BACKUP                        STATUS      WARNINGS   ERRORS   CREATED                         SELECTOR
wordpress-mysql-service     velero-daily-20200708054623   Completed   6          0        2020-07-08 05:48:20 +0000 UTC   <none>
wordpress-service-restore   velero-daily-20200708054623   Completed   6          0        2020-07-08 05:49:48 +0000 UTC   <none>
root@testvm:~# velero restore delete --all
Are you sure you want to continue (Y/N)? Y
Restore ""wordpress-mysql-service"" deleted
Restore ""wordpress-service-restore"" deleted
root@testvm:~# velero restore get
root@testvm:~#
```

**Cloud side restore components are not deleting** 

<img width=""744"" alt=""github"" src=""https://user-images.githubusercontent.com/35069795/86884163-ccedde80-c110-11ea-803f-9e47a415a58a.PNG"">



**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): 1.4.0
- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`): ubuntu


**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,carlisia,"
--
This needs testing to see if it's a bug.
--
",ashish,"
--
A quick look at the code for this.
https://github.com/vmware-tanzu/velero/blob/main/pkg/cmd/cli/restore/delete.go#L108-L115
Also, `restore_controller` does not reconcile the restore object deletion.

We don't delete the restore data from the backup storage location. The only time we delete restores from the BSL is when the associated backup is deleted.
Not deleting the restore from the BSL on `velero restore delete` maybe by design and not a bug.

I think we can make a case to add a flag that will delete the restore from the BSL.

Going to let other @vmware-tanzu/velero-maintainers chime in on this.
--

--
I think I am going to recant myself and call a bug.
Scenario:
1. Create a backup `b1`
1. Restore from `b1` as `r1`
1. Restore from `b1` as `r2`
1. `velero restore delete r1` will delete the kubernetes object, and not the restore in the BSL.
1. Now, `velero delete backup b1 --confirm` will only discover `r2` as the restore corresponding to `b1` and therefore will delete only `r2` from the BSL. This will leak `r1` in the BSL that will have to be manually deleted.

Ran this on a cluster to confirm the behavior described in this issue.

Thank you @sawansharma005 for reporting this.
--
",,,,,,,,
2690,OPEN,Providing '/' in namespace leads to a crash,Bug; P2 - Long-term important; Size/S,2020-10-22 17:51:15 +0000 UTC,rohandvora,In progress,,"**What steps did you take and what happened:**
Running the following command leads to velero pod crashing. The backup gets stuck in `in-progress` state.
`velero backup create backup-3 --include-namespaces /`


**What did you expect to happen:**
Though the namespace isn't valid, the expected behavior would be to fail the backup with validation error.

**The output of the following commands will help us better understand what's going on**:
- `kubectl logs deployment/velero -n velero`
https://gist.github.com/rohandvora/c1d7a39445bafef63650c2bf4d5c2d45
- `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
```yaml
apiVersion: velero.io/v1
kind: Backup
metadata:
  annotations:
    velero.io/source-cluster-k8s-gitversion: v1.17.4+vmware.1
    velero.io/source-cluster-k8s-major-version: ""1""
    velero.io/source-cluster-k8s-minor-version: ""17""
  creationTimestamp: ""2020-07-06T16:57:27Z""
  generation: 2
  labels:
    velero.io/storage-location: test-cred-dp
  name: backup-3
  namespace: velero
  resourceVersion: ""19813238""
  selfLink: /apis/velero.io/v1/namespaces/velero/backups/backup-3
  uid: 830d4ccb-a37e-45d7-a3a1-29f8081e131b
spec:
  hooks: {}
  includedNamespaces:
  - /
  storageLocation: test-cred-dp
  ttl: 720h0m0s
  volumeSnapshotLocations:
  - default-aws
status:
  expiration: ""2020-08-05T16:57:27Z""
  formatVersion: 1.1.0
  phase: InProgress
  startTimestamp: ""2020-07-06T16:57:27Z""
  version: 1
```

**Environment:**

- Velero version (use `velero version`): `v1.4.0`
- Velero features (use `velero client config get features`):  `features: <NOT SET>`
- Kubernetes version (use `kubectl version`): 
```
Client Version: version.Info{Major:""1"", Minor:""16"", GitVersion:""v1.16.0"", GitCommit:""2bd9643cee5b3b3a5ecbd3af49d09018f0773c77"", GitTreeState:""clean"", BuildDate:""2019-09-18T14:36:53Z"", GoVersion:""go1.12.9"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.4+vmware.1"", GitCommit:""2e240196fa7251c7f1ff96392db857c281f534e5"", GitTreeState:""clean"", BuildDate:""2020-04-16T04:00:11Z"", GoVersion:""go1.13.8"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kubernetes installer & version: VMWare Tanzu provisioned cluster.
- Cloud provider or hardware configuration: VMWare Tanzu provisioned cluster.",,,ashish,"
--
@rohandvora were you expecting all the namespaces to be backedup?
`/` is not a valid kubernetes namespace name.
Kubernetes resource should be a valid DNS-1123 label- consisting of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character.
If your intention was to backup all namespaces, then you can use `*`

Closing this issue. Please reopen or reach out if you have more questions.
--

--
It could also be possible that you were expecting Velero to handle this error more gracefully than a crash.
Reopening to clarify.
--
",rohandvora,"
--
Hey @ashish-amarnath. Yup. I was expecting Velero to handle this error more gracefully rather than crashing. Though / is not a valid name, there is no validation being done. Though we expect users to provide a valid namespace, not doing so shouldn't lead to a crash. Having said that, this isn't a critical bug blocking us, but would be good to fix it 🙂
--
",,,,,,,,
2684,OPEN,Download PV data with `velero backup download`,Area/CLI; Area/Plugins; Needs Product; Size/L,2020-07-15 22:34:47 +0000 UTC,sgielen,Opened,,"<sub>First of all, thanks for making such a nice application! It took me a bit to get it running on my arm64 cluster because of #2466, and also to build an arm64 version of the velero-plugin-for-aws image -- I don't have any AWS volumes attached to my bare-metal cluster of course, but I think the documentation doesn't specify whether I need it to backup **to** AWS, so I installed it anyway...</sub>

**Describe the problem/challenge you have**
I've just created a backup of my cluster. I use OpenEBS volumes, so I installed the `openebs/velero-plugin-arm64:1.11.0` plugin and created a VolumeSnapshotLocation. This worked according to `velero backup describe --details`, and the volume backup files are visible in my S3 bucket as separate files within the prefix specified!

However, now I'd like to inspect that the backup is complete, so I ran `velero backup download`, but the resulting `.tar.gz` file doesn't contain the volume backup files.


**Describe the solution you'd like**
A `velero backup download --download-pvc-data`, as suggested by #2006 for restic backups, would be great for volume plugins as well.


**Environment:**

- Velero version (use `velero version`): Client & server version 1.4.0
- Kubernetes version (use `kubectl version`): Client version 1.17.4, server version 1.18.3+k3s1
- Kubernetes installer & version: k3os v0.10.0
- Cloud provider or hardware configuration: Bare-metal arm64 cluster of 4 nodes, various Raspberry Pi devices; storage provided by OpenEBS 1.11.0 (arm64)
- OS (e.g. from `/etc/os-release`): k3os v0.10.0

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,nrb,"
--
Hi @sgielen - thanks for the feature request!

I think this is definitely a reasonable request, and not quite the same as 2006. Also, given how Velero works, it's unfortunately not going to be a simple thing to add, since right now snapshots made by the snapshot plugins are kind of black boxes to Velero. Restic snapshots are, however, stored in the same bucket.

I'm going to label this and add it for consideration, though, as I think it's something that will be useful to a lot of folks.
--
",,,,,,,,,,
2671,OPEN,Hybrid VolumeSnapshot/Restic backup (consistent; online; point-in-time filesystem backup),Enhancement/User; Needs Product,2020-07-16 22:48:03 +0000 UTC,asoltesz,Opened,,"**Describe the problem/challenge you have**

Velero doesn't have consistent, point-in-time filesystem backups for my storage provider (Rook/Ceph) and several others. 

It does have Restic integration which can take a filesystem backup from Persistent Volumes but in order to make it consistent, one has to stop the workload, otherwise the files may change inconsistently with each other as the backup progresses.

Velero also has support for the generic VolumeSnapshot API (snapshot.storage.k8s.io/v1beta1) but that can only take onsite backups, as opposed to storing the backup in an offsite repository (e.g.: S3 bucket). VolumeSnapshots are stored at a location managed by the cloud provider and typically cannot even be moved between regions so they cannot be called offsite.

**Describe the solution you'd like**

Combine the VolumeSnapshot capability with Restic backups:

- Velero takes a VolumeSnapshot, so a new VolumeSnapshotContent is created by the CSI driver of the storage provider

- Velero creates a standalone, temporary Restic pod with a newly created PVC which is sourced from the VolumeSnapshot taken in the previous step (PVCs support this type of creation from VolumeSnapshots)

- Velero runs Restic on the new PV and stores the files in the Repository in the remote, offsite backup repository (e.g.: s3 bucket). In this case, incremental backups may also be possible so backup is faster and consumes less storage.

- After successful backup, Velero drops the the temporary Restic pod and also drops the temporary backup PV/PVC and the VolumeSnapshot(Content).

Restore would be the same as a normal Restic restore.

This would combine the advantages of both backup mechanism (VolumeSnapshot+Restic) and allow snapshot-style, point-in-time, online and offsite backups for any storage provider that has a CSI plugin supporting the VolumeSnapshot API. 

**Anything else you would like to add:**
-

**Environment:**

- Velero version (use `velero version`): v1.4
- Kubernetes version (use `kubectl version`): v1.15.11
- Kubernetes installer & version: Rancher 3.5.2
- Cloud provider or hardware configuration: Hetzner Cloud with Rancher/RKE created cluster with Rook/Ceph as storage provider
- OS (e.g. from `/etc/os-release`): CentOS 7

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
2665,OPEN,"Document ""in-place"" migration use case",Area/Cloud/vSphere; Area/Documentation; P1 - Important,2020-06-24 20:03:44 +0000 UTC,nrb,Opened,,"**Describe the problem/challenge you have**
Velero can help users upgrade their Kubernetes cluster to a new version without making snapshots of volumes, but this case is not documented.

This is especially relevant in on-premises/air-gapped installations that have a capped amount of storage.


**Describe the solution you'd like**
Add a new use case that describes how to upgrade a Kubernetes cluster version without taking a snapshot of volumes, instead using the existing volumes ""in-place.""

This will work because in this case, Velero is using the PersistentVolume's `volumeID` as-is, without trying to make a snapshot or create a new volume on restore.


**Anything else you would like to add:**

* PVs should have their deletionPolicy set to `Retain` so that the volume is kept on the storage platform even after the PersistentVolume document is deleted.
* The backup should be made with `snapshot-volumes=false`, so that the PersistentVolume's `volumeID` is perserved and no snapshot/volumeID remapping is done on restore.
* Document should note that the current Kubernetes cluster should be stopped/deleted before starting the restore on the new cluster - otherwise, there will be contention on the volume since the two Kubernetes clusters are not aware of each other, and two PersistentVolumes will be pointing to the same volume on storage.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
2663,OPEN,Investigate using PROW,CI/CD,2020-07-27 23:19:17 +0000 UTC,carlisia,Opened,,"Let's investigate if it's worth a while adding prow to our workflow.

https://github.com/jpmcb/prow-github-actions

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,nrb,"
--
After looking at some limitations of GitHub Actions secrets when they come from forks, I think this particular project is worthwhile. Specifically, trying to assign reviewers via GitHub Actions is apparently not possible right now without some workarounds, as PRs from forks don't get write access to the repo for security reasons.

However, it should be noted that the linked GitHub Action isn't actually Prow - it is instead a set of GitHub cron actions that will run on a schedule (they recommend once an hour) and respond to some of the same commands that Prow does. 
--
",,,,,,,,,,
2654,OPEN,Velero public API (please take note),Breaking change; Public Announcement; kind/tech-debt,2020-12-16 23:48:27 +0000 UTC,carlisia,In progress,,"Please take note that what Velero intentionally exposes as a public APIs are the packages below:

https://github.com/vmware-tanzu/velero/tree/master/pkg/apis/velero/v1

https://github.com/vmware-tanzu/velero/tree/master/pkg/plugin

https://github.com/vmware-tanzu/velero/tree/master/pkg/install

https://github.com/vmware-tanzu/velero/tree/master/pkg/client

https://github.com/vmware-tanzu/velero/tree/master/pkg/generated

""github.com/vmware-tanzu/velero/pkg/builder""

We will be doing work to move code that we don't intend to be for public consumption into the `internal` directory structure so we don't expose packages unintentionally.

If you are relying on packages outside those listed above that are intentionally for public consumption, please let us know.
",,,nrb,"
--
@betta1 @dymurray @vishnuitta Letting you know as you depend on Velero code.
--

--
> i assume we will chat about this in the next 1-2 community meetings. thanks

@michmike Yeah, I think this will be an on-going community meeting agenda item.
--
",michmike,"
--
i assume we will chat about this in the next 1-2 community meetings. thanks

--
",jwmatthews,"
--
cc @sseago @alaypatel07 
--
",betta1,"
--
thanks @nrb for the heads up. 

cc @phuongatemc 
--
",mynktl,"
--
Hi @carlisia 

We are using following packages:
- https://github.com/vmware-tanzu/velero/tree/master/pkg/builder
       - We are using this package to build backup/restore/schedule resources. We can write our own package for this, but It will be good to have this package exposed since it includes builder code of Velero resources.

- https://github.com/vmware-tanzu/velero/tree/master/pkg/backup 
       - We are using this package to create delete request for backup. https://github.com/vmware-tanzu/velero/blob/master/pkg/backup/delete_helpers.go

- https://github.com/vmware-tanzu/velero/tree/master/pkg/discovery
       - We are using this package for resource discovery. https://github.com/vmware-tanzu/velero/blob/master/pkg/discovery/helper.go
--
",arianitu,"
--
Outside of the public ones, we are using:

	""github.com/vmware-tanzu/velero/pkg/builder""
--
"
2634,OPEN,PVC backup doesn't happen when pod restarts and changes name,Enhancement/User; Needs Product; Restic,2020-07-29 16:01:08 +0000 UTC,ac-hibbert,Opened,,"**What steps did you take and what happened:**

This is when using the restic feature. When a backup runs the persistent volume backup is related to the pod name and if the pod restarts it will not backup this pods volume.

**What did you expect to happen:**

Some kind of retry or picking up the new pod name. The pod runs in a deployment so the start part is the same just the end part is different.

**Environment:**

- Velero version (use `velero version`): 
```
v1.3.1
```
- Velero features (use `velero client config get features`): 
```
$ velero client config get features
features: <NOT SET>
```
- Kubernetes version (use `kubectl version`):
```
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.2"", GitCommit:""59603c6e503c87169aea6106f57b9f242f64df89"", GitTreeState:""clean"", BuildDate:""2020-01-23T14:21:36Z"", GoVersion:""go1.13.6"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""16+"", GitVersion:""v1.16.8-eks-e16311"", GitCommit:""e163110a04dcb2f39c3325af96d019b4925419eb"", GitTreeState:""clean"", BuildDate:""2020-03-27T22:37:12Z"", GoVersion:""go1.13.8"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kubernetes installer & version:
EKS
- Cloud provider or hardware configuration:
AWS EKS
- OS (e.g. from `/etc/os-release`):
Amazon Linux 2

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""I would like to see this bug fixed as soon as possible""
- :-1: for ""There are more important bugs to focus on right now""
",,,ashish,"
--
@hibbert thank you for raising this issue.
To backup pod volumes using restic, the main Velero backup process inspects each pod it is backing up to discover volumes to backup using restic. On discovering a volume to be backed up using restic, a `podVolumeBackup` object is created that will cause the restic daemon set pod, that mounts /var/lib/kublet/pods` to run restic commands to backup the specific volume. 
So there is a very tight coupling between the pod, and the restic backup process. For that reason, I am not sure how feasible retry would be in this scenario, especially if the pod, on restart, relocates to another node in the cluster.

@vmware-tanzu/velero-maintainers Please feel free to chime in.
--

--
@hibbert Supporting this scenario will require implementing retries into the back up process. That is not currently on our roadmap.
--
",ac,"
--
Any updates on this?
--

--
Hi @carlisia thanks it would be really useful to us.

We'd be happy to change to automatically backup all volumes in a pod, all our PVCs need backing up. I'm not sure what you mean by ""Between that + scheduling maybe it will make it easier with some scripting in the mix."" though. Could you elaborate. For us our backup is rather long (see #2115) it's usually someone upgrading their app which causes the restart which causes this problem.
--
",carlisia,"
--
Hey @hibbert we will keep this issue open for future reference and analysis if it's something we should add to the roadmap. It is not something that we can decide at this moment if we will add or not.

I know it's far from ideal, but we do have a new feature that allows for backing up all volumes in a pod automatically (https://velero.io/docs/main/restic/#to-back-up). Between that + scheduling maybe it will make it easier with some scripting in the mix.
--
",,,,,,
2622,OPEN,Add support for s390x,Enhancement/User; Help wanted,2021-01-05 05:44:04 +0000 UTC,e-desouza,Opened,,"**Describe the problem/challenge you have**
Take OpenShift backups on IBM Z/IBM LinuxONE systems

**Describe the solution you'd like**
s390x support similar to arm64, ppc64lee

**Environment:**

- Velero version (use `velero version`): v1.4.0 and above
- Kubernetes version (use `kubectl version`):  Kubernetes 1.16
- Cloud provider or hardware configuration: s390x arch (z13, z14, z15, LinuxONE, LinuxONE II, LinuxONE III)
- OS (e.g. from `/etc/os-release`): linux

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,e,"
--
@ncdc can you please assign this to me? I have a PR in progress.
--

--
Replicating what the other platforms did, I'm looking for a place to upload the restic binary to. [This](https://github.com/vmware-tanzu/velero/compare/master...e-desouza:master#diff-b825b986892c003b91e1beafc02e55ce6ace288c4d7ff3d0a7ed4db7f3333c63R31) part specifically needs it.
Other than that it's all ready and merged to latest at: https://github.com/e-desouza/velero
--
",nrb,"
--
@e-desouza Any update on your PR?
--
",jenting,"
--
It needs the restic supports s309x platform, ref to https://github.com/restic/restic/issues/2780.
--
",,,,,,
2612,OPEN,Allow skipping immediate backup in Schedule definition,Enhancement/User; Help wanted,2020-06-09 19:31:10 +0000 UTC,aakhundov,Opened,,"**Describe the problem/challenge you have**

As a part of setting up a clean kubernetes cluster with the intention of restoring an existing velero backup onto it, I create a velero `Schedule` for further backups (to the same destination, where my existing backup resides). Schedule creation triggers an immediate backup of the clean cluster, which is not required and, to some extent, interferes with the existing backup assets (e.g., creates a superfluous backup record and alters restic repos). Also the time a new `Schedule` is created is not necessarily a good time for taking a backup.

**Describe the solution you'd like**

I've failed to find a flag for skipping the immediate backup: both within `Schedule` [definition](https://velero.io/docs/v1.4/api-types/schedule/) and among other configuration options. Maybe it would make sense to add such a flag (e.g., `immediateBackup: true`) to the `Schedule` definition. The default behaviour would stay like it is now, but those who don't want an immediate backup on schedule creation could configure it to be skipped.

**Environment:**

- Velero version (use `velero version`): 1.4.0
- Kubernetes version (use `kubectl version`): 1.17.4
- Kubernetes installer & version: kubeadm 1.17.4
- Cloud provider or hardware configuration: currently, VMs
- OS (e.g. from `/etc/os-release`): Ubuntu 18.04.4 LTS

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""",,,skriss,"
--
xref #1980 
--
",,,,,,,,,,
2607,OPEN,Remove obsolete; generated client code,kind/tech-debt,2020-12-08 01:52:28 +0000 UTC,carlisia,Opened,,"Once all the Velero resources and controllers have been converted to the `kubebuilder` framework and the code has changed to use those libraries, remove all of the generated code that is no longer used (clientset, listers, informers).

- [ ] add the target below to the makefile, which will generate the `zz_generated.deepcopy.go`. Note that the value of the `paths` flag can be any location, or even `./...`.

- [ ] remove obsolete markers from api types (ie `+genclient` and `+k8s:`, but not `+kubebuilder`)

- [ ] remove the script that generates that code (`update-generated-crd-code.sh`)

- [ ] add the `generate` target to the `update` target.

- [ ] update the `verify-gen` target


```
# Generate code
generate: controller-gen
	$(CONTROLLER_GEN) object:headerFile=""hack/boilerplate.go.txt"" paths=""./pkg/apis/velero/v1""
```

```
verify: verify-gen
ifneq ($(SKIP_TESTS), 1)
	@$(MAKE) shell CMD=""-c 'hack/verify-all.sh'""
endif

verify-gen: generate
	@if !(git diff --quiet HEAD); then \
		git diff; \
		echo ""files were out of date, `make generate` was run""; exit 1; \
	fi
```

Note:
In the `groupversion_info.go` file, try:

```
        // Adding this:
	SchemeBuilder = &scheme.Builder{GroupVersion: SchemeGroupVersion}
        // And removing this (and also `addKnownTypes`):
	SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes)
```
Run the tests to see if they pass. Just something I wanted to try if it works when using only the kubebuilder framekwork.

Might have to add:
```
	_ = AddToScheme(scheme)
       +kubebuilder:scaffold:scheme
```

This is how each type gets registered as per kubebuilder auto-generator:
```
func init() {
	SchemeBuilder.Register(&BackupStorageLocation{}, &BackupStorageLocation{})
}
```

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,,,,,,,,,,,,
2601,OPEN,design for running multiple velero backups/restores concurrently,Area/Design,2021-02-03 17:43:28 +0000 UTC,skriss,Opened,,,,,,,,,,,,,,,,
2597,OPEN,Modernize the Velero code base,Epic,2021-02-19 23:17:16 +0000 UTC,carlisia,In progress,,"## TODO

v1.5
- [x] add kubebuilder framework + convert BSL resource - https://github.com/vmware-tanzu/velero/issues/2604
- [x] add a BSL controller to handle validation + update BSL status phase - https://github.com/vmware-tanzu/velero/issues/1967
- [x] convert ServerStatusRequest resource/controller to the kubebuilder framework - https://github.com/vmware-tanzu/velero/issues/2605

v1.6
- [ ] convert download request resource/controller - https://github.com/vmware-tanzu/velero/issues/2656

v1.7
- [ ] convert podvolume backup - https://github.com/vmware-tanzu/velero/issues/3454
- [ ] convert podvolume restore
- [ ] convert backup sync controller

v1.?
- [ ] convert delete backup request
- [ ] convert restic repository
- [ ] convert schedule
- [ ] convert backup
- [ ] convert restore
- [ ] remove obsolete code - https://github.com/vmware-tanzu/velero/issues/2607
- [ ] convert BackupTracker controller - https://github.com/vmware-tanzu/velero/issues/2657

---

### Recipe for converting:

- Change controller
- Change server.go file to call this new controller; don't forget to delete all the controller related variables (look at previous PRs for examples)
- Move any logic to under `internal`; add unit tests if there are none
- Change controller tests to use gomega, for consistency with the new controller tests
- Important: change the corresponding API file to have `kubebuilder` markers in addition to the existing markers (look at previous PRs for examples)
- add `_types` to the end of the api file name

**Note for developers:**

If you get a `"" not found""` error when attempting to patch a status, it is very likely a case of the status subresource not being enabled in the CRD. The way to verify this is by running `kubectl get crd $CRDNAME` and then checking if `.spec.versions.subresources.status` is set. Here are the usual ways to address this:
- the equivalent of these lines: https://github.com/vmware-tanzu/velero/blob/c663ce15ab468b21a19336dcc38acf3280853361/pkg/apis/velero/v1/backupstoragelocation_types.go#L109-L110
- run `make update`
- run `v install --crds-only --dry-run -o yaml | kubectl apply -f -` to ensure the updated CRD is re-installed.
---

## What
To modernize the Velero code base and tests and bring it more up-to-date and consistent with newer Kubernetes apps, while cleaning up technical debt along the way.

## Why
- opportunity to clean up some technical debt by way of removing *and keeping* logic out of controllers
- (much) better and sustainable tests
- conformity with many k8s apps
- ability to use the runtime-controller library/client instead of generated informers/listers that live in our tree

Update: it will also greatly speed up our `make update`, `make ci` and `make verify` targets since it will no longer involve generating code when running those.

## How
The end result will...

- follow what `kubebuilder` generates as far as files and folder structure, which will make the project look consistent with many other newer projects/apps
- use the `controller-runtime` library client to CRUD resources instead of the `go-client` library, which is more convoluted by comparison
- use `kubebuilder`/`controller-runtime` related test libraries instead of current mocks and action oriented style of testing for controllers, which is a very ineffective and convoluted way to test in the opinions of many
- follow the `controller-runtime` examples and best practices for both controller code and tests, which should help us move and consistently keep business logic out of controllers, which is not the case today

## Strategy

For this change to work in an incremental manner, we need to be able to CRUD some resources with the existing `go-client` library, and some resources with the `runtime-controller` library. This has been accomplished with the PR below, which instantiates the `runtime-controller` Manager and uses this manager's client to CRUD the BSL resource:

https://github.com/vmware-tanzu/velero/pull/2561

The idea is to build on this PR and convert one resource/controller at a time. We have 10 resources and 1 (BSL) is done, so that leaves us with 9.

We would leave the backup and restore ones for last, since they are the most involved in terms of logic and tests.

We would start converting the smallest resource/controller, which is the `ServerStatusRequest`, to get an idea of how long it takes to do this work for 1 resource/controller, end-to-end. This would help estimate the reminder of the work.

While we work on this transition, we will keep the existing api package/library in its entirety, in case anyone is importing and using this package. Keeping this around is of no cost since it's all generated in an automated way.

For the Velero to transition completely, it would entail removing this (by then) obsolete package, which is a braking change. So we would like to spread this work over the current and next few releases to be able to introduce this change with v2.0.

**Vote on this issue!**

This is an invitation to the Velero community to vote on issues, you can see the project's [top voted issues listed here](https://github.com/vmware-tanzu/velero/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc).  
Use the ""reaction smiley face"" up to the right of this comment to vote.

- :+1: for ""The project would be better with this feature added""
- :-1: for ""This feature will not enhance the project in a meaningful way""
",,,carlisia,"
--
@vmware-tanzu/velero-maintainers PTAL.
--

--
@nrb / @ashish-amarnath: I'm thinking https://github.com/vmware-tanzu/velero/blob/main/pkg/controller/backup_tracker.go, even though is in the `controller` package, is not a controller and has nothing to be converted. Just wanted to get a sanity check.
--
",,,,,,,,,,
2587,OPEN,[Epic] Multi-Tenancy and Self Service,Epic; Needs Product; P2 - Long-term important,2021-03-12 20:05:55 +0000 UTC,stephbman,In progress,,"**Description**
Reduce friction by enabling developers to backup their namespaces via self-service. Introduce a Velero multi-tenancy model, enabling owners of namespaces to backup and restore within their access scope.

- Product requirements to be linked
- user stories to be created
- current timeframe target December 2020 (ref. velero roadmap): https://github.com/vmware-tanzu/velero/pull/2548/files?short_path=38574c0#diff-38574c080d4e2eb38c49b86e6588ad98",,,skriss,"
--
FYI https://github.com/vmware-tanzu/velero/issues/18 and https://github.com/vmware-tanzu/velero/issues/1217 are the existing issues I'm aware of that relate to this. We may want to close 1+ of them if they're dupes.
--
",stephbman,"
--
@skriss I was thinkig of linking them for now until I have the PRD drafted (I have them linked to the PRD currently). 
--
",,,,,,,,
2557,OPEN,Re-enable skipping binary build on PRs if only site changed,Bug; CI/CD; Good first issue; Help wanted; P3 - Wouldn't it be nice if...,2020-08-11 19:21:19 +0000 UTC,skriss,Opened,,"When we were using Travis for CI on PRs, we had some code (https://github.com/vmware-tanzu/velero/blob/master/.travis.yml#L12 and https://github.com/vmware-tanzu/velero/blob/master/hack/ci-check.sh) that would skip building/testing the Velero code if only files inside `site/` had changed. This made site/docs-only PRs pass checks much faster.

When we migrated to GitHub Actions for PR builds, we lost this. It'd be nice to add it back. It looks like the `ci-check.sh` script will need some minor changes since it's got logic specifically tied to Travis.

This is a nice-to-have for 1.5.",,,,,,,,,,,,,,
2545,OPEN,Lazily resolve restore item actions' included/excluded resources,Area/Plugins; Enhancement/User; Help wanted,2020-05-18 17:14:20 +0000 UTC,skriss,Opened,,"Splitting this request out from #2374 

**Describe the problem/challenge you have**
Restore item actions can specify a set of included/excluded resources that they apply to, via the `AppliesTo` method. Currently, Velero attempts to resolve these resource identifiers via the Kubernetes discovery API, **near the beginning** of a restore.

The problem is that when a restore item action includes or excludes a resource type that does not exist in the cluster at the beginning of a restore (i.e. because it's an instance of a CRD, and the CRD itself is being created as part of the restore), it can't be correctly resolved, so it's ignored as an included/excluded resource for that plugin, leading to the plugin potentially being run for unwanted resources.

**Describe the solution you'd like**
We could avoid this by deferring the included/excluded resource resolution to later in the restore process. 

Currently, it happens here: https://github.com/vmware-tanzu/velero/blob/master/pkg/restore/restore.go#L164

However, we could probably change the restore process to do this resolution somewhere [inside this loop](https://github.com/vmware-tanzu/velero/blob/master/pkg/restore/restore.go#L379), each time we start restoring a new resource type. That way, any resources that were just created via CRD restore would be available to be resolved. It would be a little less efficient, since we'd have to do the resolution multiple times, but since we cache discovery API results, the impact should be pretty minimal.

**Anything else you would like to add:**
Originally reported along with another issue that's since been resolved in #2374.
",,,,,,,,,,,,,,
2528,OPEN,Unable to properly restore Kubernetes service catalog objects,Enhancement/User; PreserveStatus,2020-10-28 21:38:28 +0000 UTC,koundinyabs,In progress,,"**What steps did you take and what happened:**
Backed up a K8s cluster with the latest Kubernetes service catalog (0.3.0-beta.2 which happens to use CRDs) installed on it that was also used to provision an external Postgres service isntance. A restore was then attempted. However, the restored service instance from the service catalog backup is not automatically relinking to the existing external service instance (Postgres database in this case), but it instead attempts to provision the same external database instance.  

##Testing Process

**Creating Service Catalog Resources:**
* Registered Postgres service broker.
* Created a Postgres service instance.
* Created a service binding for this service instance which also implicitly creating k8s secret.

**Back up command**

```
$ velero backup create service-catalog  --include-resources ""clusterservicebrokers.servicecatalog.k8s.io,clusterserviceclasses.servicecatalog.k8s.io,clusterserviceplans.servicecatalog.k8s.io,serviceinstances.servicecatalog.k8s.io,servicebindings.servicecatalog.k8s.io,secrets"" –wait
```

**Deleting Service Catalog Resources:**
To simulate the data lost in the cluster and leave the external service instance (Postgres database) orphaned, we need to update the service broker resource to use the wrong port, so the service catalog controller could not connect to the Postgres service broker when forcibly deleting the service catalog resources.
1. Deleted the service binding for Postgres instance which also implicitly deleting the secret.
1. Deleted the Postgres service instance.
1. Deregistered the Postgres service broker.

Checked the Postgres server and made sure that we still have the database created as part of the service instance provisioning.

**Restoring Service Catalog Resources from the Backup:**

`$ velero restore create --from-backup service-catalog --wait
`

**Problem with Service Instance after Restore:**
When checking on the service catalog resources after the restore process is finished, the service instance has failed status,
 
```
$ kubectl get serviceinstances
NAME             CLASS                            PLAN            STATUS                AGE
pg-instance-01   ClusterServiceClass/dev-pg-sid   dev-pg-devpid   ProvisionCallFailed   11h
```
**NOTE**: Status is `ProvisionCallFailed`

And with the following error found in the event logs of the service instance,
 
```
Events:
  Type     Reason               Age                From                                Message
  ----     ------               ----               ----                                -------
  Warning  RetryBackoff         47s (x2 over 47s)  service-catalog-controller-manager  Delaying provision retry, next attempt will be after 2020-04-09 09:27:35.267307237 +0000 UTC m=+9.164867324
  Warning  RetryBackoff         46s (x2 over 46s)  service-catalog-controller-manager  Delaying provision retry, next attempt will be after 2020-04-09 09:27:37.662463581 +0000 UTC m=+11.560023671
  Warning  RetryBackoff         44s (x2 over 44s)  service-catalog-controller-manager  Delaying provision retry, next attempt will be after 2020-04-09 09:27:41.867657212 +0000 UTC m=+15.765217309
  Warning  RetryBackoff         39s (x2 over 39s)  service-catalog-controller-manager  Delaying provision retry, next attempt will be after 2020-04-09 09:27:50.020582426 +0000 UTC m=+23.918142515
  Warning  RetryBackoff         31s (x2 over 31s)  service-catalog-controller-manager  Delaying provision retry, next attempt will be after 2020-04-09 09:28:06.174067452 +0000 UTC m=+40.071627550
  Warning  ProvisionCallFailed  15s (x6 over 48s)  service-catalog-controller-manager  Error provisioning ServiceInstance of ClusterServiceClass (K8S: ""dev-pg-sid"" ExternalName: ""dev-pg"") at ClusterServiceBroker ""dev-postgres-broker"": Status: 409; ErrorMessage: <nil>; Description: <nil>; ResponseError: <nil>
  Warning  RetryBackoff         15s (x2 over 15s)  service-catalog-controller-manager  Delaying provision retry, next attempt will be after 2020-04-09 09:28:38.352269746 +0000 UTC m=+72.249829834
```
 
It looked like when the service instance is restored back to the cluster, the service catalog controller detects the instance as a new resource to be added, and thus tried to call the Postgres service broker to provision the same service instance (database) which already existed.  
 
Service Catalog Controller Log:
`I0409 09:27:32.450718       1 controller_instance.go:147] ServiceInstance ""backing-services/pg-instance-01"" v8370686: Received ADD event: {""kind"":""ServiceInstance"",""apiVersion"":""servicecatalog.k8s.io/v1beta1"",""metadata"":{""name"":""pg-instance-01"",""namespace"":""backing-services"",""selfLink"":""/apis/servicecatalog.k8s.io/v1beta1/namespaces/backing-services/serviceinstances/pg-instance-01"",""uid"":""869f424d-7a43-11ea-b06a-1ed9bdd25d7a"",""resourceVersion"":""8370686"",""generation"":1,""creationTimestamp"":""2020-04-09T09:21:42Z"",""labels"":{""servicecatalog.k8s.io/spec.clusterServiceClassRef.name"":""5b35f3e6aba00e259a808e4aa9aef33c4f6e1914049179faadb87b96"",""servicecatalog.k8s.io/spec.clusterServicePlanRef.name"":""d11059f11f8149fe477a6d4bace1b2c69ad423c54cdf47c8a137f2d5"",""velero.io/backup-name"":""service-catalog"",""velero.io/restore-name"":""service-catalog-20200409022129""},""finalizers"":[""kubernetes-incubator/service-catalog""]},""spec"":{""clusterServiceClassName"":""dev-pg-sid"",""clusterServicePlanName"":""dev-pg-devpid"",""parameters"":{},""externalID"":""20198954-1445-4600-ad3e-d083d3faadd6"",""userInfo"":{""username"":""system:serviceaccount:velero:velero"",""uid"":""83b3507e-72de-11ea-8e09-7e6ebe78e9a7"",""groups"":[""system:serviceaccounts"",""system:serviceaccounts:velero"",""system:authenticated""]},""updateRequests"":0},""status"":{""conditions"":null,""asyncOpInProgress"":false,""orphanMitigationInProgress"":false,""reconciledGeneration"":0,""observedGeneration"":0,""provisionStatus"":"""",""deprovisionStatus"":"""",""lastConditionState"":"""",""userSpecifiedPlanName"":"""",""userSpecifiedClassName"":""""}}
` 
Postgres Service Broker Log:
```
2020-04-09 10:01:43.292  INFO 1418 --- [http-nio-8080-exec-8] c.e.c.b.p.d.ServiceInstanceRepository    : Trying to get service instance, instance id is 20198954-1445-4600-ad3e-d083d3faadd6
2020-04-09 10:01:43.293 DEBUG 1418 --- [http-nio-8080-exec-8] o.s.jdbc.core.JdbcTemplate               : Executing prepared SQL query
2020-04-09 10:01:43.293 DEBUG 1418 --- [http-nio-8080-exec-8] o.s.jdbc.core.JdbcTemplate               : Executing prepared SQL statement [SELECT * FROM t_service_instance WHERE serviceInstanceId = ?]
2020-04-09 10:01:43.304 DEBUG 1418 --- [http-nio-8080-exec-8] c.e.c.b.p.d.ServiceInstanceRepository    : Get service instance information from database:
serviceDefinitionId=dev-pg-sid
serviceInstanceId=20198954-1445-4600-ad3e-d083d3faadd6
planId=dev-pg-devpid
organizationGuid=32bce1b5-8f8d-4ed5-a559-ecdb4cc4465a
spaceGuid=9f224685-749f-11ea-8e09-7e6ebe78e9a7
2020-04-09 10:01:43.305  INFO 1418 --- [http-nio-8080-exec-8] c.e.c.b.pg.service.PGInstanceService     : The service instance already exists.

```

Also scaling down the service catalog controller to 0 before the restore and scaling it back up again after the restore did not help either.

**Conclusion:** The restored service instance from the service catalog backup is not automatically relinked to the existing external service instance (Postgres database in this case), but attempted to provision the same external database instance instead.  

**What did you expect to happen:**

The restored service instance from the service catalog backup automatically relinks to the existing external service instance (Postgres database in this case), rather than attempt to provision the same external database instance.  

**Anything else you would like to add:**

**Note**: Check out the [following note about service catalog migration](https://github.com/kubernetes-sigs/service-catalog/blob/master/docs/migration-apiserver-to-crds.md) . Based on the steps described, it looks like there is a way to backup and then restore the service catalog resources with properly linked to externally existing service instances. Maybe after the restore, additional changes are warranted, and thus the service catalog resources such as service instances need to be updated in such a way telling the controller to link to the external database instance instead of creating the same instance.

**Environment:**

To be added shortly.

- Velero version (use `velero version`): 
- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):
",,,ashish,"
--
Summarizing the issue here:
Service catalog allows in-cluster applications to use managed services outside of Kubernetes.
The backup and restore of CRDs of the service catalog API objects should preserve links to the out-of-cluster entities on a velero backup and restore the links on a velero restore.

@koundinyabs: Please pitch in with any details that I may have missed in my summary.

The service catalog controller seems to be storing state in the CRD's status that gets stripped on restore. We might need a restore action plugin for service catalog API objects to handle this scenario.

--
",koundinyabs,"
--
@ashish-amarnath That is very well summarized. 

Do you need more details on the velero and K8s version?
--
",,,,,,,,
2522,OPEN,document & automate plugin repo release processes,CI/CD; Enhancement/Dev; P2 - Long-term important,2021-02-22 20:03:34 +0000 UTC,skriss,Opened,,"We need to both (a) document and (b) automate the release process for the plugin repos that we maintain. Things we need to consider as part of the release:

- updating the compatibility matrix on README.md
- pushing a tag
- building tagged Docker images
- generating a changelog
- creating and publishing a GitHub release

I'm putting this issue in this repo for now because we should be able to copy and paste whatever we come up with into all the repos. We can split this out into each repo later as needed.",,,,,,,,,,,,,,
2520,OPEN,"restic: unable to restore ""Local"" Persistent Volumes since they're statically provisioned",Bug; Restic; Restic - GA,2020-11-21 17:38:34 +0000 UTC,eddeh83,In progress,,"**What steps did you take and what happened:**


I backed up a namespace with pods and PVC's which included Persistent Volumes with a ""local"" spec (which the documentation suggests is supported - https://github.com/vmware-tanzu/velero/blob/master/site/docs/master/restic.md #), and associated pods annotated with the volume names.

The backup appears to work - and as far as I can tell, Restic backs up the Persistent Volume.

However when attempting to restore the namespace the PV with ""local"" storage never seems to get restored. Sometimes Restic hangs for an hour before timing out waiting for the restore to complete ie:-

```
level=info msg=""Waiting for all restic restores to complete"" logSource=""pkg/restore/restore.go:545"" restore=velero/nginx-example3-20200507164146
level=error msg=""unable to successfully complete restic restores of pod's volumes"" error=""timed out waiting for all PodVolumeRestores to complete""
```

Other times the restore completes without error - but no PV is brought down (though the namespace, pods and PVC's are restored - though in a forever ""pending"" state due to the missing PV)

In the ""successful"" restores - despite the PV being deleted from the cluster (along with namespace, pod deployments etc.) The logs seem to imply that restic/velero think the PV has already been restored and is thus skipped

```
time=""2020-05-07T16:32:50Z"" level=info msg=""Skipping persistentvolumes/nginx-logs-volume because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/nginx-example2-20200507163249
time=""2020-05-07T16:32:50Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/nginx-example2-20200507163249
time=""2020-05-07T16:32:50Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/nginx-example2-20200507163249
time=""2020-05-07T16:32:50Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/nginx-example2-20200507163249
time=""2020-05-07T16:32:50Z"" level=info msg=""Resetting PersistentVolumeClaim nginx-example/nginx-logs for dynamic provisioning because its PV nginx-logs-volume has a reclaim policy of Delete"" logSource=""pkg/restore/restore.go:1103"" restore=velero/nginx-example2-20200507163249
```

I have also tried to just restore the PV's on its own (with no preexisting pods or pod deployments) and this hasn't worked, both with a reclaim policy of Delete and Retain. 

The reason I'm trying to use ""local"" storage is that this is a environment that is on prem and not in the cloud - the existing PV's use ""hostpath"" which restic doesnt support - so attempting to prove backups and restores using ""local"" - with a view of eventually changing the existing PV's to ""local"" from ""hostpath"" if this works.

For context - I also backed up a namespace which had a PV under it using ""rook-ceph-block"" storageclass - deleted all resources - and restored - and it worked fine, bringing the PV back along with associated data - so I can only surmise that restic isnt liking my PV's with ""local"" persistent volumes, or I am doing something wrong.

**What did you expect to happen:**

The Persistent volume(s) to restore along with data - and pods/PVC's to come up healthily.

Below is a quick demo of what I did.

```

velero install \
> --provider aws \
> --use-restic \
> --bucket cva-builder-velero-backups-test \
> --secret-file ./credentials-velero \
> --backup-location-config region=us-east-1 \
> --plugins velero/velero-plugin-for-aws:v1.0.1 \
> --snapshot-location-config region=us-east-1 \
> --use-volume-snapshots=true

```

```
kubectl get pods -n velero
NAME                     READY   STATUS    RESTARTS   AGE
restic-469kh             1/1     Running   0          18s
restic-bclvc             1/1     Running   0          18s
velero-84b675ddc-p6ft7   1/1     Running   0          18s
```

```
kubectl apply -f nginx_example.yml
namespace/nginx-example created
persistentvolume/nginx-logs-volume created
persistentvolumeclaim/nginx-logs created
deployment.apps/nginx-deployment created
service/my-nginx created
```

```
kubectl get pv | grep nginx
nginx-logs-volume                          10Gi       RWO            Delete           Bound    nginx-example/nginx-logs             local-storage                 4m40s
[centos@cva-builder:4.11.4-PR-1712-29 SANDBOX2 CVA-PR-1712 tmp]$ kubectl get pvc -n nginx-example
NAME         STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS    AGE
nginx-logs   Bound    nginx-logs-volume   10Gi       RWO            local-storage   4m53s
[centos@cva-builder:4.11.4-PR-1712-29 SANDBOX2 CVA-PR-1712 tmp]$ kubectl get pod -n nginx-example
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-6fbd5f64c4-fps5c   1/1     Running   0          108s
```

```
kubectl get pv nginx-logs-volume -o yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {""apiVersion"":""v1"",""kind"":""PersistentVolume"",""metadata"":{""annotations"":{},""labels"":{""app"":""nginx"",""type"":""local""},""name"":""nginx-logs-volume""},""spec"":{""accessModes"":[""ReadWriteOnce""],""capacity"":{""storage"":""10Gi""},""local"":{""path"":""/mnt/data""},""nodeAffinity"":{""required"":{""nodeSelectorTerms"":[{""matchExpressions"":[{""key"":""workerNodeName"",""operator"":""In"",""values"":[""node-1""]}]}]}},""persistentVolumeReclaimPolicy"":""Delete"",""storageClassName"":""local-storage"",""volumeMode"":""Filesystem""}}
    pv.kubernetes.io/bound-by-controller: ""yes""
  creationTimestamp: ""2020-05-08T08:56:36Z""
  finalizers:
  - kubernetes.io/pv-protection
  labels:
    app: nginx
    type: local
  name: nginx-logs-volume
  resourceVersion: ""298587""
  selfLink: /api/v1/persistentvolumes/nginx-logs-volume
  uid: d2b6957c-9109-11ea-a626-02021a19689b
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 10Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: nginx-logs
    namespace: nginx-example
    resourceVersion: ""298572""
    uid: d2bc87d4-9109-11ea-a626-02021a19689b
  local:
    path: /mnt/data
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: workerNodeName
          operator: In
          values:
          - node-1
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  volumeMode: Filesystem
status:
  phase: Bound
```
```
kubectl get pod nginx-deployment-6fbd5f64c4-fps5c -n nginx-example -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: ""2020-05-08T08:59:50Z""
  generateName: nginx-deployment-6fbd5f64c4-
  labels:
    app: nginx
    pod-template-hash: 6fbd5f64c4
  name: nginx-deployment-6fbd5f64c4-fps5c
  namespace: nginx-example
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: nginx-deployment-6fbd5f64c4
    uid: d2c226da-9109-11ea-ac8a-028358a66085
  resourceVersion: ""299852""
  selfLink: /api/v1/namespaces/nginx-example/pods/nginx-deployment-6fbd5f64c4-fps5c
  uid: 4640dfa5-910a-11ea-ac8a-028358a66085
spec:
  containers:
  - image: nginx:1.7.9
    imagePullPolicy: IfNotPresent
    name: nginx
    ports:
    - containerPort: 80
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/log/nginx
      name: nginx-logs
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-zmrr5
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: ip-10-165-138-230.ec2.internal
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: nginx-logs
    persistentVolumeClaim:
      claimName: nginx-logs
  - name: default-token-zmrr5
    secret:
      defaultMode: 420
      secretName: default-token-zmrr5
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: ""2020-05-08T08:59:50Z""
    status: ""True""
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: ""2020-05-08T09:00:54Z""
    status: ""True""
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: ""2020-05-08T09:00:54Z""
    status: ""True""
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: ""2020-05-08T08:59:50Z""
    status: ""True""
    type: PodScheduled
  containerStatuses:
  - containerID: docker://735b18c8379ee6b452e95c6c2ab45d1c82a9086598300de7fd90374336bac13f
    image: nginx:1.7.9
    imageID: docker-pullable://nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451
    lastState: {}
    name: nginx
    ready: true
    restartCount: 0
    state:
      running:
        startedAt: ""2020-05-08T09:00:54Z""
  hostIP: 10.165.138.230
  phase: Running
  podIP: 130.174.242.199
  qosClass: BestEffort
  startTime: ""2020-05-08T08:59:50Z""
```
```
kubectl get pvc -n nginx-example -o yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {""apiVersion"":""v1"",""kind"":""PersistentVolumeClaim"",""metadata"":{""annotations"":{},""labels"":{""app"":""nginx""},""name"":""nginx-logs"",""namespace"":""nginx-example""},""spec"":{""accessModes"":[""ReadWriteOnce""],""resources"":{""requests"":{""storage"":""50Mi""}},""storageClassName"":""local-storage""}}
      pv.kubernetes.io/bind-completed: ""yes""
      pv.kubernetes.io/bound-by-controller: ""yes""
    creationTimestamp: ""2020-05-08T08:56:36Z""
    finalizers:
    - kubernetes.io/pvc-protection
    labels:
      app: nginx
    name: nginx-logs
    namespace: nginx-example
    resourceVersion: ""298592""
    selfLink: /api/v1/namespaces/nginx-example/persistentvolumeclaims/nginx-logs
    uid: d2bc87d4-9109-11ea-a626-02021a19689b
  spec:
    accessModes:
    - ReadWriteOnce
    dataSource: null
    resources:
      requests:
        storage: 50Mi
    storageClassName: local-storage
    volumeMode: Filesystem
    volumeName: nginx-logs-volume
  status:
    accessModes:
    - ReadWriteOnce
    capacity:
      storage: 10Gi
    phase: Bound
kind: List
metadata:
  resourceVersion: """"
  selfLink: """"
```

```
kubectl -n nginx-example annotate pod/nginx-deployment-6fbd5f64c4-fps5c backup.velero.io/backup-volumes=nginx-logs
pod/nginx-deployment-6fbd5f64c4-fps5c annotated
```

```
velero create backup nginx-example-backup-new --include-namespaces nginx-example --snapshot-volumes
Backup request ""nginx-example-backup-new"" submitted successfully.
Run `velero backup describe nginx-example-backup-new` or `velero backup logs nginx-example-backup-new` for more details.
```
```
velero backup describe nginx-example-backup-new --details
Name:         nginx-example-backup-new
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  <none>

Phase:  Completed

Namespaces:
  Included:  nginx-example
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  default

Snapshot PVs:  true

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  1

Started:    2020-05-08 09:33:25 +0000 UTC
Completed:  2020-05-08 09:33:44 +0000 UTC

Expiration:  2020-06-07 09:33:25 +0000 UTC

Resource List:
  apps/v1/Deployment:
    - nginx-example/nginx-deployment
  apps/v1/ReplicaSet:
    - nginx-example/nginx-deployment-6fbd5f64c4
  v1/Endpoints:
    - nginx-example/my-nginx
  v1/Event:
    - nginx-example/nginx-deployment-6fbd5f64c4-2r7zk.160d01c6f35fac21
    - nginx-example/nginx-deployment-6fbd5f64c4-2r7zk.160d01c6ffb7dd12
    - nginx-example/nginx-deployment-6fbd5f64c4-2r7zk.160d01e397c7f57f
    - nginx-example/nginx-deployment-6fbd5f64c4-2r7zk.160d020352295182
    - nginx-example/nginx-deployment-6fbd5f64c4-fps5c.160d01f3d4be1543
    - nginx-example/nginx-deployment-6fbd5f64c4-fps5c.160d02014a99421f
    - nginx-example/nginx-deployment-6fbd5f64c4-fps5c.160d020281b1aab3
    - nginx-example/nginx-deployment-6fbd5f64c4-fps5c.160d0202a4ed5f11
    - nginx-example/nginx-deployment-6fbd5f64c4-fps5c.160d0202ae7423c0
    - nginx-example/nginx-deployment-6fbd5f64c4.160d01c6b69e8011
    - nginx-example/nginx-deployment-6fbd5f64c4.160d01f3d43d2119
    - nginx-example/nginx-deployment.160d01c6b5b35eb3
    - nginx-example/nginx-logs.160d01c6b399f74e
  v1/Namespace:
    - nginx-example
  v1/PersistentVolume:
    - nginx-logs-volume
  v1/PersistentVolumeClaim:
    - nginx-example/nginx-logs
  v1/Pod:
    - nginx-example/nginx-deployment-6fbd5f64c4-fps5c
  v1/Secret:
    - nginx-example/default-token-zmrr5
  v1/Service:
    - nginx-example/my-nginx
  v1/ServiceAccount:
    - nginx-example/default

Persistent Volumes: <none included>

Restic Backups:
  Completed:
    nginx-example/nginx-deployment-6fbd5f64c4-fps5c: nginx-logs
```

```
velero backup logs nginx-example-backup-new
time=""2020-05-08T09:33:25Z"" level=info msg=""Setting up backup temp file"" backup=velero/nginx-example-backup-new logSource=""pkg/controller/backup_controller.go:494""
time=""2020-05-08T09:33:25Z"" level=info msg=""Setting up plugin manager"" backup=velero/nginx-example-backup-new logSource=""pkg/controller/backup_controller.go:501""
time=""2020-05-08T09:33:25Z"" level=info msg=""Getting backup item actions"" backup=velero/nginx-example-backup-new logSource=""pkg/controller/backup_controller.go:505""
time=""2020-05-08T09:33:25Z"" level=info msg=""Setting up backup store"" backup=velero/nginx-example-backup-new logSource=""pkg/controller/backup_controller.go:511""
time=""2020-05-08T09:33:25Z"" level=info msg=""Writing backup version file"" backup=velero/nginx-example-backup-new logSource=""pkg/backup/backup.go:213""
time=""2020-05-08T09:33:25Z"" level=info msg=""Including namespaces: nginx-example"" backup=velero/nginx-example-backup-new logSource=""pkg/backup/backup.go:219""
time=""2020-05-08T09:33:25Z"" level=info msg=""Excluding namespaces: <none>"" backup=velero/nginx-example-backup-new logSource=""pkg/backup/backup.go:220""
time=""2020-05-08T09:33:25Z"" level=info msg=""Including resources: *"" backup=velero/nginx-example-backup-new logSource=""pkg/backup/backup.go:223""
time=""2020-05-08T09:33:25Z"" level=info msg=""Excluding resources: <none>"" backup=velero/nginx-example-backup-new logSource=""pkg/backup/backup.go:224""
time=""2020-05-08T09:33:40Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:40Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=pods
time=""2020-05-08T09:33:40Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=pods
time=""2020-05-08T09:33:40Z"" level=info msg=""Retrieved 1 items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=pods
time=""2020-05-08T09:33:40Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment-6fbd5f64c4-fps5c namespace=nginx-example resource=pods
time=""2020-05-08T09:33:40Z"" level=info msg=""Executing custom action"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:330"" name=nginx-deployment-6fbd5f64c4-fps5c namespace=nginx-example resource=pods
time=""2020-05-08T09:33:40Z"" level=info msg=""Executing podAction"" backup=velero/nginx-example-backup-new cmd=/velero logSource=""pkg/backup/pod_action.go:51"" pluginName=velero
time=""2020-05-08T09:33:40Z"" level=info msg=""Adding pvc nginx-logs to additionalItems"" backup=velero/nginx-example-backup-new cmd=/velero logSource=""pkg/backup/pod_action.go:67"" pluginName=velero
time=""2020-05-08T09:33:40Z"" level=info msg=""Done executing podAction"" backup=velero/nginx-example-backup-new cmd=/velero logSource=""pkg/backup/pod_action.go:77"" pluginName=velero
time=""2020-05-08T09:33:40Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-logs namespace=nginx-example resource=persistentvolumeclaims
time=""2020-05-08T09:33:40Z"" level=info msg=""Executing custom action"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:330"" name=nginx-logs namespace=nginx-example resource=persistentvolumeclaims
time=""2020-05-08T09:33:40Z"" level=info msg=""Executing PVCAction"" backup=velero/nginx-example-backup-new cmd=/velero logSource=""pkg/backup/backup_pv_action.go:49"" pluginName=velero
time=""2020-05-08T09:33:40Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-logs-volume namespace= resource=persistentvolumes
time=""2020-05-08T09:33:40Z"" level=info msg=""Executing takePVSnapshot"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:400"" name=nginx-logs-volume namespace= resource=persistentvolumes
time=""2020-05-08T09:33:40Z"" level=info msg=""Skipping snapshot of persistent volume because volume is being backed up with restic."" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:418"" name=nginx-logs-volume namespace= persistentVolume=nginx-logs-volume resource=persistentvolumes
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=persistentvolumeclaims
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=persistentvolumeclaims
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 1 items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=persistentvolumeclaims
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping item because it's already been backed up."" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:163"" name=nginx-logs namespace=nginx-example resource=persistentvolumeclaims
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=persistentvolumes
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:127"" resource=persistentvolumes
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 13 items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment-6fbd5f64c4-2r7zk.160d01c6f35fac21 namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment-6fbd5f64c4-2r7zk.160d01c6ffb7dd12 namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment-6fbd5f64c4-2r7zk.160d01e397c7f57f namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment-6fbd5f64c4-2r7zk.160d020352295182 namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment-6fbd5f64c4-fps5c.160d01f3d4be1543 namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment-6fbd5f64c4-fps5c.160d02014a99421f namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment-6fbd5f64c4-fps5c.160d020281b1aab3 namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment-6fbd5f64c4-fps5c.160d0202a4ed5f11 namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment-6fbd5f64c4-fps5c.160d0202ae7423c0 namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment-6fbd5f64c4.160d01c6b69e8011 namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment-6fbd5f64c4.160d01f3d43d2119 namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment.160d01c6b5b35eb3 namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-logs.160d01c6b399f74e namespace=nginx-example resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=limitranges
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=limitranges
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=limitranges
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=services
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=services
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 1 items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=services
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=my-nginx namespace=nginx-example resource=services
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=configmaps
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=configmaps
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=configmaps
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=resourcequotas
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=resourcequotas
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=resourcequotas
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=serviceaccounts
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=serviceaccounts
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 1 items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=serviceaccounts
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=default namespace=nginx-example resource=serviceaccounts
time=""2020-05-08T09:33:42Z"" level=info msg=""Executing custom action"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:330"" name=default namespace=nginx-example resource=serviceaccounts
time=""2020-05-08T09:33:42Z"" level=info msg=""Running ServiceAccountAction"" backup=velero/nginx-example-backup-new cmd=/velero logSource=""pkg/backup/service_account_action.go:77"" pluginName=velero
time=""2020-05-08T09:33:42Z"" level=info msg=""Done running ServiceAccountAction"" backup=velero/nginx-example-backup-new cmd=/velero logSource=""pkg/backup/service_account_action.go:120"" pluginName=velero
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=nodes
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:127"" resource=nodes
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=secrets
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=secrets
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 1 items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=secrets
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=default-token-zmrr5 namespace=nginx-example resource=secrets
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=namespaces
time=""2020-05-08T09:33:42Z"" level=info msg=""Getting namespace"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:184"" namespace=nginx-example resource=namespaces
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-example namespace= resource=namespaces
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=endpoints
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=endpoints
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 1 items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=endpoints
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/item_backupper.go:169"" name=my-nginx namespace=nginx-example resource=endpoints
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=podtemplates
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=podtemplates
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=podtemplates
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=replicationcontrollers
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=replicationcontrollers
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=replicationcontrollers
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=apiregistration.k8s.io/v1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=apiregistration.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=apiservices
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=apiregistration.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:127"" resource=apiservices
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=controllerrevisions
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=controllerrevisions
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=controllerrevisions
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=daemonsets
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=daemonsets
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=daemonsets
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=statefulsets
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=statefulsets
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=statefulsets
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=deployments
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=deployments
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 1 items"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=deployments
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment namespace=nginx-example resource=deployments.apps
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=replicasets
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=replicasets
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 1 items"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=replicasets
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up item"" backup=velero/nginx-example-backup-new group=apps/v1 logSource=""pkg/backup/item_backupper.go:169"" name=nginx-deployment-6fbd5f64c4 namespace=nginx-example resource=replicasets.apps
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=events.k8s.io/v1beta1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=events.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it cohabitates and we've already processed it"" backup=velero/nginx-example-backup-new cohabitatingResource1=events cohabitatingResource2=events.events.k8s.io group=events.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:148"" resource=events
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=autoscaling/v1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=autoscaling/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=horizontalpodautoscalers
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=autoscaling/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=horizontalpodautoscalers
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=autoscaling/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=horizontalpodautoscalers
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=batch/v1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=batch/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=jobs
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=batch/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=jobs
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=batch/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=jobs
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=batch/v1beta1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=batch/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=cronjobs
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=batch/v1beta1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=cronjobs
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=batch/v1beta1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=cronjobs
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=certificates.k8s.io/v1beta1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=certificates.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=certificatesigningrequests
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=certificates.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:127"" resource=certificatesigningrequests
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=networking.k8s.io/v1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=networking.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=networkpolicies
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=networking.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=networkpolicies
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=networking.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=networkpolicies
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=policy/v1beta1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=policy/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=podsecuritypolicies
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=policy/v1beta1 logSource=""pkg/backup/resource_backupper.go:127"" resource=podsecuritypolicies
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=policy/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=poddisruptionbudgets
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=policy/v1beta1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=poddisruptionbudgets
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=policy/v1beta1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=poddisruptionbudgets
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=roles
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=roles
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=roles
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=clusterrolebindings
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:127"" resource=clusterrolebindings
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=clusterroles
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:127"" resource=clusterroles
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=rolebindings
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=rolebindings
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=rbac.authorization.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=rolebindings
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=storage.k8s.io/v1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=storage.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=volumeattachments
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=storage.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:127"" resource=volumeattachments
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=storage.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=storageclasses
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=storage.k8s.io/v1 logSource=""pkg/backup/resource_backupper.go:127"" resource=storageclasses
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=mutatingwebhookconfigurations
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:127"" resource=mutatingwebhookconfigurations
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=validatingwebhookconfigurations
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=admissionregistration.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:127"" resource=validatingwebhookconfigurations
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=admissionregistration.k8s.io/v1alpha1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=admissionregistration.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:105"" resource=initializerconfigurations
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=admissionregistration.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:127"" resource=initializerconfigurations
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=apiextensions.k8s.io/v1beta1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=apiextensions.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=customresourcedefinitions
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=apiextensions.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:127"" resource=customresourcedefinitions
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=scheduling.k8s.io/v1beta1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=scheduling.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=priorityclasses
time=""2020-05-08T09:33:42Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=scheduling.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:127"" resource=priorityclasses
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=coordination.k8s.io/v1beta1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=coordination.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=leases
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=coordination.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=leases
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=coordination.k8s.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=leases
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=cephnfses
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=cephnfses
time=""2020-05-08T09:33:42Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=cephnfses
time=""2020-05-08T09:33:42Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=cephblockpools
time=""2020-05-08T09:33:42Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=cephblockpools
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=cephblockpools
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=cephfilesystems
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=cephfilesystems
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=cephfilesystems
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=cephclusters
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=cephclusters
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=cephclusters
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=cephobjectstoreusers
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=cephobjectstoreusers
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=cephobjectstoreusers
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=cephobjectstores
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=cephobjectstores
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=ceph.rook.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=cephobjectstores
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=prometheusrules
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=prometheusrules
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=prometheusrules
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=servicemonitors
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=servicemonitors
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=servicemonitors
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=prometheuses
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=prometheuses
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=prometheuses
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=alertmanagers
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=alertmanagers
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=alertmanagers
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=podmonitors
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=podmonitors
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=monitoring.coreos.com/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=podmonitors
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=restores
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=restores
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=restores
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=schedules
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=schedules
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=schedules
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=podvolumebackups
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=podvolumebackups
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=podvolumebackups
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=podvolumerestores
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=podvolumerestores
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=podvolumerestores
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=resticrepositories
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=resticrepositories
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=resticrepositories
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=backups
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=backups
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=backups
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=downloadrequests
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=downloadrequests
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=downloadrequests
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=deletebackuprequests
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=deletebackuprequests
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=deletebackuprequests
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=serverstatusrequests
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=serverstatusrequests
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=serverstatusrequests
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=backupstoragelocations
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=backupstoragelocations
time=""2020-05-08T09:33:43Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=backupstoragelocations
time=""2020-05-08T09:33:43Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:105"" resource=volumesnapshotlocations
time=""2020-05-08T09:33:43Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=volumesnapshotlocations
time=""2020-05-08T09:33:44Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=velero.io/v1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=volumesnapshotlocations
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=certmanager.k8s.io/v1alpha1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=certmanager.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:105"" resource=certificates
time=""2020-05-08T09:33:44Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=certmanager.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=certificates
time=""2020-05-08T09:33:44Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=certmanager.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=certificates
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=certmanager.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:105"" resource=issuers
time=""2020-05-08T09:33:44Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=certmanager.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=issuers
time=""2020-05-08T09:33:44Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=certmanager.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=issuers
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=certmanager.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:105"" resource=clusterissuers
time=""2020-05-08T09:33:44Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=certmanager.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:127"" resource=clusterissuers
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=nfs.rook.io/v1alpha1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=nfs.rook.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:105"" resource=nfsservers
time=""2020-05-08T09:33:44Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=nfs.rook.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=nfsservers
time=""2020-05-08T09:33:44Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=nfs.rook.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=nfsservers
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=objectbucket.io/v1alpha1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=objectbucket.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:105"" resource=objectbuckets
time=""2020-05-08T09:33:44Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=objectbucket.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:127"" resource=objectbuckets
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=objectbucket.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:105"" resource=objectbucketclaims
time=""2020-05-08T09:33:44Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=objectbucket.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=objectbucketclaims
time=""2020-05-08T09:33:44Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=objectbucket.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=objectbucketclaims
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=snapshot.storage.k8s.io/v1alpha1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=snapshot.storage.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:105"" resource=volumesnapshotcontents
time=""2020-05-08T09:33:44Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=snapshot.storage.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:127"" resource=volumesnapshotcontents
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=snapshot.storage.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:105"" resource=volumesnapshots
time=""2020-05-08T09:33:44Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=snapshot.storage.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=volumesnapshots
time=""2020-05-08T09:33:44Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=snapshot.storage.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=volumesnapshots
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=snapshot.storage.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:105"" resource=volumesnapshotclasses
time=""2020-05-08T09:33:44Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=snapshot.storage.k8s.io/v1alpha1 logSource=""pkg/backup/resource_backupper.go:127"" resource=volumesnapshotclasses
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=rook.io/v1alpha2 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=rook.io/v1alpha2 logSource=""pkg/backup/resource_backupper.go:105"" resource=volumes
time=""2020-05-08T09:33:44Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=rook.io/v1alpha2 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=volumes
time=""2020-05-08T09:33:44Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=rook.io/v1alpha2 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=volumes
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=kubeless.io/v1beta1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=kubeless.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=cronjobtriggers
time=""2020-05-08T09:33:44Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=kubeless.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=cronjobtriggers
time=""2020-05-08T09:33:44Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=kubeless.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=cronjobtriggers
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=kubeless.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=httptriggers
time=""2020-05-08T09:33:44Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=kubeless.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=httptriggers
time=""2020-05-08T09:33:44Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=kubeless.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=httptriggers
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=kubeless.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=functions
time=""2020-05-08T09:33:44Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=kubeless.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=functions
time=""2020-05-08T09:33:44Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=kubeless.io/v1beta1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=functions
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up group"" backup=velero/nginx-example-backup-new group=extensions/v1beta1 logSource=""pkg/backup/group_backupper.go:101""
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=extensions/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=podsecuritypolicies
time=""2020-05-08T09:33:44Z"" level=info msg=""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"" backup=velero/nginx-example-backup-new group=extensions/v1beta1 logSource=""pkg/backup/resource_backupper.go:127"" resource=podsecuritypolicies
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=extensions/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=networkpolicies
time=""2020-05-08T09:33:44Z"" level=info msg=""Skipping resource because it cohabitates and we've already processed it"" backup=velero/nginx-example-backup-new cohabitatingResource1=networkpolicies.extensions cohabitatingResource2=networkpolicies.networking.k8s.io group=extensions/v1beta1 logSource=""pkg/backup/resource_backupper.go:148"" resource=networkpolicies
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=extensions/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=replicasets
time=""2020-05-08T09:33:44Z"" level=info msg=""Skipping resource because it cohabitates and we've already processed it"" backup=velero/nginx-example-backup-new cohabitatingResource1=replicasets.extensions cohabitatingResource2=replicasets.apps group=extensions/v1beta1 logSource=""pkg/backup/resource_backupper.go:148"" resource=replicasets
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=extensions/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=deployments
time=""2020-05-08T09:33:44Z"" level=info msg=""Skipping resource because it cohabitates and we've already processed it"" backup=velero/nginx-example-backup-new cohabitatingResource1=deployments.extensions cohabitatingResource2=deployments.apps group=extensions/v1beta1 logSource=""pkg/backup/resource_backupper.go:148"" resource=deployments
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=extensions/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=ingresses
time=""2020-05-08T09:33:44Z"" level=info msg=""Listing items"" backup=velero/nginx-example-backup-new group=extensions/v1beta1 logSource=""pkg/backup/resource_backupper.go:226"" namespace=nginx-example resource=ingresses
time=""2020-05-08T09:33:44Z"" level=info msg=""Retrieved 0 items"" backup=velero/nginx-example-backup-new group=extensions/v1beta1 logSource=""pkg/backup/resource_backupper.go:240"" namespace=nginx-example resource=ingresses
time=""2020-05-08T09:33:44Z"" level=info msg=""Backing up resource"" backup=velero/nginx-example-backup-new group=extensions/v1beta1 logSource=""pkg/backup/resource_backupper.go:105"" resource=daemonsets
time=""2020-05-08T09:33:44Z"" level=info msg=""Skipping resource because it cohabitates and we've already processed it"" backup=velero/nginx-example-backup-new cohabitatingResource1=daemonsets.extensions cohabitatingResource2=daemonsets.apps group=extensions/v1beta1 logSource=""pkg/backup/resource_backupper.go:148"" resource=daemonsets
```

```
kubectl -n velero get podvolumebackups -l velero.io/backup-name=nginx-example-backup-new -o yaml
apiVersion: v1
items:
- apiVersion: velero.io/v1
  kind: PodVolumeBackup
  metadata:
    annotations:
      velero.io/pvc-name: nginx-logs
    creationTimestamp: ""2020-05-08T09:33:40Z""
    generateName: nginx-example-backup-new-
    generation: 3
    labels:
      velero.io/backup-name: nginx-example-backup-new
      velero.io/backup-uid: f7226a8c-910e-11ea-ac8a-028358a66085
      velero.io/pvc-uid: d2bc87d4-9109-11ea-a626-02021a19689b
    name: nginx-example-backup-new-wc88w
    namespace: velero
    ownerReferences:
    - apiVersion: velero.io/v1
      controller: true
      kind: Backup
      name: nginx-example-backup-new
      uid: f7226a8c-910e-11ea-ac8a-028358a66085
    resourceVersion: ""309164""
    selfLink: /apis/velero.io/v1/namespaces/velero/podvolumebackups/nginx-example-backup-new-wc88w
    uid: 002c9523-910f-11ea-ac8a-028358a66085
  spec:
    backupStorageLocation: default
    node: ip-10-165-138-230.ec2.internal
    pod:
      kind: Pod
      name: nginx-deployment-6fbd5f64c4-fps5c
      namespace: nginx-example
      uid: 4640dfa5-910a-11ea-ac8a-028358a66085
    repoIdentifier: s3:s3.amazonaws.com/cva-builder-velero-backups-test/restic/nginx-example
    tags:
      backup: nginx-example-backup-new
      backup-uid: f7226a8c-910e-11ea-ac8a-028358a66085
      ns: nginx-example
      pod: nginx-deployment-6fbd5f64c4-fps5c
      pod-uid: 4640dfa5-910a-11ea-ac8a-028358a66085
      pvc-uid: d2bc87d4-9109-11ea-a626-02021a19689b
      volume: nginx-logs
    volume: nginx-logs
  status:
    completionTimestamp: ""2020-05-08T09:33:42Z""
    path: /host_pods/4640dfa5-910a-11ea-ac8a-028358a66085/volumes/kubernetes.io~local-volume/nginx-logs-volume
    phase: Completed
    progress: {}
    snapshotID: ffabaa99
    startTimestamp: ""2020-05-08T09:33:40Z""
kind: List
metadata:
  resourceVersion: """"
  selfLink: """"
```


```
kubectl delete deployment -n nginx-example --all
deployment.extensions ""nginx-deployment"" deleted

kubectl delete pv nginx-logs-volume
persistentvolume ""nginx-logs-volume"" deleted

kubectl delete namespace nginx-example
namespace ""nginx-example"" deleted
```

```
velero restore create --from-backup nginx-example-backup-new
Restore request ""nginx-example-backup-new-20200508094438"" submitted successfully.
Run `velero restore describe nginx-example-backup-new-20200508094438` or `velero restore logs nginx-example-backup-new-20200508094438` for more details.
```

```
velero restore describe nginx-example-backup-new-20200508094438 --details
Name:         nginx-example-backup-new-20200508094438
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:  InProgress

Backup:  nginx-example-backup-new

Namespaces:
  Included:  *
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io
  Cluster-scoped:  auto

Namespace mappings:  <none>

Label selector:  <none>

Restore PVs:  auto

Restic Restores:
  New:
    nginx-example/nginx-deployment-6fbd5f64c4-fps5c: nginx-logs
```

In this attempt - the restore hung for an hour before failing. It hung on the ""waiting for all restic restores to complete""

```
time=""2020-05-08T09:44:38Z"" level=info msg=""starting restore"" logSource=""pkg/controller/restore_controller.go:458"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Not including resource"" groupResource=nodes logSource=""pkg/restore/restore.go:138"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Not including resource"" groupResource=events logSource=""pkg/restore/restore.go:138"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Not including resource"" groupResource=events.events.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Not including resource"" groupResource=backups.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Not including resource"" groupResource=restores.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Not including resource"" groupResource=resticrepositories.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Starting restore of backup velero/nginx-example-backup-new"" logSource=""pkg/restore/restore.go:394"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Restoring cluster level resource 'persistentvolumes'"" logSource=""pkg/restore/restore.go:779"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Getting client for /v1, Kind=PersistentVolume"" logSource=""pkg/restore/restore.go:821"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Restoring resource 'persistentvolumeclaims' into namespace 'nginx-example'"" logSource=""pkg/restore/restore.go:777"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Getting client for /v1, Kind=PersistentVolumeClaim"" logSource=""pkg/restore/restore.go:821"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Executing AddPVFromPVCAction"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:44"" pluginName=velero restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Adding PV nginx-logs-volume as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:66"" pluginName=velero restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Skipping persistentvolumes/nginx-logs-volume because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Resetting PersistentVolumeClaim nginx-example/nginx-logs for dynamic provisioning because its PV nginx-logs-volume has a reclaim policy of Delete"" logSource=""pkg/restore/restore.go:1103"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Attempting to restore PersistentVolumeClaim: nginx-logs"" logSource=""pkg/restore/restore.go:1136"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Restoring resource 'secrets' into namespace 'nginx-example'"" logSource=""pkg/restore/restore.go:777"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Getting client for /v1, Kind=Secret"" logSource=""pkg/restore/restore.go:821"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Attempting to restore Secret: default-token-zmrr5"" logSource=""pkg/restore/restore.go:1136"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Restoring resource 'serviceaccounts' into namespace 'nginx-example'"" logSource=""pkg/restore/restore.go:777"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Getting client for /v1, Kind=ServiceAccount"" logSource=""pkg/restore/restore.go:821"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Executing item action for serviceaccounts"" logSource=""pkg/restore/restore.go:1030"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Executing ServiceAccountAction"" cmd=/velero logSource=""pkg/restore/service_account_action.go:47"" pluginName=velero restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Done executing ServiceAccountAction"" cmd=/velero logSource=""pkg/restore/service_account_action.go:78"" pluginName=velero restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Attempting to restore ServiceAccount: default"" logSource=""pkg/restore/restore.go:1136"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Restoring resource 'pods' into namespace 'nginx-example'"" logSource=""pkg/restore/restore.go:777"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Getting client for /v1, Kind=Pod"" logSource=""pkg/restore/restore.go:821"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Executing item action for pods"" logSource=""pkg/restore/restore.go:1030"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Executing AddPVCFromPodAction"" cmd=/velero logSource=""pkg/restore/add_pvc_from_pod_action.go:44"" pluginName=velero restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Adding PVC nginx-example/nginx-logs as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pvc_from_pod_action.go:58"" pluginName=velero restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Skipping persistentvolumeclaims/nginx-example/nginx-logs because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Executing item action for pods"" logSource=""pkg/restore/restore.go:1030"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Executing item action for pods"" logSource=""pkg/restore/restore.go:1030"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Executing ResticRestoreAction"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:68"" pluginName=velero restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Restic backups for pod found"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:94"" pluginName=velero pod=nginx-example/nginx-deployment-6fbd5f64c4-fps5c restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Using image \""velero/velero-restic-restore-helper:v1.3.2\"""" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:105"" pluginName=velero pod=nginx-example/nginx-deployment-6fbd5f64c4-fps5c restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Done executing ResticRestoreAction"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:154"" pluginName=velero restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Attempting to restore Pod: nginx-deployment-6fbd5f64c4-fps5c"" logSource=""pkg/restore/restore.go:1136"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Restoring resource 'replicasets.apps' into namespace 'nginx-example'"" logSource=""pkg/restore/restore.go:777"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Getting client for apps/v1, Kind=ReplicaSet"" logSource=""pkg/restore/restore.go:821"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Attempting to restore ReplicaSet: nginx-deployment-6fbd5f64c4"" logSource=""pkg/restore/restore.go:1136"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Restoring resource 'deployments.apps' into namespace 'nginx-example'"" logSource=""pkg/restore/restore.go:777"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Getting client for apps/v1, Kind=Deployment"" logSource=""pkg/restore/restore.go:821"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Attempting to restore Deployment: nginx-deployment"" logSource=""pkg/restore/restore.go:1136"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Restoring resource 'endpoints' into namespace 'nginx-example'"" logSource=""pkg/restore/restore.go:777"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Getting client for /v1, Kind=Endpoints"" logSource=""pkg/restore/restore.go:821"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Attempting to restore Endpoints: my-nginx"" logSource=""pkg/restore/restore.go:1136"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Restoring resource 'services' into namespace 'nginx-example'"" logSource=""pkg/restore/restore.go:777"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Getting client for /v1, Kind=Service"" logSource=""pkg/restore/restore.go:821"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Executing item action for services"" logSource=""pkg/restore/restore.go:1030"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:38Z"" level=info msg=""Attempting to restore Service: my-nginx"" logSource=""pkg/restore/restore.go:1136"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:41Z"" level=info msg=""Not including resource"" groupResource=events logSource=""pkg/restore/restore.go:138"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:41Z"" level=info msg=""Not including resource"" groupResource=nodes logSource=""pkg/restore/restore.go:138"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:41Z"" level=info msg=""Not including resource"" groupResource=events.events.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:41Z"" level=info msg=""Not including resource"" groupResource=resticrepositories.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:41Z"" level=info msg=""Not including resource"" groupResource=restores.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:41Z"" level=info msg=""Not including resource"" groupResource=backups.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/nginx-example-backup-new-20200508094438
time=""2020-05-08T09:44:41Z"" level=info msg=""Waiting for all restic restores to complete"" logSource=""pkg/restore/restore.go:545"" restore=velero/nginx-example-backup-new-20200508094438
```

Some resources get restored or attempt to be - but stay in pending state due to the absence of the PV.

```
kubectl get pods -n nginx-example
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-6fbd5f64c4-fps5c   0/1     Pending   0          5m1s

kubectl get pvc -n nginx-example
NAME         STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    AGE
nginx-logs   Pending                                      local-storage   5m11s

kubectl get pv | grep -i nginx 
```

As mentioned previously other storage seems to backup and restore ok. 

Can anyone shed any light as to what might be happening?


Something Ive noticed - in the getPodVolumeBackups command we have this

```path: /host_pods/4640dfa5-910a-11ea-ac8a-028358a66085/volumes/kubernetes.io~local-volume/nginx-logs-volume```

but when I exec into the restic pods I cannot see that path or any nginx-example references.

Looking at the restic repo though there is reference to nginx-example volume...

```
[centos@cva-builder:4.11.4-PR-1712-29 SANDBOX2 CVA-PR-1712 tmp]$ velero restic repo get -o yaml
apiVersion: velero.io/v1
kind: ResticRepository
metadata:
  creationTimestamp: 2020-05-08T09:26:58Z
  generateName: nginx-example-default-
  generation: 3
  labels:
    velero.io/storage-location: default
    velero.io/volume-namespace: nginx-example
  name: nginx-example-default-htvtg
  namespace: velero
  resourceVersion: ""308973""
  selfLink: /apis/velero.io/v1/namespaces/velero/resticrepositories/nginx-example-default-htvtg
  uid: 10bf1552-910e-11ea-ac8a-028358a66085
spec:
  backupStorageLocation: default
  maintenanceFrequency: 168h0m0s
  resticIdentifier: s3:s3.amazonaws.com/cva-builder-velero-backups-test/restic/nginx-example
  volumeNamespace: nginx-example
status:
  lastMaintenanceTime: 2020-05-08T09:33:00Z
  phase: Ready


**Environment:**

- Velero version (use `velero version`):   
velero version
Client:
        Version: v1.3.2
        Git commit: 55a9914a3e4719fb1578529c45430a8c11c28145
Server:
        Version: v1.3.2

- Velero features (use `velero client config get features`): 

velero client config get feature
feature: <NOT SET>

- Kubernetes version (use `kubectl version`):

kubectl version
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.8"", GitCommit:""0c6d31a99f81476dfc9871ba3cf3f597bec29b58"", GitTreeState:""clean"", BuildDate:""2019-07-08T08:46:01Z"", GoVersion:""go1.11.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.8"", GitCommit:""0c6d31a99f81476dfc9871ba3cf3f597bec29b58"", GitTreeState:""clean"", BuildDate:""2019-07-08T08:38:54Z"", GoVersion:""go1.11.5"", Compiler:""gc"", Platform:""linux/amd64""}


- Kubernetes installer & version:
- Cloud provider or hardware configuration: **Test performed on AWS - but need ""local"" storage to emulate on prem none cloud environments**
- OS (e.g. from `/etc/os-release`):

cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7

",,,ashish,"
--
@eddeh83 thank you for the detailed issue!
We are going to have to investigate this further.
Would you be able to provide info about the size of the restic backup that was being restored?
This is going to help us attempt a repro that resembles your workload.
--

--
Also, if you are able to get a consistent repro of this, can you please run the velero server with debug logs enabled which would give us more information. You will find instructions on how to enable this on our documentation site https://velero.io/docs/v1.4-pre/troubleshooting/#getting-velero-debug-logs
--

--
@asoltesz Steve doesn't work on the project. We will investigate this and prioritize it accordingly.
--
",eddeh83,"
--
Hi Ashish

The backup being restored wasn't large - a few hundred MB at most.

Since raising the issue I have managed to complete a restore by using the following method at a high level.

- Take backup of namespace as normal: - 
   velero create backup <backup_name> --include-namespaces <namespace> --snapshot-volumes
 
- Create a yaml file with the PV's that I'm going to restore into - eg backup the existing PV's under 
   that namespace

- kubectl get pv <name> -o yaml > namespace_pv.yaml

- Format it to be deployable via kubectl.

- Delete the namespace to be restored. This will delete everything except the PV's which are now 
  ""available""

- Delete these PV
 
- Recreate these PV with the yaml created earlier so that we have new PV's for the data to restore to
 
- Now run the restore - it should work fine.

This was with a namespace with multiple PV's and PVC's. oddly when I've just tried on one with only one PV and PVC it worked fine!

Velero log below for the multiple pv namespace

```
time=""2020-05-12T18:40:47Z"" level=info msg=""starting restore"" logSource=""pkg/controller/restore_controller.go:458"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Not including resource"" groupResource=events logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Not including resource"" groupResource=nodes logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Not including resource"" groupResource=events.events.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Not including resource"" groupResource=restores.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Not including resource"" groupResource=resticrepositories.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Not including resource"" groupResource=backups.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Starting restore of backup velero/raffia-demo3"" logSource=""pkg/restore/restore.go:394"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Restoring cluster level resource 'persistentvolumes'"" logSource=""pkg/restore/restore.go:779"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Getting client for /v1, Kind=PersistentVolume"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Restoring resource 'persistentvolumeclaims' into namespace 'raffia'"" logSource=""pkg/restore/restore.go:777"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Getting client for /v1, Kind=PersistentVolumeClaim"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing AddPVFromPVCAction"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:44"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PV raffia-mongodb-data as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumes/raffia-mongodb-data because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Getting plugin config"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No storage class mappings found"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:73"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Resetting PersistentVolumeClaim raffia/raffia-mongodb for dynamic provisioning because its PV raffia-mongodb-data has a reclaim policy of Delete"" logSource=""pkg/restore/restore.go:1103"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore PersistentVolumeClaim: raffia-mongodb"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing AddPVFromPVCAction"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:44"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PV raffia-config as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumes/raffia-config because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Getting plugin config"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No storage class mappings found"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:73"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Resetting PersistentVolumeClaim raffia/raffia-raffia-config for dynamic provisioning because its PV raffia-config has a reclaim policy of Delete"" logSource=""pkg/restore/restore.go:1103"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore PersistentVolumeClaim: raffia-raffia-config"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing AddPVFromPVCAction"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:44"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PV raffia-domains as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumes/raffia-domains because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Getting plugin config"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No storage class mappings found"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:73"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Resetting PersistentVolumeClaim raffia/raffia-raffia-domains for dynamic provisioning because its PV raffia-domains has a reclaim policy of Delete"" logSource=""pkg/restore/restore.go:1103"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore PersistentVolumeClaim: raffia-raffia-domains"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing AddPVFromPVCAction"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:44"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PV raffia-engine-config as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumes/raffia-engine-config because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Getting plugin config"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No storage class mappings found"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:73"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Resetting PersistentVolumeClaim raffia/raffia-raffia-engine-config for dynamic provisioning because its PV raffia-engine-config has a reclaim policy of Delete"" logSource=""pkg/restore/restore.go:1103"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore PersistentVolumeClaim: raffia-raffia-engine-config"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing AddPVFromPVCAction"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:44"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PV raffia-engine-home as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumes/raffia-engine-home because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Getting plugin config"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No storage class mappings found"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:73"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Resetting PersistentVolumeClaim raffia/raffia-raffia-engine-home for dynamic provisioning because its PV raffia-engine-home has a reclaim policy of Delete"" logSource=""pkg/restore/restore.go:1103"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore PersistentVolumeClaim: raffia-raffia-engine-home"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing AddPVFromPVCAction"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:44"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PV raffia-engine-triage as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumes/raffia-engine-triage because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Getting plugin config"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No storage class mappings found"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:73"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Resetting PersistentVolumeClaim raffia/raffia-raffia-engine-triage for dynamic provisioning because its PV raffia-engine-triage has a reclaim policy of Delete"" logSource=""pkg/restore/restore.go:1103"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore PersistentVolumeClaim: raffia-raffia-engine-triage"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing AddPVFromPVCAction"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:44"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PV raffia-engine-var as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumes/raffia-engine-var because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Getting plugin config"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No storage class mappings found"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:73"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Resetting PersistentVolumeClaim raffia/raffia-raffia-engine-var for dynamic provisioning because its PV raffia-engine-var has a reclaim policy of Delete"" logSource=""pkg/restore/restore.go:1103"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore PersistentVolumeClaim: raffia-raffia-engine-var"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing AddPVFromPVCAction"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:44"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PV raffia-home as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumes/raffia-home because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Getting plugin config"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No storage class mappings found"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:73"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Resetting PersistentVolumeClaim raffia/raffia-raffia-home for dynamic provisioning because its PV raffia-home has a reclaim policy of Delete"" logSource=""pkg/restore/restore.go:1103"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore PersistentVolumeClaim: raffia-raffia-home"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing AddPVFromPVCAction"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:44"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PV raffia-raffia-var as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pv_from_pvc_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumes/raffia-raffia-var because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for persistentvolumeclaims"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Getting plugin config"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No storage class mappings found"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:73"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Resetting PersistentVolumeClaim raffia/raffia-raffia-var for dynamic provisioning because its PV raffia-raffia-var has a reclaim policy of Delete"" logSource=""pkg/restore/restore.go:1103"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore PersistentVolumeClaim: raffia-raffia-var"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Restoring resource 'secrets' into namespace 'raffia'"" logSource=""pkg/restore/restore.go:777"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Getting client for /v1, Kind=Secret"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Secret: cva-id"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Secret: cva-info"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Secret: default-token-rkbd4"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Secret: dxcdockerslmcred"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Secret: raffia-cert-vault-approle"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Secret: raffia-deploy-vault-approle"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Secret: raffia-ephemeral-path"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Secret: raffia-mongodb"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Secret: raffia-sa-token-j69gm"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Secret: raffia-vault-approle"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Secret: vault-client-tls"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Restoring resource 'configmaps' into namespace 'raffia'"" logSource=""pkg/restore/restore.go:777"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Getting client for /v1, Kind=ConfigMap"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore ConfigMap: raffia-raffia-engine-scripts"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore ConfigMap: raffia-raffia-scripts"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Restoring resource 'serviceaccounts' into namespace 'raffia'"" logSource=""pkg/restore/restore.go:777"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Getting client for /v1, Kind=ServiceAccount"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for serviceaccounts"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing ServiceAccountAction"" cmd=/velero logSource=""pkg/restore/service_account_action.go:47"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Checking secrets"" cmd=/velero logSource=""pkg/restore/service_account_action.go:57"" pluginName=velero restore=velero/raffia-demo3-20200512184046 serviceaccount=raffia/default
time=""2020-05-12T18:40:47Z"" level=debug msg=""Checking if secret default-token-rkbd4 matches default-token-"" cmd=/velero logSource=""pkg/restore/service_account_action.go:61"" pluginName=velero restore=velero/raffia-demo3-20200512184046 serviceaccount=raffia/default
time=""2020-05-12T18:40:47Z"" level=debug msg=""Match found - excluding this secret"" cmd=/velero logSource=""pkg/restore/service_account_action.go:65"" pluginName=velero restore=velero/raffia-demo3-20200512184046 serviceaccount=raffia/default
time=""2020-05-12T18:40:47Z"" level=info msg=""Done executing ServiceAccountAction"" cmd=/velero logSource=""pkg/restore/service_account_action.go:78"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore ServiceAccount: default"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for serviceaccounts"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing ServiceAccountAction"" cmd=/velero logSource=""pkg/restore/service_account_action.go:47"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Checking secrets"" cmd=/velero logSource=""pkg/restore/service_account_action.go:57"" pluginName=velero restore=velero/raffia-demo3-20200512184046 serviceaccount=raffia/raffia-sa
time=""2020-05-12T18:40:47Z"" level=debug msg=""Checking if secret raffia-sa-token-j69gm matches raffia-sa-token-"" cmd=/velero logSource=""pkg/restore/service_account_action.go:61"" pluginName=velero restore=velero/raffia-demo3-20200512184046 serviceaccount=raffia/raffia-sa
time=""2020-05-12T18:40:47Z"" level=debug msg=""Match found - excluding this secret"" cmd=/velero logSource=""pkg/restore/service_account_action.go:65"" pluginName=velero restore=velero/raffia-demo3-20200512184046 serviceaccount=raffia/raffia-sa
time=""2020-05-12T18:40:47Z"" level=info msg=""Done executing ServiceAccountAction"" cmd=/velero logSource=""pkg/restore/service_account_action.go:78"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore ServiceAccount: raffia-sa"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Restoring resource 'pods' into namespace 'raffia'"" logSource=""pkg/restore/restore.go:777"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Getting client for /v1, Kind=Pod"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for pods"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing AddPVCFromPodAction"" cmd=/velero logSource=""pkg/restore/add_pvc_from_pod_action.go:44"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PVC raffia/raffia-mongodb as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pvc_from_pod_action.go:58"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumeclaims/raffia/raffia-mongodb because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for pods"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for pods"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing ResticRestoreAction"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:68"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Restic backups for pod found"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:94"" pluginName=velero pod=raffia/raffia-mongodb-54ccbdcf87-c9b6m restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Getting plugin config"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:98"" pluginName=velero pod=raffia/raffia-mongodb-54ccbdcf87-c9b6m restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No config found for plugin"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:159"" pluginName=velero pod=raffia/raffia-mongodb-54ccbdcf87-c9b6m restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Using image \""velero/velero-restic-restore-helper:v1.3.1\"""" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:105"" pluginName=velero pod=raffia/raffia-mongodb-54ccbdcf87-c9b6m restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No config found for plugin"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:194"" pluginName=velero pod=raffia/raffia-mongodb-54ccbdcf87-c9b6m restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No config found for plugin"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:205"" pluginName=velero pod=raffia/raffia-mongodb-54ccbdcf87-c9b6m restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Done executing ResticRestoreAction"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:154"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Pod: raffia-mongodb-54ccbdcf87-c9b6m"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for pods"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing AddPVCFromPodAction"" cmd=/velero logSource=""pkg/restore/add_pvc_from_pod_action.go:44"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Acquiring lock"" backupLocation=default logSource=""pkg/restic/repository_ensurer.go:122"" volumeNamespace=raffia
time=""2020-05-12T18:40:47Z"" level=debug msg=""Acquired lock"" backupLocation=default logSource=""pkg/restic/repository_ensurer.go:131"" volumeNamespace=raffia
time=""2020-05-12T18:40:47Z"" level=debug msg=""Ready repository found"" backupLocation=default logSource=""pkg/restic/repository_ensurer.go:147"" volumeNamespace=raffia
time=""2020-05-12T18:40:47Z"" level=debug msg=""Released lock"" backupLocation=default logSource=""pkg/restic/repository_ensurer.go:128"" volumeNamespace=raffia
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PVC raffia/raffia-raffia-config as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pvc_from_pod_action.go:58"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PVC raffia/raffia-raffia-domains as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pvc_from_pod_action.go:58"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PVC raffia/raffia-raffia-var as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pvc_from_pod_action.go:58"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PVC raffia/raffia-raffia-home as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pvc_from_pod_action.go:58"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumeclaims/raffia/raffia-raffia-config because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumeclaims/raffia/raffia-raffia-domains because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumeclaims/raffia/raffia-raffia-var because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumeclaims/raffia/raffia-raffia-home because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for pods"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for pods"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing ResticRestoreAction"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:68"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Restic backups for pod found"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:94"" pluginName=velero pod=raffia/raffia-raffia-5667cc7768-5cn64 restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Getting plugin config"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:98"" pluginName=velero pod=raffia/raffia-raffia-5667cc7768-5cn64 restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No config found for plugin"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:159"" pluginName=velero pod=raffia/raffia-raffia-5667cc7768-5cn64 restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Using image \""velero/velero-restic-restore-helper:v1.3.1\"""" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:105"" pluginName=velero pod=raffia/raffia-raffia-5667cc7768-5cn64 restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No config found for plugin"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:194"" pluginName=velero pod=raffia/raffia-raffia-5667cc7768-5cn64 restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No config found for plugin"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:205"" pluginName=velero pod=raffia/raffia-raffia-5667cc7768-5cn64 restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Done executing ResticRestoreAction"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:154"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Pod: raffia-raffia-5667cc7768-5cn64"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for pods"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Acquiring lock"" backupLocation=default logSource=""pkg/restic/repository_ensurer.go:122"" volumeNamespace=raffia
time=""2020-05-12T18:40:47Z"" level=debug msg=""Acquired lock"" backupLocation=default logSource=""pkg/restic/repository_ensurer.go:131"" volumeNamespace=raffia
time=""2020-05-12T18:40:47Z"" level=debug msg=""Ready repository found"" backupLocation=default logSource=""pkg/restic/repository_ensurer.go:147"" volumeNamespace=raffia
time=""2020-05-12T18:40:47Z"" level=debug msg=""Released lock"" backupLocation=default logSource=""pkg/restic/repository_ensurer.go:128"" volumeNamespace=raffia
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing AddPVCFromPodAction"" cmd=/velero logSource=""pkg/restore/add_pvc_from_pod_action.go:44"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PVC raffia/raffia-raffia-engine-config as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pvc_from_pod_action.go:58"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PVC raffia/raffia-raffia-engine-var as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pvc_from_pod_action.go:58"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PVC raffia/raffia-raffia-engine-home as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pvc_from_pod_action.go:58"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Adding PVC raffia/raffia-raffia-engine-triage as an additional item to restore"" cmd=/velero logSource=""pkg/restore/add_pvc_from_pod_action.go:58"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumeclaims/raffia/raffia-raffia-engine-config because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumeclaims/raffia/raffia-raffia-engine-var because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumeclaims/raffia/raffia-raffia-engine-home because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Skipping persistentvolumeclaims/raffia/raffia-raffia-engine-triage because it's already been restored."" logSource=""pkg/restore/restore.go:910"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for pods"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing item action for pods"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Executing ResticRestoreAction"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:68"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Restic backups for pod found"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:94"" pluginName=velero pod=raffia/raffia-raffia-engine-7b665ddbb4-fq8zv restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Getting plugin config"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:98"" pluginName=velero pod=raffia/raffia-raffia-engine-7b665ddbb4-fq8zv restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No config found for plugin"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:159"" pluginName=velero pod=raffia/raffia-raffia-engine-7b665ddbb4-fq8zv restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Using image \""velero/velero-restic-restore-helper:v1.3.1\"""" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:105"" pluginName=velero pod=raffia/raffia-raffia-engine-7b665ddbb4-fq8zv restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No config found for plugin"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:194"" pluginName=velero pod=raffia/raffia-raffia-engine-7b665ddbb4-fq8zv restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""No config found for plugin"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:205"" pluginName=velero pod=raffia/raffia-raffia-engine-7b665ddbb4-fq8zv restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Done executing ResticRestoreAction"" cmd=/velero logSource=""pkg/restore/restic_restore_action.go:154"" pluginName=velero restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Pod: raffia-raffia-engine-7b665ddbb4-fq8zv"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Restoring resource 'replicasets.apps' into namespace 'raffia'"" logSource=""pkg/restore/restore.go:777"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Getting client for apps/v1, Kind=ReplicaSet"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore ReplicaSet: raffia-mongodb-54ccbdcf87"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=debug msg=""Acquiring lock"" backupLocation=default logSource=""pkg/restic/repository_ensurer.go:122"" volumeNamespace=raffia
time=""2020-05-12T18:40:47Z"" level=debug msg=""Acquired lock"" backupLocation=default logSource=""pkg/restic/repository_ensurer.go:131"" volumeNamespace=raffia
time=""2020-05-12T18:40:47Z"" level=debug msg=""Ready repository found"" backupLocation=default logSource=""pkg/restic/repository_ensurer.go:147"" volumeNamespace=raffia
time=""2020-05-12T18:40:47Z"" level=debug msg=""Released lock"" backupLocation=default logSource=""pkg/restic/repository_ensurer.go:128"" volumeNamespace=raffia
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore ReplicaSet: raffia-raffia-5667cc7768"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore ReplicaSet: raffia-raffia-engine-7b665ddbb4"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Restoring resource 'deployments.apps' into namespace 'raffia'"" logSource=""pkg/restore/restore.go:777"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Getting client for apps/v1, Kind=Deployment"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Deployment: raffia-mongodb"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Deployment: raffia-raffia-engine"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Deployment: raffia-raffia"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Restoring resource 'endpoints' into namespace 'raffia'"" logSource=""pkg/restore/restore.go:777"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Getting client for /v1, Kind=Endpoints"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Endpoints: raffia-mongodb"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:47Z"" level=info msg=""Attempting to restore Endpoints: raffia-raffia"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Restoring resource 'ingresses.extensions' into namespace 'raffia'"" logSource=""pkg/restore/restore.go:777"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Getting client for extensions/v1beta1, Kind=Ingress"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Attempting to restore Ingress: raffia-raffia"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Restoring resource 'rolebindings.rbac.authorization.k8s.io' into namespace 'raffia'"" logSource=""pkg/restore/restore.go:777"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Getting client for rbac.authorization.k8s.io/v1, Kind=RoleBinding"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Executing item action for rolebindings.rbac.authorization.k8s.io"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Attempting to restore RoleBinding: raffia-read-only-role-binding"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Restoring resource 'roles.rbac.authorization.k8s.io' into namespace 'raffia'"" logSource=""pkg/restore/restore.go:777"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Getting client for rbac.authorization.k8s.io/v1, Kind=Role"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Attempting to restore Role: raffia-role-read-only"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Restoring resource 'services' into namespace 'raffia'"" logSource=""pkg/restore/restore.go:777"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Getting client for /v1, Kind=Service"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Executing item action for services"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Attempting to restore Service: raffia-mongodb"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Executing item action for services"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:48Z"" level=info msg=""Attempting to restore Service: raffia-raffia"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:51Z"" level=info msg=""Not including resource"" groupResource=events logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:51Z"" level=info msg=""Not including resource"" groupResource=nodes logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:51Z"" level=info msg=""Not including resource"" groupResource=events.events.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:51Z"" level=info msg=""Not including resource"" groupResource=backups.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:51Z"" level=info msg=""Not including resource"" groupResource=resticrepositories.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:51Z"" level=info msg=""Not including resource"" groupResource=restores.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:51Z"" level=info msg=""Waiting for all restic restores to complete"" logSource=""pkg/restore/restore.go:545"" restore=velero/raffia-demo3-20200512184046
time=""2020-05-12T18:40:59Z"" level=debug msg=""Checking for existing backup storage locations to sync into cluster"" controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:124""
time=""2020-05-12T18:40:59Z"" level=debug msg=""Checking if backups need to be synced at this time for this location"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:156""
time=""2020-05-12T18:40:59Z"" level=debug msg=""Checking backup location for backups to sync into cluster"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:163""
time=""2020-05-12T18:40:59Z"" level=debug msg=""looking for plugin in registry"" controller=backup-sync kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/aws
time=""2020-05-12T18:40:59Z"" level=debug msg=""creating new restartable plugin process"" command=/plugins/velero-plugin-for-aws controller=backup-sync kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:114"" name=velero.io/aws
time=""2020-05-12T18:40:59Z"" level=debug msg=""starting plugin"" args=""[/plugins/velero-plugin-for-aws --log-level debug]"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws
time=""2020-05-12T18:40:59Z"" level=debug msg=""plugin started"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws pid=1863
time=""2020-05-12T18:40:59Z"" level=debug msg=""waiting for RPC address"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws
time=""2020-05-12T18:40:59Z"" level=debug msg=""using plugin"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" version=2
time=""2020-05-12T18:40:59Z"" level=debug msg=""plugin address"" address=/tmp/plugin788120112 cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" network=unix pluginName=velero-plugin-for-aws
time=""2020-05-12T18:40:59Z"" level=debug msg=""Got backups from backup store"" backupCount=16 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:178""
time=""2020-05-12T18:40:59Z"" level=debug msg=""Got backups from cluster"" backupCount=16 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:185""
time=""2020-05-12T18:40:59Z"" level=debug msg=""No backups found in the backup location that need to be synced into the cluster"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:198""
time=""2020-05-12T18:40:59Z"" level=debug msg=""plugin process exited"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws pid=1863
time=""2020-05-12T18:40:59Z"" level=debug msg=""plugin exited"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74""
time=""2020-05-12T18:41:29Z"" level=debug msg=""Checking for existing backup storage locations to sync into cluster"" controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:124""
time=""2020-05-12T18:41:29Z"" level=debug msg=""Checking if backups need to be synced at this time for this location"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:156""
time=""2020-05-12T18:41:59Z"" level=debug msg=""Checking for existing backup storage locations to sync into cluster"" controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:124""
time=""2020-05-12T18:41:59Z"" level=debug msg=""Checking if backups need to be synced at this time for this location"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:156""
time=""2020-05-12T18:41:59Z"" level=debug msg=""Checking backup location for backups to sync into cluster"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:163""
time=""2020-05-12T18:41:59Z"" level=debug msg=""looking for plugin in registry"" controller=backup-sync kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/aws
time=""2020-05-12T18:41:59Z"" level=debug msg=""creating new restartable plugin process"" command=/plugins/velero-plugin-for-aws controller=backup-sync kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:114"" name=velero.io/aws
time=""2020-05-12T18:41:59Z"" level=debug msg=""starting plugin"" args=""[/plugins/velero-plugin-for-aws --log-level debug]"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws
time=""2020-05-12T18:41:59Z"" level=debug msg=""plugin started"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws pid=1880
time=""2020-05-12T18:41:59Z"" level=debug msg=""waiting for RPC address"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws
time=""2020-05-12T18:41:59Z"" level=debug msg=""plugin address"" address=/tmp/plugin575175281 cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" network=unix pluginName=velero-plugin-for-aws
time=""2020-05-12T18:41:59Z"" level=debug msg=""using plugin"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" version=2
time=""2020-05-12T18:41:59Z"" level=debug msg=""Got backups from backup store"" backupCount=16 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:178""
time=""2020-05-12T18:41:59Z"" level=debug msg=""Got backups from cluster"" backupCount=16 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:185""
time=""2020-05-12T18:41:59Z"" level=debug msg=""No backups found in the backup location that need to be synced into the cluster"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:198""
time=""2020-05-12T18:41:59Z"" level=debug msg=""plugin process exited"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws pid=1880
time=""2020-05-12T18:41:59Z"" level=debug msg=""plugin exited"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74""
time=""2020-05-12T18:42:19Z"" level=debug msg=resticRepositoryController.enqueueAllRepositories controller=restic-repository logSource=""pkg/controller/restic_repository_controller.go:96""
time=""2020-05-12T18:42:19Z"" level=debug msg=""Running processQueueItem"" controller=restic-repository key=velero/vault-default-4nhjh logSource=""pkg/controller/restic_repository_controller.go:111""
time=""2020-05-12T18:42:19Z"" level=debug msg=""Checking repository for stale locks"" controller=restic-repository logSource=""pkg/controller/restic_repository_controller.go:140"" name=vault-default-4nhjh namespace=velero
time=""2020-05-12T18:42:29Z"" level=debug msg=""Checking for existing backup storage locations to sync into cluster"" controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:124""
time=""2020-05-12T18:42:29Z"" level=debug msg=""Checking if backups need to be synced at this time for this location"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:156""
time=""2020-05-12T18:42:46Z"" level=debug msg=""Running processDownloadRequest"" controller=downloadrequest key=velero/nginx-example-restic-test3-20200512184246 logSource=""pkg/controller/download_request_controller.go:114""
time=""2020-05-12T18:42:46Z"" level=debug msg=""looking for plugin in registry"" controller=downloadrequest key=velero/nginx-example-restic-test3-20200512184246 kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/aws
time=""2020-05-12T18:42:46Z"" level=debug msg=""creating new restartable plugin process"" command=/plugins/velero-plugin-for-aws controller=downloadrequest key=velero/nginx-example-restic-test3-20200512184246 kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:114"" name=velero.io/aws
time=""2020-05-12T18:42:46Z"" level=debug msg=""starting plugin"" args=""[/plugins/velero-plugin-for-aws --log-level debug]"" cmd=/plugins/velero-plugin-for-aws controller=downloadrequest key=velero/nginx-example-restic-test3-20200512184246 logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws
time=""2020-05-12T18:42:46Z"" level=debug msg=""plugin started"" cmd=/plugins/velero-plugin-for-aws controller=downloadrequest key=velero/nginx-example-restic-test3-20200512184246 logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws pid=1911
time=""2020-05-12T18:42:46Z"" level=debug msg=""waiting for RPC address"" cmd=/plugins/velero-plugin-for-aws controller=downloadrequest key=velero/nginx-example-restic-test3-20200512184246 logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws
time=""2020-05-12T18:42:46Z"" level=debug msg=""using plugin"" cmd=/plugins/velero-plugin-for-aws controller=downloadrequest key=velero/nginx-example-restic-test3-20200512184246 logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" version=2
time=""2020-05-12T18:42:46Z"" level=debug msg=""plugin address"" address=/tmp/plugin732159865 cmd=/plugins/velero-plugin-for-aws controller=downloadrequest key=velero/nginx-example-restic-test3-20200512184246 logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" network=unix pluginName=velero-plugin-for-aws
time=""2020-05-12T18:42:46Z"" level=debug msg=""plugin process exited"" cmd=/plugins/velero-plugin-for-aws controller=downloadrequest key=velero/nginx-example-restic-test3-20200512184246 logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws pid=1911
time=""2020-05-12T18:42:46Z"" level=debug msg=""plugin exited"" cmd=/plugins/velero-plugin-for-aws controller=downloadrequest key=velero/nginx-example-restic-test3-20200512184246 logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74""
time=""2020-05-12T18:42:50Z"" level=debug msg=""Ran restic command"" command=""restic unlock --repo=s3:s3.amazonaws.com/cva-builder-velero-backups-test/restic/vault --password-file=/tmp/velero-restic-credentials-vault499987547 --cache-dir=/scratch/.cache/restic"" logSource=""pkg/restic/repository_manager.go:275"" repository=vault stderr= stdout=""successfully removed locks\n""
time=""2020-05-12T18:42:50Z"" level=debug msg=resticRepositoryController.runMaintenanceIfDue controller=restic-repository logSource=""pkg/controller/restic_repository_controller.go:217"" name=vault-default-4nhjh namespace=velero
time=""2020-05-12T18:42:50Z"" level=debug msg=""not due for maintenance"" controller=restic-repository logSource=""pkg/controller/restic_repository_controller.go:222"" name=vault-default-4nhjh namespace=velero
time=""2020-05-12T18:42:50Z"" level=debug msg=""Running processQueueItem"" controller=restic-repository key=velero/nginx-example-default-44t8d logSource=""pkg/controller/restic_repository_controller.go:111""
time=""2020-05-12T18:42:50Z"" level=debug msg=""Checking repository for stale locks"" controller=restic-repository logSource=""pkg/controller/restic_repository_controller.go:140"" name=nginx-example-default-44t8d namespace=velero
time=""2020-05-12T18:42:59Z"" level=debug msg=""Checking for existing backup storage locations to sync into cluster"" controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:124""
time=""2020-05-12T18:42:59Z"" level=debug msg=""Checking if backups need to be synced at this time for this location"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:156""
time=""2020-05-12T18:42:59Z"" level=debug msg=""Checking backup location for backups to sync into cluster"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:163""
time=""2020-05-12T18:42:59Z"" level=debug msg=""looking for plugin in registry"" controller=backup-sync kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/aws
time=""2020-05-12T18:42:59Z"" level=debug msg=""creating new restartable plugin process"" command=/plugins/velero-plugin-for-aws controller=backup-sync kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:114"" name=velero.io/aws
time=""2020-05-12T18:42:59Z"" level=debug msg=""starting plugin"" args=""[/plugins/velero-plugin-for-aws --log-level debug]"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws
time=""2020-05-12T18:42:59Z"" level=debug msg=""plugin started"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws pid=1937
time=""2020-05-12T18:42:59Z"" level=debug msg=""waiting for RPC address"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws
time=""2020-05-12T18:42:59Z"" level=debug msg=""plugin address"" address=/tmp/plugin194370527 cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" network=unix pluginName=velero-plugin-for-aws
time=""2020-05-12T18:42:59Z"" level=debug msg=""using plugin"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" version=2
time=""2020-05-12T18:42:59Z"" level=debug msg=""Got backups from backup store"" backupCount=16 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:178""
time=""2020-05-12T18:42:59Z"" level=debug msg=""Got backups from cluster"" backupCount=16 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:185""
time=""2020-05-12T18:42:59Z"" level=debug msg=""No backups found in the backup location that need to be synced into the cluster"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:198""
time=""2020-05-12T18:42:59Z"" level=debug msg=""plugin process exited"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws pid=1937
time=""2020-05-12T18:42:59Z"" level=debug msg=""plugin exited"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74""
time=""2020-05-12T18:43:21Z"" level=debug msg=""Ran restic command"" command=""restic unlock --repo=s3:s3.amazonaws.com/cva-builder-velero-backups-test/restic/nginx-example --password-file=/tmp/velero-restic-credentials-nginx-example886195710 --cache-dir=/scratch/.cache/restic"" logSource=""pkg/restic/repository_manager.go:275"" repository=nginx-example stderr= stdout=""successfully removed locks\n""
time=""2020-05-12T18:43:21Z"" level=debug msg=resticRepositoryController.runMaintenanceIfDue controller=restic-repository logSource=""pkg/controller/restic_repository_controller.go:217"" name=nginx-example-default-44t8d namespace=velero
time=""2020-05-12T18:43:21Z"" level=debug msg=""not due for maintenance"" controller=restic-repository logSource=""pkg/controller/restic_repository_controller.go:222"" name=nginx-example-default-44t8d namespace=velero
time=""2020-05-12T18:43:21Z"" level=debug msg=""Running processQueueItem"" controller=restic-repository key=velero/raffia-default-47hcn logSource=""pkg/controller/restic_repository_controller.go:111""
time=""2020-05-12T18:43:21Z"" level=debug msg=""Checking repository for stale locks"" controller=restic-repository logSource=""pkg/controller/restic_repository_controller.go:140"" name=raffia-default-47hcn namespace=velero
time=""2020-05-12T18:43:22Z"" level=debug msg=""Ran restic command"" command=""restic unlock --repo=s3:s3.amazonaws.com/cva-builder-velero-backups-test/restic/raffia --password-file=/tmp/velero-restic-credentials-raffia503717189 --cache-dir=/scratch/.cache/restic"" logSource=""pkg/restic/repository_manager.go:275"" repository=raffia stderr= stdout=""successfully removed locks\n""
time=""2020-05-12T18:43:22Z"" level=debug msg=resticRepositoryController.runMaintenanceIfDue controller=restic-repository logSource=""pkg/controller/restic_repository_controller.go:217"" name=raffia-default-47hcn namespace=velero
time=""2020-05-12T18:43:22Z"" level=debug msg=""not due for maintenance"" controller=restic-repository logSource=""pkg/controller/restic_repository_controller.go:222"" name=raffia-default-47hcn namespace=velero
time=""2020-05-12T18:43:29Z"" level=debug msg=""Checking for existing backup storage locations to sync into cluster"" controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:124""
time=""2020-05-12T18:43:29Z"" level=debug msg=""Checking if backups need to be synced at this time for this location"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:156""
time=""2020-05-12T18:43:59Z"" level=debug msg=""Checking for existing backup storage locations to sync into cluster"" controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:124""
time=""2020-05-12T18:43:59Z"" level=debug msg=""Checking if backups need to be synced at this time for this location"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:156""
time=""2020-05-12T18:43:59Z"" level=debug msg=""Checking backup location for backups to sync into cluster"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:163""
time=""2020-05-12T18:43:59Z"" level=debug msg=""looking for plugin in registry"" controller=backup-sync kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/aws
time=""2020-05-12T18:43:59Z"" level=debug msg=""creating new restartable plugin process"" command=/plugins/velero-plugin-for-aws controller=backup-sync kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:114"" name=velero.io/aws
time=""2020-05-12T18:43:59Z"" level=debug msg=""starting plugin"" args=""[/plugins/velero-plugin-for-aws --log-level debug]"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws
time=""2020-05-12T18:43:59Z"" level=debug msg=""plugin started"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws pid=1971
time=""2020-05-12T18:43:59Z"" level=debug msg=""waiting for RPC address"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws
time=""2020-05-12T18:43:59Z"" level=debug msg=""plugin address"" address=/tmp/plugin926215913 cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" network=unix pluginName=velero-plugin-for-aws
time=""2020-05-12T18:43:59Z"" level=debug msg=""using plugin"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" version=2
time=""2020-05-12T18:43:59Z"" level=debug msg=""Got backups from backup store"" backupCount=16 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:178""
time=""2020-05-12T18:43:59Z"" level=debug msg=""Got backups from cluster"" backupCount=16 backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:185""
time=""2020-05-12T18:43:59Z"" level=debug msg=""No backups found in the backup location that need to be synced into the cluster"" backupLocation=default controller=backup-sync logSource=""pkg/controller/backup_sync_controller.go:198""
time=""2020-05-12T18:43:59Z"" level=debug msg=""plugin process exited"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/plugins/velero-plugin-for-aws pid=1971
time=""2020-05-12T18:43:59Z"" level=debug msg=""plugin exited"" cmd=/plugins/velero-plugin-for-aws controller=backup-sync logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74""
```

The restore hangs on in progress, pvcs get created but no pv's, so they stay in pending until timing out after an hour.

--

--
Just trying another approach - I deleted the namespace and all pvs (of which there are 9)

I then tried a restore of just the PV's (without creating them via yaml first)

```
velero restore create --from-backup raffia-demo3 --include-resources persistentvolumes
Restore request ""raffia-demo3-20200513100836"" submitted successfully.
Run `velero restore describe raffia-demo3-20200513100836` or `velero restore logs raffia-demo3-20200513100836` for more details.

velero restore logs raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/add-pv-from-pvc restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""creating new restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:114"" name=velero.io/add-pv-from-pvc restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""starting plugin"" args=""[/velero run-plugins --log-level debug]"" cmd=/velero logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/velero restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""plugin started"" cmd=/velero logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/velero pid=130 restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""waiting for RPC address"" cmd=/velero logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/velero restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""Setting log level to DEBUG"" cmd=/velero logSource=""pkg/plugin/framework/server.go:172"" pluginName=velero restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""plugin address"" address=/tmp/plugin512942643 cmd=/velero logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" network=unix pluginName=velero restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""using plugin"" cmd=/velero logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" restore=velero/raffia-demo3-20200513100836 version=2
time=""2020-05-13T10:08:36Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/add-pvc-from-pod restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/add-pvc-from-pod restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/change-storage-class restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/change-storage-class restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/cluster-role-bindings restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/cluster-role-bindings restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/crd-preserve-fields restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/crd-preserve-fields restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/job restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/job restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/pod restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/pod restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/restic restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/restic restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/role-bindings restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/role-bindings restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/service restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/service restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/service-account restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/service-account restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""Copied Backup to file"" backup=raffia-demo3 bytes=24618 fileName=/tmp/raffia-demo3808738384 logSource=""pkg/controller/restore_controller.go:550"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""starting restore"" logSource=""pkg/controller/restore_controller.go:458"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=customresourcedefinitions.apiextensions.k8s.io logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=namespaces logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=storageclasses.storage.k8s.io logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=persistentvolumeclaims logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=secrets logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=configmaps logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=serviceaccounts logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=limitranges logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=pods logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=replicasets.apps logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=namespaces logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=events logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=limitranges logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=services logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=configmaps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=secrets logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=nodes logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=pods logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=endpoints logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=podtemplates logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=resourcequotas logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=persistentvolumeclaims logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=serviceaccounts logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=replicationcontrollers logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=apiservices.apiregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=deployments.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=controllerrevisions.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=daemonsets.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=statefulsets.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=replicasets.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=events.events.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=horizontalpodautoscalers.autoscaling logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=jobs.batch logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=cronjobs.batch logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=certificatesigningrequests.certificates.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=networkpolicies.networking.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=poddisruptionbudgets.policy logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=podsecuritypolicies.policy logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=roles.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=clusterrolebindings.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=rolebindings.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=clusterroles.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=storageclasses.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=volumeattachments.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=validatingwebhookconfigurations.admissionregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=mutatingwebhookconfigurations.admissionregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=initializerconfigurations.admissionregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=customresourcedefinitions.apiextensions.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=priorityclasses.scheduling.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=leases.coordination.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=cephnfses.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=cephclusters.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=cephfilesystems.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=cephobjectstoreusers.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=cephobjectstores.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=cephblockpools.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=servicemonitors.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=prometheusrules.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=prometheuses.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=alertmanagers.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=podmonitors.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=restores.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=serverstatusrequests.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=backupstoragelocations.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=podvolumerestores.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=volumesnapshotlocations.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=deletebackuprequests.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=schedules.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=downloadrequests.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=backups.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=resticrepositories.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=podvolumebackups.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=issuers.certmanager.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=clusterissuers.certmanager.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=certificates.certmanager.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=nfsservers.nfs.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=objectbucketclaims.objectbucket.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=objectbuckets.objectbucket.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=volumesnapshots.snapshot.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=volumesnapshotcontents.snapshot.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=volumesnapshotclasses.snapshot.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=volumes.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=httptriggers.kubeless.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=cronjobtriggers.kubeless.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=functions.kubeless.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=deployments.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=daemonsets.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=podsecuritypolicies.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=networkpolicies.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=replicasets.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Not including resource"" groupResource=ingresses.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Starting restore of backup velero/raffia-demo3"" logSource=""pkg/restore/restore.go:394"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Restoring cluster level resource 'persistentvolumes'"" logSource=""pkg/restore/restore.go:779"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Getting client for /v1, Kind=PersistentVolume"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:1006"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=raffia-engine-triage restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""Getting plugin config"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""No storage class mappings found"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:73"" pluginName=velero restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Attempting to restore PersistentVolume: raffia-engine-triage"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:1006"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=raffia-home restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:1030"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""Getting plugin config"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:66"" pluginName=velero restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=debug msg=""No storage class mappings found"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:73"" pluginName=velero restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Attempting to restore PersistentVolume: raffia-home"" logSource=""pkg/restore/restore.go:1136"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:36Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=customresourcedefinitions.apiextensions.k8s.io logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=namespaces logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=storageclasses.storage.k8s.io logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=persistentvolumeclaims logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=secrets logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=configmaps logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=serviceaccounts logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=limitranges logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=pods logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=replicasets.apps logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=endpoints logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=podtemplates logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=replicationcontrollers logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=services logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=namespaces logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=pods logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=configmaps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=nodes logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=persistentvolumeclaims logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=resourcequotas logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=secrets logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=events logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=limitranges logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=serviceaccounts logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=apiservices.apiregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=controllerrevisions.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=statefulsets.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=daemonsets.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=replicasets.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=deployments.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=events.events.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=horizontalpodautoscalers.autoscaling logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=jobs.batch logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=cronjobs.batch logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=certificatesigningrequests.certificates.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=networkpolicies.networking.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=poddisruptionbudgets.policy logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=podsecuritypolicies.policy logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=rolebindings.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=clusterroles.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=clusterrolebindings.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=roles.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=volumeattachments.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=storageclasses.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=mutatingwebhookconfigurations.admissionregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=validatingwebhookconfigurations.admissionregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=initializerconfigurations.admissionregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=customresourcedefinitions.apiextensions.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=priorityclasses.scheduling.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=leases.coordination.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=cephnfses.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=cephclusters.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=cephblockpools.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=cephfilesystems.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=cephobjectstoreusers.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=cephobjectstores.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=prometheuses.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=podmonitors.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=prometheusrules.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=servicemonitors.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=alertmanagers.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=volumesnapshotlocations.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=backupstoragelocations.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=resticrepositories.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=podvolumebackups.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=schedules.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=podvolumerestores.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=backups.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=serverstatusrequests.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=deletebackuprequests.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=restores.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=downloadrequests.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=issuers.certmanager.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=certificates.certmanager.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=clusterissuers.certmanager.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=nfsservers.nfs.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=objectbuckets.objectbucket.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=objectbucketclaims.objectbucket.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=volumesnapshotcontents.snapshot.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=volumesnapshots.snapshot.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=volumesnapshotclasses.snapshot.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=volumes.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=cronjobtriggers.kubeless.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=functions.kubeless.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=httptriggers.kubeless.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=daemonsets.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=networkpolicies.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=deployments.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=ingresses.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=podsecuritypolicies.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Not including resource"" groupResource=replicasets.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Waiting for all restic restores to complete"" logSource=""pkg/restore/restore.go:545"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""Done waiting for all restic restores to complete"" logSource=""pkg/restore/restore.go:561"" restore=velero/raffia-demo3-20200513100836
time=""2020-05-13T10:08:39Z"" level=info msg=""restore completed"" logSource=""pkg/controller/restore_controller.go:473"" restore=velero/raffia-demo3-20200513100836
```

only two PV's get restored - and the others were not excluded from the backup

```
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS        CLAIM                                STORAGECLASS         REASON   AGE
raffia-engine-triage                       2Gi        RWO            Retain           Available                                          local-storage                 3m28s
raffia-home                                1M         RWO            Retain           Available                                          local-storage                 3m28s
```

The errors oddly say ""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=raffia-home "" and
 ""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=raffia-engine-triage""

which are the ones which actually have restored.


When describing the backup I can see some PV's - however im going to re-run a pr and backups to double check my annotations are correct as some look to be missing

```
Persistent Volumes: <none included>

Restic Backups:
  Completed:
    raffia/raffia-mongodb-54ccbdcf87-c9b6m: data
    raffia/raffia-raffia-5667cc7768-5cn64: raffia-config, raffia-domains, raffia-scripts, raffia-var, vaultca
    raffia/raffia-raffia-engine-7b665ddbb4-fq8zv: raffia-engine-scripts, raffiaengine-config, raffiaengine-home, raffiaengine-var, vaultca
``
--

--
I've tried another test - creating a new backup on a new cluster of a namespace. All the pods are correctly annotated with the volumenames. I deleted the raffia namespace and the volumes before the restore


```
velero backup create raffia-pr-1724 --include-namespaces raffia --snapshot-volumes
```

```
kubectl delete namespace raffia
```


```
kubectl delete pv raffia-config raffia-domains raffia-engine-config raffia-engine-home raffia-engine-triage raffia-engine-var raffia-home raffia-mongodb-data raffia-raffia-var
persistentvolume ""raffia-config"" deleted
persistentvolume ""raffia-domains"" deleted
persistentvolume ""raffia-engine-config"" deleted
persistentvolume ""raffia-engine-home"" deleted
persistentvolume ""raffia-engine-triage"" deleted
persistentvolume ""raffia-engine-var"" deleted
persistentvolume ""raffia-home"" deleted
persistentvolume ""raffia-mongodb-data"" deleted
persistentvolume ""raffia-raffia-var"" deleted
```

```
velero describe backup raffia-pr-1724 --details
Name:         raffia-pr-1724
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  <none>

Phase:  Completed

Namespaces:
  Included:  raffia
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  default

Snapshot PVs:  true

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  1

Started:    2020-05-13 11:19:33 +0000 UTC
Completed:  2020-05-13 11:20:43 +0000 UTC

Expiration:  2020-06-12 11:19:33 +0000 UTC

Resource List:
  apps/v1/Deployment:
    - raffia/raffia-mongodb
    - raffia/raffia-raffia
    - raffia/raffia-raffia-engine
  apps/v1/ReplicaSet:
    - raffia/raffia-mongodb-54ccbdcf87
    - raffia/raffia-raffia-5667cc7768
    - raffia/raffia-raffia-engine-7b665ddbb4
  extensions/v1beta1/Ingress:
    - raffia/raffia-raffia
  rbac.authorization.k8s.io/v1/Role:
    - raffia/raffia-role-read-only
  rbac.authorization.k8s.io/v1/RoleBinding:
    - raffia/raffia-read-only-role-binding
  v1/ConfigMap:
    - raffia/raffia-raffia-engine-scripts
    - raffia/raffia-raffia-scripts
  v1/Endpoints:
    - raffia/raffia-mongodb
    - raffia/raffia-raffia
  v1/Event:
    - raffia/raffia-mongodb-54ccbdcf87-c9b6m.160e910a2726cbba
    - raffia/raffia-mongodb-54ccbdcf87-c9b6m.160e910a55654e67
    - raffia/raffia-mongodb-54ccbdcf87-c9b6m.160e910a9d9720e2
    - raffia/raffia-mongodb-54ccbdcf87-c9b6m.160e910aa08d5c25
    - raffia/raffia-mongodb-54ccbdcf87-c9b6m.160e910aa9df38fb
    - raffia/raffia-mongodb-54ccbdcf87-c9b6m.160e910cd57b8a46
    - raffia/raffia-mongodb-54ccbdcf87-c9b6m.160e910cf47878fe
    - raffia/raffia-mongodb-54ccbdcf87-c9b6m.160e910cfdcd5c4a
    - raffia/raffia-mongodb.160e9109fd1dc5d3
    - raffia/raffia-raffia-5667cc7768-5cn64.160e910a7016456e
    - raffia/raffia-raffia-5667cc7768-5cn64.160e910b24a68184
    - raffia/raffia-raffia-5667cc7768-5cn64.160e910b26ff5740
    - raffia/raffia-raffia-5667cc7768-5cn64.160e910b30440692
    - raffia/raffia-raffia-5667cc7768-5cn64.160e9111fb46dfb5
    - raffia/raffia-raffia-5667cc7768-5cn64.160e9111fec427d6
    - raffia/raffia-raffia-5667cc7768-5cn64.160e91120b814e0e
    - raffia/raffia-raffia-5667cc7768-5cn64.160e9115b3761e5d
    - raffia/raffia-raffia-5667cc7768-5cn64.160e9115b66cdfff
    - raffia/raffia-raffia-5667cc7768-5cn64.160e9115c02cc9c5
    - raffia/raffia-raffia-5667cc7768-5cn64.160e91197c2c70ee
    - raffia/raffia-raffia-5667cc7768-5cn64.160e911a0d8278f5
    - raffia/raffia-raffia-5667cc7768-5cn64.160e911a164d9fca
    - raffia/raffia-raffia-config.160e9109fe5ec659
    - raffia/raffia-raffia-domains.160e9109ffe31e90
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e910a89aedbed
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e910c1f5d7b2b
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e910c21f5537d
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e910c2adf937a
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e91156c04430d
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e91156ea01d51
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e911578322a40
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e91191cc0f012
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e91191f6653c5
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e911928e937e0
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e91252862bf6e
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e91252ade8fe1
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e912533cc0730
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e9128db373306
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e9128df4f1af1
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv.160e9128e9223df2
    - raffia/raffia-raffia-engine-config.160e910a0167de6a
    - raffia/raffia-raffia-engine-home.160e910a022c64ef
    - raffia/raffia-raffia-engine-var.160e910a0545076a
    - raffia/raffia-raffia-var.160e910a0937d349
    - raffia/raffia-raffia.160e910a2a405cd6
    - raffia/raffia-raffia.160e910a2bed4257
    - raffia/raffia-raffia.160e911569c7b95d
    - raffia/raffia-raffia.160e911569c96430
  v1/Namespace:
    - raffia
  v1/PersistentVolume:
    - raffia-config
    - raffia-domains
    - raffia-engine-config
    - raffia-engine-home
    - raffia-engine-triage
    - raffia-engine-var
    - raffia-home
    - raffia-mongodb-data
    - raffia-raffia-var
  v1/PersistentVolumeClaim:
    - raffia/raffia-mongodb
    - raffia/raffia-raffia-config
    - raffia/raffia-raffia-domains
    - raffia/raffia-raffia-engine-config
    - raffia/raffia-raffia-engine-home
    - raffia/raffia-raffia-engine-triage
    - raffia/raffia-raffia-engine-var
    - raffia/raffia-raffia-home
    - raffia/raffia-raffia-var
  v1/Pod:
    - raffia/raffia-mongodb-54ccbdcf87-c9b6m
    - raffia/raffia-raffia-5667cc7768-5cn64
    - raffia/raffia-raffia-engine-7b665ddbb4-fq8zv
  v1/Secret:
    - raffia/cva-id
    - raffia/cva-info
    - raffia/default-token-287bb
    - raffia/dxcdockerslmcred
    - raffia/raffia-cert-vault-approle
    - raffia/raffia-deploy-vault-approle
    - raffia/raffia-ephemeral-path
    - raffia/raffia-mongodb
    - raffia/raffia-sa-token-xhls7
    - raffia/raffia-vault-approle
    - raffia/vault-client-tls
  v1/Service:
    - raffia/raffia-mongodb
    - raffia/raffia-raffia
  v1/ServiceAccount:
    - raffia/default
    - raffia/raffia-sa

Persistent Volumes: <none included>

Restic Backups:
  Completed:
    raffia/raffia-mongodb-54ccbdcf87-c9b6m: data, default-token-287bb
    raffia/raffia-raffia-5667cc7768-5cn64: default-token-287bb, raffia-config, raffia-domains, raffia-home, raffia-scripts, raffia-var, vaultca
    raffia/raffia-raffia-engine-7b665ddbb4-fq8zv: default-token-287bb, raffia-engine-scripts, raffiaengine-config, raffiaengine-home, raffiaengine-triage, raffiaengine-var, vaultca
```
Tried a restore of JUST persistentvolumes

```
velero restore create --from-backup raffia-pr-1724 --include-resources persistentvolumes
Restore request ""raffia-pr-1724-20200513112330"" submitted successfully.
```

```
velero restore logs raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/add-pv-from-pvc restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""creating new restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:114"" name=velero.io/add-pv-from-pvc restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""starting plugin"" args=""[/velero run-plugins --log-level debug]"" cmd=/velero logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/velero restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""plugin started"" cmd=/velero logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/velero pid=1776 restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""waiting for RPC address"" cmd=/velero logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" path=/velero restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""Setting log level to DEBUG"" cmd=/velero logSource=""pkg/plugin/framework/server.go:172"" pluginName=velero restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""plugin address"" address=/tmp/plugin013891697 cmd=/velero logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" network=unix pluginName=velero restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""using plugin"" cmd=/velero logSource=""pkg/plugin/clientmgmt/logrus_adapter.go:74"" restore=velero/raffia-pr-1724-20200513112330 version=2
time=""2020-05-13T11:23:30Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/add-pvc-from-pod restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/add-pvc-from-pod restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/change-storage-class restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/change-storage-class restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/cluster-role-bindings restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/cluster-role-bindings restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/crd-preserve-fields restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/crd-preserve-fields restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/job restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/job restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/pod restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/pod restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/restic restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/restic restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/role-bindings restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/role-bindings restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/service restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/service restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""looking for plugin in registry"" kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:99"" name=velero.io/service-account restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""found preexisting restartable plugin process"" command=/velero kind=ObjectStore logSource=""pkg/plugin/clientmgmt/manager.go:110"" name=velero.io/service-account restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=debug msg=""Copied Backup to file"" backup=raffia-pr-1724 bytes=28473 fileName=/tmp/raffia-pr-1724837252429 logSource=""pkg/controller/restore_controller.go:550"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""starting restore"" logSource=""pkg/controller/restore_controller.go:458"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=customresourcedefinitions.apiextensions.k8s.io logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=namespaces logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=storageclasses.storage.k8s.io logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=persistentvolumeclaims logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=secrets logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=configmaps logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=serviceaccounts logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=limitranges logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=pods logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=replicasets.apps logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=resourcequotas logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=secrets logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=configmaps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=events logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=nodes logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=podtemplates logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=limitranges logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=replicationcontrollers logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=serviceaccounts logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=services logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=endpoints logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=pods logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=namespaces logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=persistentvolumeclaims logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=apiservices.apiregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=statefulsets.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=replicasets.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=controllerrevisions.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=deployments.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=daemonsets.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=events.events.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=horizontalpodautoscalers.autoscaling logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=jobs.batch logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=cronjobs.batch logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=certificatesigningrequests.certificates.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=networkpolicies.networking.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=poddisruptionbudgets.policy logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=podsecuritypolicies.policy logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=roles.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=clusterroles.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=clusterrolebindings.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=rolebindings.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=storageclasses.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=volumeattachments.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=mutatingwebhookconfigurations.admissionregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=validatingwebhookconfigurations.admissionregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=initializerconfigurations.admissionregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=customresourcedefinitions.apiextensions.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=priorityclasses.scheduling.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=leases.coordination.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=cephblockpools.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=cephnfses.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=cephobjectstoreusers.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=cephobjectstores.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=cephfilesystems.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=cephclusters.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=servicemonitors.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=prometheusrules.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=alertmanagers.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=prometheuses.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=podmonitors.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=podvolumebackups.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=backupstoragelocations.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=restores.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=serverstatusrequests.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=resticrepositories.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=downloadrequests.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=schedules.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=deletebackuprequests.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=volumesnapshotlocations.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=podvolumerestores.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=backups.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=issuers.certmanager.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=certificates.certmanager.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=clusterissuers.certmanager.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=nfsservers.nfs.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=objectbucketclaims.objectbucket.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=objectbuckets.objectbucket.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=volumesnapshots.snapshot.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=volumesnapshotclasses.snapshot.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=volumesnapshotcontents.snapshot.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=volumes.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=functions.kubeless.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=httptriggers.kubeless.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=cronjobtriggers.kubeless.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=daemonsets.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=replicasets.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=ingresses.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=networkpolicies.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=podsecuritypolicies.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Not including resource"" groupResource=deployments.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Starting restore of backup velero/raffia-pr-1724"" logSource=""pkg/restore/restore.go:394"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Restoring cluster level resource 'persistentvolumes'"" logSource=""pkg/restore/restore.go:779"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Getting client for /v1, Kind=PersistentVolume"" logSource=""pkg/restore/restore.go:821"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:30Z"" level=info msg=""Dynamically re-provisioning persistent volume because it has a restic backup to be restored."" logSource=""pkg/restore/restore.go:992"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=customresourcedefinitions.apiextensions.k8s.io logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=namespaces logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=storageclasses.storage.k8s.io logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=persistentvolumeclaims logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=secrets logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=configmaps logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=serviceaccounts logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=limitranges logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=pods logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=replicasets.apps logSource=""pkg/restore/restore.go:118"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=podtemplates logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=pods logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=secrets logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=replicationcontrollers logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=endpoints logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=limitranges logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=nodes logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=resourcequotas logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=events logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=serviceaccounts logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=services logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=namespaces logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=configmaps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=persistentvolumeclaims logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=apiservices.apiregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=replicasets.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=statefulsets.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=daemonsets.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=deployments.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=controllerrevisions.apps logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=events.events.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=horizontalpodautoscalers.autoscaling logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=jobs.batch logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=cronjobs.batch logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=certificatesigningrequests.certificates.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=networkpolicies.networking.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=podsecuritypolicies.policy logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=poddisruptionbudgets.policy logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=roles.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=clusterrolebindings.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=clusterroles.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=rolebindings.rbac.authorization.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=storageclasses.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=volumeattachments.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=mutatingwebhookconfigurations.admissionregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=validatingwebhookconfigurations.admissionregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=initializerconfigurations.admissionregistration.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=customresourcedefinitions.apiextensions.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=priorityclasses.scheduling.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=leases.coordination.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=cephobjectstores.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=cephfilesystems.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=cephblockpools.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=cephnfses.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=cephobjectstoreusers.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=cephclusters.ceph.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=alertmanagers.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=podmonitors.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=prometheusrules.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=prometheuses.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=servicemonitors.monitoring.coreos.com logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=volumesnapshotlocations.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=podvolumerestores.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=backups.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=downloadrequests.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=restores.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=serverstatusrequests.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=backupstoragelocations.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=schedules.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=resticrepositories.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=deletebackuprequests.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=podvolumebackups.velero.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=issuers.certmanager.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=clusterissuers.certmanager.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=certificates.certmanager.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=nfsservers.nfs.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=objectbucketclaims.objectbucket.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=objectbuckets.objectbucket.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=volumesnapshotclasses.snapshot.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=volumesnapshotcontents.snapshot.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=volumesnapshots.snapshot.storage.k8s.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=volumes.rook.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=cronjobtriggers.kubeless.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=httptriggers.kubeless.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=functions.kubeless.io logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=networkpolicies.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=podsecuritypolicies.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=ingresses.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=daemonsets.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=deployments.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Not including resource"" groupResource=replicasets.extensions logSource=""pkg/restore/restore.go:138"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Waiting for all restic restores to complete"" logSource=""pkg/restore/restore.go:545"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""Done waiting for all restic restores to complete"" logSource=""pkg/restore/restore.go:561"" restore=velero/raffia-pr-1724-20200513112330
time=""2020-05-13T11:23:33Z"" level=info msg=""restore completed"" logSource=""pkg/controller/restore_controller.go:473"" restore=velero/raffia-pr-1724-20200513112330
```

raffia namespace is created - but no PV's


```
kubectl get namespace | grep raffia
raffia               Active   12m


kubectl get pv | grep -i raffia
tmp]$
```

Looked at the restic log and see the following...

```
kubectl logs restic-x5z2g -n velero
time=""2020-05-13T09:56:24Z"" level=info msg=""Setting log-level to INFO""
time=""2020-05-13T09:56:24Z"" level=info msg=""Starting Velero restic server v1.3.1 (0665b05321eefeb7b7fdd6984750745b7429774f)"" logSource=""pkg/cmd/cli/restic/server.go:62""
time=""2020-05-13T09:56:24Z"" level=info msg=""Starting controllers"" logSource=""pkg/cmd/cli/restic/server.go:156""
time=""2020-05-13T09:56:24Z"" level=info msg=""Controllers started successfully"" logSource=""pkg/cmd/cli/restic/server.go:199""
time=""2020-05-13T09:56:24Z"" level=info msg=""Starting controller"" controller=pod-volume-backup logSource=""pkg/controller/generic_controller.go:76""
time=""2020-05-13T09:56:24Z"" level=info msg=""Waiting for caches to sync"" controller=pod-volume-backup logSource=""pkg/controller/generic_controller.go:79""
time=""2020-05-13T09:56:24Z"" level=info msg=""Starting controller"" controller=pod-volume-restore logSource=""pkg/controller/generic_controller.go:76""
time=""2020-05-13T09:56:24Z"" level=info msg=""Waiting for caches to sync"" controller=pod-volume-restore logSource=""pkg/controller/generic_controller.go:79""
time=""2020-05-13T09:56:24Z"" level=info msg=""Caches are synced"" controller=pod-volume-backup logSource=""pkg/controller/generic_controller.go:83""
time=""2020-05-13T09:56:24Z"" level=info msg=""Caches are synced"" controller=pod-volume-restore logSource=""pkg/controller/generic_controller.go:83""
time=""2020-05-13T10:53:15Z"" level=info msg=""Restore starting"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:262"" name=raffia-demo3-20200513105239-7fccp namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:22Z"" level=info msg=""Restore completed"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:311"" name=raffia-demo3-20200513105239-7fccp namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:22Z"" level=info msg=""Restore starting"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:262"" name=raffia-demo3-20200513105239-nh4r6 namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:25Z"" level=info msg=""Restore completed"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:311"" name=raffia-demo3-20200513105239-nh4r6 namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:25Z"" level=info msg=""Restore starting"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:262"" name=raffia-demo3-20200513105239-25ts4 namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:28Z"" level=info msg=""Restore completed"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:311"" name=raffia-demo3-20200513105239-25ts4 namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:28Z"" level=info msg=""Restore starting"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:262"" name=raffia-demo3-20200513105239-5h2xq namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:37Z"" level=info msg=""Restore completed"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:311"" name=raffia-demo3-20200513105239-5h2xq namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:37Z"" level=info msg=""Restore starting"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:262"" name=raffia-demo3-20200513105239-vjmjq namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:40Z"" level=info msg=""Restore completed"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:311"" name=raffia-demo3-20200513105239-vjmjq namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:40Z"" level=info msg=""Restore starting"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:262"" name=raffia-demo3-20200513105239-mkx9g namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:43Z"" level=info msg=""Restore completed"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:311"" name=raffia-demo3-20200513105239-mkx9g namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:43Z"" level=info msg=""Restore starting"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:262"" name=raffia-demo3-20200513105239-llxvn namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:46Z"" level=info msg=""Restore completed"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:311"" name=raffia-demo3-20200513105239-llxvn namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:46Z"" level=info msg=""Restore starting"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:262"" name=raffia-demo3-20200513105239-lk4mh namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:49Z"" level=info msg=""Restore completed"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:311"" name=raffia-demo3-20200513105239-lk4mh namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:49Z"" level=info msg=""Restore starting"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:262"" name=raffia-demo3-20200513105239-9wrrj namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:52Z"" level=info msg=""Restore completed"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:311"" name=raffia-demo3-20200513105239-9wrrj namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:52Z"" level=info msg=""Restore starting"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:262"" name=raffia-demo3-20200513105239-hqn8t namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:55Z"" level=info msg=""Restore completed"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:311"" name=raffia-demo3-20200513105239-hqn8t namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:55Z"" level=info msg=""Restore starting"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:262"" name=raffia-demo3-20200513105239-s8tjg namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T10:53:58Z"" level=info msg=""Restore completed"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:311"" name=raffia-demo3-20200513105239-s8tjg namespace=velero restore=velero/raffia-demo3-20200513105239
time=""2020-05-13T11:19:48Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-4pgdf namespace=velero
time=""2020-05-13T11:19:48Z"" level=info msg=""Looking for most recent completed pod volume backup for this PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:307"" name=raffia-pr-1724-4pgdf namespace=velero pvcUID=efa100df-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:19:48Z"" level=info msg=""No completed pod volume backup found for PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:337"" name=raffia-pr-1724-4pgdf namespace=velero pvcUID=efa100df-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:19:48Z"" level=info msg=""No parent snapshot found for PVC, not using --parent flag for this backup"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:253"" name=raffia-pr-1724-4pgdf namespace=velero
time=""2020-05-13T11:19:58Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-4pgdf namespace=velero
time=""2020-05-13T11:19:58Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-2nzgd namespace=velero
time=""2020-05-13T11:20:01Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-2nzgd namespace=velero
time=""2020-05-13T11:20:01Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-sd4xp namespace=velero
time=""2020-05-13T11:20:04Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-sd4xp namespace=velero
time=""2020-05-13T11:20:04Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-zwqqt namespace=velero
time=""2020-05-13T11:20:04Z"" level=info msg=""Looking for most recent completed pod volume backup for this PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:307"" name=raffia-pr-1724-zwqqt namespace=velero pvcUID=efa3f3dc-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:04Z"" level=info msg=""No completed pod volume backup found for PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:337"" name=raffia-pr-1724-zwqqt namespace=velero pvcUID=efa3f3dc-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:04Z"" level=info msg=""No parent snapshot found for PVC, not using --parent flag for this backup"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:253"" name=raffia-pr-1724-zwqqt namespace=velero
time=""2020-05-13T11:20:06Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-zwqqt namespace=velero
time=""2020-05-13T11:20:06Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-xvnlq namespace=velero
time=""2020-05-13T11:20:06Z"" level=info msg=""Looking for most recent completed pod volume backup for this PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:307"" name=raffia-pr-1724-xvnlq namespace=velero pvcUID=efa89a58-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:06Z"" level=info msg=""No completed pod volume backup found for PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:337"" name=raffia-pr-1724-xvnlq namespace=velero pvcUID=efa89a58-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:06Z"" level=info msg=""No parent snapshot found for PVC, not using --parent flag for this backup"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:253"" name=raffia-pr-1724-xvnlq namespace=velero
time=""2020-05-13T11:20:12Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-xvnlq namespace=velero
time=""2020-05-13T11:20:12Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-r6x2t namespace=velero
time=""2020-05-13T11:20:12Z"" level=info msg=""Looking for most recent completed pod volume backup for this PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:307"" name=raffia-pr-1724-r6x2t namespace=velero pvcUID=efbcaa9c-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:12Z"" level=info msg=""No completed pod volume backup found for PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:337"" name=raffia-pr-1724-r6x2t namespace=velero pvcUID=efbcaa9c-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:12Z"" level=info msg=""No parent snapshot found for PVC, not using --parent flag for this backup"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:253"" name=raffia-pr-1724-r6x2t namespace=velero
time=""2020-05-13T11:20:15Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-r6x2t namespace=velero
time=""2020-05-13T11:20:15Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-pqfw2 namespace=velero
time=""2020-05-13T11:20:15Z"" level=info msg=""Looking for most recent completed pod volume backup for this PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:307"" name=raffia-pr-1724-pqfw2 namespace=velero pvcUID=efb8de9c-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:15Z"" level=info msg=""No completed pod volume backup found for PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:337"" name=raffia-pr-1724-pqfw2 namespace=velero pvcUID=efb8de9c-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:15Z"" level=info msg=""No parent snapshot found for PVC, not using --parent flag for this backup"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:253"" name=raffia-pr-1724-pqfw2 namespace=velero
time=""2020-05-13T11:20:16Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-pqfw2 namespace=velero
time=""2020-05-13T11:20:16Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-hwgqv namespace=velero
time=""2020-05-13T11:20:19Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-hwgqv namespace=velero
time=""2020-05-13T11:20:19Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-k2qqb namespace=velero
time=""2020-05-13T11:20:22Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-k2qqb namespace=velero
time=""2020-05-13T11:20:22Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-pr7nr namespace=velero
time=""2020-05-13T11:20:24Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-pr7nr namespace=velero
time=""2020-05-13T11:20:24Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-2sf8k namespace=velero
time=""2020-05-13T11:20:25Z"" level=info msg=""Looking for most recent completed pod volume backup for this PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:307"" name=raffia-pr-1724-2sf8k namespace=velero pvcUID=efab9589-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:25Z"" level=info msg=""No completed pod volume backup found for PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:337"" name=raffia-pr-1724-2sf8k namespace=velero pvcUID=efab9589-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:25Z"" level=info msg=""No parent snapshot found for PVC, not using --parent flag for this backup"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:253"" name=raffia-pr-1724-2sf8k namespace=velero
time=""2020-05-13T11:20:28Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-2sf8k namespace=velero
time=""2020-05-13T11:20:28Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-p4zt2 namespace=velero
time=""2020-05-13T11:20:28Z"" level=info msg=""Looking for most recent completed pod volume backup for this PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:307"" name=raffia-pr-1724-p4zt2 namespace=velero pvcUID=efb5ff09-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:28Z"" level=info msg=""No completed pod volume backup found for PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:337"" name=raffia-pr-1724-p4zt2 namespace=velero pvcUID=efb5ff09-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:28Z"" level=info msg=""No parent snapshot found for PVC, not using --parent flag for this backup"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:253"" name=raffia-pr-1724-p4zt2 namespace=velero
time=""2020-05-13T11:20:30Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-p4zt2 namespace=velero
time=""2020-05-13T11:20:30Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-xrnjb namespace=velero
time=""2020-05-13T11:20:30Z"" level=info msg=""Looking for most recent completed pod volume backup for this PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:307"" name=raffia-pr-1724-xrnjb namespace=velero pvcUID=efae685e-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:30Z"" level=info msg=""No completed pod volume backup found for PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:337"" name=raffia-pr-1724-xrnjb namespace=velero pvcUID=efae685e-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:30Z"" level=info msg=""No parent snapshot found for PVC, not using --parent flag for this backup"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:253"" name=raffia-pr-1724-xrnjb namespace=velero
time=""2020-05-13T11:20:34Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-xrnjb namespace=velero
time=""2020-05-13T11:20:34Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-tpwcw namespace=velero
time=""2020-05-13T11:20:34Z"" level=info msg=""Looking for most recent completed pod volume backup for this PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:307"" name=raffia-pr-1724-tpwcw namespace=velero pvcUID=efb1151c-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:34Z"" level=info msg=""No completed pod volume backup found for PVC"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:337"" name=raffia-pr-1724-tpwcw namespace=velero pvcUID=efb1151c-9507-11ea-8f88-02bc09c18c05
time=""2020-05-13T11:20:34Z"" level=info msg=""No parent snapshot found for PVC, not using --parent flag for this backup"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:253"" name=raffia-pr-1724-tpwcw namespace=velero
time=""2020-05-13T11:20:36Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-tpwcw namespace=velero
time=""2020-05-13T11:20:36Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-hfrgx namespace=velero
time=""2020-05-13T11:20:39Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-hfrgx namespace=velero
time=""2020-05-13T11:20:39Z"" level=info msg=""Backup starting"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:178"" name=raffia-pr-1724-gv99j namespace=velero
time=""2020-05-13T11:20:41Z"" level=info msg=""Backup completed"" backup=velero/raffia-pr-1724 controller=pod-volume-backup logSource=""pkg/controller/pod_volume_backup_controller.go:297"" name=raffia-pr-1724-gv99j namespace=velero
```

So far I have only been able to restore the PV's after creating them first via kubectl apply -f <pvs.yaml> before running the restore.

I did notice when I tried on another more simple app with one PV I didnt have to create the PV first, the restore took care of that - but i don't know if that was a one-off - i can try that again though.

Does restic struggle with multiple PV's to one app?








--

--
Hi - is anyone still looking at this please?
--

--
Hi @skriss  - thanks for getting back to me

That does sound plausible - and a test I've done since raising the issue does suggest it's some kind of conflict with restic that causes the issue.

To test I did what I previously did - backup the whole cluster, delete the raffia namespace and pv's then attempt a restore from the full cluster backup. The behavior I'd seen before was the same (as an addendum, this is now using 1.4.0 rather than 1.3.2 previously)

I also created a pv only backup which I'll get to later, but this didnt include any restic resources.

```
velero backup create full-cluster

velero backup create pv-only --include-resources pv

for i in `kubectl get pv | grep raffia | awk '{print $1}'`
> do
> kubectl delete pv $i
> done
persistentvolume ""raffia-config"" deleted
persistentvolume ""raffia-domains"" deleted
persistentvolume ""raffia-engine-config"" deleted
persistentvolume ""raffia-engine-home"" deleted
persistentvolume ""raffia-engine-triage"" deleted
persistentvolume ""raffia-engine-var"" deleted
persistentvolume ""raffia-home"" deleted
persistentvolume ""raffia-mongodb-data"" deleted
persistentvolume ""raffia-raffia-var"" deleted


kubectl delete namespace raffia
namespace ""raffia"" deleted


velero restore create --from-backup full-cluster


velero restore describe full-cluster-20200611120024 --details
Name:         full-cluster-20200611120024
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:  InProgress

Backup:  full-cluster

Namespaces:
  Included:  all namespaces found in the backup
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io
  Cluster-scoped:  auto

Namespace mappings:  <none>

Label selector:  <none>

Restore PVs:  auto

Restic Restores:
  New:
    raffia/raffia-mongodb-5d47b57cbd-6rfnm: data
    raffia/raffia-raffia-54f7475668-hwgnz: raffia-config, raffia-domains, raffia-scripts, raffia-var, vaultca
    raffia/raffia-raffia-engine-5c5f6669b6-rr2t9: raffia-engine-scripts, raffiaengine-config, raffiaengine-home, raffiaengine-var, vaultca
```

The restore pauses with the above and times out after an hour

checking resources on the cluster...

```
kubectl get all -n raffia
NAME                                        READY   STATUS    RESTARTS   AGE
pod/raffia-mongodb-5d47b57cbd-6rfnm         0/1     Pending   0          23m
pod/raffia-raffia-54f7475668-hwgnz          0/1     Pending   0          23m
pod/raffia-raffia-engine-5c5f6669b6-rr2t9   0/1     Pending   0          23m

NAME                     TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)     AGE
service/raffia-mongodb   ClusterIP   130.174.241.246   <none>        27017/TCP   22m
service/raffia-raffia    ClusterIP   130.174.240.172   <none>        3030/TCP    22m

NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/raffia-mongodb         0/1     1            0           22m
deployment.apps/raffia-raffia          0/1     1            0           22m
deployment.apps/raffia-raffia-engine   0/1     1            0           22m

NAME                                              DESIRED   CURRENT   READY   AGE
replicaset.apps/raffia-mongodb-54ccbdcf87         0         0         0       23m
replicaset.apps/raffia-mongodb-5d47b57cbd         1         1         0       23m
replicaset.apps/raffia-raffia-54f7475668          1         1         0       23m
replicaset.apps/raffia-raffia-585dfd7d96          0         0         0       23m
replicaset.apps/raffia-raffia-engine-5c5f6669b6   1         1         0       23m
replicaset.apps/raffia-raffia-engine-858bc9dc44   0         0         0       23m




kubectl get pvc -n raffia
NAME                          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS    AGE
raffia-mongodb                Pending                                      local-storage   24m
raffia-raffia-config          Pending                                      local-storage   24m
raffia-raffia-domains         Pending                                      local-storage   24m
raffia-raffia-engine-config   Pending                                      local-storage   24m
raffia-raffia-engine-home     Pending                                      local-storage   24m
raffia-raffia-engine-triage   Pending                                      local-storage   24m
raffia-raffia-engine-var      Pending                                      local-storage   24m
raffia-raffia-home            Pending                                      local-storage   24m
raffia-raffia-var             Pending                                      local-storage   24m

kubectl get pv | grep raffia
*no results*

```

I deleted everything to do with raffia and the namespace again, then carried out a restore from the pv-only backup, then the cluster backup

```
velero restore create --from-backup pv-only

kubectl get pv | grep raffia
raffia-config                              1M         RWO            Retain           Available                                        local-storage                 60s
raffia-domains                             2Gi        RWO            Retain           Available                                        local-storage                 60s
raffia-engine-config                       1M         RWO            Retain           Available                                        local-storage                 60s
raffia-engine-home                         10M        RWO            Retain           Available                                        local-storage                 60s
raffia-engine-triage                       2Gi        RWO            Retain           Available                                        local-storage                 60s
raffia-engine-var                          1M         RWO            Retain           Available                                        local-storage                 59s
raffia-home                                1M         RWO            Retain           Available                                        local-storage                 59s
raffia-mongodb-data                        8Gi        RWO            Retain           Available                                        local-storage                 59s
raffia-raffia-var                          2Gi        RWO            Retain           Available                                        local-storage                 59s


velero restore create --from-backup full-cluster


kubectl get all -n raffia
NAME                                        READY   STATUS    RESTARTS   AGE
pod/raffia-mongodb-5d47b57cbd-6rfnm         1/1     Running   0          3m8s
pod/raffia-raffia-54f7475668-hwgnz          1/1     Running   0          3m7s
pod/raffia-raffia-engine-5c5f6669b6-rr2t9   1/1     Running   0          3m7s

NAME                     TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)     AGE
service/raffia-mongodb   ClusterIP   130.174.240.53    <none>        27017/TCP   2m1s
service/raffia-raffia    ClusterIP   130.174.240.150   <none>        3030/TCP    2m1s

NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/raffia-mongodb         1/1     1            1           2m25s
deployment.apps/raffia-raffia          1/1     1            1           2m25s
deployment.apps/raffia-raffia-engine   1/1     1            1           2m25s

NAME                                              DESIRED   CURRENT   READY   AGE
replicaset.apps/raffia-mongodb-54ccbdcf87         0         0         0       2m56s
replicaset.apps/raffia-mongodb-5d47b57cbd         1         1         1       2m56s
replicaset.apps/raffia-raffia-54f7475668          1         1         1       2m56s
replicaset.apps/raffia-raffia-585dfd7d96          0         0         0       2m56s
replicaset.apps/raffia-raffia-engine-5c5f6669b6   1         1         1       2m56s
replicaset.apps/raffia-raffia-engine-858bc9dc44   0         0         0       2m56s



kubectl get pv | grep raffia
raffia-config                              1M         RWO            Retain           Bound    raffia/raffia-raffia-config          local-storage                 6m4s
raffia-domains                             2Gi        RWO            Retain           Bound    raffia/raffia-raffia-domains         local-storage                 6m4s
raffia-engine-config                       1M         RWO            Retain           Bound    raffia/raffia-raffia-engine-config   local-storage                 6m4s
raffia-engine-home                         10M        RWO            Retain           Bound    raffia/raffia-raffia-engine-home     local-storage                 6m4s
raffia-engine-triage                       2Gi        RWO            Retain           Bound    raffia/raffia-raffia-engine-triage   local-storage                 6m4s
raffia-engine-var                          1M         RWO            Retain           Bound    raffia/raffia-raffia-engine-var      local-storage                 6m3s
raffia-home                                1M         RWO            Retain           Bound    raffia/raffia-raffia-home            local-storage                 6m3s
raffia-mongodb-data                        8Gi        RWO            Retain           Bound    raffia/raffia-mongodb                local-storage                 6m3s
raffia-raffia-var                          2Gi        RWO            Retain           Bound    raffia/raffia-raffia-var             local-storage                 6m3s
```

Everything is back

the log for the pv restore:-

```
velero restore logs pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""starting restore"" logSource=""pkg/controller/restore_controller.go:453"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Starting restore of backup velero/pv-only"" logSource=""pkg/restore/restore.go:345"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Restoring cluster level resource 'persistentvolumes'"" logSource=""pkg/restore/restore.go:704"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Getting client for /v1, Kind=PersistentVolume"" logSource=""pkg/restore/restore.go:746"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=datadir-vault-consul-0 restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Attempting to restore PersistentVolume: datadir-vault-consul-0"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=datadir-vault-consul-1 restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Attempting to restore PersistentVolume: datadir-vault-consul-1"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=datadir-vault-consul-2 restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Attempting to restore PersistentVolume: datadir-vault-consul-2"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=datadir-vault-vault-audit-logs-0 restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Attempting to restore PersistentVolume: datadir-vault-vault-audit-logs-0"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=datadir-vault-vault-audit-logs-1 restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Attempting to restore PersistentVolume: datadir-vault-vault-audit-logs-1"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=datadir-vault-vault-audit-logs-2 restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Attempting to restore PersistentVolume: datadir-vault-vault-audit-logs-2"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=logstash-config restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Attempting to restore PersistentVolume: logstash-config"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=logstash-data restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Attempting to restore PersistentVolume: logstash-data"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=nagios-configs restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Attempting to restore PersistentVolume: nagios-configs"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=nagios-last-servers restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Attempting to restore PersistentVolume: nagios-last-servers"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=nagios-servers restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Attempting to restore PersistentVolume: nagios-servers"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Dynamically re-provisioning persistent volume because it doesn't have a snapshot and its reclaim policy is Delete."" logSource=""pkg/restore/restore.go:933"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Dynamically re-provisioning persistent volume because it doesn't have a snapshot and its reclaim policy is Delete."" logSource=""pkg/restore/restore.go:933"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=raffia-config restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:22Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Attempting to restore PersistentVolume: raffia-config"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=raffia-domains restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Attempting to restore PersistentVolume: raffia-domains"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=raffia-engine-config restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Attempting to restore PersistentVolume: raffia-engine-config"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=raffia-engine-home restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Attempting to restore PersistentVolume: raffia-engine-home"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=raffia-engine-triage restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Attempting to restore PersistentVolume: raffia-engine-triage"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=raffia-engine-var restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:23Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Attempting to restore PersistentVolume: raffia-engine-var"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=raffia-home restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Attempting to restore PersistentVolume: raffia-home"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=raffia-mongodb-data restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Attempting to restore PersistentVolume: raffia-mongodb-data"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Restoring persistent volume as-is because it doesn't have a snapshot and its reclaim policy is not Delete."" logSource=""pkg/restore/restore.go:940"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""No snapshot found for persistent volume"" logSource=""pkg/restore/pv_restorer.go:81"" persistentVolume=raffia-raffia-var restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Executing item action for persistentvolumes"" logSource=""pkg/restore/restore.go:964"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:63"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Done executing ChangeStorageClassAction"" cmd=/velero logSource=""pkg/restore/change_storageclass_action.go:74"" pluginName=velero restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Attempting to restore PersistentVolume: raffia-raffia-var"" logSource=""pkg/restore/restore.go:1070"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Waiting for all restic restores to complete"" logSource=""pkg/restore/restore.go:470"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""Done waiting for all restic restores to complete"" logSource=""pkg/restore/restore.go:486"" restore=velero/pv-only-20200611122854
time=""2020-06-11T12:30:24Z"" level=info msg=""restore completed"" logSource=""pkg/controller/restore_controller.go:468"" restore=velero/pv-only-20200611122854
```

So perhaps there needs to be logic when carrying out a restore from a full cluster backup that the pv's be done first? 

Also to bear in mind the pv's arent namespaced



--

--
@skriss Yeah they are created directly on the cluster via their own yaml
--

--
Thanks @skriss 

It would be good if as part of the restore, that before the pv's come down existing data on the pv's were purged - or at least velero/restic was able to look and decipher if there are any differences between the data on disk and in the backup - and decide whether to restore or not - an idea?

In the meantime it isnt too much of a problem for us to document the backup and restore process at our end to include two seperate backups and two restores - its workable for now
--
",skriss,"
--
@eddeh83 here's what I think is going on: when velero goes to restore a PV using restic, the first thing it does is dynamically provision a new, empty PV. It does this by creating a PVC, and then waiting for a storage provisioner to create a PV and bind it to the PVC.

If I'm not mistaken, local PVs are not usually dynamically provisioned. So, there's no provisioner that is processing the PVC that's waiting for a PV. As a result, velero never has a volume to restore into.

You said that if you manually (statically) provisioned a PV with the right name before the restore, then it worked - which would make sense here, since the velero-restored PVC would be able to bind to that PV that you created, and then restore data into it.

Does that match your observations? We can get into possible solutions/fixes next, but just want to make sure I'm understanding the problem first.
--

--
How do you usually create the PVs that your app uses, before Velero even comes into the picture? Do you ""statically provision"" them by directly applying PV YAML to your cluster, then creating the PVCs to claim them?
--

--
OK, that makes sense. So I think I at least understand this issue. 

The restic restore code starts by attempting to dynamically provision a new, empty PV that it can restore into. It does this by creating a PVC, and then waiting for an external provisioner to create a PV + underlying storage disk and bind it to the PVC. After the bind is complete and the pod using the PVC can start, the restic restore can actually begin writing data to the new, empty disk.

In your case (and likely for all local PV users), the PV is statically provisioned and there is no dynamic provisioner. So, when the velero code creates a PVC and waits for an external provisioner to create a PV, nothing happens, and velero eventually times out and does not perform a restic restore.

Will need to think some more about if/how we can address this. The two-step restore process that you described (first restore the PVs themselves, then restore everything else) sounds workable. The one pitfall I can think of is that after step 1, if the restored PVs are pointing to an existing location on the host, you might have pre-existing data in there that you don't want. Ideally, after step 1, the PVs would be empty so velero/restic could do a clean restore into them.
--
",aaronbernardino,"
--
I believe I encountered the same issue but I wasn't using ""local"" PVs.  I had multiple NFS backed PVs and only 1 was getting restored.  The workaround to have a pv-only backup and restore worked for me as well.  
However, I did notice a related issue if I try to backup and restore with PVs in 1 go:

So restoring my PVC gets stuck in pending with the following error:

```
Type       Reason                   Age                      From                                     Message
 ----        ------                      ----                       ----                                       -------
Warning  ProvisioningFailed  4s (x3 over 31s)  persistentvolume-controller  storageclass.storage.k8s.io ""cfg"" not found
```

The thing is ""cfg"" is not a real dynamic storage class.  We have static PV and PVCs with ""cfg"" as storageClassName used to logically bind our PV and PVCs of a certain ""class"".  Our PVs are still statically provisioned.
By the way this error goes away if I remove using storageClassNames with the static PV and PVCs.  It will then still stay in Pending because the PV does not get restored due to the original problem.
--
",maikeldotuk,"
--
I've got this exact same problem. The only solution that works for me is to create the PV (local volume) first then restore with Velero. 
--

--
What work for me (local storage is what I use) is to create first the local storage (different yaml file with the pv and pvc) then restore a velero backup. This is the only way I've managed to get the backed up files back in there from a blank folder.

It's quite annoying though. It shouldn't be a hack.

Get Outlook for Android<https://aka.ms/ghei36>
________________________________
From: asoltesz <notifications@github.com>
Sent: Saturday, November 21, 2020 12:51:08 PM
To: vmware-tanzu/velero <velero@noreply.github.com>
Cc: Miguel Frias Mosquea <maikel@maikel.uk>; Comment <comment@noreply.github.com>
Subject: Re: [vmware-tanzu/velero] restic: unable to restore ""Local"" Persistent Volumes since they're statically provisioned (#2520)


@ashish-amarnath<https://github.com/ashish-amarnath> : Thanks. This is currently a blocker for me so any workaround is appreciated.

—
You are receiving this because you commented.
Reply to this email directly, view it on GitHub<https://github.com/vmware-tanzu/velero/issues/2520#issuecomment-731568472>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AHAYX5N5CTOPPYTN5XEYVALSQ6SSZANCNFSM4M4CDHIQ>.

--
",cloudcafetech,"
--
Same issue my PVs are NFS backed PVs. Restic not restoring snapshot. Instead its creating new one.
--
"
2518,OPEN,Investigate new integration test possibilities,CI/CD; Enhancement/Dev,2020-05-12 14:46:07 +0000 UTC,carlisia,Opened,,"Why? Because mocks argh.

Below are options not for e2e, not for unit testing, but for integration testing.

# Integration testing

## evntest
https://book.kubebuilder.io/reference/writing-tests.html

Examples:
- https://github.com/kubernetes-sigs/cluster-api/tree/master/controllers
- https://github.com/microsoft/azure-databricks-operator/tree/master/controllers

Requirements
- it can point to a locally built etcd/apiserver binaries (probably the more CI/CD friendly)
- or can point to an existing/running cluster 

Note: Maybe packaging the etcd and apiserver binaries in a docker container used by a github action would work.

It works with other testing packages that test in the BDD style. I personally think this is a plus, we could benefit from laying out the behavior to test from the time of spec'ing.

Only for controllers.

## rigging
https://github.com/n3wscott/rigging

Examples:
- https://github.com/n3wscott/rigging/tree/master/example
- https://github.com/mattmoor/mink/blob/master/.github/workflows/minkind.yaml (GH action that runs e2e tests in kind)

It ""leverages lightly templated yaml files"". The knative folks are trying to get it into that project (https://github.com/knative/sample-controller/pull/175). @n3wscott has offered to showcase.

Fork of https://github.com/manifestival/manifestival.

Requirements
- needs a running cluster.

## KUTTL

https://kuttl.dev/",,,,,,,,,,,,,,
2515,OPEN,Document Velero feature flags,Area/Documentation,2020-10-22 14:52:57 +0000 UTC,ashish-amarnath,Opened,,"+1 for keeping the feature flag list small.
But we should also have a way for users to discover feature flags. That's the problem I am looking to solve here and this was the easiest way to do that today.
Some options for us to consider:
1. having a features page on the website
2. having a CLI command list all features

My vote is for option2

_Originally posted by @ashish-amarnath in https://github.com/vmware-tanzu/velero/pull/2511_",,,a,"
--
@ashish-amarnath can you comment here if this is still needed? Do you still think this is better as a CLI update than as docs?
--
",,,,,,,,,,
3155,OPEN,Update Azure plugin repo structure; Dockerfile; and Makefile to match plugin-example repo,Area/Cloud/Azure; Enhancement/Dev,2020-12-08 00:04:31 +0000 UTC,skriss,Opened,,,,,,,,,,,,,,,,
3148,OPEN,Update GCP plugin repo structure; Dockerfile; and Makefile to match plugin-example repo,Area/Cloud/GCP; Enhancement/Dev,2020-12-07 23:49:23 +0000 UTC,skriss,Opened,,,,,,,,,,,,,,,,
3141,OPEN,Update AWS plugin repo structure; Dockerfile; and Makefile to match plugin-example repo,Area/Cloud/AWS; Enhancement/Dev; Help wanted,2020-12-07 23:35:10 +0000 UTC,skriss,Opened,,,,,,,,,,,,,,,,
2505,OPEN,Update velero CRDs to v1,Breaking change; Enhancement/Dev; P2 - Long-term important; versioning,2021-03-29 19:32:47 +0000 UTC,skriss,In progress,,"We're currently generating and installing `v1beta1` CRDs for Velero, mainly for backwards compatibility. The apiextensions API group graduated to `v1` as of Kubernetes 1.16, and the `v1beta1` version of the group is scheduled to be dropped in an upcoming Kubernetes release.

At some point (before `v1beta1` is dropped for good) we need to updated our generated CRDs to be `v1`. There's an existing PR (#2190) that we may want to dust off.

Additionally, it'd be nice to take advantage of some of the `v1` features, like [defaulting](https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#defaulting). 

Tagging this for v1.5 for now -- not sure yet if it's must-do for then or not.",,,skriss,"
--
FYI per https://github.com/kubernetes/kubernetes/issues/82022#issuecomment-636872808 removal of `apiextensions.k8s.io/v1beta1` has been targeted for 1.22. cc @nrb 
--

--
Yep, so 1.16 by default - I think by the time 1.22 rolls around, that should be fine (and older versions of Velero will still work on older Kubernetes versions).
--
",nrb,"
--
Good to know!

When we move this, we'll likely end up bumping our minimum supporting k8s version to one that at least has the v1 endpoint.
--

--
@mapdegree While the community may have upgraded, it is very possible enterprise users have not. As a migration tool, Velero should be useful on older versions of Kubernetes, without requiring users to use 2 different versions of Velero to do the job.

Per [the Kubernetes deprecation guidelines](https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-22) v1beta1 will be removed in v1.22.

That said, we're looking to get this upgraded or allow dual version usage soon, so our supported window of Kubernetes versions is greater than 6 releases.
--
",jenting,"
--
I'd like to revisit this again since at K8s 1.19+, installation CRD `apiextensions.k8s.io/v1beta1` shows the deprecated warning message, use `apiextensions.k8s.io/v1` instead. The `apiextensions.k8s.io/v1beta1` introduced from K8s 1.16+ and `apiextensions.k8s.io/v1beta1` will be dropped at 1.22.

I'd like to take the approach that let user executes `--validate=false` for k8s 1.16- cluster, like https://velero.io/docs/v1.5/customize-installation/#generate-yaml-only.

cc @nrb any comments or feedback?
--
",mapdegree,"
--
Hi @skriss,

I would appreciate, if we could move Velero CRD to the v1 Kubernetes APIs soon. I checked that version 1.16 was available since 8 September 2019, so at least a 1.5 years has been given to the community to migrate over.

In many organisations /companies we focus a lot currently when migrating to 1.20 that we abandon as many beta Kubernetes API calls in the manifests. We got a surprise that Velero is left behind at current version. 

Thanks,
Dejan 
--
",,,,
2504,OPEN,Check for apiextensions.k8s.io/v1beta1 before using a v1beta1 CRD client,versioning,2020-10-06 12:00:39 +0000 UTC,nrb,Opened,,"**Describe the problem/challenge you have**
The upstream [CRD code](https://github.com/kubernetes/apiextensions-apiserver/blob/99685ba23021e88fc9a771f029157b348fd92bab/pkg/apis/apiextensions/v1beta1/types.go#L380) states that v1beta1 is planned for removal in v1.19.

**Describe the solution you'd like**
Velero should check the apiextensions.k8s.io/v1beta1 endpoint before using a client for it. We don't want to invoke a client for an unsupported endpoint.
",,,angelbarrera92,"
--
```
WARN - velero.yml - CustomResourceDefinition/backups.velero.io: API apiextensions.k8s.io/v1beta1 for CustomResourceDefinition is deprecated in 1.19, use apiextensions.k8s.io/v1 instead.
WARN - velero.yml - CustomResourceDefinition/backupstoragelocations.velero.io: API apiextensions.k8s.io/v1beta1 for CustomResourceDefinition is deprecated in 1.19, use apiextensions.k8s.io/v1 instead.
WARN - velero.yml - CustomResourceDefinition/deletebackuprequests.velero.io: API apiextensions.k8s.io/v1beta1 for CustomResourceDefinition is deprecated in 1.19, use apiextensions.k8s.io/v1 instead.
WARN - velero.yml - CustomResourceDefinition/downloadrequests.velero.io: API apiextensions.k8s.io/v1beta1 for CustomResourceDefinition is deprecated in 1.19, use apiextensions.k8s.io/v1 instead.
WARN - velero.yml - CustomResourceDefinition/podvolumebackups.velero.io: API apiextensions.k8s.io/v1beta1 for CustomResourceDefinition is deprecated in 1.19, use apiextensions.k8s.io/v1 instead.
WARN - velero.yml - CustomResourceDefinition/podvolumerestores.velero.io: API apiextensions.k8s.io/v1beta1 for CustomResourceDefinition is deprecated in 1.19, use apiextensions.k8s.io/v1 instead.
WARN - velero.yml - CustomResourceDefinition/resticrepositories.velero.io: API apiextensions.k8s.io/v1beta1 for CustomResourceDefinition is deprecated in 1.19, use apiextensions.k8s.io/v1 instead.
WARN - velero.yml - CustomResourceDefinition/restores.velero.io: API apiextensions.k8s.io/v1beta1 for CustomResourceDefinition is deprecated in 1.19, use apiextensions.k8s.io/v1 instead.
WARN - velero.yml - CustomResourceDefinition/schedules.velero.io: API apiextensions.k8s.io/v1beta1 for CustomResourceDefinition is deprecated in 1.19, use apiextensions.k8s.io/v1 instead.
WARN - velero.yml - CustomResourceDefinition/serverstatusrequests.velero.io: API apiextensions.k8s.io/v1beta1 for CustomResourceDefinition is deprecated in 1.19, use apiextensions.k8s.io/v1 instead.
WARN - velero.yml - CustomResourceDefinition/volumesnapshotlocations.velero.io: API apiextensions.k8s.io/v1beta1 for CustomResourceDefinition is deprecated in 1.19, use apiextensions.k8s.io/v1 instead.
```
🔝 powered by https://github.com/swade1987/deprek8ion

Kubernetes 1.19 deprecates v1beta1 api.
Is there any plan to release a new Velero version migrating from v1beta1 to v1 to support Kubernetes 1.19?
Thanks!
--
",,,,,,,,,,
2499,OPEN,Report metrics for CSI snapshots,Area/CSI; Metrics,2020-10-22 17:44:32 +0000 UTC,nrb,Opened,,"When generating CSI snapshots, we should generate metrics associated with the VolumeSnapshots and VolumeSnapshotContents

This could include:
 
 * number of VolumeSnapshots & VolumeSnapshotContents in a backup
 * total size of on-storage snapshots

This should be in CSI's GA release, but isn't essential for beta support.",,,,,,,,,,,,,,
2488,OPEN,Have the BSL controller continuously update the server `status` based on the state of the BSLs,Area/CLI,2021-02-03 17:43:18 +0000 UTC,carlisia,In progress,,"## BSL controller behavior:
### set server status as `ready`
- all BSLs are `available`, AND
- BSL specified as `default` exists

### set server status as `partially ready`
- multiple BSLs, any one of them is `unavailable`, but at least one other is `available`
- multiple BSLs, all `available`, but no BSL matches the specified `default`

### set server status as `waiting` 
- no BSL, OR
- existing BSLs, all `pending`",,,ashish,"
--
@carlisia did https://github.com/vmware-tanzu/velero/pull/2674 address this?
If it did, can we close this issue?
--
",carlisia,"
--
No, these are different things.
--
",,,,,,,,
2483,OPEN,Allow annotations and/or labels on Backup objects to be added onto the backup object in plugins during a backup,Area/Plugins; Needs Product,2020-10-22 17:39:33 +0000 UTC,jimbo459,Opened,,"**Issue**
We are writing a custom backup plugin where we wish to amend the backup object which is passed in as the second arg. 
When attempting to amend the labels or annotations we are not seeing this change reflected in the final backup object.
To amend the passed in *v1.backup we attempted to use the SetAnnotation() method on the backup.ObjectMeta property. 

```
annotations := make(map[string]string)
annotations[""velero.io/foo""] = ""bar""
backup.ObjectMeta.SetAnnotations(annotations)
```

As this failed, we then attempted to set the field directly by accessing the property and setting the value. This change also did not persist. 

```
annotations := &backup.ObjectMeta.Annotations  
content := make(map[string]string) 
content[""foo""] = ""bar""
*annotations = content
```

**Expectation**
We expected the change to reflect in the final backup object, reachable either by `velero backup describe <backup_name>` or `kubectl get backups.velero.io <backup_name>`

**Replication**
An easy way to replicate this issue is by running the following code snippet in a custom plugin:
```
p.log.Info(""backup Object before instantiation"")
p.log.Info(backup)
backup = new(v1.Backup)                                                                                                                                   
p.log.Info(""backup Object after instantiation"")                                                                                                            
p.log.Info(backup)
```
This instantiates a new backup object when the plugin in called. On the p.log.Info calls you can see this change has reflected, however when the plugin in called on the next item the backup object is the original one, indicating that the change made in the previous call to the plugin has not persisted.

**Environment:**

- Velero version: 1.3.1
- Velero features: <Not Set>
- Kubernetes version:
--Client Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.2"", GitCommit:""52c56ce7a8272c798dbc29846288d7cd9fbae032"", GitTreeState:""clean"", BuildDate:""2020-04-21T01:25:41Z"", GoVersion:""go1.13.10"", Compiler:""gc"", Platform:""linux/amd64""}
--Server Version: version.Info{Major:""1"", Minor:""15"", GitVersion:""v1.15.5"", GitCommit:""20c265fef0741dd71a66480e35bd69f18351daea"", GitTreeState:""clean"", BuildDate:""2019-10-15T19:07:57Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""linux/amd64""}
- Kubernetes installer & version: TKG-i
- Cloud provider or hardware configuration: GCP
- OS: Ubuntu 18.04.4 LTS (Bionic Beaver)""


CC @neil-hickey @aclevername",,,ashish,"
--
@jimbo459 It is definitely possible to add annotations or update the object to be backed un in a backupItemAction plugin.
Here is an example of our `velero-plugin-for-csi` where this is done.
https://github.com/vmware-tanzu/velero-plugin-for-csi/blob/master/internal/backup/pvc_action.go#L147

What you might be missing is the marshaling and un-marshaling steps performed at https://github.com/vmware-tanzu/velero-plugin-for-csi/blob/master/internal/backup/pvc_action.go#L65-L68 and https://github.com/vmware-tanzu/velero-plugin-for-csi/blob/master/internal/backup/pvc_action.go#L163-L166 respectively.

If this doesn't solve your problem, please share a link to your code and we'll be happy to help further.
--

--
@neil-hickey Thanks for the update. I am going to close this issue. Feel free to re-open or engage w/ us on slack.
--

--
Reopening as there are use cases where this may be useful for plugins to add annotations/labels.
--
",neil,"
--
Hey @ashish-amarnath Thanks for the update!

We are trying to modify the `Backup` object of Velero itself (as opposed to the item to be backed-up)

In your examples I think the csi plugin is modifying `item runtime.Unstructured`, where we want to edit `backup *velerov1api.Backup`.  

From slack: https://kubernetes.slack.com/archives/C6VCGP4MT/p1588064584438000 it seems like this isn't a use-case that is expected to work. So we may try out the workaround suggested in the thread and update here if that works 👍 
--
",,,,,,,,
2477,OPEN,Allow selecting namespaces to backup on a NS label instead of the NS names,Enhancement/User,2020-06-19 15:16:43 +0000 UTC,titou10titou10,Opened,,"`velero backup create`allows to backup:
- all resources from namespaces included by their names:`--include-namespaces`
- resources that match a certain label :`--selector` (possibly more restricted by the`--include-namespaces` above)

What we need is to backup everything belonging to namespaces that have specifically a certain label attached, ie select the namespaces to backup based on an label assigned directly to the namespace, instead of their names

Is there a way to do this?

If not, it would be something like adding a new parameter for this:  `--include-namespaces-selector`?
Thx",,,skriss,"
--
@titou10titou10 unfortunately Velero does not currently support this. 

We do already have a feature request for this in the backlog - see https://github.com/vmware-tanzu/velero/issues/503. I'm going to close that one out and leave this one open for tracking. 

Are you interested in working on a contribution here? If so, we can discuss more on what the design for this would look like.
--
",bsctl,"
--
Having a namespace label selector for all the objects in that namespace, would be a great improvement usage of Velero.
--
",,,,,,,,
2474,OPEN,Expand Velero's plugin interfaces to allow information about external systems and resoures used,P3 - Wouldn't it be nice if...,2020-04-29 00:09:52 +0000 UTC,nrb,Opened,,"**Describe the problem/challenge you have**
Currently, Velero plugins may create external resources or call external systems, but the Velero core system has no insight into this. Such information would be useful when trying to, for example, allowing a plugin to clean up resources when running `velero plugin remove`, or letting Velero know whether or not a plugin may have side effects in the event that Velero is trying to do a dry-run operation.


**Describe the solution you'd like**
Velero BackupItemAction and RestoreItemAction could have a function or field describing whether or not they have side effects.

Velero ObjectStorage and VolumeSnapshotter plugins could have a `CleanUp` function that's run on `velero plugin remove` that is invoked and allows them to do things like delete any resources they have created.

**Anything else you would like to add:**
This issue should be considered more of an epic than capturing specific use cases.
",,,,,,,,,,,,,,
3147,OPEN,Backup searches for disk on the wrong GCP project,Area/Cloud/GCP; Help wanted,2021-02-04 13:24:28 +0000 UTC,robersongomes,Opened,,"**Describe the problem/challenge you have:**

We have multiple GCP projects and we manage all the access using [IAM Folders](https://cloud.google.com/resource-manager/docs/access-control-folders), with one folder for each environment (production/development/staging/qa/etc).
For each environment, we create a folder with all GCP projects for that environment. Service accounts are created on a single GCP project and the permissions are set at folder level.

We have clusters on all GCP projects, so the backup fails because it searches for the disk on the GCP project that the credentials belongs to, instead of searching on the GCP project that the cluster resides.

For example, if the service account is created on `project-one' and the backup is running on a cluster on project `project-two`, we get the following error:
> level=error msg=""Error backing up item"" backup=velero/backup-1 error=""error getting volume info: rpc error: code = Unknown desc = googleapi: Error 404: The resource 'projects/**project-one**/zones/europe-west2-a/disks/my-cluster-pvc-b908200f-3d8f-461e-a51a-b8432bf4cca2' was not found, notFound"" group=v1 logSource=""pkg/backup/resource_backupper.go:287"" name=search-0 namespace= resource=pods

That happens because the project id is retrieved from the credentials file:
https://github.com/vmware-tanzu/velero-plugin-for-gcp/blob/v1.0.1/velero-plugin-for-gcp/volume_snapshotter.go#L68

**Describe the solution you'd like:**

I would like to be able to override the project id where the disks are located (or that this information is retrieved from the cluster).

**Environment:**

Velero version: 1.3.2
Velero plugin for GCP version: 1.0.1
Kubernetes version: 1.15
Cloud provider or hardware configuration: GCP/GKE",,,blasrodriguez,"
--
I wrote a PR that tries to solve this issue 
https://github.com/vmware-tanzu/velero-plugin-for-gcp/pull/31

--
",nrb,"
--
Thanks for the PR @blasrodriguez! I'm scheduling this for review this week.
--
",vadasambar,"
--
Any updates on this?
I am trying to restore clusters from our production GCP project to QA GCP project so that we can test upgrades in a pre prod setup. This issue is a blocker for us.
--
",,,,,,
2458,OPEN,Retry failed backups,Enhancement/User,2021-01-22 19:04:25 +0000 UTC,cblecker,Opened,,"**Describe the problem/challenge you have**
There are sometimes transient network issues that can prevent a scheduled backup from completing. It would be nice to have a way to automatically retry a `PartiallyFailed` backup.

**Describe the solution you'd like**
A field in the backup spec to specify the number of times a backup should be retried. A field in the status to describe how many runs have been attempted. If runs < attempts, then move a `PartiallyFailed` backup back into `New` and let the backup reattempt.

**Environment:**
- Velero version (use `velero version`): 1.3.2
",,,dharmab,"
--
We see a _lot_ of flaky backups and this feature would be really helpful:

```
time=""2020-05-05T02:04:22Z"" level=error msg=""backup failed"" controller=backup error=""rpc error: code = Unknown desc = error putting block 00000000: Put https://redactedaccount.blob.core.windows.net/redactedcontianer/backups/hourly-k8s-backup-20200505020014/hourly-k8s-backup-20200505020014.tar.gz?blockid=00000000&comp=block: write tcp 172.16.42.216:38834->52.239.207.100:443: write: connection reset by peer"" key=velero/hourly-k8s-backup-20200505020014 logSource=""pkg/controller/backup_controller.go:265""
```
--
",jewzaam,"
--
A few more examples that worked fine after creating a new Backup:
```
time=""2020-05-11T18:18:23Z"" level=error msg=""Error listing items"" backup=openshift-velero/hourly-object-backup-20200511181749 error=""unexpected error when reading response body. Please retry. Original error: http2: server sent GOAWAY and closed the connection; LastStreamID=487, ErrCode=NO_ERROR, debug=\""\"""" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/backup/resource_backupper.go:229"" error.function=""github.com/vmware-tanzu/velero/pkg/backup.(*defaultResourceBackupper).backupResource"" group=v1 logSource=""pkg/backup/resource_backupper.go:229"" namespace= resource=secrets
```

```
time=""2020-05-11T20:18:40Z"" level=error msg=""Error listing items"" backup=openshift-velero/hourly-object-backup-20200511201749 error=""Get https://10.120.0.1:443/apis/apps/v1/replicasets: unexpected EOF"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/backup/resource_backupper.go:229"" error.function=""github.com/vmware-tanzu/velero/pkg/backup.(*defaultResourceBackupper).backupResource"" group=apps/v1 logSource=""pkg/backup/resource_backupper.go:229"" namespace= resource=replicasets
```
--
",turkenh,"
--
Same here, we are also getting similar issues which are just fixed by retrying.
--
",georgettica,"
--
just to add to the party, another error
`time=""2020-10-07T01:01:38Z"" level=error msg=""Error listing items"" error=""etcdserver: leader changed"" error.file=""/github.com/vmware-tanzu/velero/pkg/backup/item_collector.go:234"" error.function=""github.com/vmware-tanzu/velero/pkg/backup.(*itemCollector).getResourceItems"" group=storage.k8s.io/v1 logSource=""pkg/backup/item_collector.go:234"" namespace= resource=volumeattachments`
--
",KauzClay,"
--
Hello Velero team (@carlisia),

We saw this issue and want to have a go at implementing the request. It appears to be a pretty straightforward request:
* add a `spec.maxRetries` field to the Backups CRD
* add a `status.retryAttempts` field to the Backups CRD
* modify the controller to set a backup's phase to `New` if it is in the state `PartiallyFailed` and increment the `retryAttempts` until it reaches `maxRetries`
* When describing a backup, add fields ""MaxRetries"" and ""RetryAttempts"" to the output, beneath the ""Phase"" line.

Do these sound like reasonable changes? 

Thanks,
Clay and @astrieanna 
--

--
Hello again Velero team,

@astrieanna and I poked around at this some more, and we realized it was not as simple as we thought.

As a small recap, we got it to the point where it would make a second attempt, but then the retry fails on the second attempt because the backup already exists in the object storage.

Here, we realized we just aren't familiar enough with the [backup process](https://github.com/vmware-tanzu/velero/blob/a42284ed176ca1de2b5e8b9f52dac7b1d1785bb4/pkg/controller/backup_controller.go#L206) yet 😅 

With partial failures, is it okay that a backup already exists? Or should we clean everything up and try again? Do you have different kinds of partial failures that would mean different things?

                                                                                                                                                                                                                                                                                                                            
--
",,
2447,OPEN,Document regression testing cases,CI/CD,2021-02-22 22:31:52 +0000 UTC,nrb,In progress,,"We should document regression test cases that will eventually run on top of the e2e clusters made from #2131.

Feel free to add to this list.

I'll start with one:

 * Confirm successful restoration of ElasticSearch CRDs (#2383)",,,nrb,"
--
@ashish-amarnath I know you were interested in this topic, too. Right now I'm just capturing high level cases, not necessarily step-by-step instructions.
--
",ashish,"
--
Confirm: Velero CRDs can be applied in Various versions of Kubernetes.
Decide the list of Kubernetes versions that we want to support
--
",,,,,,,,
2437,OPEN,restic-restore-helper container doesn't run when pod has runAsNonRoot: true,Bug; Help wanted; Restic,2020-06-03 21:02:14 +0000 UTC,JonDGH,Opened,,"**What steps did you take and what happened:**

Annotate pods with: backup.velero.io/backup-volumes: data
velero backup create my-data-backup --include-namespaces=jon-test-system

backup completes successfully

oc delete namespace jon-test-system
velero restore create --from-backup=my-backup --restore-volumes --include-resources=pvcs,pvs

(I don't think the --include-resources argument is required)

Most of the resources are restored correctly, however pods with:

```
  securityContext:
    runAsNonRoot: true
    seLinuxOptions:
      level: s0:c37,c19
```

Fail with:

```
  initContainerStatuses:
  - image: velero/velero-restic-restore-helper:v1.3.2
    imageID: """"
    lastState: {}
    name: restic-wait
    ready: false
    restartCount: 0
    state:
      waiting:
        message: container has runAsNonRoot and image has non-numeric user (nobody),
          cannot verify user is non-root
        reason: CreateContainerConfigError
```

**What did you expect to happen:**

All resources and volumes restored correctly.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

* `kubectl logs deployment/velero -n velero`
* `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
```
apiVersion: velero.io/v1
kind: Backup
metadata:
  creationTimestamp: ""2020-04-15T08:05:06Z""
  generation: 3
  labels:
    velero.io/storage-location: default
  name: my-data-backup
  namespace: jon-velero
  resourceVersion: ""11884425""
  selfLink: /apis/velero.io/v1/namespaces/jon-velero/backups/my-data-backup
  uid: 5a4173fb-909e-486e-ba1b-adf1eb65ba08
spec:
  hooks: {}
  includedNamespaces:
  - jon-test-system
  includedResources:
  - pvs
  - pvcs
  storageLocation: default
  ttl: 720h0m0s
status:
  completionTimestamp: ""2020-04-15T08:06:12Z""
  expiration: ""2020-05-15T08:05:06Z""
  phase: Completed
  startTimestamp: ""2020-04-15T08:05:06Z""
  version: 1
```

* `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
apiVersion: velero.io/v1
```
kind: Restore
metadata:
  creationTimestamp: ""2020-04-15T12:25:48Z""
  generation: 2
  name: my-data-backup-20200415132548
  namespace: jon-velero
  resourceVersion: ""12031491""
  selfLink: /apis/velero.io/v1/namespaces/jon-velero/restores/my-data-backup-20200415132548
  uid: a52a2b83-adb9-499b-8de4-a7cda9b40f26
spec:
  backupName: my-data-backup
  excludedResources:
  - nodes
  - events
  - events.events.k8s.io
  - backups.velero.io
  - restores.velero.io
  - resticrepositories.velero.io
  includedNamespaces:
  - '*'
  restorePVs: true
status:
  phase: InProgress
```


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`):v1.3.2 55a9914a3e4719fb1578529c45430a8c11c28145
- Velero features (use `velero client config get features`): features: <NOT SET>
- Kubernetes version (use `kubectl version`):
$ oc version
Client Version: 4.3.5
Server Version: 4.3.0
Kubernetes Version: v1.16.2
- Cloud provider or hardware configuration: OpenShift
- OS (e.g. from `/etc/os-release`):
	- RuntimeOS: windows	- RuntimeArch: amd64",,,,,,,,,,,,,,
2435,OPEN,velero with restic plugin kills openshift cluster,Performance; Restic,2020-04-16 08:10:57 +0000 UTC,dejwsz,In progress,,"**What steps did you take and what happened:**

I don't know if it  is normal but:

I'm using openshift 3.11 with 80 worker nodes, around 1300 volumes attached to PODs. I used latest Velero with restic to backup these volumes. If restic is used the cluster is almost killed, ETCD is not able to proceed normally. If I kill restic daemon everything starts working well right away.

**What did you expect to happen:**
Restic should not overwhelm a cluster. With large number of volumes and nodes it starts killing a cluster and its performance is lowered a lot.

Valero 1.3.2, Restic 1.3.2. AWS plugin 1.0.1 - data pushed to S3 service based on Ceph.
",,,dejwsz,"
--
Maybe some can comment on this? But it looks like Velero together with Restic daemon is making some kind of DDoS attack on API if you have a larger number of nodes and thousands of PODs with attached volumes. Is that true? It scans all objects and filters by ""backup.velero.io/backup-volumes"" annotation to identify volumes for processing. But in my case, the process is so extensive that Etcd is almost killed and the cluster is almost dying. I do not know how it works with regard to schedule - all Restic daemons are trying to start and process data at the same time? Looks like the whole process is not optimized regarding potential performance impact. Or maybe I am wrong and I'm doing something not quite properly?
--

--
You do not need to apologize, I should thank you for your effort ;) I have one schedule at night which tries to backup whole cluster so all objects and annotated volumes (about 1 thousand). But if only velero main pod is running everything is OK, right after I enable Restic daemon it starts flooding the cluster API so much that Etcd struggles to work normally.
--

--
I would suggest to check the whole process/workflow of using openshift/k8s API and do it less aggressive trying to minimize all requests to a required minimum (if possible?).
--
",skriss,"
--
@dejwsz sorry you're having problems. We have not scale-tested the restic integration at this size. Are you trying to back up *all* of these volumes within a single backup?
--
",,,,,,,,
2432,OPEN,Add `--dry-run` flag to the new install and config commands,Area/CLI; Enhancement/User,2021-02-03 17:42:22 +0000 UTC,carlisia,Opened,,,,,,,,,,,,,,,,
2428,OPEN,Feature request: webhooks/notifications,Enhancement/User,2020-11-10 16:14:01 +0000 UTC,lukasmrtvy,Opened,,"Would be supernice nice to implement custom notifications/webhooks if backup failed/succeded.
* Slack,MSTeams,Matrix,Mattermost 
* Custom webhook ( To support https://github.com/healthchecks/healthchecks for exampel .. )

Thanks

Related:
https://github.com/vitobotta/velero-backup-notification
https://github.com/vmware-tanzu/velero/issues/172",,,sharkymcdongles,"
--
I agree. I explored doing this via the plugins system, but it seems what is needed isn't  exposed there and any changes will need to be within Velero itself sadly.
--
",,,,,,,,,,
2427,OPEN,Improve the `velero plugin` command,Area/CLI; Enhancement/User; Help wanted,2021-02-03 17:41:04 +0000 UTC,carlisia,Opened,,"Add this flag:

```
set
        --sa-annotations mapStringString        annotations to add to the Velero ServiceAccount for GKE. Add iam.gke.io/gcp-service-account=[GSA_NAME]@[PROJECT_NAME].iam.gserviceaccount.com for workload identity. Optional. Format is key1=value1,key2=value2
```

Note: it currently exists under `velero install`.",,,jenting,"
--
> Add this flag:
> 
> ```
> set
>         --sa-annotations mapStringString        annotations to add to the Velero ServiceAccount for GKE. Add iam.gke.io/gcp-service-account=[GSA_NAME]@[PROJECT_NAME].iam.gserviceaccount.com for workload identity. Optional. Format is key1=value1,key2=value2
> ```

From my understanding, it's more appropriate to add this flag under `velero config server` because the purpose is to configure the Velero server service account.
--
",,,,,,,,,,
2426,OPEN,Improve the `velero snapshot-location` command,Area/CLI; Enhancement/User,2021-02-22 20:01:58 +0000 UTC,carlisia,Opened,,"Add these flags:

```
set
        --default mapStringString         sets the list of unique volume providers and default volume snapshot location (provider1:location-01,provider2:location-02,...) (NEW, -- was `server --default-volume-snapshot-locations; could be set as an annotation on the VSL)  
        --credentials mapStringString     sets the list of name of the corresponding credentials secret for providers. Format is (provider1:credentials-secret-name1,provider2:credentials-secret-name2,...) (NEW)

create                                    NAME [flags]
        --default                         Sets these new locations to be the new default snapshot locations. Default is false. (NEW)
        --credentials mapStringString     sets the list of name of the corresponding credentials secret for providers. Format is (provider1:credentials-secret-name1,provider2:credentials-secret-name2,...) (NEW)
```",,,,,,,,,,,,,,
2424,OPEN,Add `velero config restic` command,Area/CLI; Enhancement/User,2021-02-03 17:42:03 +0000 UTC,carlisia,Opened,,See https://github.com/vmware-tanzu/velero/blob/master/design/cli-install-changes.md for what `velero config restic` will do.,,,,,,,,,,,,,,
2423,OPEN,Add `velero config server` command,Area/CLI; Enhancement/User,2021-02-03 17:41:50 +0000 UTC,carlisia,Opened,,See https://github.com/vmware-tanzu/velero/blob/master/design/cli-install-changes.md for what `velero config server` will do,,,tharun208,"
--
@carlisia I like to work on this
--
",,,,,,,,,,
2420,OPEN,"Rename ""provider"" to ""location-plugin""",Area/CLI; Enhancement/User,2021-02-03 17:43:06 +0000 UTC,carlisia,Opened,,"
- [ ] change BackupStorageLocation/VolumeSnapshotLocation `Spec.Provider` field to `Spec.Location-Plugin` 

- [ ] change all related CLI command flags to location-plugin

Specs: https://github.com/vmware-tanzu/velero/blob/master/design/cli-install-changes.md#renaming-provider-to-location-plugin",,,,,,,,,,,,,,
2419,OPEN,New CLI for install and config,Area/CLI; Enhancement/User; Epic; P2 - Long-term important,2021-02-03 17:41:12 +0000 UTC,carlisia,Opened,,"Specs: https://github.com/vmware-tanzu/velero/blob/master/design/cli-install-changes.md

v1.5
- [x] 2Add a column to indicate the BSL status on `velero backup-location get` - #2489 
- [x] Go over all flags and document what is optional, what is required, and default values. Capitalize all help messages - #2422
- [ ] Improve the `velero backup-location` command - #2425
- [ ] Improve the `velero snapshot-location` command - #2426

v1.6
- [ ] Rename ""provider"" to ""location-plugin"" - #2420
- [ ] Add columns to BSL/VSL to indicate which one is the default - #2421
- [ ] Add `velero config server` command - #2423
- [ ] Add `velero config restic` command - #2424
- [ ] Improve the `velero plugin` command - #2427
- [ ] Add `--dry-run` flag to the new install and config commands - #2432
- [ ] Have the BSL controller continuously update the server `status` based on the state of the BSLs - #2488
- [ ] Add a `velero status` command to report if the server is ready to create backup or not - #979

",,,,,,,,,,,,,,
2416,OPEN,Handling metadata.ManagedFields,Bug; Kubernetes Resources,2021-02-03 17:43:42 +0000 UTC,nrb,In progress,,"https://github.com/kubernetes/kubernetes/pull/72947 introduced ManagedFields into Kubernetes v1.18.

We need to determine whether or not Velero needs to do anything special with these at restore time, such as clear them off the object with the `resetMetadataAndStatus` function, or if they're already handled.",,,nrb,"
--
This appears to be part of server-side apply per https://kubernetes.io/blog/2020/04/01/kubernetes-1.18-feature-server-side-apply-beta-2/

Per https://kubernetes.io/docs/reference/using-api/server-side-apply/#field-management, Velero should not remove these fields and we should update `resetMetadataAndStatus` to reflect that. One caveat is that we may not be able to restore objects with these fields on pre-Kuberetes 1.18 clusters, but I think that may be minor as we don't technically support downgrades anyway.

Additionally, this metadata may be useful in making decisions about what to restore as opposed to using the `last-applied-configuration` annotation, which is subject to change.
--
",,,,,,,,,,
2413,OPEN,Namespace with label velero.io/exclude-from-backup=true should exclude all resources within from backing up,Enhancement/User,2020-04-15 17:03:47 +0000 UTC,iblackman,In progress,,"**Describe the problem/challenge you have**
I want to have an option to exclude a namespace (all resources) without needing to edit velero config every time I create a new namespace. The actual option of `--exclude-namespaces` isn't enough because I don't want to create a naming pattern because of velero and since it already accept the label `velero.io/exclude-from-backup=true` to exclude resources dynamically why not treat everything from a namespace with that label as if all have that label? It just doesn't make sense for me to only skip the resource namespace backup but backup all the resources within it.


**Describe the solution you'd like**
A namespace with label `velero.io/exclude-from-backup=true` should be enough to exclude all resources in that namespace from velero backup without the need to add that label to all of them.

**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`):
```
Client:
	Version: v1.3.1
	Git commit: -
Server:
	Version: v1.3.1
```

- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""15"", GitVersion:""v1.15.5"", GitCommit:""20c265fef0741dd71a66480e35bd69f18351daea"", GitTreeState:""clean"", BuildDate:""2019-10-15T19:16:51Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""15"", GitVersion:""v1.15.5"", GitCommit:""20c265fef0741dd71a66480e35bd69f18351daea"", GitTreeState:""clean"", BuildDate:""2019-10-15T19:07:57Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Kubernetes installer & version:
```
kubeadm version: &version.Info{Major:""1"", Minor:""15"", GitVersion:""v1.15.5"", GitCommit:""20c265fef0741dd71a66480e35bd69f18351daea"", GitTreeState:""clean"", BuildDate:""2019-10-15T19:14:19Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):
```
NAME=""Ubuntu""
VERSION=""18.04.4 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.4 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
```
",,,skriss,"
--
@iblackman I think that could be reasonable behavior for when the `velero.io/exclude-from-backup=true` is applied to a `Namespace` resource. Right now, it results in just the `Namespace` resource itself being excluded, but any other resources in that namespace would still be backed up.

Let's see what other maintainers think (cc @nrb @carlisia @ashish-amarnath).

Do you have any interest in working on a PR for this, if we decide it makes sense to implement?
--
",nrb,"
--
👍 I think this is reasonable, too, since annotating everything in a namespace could be pretty tedious.
--
",iblackman,"
--
@skriss 
> Do you have any interest in working on a PR for this, if we decide it makes sense to implement?

I can try to if you decide it makes sense, even though I don't have much experience with Go.
--
",ashish,"
--
👍 from me. Excluding namespace resources when the namespace is excluded is very intuitive.
--
",,,,
3142,OPEN,AWS Role to be assumed not being read from configuration file.,Area/Cloud/AWS; Help wanted,2020-12-07 23:36:11 +0000 UTC,jwalters-gpsw,Opened,,"### Versions
Version: v1.3.2
AWS Plugin Version: 1.0.1
AWS EKS Version: 1.15

### Description
Though my config file has role_arn and source_profile entries, it does not appear the role is being assumed.

### Details
`$ velero install --provider aws --plugins velero/velero-plugin-for-aws:v1.0.1 --bucket backup-velero  --secret-file ./config`

```
time=""2020-04-07T23:01:43Z"" level=info msg=""Checking that all backup storage locations are valid"" logSource=""pkg/cmd/server/server.go:413""
An error occurred: some backup storage locations are invalid: backup store for location ""default"" is invalid: rpc error: code = Unknown desc = AccessDenied: Access Denied
```
`./config`
```
[default]
region = us-west-2
role_arn=arn:aws:iam::99999999999:role/IAM_CARole_XXX
source_profile=eks

[profile eks]
aws_access_key_id = AKIAXXXXXXXX
aws_secret_access_key = mfrXXXXXXXXXXXXXXXX
```
### Additional Information
I ran up another pod in the same cluster using the `mesosphere/aws-cli:latest` docker image. Then `exec`ed into the image and copied the same config (pointed to with `AWS_CONFIG_FILE` environment variable ). All the `aws s3` commands worked fine on the bucket.

I suspect the `stscreds.NewCredentials` call needs to be used as described in **Assume Role** section [here](stscreds.NewCredentials).
",,,jwalters,"
--
I'm guessing something like what is being done in [aws-okta ](https://github.com/segmentio/aws-okta)needs to be done.
https://github.com/segmentio/aws-okta/blob/2840c02437b9069bede10682de2298cef7aa30f1/lib/provider.go#L303

I'll see if I can get a pull request together.
--
",carlisia,"
--
Hey @jwalters-gpsw, would you still like to take a stab at this?
--
",,,,,,,,
2405,OPEN,Structure and maintenance of Velero YAML files,Enhancement/User,2020-04-07 21:42:46 +0000 UTC,carlisia,In progress,,"Reference: https://github.com/vmware-tanzu/velero/pull/2202#discussion_r404202795

In consideration to reverting to maintaining YAML files containing resources to be deployed for Velero, and taking into account the CLI changes to install Velero (https://github.com/vmware-tanzu/velero/pull/2202), some issues to think about and resolve:

- Any opportunity to automate the generation of these files? As @ashish-amarnath suggested, perhaps at release time?
- Should we also maintain the files for plugins not supported by the Velero team?
- Can we generate files structured to be consumed by kustomize? For example, a smaller subset of deployment representing a plugin, to be used as a patch? What tooling could we use, `diff`?",,,carlisia,"
--
c/c @vmware-tanzu/velero-owners.
--
",,,,,,,,,,
2402,OPEN,Velero location plugins and env variables,Area/Plugins; Enhancement/Dev; Enhancement/User,2020-04-07 00:12:46 +0000 UTC,carlisia,Opened,,How can we modify Velero so it is aware of env vars that different provider plugins require? Right now we just hardcode the AWS/Azure/GCP ones.,,,,,,,,,,,,,,
2397,OPEN,Code cleanup,Good first issue; kind/tech-debt,2020-08-19 16:49:14 +0000 UTC,carlisia,In progress,,"Running a tool like `gosec` finds a bunch of unhandled error and other red flags in the code. We should run it and clean up as much as possible on this and the other Velero related repos.

Note: `gosec` is bundled with `golangci-lint` and you can run it as a plugin from there.

----
Related issues:

https://github.com/vmware-tanzu/velero/issues/2841",,,dbgoytia,"
--
Hey @carlisia ! I'm new here and would like to further #investigate this problem! 
--

--
Thanks a lot!! I'll start working on it :D and go back for any issues or guidance.
--

--
@RobbieJVMW Of course!!! Let's tackle this one. There are quite a while messages so we can get this together if you want.
Run the tool and let's pick some :) 
--
",carlisia,"
--
Awesome! I assigned this to you. Please let me know if I can guide you with anything.
--

--
Hey @ironknight78 since you were the first to volunteer, why don't you make a list of packages you'll address, and run that by @RobbieJVMW to see if he can do the remainder? We can create a second ticket for @RobbieJVMW.
--

--
> I was considering doing a 'removed dead code' push as the first pass against all the packages. Any thoughts on that approach ?

Yes, let's get this rolling. Please open a ticket for this one and I'll assign it to you. Thank you!
--
",RobbieJVMW,"
--
@carlisia @ironknight78 if you need another set of fingers on the keyboard let me know.  
--

--
Sounds good to me, I've got a test rig setup and cloned the repo.  Currently just looking over all the lines that are flagged with deadcode.   I was considering doing a 'removed dead code' push as the first pass against all the packages.   Any thoughts on that approach ?

--
",,,,,,
2395,OPEN,Restic is taking backup from evicted pod.,Bug; Restic; Restic - GA,2021-03-03 08:46:38 +0000 UTC,amarshaw,In progress,,"Running schedule backup and it's getting PartiallyFailed.

```
TION   SELECTOR
daily-k8s-backup-120d-20200403020007   PartiallyFailed   2020-04-02 22:00:07 -0400 EDT   119d      default            !openebs.io/controller,!openebs.io/replica
daily-k8s-backup-120d-20200402020037   PartiallyFailed   2020-04-01 22:00:37 -0400 EDT   118d      default            !openebs.io/controller,!openebs.io/replica
```

By troubleshooting i found that restic is trying to take backup from Evicted pods.

**Failed Backup for namespace :**

```
    k8s-xxx-qa/xxx-85dbcc67b4-pdj6x: storage-0
    k8s-xxx/xxx-6859ffc4-mhrjl: storage-0
    k8s-qqq-qa/qqq-qa-6db8d5c876-5rfmd: storage-0
    k8s-qqq/qqq-5bc89bdfcc-qrjf6: storage-0
    k8s-www-qa/www-qa-65f5966c97-2jnhn: storage-0
    k8s-watcher/k8s-watcher-nfs-665cd5dbd9-rv4v4: storage-0
  Failed:
    k8s-ccc/ccc-64fcc6bccd-248zb: storage-0
```

**Statue of k8s-ccc namespace and pods ccc-64fcc6bccd-248zb.**

```
kubectl -n k8s-peoplefunction get pods
NAME                               READY   STATUS    RESTARTS   AGE
ccc-64fcc6bccd-248zb   0/1     Evicted   0          12h
ccc-64fcc6bccd-5lflt   1/1     Running   0          5d19h
ccc-64fcc6bccd-5sgc6   1/1     Running   0          12h

```

**Backup Error Log.**

```
velero backup  logs daily-k8s-backup-120d-20200403020007 | grep -i error
time=""2020-04-03T02:13:40Z"" level=info msg=""1 errors encountered backup up item"" backup=velero/daily-k8s-backup-120d-20200403020007 group=v1 logSource=""pkg/backup/resource_backupper.go:283"" name=ccc-64fcc6bccd-248zb namespace= resource=pods
time=""2020-04-03T02:13:40Z"" level=error msg=""Error backing up item"" backup=velero/daily-k8s-backup-120d-20200403020007 error=""pod volume backup failed: error getting volume path on host: expected one matching path, got 0""error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/restic/backupper.go:182"" error.function=""github.com/vmware-tanzu/velero/pkg/restic.(*backupper).BackupPodVolumes"" group=v1 logSource=""pkg/backup/resource_backupper.go:287"" name=ccc-64fcc6bccd-248zb namespace= resource=pods
```

Velero Version : 1.3.1
storage-0 is openebs NFS mount.

```
kubectl -n velero get deployment velero -o yaml | grep image
        image: artifacts.aaa.com/deployer-tools/velero:v1.3.1
        imagePullPolicy: IfNotPresent
      - image: openebs/velero-plugin:1.8.0-velero_1.0.0
        imagePullPolicy: IfNotPresent
      - image: velero/velero-plugin-for-aws:v1.0.0
        imagePullPolicy: IfNotPresent
```

**Velero Namespace Status :**

```
kubectl -n velero get pods
NAME                      READY   STATUS      RESTARTS   AGE
minio-setup-nfvh9         0/1     Completed   0          20h
restic-5czcv              1/1     Running     0          20h
restic-5pkqn              1/1     Running     0          20h
restic-6knq7              1/1     Running     0          20h
restic-7g57s              1/1     Running     0          20h
restic-8446z              1/1     Running     0          20h
restic-9lrrj              1/1     Running     0          20h
restic-9mcbv              1/1     Running     0          20h
restic-ddg5v              1/1     Running     0          20h
restic-f4bj8              1/1     Running     0          20h
restic-fh6q8              1/1     Running     0          20h
restic-ft4nx              1/1     Running     0          20h
restic-ftl9p              1/1     Running     0          20h
restic-g96g2              1/1     Running     0          20h
restic-hl9b5              1/1     Running     0          20h
restic-m4pwl              1/1     Running     0          20h
restic-ngmgc              1/1     Running     0          20h
restic-nhd98              1/1     Running     0          20h
restic-rn8hw              1/1     Running     0          20h
restic-rpnxd              1/1     Running     0          20h
restic-s7w79              1/1     Running     0          20h
restic-tcvhf              1/1     Running     0          20h
restic-tnvgz              1/1     Running     0          20h
restic-ttkfp              1/1     Running     0          20h
restic-tvz54              1/1     Running     0          20h
velero-6b696d6fb8-8qf9z   1/1     Running     0          20h 
```

**Backup Schedule**

```
kubectl -n velero get schedule daily-k8s-backup-120d  -o yaml
apiVersion: velero.io/v1
kind: Schedule
metadata:
  creationTimestamp: ""2020-04-02T11:04:25Z""
  generation: 4
  name: daily-k8s-backup-120d
  namespace: velero
  resourceVersion: ""332594823""
  selfLink: /apis/velero.io/v1/namespaces/velero/schedules/daily-k8s-backup-120d
  uid: b7205264-74d1-11ea-bf21-0050569a4d16
spec:
  schedule: 00 02 * * *
  template:
    excludedNamespaces:
    - openebs
    includedNamespaces:
    - '*'
    labelSelector:
      matchExpressions:
      - key: openebs.io/controller
        operator: DoesNotExist
      - key: openebs.io/replica
        operator: DoesNotExist
    snapshotVolumes: true
    ttl: 2880h0m0s
status:
  lastBackup: ""2020-04-03T02:00:07Z""
  phase: Enabled
```",,,nrb,"
--
Thanks for reporting this @amarshaw, I've added it to our backlog.
--
",amarshaw,"
--
welcome @nrb 
--
",ashish,"
--
The phase of the pod is something that is external to Velero. Trying to think what would be an expected behavior when this happens? Putting out some options:
1. Check Pod status before creating the `PodVolumeBackup` objects [here](https://github.com/vmware-tanzu/velero/blob/master/pkg/restic/backupper.go#L160)
2. Check pod status in the pod_volume_backup controller [here](https://github.com/vmware-tanzu/velero/blob/master/pkg/controller/pod_volume_backup_controller.go#L193)

I think option 2 seems more right and is more just-in-time fashion. 
In either case, we would want to return an error indicating that it was caused by a pod not being in the `Running` phase.

--
",fredgate,"
--
Velero should ignore evicted pods since it will not be able to perform backup of their volumes. In addition, an evicted pod is generally replaced by an active pod, so backups should be performed only from `Running` pods.
For schedule backups we can not define which pod will be evicted. 
I think this is also the case for completed pods of jobs.
--
",,,,
2386,OPEN,enable plugins to pass environment variables required by restic,Area/Plugins; Enhancement/Dev; Enhancement/User; Restic,2020-04-02 15:41:30 +0000 UTC,skriss,Opened,,"**Describe the problem/challenge you have**
[A description of the current limitation/problem/challenge that you are experiencing.]

Restic requires specific environment variables to be set for certain object storage providers in order to run properly. Currently, there's no way for ObjectStore plugins to pass these to Velero, so Velero core includes code to properly set the env vars across our major supported ObjectStore providers. This is a problem because we've moved the rest of the provider-specific code out of core.

AWS: https://github.com/vmware-tanzu/velero/blob/master/pkg/restic/common.go#L284-L305
Azure: https://github.com/vmware-tanzu/velero/blob/master/pkg/restic/common.go#L261-L282

**Describe the solution you'd like**
[A clear and concise description of what you want to happen.]

ObjectStore plugins should be able to pass restic-related environment variables to the main Velero process, so that logic can be encapsulated within plugins rather than in core.
",,,,,,,,,,,,,,
2363,OPEN,Feature enhancement request: Pause a Schedule,Enhancement/User; P3 - Wouldn't it be nice if...,2020-03-25 13:53:49 +0000 UTC,oneoneonepig,Opened,,"**Describe the problem/challenge you have**
Can pause a schedule so that it won't create backups, while not actually deleting the schedule object.
Something like `.spec.suspend` in Kubernetes CronJobs

**Describe the solution you'd like**
`velero schedule pause NAME`
`velero schedule unpause/resume NAME`

**Environment:**

- Velero version:
```
Client:
        Version: v1.3.1
        Git commit: 0665b05321eefeb7b7fdd6984750745b7429774f
Server:
        Version: v1.3.1
```

- Kubernetes version:
```
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.4"", GitCommit:""8d8aa39598534325ad77120c120a22b3a990b5ea"", GitTreeState:""clean"", BuildDate:""2020-03-12T21:03:42Z"", GoVersion:""go1.13.8"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.4"", GitCommit:""8d8aa39598534325ad77120c120a22b3a990b5ea"", GitTreeState:""clean"", BuildDate:""2020-03-12T20:55:23Z"", GoVersion:""go1.13.8"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Kubernetes installer & version:
`kubeadm v1.17.4 with HA setup`

- Cloud provider or hardware configuration:
`Ubuntu VM on Hyper-V on-prem`

- OS:
`Ubuntu 18.04.4 LTS (Bionic Beaver)`

",,,,,,,,,,,,,,
2361,OPEN,Cassandra backups and missing features,Enhancement/User; Restic,2020-04-23 18:03:02 +0000 UTC,cscetbon,Opened,,"**Describe the problem/challenge you have**
A few features are missing in order to backup objects in a Cassandra database without recreating a new cluster. Same in order to have the ability to use different retentions for specific objects. That's what this issue is about.

**Describe the solution you'd like**
- In order to have different retention per object we would need a way to specify includes/excludes rules for restic backups. The question is where to put this information. Adding this information as annotations on pods would mean they would be global, except if the annotation has a specific format by being a json string for instance, like 
```
{
 ""k1"": {""include"": ""data/k1/*/snapshots/""},
 ""k2/t1"": {""include"": ""data/k2/t1/snapshots/""} 
...
}
```
We would then need a way to say which restic rule we want to apply for a specific backup.

- Hooks should have access to those params in order for instance to know what to snapshot for a Cassandra backup. We could also want to keep some snapshots local and use them to restore from instead of restoring from s3 using restic. That could speed up restore after a an operation that corrupted data and decrease the [MTTR](https://en.wikipedia.org/wiki/Availability_(system)#MTTR).

- Restoring is done by creating a new cluster, however with Cassandra there is the ability to restore files in place and ask Cassandra to refresh its internal state to see the new content of the files. We would need the ability to restore files in place but also to have hooks on restore to trigger the refresh.

I'm probably missing other requirements but we can flesh them out.",,,,,,,,,,,,,,
2358,OPEN,Constant; watch based backup,Enhancement/User,2020-03-25 20:40:55 +0000 UTC,negz,Opened,,"**Describe the problem/challenge you have**

At https://upbound.io/ we're considering using Velero to backup Kubernetes API servers that contain customer state. One concern we have with doing this today is that (as best we can tell) Velero supports only on-demand or scheduled backups. This presumably means we'd have to tolerate a window in time in which our backups were stale; for example if we backed up the API server hourly.


**Describe the solution you'd like**

Ideally it would be possible to backup a subset of cluster resources on-demand. Perhaps we could describe the resources we wanted constantly backed up (by some combination of GVK and namespace?) and a controller or webhook would ensure a backup was created each time a CR was mutated, either by a human or another controller.

I think this is very similar to https://github.com/vmware-tanzu/velero/issues/2111, except that seems to pertain to [`kind: Event`](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#event-v1-core) rather than watches. I notice that issue suggested building a separate controller to add this functionality; we may be interested in contributing or collaborating on such a controller if others would find it useful.",,,,,,,,,,,,,,
2351,OPEN,Test and document behavior when backing up applications deployed by Helm,Area/Documentation,2021-03-09 03:55:48 +0000 UTC,nrb,In progress,,"As a maintainer, I know Velero does not fully support restoring Helm 2-managed applications such that Helm/Tiller can track and account for them, but I don't have those specifics. This should be tested and any issues documented. Given Helm 2's deprecation status, I don't know that this will be fixed, but if we document issues it would also make it easier for someone to develop a plugin to address them, given the huge catalog that already exists based on Helm 2.

Additionally, we should test applications (preferably the same ones) with Helm 3 to see what the differences are when the application is restored. Given Helm 3's design removes Tiller, it could be that there are fewer issues with restoring these applications since there's no in-cluster objects to re-associate.",,,nrb,"
--
Current findings:

On a Helm 2 installation, the ConfigMaps that specify a release appear to continue working if Tiller is restored or deployed before the ConfigMaps are.

If the ConfigMaps are restore before the Tiller server is running, Tiller does not detect them, and the releases are orphaned.

It is possible to restore Tiller first out of a full cluster backup by restoring the `kube-system` namespace before any others, but that assumes that a) Tiller is in the `kube-system` namespace and b) no releases are being managed in that namespace.

Again, I'm not really inclined to chase down fixing this due to Helm 2 development being sunset, though I recognize that it is still widely deployed.

I think ensuring that this works would require implementation of a fix for #1350, since we'd have to be sure that the Deployment (ultimately, pod) for Tiller is up and running before we restore any release ConfigMaps.
--
",jenting,"
--
Start from velero helm-chart 2.15.0, Velero no longer supports Helm v2 since it's deprecated in November 2020.
--
",,,,,,,,
2331,OPEN,Optionally drive all object storage access through the velero server,Enhancement/User,2020-03-06 17:40:16 +0000 UTC,TJM,In progress,,"**Describe the problem/challenge you have**

Our object storage (PureStorage Flashblade) is not accessible to end users (clients), only the servers. Unfortunately, the velero client appears to connect directly to the object storage for at least sub-commands like ""logs"" and the ""--details"" from describe, and likely others.

**Describe the solution you'd like**

I would like the velero client to request the velero server process to interact with the object storage and relay the information to the client (proxy?). I am sure that there would be some case where this would need to be optional. I honestly think that the client should never have credentials to access anything in object storage. 

**Anything else you would like to add:**

We can workaround this by ssh-ing to a host in the datacenter and running the velero client, or running it from a pod.

**Environment:**

- Velero version (use `velero version`): `v1.2.0`
- Kubernetes version (use `kubectl version`): `v1.16.3`
- Kubernetes installer & version: (vanilla)
- Cloud provider or hardware configuration: Internal Infrastructure, VMware and Flashblade
- OS (e.g. from `/etc/os-release`): Client: OSX 10.14; Server: CentOS 7.x
",,,bgagnon,"
--
> I honestly think that the client should never have credentials to access anything in object storage.

I am pretty sure the velero client uses temporary URLs signed by the velero server in cases where it needs to hand off direct access to objects in storage. The client is never given the credentials.

Nevertheless, I think it could be possible that the client has no network connectivity to the object storage while having connectivity to the velero server and Kubernetes API. This would still require a full proxying of object storage calls.
--
",skriss,"
--
> I am pretty sure the velero client uses temporary URLs signed by the velero server in cases where it needs to hand off direct access to objects in storage. The client is never given the credentials.

That's correct.

Regardless, I agree with the sentiment of this enhancement request and would like to find a way to do this as well - using pre-signed URLs has a bunch of limitations.
--
",TJM,"
--
With the ""security"" implications aside (as I think thats something that should be handled outside of github issues), I think it would be fairly common to have a NAS device that is not externally accessible. I know I could create a load balancer VIP and point it towards the NAS, and set the ""public"" URL to that, but I really don't like the idea. We do have a VIP for the K8s API Server, which is apparently providing port forwarding to access the velero backend? (I think)
--
",nrb,"
--
IMO, if we pursued this, I would prefer to make it the only way for the client to access the object store - maintaining the two code paths for an extended period of time would be onerous.
--
",,,,
2308,OPEN,Restoring nodePort service that has nodePort preservation always fails if service already exists in the namespace,Enhancement/User,2020-11-29 14:03:17 +0000 UTC,betta1,In progress,,"**What steps did you take and what happened:**
During restore Velero skips restoring a service if the service already exists in the namespace. However for nodePort services that require node ports to be preserved on restore, restores always fail if the same nodePort service already exists in the target namespace. It seems during the restore Velero tries to create the service on the target namespace but the API server rejects the operation since the node port to be restored is already allocated to the preexisting service. Below steps should help in reproducing this issue:

```
$ kubectl create ns nginx

$ cat <<EOF | kubectl apply -n nginx -f -
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
    - port: 80
      nodePort: 32555
  selector:
    app: nginx
  type: NodePort
EOF

$ kubectl create deployment nginx --image=nginx -n nginx
$ velero backup create nginx-backup --include-namespaces nginx
$ kubectl delete deployment nginx -n nginx
$ velero restore create --from-backup nginx-backup

```
Restores fail consistently with the error ""**provided port is already allocated**"":

{""errors"":{""namespaces"":{""**nginx"":[""error restoring services/nginx/nginx: Service \""nginx\"" is invalid: spec.ports[0].nodePort: Invalid value: 32555: provided port is already allocated""**]}},""warnings"":{""namespaces"":{""nginx"":[""could not restore, endpoints \""nginx\"" already exists. Warning: the in-cluster version is different than the backed-up version.""]}}}

**What did you expect to happen:**
Velero to skip restore of the nodePort service if the service already exists in the target namespace. This maintains restore behavior consistent with how Velero handles restore of other objects it finds in the namespace.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

* `kubectl logs deployment/velero -n velero`
* `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
* `velero backup logs <backupname>`
* `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
* `velero restore logs <restorename>`


**Anything else you would like to add:**
This issue is closely related to #1950 .


**Environment:**

- Velero version (use `velero version`): v1.2.0
- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):
",,,nrb,"
--
I think the issue is at [this line](https://github.com/vmware-tanzu/velero/blob/master/pkg/restore/restore.go#L1197-L1198). We expect the API server to say that the object already exists if it looks the same, but in the case of NodePorts that are re-using the same port, it gives us a different error - that it's invalid.

I confirmed this by printing out the error, and this is what I saw:

```go
restoreErr was: &errors.StatusError{ErrStatus:v1.Status{TypeMeta:v1.TypeMeta{Kind:""Status"", APIVersion:""v1""}, ListMeta:v1.ListMeta{SelfLink:"""", ResourceVersion:"""", Continue:"""", RemainingItemCount:(*int64)(nil)}, Status:""Failure"", Message:""Service \""nginx\"" is invalid: spec.ports[0].nodePort: Invalid value: 32555: provided port is already allocated"", Reason:""Invalid"", Details:(*v1.StatusDetails)(0xc0007c1680), Code:422}}time=""2020-03-04T20:50:34-05:00"" level=info msg=""error restoring nginx: Service \""nginx\"" is invalid: spec.ports[0].nodePort: Invalid value: 32555: provided port is already allocated"" logSource=""pkg/restore/restore.go:1200"" restore=velero/nginx-backup-20200304205033
```

Digging into the `StatusDetails` a little more with the following patch:

```go
		if apierrors.IsInvalid(restoreErr) {
			statusErr := restoreErr.(*apierrors.StatusError)
			fmt.Printf(""Err Reason: %#v: "", statusErr.Status().Reason)
			fmt.Printf(""Err Details:   %#v"", statusErr.Status().Details)
		}
```

```
Err Reason: ""Invalid"": Err Details:   &v1.StatusDetails{Name:""nginx"", Group:"""", Kind:""Service"", UID:"""", Causes:[]v1.StatusCause{v1.StatusCause{Type:""FieldValueInvalid"", Message:""Invalid value: 32555: provided port is already allocated"", Field:""spec.ports[0].nodePort""}}, RetryAfterSeconds:0}time=""2020-03-04T20:57:21-05:00"" level=info msg=""error restoring nginx: Service \""nginx\"" is invalid: spec.ports[0].nodePort: Invalid value: 32555: provided port is already allocated"" logSource=""pkg/restore/restore.go:1204"" restore=velero/nginx-backup-20200304205719
```

So, to fix this for nodePort services, I think we could do the following, which I have not yet tested.

1. After checking for `apierrors.AlreadyExists`, check for `apierrors.IsInvalid`
1. If it is invalid, check the `restoreError.Status().Details.Causes` slice for `nodePort` in `Field` and `already allocated` in `Message`.
1. If all `Causes` match this case, then the `Service` is the same and log it as such instead of erroring.

I'm not 100% happy with this as it relies on a text string in the error message that could change, but I haven't yet checked on the API server side to see what exactly it generates, so maybe there's a better solution.

More generally, chasing down each case like Service Accounts and now Services in the core restore code doesn't seem sustainable. I'm wondering if some sort of `RestoreConflict` plugin type might make sense, which would allow us to specify criteria by which instances of a given type would be considered the same. That's a very rough idea at the moment and would require some more design work.

@skriss and @ashish-amarnath does my rough algorithm seem workable to you? I'll try to track down what the API server does to return the invalid error to see if there's something more reliable than string matching that we can do.

@ajohnstone and @jenting I'm tagging you too as you both commented on the other issue, and I'm able to reproduce, with detailed error structs.
--

--
Ok, so the error seems to originate [in the port allocator](https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/core/service/portallocator/allocator.go#L128), which is pretty core to Kubernetes. The error is passed directly through the REST API if the port is already allocated, which is why this doesn't show up as `AlreadyExists`.

Looking [around here](https://github.com/kubernetes/kubernetes/blob/master/pkg/registry/core/rest/storage_core.go#L243), I see it uses a `RangeRegistry`, which is similar to how IP address fields are exposed. My guess is that objects that have IPs for allocation will get similar errors when retained, although those have been more dynamic in my experience.

NodePorts have been a far more recurring problem for users of Velero.
--

--
> How about trying a GET only on the services iff we find the last-applied-config annotation on the service during restore?

That works for Services applied by Kubectl, but I think it still has the problem of adding special cases as they come up.

Reworking the entire logic to be based on a `Get` vs doing a `Create` and expecting an `AlreadyExists` error may be more useful in more cases, and may also serve us better for doing things like trying to overwrite items that differ from an existing backup, because then we'll have a lot more information about the exact differences versus just information that it already exists.
--
",skriss,"
--
@nrb that approach sounds reasonable, though I agree that adding lots of special cases here is not going to be sustainable.

Another option we could consider is trying to `Get` the resource before trying to restore it, and use that to determine if the resource exists already. That means an additional API call per resource we're restoring, though, so potentially has some runtime impact.
--
",betta1,"
--
@nrb @skriss How about trying a GET only on the services iff we find the last-applied-config annotation on the service during restore? This way we only make the API call if we're restoring a service that needs node port preservation. If a service does not have the last-applied-config annotation, we simply delete the node ports as we're doing currently and let the API server auto-assign the node ports.
--
",SVronskiy,"
--
Problem is actual in 1.5.2
--
",,,,
2287,OPEN,Unify snapshotting options across CSI and Velero-native snapshots,Area/CSI; P2 - Long-term important,2020-02-21 17:18:15 +0000 UTC,nrb,Opened,,"With CSI support, the `velero install --use-volume-snapshots` option is confusing, as it's not required. Velero's CSI support can work just fine without a VolumeSnapshotLocation.

Similarly, on a given backup, even if the backup is told not to generate new snapshots, Velero should capture any `VolumeSnapshot*` CRDs that already exist in the cluster.

As our CSI support matures, we need to unify our language, options, and logic around snapshotting.",,,,,,,,,,,,,,
2286,OPEN,GCP: Backup marked completed; even though snapshots are not ready yet,Bug; Needs Product; P2 - Long-term important,2020-05-05 16:29:07 +0000 UTC,boxcee,In progress,,"**What steps did you take and what happened:**
In our automation, I am creating backups on one cluster and then restore them on another within the same GCP project.
To backup, I use: 
```
velero backup create mybackup --storage-location gcp-s3 --include-namespaces mynamespace,myothernamespace --include-resources persistentvolumes --include-cluster-resources --snapshot-volumes --ttl 24h --wait
```
After some time the backup completes. However, when I check GCP -> Compute -> Snapshots, I see that not all snapshots are ready yet.
When restoring this 'completed' backup, I get some error messages,
so in my automation I included a step where I check all snapshots from the backup and only continue when all snapshots are marked as ready.

**What did you expect to happen:**
When I pass `--wait` to the backup command I expect the backup to be usable and all involved steps to be done before reporting a 'completed' status.

**The output of the following commands will help us better understand what's going on**:
`velero restore describe myrestore --details`
```
Name:         myrestore
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:  PartiallyFailed (run 'velero restore logs myrestore' for more information)

Errors:
  Velero:     <none>
  Cluster:  error executing PVAction for persistentvolumes/pvc-442c6854-971d-4c0b-acff-ca4e840ccf0e: rpc error: code = Unknown desc = googleapi: Error 400: The resource 'projects/my-gcp-project/global/snapshots/cluster-pvc-ab9b674c-601f-411e-b565-4b244f1c9ce1' is not ready, resourceNotReady
  Namespaces: <none>

Backup:  mybackup

Namespaces:
  Included:  mynamespace, myothernamespace
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io
  Cluster-scoped:  auto

Namespace mappings:  <none>

Label selector:  <none>

Restore PVs:  auto
```

**Environment:**

- Velero version (use `velero version`): 
```
Client:
	Version: v1.2.0
	Git commit: 5d008491bbf681658d3e372da1a9d3a21ca4c03c
Server:
	Version: v1.2.0 
```
- Velero features (use `velero client config get features`): None
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""17"", GitVersion:""v1.17.2"", GitCommit:""59603c6e503c87169aea6106f57b9f242f64df89"", GitTreeState:""clean"", BuildDate:""2020-01-18T23:30:10Z"", GoVersion:""go1.13.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""15+"", GitVersion:""v1.15.7-gke.23"", GitCommit:""06e05fd0390a51ea009245a90363f9161b6f2389"", GitTreeState:""clean"", BuildDate:""2020-01-17T23:10:45Z"", GoVersion:""go1.12.12b4"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kubernetes installer & version: GKE
- Cloud provider or hardware configuration: GKE
- OS (e.g. from `/etc/os-release`): Ubuntu 18.04
",,,skriss,"
--
@boxcee yeah we're aware of this issue - see https://github.com/vmware-tanzu/velero/issues/1799 for another report (on AWS, but same issue).
--

--
ideally we'd model this as an additional phase on the backup, to indicate that the snapshots have been created but are not yet ready.
--

--
I think it'd be useful to differentiate between ""we're actively scraping the API to create this backup"" vs. ""we're waiting for the storage system to finish moving the snapshot data to durable storage"" - more clear for users as to what's going on, and also likely makes some things easier on the back end (e.g. not blocking the backup controller queue if we're just waiting for snapshots to be ready).

But, all of this probably needs some more thought and design. If you're interested in working on this, we're happy to provide feedback.
--

--
> When the storage quotas are reached and a backup and snapshots are being created the snapshots will silently fail.

Possibly related to #2212 or #2255?
--
",boxcee,"
--
Why not flag it as 'BackupInProgress' ?
--

--
Hm, I am only concerned when it comes to using the `--wait` flag. From a user point of view I expect the backup to be finished when I explicitly add the `--wait` flag, but it isn't. The backup is not really 'Completed', only parts are.

Anyway, I am interested in finding a solution for this. Will spend some time looking into this.
--

--
Ah, another thing I experienced, which is indirectly related.

When the storage quotas are reached and a backup and snapshots are being created the snapshots will silently fail.

Am trying to create a setup and test this further.
--
",sseago,"
--
@skriss btw regarding https://github.com/vmware-tanzu/velero/issues/1799 -- while we initially found it on AWS, the same problem showed up on Azure and GCP.
--

--
@skriss I know we'd discussed options around doing this before, but we ended up fixing it in our fork (temporarily). I'm happy to remove our local commits if an upstream solution is found. Here's what we put in place: https://github.com/konveyor/velero-plugin-for-aws/pull/2
https://github.com/konveyor/velero-plugin-for-gcp/pull/2
https://github.com/konveyor/velero-plugin-for-microsoft-azure/pull/2
--
",arianitu,"
--
Nice @sseago, unfortunately we don't want to use our own Fork so we want to place the code outside of Velero until Velero supports it.

@skriss do you know how we could get the Volume Snapshot ID to check in GKE without hooking into Velero directly?

If we run:

```
  backup, err := b.vc.VeleroV1().Backups(backup.Namespace).Get(backup)
```

I think we only get the BackupSpec which I don't think has the VolumeSnapshot ID in GKE. Can you point me in the right direction with the API to get the ID so we can monitor it ourselves before running a restore?



--
",,,,
2262,OPEN,Prometheus Metrics on Restic Volume Backups,Enhancement/User; Metrics; Restic,2020-08-11 18:57:54 +0000 UTC,connorearl,Opened,,"**Describe the problem/challenge you have**
I currently am looking to create monitoring or a dashboard on what volumes are being backed up. I hope to compare this with other volume info from prometheus to get a better idea on what might be missed that we need to backup (Someone forgot to add an annotation etc.)


**Describe the solution you'd like**
It would be great if we can just export the restic backups section when describing backups with --details to a prometheus friendly format. The existing pod volume snapshots from prometheus look interesting, but they show nothing for restic. Something like that could work too.


**Environment:**

- Velero version (use `velero version`): 1.2.0
- Kubernetes version (use `kubectl version`): 1.16.3
- Kubernetes installer & version: kubeadm 1.16.3
- Cloud provider or hardware configuration: vSphere
- OS (e.g. from `/etc/os-release`): Ubuntu 18.04.04
",,,HaveFun83,"
--
Metrics from the restic pods will be really nice
--

--
https://github.com/restic/restic/issues/1878
--
",ashish,"
--
@connorearl and @HaveFun83 we now have the necessary scaffolding to add metrics for restic backup. https://github.com/vmware-tanzu/velero/pull/2719
Please feel free to add more metrics.
--
",,,,,,,,
2254,OPEN,Add velero_backup_active_total metric,Easy Pickings; Enhancement/User; Metrics,2020-02-28 18:27:30 +0000 UTC,dns2utf8,In progress,,"**Describe the problem/challenge you have**
Currently it is hard to see how many backup jobs are running.

**Describe the solution you'd like**
Expose a `velero_backup_active_total` metric.
An integer value starting at 0.

**Anything else you would like to add:**
Maybe the pods running the actual backup could report their progress too?

**Environment:**

- Velero version (use `velero version`): v1.2.0
- Kubernetes version (use `kubectl version`): v1.15.6
- Kubernetes installer & version: v1.15.3
- Cloud provider or hardware configuration: on-premise
- OS (e.g. from `/etc/os-release`): Ubuntu 18.04
",,,skriss,"
--
@dns2utf8 are you looking for the # of backups currently running? If so, that'll always be either 0 or 1, given that Velero only processes one at a time.
--

--
@dns2utf8 just looking for some clarification here, per previous comment. thanks.
--

--
Got it.

Are you interested in contributing a PR here?
--
",dns2utf8,"
--
Hi @skriss 

Yes. Even if only one is running at the time it would help with the alerting if one knew if there is a job running currently.

For example: we have an alert if the last successfull backup is more than 8 hours old.
However, there is no need to fire the alarm if the current backup is already running and is about to finish.
--
",,,,,,,,
2251,OPEN,Option to choose another GVK besides the preferred-version,Enhancement/User; versioning,2020-04-28 22:43:44 +0000 UTC,gildub,In progress,,"**Describe the problem/challenge you have**
A use case is when exporting a resource from a version of K8s being older than the version of the cluster restoring that resource into.

Let's say the backup cluster has api-server supporting:
* ""some-api-group/v1"" <- preferred-version
* ""some-api-group/v2beta1""

And the restore side has:
* ""some-api-group/v2""
* ""some-api-group/v2beta1""

At the moment this is not possible because the preferred version would export the resource represented in v1 and the api-server on the restore side doesn't support it.

**Describe the solution you'd like**
Instead of having the version automatically selected from the preferred-version, 
being able to choose another API-Group/version (of course only if that GVK is supported by the api-server), would allow to export the involved resource using ""some-api-group/v2beta1"" and make the import possible into the destination.

**Anything else you would like to add:**
Overall the idea is to help creating GKVs export/import paths when 2 clusters are apart version-wise.
And the more 2 clusters are away from each others the higher the GVK gap, and this is where such option could really help. And even if a destination doesn't support the exported version, being able to bring it closer to a supported version would help too.

**Environment:**
- Velero version (use `velero version`): 1.2.0+
- Kubernetes version (use `kubectl version`): 1.7+
- Kubernetes installer & version: any
- Cloud provider or hardware configuration: any
- OS (e.g. from `/etc/os-release`): any


",,,skriss,"
--
@gildub what if Velero backed up *all* versions of a resource, and then intelligently restored the appropriate version based on which versions it found in the target cluster? Would that meet your need without requiring you to provide the mapping?
--

--
> Now I'm not sure how you're going to do that magic unless using a similar approach as oc convert.

Well, I think for starters we wouldn't do anything too magical. In the example you gave, we'd back up both `v1` and `v2beta1` representations of the resources. Then, on restore, we'd see that the target cluster supported `v2beta1` and `v2`. Since the only overlap is `v2beta1`, we'd restore that version of the resource. So, no conversion required by Velero.

The next step would be to support conversion when there is no overlapping version between the backup and the target cluster. That *would* be pretty magical..
--

--
xref #2310 
--
",gildub,"
--
@skriss, that would be ideal!
Now I'm not sure how you're going to do that magic unless using a similar approach as `oc convert`.
--

--
@brito-rafa, I could definitely help you test.
--

--
@brito-rafa, that's really great, thank you!
--
",brito,"
--
Hi @gildub, if I code a prototype of this request, would you be able to help me with the testing... like running a backup on your environment?
--

--
Thanks! I give you the testing image once I have the code running with 100% backward compatible with the standard format. I am particularly interested to see APIGroups that are not native to K8s.
--

--
Hi @gildub 

So I have a prototype of the velero backup server with the option to backup all versions. To test, please follow these steps:

1. Install Velero as usual on your cluster.

2. Create a file ""velero-migration-patch.yaml"" with the following content:
```
spec:
  template:
    spec:
      containers:
      - args:
        - server
        - --log-level
        - debug
        - --features=Migration
        name: velero
        image: quay.io/brito_rafa/velero:dev-2251-0326-D
```
3. Run the kubectl patch command:
```
kubectl patch deployment velero --patch ""$(cat velero-migration-patch.yaml)"" -n velero
```
4. Take a normal backup and verify the content of it. You will see additional directories for each version of the resource lists (along with the default preferred group version).

At this point, this version only takes the backup of all API group versions. 
The Restore would be coded on the next phase, probably after we confirm this version is fully backward compatible, and the logic makes sense. 

Here what I would like to confirm:
1. It is indeed 100% backward compatible (my canned tests say yes). I would benefit you to run some restores from the ""Migration"" backup.
2. What the expected outcome of your CRDs with this prototyped version? Is it getting all API group versions?

Any help is appreciated!

Cheers,
Rafa
--

--
Just an update, please see [script](https://github.com/brito-rafa/velero-test/blob/master/2251-dev-test.sh) how to test the latest release. 
--
",djwhatle,"
--
Hey @brito-rafa, I'm taking over for @gildub on testing this out. Will try this out today and report back.
--

--
@brito-rafa I used your test script to check out the changes you've made. Looks like it works as described 👍

- [x] Existing ""legacy"" directory structure has been preserved (top level namespaces dir)
- [x] Conversions of each resource to all apiVersions known to apiserver are included in archive
- [x] Export of resource using preferredVersion is marked accordingly


_**New Format of Backup Content Archive**_ (includes legacy dir, plus labeled version dirs)
![image](https://user-images.githubusercontent.com/7576968/78935646-2182ae80-7a7b-11ea-9faf-a6a4609dd578.png)


--
",,,,
2212,OPEN,Backup marked completed when disk snapshot call ignored by GCP,Area/Cloud/GCP; Bug,2020-03-19 20:26:46 +0000 UTC,dsuievalov,In progress,,"**What steps did you take and what happened:**

Created backup (restic is disabled):
```bash
$ velero backup create dummy-4325-2020-01-21-1579613072 --wait \
  --include-namespaces dummy-4325 \
  --include-resources pvc,pv \
  --ttl 144h \
  --labels transient=true \
  --labels app.kubernetes.io/namespace=dummy-4325 \
  --labels app.kubernetes.io/component=data \
  --labels velero.io/backup=dummy-4325-2020-01-21-1579613072
```

Backup completed and here is the list of snapshots:

```
$ velero backup describe --details test-build-4325-data-2020-01-21-1579613072 | grep 'Snapshot ID'
    Snapshot ID:        restore-dfe96ed9-9a09-4886-c9ce1306-c33d-45f5-b5af-9d9884595644
    Snapshot ID:        restore-a041c896-cd27-4542-b541bc85-f508-4990-b4f9-b9d76c3901a3
    Snapshot ID:        restore-6acd21a8-56bc-49bc-0554d606-54b9-489c-9826-b8787eb3bb25
    Snapshot ID:        restore-7ecbed72-0fac-4be9-c16020c5-1f14-47fa-9016-9b65b2c8262c
    Snapshot ID:        restore-9c80eca0-9fe6-4c7b-dc76b78f-d867-44c1-b204-7a276b302818
    Snapshot ID:        restore-bf5a7c56-a726-40ee-c0d4e95d-f955-48ed-a618-5db32ccb1a44
```

Disk snapshots were note created for all disks:

```
$ for i in restore-dfe96ed9-9a09-4886-c9ce1306-c33d-45f5-b5af-9d9884595644 restore-a041c896-cd27-4542-b541bc85-f508-4990-b4f9-b9d76c3901a3 restore-6acd21a8-56bc-49bc-0554d606-54b9-489c-9826-b8787eb3bb25 restore-7ecbed72-0fac-4be9-c16020c5-1f14-47fa-9016-9b65b2c8262c restore-9c80eca0-9fe6-4c7b-dc76b78f-d867-44c1-b204-7a276b302818 restore-bf5a7c56-a726-40ee-c0d4e95d-f955-48ed-a618-5db32ccb1a44; do echo $i; gcloud compute snapshots list --filter=""name=($i)""; done
restore-dfe96ed9-9a09-4886-c9ce1306-c33d-45f5-b5af-9d9884595644
NAME                                                             DISK_SIZE_GB  SRC_DISK                                                           STATUS
restore-dfe96ed9-9a09-4886-c9ce1306-c33d-45f5-b5af-9d9884595644  15            europe-west1-b/disks/restore-dfe96ed9-9a09-4886-afc9-cff9d0cdf75f  READY
restore-a041c896-cd27-4542-b541bc85-f508-4990-b4f9-b9d76c3901a3
NAME                                                             DISK_SIZE_GB  SRC_DISK                                                           STATUS
restore-a041c896-cd27-4542-b541bc85-f508-4990-b4f9-b9d76c3901a3  5             europe-west1-b/disks/restore-a041c896-cd27-4542-b280-37a83b7d085e  READY
restore-6acd21a8-56bc-49bc-0554d606-54b9-489c-9826-b8787eb3bb25
Listed 0 items.
restore-7ecbed72-0fac-4be9-c16020c5-1f14-47fa-9016-9b65b2c8262c
NAME                                                             DISK_SIZE_GB  SRC_DISK                                                           STATUS
restore-7ecbed72-0fac-4be9-c16020c5-1f14-47fa-9016-9b65b2c8262c  40            europe-west1-b/disks/restore-7ecbed72-0fac-4be9-9d86-3f1c8aa3b337  READY
restore-9c80eca0-9fe6-4c7b-dc76b78f-d867-44c1-b204-7a276b302818
NAME                                                             DISK_SIZE_GB  SRC_DISK                                                           STATUS
restore-9c80eca0-9fe6-4c7b-dc76b78f-d867-44c1-b204-7a276b302818  10            europe-west1-b/disks/restore-9c80eca0-9fe6-4c7b-941f-7fefcd42b8ba  READY
restore-bf5a7c56-a726-40ee-c0d4e95d-f955-48ed-a618-5db32ccb1a44
Listed 0 items.
```

**What did you expect to happen:**

Snapshots for all disks were created.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

* `kubectl logs deployment/velero -n velero`
```json
{""controller"":""backup"",""key"":""backup/dummy-4325-2020-01-21-1579613072"",""level"":""debug"",""logSource"":""pkg/controller/backup_controller.go:162"",""msg"":""Running processBackup"",""time"":""2020-01-21T13:24:34Z""}
{""controller"":""backup"",""key"":""backup/dummy-4325-2020-01-21-1579613072"",""level"":""debug"",""logSource"":""pkg/controller/backup_controller.go:169"",""msg"":""Getting backup"",""time"":""2020-01-21T13:24:34Z""}
{""controller"":""backup"",""key"":""backup/dummy-4325-2020-01-21-1579613072"",""level"":""debug"",""logSource"":""pkg/controller/backup_controller.go:194"",""msg"":""Preparing backup request"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""controller"":""gc-controller"",""expiration"":null,""level"":""debug"",""logSource"":""pkg/controller/gc_controller.go:133"",""msg"":""Backup has not expired yet, skipping"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""controller"":""gc-controller"",""expiration"":""2020-01-27T13:24:34Z"",""level"":""debug"",""logSource"":""pkg/controller/gc_controller.go:133"",""msg"":""Backup has not expired yet, skipping"",""time"":""2020-01-21T13:24:34Z""}
{""controller"":""backup"",""key"":""backup/dummy-4325-2020-01-21-1579613072"",""level"":""debug"",""logSource"":""pkg/controller/backup_controller.go:220"",""msg"":""Running backup"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""controller"":""backup"",""level"":""info"",""logSource"":""pkg/controller/backup_controller.go:440"",""msg"":""Setting up backup log"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""level"":""info"",""logSource"":""pkg/controller/backup_controller.go:462"",""msg"":""Setting up backup temp file"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""level"":""info"",""logSource"":""pkg/controller/backup_controller.go:469"",""msg"":""Setting up plugin manager"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""level"":""info"",""logSource"":""pkg/controller/backup_controller.go:473"",""msg"":""Getting backup item actions"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""kind"":""ObjectStore"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/manager.go:99"",""msg"":""looking for plugin in registry"",""name"":""velero.io/pod"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""command"":""/velero"",""kind"":""ObjectStore"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/manager.go:114"",""msg"":""creating new restartable plugin process"",""name"":""velero.io/pod"",""time"":""2020-01-21T13:24:34Z""}
{""args"":[""/velero"",""run-plugins"",""--log-level"",""debug""],""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/velero"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/logrus_adapter.go:74"",""msg"":""starting plugin"",""path"":""/velero"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/velero"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/logrus_adapter.go:74"",""msg"":""plugin started"",""path"":""/velero"",""pid"":147,""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/velero"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/logrus_adapter.go:74"",""msg"":""waiting for RPC address"",""path"":""/velero"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/velero"",""level"":""debug"",""logSource"":""pkg/plugin/framework/server.go:172"",""msg"":""Setting log level to DEBUG"",""pluginName"":""velero"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/velero"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/logrus_adapter.go:74"",""msg"":""using plugin"",""time"":""2020-01-21T13:24:34Z"",""version"":2}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""kind"":""ObjectStore"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/manager.go:99"",""msg"":""looking for plugin in registry"",""name"":""velero.io/pv"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""command"":""/velero"",""kind"":""ObjectStore"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/manager.go:110"",""msg"":""found preexisting restartable plugin process"",""name"":""velero.io/pv"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""kind"":""ObjectStore"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/manager.go:99"",""msg"":""looking for plugin in registry"",""name"":""velero.io/service-account"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""command"":""/velero"",""kind"":""ObjectStore"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/manager.go:110"",""msg"":""found preexisting restartable plugin process"",""name"":""velero.io/service-account"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""level"":""info"",""logSource"":""pkg/controller/backup_controller.go:479"",""msg"":""Setting up backup store"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""kind"":""ObjectStore"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/manager.go:99"",""msg"":""looking for plugin in registry"",""name"":""velero.io/gcp"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""command"":""/plugins/velero-plugin-for-gcp"",""kind"":""ObjectStore"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/manager.go:114"",""msg"":""creating new restartable plugin process"",""name"":""velero.io/gcp"",""time"":""2020-01-21T13:24:34Z""}
{""address"":""/tmp/plugin342398280"",""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/velero"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/logrus_adapter.go:74"",""msg"":""plugin address"",""network"":""unix"",""pluginName"":""velero"",""time"":""2020-01-21T13:24:34Z""}
{""args"":[""/plugins/velero-plugin-for-gcp"",""--log-level"",""debug""],""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/plugins/velero-plugin-for-gcp"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/logrus_adapter.go:74"",""msg"":""starting plugin"",""path"":""/plugins/velero-plugin-for-gcp"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/plugins/velero-plugin-for-gcp"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/logrus_adapter.go:74"",""msg"":""plugin started"",""path"":""/plugins/velero-plugin-for-gcp"",""pid"":156,""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/plugins/velero-plugin-for-gcp"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/logrus_adapter.go:74"",""msg"":""waiting for RPC address"",""path"":""/plugins/velero-plugin-for-gcp"",""time"":""2020-01-21T13:24:34Z""}
{""address"":""/tmp/plugin328891762"",""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/plugins/velero-plugin-for-gcp"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/logrus_adapter.go:74"",""msg"":""plugin address"",""network"":""unix"",""pluginName"":""velero-plugin-for-gcp"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/plugins/velero-plugin-for-gcp"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/logrus_adapter.go:74"",""msg"":""using plugin"",""time"":""2020-01-21T13:24:34Z"",""version"":2}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""level"":""info"",""logSource"":""pkg/backup/backup.go:213"",""msg"":""Writing backup version file"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""level"":""info"",""logSource"":""pkg/backup/backup.go:219"",""msg"":""Including namespaces: dummy-4325"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""level"":""info"",""logSource"":""pkg/backup/backup.go:220"",""msg"":""Excluding namespaces: \u003cnone\u003e"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""level"":""info"",""logSource"":""pkg/backup/backup.go:223"",""msg"":""Including resources: persistentvolumeclaims, persistentvolumes"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""level"":""info"",""logSource"":""pkg/backup/backup.go:224"",""msg"":""Excluding resources: \u003cnone\u003e"",""time"":""2020-01-21T13:24:34Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""pods"",""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""pods"",""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""persistentvolumeclaims"",""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:227"",""msg"":""Listing items"",""namespace"":""dummy-4325"",""resource"":""persistentvolumeclaims"",""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:241"",""msg"":""Retrieved 6 items"",""namespace"":""dummy-4325"",""resource"":""persistentvolumeclaims"",""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:169"",""msg"":""Backing up item"",""name"":""data-dummy-green-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:171"",""msg"":""Executing pre hooks"",""name"":""data-dummy-green-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""data-dummy-green-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:330"",""msg"":""Executing custom action"",""name"":""data-dummy-green-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/velero"",""level"":""info"",""logSource"":""pkg/backup/backup_pv_action.go:49"",""msg"":""Executing PVCAction"",""pluginName"":""velero"",""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:169"",""msg"":""Backing up item"",""name"":""pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:171"",""msg"":""Executing pre hooks"",""name"":""pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:395"",""msg"":""Executing takePVSnapshot"",""name"":""pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""kind"":""ObjectStore"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/manager.go:99"",""msg"":""looking for plugin in registry"",""name"":""velero.io/gcp"",""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""command"":""/plugins/velero-plugin-for-gcp"",""kind"":""ObjectStore"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/manager.go:110"",""msg"":""found preexisting restartable plugin process"",""name"":""velero.io/gcp"",""time"":""2020-01-21T13:24:36Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:446"",""msg"":""Got volume ID for persistent volume"",""name"":""pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:36Z"",""volumeSnapshotLocation"":""default""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:466"",""msg"":""Getting volume information"",""name"":""pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:36Z"",""volumeID"":""restore-dfe96ed9-9a09-4886-afc9-cff9d0cdf75f""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:472"",""msg"":""Snapshotting persistent volume"",""name"":""pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:36Z"",""volumeID"":""restore-dfe96ed9-9a09-4886-afc9-cff9d0cdf75f""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:247"",""msg"":""Executing post hooks"",""name"":""pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:37Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""data-dummy-green-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:37Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:247"",""msg"":""Executing post hooks"",""name"":""data-dummy-green-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:37Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:169"",""msg"":""Backing up item"",""name"":""data-fake-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:37Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:171"",""msg"":""Executing pre hooks"",""name"":""data-fake-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:37Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""data-fake-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:37Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:330"",""msg"":""Executing custom action"",""name"":""data-fake-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:37Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/velero"",""level"":""info"",""logSource"":""pkg/backup/backup_pv_action.go:49"",""msg"":""Executing PVCAction"",""pluginName"":""velero"",""time"":""2020-01-21T13:24:37Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:169"",""msg"":""Backing up item"",""name"":""pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:37Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:171"",""msg"":""Executing pre hooks"",""name"":""pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:37Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:37Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:37Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:37Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:395"",""msg"":""Executing takePVSnapshot"",""name"":""pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:37Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:446"",""msg"":""Got volume ID for persistent volume"",""name"":""pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:37Z"",""volumeSnapshotLocation"":""default""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:466"",""msg"":""Getting volume information"",""name"":""pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:37Z"",""volumeID"":""restore-a041c896-cd27-4542-b280-37a83b7d085e""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:472"",""msg"":""Snapshotting persistent volume"",""name"":""pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:38Z"",""volumeID"":""restore-a041c896-cd27-4542-b280-37a83b7d085e""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:247"",""msg"":""Executing post hooks"",""name"":""pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:39Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""data-fake-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:39Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:247"",""msg"":""Executing post hooks"",""name"":""data-fake-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:39Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:169"",""msg"":""Backing up item"",""name"":""data-redis-dummy-green-server-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:39Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:171"",""msg"":""Executing pre hooks"",""name"":""data-redis-dummy-green-server-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:39Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""data-redis-dummy-green-server-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:39Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:330"",""msg"":""Executing custom action"",""name"":""data-redis-dummy-green-server-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:39Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/velero"",""level"":""info"",""logSource"":""pkg/backup/backup_pv_action.go:49"",""msg"":""Executing PVCAction"",""pluginName"":""velero"",""time"":""2020-01-21T13:24:39Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:169"",""msg"":""Backing up item"",""name"":""pvc-006fd481-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:39Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:171"",""msg"":""Executing pre hooks"",""name"":""pvc-006fd481-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:39Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-006fd481-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:39Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-006fd481-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:39Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-006fd481-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:39Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:395"",""msg"":""Executing takePVSnapshot"",""name"":""pvc-006fd481-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:39Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:446"",""msg"":""Got volume ID for persistent volume"",""name"":""pvc-006fd481-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-006fd481-3c35-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:39Z"",""volumeSnapshotLocation"":""default""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:466"",""msg"":""Getting volume information"",""name"":""pvc-006fd481-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-006fd481-3c35-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:39Z"",""volumeID"":""restore-6acd21a8-56bc-49bc-8f4b-54360c558681""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:472"",""msg"":""Snapshotting persistent volume"",""name"":""pvc-006fd481-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-006fd481-3c35-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:39Z"",""volumeID"":""restore-6acd21a8-56bc-49bc-8f4b-54360c558681""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:247"",""msg"":""Executing post hooks"",""name"":""pvc-006fd481-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:40Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""data-redis-dummy-green-server-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:40Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:247"",""msg"":""Executing post hooks"",""name"":""data-redis-dummy-green-server-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:40Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:169"",""msg"":""Backing up item"",""name"":""data-redis-fake-green-master-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:40Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:171"",""msg"":""Executing pre hooks"",""name"":""data-redis-fake-green-master-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:40Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""data-redis-fake-green-master-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:40Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:330"",""msg"":""Executing custom action"",""name"":""data-redis-fake-green-master-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:40Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/velero"",""level"":""info"",""logSource"":""pkg/backup/backup_pv_action.go:49"",""msg"":""Executing PVCAction"",""pluginName"":""velero"",""time"":""2020-01-21T13:24:40Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:169"",""msg"":""Backing up item"",""name"":""pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:40Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:171"",""msg"":""Executing pre hooks"",""name"":""pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:40Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:40Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:40Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:40Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:395"",""msg"":""Executing takePVSnapshot"",""name"":""pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:40Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:446"",""msg"":""Got volume ID for persistent volume"",""name"":""pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:40Z"",""volumeSnapshotLocation"":""default""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:466"",""msg"":""Getting volume information"",""name"":""pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:40Z"",""volumeID"":""restore-7ecbed72-0fac-4be9-9d86-3f1c8aa3b337""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:472"",""msg"":""Snapshotting persistent volume"",""name"":""pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:41Z"",""volumeID"":""restore-7ecbed72-0fac-4be9-9d86-3f1c8aa3b337""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:247"",""msg"":""Executing post hooks"",""name"":""pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:42Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""data-redis-fake-green-master-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:42Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:247"",""msg"":""Executing post hooks"",""name"":""data-redis-fake-green-master-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:42Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:169"",""msg"":""Backing up item"",""name"":""data-redis-boo-server-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:42Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:171"",""msg"":""Executing pre hooks"",""name"":""data-redis-boo-server-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:42Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""data-redis-boo-server-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:42Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:330"",""msg"":""Executing custom action"",""name"":""data-redis-boo-server-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:42Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/velero"",""level"":""info"",""logSource"":""pkg/backup/backup_pv_action.go:49"",""msg"":""Executing PVCAction"",""pluginName"":""velero"",""time"":""2020-01-21T13:24:42Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:169"",""msg"":""Backing up item"",""name"":""pvc-00504355-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:42Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:171"",""msg"":""Executing pre hooks"",""name"":""pvc-00504355-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:42Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-00504355-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:42Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-00504355-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:42Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""pvc-00504355-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:42Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:395"",""msg"":""Executing takePVSnapshot"",""name"":""pvc-00504355-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:42Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:446"",""msg"":""Got volume ID for persistent volume"",""name"":""pvc-00504355-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-00504355-3c35-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:42Z"",""volumeSnapshotLocation"":""default""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:466"",""msg"":""Getting volume information"",""name"":""pvc-00504355-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-00504355-3c35-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:42Z"",""volumeID"":""restore-9c80eca0-9fe6-4c7b-941f-7fefcd42b8ba""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:472"",""msg"":""Snapshotting persistent volume"",""name"":""pvc-00504355-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""persistentVolume"":""pvc-00504355-3c35-11ea-bdc7-42010a8401fb"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:42Z"",""volumeID"":""restore-9c80eca0-9fe6-4c7b-941f-7fefcd42b8ba""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:247"",""msg"":""Executing post hooks"",""name"":""pvc-00504355-3c35-11ea-bdc7-42010a8401fb"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:43Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""data-redis-boo-server-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:43Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:247"",""msg"":""Executing post hooks"",""name"":""data-redis-boo-server-0"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:43Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:169"",""msg"":""Backing up item"",""name"":""mysql"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:43Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:171"",""msg"":""Executing pre hooks"",""name"":""mysql"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:43Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""mysql"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:43Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:330"",""msg"":""Executing custom action"",""name"":""mysql"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:43Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/velero"",""level"":""info"",""logSource"":""pkg/backup/backup_pv_action.go:49"",""msg"":""Executing PVCAction"",""pluginName"":""velero"",""time"":""2020-01-21T13:24:43Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:169"",""msg"":""Backing up item"",""name"":""dummy-4325-mysql"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:43Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:171"",""msg"":""Executing pre hooks"",""name"":""dummy-4325-mysql"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:43Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""dummy-4325-mysql"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:43Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""dummy-4325-mysql"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:43Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""dummy-4325-mysql"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:43Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:395"",""msg"":""Executing takePVSnapshot"",""name"":""dummy-4325-mysql"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:43Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:446"",""msg"":""Got volume ID for persistent volume"",""name"":""dummy-4325-mysql"",""namespace"":"""",""persistentVolume"":""dummy-4325-mysql"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:43Z"",""volumeSnapshotLocation"":""default""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:466"",""msg"":""Getting volume information"",""name"":""dummy-4325-mysql"",""namespace"":"""",""persistentVolume"":""dummy-4325-mysql"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:43Z"",""volumeID"":""restore-bf5a7c56-a726-40ee-98a5-8e4af9994119""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/item_backupper.go:472"",""msg"":""Snapshotting persistent volume"",""name"":""dummy-4325-mysql"",""namespace"":"""",""persistentVolume"":""dummy-4325-mysql"",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:43Z"",""volumeID"":""restore-bf5a7c56-a726-40ee-98a5-8e4af9994119""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:247"",""msg"":""Executing post hooks"",""name"":""dummy-4325-mysql"",""namespace"":"""",""resource"":{""Group"":"""",""Resource"":""persistentvolumes""},""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:311"",""msg"":""Skipping action because it does not apply to this resource"",""name"":""mysql"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""debug"",""logSource"":""pkg/backup/item_backupper.go:247"",""msg"":""Executing post hooks"",""name"":""mysql"",""namespace"":""dummy-4325"",""resource"":{""Group"":"""",""Resource"":""persistentvolumeclaims""},""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""persistentvolumes"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""persistentvolumes"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""serviceaccounts"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""serviceaccounts"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""limitranges"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""limitranges"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""replicationcontrollers"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""replicationcontrollers"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""namespaces"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""namespaces"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""events"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""events"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""resourcequotas"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""resourcequotas"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""configmaps"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""configmaps"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""endpoints"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""endpoints"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""nodes"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""nodes"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""services"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""services"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""podtemplates"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""podtemplates"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""secrets"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""secrets"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apiregistration.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apiregistration.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""apiservices"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apiregistration.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""apiservices"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apps/v1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apps/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""statefulsets"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apps/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""statefulsets"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apps/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""daemonsets"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apps/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""daemonsets"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apps/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""replicasets"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apps/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""replicasets"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apps/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""controllerrevisions"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apps/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""controllerrevisions"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apps/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""deployments"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apps/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""deployments"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""autoscaling/v1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""autoscaling/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""horizontalpodautoscalers"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""autoscaling/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""horizontalpodautoscalers"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""batch/v1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""batch/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""jobs"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""batch/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""jobs"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""batch/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""batch/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""cronjobs"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""batch/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""cronjobs"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""certificates.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""certificates.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""certificatesigningrequests"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""certificates.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""certificatesigningrequests"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""networking.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""networking.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""networkpolicies"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""networking.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""networkpolicies"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""networking.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""networking.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""ingresses"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""networking.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""ingresses"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""policy/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""policy/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""poddisruptionbudgets"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""policy/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""poddisruptionbudgets"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""policy/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""podsecuritypolicies"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""policy/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""podsecuritypolicies"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""rbac.authorization.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""rbac.authorization.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""clusterrolebindings"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""rbac.authorization.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""clusterrolebindings"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""rbac.authorization.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""clusterroles"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""rbac.authorization.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""clusterroles"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""rbac.authorization.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""rolebindings"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""rbac.authorization.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""rolebindings"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""rbac.authorization.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""roles"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""rbac.authorization.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""roles"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""storage.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""storage.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""volumeattachments"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""storage.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""volumeattachments"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""storage.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""storageclasses"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""storage.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""storageclasses"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""storage.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""storage.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""csidrivers"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""storage.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""csidrivers"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""storage.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""csinodes"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""storage.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""csinodes"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""admissionregistration.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""admissionregistration.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""mutatingwebhookconfigurations"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""admissionregistration.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""mutatingwebhookconfigurations"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""admissionregistration.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""validatingwebhookconfigurations"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""admissionregistration.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""validatingwebhookconfigurations"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apiextensions.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apiextensions.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""customresourcedefinitions"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""apiextensions.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""customresourcedefinitions"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""scheduling.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""scheduling.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""priorityclasses"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""scheduling.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""priorityclasses"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""coordination.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""coordination.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""leases"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""coordination.k8s.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""leases"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""node.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""node.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""runtimeclasses"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""node.k8s.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""runtimeclasses"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""monitoring.coreos.com/v1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""monitoring.coreos.com/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""prometheuses"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""monitoring.coreos.com/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""prometheuses"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""monitoring.coreos.com/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""prometheusrules"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""monitoring.coreos.com/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""prometheusrules"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""monitoring.coreos.com/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""alertmanagers"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""monitoring.coreos.com/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""alertmanagers"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""monitoring.coreos.com/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""servicemonitors"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""monitoring.coreos.com/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""servicemonitors"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""monitoring.coreos.com/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""podmonitors"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""monitoring.coreos.com/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""podmonitors"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""downloadrequests"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""downloadrequests"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""restores"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""restores"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""resticrepositories"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""resticrepositories"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""podvolumebackups"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""podvolumebackups"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""deletebackuprequests"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""deletebackuprequests"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""serverstatusrequests"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""serverstatusrequests"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""backups"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""backups"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""volumesnapshotlocations"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""volumesnapshotlocations"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""backupstoragelocations"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""backupstoragelocations"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""schedules"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""schedules"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""podvolumerestores"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""velero.io/v1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""podvolumerestores"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""internal.autoscaling.k8s.io/v1alpha1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""internal.autoscaling.k8s.io/v1alpha1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""capacityrequests"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""internal.autoscaling.k8s.io/v1alpha1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""capacityrequests"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""nodemanagement.gke.io/v1alpha1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""nodemanagement.gke.io/v1alpha1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""updateinfos"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""nodemanagement.gke.io/v1alpha1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""updateinfos"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""scalingpolicy.kope.io/v1alpha1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""scalingpolicy.kope.io/v1alpha1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""scalingpolicies"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""scalingpolicy.kope.io/v1alpha1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""scalingpolicies"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""acme.cert-manager.io/v1alpha2"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""acme.cert-manager.io/v1alpha2"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""challenges"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""acme.cert-manager.io/v1alpha2"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""challenges"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""acme.cert-manager.io/v1alpha2"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""orders"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""acme.cert-manager.io/v1alpha2"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""orders"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""cert-manager.io/v1alpha2"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""cert-manager.io/v1alpha2"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""issuers"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""cert-manager.io/v1alpha2"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""issuers"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""cert-manager.io/v1alpha2"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""clusterissuers"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""cert-manager.io/v1alpha2"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""clusterissuers"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""cert-manager.io/v1alpha2"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""certificaterequests"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""cert-manager.io/v1alpha2"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""certificaterequests"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""cert-manager.io/v1alpha2"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""certificates"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""cert-manager.io/v1alpha2"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""certificates"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""cloud.google.com/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""cloud.google.com/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""backendconfigs"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""cloud.google.com/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""backendconfigs"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""networking.gke.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""networking.gke.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""managedcertificates"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""networking.gke.io/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""managedcertificates"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""extensions/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/group_backupper.go:101"",""msg"":""Backing up group"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""extensions/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""replicasets"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""extensions/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""replicasets"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""extensions/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""deployments"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""extensions/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""deployments"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""extensions/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""networkpolicies"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""extensions/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""networkpolicies"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""extensions/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""podsecuritypolicies"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""extensions/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:128"",""msg"":""Skipping resource because it's cluster-scoped and only specific namespaces are included in the backup"",""resource"":""podsecuritypolicies"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""extensions/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""daemonsets"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""extensions/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""daemonsets"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""extensions/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:106"",""msg"":""Backing up resource"",""resource"":""ingresses"",""time"":""2020-01-21T13:24:44Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""group"":""extensions/v1beta1"",""level"":""info"",""logSource"":""pkg/backup/resource_backupper.go:138"",""msg"":""Skipping resource because it's excluded"",""resource"":""ingresses"",""time"":""2020-01-21T13:24:44Z""}
{""controller"":""backup"",""level"":""info"",""logSource"":""pkg/controller/backup_controller.go:537"",""msg"":""Backup completed"",""time"":""2020-01-21T13:24:45Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/velero"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/logrus_adapter.go:74"",""msg"":""plugin process exited"",""path"":""/velero"",""pid"":147,""time"":""2020-01-21T13:24:45Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/velero"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/logrus_adapter.go:74"",""msg"":""plugin exited"",""time"":""2020-01-21T13:24:45Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/plugins/velero-plugin-for-gcp"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/logrus_adapter.go:74"",""msg"":""plugin process exited"",""path"":""/plugins/velero-plugin-for-gcp"",""pid"":156,""time"":""2020-01-21T13:24:45Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""cmd"":""/plugins/velero-plugin-for-gcp"",""level"":""debug"",""logSource"":""pkg/plugin/clientmgmt/logrus_adapter.go:74"",""msg"":""plugin exited"",""time"":""2020-01-21T13:24:45Z""}
{""controller"":""backup"",""key"":""backup/dummy-4325-2020-01-21-1579613072"",""level"":""debug"",""logSource"":""pkg/controller/backup_controller.go:246"",""msg"":""Updating backup's final status"",""time"":""2020-01-21T13:24:45Z""}
{""backup"":""backup/dummy-4325-2020-01-21-1579613072"",""controller"":""gc-controller"",""expiration"":""2020-01-27T13:24:34Z"",""level"":""debug"",""logSource"":""pkg/controller/gc_controller.go:133"",""msg"":""Backup has not expired yet, skipping"",""time"":""2020-01-21T13:24:45Z""}
```
* `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
```
Name:         dummy-4325-2020-01-21-1579613072
Namespace:    backup
Labels:       app.kubernetes.io/component=data
              app.kubernetes.io/namespace=dummy-4325
              transient=true
              velero.io/backup=dummy-4325-2020-01-21-1579613072
              velero.io/pv=dummy-4325-mysql
              velero.io/storage-location=default
Annotations:  <none>

Phase:  Completed

Namespaces:
  Included:  dummy-4325
  Excluded:  <none>

Resources:
  Included:        pvc, pv
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  default

Snapshot PVs:  auto

TTL:  144h0m0s

Hooks:  <none>

Backup Format Version:  1

Started:    2020-01-21 15:24:34 +0200 EET
Completed:  2020-01-21 15:24:44 +0200 EET

Expiration:  2020-01-27 15:24:34 +0200 EET

Resource List:
  v1/PersistentVolume:
    - pvc-00504355-3c35-11ea-bdc7-42010a8401fb
    - pvc-006fd481-3c35-11ea-bdc7-42010a8401fb
    - pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb
    - pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb
    - pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb
    - dummy-4325-mysql
  v1/PersistentVolumeClaim:
    - dummy-4325/data-dummy-green-0
    - dummy-4325/data-fake-0
    - dummy-4325/data-redis-dummy-green-server-0
    - dummy-4325/data-redis-fake-green-master-0
    - dummy-4325/data-redis-boo-server-0
    - dummy-4325/mysql

Persistent Volumes:
  pvc-ff4f5478-3c34-11ea-bdc7-42010a8401fb:
    Snapshot ID:        restore-dfe96ed9-9a09-4886-c9ce1306-c33d-45f5-b5af-9d9884595644
    Type:               https://www.googleapis.com/compute/v1/projects/dummy-project/zones/europe-west1-b/diskTypes/pd-standard
    Availability Zone:  europe-west1-b
    IOPS:               <N/A>
  pvc-4726adf2-3c35-11ea-bdc7-42010a8401fb:
    Snapshot ID:        restore-a041c896-cd27-4542-b541bc85-f508-4990-b4f9-b9d76c3901a3
    Type:               https://www.googleapis.com/compute/v1/projects/dummy-project/zones/europe-west1-b/diskTypes/pd-standard
    Availability Zone:  europe-west1-b
    IOPS:               <N/A>
  pvc-006fd481-3c35-11ea-bdc7-42010a8401fb:
    Snapshot ID:        restore-6acd21a8-56bc-49bc-0554d606-54b9-489c-9826-b8787eb3bb25
    Type:               https://www.googleapis.com/compute/v1/projects/dummy-project/zones/europe-west1-b/diskTypes/pd-standard
    Availability Zone:  europe-west1-b
    IOPS:               <N/A>
  pvc-ffa506b4-3c34-11ea-bdc7-42010a8401fb:
    Snapshot ID:        restore-7ecbed72-0fac-4be9-c16020c5-1f14-47fa-9016-9b65b2c8262c
    Type:               https://www.googleapis.com/compute/v1/projects/dummy-project/zones/europe-west1-b/diskTypes/pd-standard
    Availability Zone:  europe-west1-b
    IOPS:               <N/A>
  pvc-00504355-3c35-11ea-bdc7-42010a8401fb:
    Snapshot ID:        restore-9c80eca0-9fe6-4c7b-dc76b78f-d867-44c1-b204-7a276b302818
    Type:               https://www.googleapis.com/compute/v1/projects/dummy-project/zones/europe-west1-b/diskTypes/pd-standard
    Availability Zone:  europe-west1-b
    IOPS:               <N/A>
  dummy-4325-mysql:
    Snapshot ID:        restore-bf5a7c56-a726-40ee-c0d4e95d-f955-48ed-a618-5db32ccb1a44
    Type:               https://www.googleapis.com/compute/v1/projects/dummy-project/zones/europe-west1-b/diskTypes/pd-ssd
    Availability Zone:  europe-west1-b
    IOPS:               <N/A>
```


**Anything else you would like to add:**

Deployment limits are the following:
```yaml
        resources:
          limits:
            cpu: ""1""
            memory: 1Gi
          requests:
            cpu: ""1""
            memory: 1Gi
```

Pod memory usage is low:
![Screenshot 2020-01-21 at 16 30 51](https://user-images.githubusercontent.com/59164447/72813083-6f053180-3c6b-11ea-8854-34c7b5ab25ea.png)

GCP audit logs do not log the snapshots `create` events but only `delete` events which is why can not provide audit logs from the GCP snapshots API but have failed to replicate the same behaviour via `gcloud compute disks shanshots` command. With `gcloud` everything works as expected.

**Environment:**

- Velero version (use `velero version`): `v1.2.0`
- Velero features (use `velero client config get features`): `none`
- Kubernetes version (use `kubectl version`): `v1.14.7`
- Kubernetes installer & version: `v1.14.7-gke.23`
- Cloud provider or hardware configuration: `GKE`
- OS (e.g. from `/etc/os-release`): `Darwin`
",,,skriss,"
--
hi @dsuievalov, I'm really not sure what could be going on here. From the logs and outputs you included, it sure looks like things are working as expected from the `velero` side.

It might be interesting to run `gcloud compute snapshots list` in a watch while the backup is executing, to see if those snapshots ever appear via `gcloud`. Perhaps that can give us some more information to go on.
--

--
@dsuievalov I built a custom image with some additional logging that you could try out and see if you get any more info.

The branch with the code change is here: https://github.com/skriss/velero-plugin-for-gcp/commit/961d5025f088bc62b3001c8ae43d3e2addf6e6eb

The image you can use is `steveheptio/velero-plugin-for-gcp:snapshot-logging`

If you have velero up and running, you can swap this plugin image in by doing:

```bash
velero plugin remove velero/velero-plugin-for-gcp:v1.0.0

velero plugin add steveheptio/velero-plugin-for-gcp:snapshot-logging
```

Please try this out and let me know what you find in the logs! Hopefully the ""Got response from CreateSnapshot call"" log lines will provide some more info.
--

--
@dsuievalov were you able to test this out?
--
",carlisia,"
--
Hey @dsuievalov, has your issue been resolved?
--
",dsuievalov,"
--
> Hey @dsuievalov, has your issue been resolved?

Hello, unfortunately no. Sorry, forget to respond in here after testing of the `gcloud compute snapshots list` during the back procedure. Eventually gcloud does not tell anything new and there are no errors about missing snapshots.
I would like to see the full responce from GCP side for each velero snapshot creation call. If GCP responce with successful creation - velero working as expected and it is a problem of GCP (some limitation maybe). But I am not very good with a golang so will appreciate if someone can point me to the correct way of changing the source code to archive that goal :) I will build the custom binary just for the test.
--

--
> Was any issue logged with Google Support about this? Given that it may well be a problem on the Google side.

Did not contact Google Support since the issue happens only when using velero - direct calls via `gcloud` always report with snapshots creation/failure (stress testing it with ~200 disks (>50Gb) snapshots creation per hour). Which is why would like to debug it from velero side.
--

--
@skriss sorry, I was not able to test it this week. I will do that on monday.
--

--
@skriss Thank you very much for the extra debugging message, it helps to find the actual issue. Here is a velero log from gcloud for failed snapshot:

```
{""backup"":""backup/test-build-5396"",""cmd"":""/plugins/velero-plugin-for-gcp"",""createSnapshotResponse"":{""id"":""1593028888651089567"",""insertTime"":""2020-02-26T05:53:52.115-08:00"",""kind"":""compute#operation"",""name"":""operation-1582725231622-59f7aef353a2f-6746de61-a55034dc"",""operationType"":""createSnapshot"",""selfLink"":""https://www.googleapis.com/compute/v1/projects/dummy-project/zones/europe-west1-b/operations/operation-1582725231622-59f7aef353a2f-6746de61-a55034dc"",""startTime"":""2020-02-26T05:53:52.117-08:00"",""status"":""RUNNING"",""targetId"":""1125778073669331382"",""targetLink"":""https://www.googleapis.com/compute/v1/projects/dummy-project/zones/europe-west1-b/disks/restore-a041c896-cd27-4542-b541bc85-f508-4990-b4f9-b9d76c3901a3"",""user"":""dummy-user@dummy-project.iam.gserviceaccount.com"",""zone"":""https://www.googleapis.com/compute/v1/projects/dummy-project/zones/europe-west1-b""},""level"":""info"",""logSource"":""/go/src/github.com/vmware-tanzu/velero-plugin-for-gcp/velero-plugin-for-gcp/volume_snapshotter.go:245"",""msg"":""Got response from CreateSnapshot call"",""pluginName"":""velero-plugin-for-gcp"",""time"":""2020-02-26T13:53:52Z""}
``` 

And the detail information about snapshot creation operation:

```
$ gcloud compute operations describe https://www.googleapis.com/compute/v1/projects/dummy-project/zones/europe-west1-b/operations/operation-1582725231622-59f7aef353a2f-6746de61-a55034dc
endTime: '2020-02-26T05:54:01.343-08:00'
error:
  errors:
  - code: INVALID_USAGE
    message: Disk attachment changed while trying to make a snapshot.
httpErrorMessage: BAD REQUEST
httpErrorStatusCode: 400
id: '1593028888651089567'
insertTime: '2020-02-26T05:53:52.115-08:00'
kind: compute#operation
name: operation-1582725231622-59f7aef353a2f-6746de61-a55034dc
operationType: createSnapshot
progress: 100
selfLink: https://www.googleapis.com/compute/v1/projects/dummy-project/zones/europe-west1-b/operations/operation-1582725231622-59f7aef353a2f-6746de61-a55034dc
startTime: '2020-02-26T05:53:52.117-08:00'
status: DONE
targetId: '1125778073669331382'
targetLink: https://www.googleapis.com/compute/v1/projects/dummy-project/zones/europe-west1-b/disks/restore-a041c896-cd27-4542-b541bc85-f508-4990-b4f9-b9d76c3901a3
user: dummy-user@dummy-project.iam.gserviceaccount.com
zone: https://www.googleapis.com/compute/v1/projects/dummy-project/zones/europe-west1-b
```

So the snapshot was not created because of gcloud internal issue. Since I have scaled all the pods in the namespace (and wait until all of them will be terminated) before `velero backup` call it looks during the snapshotting gcloud disk was detached and its information changed. But as I mentioned - it is not gcloud issue.

I think if @boxcee finishes with PR https://github.com/vmware-tanzu/velero/pull/2297 and velero backup with `--wait` argument wait until the snapshot creates successfully or alert in case of failures (like this one, for example) this issue should not happen again.

💯 kudos to you guys: @skriss / @boxcee
--
",gorzek,"
--
Was any issue logged with Google Support about this? Given that it may well be a problem on the Google side.
--
",,,,
2198,OPEN,Restic backups with an S3 destination don't use eks.amazonaws.com/role-arn annotations,Area/Cloud/AWS; Enhancement/User; Restic,2021-02-22 19:04:29 +0000 UTC,geofffranks,In progress,,"**What steps did you take and what happened:**

We're using velero 1.2.0 to back up our EKS cluster and some EFS PVs. The velero service account is using AWS's eks.amazonaws.com/role-arn annotation to attach a role to it. This works fine for backing up data from velero, however restic does not seem to support pulling the AWS token properly with that mechanism, and it is using anonymous authentication, and failing to store data in S3.

**What did you expect to happen:**

Restic should be able to back up using the same credentials velero uses

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

Error message from backup logs:
```
time=""2020-01-14T22:06:29Z"" level=error msg=""Error backing up item"" backup=velero/infra-test-efs-backup-for-realz3 error=""restic repository is not ready: error running command=restic init --repo=s3:s3-us-east-1.amazonaws.com/velero-platform-v2-backup/restic/kube-system --password-file=/tmp/velero-restic-credentials-kube-system355348533 --cache-dir=/scratch/.cache/restic, stdout=, stderr=Fatal: create key in repository at s3:s3-us-east-1.amazonaws.com/velero-platform-v2-backup/restic/kube-system failed: client.PutObject: Access Denied\n\n: exit status 1"" error.file=""/go/src/github.com/vmware-tanzu/velero/pkg/restic/repository_ensurer.go:144"" error.function=""github.com/vmware-tanzu/velero/pkg/restic.(*repositoryEnsurer).EnsureRepo"" group=v1 logSource=""pkg/backup/resource_backupper.go:288"" name=test-pod2 namespace= resource=pods
```

Restic pod logs:
```
time=""2020-01-13T21:26:09Z"" level=info msg=""Setting log-level to INFO""
time=""2020-01-13T21:26:09Z"" level=info msg=""Starting Velero restic server v1.2.0 (5d008491bbf681658d3e372da1a9d3a21ca4c03c)"" logSource=""pkg/cmd/cli/restic/server.go:62""
time=""2020-01-13T21:26:10Z"" level=info msg=""Starting controllers"" logSource=""pkg/cmd/cli/restic/server.go:156""
time=""2020-01-13T21:26:10Z"" level=info msg=""Starting controller"" controller=pod-volume-backup logSource=""pkg/controller/generic_controller.go:76""
time=""2020-01-13T21:26:10Z"" level=info msg=""Waiting for caches to sync"" controller=pod-volume-backup logSource=""pkg/controller/generic_controller.go:79""
time=""2020-01-13T21:26:10Z"" level=info msg=""Controllers started successfully"" logSource=""pkg/cmd/cli/restic/server.go:199""
time=""2020-01-13T21:26:10Z"" level=info msg=""Starting controller"" controller=pod-volume-restore logSource=""pkg/controller/generic_controller.go:76""
time=""2020-01-13T21:26:10Z"" level=info msg=""Waiting for caches to sync"" controller=pod-volume-restore logSource=""pkg/controller/generic_controller.go:79""
time=""2020-01-13T21:26:10Z"" level=info msg=""Caches are synced"" controller=pod-volume-backup logSource=""pkg/controller/generic_controller.go:83""
time=""2020-01-13T21:26:10Z"" level=info msg=""Caches are synced"" controller=pod-volume-restore logSource=""pkg/controller/generic_controller.go:83""
W0114 10:25:17.438939       1 reflector.go:302] github.com/vmware-tanzu/velero/pkg/cmd/cli/restic/server.go:197: watch of *v1.Secret ended with: too old resource version: 11493244 (11502245)
W0114 17:48:06.516931       1 reflector.go:302] github.com/vmware-tanzu/velero/pkg/cmd/cli/restic/server.go:197: watch of *v1.Secret ended with: too old resource version: 11609586 (11656444)
```
AWS Error message:
```
{
  ""recipientAccountId"": ""REDACTED"",
  ""userIdentity"": {
    ""type"": ""AWSAccount"",
    ""principalId"": """",
    ""accountId"": ""ANONYMOUS_PRINCIPAL""
  },
  ""responseElements"": null,
  ""errorMessage"": ""Access Denied"",
  ""requestID"": ""REDACTED"",
  ""managementEvent"": false,
  ""eventID"": ""REDACTED"",
  ""userAgent"": ""[Minio (linux; amd64) minio-go/v6.0.14]"",
  ""eventName"": ""PutObject"",
  ""resources"": [
    {
      ""type"": ""AWS::S3::Object"",
      ""ARN"": ""REDACTED/restic/kube-system/keys/165975608458061c33f1fd9d74c71589a39c1c603bd680ed027a26a7a42b3aa7""
    },
    {
      ""type"": ""AWS::S3::Bucket"",
      ""ARN"": ""REDACTED"",
      ""accountId"": ""REDACTED""
    }
  ],
  ""readOnly"": false,
  ""eventVersion"": ""1.06"",
  ""eventTime"": ""2020-01-14T22:05:04Z"",
  ""vpcEndpointId"": ""REDACTED"",
  ""requestParameters"": {
    ""key"": ""restic/kube-system/keys/165975608458061c33f1fd9d74c71589a39c1c603bd680ed027a26a7a42b3aa7"",
    ""bucketName"": ""REDACTED"",
    ""Host"": ""REDACTED""
  },
  ""awsRegion"": ""us-east-1"",
  ""eventSource"": ""s3.amazonaws.com"",
  ""sharedEventID"": ""REDACTED"",
  ""additionalEventData"": {
    ""CipherSuite"": ""ECDHE-RSA-AES128-GCM-SHA256"",
    ""bytesTransferredOut"": 243,
    ""bytesTransferredIn"": 0,
    ""x-amz-id-2"": ""REDACTED""
  },
  ""sourceIPAddress"": ""REDACTED"",
  ""errorCode"": ""AccessDenied"",
  ""eventType"": ""AwsApiCall""
}
```

**Anything else you would like to add:**
This was an issue for us last fall with velero. The solution was to update the AWS SDK to the latest version, and things magically worked afterwards. Not sure what might be needed since restic is using minio though. Looks like the latest version of restic uses minio 6.0.43 so maybe updating to that is enough?

For more info on the service-account IAM role stuff:
https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html
https://docs.aws.amazon.com/eks/latest/userguide/specify-service-account-role.html

**Environment:**

- Velero version (use `velero version`):  1.2.0
- Velero features (use `velero client config get features`): <NOT SET>
- Kubernetes version (use `kubectl version`): 1.14.9
- Kubernetes installer & version: EKS
- Cloud provider or hardware configuration: AWS
- OS (e.g. from `/etc/os-release`): amazon-linux
",,,geofffranks,"
--
Tried building a local image of velero using restic 0.9.6 and that doesn't appear to have solved it for us, so not sure what would be necessary here.
--
",skriss,"
--
Hi @geofffranks -- I think this is a limitation of `restic` itself, as it uses the `minio` SDK rather than the  `AWS` one, so this mode of authentication likely isn't supported. You could try bringing it up there (https://github.com/restic/restic or https://forum.restic.net/), though I'm not sure how successful that would be.  Sorry I don't have a better option for you..
--

--
Great, now just need to wait for a new restic release that contains it! cc @vmware-tanzu/velero-maintainers 
--
",christian,"
--
@skriss Did you find a way to enable IAM for service account while using restic/velero
--

--
Restic is using minio which already has support for IAM for service accounts (https://github.com/minio/minio-go/pull/1183). I created a feature request for restic to enable this feature as well (https://github.com/restic/restic/issues/2703).
--
",galindro,"
--
@skriss According to https://github.com/restic/restic/pull/2733, restic now contains the fix for use IRSA.
--
",ac,"
--
@skriss Looks like restic have made a new release 0.10.0 in the last couple of days that contains restic/restic#2733
--
",braudel,"
--
Is there an ETA for a velero docker image release bundled with restic 0.10.0?
--
"
3145,OPEN,EBS Snapshot restores do not show failures when KMS permission is denied,Area/Cloud/AWS; Bug,2020-12-07 23:42:30 +0000 UTC,geofffranks,Opened,,"We tried doing a restore including 3 EBS snapshots today. Velero's logs indicate they restored successfully. There are PVs and PVCs created in the namespace as they should, and the PVs have volume IDs. However, the volume IDs did not exist in EC2 anywhere, and kubernetes failed to attach them (volume not found errors). When we got to the bottom of it, our velero IAM policy didn't have permission for `kms:ReEncrypt*` on the key used to encrypt the volumes, and an asynchronous error was generated, and the volumes never finished creating.

It would be really nice if velero could check the status of EBS volume restores and only mark the restore complete/successful if the EBS volume gets created, and display any errors related to lack of permissions if possible.",,,skriss,"
--
transferring to AWS plugin repo
--
",,,,,,,,,,
2179,OPEN,Update restic PPC binary to official build,P2 - Long-term important; kind/tech-debt,2020-02-19 17:02:01 +0000 UTC,nrb,Opened,,"Once restic releases official PPC binaries, we should remove the [restic download](https://github.com/vmware-tanzu/velero/blob/master/hack/get-restic-ppc64le.sh) script and make process match the amd64 builds.",,,,,,,,,,,,,,
2171,OPEN,restore stuck with `InProgress` status,Enhancement/User,2020-02-28 18:49:46 +0000 UTC,dsuievalov,Opened,,"**What steps did you take and what happened:**
If the restoration process stucks because it can not restore the resource (or resource was restored correctly but the service is not seeing it) main `velero` process blocks and do not respond. To unblock it restoration event that blocks it should be removed and `velero` container restarted. 


**What did you expect to happen:**

It would be nice if:
- there is an option to cancel restoration procedure if needed or when the restore event was deleted after creation (without `velero` container restart);
- velero can work out with restoration events in parallel;

**The output of the following commands will help us better understand what's going on**:

Backup describe: https://gist.github.com/dsuievalov/c47c171b191e128e735638eafbf40b89#file-backup-describe
Restore describe: https://gist.github.com/dsuievalov/c47c171b191e128e735638eafbf40b89#file-restore-describe
Server logs during stucked restoration: https://gist.github.com/dsuievalov/c47c171b191e128e735638eafbf40b89#file-server-logs-during-stucked-restoration


**Anything else you would like to add:**

**Environment:**

- Velero version: v1.2.0
- Kubernetes version: v1.14.7
- Cloud provider or hardware configuration: GKE
",,,skriss,"
--
xref #1653 - potentially solved by worker pods
--
",,,,,,,,,,
2170,OPEN,restic: ability to run `restic restore` even when the PV already been restored,Enhancement/User; Restic,2020-01-17 17:28:06 +0000 UTC,dsuievalov,Opened,,"**Describe the problem/challenge you have**
Velero service installed via the official [helm chart](https://github.com/vmware-tanzu/helm-charts/tree/master/charts/velero) with restic integration. K8s deployment was update with a proper annotation to make sure that PV will be backuped via restic. `velero backup create`/`velero restore create` been run once. Random files were removed from the PV via `kubectl exec` from the pod to simulate unexpected data loss. `velero restore create` been run again but the files were not recovered. According to `velero` logs PV was skipped:

```
time=""2020-01-03T17:38:50Z"" level=info msg=""Skipping persistentvolumes/pvc-189b3264-2e4c-11ea-b653-42010a8401b9 because it's already been restored."" logSource=""pkg/restore/restore.go:800"" restore=velero/test-restic-20200103193849
```

Helm installation command:
```bash
[~]$ helm upgrade velero vmware-tanzu/velero \
  --install \
  --namespace velero \
  --set configuration.provider=gcp \
  --set-file credentials.secretContents.cloud=<secret.json> \
  --set configuration.backupStorageLocation.name=gcp \
  --set configuration.backupStorageLocation.bucket=<bucket> \
  --set configuration.backupStorageLocation.config.serviceAccount=<serviceAccount> \
  --set configuration.volumeSnapshotLocation.name=gcp \
  --set configuration.volumeSnapshotLocation.config.serviceAccount=<serviceAccount> \
  --set image.repository=velero/velero \
  --set image.tag=v1.2.0 \
  --set image.pullPolicy=IfNotPresent \
  --set initContainers[0].name=velero-plugin-for-gcp \
  --set initContainers[0].image=velero/velero-plugin-for-gcp:v1.0.0 \
  --set initContainers[0].volumeMounts[0].mountPath=/target \
  --set initContainers[0].volumeMounts[0].name=plugins \
  --set-string deployRestic=true \
  --wait --atomic
```

Backup:
```
[~]$ velero backup create test-restic --include-namespaces default -l app=nginx -l volume=true --wait
```

Backup describe: https://gist.github.com/dsuievalov/8048af3f515f1b23dd99225cb0f58c11#file-backup-test-restic-describe

Restore for the first time:
```
[~]$ kubectl delete deployments/nginx-deployment pvc/nginx-logs
[~]$ velero restore create --from-backup test-restic --wait
```

Restore describe: https://gist.github.com/dsuievalov/8048af3f515f1b23dd99225cb0f58c11#file-restore-first-time-test-restic-describe

Restore describe after random files removal: https://gist.github.com/dsuievalov/8048af3f515f1b23dd99225cb0f58c11#file-restore-not-first-time-test-restic-describe
Restore logs: https://gist.github.com/dsuievalov/8048af3f515f1b23dd99225cb0f58c11#file-restore-not-first-time-test-restic-logs

**Describe the solution you'd like**
It would be great if `velero restore create` can run `restic restore` even when the PV already been restored.


**Anything else you would like to add:**


**Environment:**

- Velero version: v1.2.0
- Kubernetes version: v1.14.7
- Cloud provider or hardware configuration: GKE
",,,mynktl,"
--
This should be applied to all type of snapshot. Multiple restore on the same PV should be configurable. If restored snapshot/data gets deleted due to some technical issue then only option available is to create a new PV. 
--
",,,,,,,,,,
2167,OPEN,Cluster Migration Documentation not clear - Unusable,Area/Documentation; Help wanted,2020-01-29 21:07:49 +0000 UTC,archmangler,Opened,,"**Describe the problem/challenge you have**

The documentation link on cluster migration here:

https://velero.io/docs/master/migration-case/

a) Does not indicate how to specify the new backup location when restoring a backup from the newly created backup location that points to the-cluster-that-must-be-migrated. Should it be passed as an argument to the ""velero create restore from <backup> ...."" command?

Because of this the attempt to migrate the cluster stops at the point where the second backup location pointing to the source cluster's container is created. There does not seem to be a way to indicate that the backup desired must come from the newly created backup location.

b) There does not seem to be clear documentation on specifying additional backup locations for each cloud vendor (Azure in my case). This makes the migration process even more uncertain.

Note: This test is being carried out on azure cloud.

**Describe the solution you'd like**

- Demonstrate using exact commands in the documentation how the second backup location is created, including the authentication configuration, for each of the major cloud vendors (Azure, AWS, etc ...)
- Demonstrate the exact command(s) use to restore a backup from a particular storage location.

**Anything else you would like to add:**

The velero command to restore from a particular backup doesn't seem to include an option to specify the exact backup location (container) from which the backup must be extracted. It's hard to understand how the documented migration process will work without this feature.

Some clarification would be appreciated here.

**Environment:**

- Velero version (use `velero version`):

v1.1.0

- Kubernetes version (use `kubectl version`):

1.14.8

- Kubernetes installer & version:

AKS 1.14.8

- Cloud provider or hardware configuration:

Azure, Azure blob storage

- OS (e.g. from `/etc/os-release`):

Ubuntu Linux",,,,,,,,,,,,,,
2164,OPEN,Better document schedule logic,Area/Documentation,2021-02-22 12:56:38 +0000 UTC,nrb,Opened,,"From a [Slack question](https://kubernetes.slack.com/archives/C6VCGP4MT/p1576878205057100)

>consider a case I have made a backup schedule for daily backup at 3:00 am morning, now the Question .... Suppose on 21 Dec the Schedule was created at 1 am and 1st  backup was successful, so will it Schedule run  backup again same day at 3 am or will start running from next day.

We should have a document describing a few example schedule scenarios to help understand how schedules work, so that this question can be more easily answered in the future.",,,ashish,"
--
Copying @nrb's answer from slack:
> When you use velero create schedule, it shouldn't trigger a backup right away - it should instead wait until the next due period.
So if you created a schedule that runs at 3am at 1am Dec 21, it would not create the first backup until 3am Dec 21.
If you created the schedule at 1am Dec 21, then ran velero create backup --from-schedule <schedule name> at 1:01am Dec 21, it would trigger a backup using the specified schedule's backup parameters and run at the next schedule time. So it would run starting at 1:01am and 3am Dec 21, assuming the 1:01am run was finished.
--

--
https://kubernetes.slack.com/archives/C6VCGP4MT/p1576878205057100
--
",,,,,,,,,,
2143,OPEN,Allow cross-namespace restore of CSI snapshots,Area/CSI; P2 - Long-term important,2021-02-03 17:44:06 +0000 UTC,nrb,In progress,,"PR #1779 allowed users to restore a snapshot into a different namespace if they were remapping that namespace during a restore.

In order to maintain feature parity, Velero's CSI support should allow users to do the same.

This will likely entail cloning at least the associated `VolumeSnapshot`, and maybe copying the `VolumeSnapshotContent` object, as these two are often in a 1:1 relationship and the `VolumeSnapshotContent` object is what holds the `snapshotHandle` reference.",,,nrb,"
--
Note that native CSI support for this is planned, but I do not know where it is in the roadmap yet.
--

--
https://github.com/kubernetes/enhancements/pull/643 is the upstream KEP, and is currently planned for design (but not implementation) during the v1.18 cycle

Whoops: https://github.com/kubernetes/enhancements/pull/1112 is the most up-to-date KEP, 643 was closed.
--

--
Don't forget KEP 1112 - that may be more relevant.
--

--
I think we should look at moving this out of v1.4's beta and into v1.5. Thoughts @ashish-amarnath and @skriss?
--
",ashish,"
--
Related upstream issues:
https://github.com/kubernetes-csi/external-snapshotter/issues/274
https://github.com/kubernetes-csi/external-snapshotter/issues/273
--

--
From the outset, it seems like a simple change to address this. Need to check some of my assumptions though. I am OK w/ moving this out of 1.4. 
--
",skriss,"
--
I'm fine with considering it non-release-blocking, yeah.
--
",sshende,"
--
Submitted a PR for review: https://github.com/vmware-tanzu/velero-plugin-for-csi/pull/82
It is a very small change.
--
",arianitu,"
--
Following up on this, we originally pushed the namespace remapping feature for in-tree volumes and want to help with getting the CSI one in as well. The namespace remapping feature on the original did lead to some security issues, so I can help review remapping behaviour in the CSI plugin as well.

Are there any blockers on this or need assistance? Is the PR from @sshende-catalogicsoftware a valid fix?

--
",,
2134,OPEN,Openshift randomizes the GID,Area/Documentation; Good first issue; Help wanted; Restic,2021-02-19 15:12:13 +0000 UTC,carlisia,Opened,,"It would be helpful to document that OS randomizes the gid. For more context:

https://github.com/vmware-tanzu/velero/issues/2023",,,philipp1992,"
--
Corresponding openshift documentation. 

https://docs.openshift.com/enterprise/3.1/install_config/persistent_storage/pod_security_context.html#fsgroup

--
",a,"
--
We should add a note to the Restic Openshift install section, https://velero.io/docs/v1.5/restic/#install-restic to note that Openshift randomizes group IDs, so when restoring a backup, it will have a randomized GID unless you set the GID. We should also point users to the opeenshift docs that explain this in more detail (see previous comment) 
--
",,,,,,,,
3139,OPEN,Provide multi-arch Docker images for AWS plugin,Area/Cloud/AWS; Enhancement/Dev; Help wanted,2021-01-05 03:08:41 +0000 UTC,onedr0p,Opened,,"See https://github.com/vmware-tanzu/velero/issues/1645
and
https://github.com/vmware-tanzu/velero/pull/1768",,,firethestars,"
--
Hey, I have issued a [pull request](https://github.com/vmware-tanzu/velero-plugin-for-aws/pull/36) to hopefully solve the arm portion of this issue. I have tested it on my own arm k8s cluster, and it seems to enable the aws plugin without issue. Not got any power systems to try against, so someone else would need to get ppc64le. 
--
",autarchprinceps,"
--
I can confirm the velero/velero-plugin-for-aws:main image works fine on my cluster on AWS Graviton instances with velero/velero:v1.5.2 (which also has this feature), but it would be nice, if you could do the same for versioned tags as well.
--
",,,,,,,,
2128,OPEN,Feature request: Add velero_build_info metric to metrics,Enhancement/User; Metrics; P3 - Wouldn't it be nice if...,2019-12-13 15:51:41 +0000 UTC,nralbers,Opened,,"As a user, I want to keep track of LCM on components. A build_info metric containing build and version information is helpful in tracing what version is running in my environment, and allows me to keep track of how far behind the current latest version I'm running.",,,,,,,,,,,,,,
2114,OPEN,Can not delete job before it starts,Bug; Good first issue; Help wanted; P3 - Wouldn't it be nice if...,2021-03-10 14:40:23 +0000 UTC,dns2utf8,Opened,,"**What steps did you take and what happened:**
Create two backups A and B one after the other:
```bash
velero backup create A 
velero backup create B
```

Get the jobs:
```bash
velero backup get
# ...
# A                            InProgress
# B                            New
```

While A is running, delete B:
```bash
velero backup delete B
```

After A is finished B gets executed.

**What did you expect to happen:**
I expected the B job to not get executed or an error message like ""one can only delete completed or failed jobs"".

**Anything else you would like to add:**

**Environment:**

* velero version 1.2.0
- OS (e.g. from `/etc/os-release`): Ubuntu 18.04.3 LTS
",,,skriss,"
--
As far as I can tell, this is because backup B doesn't have a `spec.storageLocation` assigned when it's created, so the backup deletion controller errors out.  The backup controller assigns the default location to the backup once it starts processing.

We should change something in the deletion controller to either (a) not bother trying to get the BackupStorageLocation for new backups (since there's no need to access them), or (b) assume the default BSL if none is assigned to the backup when trying to delete.
--

--
A viable workaround would be to `kubectl -n velero delete backup B` for a `New` backup since that wouldn't go through the deletion controller.
--
",MadhavJivrajani,"
--
Hi! I'm a beginner, I was hoping to try this if possible, any advice on how I can get started? 
--
",,,,,,,,
2113,OPEN,Using fsfreeze leads to hanging system on XFS,Area/Documentation,2019-12-17 15:31:20 +0000 UTC,dns2utf8,In progress,,"**What steps did you take and what happened:**
I have a [test application that simulates a DB](https://gitlab.com/dns2utf8/multi_file_writer) by writing to 4 different files at the same time.
Sometimes velero, restic or fsfreeze crash and the whole setup stops.

**What did you expect to happen:**
The unfreeze must happen when
* [x] backup succeeds (about 50% chance)
* [x] backup fails (less than 5% chance)
* [ ] backup worker gets killed/crashes

**The output of the following commands will help us better understand what's going on**:
`velero backup get` lists the broken backups as `InProgress` even after the worker has been restarted.

The backing storage minio does not contain any files of that job.

* `kubectl logs deployment/velero -n velero`
* `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
* `velero backup logs <backupname>` not available
* velero backup describe snapshot-test-2019-12-10-1409 --details
```
Name:         snapshot-test-2019-12-10-1409
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:  New

Namespaces:
  Included:  snapshot-test
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  

Snapshot PVs:  false

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  0

Started:    <n/a>
Completed:  <n/a>

Expiration:  <nil>

Resource List:  <error getting backup resource list: timed out waiting for download URL>

Persistent Volumes: <none included>
```


**Anything else you would like to add:**
The storage was installed with this file and these commands:
```
apiVersion: velero.io/v1
kind: VolumeSnapshotLocation
metadata:
  name: hpe-csp
  namespace: velero
spec:
  provider: hpe.com/snapshotter
  config: {
       ""secret-name"": ""nimble-secret"",
       ""secret-namespace"": ""kube-system""
  }
```
```bash
kubectl apply -f hpe_crd_0-snapshotter.yml
kubectl create -f https://raw.githubusercontent.com/hpe-storage/velero-plugin/master/nimble-csp.yaml
velero plugin add hpestorage/velero-hpe-blockstore:beta --image-pull-policy Always
```

**Environment:**

- Velero version: 1.2.0 (Git commit: 5d008491bbf681658d3e372da1a9d3a21ca4c03c)
- Velero features (use `velero client config get features`): features: <NOT SET>
- Kubernetes version (use `kubectl version`): Client Version: version.Info{Major:""1"", Minor:""15"", GitVersion:""v1.15.6"", GitCommit:""7015f71e75f670eb9e7ebd4b5749639d42e20079"", GitTreeState:""clean"", BuildDate:""2019-11-13T11:20:18Z"", GoVersion:""go1.12.12"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""15"", GitVersion:""v1.15.3"", GitCommit:""2d3c76f9091b6bec110a5e63777c332469e0cba2"", GitTreeState:""clean"", BuildDate:""2019-08-19T11:05:50Z"", GoVersion:""go1.12.9"", Compiler:""gc"", Platform:""linux/amd64""}

- Kubernetes installer & version:
- Cloud provider or hardware configuration: Intel Xeon on worker, HPE storage
- OS (e.g. from `/etc/os-release`): 18.04.3 LTS (Bionic Beaver)

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: snapshot-test
  labels:
    app: multi-file-writer

---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: mfw-volume
  namespace: snapshot-test
  labels:
    app: multi-file-writer
spec:
  #storageClassName: <YOUR_STORAGE_CLASS_NAME>
  #storageClassName: generic-nimble
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 250Mi

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-file-writer
  namespace: snapshot-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: multi-file-writer
  minReadySeconds: 5
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: multi-file-writer
      annotations:
        pre.hook.backup.velero.io/container: fsfreeze
        pre.hook.backup.velero.io/command: '[""/sbin/fsfreeze"", ""--freeze"", ""/generated""]'
        post.hook.backup.velero.io/container: fsfreeze
        post.hook.backup.velero.io/command: '[""/sbin/fsfreeze"", ""--unfreeze"", ""/generated""]'
    spec:
      volumes:
        - name: mfw-volume
          persistentVolumeClaim:
           claimName: mfw-volume
      containers:
      - name: multi-file-writer
        image: registry.gitlab.com/dns2utf8/multi_file_writer:0.2.1
        imagePullPolicy: Always
        env:
            - name: MFW_RUN_FOREVER
              value: ""yes""
            - name: MFW_DELAY
              value: ""0.01""
        resources:
          requests:
            cpu: ""4""
            memory: 128M
          limits:
            cpu: ""8""
            memory: 256M
        volumeMounts:
        - mountPath: ""/generated""
          name: mfw-volume
          readOnly: false
      - name: fsfreeze
        image: velero/fsfreeze-pause:latest
        imagePullPolicy: Always
        securityContext:
          privileged: true
        volumeMounts:
          - mountPath: ""/generated""
            name: mfw-volume
            readOnly: false
```",,,dns2utf8,"
--
I did some more research and found a possible question:

* Does one need to mount the PV with the `noatime` attribute?

Best regards
--

--
PS: [one log file came in the end](https://gist.github.com/dns2utf8/577fb666f53b07ec2ba06b5637aa56bd)
--

--
Today I reached the following conclusion:

1. If the filesystem is mounted with `relatime` a `read(...)` from any program will trigger a `write( fd->atime )` if said file has been in the process of being written to while `fsfreeze --freeze ...` occurred, resulting in a complete dead-lock of restic.
    1. To resolve this one has to `fsfreeze --unlock ...` by hand before the timeout
    2. Or delete all pods inside the velero namespace as well as unfreezing the affected pod and finally delete the backup because it will never loose its `InProgress` state
2. With Ubuntu 18.04.3 LTS one can semi randomly trigger a bug with fsfreeze and reads on xfs:
    1. Write multiple MB to a file (eg. 100MB) while after one or two MB freeze the filesystem from the sidecar pod
    2. From the sidecar pod, issue multiple `strace tail /generated/data/0.txt`
    3. After a couple of tries strace shows that the `read(...)` works but `close(...)` hangs
    4. From now on all `read(...)` operations are blocked until the freeze is lifted

IMHO The first point should be documented since one can prevent that freeze by adding the `mountOption`
```bash
kubectl edit storageclasses.storage.k8s.io
```
```yaml
- allowVolumeExpansion: true
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    ...
  mountOptions:
  - noatime
  parameters:
    ...
```
--

--
Dear all

After some more investigating I found three blockers for this:

1. mounts with `relatime` must deadlock if the file has been written to while the fsfreeze happend -> Fixable with `noatime` hint in the docs.
2. [XFS kernel bug](https://bugzilla.kernel.org/show_bug.cgi?id=205833) where non-touching operations trigger writes and lock the process.
3. velero hangs completely if one job hangs. (Maybe the new design with multiple workers resolves this too?)

The second one needs somebody with kernel experience to get motivated/paid and improve the situation. Maybe you know somebody inside vmware?

Best,
Stefan
--
",,,,,,,,,,
2109,OPEN,Restic backup fails after changing backup location,Area/Documentation; Help wanted; Restic,2021-02-19 17:37:37 +0000 UTC,vitobotta,In progress,,"**What steps did you take and what happened:**
I migrated my backup data from DigitalOcean Spaces to Exoscale Object Storage. The backups work fine apart from the Restic backups, which fail with `invalid id ""a476b9b4"": no matching ID found`. The first time I just changed the s3 settings in the backup location, but I've also tried uninstalling Velero completely and reinstalling with the new settings, same error.

**What did you expect to happen:**
I would expect Restic to continue backing up to the new location.

**The output of the following commands will help us better understand what's going on**:

* `kubectl logs deployment/velero -n velero`
[https://privatebin.net/?aa037737da086754#2mbShce7o7Lr3epArNoJE3DQpebzuSCuaNpSLqdQVoEG](https://privatebin.net/?aa037737da086754#2mbShce7o7Lr3epArNoJE3DQpebzuSCuaNpSLqdQVoEG)

* `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
[https://privatebin.net/?f0f842793e25bffc#ECcVMJLFqgo71EDwcppDHQwbVsJyayRVt2vgE4p3mtvu](https://privatebin.net/?f0f842793e25bffc#ECcVMJLFqgo71EDwcppDHQwbVsJyayRVt2vgE4p3mtvu)
[https://privatebin.net/?879d856ac3e4899e#5hj55e34NgwjFpfXNQTn9jL3kfYZMYd6LYYbjn13c7Xd](https://privatebin.net/?879d856ac3e4899e#5hj55e34NgwjFpfXNQTn9jL3kfYZMYd6LYYbjn13c7Xd)

* `velero backup logs <backupname>`
[https://privatebin.net/?8593023f185b18c0#7jcCVdAZKpSoJG9JAKF7pLDUZyAniTd8PNf5n3Wv5mTT](https://privatebin.net/?8593023f185b18c0#7jcCVdAZKpSoJG9JAKF7pLDUZyAniTd8PNf5n3Wv5mTT)

**Anything else you would like to add:**
I thought there might be a problem with the Restic repository but I tried connecting to it from my Mac and could run check/snapshots commands without any problems.


**Environment:**

- Velero version (use `velero version`): 
```
Client:
	Version: v1.2.0
	Git commit: -
Server:
	Version: v1.2.0
```

- Velero features (use `velero client config get features`): 
```
features: <NOT SET>
```

- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""16"", GitVersion:""v1.16.2"", GitCommit:""c97fe5036ef3df2967d086711e6c0c405941e14b"", GitTreeState:""clean"", BuildDate:""2019-10-15T23:42:50Z"", GoVersion:""go1.12.10"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""16"", GitVersion:""v1.16.3"", GitCommit:""b3cbbae08ec52a7fc73d334838e18d17e8512749"", GitTreeState:""clean"", BuildDate:""2019-11-13T11:13:49Z"", GoVersion:""go1.12.12"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Kubernetes installer & version:
Rancher 2.3.3

- Cloud provider or hardware configuration:
4 cores, 8GB RAM, 200GB storage on each node

- OS (e.g. from `/etc/os-release`):
Ubuntu 18.04.3 LTS

",,,vitobotta,"
--
Uhm I think I know what happened. 

When I updated the backup location for the new provider Exoscale, I did a backup to test BUT I didn't update the restic repo resource as well, so that was still pointing out to the bucket in DigitalOcean. I didn't restart the restic pods either, so I guess that test backup succeeded but put the Velero data into Exoscale and the restic data into DigitalOcean, completing successfully...

After deleting the DigitalOcean bucket, but still without updating repo resource nor restarting the restic pods, the daily backup failed this morning; I am not sure why, but even after updating the restic repo resource with the new restic identifier and restarting the restic pods, the subsequent backups failed.

Reinstalling Velero didn't help, because there seems to be some connection between the restic snapshots I guess because of the incremental backups? Can you confirm?

Anyway I deleted the failed backups AND the last backup that succeeded but split the data into the two different providers, and then I took a couple of backup which completed successfully, including restic volumes.

It would be nice if the documentation clarified a little what to do to change a backup location and what can go wrong.
--

--
@a-mccarthy I think so, thanks :)
--
",a,"
--
To summarize, when updating the backup location, 
1. Update the location in Velero 
2. Update the Restic repo location
3.  restart the Restic pods

Are those all the steps we should clarify?
--
",,,,,,,,
3146,OPEN,GCP Create Snapshot Across Projects,Area/Cloud/GCP; Enhancement/User; Help wanted; Volumes,2020-12-07 23:46:58 +0000 UTC,brondum,In progress,,"**Describe the problem/challenge you have**
Trying to create a snapshot across projects in GCP(gke).

**Describe the solution you'd like**
Would like to be able to specify which project the snapshot should be stored
e.g. creating a centralized backup project with both backups and snapshots.

**Environment:**

- Velero version (use `velero version`): 1.2.0
- Kubernetes version (use `kubectl version`): 1.14
- Cloud provider or hardware configuration: GKE
",,,skriss,"
--
@brondum you should be able to do this by specifying the `project` config key in your VolumeSnapshotLocation, under `spec.config`.  See https://github.com/vmware-tanzu/velero-plugin-for-gcp/blob/master/volumesnapshotlocation.md for some documentation. You'll also need to ensure you have the IAM set up appropriately. 

A similar use case is documented here: https://github.com/vmware-tanzu/velero-plugin-for-gcp/blob/master/examples/gcp-projects.md.


--

--
Looking at the code, it looks like the ""volume project"" is based on the IAM credentials - so your IAM account should be in the same project as where the volumes are. The `project` key optionally defines the ""snapshot project"" -- so where the snapshots should be created/restored from.  Your IAM account will need the appropriate permissions to create snaphots there.

If you look at https://github.com/vmware-tanzu/velero-plugin-for-gcp/blob/master/velero-plugin-for-gcp/volume_snapshotter.go#L227 you can see that the disk info is gotten from the ""volume project"", and subsequently the snapshot is created in the ""snapshot project"".

It looks like this should be valid on both backup and restore, though of course possible there's a bug.
--

--
@brondum if the service account is in the same project as the cluster/disks, and you specify `project` in your `VolumeSnapshotLocation` config, then this *should* work -- the feature was implemented to address exactly that scenario. Is that not working?
--

--
@brondum OK, I'll have to dig into this some more.
--

--
transferring to GCP plugin repo
--

--
@brondum sorry for the delay here. I attempted to reproduce this, and as far as I can tell, things worked as expected, with the caveat that I don't actually have a second project set up to create a snapshot in, but I got to an error telling me that the ""snapshot project"" didn't exist when trying to create a snapshot.

My `cloud-credentials` secret contains the GCP service account JSON, including the following line:

```
...
""project_id"": ""<project where my disk is located>"",
...
```

And my `VolumeSnapshotLocation` looks like the following:

```
apiVersion: velero.io/v1
kind: VolumeSnapshotLocation
metadata:
  labels:
    component: velero
  name: default
  namespace: velero
spec:
  config:
    project: snap-proj
  provider: gcp
```

The only error I end up getting with this setup is:

```
time=""2020-01-17T16:15:24Z"" level=error msg=""Error backing up item"" backup=velero/nginx-2 error=""error taking snapshot of volume: rpc error: code = Unknown desc = googleapi: Error 404: Failed to find project snap-proj, notFound"" group=v1 logSource=""pkg/backup/resource_backupper.go:288"" name=nginx-deployment-589cdf7bc4-7qtmc namespace=nginx-example resource=pods
```

Which indicates to me that it's correctly trying to create a snapshot in the `snap-proj` project.

Can you take a look and confirm that your config matches this? It sounds from your description like the `project_id` in your credentials file (in your `cloud-credentials` secret) contains the ""snapshot project"", rather than matching the project where the disk is.

--

--
@brondum @kunickiaj now that I'm looking again, I'm pretty sure that this use case (create the snapshot in a different project than where the disk is) is not currently supported, as you've found.

It doesn't seem like GCP actually supports this, irrespective of velero - can you confirm this?

If true, then you would need a separate ""copy"" operation to copy the snapshot off to the project you want to store it in.
--

--
@kunickiaj gotcha - so unfortunately still not supported, but at least this one should be implementable in velero.  This is a different issue, or at least a different variation, than what @brondum originally reported.

We'd probably want to add yet another config param to the BSL definition, that if present, overrides the ""volume project"" (which is by default gotten from where the service account is). I think this needs to be separate from the existing `project` config param, since that defines where to look for the snapshot during a restore, which isn't necessarily the same as where the disks are.
--

--
is anyone interested in working on a PR for this? I'm happy to provide input, if so.
--
",brondum,"
--
@skriss  Thank you for the quick response. As far as i can see from the examples that is only if you want to restore between projects. What i am trying to achieve is to make one ""backup project"" where other projects store their backups and snapshots.

When i set the backup project-id in the `project` spec in volumesnapshotlocation and creates a backup it looks for the disk in the backup-project rather than in the actual project where the disk is located.
--

--
@skriss yeah that is quite clear when looking at that line of code.
I will go ahead and test it next week.
But, would it be possible to maybe and a config option of ""volume project"" as well, such you can define if you want to centralized accounts as well ? 
--

--
@skriss Just to provide some more info on this one.
I actually forgot that our service account which handles the GCP snapshots is in the same project as the cluster. Meaning that the ""project"" spec overwrites the location.
Makes sense ? 
--

--
@skriss Exactly the scenario here. Unfortunately that does not seem to work.

Snippet from the log, some values have been substituted.
```time=""2019-12-11T23:00:36Z"" level=info msg=""Getting volume information"" backup=velero/sched-20191211230034 group=v1 logSource=""pkg/backup/item_backupper.go:466"" name=pvc-f1477372-11e2-11ea-bb4b-42010aa4009b namespace= persistentVolume=pvc-f1477372-11e2-11ea-bb4b-42010aa4009b resource=persistentvolumes volumeID=<disk id>

time=""2019-12-11T23:00:37Z"" level=info msg=""Snapshotting persistent volume"" backup=velero/sched-20191211230034 group=v1 logSource=""pkg/backup/item_backupper.go:472"" name=pvc-f1477372-11e2-11ea-bb4b-42010aa4009b namespace= persistentVolume=pvc-f1477372-11e2-11ea-bb4b-42010aa4009b resource=persistentvolumes volumeID=<disk id>
time=""2019-12-11T23:00:38Z"" level=info msg=""1 errors encountered backup up item"" backup=velero/sched-20191211230034 group=v1 logSource=""pkg/backup/resource_backupper.go:284"" name=server-1 namespace= resource=pods
time=""2019-12-11T23:00:38Z"" level=error msg=""Error backing up item"" backup=velero/sched-20191211230034 error=""error taking snapshot of volume: rpc error: code = Unknown desc = googleapi: Error 404: The resource 'projects/<ID of the backup dest>/zones/europe-west4-c/disks/<disk id>' was not found, notFound"" group=v1 logSource=""pkg/backup/resource_backupper.go:288"" name=server-1 namespace= resource=pods
--

--
@skriss I will try and set time aside for this monday, see if i can recreate the issue post the details here :)
--

--
@skriss Finally got to test the stuff, sorry for the delay here.

Okay so i double checked my credentials file.
it contains `the project_id` where both my cluster and my disks reside.

Then i added 
```yaml
apiVersion: velero.io/v1
kind: VolumeSnapshotLocation
metadata:
  name: default
  namespace: velero
spec:
  config:
    project: <THE PROJECT WHERE IT SHOULD END UP>
    snapshotLocation: europe-west4
  provider: velero.io/gcp
```

Outcome:
```
time=""2020-01-22T13:42:21Z"" level=info msg=""1 errors encountered backup up item"" backup=velero/testsnap group=v1 logSource=""pkg/backup/resource_backupper.go:284"" name=<SOMENAME> namespace= resource=pods
time=""2020-01-22T13:42:21Z"" level=error msg=""Error backing up item"" backup=velero/testsnap error=""error taking snapshot of volume: rpc error: code = Unknown desc = googleapi: Error 404: The resource 'projects/<THE PROJECT WHERE IT SHOULD END UP>/zones/europe-west4-a/disks/<SOMEDISK>--pvc-c20bc2ef-1765-11ea-a35b-4201ac1f800a' was not found, notFound"" group=v1 logSource=""pkg/backup/resource_backupper.go:288"" name=<POD_NAME> namespace= resource=pods
```

So from what i can se, it looks for the disk in the destination project instead of the source project ? 
--

--
@skriss  Sorry for now getting back to you before now.
Finally did some checkups, and i think you are right, in order for my scenario til work a separate copy job needs to be initiated.

And actually it does not seem to be possible within GCP to transfer snapshots, only images can be transferred as far as i can see?
--
",kunickiaj,"
--
I'm running into the same issue, where I'm using one SA to try to back up multiple projects. During the backup its looking for the correct disk ID, but in the wrong project (the project the SA belongs to), and then fails to create the snapshot. The `config.project` seems to be irrelevant here.
--

--
To clarify, I'm not wanting to create the snapshot in a different project than the disk. Velero always _expects_ that the project the _ServiceAccount_ is in, is the _same_ location as the disks. However a ServiceAccount can be from any project and have access to multiple projects.

`my-service-account@project-abc.iam.gserviceaccount.com` is provided as cloud credentials for a Velero server running in project `project-def`. Disks are located in `project-def` and so snapshots should be in `project-def` but Velero looks for `project-abc/region/disk-xyz` rather than `project-def/region/disk-xyz`.

In this case we have one SA that can access other projects. Velero assumes that the the project id in `my-service-account@project-id.iam.gserviceaccount.com` is the same as where its running and looks for the disks there, rather than the project Velero is actually running in.
--
",relgisri,"
--
That's exactly what happened me today, after looking further in my current setup. Therefore I have to create ServiceAccounts per Project and install Velero with the corresponding credential files of each ServiceAccount.
I would really welcome a feature to allow one ""Master"" ServiceAccount to do the backups for multiple projects.
--
",,,,
2098,OPEN,Implement abort running backup job,Enhancement/User; P2 - Long-term important,2021-02-12 23:16:53 +0000 UTC,dns2utf8,Opened,,"**Describe the problem/challenge you have**
If one submits a backup job and forgets the narrowing select the system is blocked.


**Describe the solution you'd like**
I would like to be able to abort a job so the queue continues to work.


**Anything else you would like to add:**
Having short jobs that run every minute pile up very quickly


**Environment:**

- velero version
Client:
        Version: v1.2.0
        Git commit: 5d008491bbf681658d3e372da1a9d3a21ca4c03c
Server:
        Version: v1.2.0

- kubectl version
Client Version: version.Info{Major:""1"", Minor:""15"", GitVersion:""v1.15.6"", GitCommit:""7015f71e75f670eb9e7ebd4b5749639d42e20079"", GitTreeState:""clean"", BuildDate:""2019-11-13T11:20:18Z"", GoVersion:""go1.12.12"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""15"", GitVersion:""v1.15.3"", GitCommit:""2d3c76f9091b6bec110a5e63777c332469e0cba2"", GitTreeState:""clean"", BuildDate:""2019-08-19T11:05:50Z"", GoVersion:""go1.12.9"", Compiler:""gc"", Platform:""linux/amd64""}

- Kubernetes installer & version: v1.15.3
- Cloud provider or hardware configuration: Intel(R) Xeon(R) CPU E5-2650
- OS (e.g. from `/etc/os-release`): Ubuntu 18.04.3 LTS (Bionic Beaver)
",,,skriss,"
--
Adding to this, it would be nice to be able to:
- abort a running job
- detect and either fail or restart jobs that are hung

We might be able to get some of this behavior for free if we move to running backups/restores in their own Pods/Jobs, per #1653 
--
",d3473r,"
--
I started a backup job on a tainted node on which no restic workload was running until i added the appropriate toleration key.
As i couldn't abort the running backup I had to wait ~4 hours until the job runned into the timeout :/
The abort feature would be really helpfull in a misshap like this.
--
",,,,,,,,
3154,OPEN,Least privileges of Service Principal,Area/Cloud/Azure; Area/Documentation; Help wanted,2020-12-08 00:03:45 +0000 UTC,jaygridley,Opened,,The documentation states that the Service Principal used with Velero should have a role Contributor. This role is quite open and some company policies do not allow this. The documentation should be updated to state the least privileges for the Service Principal.,,,skriss,"
--
agreed, it would be great to refine the privileges here. Would love some help on this issue!
--
",fl,"
--
Does it truly need Subscription wide access or can it be scoped to just the Resource Group?
--
",pietervincken,"
--
It requires permissions for (at least) the storage account and the disks to snapshot them right? I'm not sure which other permissions are required. 
--
",prutsert,"
--
I've been able to narrow it down to _Contributor_ role on the _Resource Groups_ that are involved. Those are the Resource Group of the AKS cluster: `$AZURE_RESOURCE_GROUP`, and the Resource Group of the backup storage: `$AZURE_BACKUP_RESOURCE_GROUP`.
This could probably be even more restrictive on the backup storage Resource Group, but as long as it only contains data created and used by Velero anyway, this works for me. 

```
AZURE_CLIENT_SECRET=`az ad sp create-for-rbac --name ""velero"" --role ""Contributor"" \
   --query 'password' -o tsv --scopes \
 /subscriptions/${AZURE_SUBSCRIPTION_ID}/resourceGroups/${AZURE_BACKUP_RESOURCE_GROUP} \
 /subscriptions/${AZURE_SUBSCRIPTION_ID}/resourceGroups/${AZURE_RESOURCE_GROUP}`
```

Both Resource Groups are in the same Subscription, in my case. I'm not sure if this would work if they are in separate Subscriptions, and you would change `/subscriptions/${AZURE_SUBSCRIPTION_ID}/resourceGroups/${AZURE_BACKUP_RESOURCE_GROUP}` to `/subscriptions/${AZURE_BACKUP_SUBSCRIPTION_ID}/resourceGroups/${AZURE_BACKUP_RESOURCE_GROUP}`

--
",fredgate,"
--
Yes configuring the service principal with the least privileges is almost essentials. Because having a service principal with full access on a subscription is not acceptable by company policy of some of our customers.
Already being able to limit to the resources group is appreciable. But maybe we can be even more precise.
--
",seboss666,"
--
I just found myself getting involved with deploying velero on a client-deployed cluster with no direct access to their azuread, I'm pretty sure that if I ask them to deploy a service principal with contributor role just to perform aks related backups, they will just laugh at me.

Strangely, the gcp documentation is far more precise on IAM policies : https://github.com/vmware-tanzu/velero-plugin-for-gcp. Why is this not the case here ?
--

--
Hello michael,
are you able to generate dynamic volume snapshot with that coverage ? I can see why it's sufficient for object/manifests backup, and it's great to know that, but I'm afraid it may be too narrow for the snapshot feature :/
--
"
3153,OPEN,Support for zone redundant snapshot of Azure Disk,Area/Cloud/Azure; Help wanted; Volumes,2021-03-17 14:40:40 +0000 UTC,jaygridley,Opened,,There is a option to create zone redundant snapshots of Azure Disks in regions with availabiltiy zones see https://azure.microsoft.com/en-us/updates/azure-managed-snapshots-images-ga.,,,stromvirvel,"
--
Is there an update on this?
--
",ryanmcafee,"
--
Any update on this?
--
",antonmatsiuk,"
--
any news on this? The feature would be highly appreciated.
--
",,,,,,
2090,OPEN,allow for kustomize customizations before objects are restored,Enhancement/User; P2 - Long-term important,2020-01-17 19:46:59 +0000 UTC,grozan,Opened,,"**Describe the problem/challenge you have**
let's take a cluster migration scenario. A workload is to be moved from clusterA to clusterB.
That workload uses ""things"" which are not available (or simply different) on clusterB.

Examples would be:
- pods with NFS volumes. The NFS paths are different between the DEV and TEST clusters
- private registries, which are different between DEV and TEST
This kind of things.

Basically a backup taken on clusterA cannot be restored on clusterB unless the restored YAML is edited first, to update those few fields. In velero the restore happens as being in progress. a `kubectl get pods` shows the restored pod cannot start.

**Describe the solution you'd like**
in the restore command, I'd like to be able to specify some customization to be applied to the YAML before it gets pushed to the cluster. Kustomize being kind of built-in, I guess it would be good candidate.
so the restore command would look something like
`velero restore create --from-backup <BACKUP-NAME> --kustomize /path/to/overlays/folder`


",,,skriss,"
--
xref #474 
--

--
@debianmaster nobody's actively working on this right now - it would be great if you wanted to pick it up! Did you have any ideas about how to go about implementing yet?

Usually for a feature of this size, we write up a brief design doc first to make sure everyone's in agreement about the approach.  The document template is [here](https://github.com/vmware-tanzu/velero/blob/master/design/_template.md) and you can see some past docs in that directory as well.
--

--
@sseago yeah, you make good points about the overlap and we'd definitely need to figure those things out. In my mind, the goal is to have a lighter-weight way to apply changes to the objects being restored, that doesn't require writing and deploying a containerized go binary.  But I'm guessing that for more complex scenarios, plugins may still be required.  
--
",debianmaster,"
--
@skriss    is this being worked on by anyone?   we would like to join hands and contribute to this.
cc @dinesh 
--

--
@skriss  we are currently exploring kustomize based approach like suggested in thread.   we will do some initial study and come up with doc
--
",sseago,"
--
It seems like there's a fair amount of overlap between the proposed --kustomize functionality and what's currently being done with plugins. We are currently doing similar things to this with plugins -- registry paths, PV differences on restore, etc. Maybe the difference is that you'd write a plugin if there's some sort of logic that will always apply in terms of what you want to change on restore, and the kustomize approach would be used when the changes are determined based on user input at restore time. We do need to make sure these two things don't step on each other. For example, would the kustomize changes be applied *after* plugins run (which means the kustomize user input would need to know about the post-plugin view of the resources), or *before* they run (in which case the plugins would need to understand that they're looking at post-kustomize resources?
--
",,,,,,
2087,OPEN,Manage the restic cache dir more effectively,Bug; Restic,2019-12-17 16:09:54 +0000 UTC,crdemon09,In progress,,"**/scratch/.cache/restic/restic-check-cache-* folder is too huge**
Can we somehow disable a restic cache using velero?


**Solution**
velero restic --no-cache


**Anything else you would like to add:**
The size is growing up really quick while doing a schedule backup, with 4GB of neo4j database - I have /scratch folder in 2 days - 13G and then velero pod got ""Evicted""


**Environment:**

- Velero version 1.0.0
- Kubernetes version 1.14
- Cloud provider or hardware configuration:
- OS CentOS
",,,crdemon09,"
--
Small update - I think it's related to this [#1659](https://github.com/vmware-tanzu/velero/issues/1659)
Restic upon a crash - creating a restic-cache folder - making size inside a k8s pod growing up.
--
",skriss,"
--
We don't currently have a way to disable the restic cache; you could consider mounting a PV into the velero pod to use for the scratch dir, instead of using an empty dir.
--

--
I don't think there are really any downsides apart from the cost of the persistent storage. It's possible that you'll still run into issues where the cache eventually exceeds the size of the PV - I'm not sure off the top of my head.
--
",Davidsod,"
--
@skriss thanks for the answer. What would be the downsides in that case?
--
",,,,,,
2082,OPEN,when a backup storage location is deleted; remove associated backups & restic repos from the cluster,Enhancement/User; Help wanted; P2 - Long-term important; Restic,2021-01-13 22:49:49 +0000 UTC,grozan,Opened,,"**What steps did you take and what happened:**
cluster migration scenario, using velero v1.2.0.
On my new-cluster, I add a read-only second storage-location (containing the backups of the old-cluster). I restore my backups from the old-cluster to the new-cluster. When done, I delete the second storage-location on my new-cluster.

problem: `velero backup get` still shows me the backups I have on the storage-location I have now removed. The restic repositories are still known as well
`kubectl -n velero get backups.velero.io` and `resticrepositories.velero.io` still show stuff from the storage-location that does not show up anymore in the output of `kubectl -n velero get backupstoragelocations.velero.io`


**What did you expect to happen:**

after I run `kubectl -n velero delete backupstoragelocations.velero.io old-cluster`, I expect to not see anything related to `old-cluster`.

Also, the command-line tool should support a more intuitive `velero backup-location delete old-cluster` call IMO


",,,drazul,"
--
Also, as related topic, we can't delete a backup or snapshot location using the command like tool.

`velero backup-location -h` only shows `create` and `get` commands.


So I asume the associated backups are not deleted because the operator didn't implement that functionality.


Also when I try to remove the backups from a deleted storage location the deletion request fail because it didn't find the configured storage location

```bash
$velero version
Client:
        Version: v1.4.2
        Git commit: -
Server:
        Version: v1.4.0
```
--
",HaveFun83,"
--
same here
```
velero version   
Client:
	Version: v1.4.0
	Git commit: 5963650c9d64643daaf510ef93ac4a36b6483392
Server:
	Version: v1.4.2
```

when we delete the old backuplocation the velero pod log errors like:
```
time=""2020-10-23T14:14:18Z"" level=error msg=""Error checking repository for stale locks"" controller=restic-repository error=""error getting backup storage location: backupstoragelocation.velero.io \""main1\"" not found"" error.file=""/github.com/vmware-tanzu/velero/pkg/restic/common.go:235"" error.function=github.com/vmware-tanzu/velero/pkg/restic.GetCACert logSource=""pkg/controller/restic_repository_controller.go:141"" name=prod-main1-75cp7 namespace=velero
```

--
",carlisia,"
--
@drazul and @HaveFun83: we have recently added a sub command to delete a BSL (https://github.com/vmware-tanzu/velero/pull/3073). It'll be available with v1.6.

However, a BSL is only a representation of the storage, and deleting it will never delete the backups uploaded to that storage. This is intentional. 

@drazul: If you need to delete backups from a deleted BSL, create a new BSL pointing to that same storage.

As for the original request, which is to have a way to force delete backups associated with a BSL when that BSL is deleted, this is still in the backlog.
--
",,,,,,
2066,OPEN,[Epic] CSI snapshot integration,Area/CSI; Enhancement/User; Epic; P1 - Important,2021-03-10 02:28:53 +0000 UTC,skriss,In progress,,,,,skriss,"
--
we've got 3 issues under this epic, but we need to break this down further. To do as a team once we finalize design.
--

--
@dns2utf8 in v1.2, you can back up non-CSI volumes (e.g. EBS, Google PD, Azure Disk) using snapshots, or any other type of volume using restic (https://velero.io/docs/v1.2.0/restic/).

This issue covers integrating with the new CSI snapshot API, to be able to take snapshots of CSI-provisioned volumes.
--

--
@dns2utf8 that is correct. As of today, Velero can snapshot non-CSI volumes, or use restic to back up CSI volumes.
--
",dns2utf8,"
--
What is the status of this? Is it possible to backup the contents of a pvc with version 1.2.0?
--

--
Do I understand correctly that this means CSI volumes are not snapshoted as of today?
I would like to backup the PVs of a MariaDB and a PostgreSQL.
--

--
Thank you for your replies. I had some initial success with restic.
However, now the contents of the PV don't get into the s3 bucket anymore and the pods are stuck at the ""Terminating"" state.
I searched the web for a solution, is the `MountPropagation` still needed with the `securityContext: privileged: true`?
My test-setup is in this repo: https://gitlab.com/dns2utf8/multi_file_writer
--
",nrb,"
--
Currently waiting on upstream providers to add v2.0.0 controller/v1beta1 CRD support to their drivers. I've [logged an issue on the Amazon driver](https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/442) to start.

While we could proceed with the v1alpha1 code, I'd prefer to keep moving forward and help update drivers rather than move backward and end up updating a lot of code later.
--
",ashish,"
--
This is the issue with the GCP driver: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/457
--

--
There are 3 more issues pending in this epic. https://github.com/vmware-tanzu/velero/labels/CSI
Which are not going to make it for 1.5.
Punting the remaining issues to 1.6.
--
",,,,
2044,OPEN,Velero support for object name mapping as we have --namespace-mapping,Enhancement/User; Help wanted; P3 - Wouldn't it be nice if...,2020-02-21 16:53:52 +0000 UTC,abhinavsinha1991,Opened,,"**Describe the problem/challenge you have**

Currenlty, Velero doesn't support mapping restored Kubernetes objects with a different name, the way we have `--namespace-mapping old-ns:new-ns`


**Describe the solution you'd like**

It would  be good to have a feature like:

`velero restore create --from-backup my-ns-backup --mappings [{""pvcs"":[{""old-pvc"":""new-pvc""}]},{""secrets"":[{""old-secret"":""new-secret""}]}]`

**Anything else you would like to add:**

This way, I can restore an older version of a PVC on the existing namespace as a temp PVC, mount it to my application's pods and do file restorations by simple copying.

Suggestion welcome for the use-case.

**Environment:**

- Velero version (use `velero version`): `v1.2.0-beta.1`
- Kubernetes version (use `kubectl version`): 
```
   Client Version: v1.15.0
   Server Version: v1.14.6
```
- Kubernetes installer & version: `NA (using AKS)`
- Cloud provider or hardware configuration: `Azure`
- OS (e.g. from `/etc/os-release`): `Not sure`
",,,skriss,"
--
xref #474 
--

--
thanks for the request @abhinavsinha1991. You could use a plugin to implement this behavior today - the plugin could run during restores, and could use e.g. a `ConfigMap` to store mappings. It's certainly less convenient than having CLI flags/API fields, but maybe it's an option for you.

See https://velero.io/docs/v1.2.0/custom-plugins/ for more information if you're interested!
--
",bruce2409,"
--
We run stateful websites on our cluster, it would be handy to be able to restore the deployments under new names for automating staging environment creation.
--
",,,,,,,,
3152,OPEN,When using azure volume_snapshotter; env update from /credentials/cloud doesn't handle removed params,Area/Cloud/Azure; Bug; Good first issue,2020-12-08 00:00:03 +0000 UTC,sseago,Opened,,"**What steps did you take and what happened:**
I was testing Azure snapshot support, and making sure that things worked fine with AZURE_RESOURCE_GROUP omitted when backing up without snapshots, and that with snapshots, things worked properly with it set, and we were seeing appropriate error messages with it either unset or set to a bad value (wrong group or invalid group).

What I think I'm seeing is that when the Azure volume snapshotter updates the environment based on the current value of `/credentials/cloud`, it's properly handling changed env vars, but it's not handling removed env vars. If an env var is removed, the old stale value is used rather than it being removed.

What I did was first ran a backup (with snapshots) with the wrong resource group specified. I got the expected error in my velero logs that the volume was not found in that resource group.
Then I removed AZURE_RESOURCE_GROUP from my cloud-credentials secret. I made sure that the update was propagated into /cloud/credentials in the velero pod, and then I attempted the snapshot backup again. Instead of seeing the expected message about the missing required AZURE_RESOURCE_GROUP env var, I saw the old error about the wrong resource group. Upon restarting velero, I got the expected error message.

So that was the case of *removing* an env var from the secret. I then tried adding one -- the correct one this time -- and once the secret update propagated to the velero pod, my backup succeeded. To test whether *changing* an env var worked, I updated the secret again, this time to an invalid group, and I got an error message about the resource group being invalid. Changing it back to the right group resulted in a backup success again.


**What did you expect to happen:**
I expected removing an env var from the secret to result in it being removed from the environment the volume snapshotter runs in. Instead, removals don't have an effect, since it looks like velero is only updating the env vars it finds. It should probably remove any old values in the expected env var list if they've been removed from the secret.

This is the error message seen after removing AZURE_RESOURCE_GROUP and waiting for the update to propagate:
```
time=""2019-10-30T19:49:42Z"" level=error msg=""Error backing up item"" backup=openshift-migration/mysql-persistent-azure2-migration-hv7bs error=""error getting volume info: rpc error: code = Unknown desc = compute.DisksClient#Get: Failure responding to request: StatusCode=404 -- Original Error: autorest/azure: Service returned an error. Status=404 Code=\""ResourceNotFound\"" Message=\""The Resource 'Microsoft.Compute/disks/sseago1-ld57c-dynamic-pvc-76d7cfae-f5aa-11e9-81dd-000d3a94127f' under resource group 'Velero_Backups' was not found.\"""" group=v1 logSource=""pkg/backup/resource_backupper.go:264"" name=mysql namespace=mysql-persistent resource=persistentvolumeclaims
```
This is the error message seen after restarting velero (and what would have been expected in the first place):
```
time=""2019-10-30T20:10:00Z"" level=error msg=""Error getting volume snapshotter for volume snapshot location"" backup=openshift-migration/mysql-persistent-azure2-migration-v8gcx error=""rpc error: code = Unknown desc = unable to get all required environment variables: the following keys do not have values: AZURE_RESOURCE_GROUP"" error.file=""/go/src/github.com/heptio/velero/pkg/cloudprovider/azure/volume_snapshotter.go:87"" error.function=""github.com/heptio/velero/pkg/cloudprovider/azure.(*VolumeSnapshotter).Init"" group=v1 logSource=""pkg/backup/item_backupper.go:413"" name=pvc-76d7cfae-f5aa-11e9-81dd-000d3a94127f namespace=mysql-persistent persistentVolume=pvc-76d7cfae-f5aa-11e9-81dd-000d3a94127f resource=persistentvolumeclaims volumeSnapshotLocation=migstorage-azure-eef7fd8b-f5b7-11e9-bb63-0610097184c8
```


**Environment:**

- Velero version (use `velero version`): 1.1
- Velero features (use `velero client config get features`):  <NOT SET>
- Kubernetes version (use `kubectl version`): 1.14
- Kubernetes installer & version:
- Cloud provider or hardware configuration: Azure
- OS (e.g. from `/etc/os-release`):
",,,,,,,,,,,,,,
2020,OPEN,enable sorting `velero backup get` output by fields other than name,Area/CLI; Enhancement/User; Help wanted; P3 - Wouldn't it be nice if...,2020-02-19 17:00:11 +0000 UTC,mad01,In progress,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)
Leats assume that you create three schedules. like this 
```
velero schedule create hourly-backup --schedule ""8 * * * *""
velero schedule create backup-hourly --schedule ""8 * * * *""
velero schedule create demo-backup --schedule ""8 * * * *""
```
after some backups have happened and you do `velero backup get` the order or the backups are named order or the name of the backup.  This will look something like this. (i have removed a few collums since they did not bring any info in this case)
```
NAME                             CREATED                          EXPIRES  
backup-hourly-20191028150014     2019-10-28 16:00:14 +0100 CET    28d      
backup-hourly-20191028130014     2019-10-28 14:00:14 +0100 CET    28d      
.
.
backup-hourly-20191015140039     2019-10-15 16:00:39 +0200 CEST   15d      
backup-hourly-20191015122036     2019-10-15 14:20:36 +0200 CEST   15d      
demo-backup-20191028150014       2019-10-28 16:00:25 +0100 CET    28d      
demo-backup-20191028100014       2019-10-28 11:00:23 +0100 CET    28d      
.
.
demo-backup-20191018140028       2019-10-18 16:00:37 +0200 CEST   18d      
demo-backup-20191018130153       2019-10-18 15:02:04 +0200 CEST   18d      
hourly-backup-20191029220815     2019-10-29 23:08:15 +0100 CET    29d      
hourly-backup-20191029170815     2019-10-29 18:08:15 +0100 CET    29d      
.
.
hourly-backup-20191018130828     2019-10-18 15:08:28 +0200 CEST   18d      
hourly-backup-20191018115634     2019-10-18 13:56:34 +0200 CEST   18d      
```


**What did you expect to happen:**
What is expected to happen is that i get a list sorted by created time. During an incident i want to have all the latest at the end of the list or at the top. That will not be the case if you have multiple schedules.

The default behaviour is to sort by name since that is what we get from k8s that might be okay with just a few but after a month there can be ~700 backups if you do hourly and then if they are mixed it can be hard to find the last one. 

what i would like is to have the option to sort by backup creation time even if default sorting is by name of backup

**Environment:**

- Velero version (use `velero version`): 
Client:
	Version: v1.1.0
	Git commit: a357f21aec6b39a8244dd23e469cc4519f1fe608
Server:
	Version: master

- Kubernetes version (use `kubectl version`): 
```
Client Version: version.Info{Major:""1"", Minor:""15"", GitVersion:""v1.15.3"", GitCommit:""2d3c76f9091b6bec110a5e63777c332469e0cba2"", GitTreeState:""clean"", BuildDate:""2019-08-19T12:36:28Z"", GoVersion:""go1.12.9"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14+"", GitVersion:""v1.14.7-gke.10"", GitCommit:""8cea5f8ae165065f0d35e5de5dfa2f73617f02d1"", GitTreeState:""clean"", BuildDate:""2019-10-05T00:08:10Z"", GoVersion:""go1.12.9b4"", Compiler:""gc"", Platform:""linux/amd64""}
```

- Kubernetes installer & version: gke
- Cloud provider or hardware configuration: google

",,,skriss,"
--
thanks for the request @mad01, we'll take a look! do you have any interest in working on a PR for this? 
--

--
I could see sorting by created timestamp being a better default. Sorting by name is nice in that it keeps all backups from a given schedule together, but other than that it can potentially be very unhelpful. @nrb @carlisia @prydonius thoughts?

Couple other thoughts:
- it would be nice to have a flag like `--sort-by`, similar to `kubectl get`, that allows passing field names in as args, so we can support sorting by arbitrary fields. I know `kubectl` uses JSONPath to specify these -- need to figure out if that's appropriate here.
- there's both the `creationTimestamp` in metadata, and the `startTimestamp` in the status. The former is Kubernetes-assigned whenever the CR is created. This could get weird if you're importing backups into a cluster from an object storage bucket - the `creationTimestamp` will be ~now, whereas the `startTimestamp` would be the time in the past when the backup was actually processed. On the other hand, `startTimestamp` won't be populated at all until a velero controller starts processing the custom resource.
--

--
xref #1780 
--
",mad01,"
--
took a look on the source. Could take a look and se if i can get something working. 

@skriss what would be the suggested way for this to work? Do you think the default should just be sort by created ts or as an option? 
--

--
based on the feedback on the issue. i don't think i know enough for get something working right now. if someone else like to pick it feel free  
--
",carlisia,"
--
Yes, this is a good idea. 

I think valid values for the `sort-by` flag should be the same columns that we get with `velero backup get`, which are `NAME   STATUS   CREATED   EXPIRES   STORAGE LOCATION   SELECTOR`.

If we do that ^, then the default sort should be by `created`, since `startTimestamp` is not one of the columns.
--
",prydonius,"
--
I would love to get a point to where `kubectl get` and `velero get` are the same. I wonder if it's worth exploring having `velero get` call out to the relevant `kubectl get` command, and that way we could leverage the sorting by kubectl. We could of course add some helper flags so users don't need to specify the JSONPath, but we can then translate it to JSONPath and pass it to `kubectl get`. What do you think?
--
",,,,
3138,OPEN,Cannot load AWS token file when using AWS IAM-backed service accounts,Area/Cloud/AWS; Area/Documentation; Enhancement/User; Help wanted,2020-12-07 23:28:56 +0000 UTC,geofffranks,Opened,,"**What steps did you take and what happened:**
We deployed velero v1.2.0-beta1 in an attempt to use AWS IAM backed Service Accounts in EKS, as described in vmware-tanzu/velero#1965.  When velero started, it failed with the following error:

```
time=""2019-10-28T19:46:05Z"" level=info msg=""Checking that all backup storage locations are valid"" logSource=""pkg/cmd/server/server.go:421""
An error occurred: some backup storage locations are invalid: error getting backup store for location ""default"": rpc error: code = Unknown desc = WebIdentityErr: unable to read file at /var/run/secrets/eks.amazonaws.com/serviceaccount/token
caused by: open /var/run/secrets/eks.amazonaws.com/serviceaccount/token: permission denied
```


**What did you expect to happen:**

Velero to start up and work

**The output of the following commands will help us better understand what's going on**:

* `kubectl logs deployment/velero -n velero`

```
time=""2019-10-28T19:46:05Z"" level=info msg=""Checking that all backup storage locations are valid"" logSource=""pkg/cmd/server/server.go:421""
An error occurred: some backup storage locations are invalid: error getting backup store for location ""default"": rpc error: code = Unknown desc = WebIdentityErr: unable to read file at /var/run/secrets/eks.amazonaws.com/serviceaccount/token
caused by: open /var/run/secrets/eks.amazonaws.com/serviceaccount/token: permission denied
```

**Anything else you would like to add:**

This looks similar to the issue described here: https://github.com/kubernetes-incubator/external-dns/pull/1185, so I applied the fix to our velero deployment yaml, and that resolved the issue. Is this something that can be added to the velero cli's auto-generated deployment yaml?

```
securityContext:
        fsGroup: 65534
```
",,,redradrat,"
--
I can confirm this. Adding the securityContext ""fixes"" the issue. @geofffranks thanks for pointing to this.
--
",skriss,"
--
thanks for reporting @geofffranks -- will take a more detailed look and decide how to proceed.
--

--
Transferring this to the AWS plugin repo. I think for now we probably want to just document this for AWS users using this setup.
--
",garystafford,"
--
Confirming as of today, 2/12/2020, this issue still exists, and the fix, referenced above by @geofffranks, still works.
--
",acegrader33,"
--
Issue still exists 3/25/2020, fix referenced above still works for resolving the listed error. 

However, depending on networking configuration, there can be an additional error where velero cannot reach the sts.amazonaws.com endpoint which prevents use of the AWS IAM-backed service accounts. This would be fixed by using a newer version of the aws-sdk though since additional environment variables become available to configure the STS endpoint in [v1.25.18](https://github.com/aws/aws-sdk-go/blob/master/CHANGELOG.md#release-v12518-2019-10-23). 

Are there any plans to update the plugin to use a newer version of the aws-sdk?
--
",airwalk225,"
--
Confirmed that this issue still exists.

The fix suggested by @geoffranks still works.
The fix also works if you are installing via the Helm chart.
--
",carlisia,"
--
Update:

As per @zubron on a different ticket related to this:
> The core issue seems to be that all containers for Velero run as user nobody and the service account token is mounted with permissions 0600 preventing non-root users from reading the file (see kubernetes/kubernetes#82573). This issue has been resolved in Kubernetes and looks like it was released in v1.19.0. I don't know how that fix will be made available in EKS or whether there is more to do on the Velero side.

Action needed: document the workaround and also the fact that it is addressed on k8s v1.19.0.
--
"
2006,OPEN,[Feature Request] Downloading/mounting a restic backup locally,Enhancement/User; Restic,2020-03-06 14:17:14 +0000 UTC,davidhiendl,In progress,,"**Describe the problem/challenge you have**
Inspecting, restoring or downloading files from a specific backup is difficult because velero CLI has no option to download the data stored in Restic repositories.
I first encountered this problem when trying to verify backup contents and again when trying to do a partial restore of deleted files.

**Describe the solution you'd like**
There are a few options I think would be useful:
1) Include a flag to `velero backup download` like `--download-pvc-data` which downloads the pvc's into separate files (1 per pvc)
2) Include several generic restic commands from `restic <cmd>` into velero like so:
```bash
velero restic mount <backup-name> <pvc-name> <mount-path> # mount a specific backup and pvc
velero restic download <backup-name> <pvc-name> <target-path> # download a specific backup and pvc
velero restic repo mount <repo-name> # mount the entire repo similar to 'restic mount'
...
```
Obviously these ideas only work for restic itself and not for volume snapshots.

**Environment:**

- Velero version (use `velero version`): v1.1.0
- Kubernetes version (use `kubectl version`): affects all
- Kubernetes installer & version: affects all
- Cloud provider or hardware configuration: affects all
- OS (e.g. from `/etc/os-release`): affects all
",,,skriss,"
--
@davidhiendl it is possible to do this using the `restic` CLI directly. You can get the repository identifier from the appropriate `ResticRepository` CR (`velero restic repo get NAME -o yaml`). You additionally need (a) credentials to directly access the object storage bucket, and (b) the repository's encryption password, which is [here](https://github.com/vmware-tanzu/velero/blob/master/pkg/restic/repository_keys.go#L32) (we rely on IAM to protect the data).

Hope that helps!
--
",davidhiendl,"
--
@skriss Thanks for the info, I already figured that out. But that is an quite is an awful lot of actions a user needs to take to accomplish such a simple task. I think it should be easier.
--

--
While a command to export the condiguration would be useful it would not solve the problem to be able to easily access data the way you identify it in velero. You still have to identify and select the correct host and snapshot Name that belongs to a particular velero backup object.

I think both options should be supported. Since velero only has to lookup the correct backup name/id using it's metadata. 
--
",prydonius,"
--
A `velero restic repo export NAME` command could be nice, it could export the env vars necessary for connecting via restic directly.
--
",TJM,"
--
I personally think that it should just be part of `velero backup download`, perhaps with a command line option like `--download-pv` (or `--no-download-pv`), which would require implementation across other providers. While I am using restic currently, so a restic-specific solution would benefit me, I feel like it would be useful elsewhere. 


I also feel like it is somewhat disingenuous to offer a feature to ""download"" a backup, but not actually send any data. That seems like a pretty critical part of the backup?

~tommy
--
",,,,
2002,OPEN,document cross-provider migration steps,Area/Documentation,2020-02-19 17:03:17 +0000 UTC,skriss,Opened,,Cross-provider migration requires manually altering Velero's configuration to add an additional secret to the target cluster containing credentials for the source cluster's provider. This is not covered in our documentation anywhere. We should document this use case.,,,,,,,,,,,,,,
1981,OPEN,Restic volume not restored when using OpenShift (DeploymentConfig + ReplicationController),Bug; Restic,2020-08-19 10:02:01 +0000 UTC,Stolr,In progress,,"**What steps did you take and what happened:**

I'm trying to restore a restic volume.

My backup got 2 volumes with 2 deployments

Backup 

```

tools-bitbucket-backup-prv2   Completed                    2019-10-18 08:45:44 +0200 CEST   29d       cluster-tools      <none>


Persistent Volumes: <none included>
Restic Backups:
  Completed:
    ok101-bitbucket-pr/bitbucket-postgresql-5-vvwsm: bitbucket-postgresql-data
    ok101-bitbucket-pr/bitbucket-server-14-ddrpp: bitbucket-server-data
```

When I make a restore from this backup , The postgres pod is restore properly but not the bitbucket server
```
tools-bitbucket-backup-prv2-20191018090027   tools-bitbucket-backup-prv2   InProgress   0          0        2019-10-18 09:00:27 +0200 CEST   <none>
 velero restore describe tools-bitbucket-backup-prv2-20191018090027

Restic Restores:
  Completed:
    ok101-bitbucket-pr/bitbucket-postgresql-5-vvwsm: bitbucket-postgresql-data
  New:
    ok101-bitbucket-pr/bitbucket-server-14-ddrpp: bitbucket-server-data
```

The init container is not created on the bitbucket-server pod , so the restic stay stuck in ""New"" phase but the pod is created and  running. It shouldn't

```
 kubectl get po
NAME                                        READY     STATUS             RESTARTS   AGE
bitbucket-server-15-glk7c                   1/1       Running            0          5m
```

*** Restic log ***
```
time=""2019-10-18T07:00:48Z"" level=debug msg=""Pod is not running restic-wait init container, not enqueuing restores for pod"" controller=pod-volume-restore key=ok24-velero/velero-7f5d784896-l66m7 logSource=""pkg/controller/pod_volume_restore_controller.go:170""
time=""2019-10-18T07:00:48Z"" level=debug msg=""Pod is not running restic-wait init container, not enqueuing restores for pod"" controller=pod-volume-restore key=ok24-velero/velero-7f5d784896-l66m7 logSource=""pkg/controller/pod_volume_restore_controller.go:170""
time=""2019-10-18T07:00:59Z"" level=debug msg=""Restore's pod ok101-bitbucket-pr/bitbucket-postgresql-5-vvwsm not found, not enqueueing."" controller=pod-volume-restore error=""pod \""bitbucket-postgresql-5-vvwsm\"" not found"" logSource=""pkg/controller/pod_volume_restore_controller.go:137"" name=tools-bitbucket-backup-prv2-20191018090027-6dkgf namespace=ok24-velero restore=ok24-velero/tools-bitbucket-backup-prv2-20191018090027
time=""2019-10-18T07:00:59Z"" level=debug msg=""Restore's pod ok101-bitbucket-pr/bitbucket-server-14-ddrpp not found, not enqueueing."" controller=pod-volume-restore error=""pod \""bitbucket-server-14-ddrpp\"" not found"" logSource=""pkg/controller/pod_volume_restore_controller.go:137"" name=tools-bitbucket-backup-prv2-20191018090027-gghc8 namespace=ok24-velero restore=ok24-velero/tools-bitbucket-backup-prv2-20191018090027
time=""2019-10-18T07:01:01Z"" level=debug msg=""Pod is not running restic-wait init container, not enqueuing restores for pod"" controller=pod-volume-restore key=ok101-bitbucket-pr/bitbucket-server-15-deploy logSource=""pkg/controller/pod_volume_restore_controller.go:170""
time=""2019-10-18T07:01:01Z"" level=debug msg=""Pod is not running restic-wait init container, not enqueuing restores for pod"" controller=pod-volume-restore key=ok101-bitbucket-pr/bitbucket-server-15-deploy logSource=""pkg/controller/pod_volume_restore_controller.go:170""
time=""2019-10-18T07:01:03Z"" level=debug msg=""Pod is not running restic-wait init container, not enqueuing restores for pod"" controller=pod-volume-restore key=ok101-bitbucket-pr/bitbucket-server-15-deploy logSource=""pkg/controller/pod_volume_restore_controller.go:170""
time=""2019-10-18T07:01:09Z"" level=debug msg=""Pod is not running restic-wait init container, not enqueuing restores for pod"" controller=pod-volume-restore key=ok101-bitbucket-pr/bitbucket-server-15-glk7c logSource=""pkg/controller/pod_volume_restore_controller.go:170""
time=""2019-10-18T07:01:09Z"" level=debug msg=""Pod is not running restic-wait init container, not enqueuing restores for pod"" controller=pod-volume-restore key=ok101-bitbucket-pr/bitbucket-server-15-glk7c logSource=""pkg/controller/pod_volume_restore_controller.go:170""
time=""2019-10-18T07:01:11Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=tools-bitbucket-backup-prv2-20191018090027-6dkgf namespace=ok24-velero restore=ok24-velero/tools-bitbucket-backup-prv2-20191018090027
time=""2019-10-18T07:01:12Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=tools-bitbucket-backup-prv2-20191018090027-6dkgf namespace=ok24-velero restore=ok24-velero/tools-bitbucket-backup-prv2-20191018090027
time=""2019-10-18T07:01:13Z"" level=debug msg=""Pod is not running restic-wait init container, not enqueuing restores for pod"" controller=pod-volume-restore key=ok101-bitbucket-pr/bitbucket-server-15-glk7c logSource=""pkg/controller/pod_volume_restore_controller.go:170""
time=""2019-10-18T07:01:14Z"" level=debug msg=""Pod is not running restic-wait init container, not enqueuing restores for pod"" controller=pod-volume-restore key=ok101-bitbucket-pr/bitbucket-server-15-deploy logSource=""pkg/controller/pod_volume_restore_controller.go:170""
time=""2019-10-18T07:01:14Z"" level=debug msg=""Pod is not running restic-wait init container, not enqueuing restores for pod"" controller=pod-volume-restore key=ok101-bitbucket-pr/bitbucket-server-15-deploy logSource=""pkg/controller/pod_volume_restore_controller.go:170""
time=""2019-10-18T07:01:19Z"" level=debug msg=""Pod is not running restic-wait init container, not enqueuing restores for pod"" controller=pod-volume-restore key=ok24-velero/velero-7f5d784896-l66m7 logSource=""pkg/controller/pod_volume_restore_controller.go:170""
time=""2019-10-18T07:01:20Z"" level=debug msg=""Pod is not running restic-wait init container, not enqueuing restores for pod"" controller=pod-volume-restore key=ok24-velero/velero-7f5d784896-l66m7 logSource=""pkg/controller/pod_volume_restore_controller.go:170""
time=""2019-10-18T07:01:22Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=tools-bitbucket-backup-prv2-20191018090027-6dkgf namespace=ok24-velero restore=ok24-velero/tools-bitbucket-backup-prv2-20191018090027
time=""2019-10-18T07:01:31Z"" level=debug msg=""Restore is not new, not enqueuing"" controller=pod-volume-restore logSource=""pkg/controller/pod_volume_restore_controller.go:131"" name=tools-bitbucket-backup-prv2-20191018090027-6dkgf namespace=ok24-velero restore=ok24-velero/tools-bitbucket-backup-prv2-20191018090027
W1018 07:05:36.261218       1 reflector.go:302] github.com/vmware-tanzu/velero/pkg/cmd/cli/restic/server.go:197: watch of *v1.Secret ended with: The resourceVersion for the provided watch is too old.
W1018 07:05:52.312131       1 reflector.go:302] github.com/vmware-tanzu/velero/pkg/generated/informers/externalversions/factory.go:117: watch of *v1.PodVolumeBackup ended with: The resourceVersion for the provided watch is too old.
```

*** Velero Log ***

https://gist.github.com/Stolr/02ee7e4ee7d662b94df52de93f953ab3

*** PodVolumeRestore ***
```
kubectl -n ok24-velero get podvolumerestores -l velero.io/restore-name=tools-bitbucket-backup-prv2-20191018090027  -o yaml
apiVersion: v1
items:
- apiVersion: velero.io/v1
  kind: PodVolumeRestore
  metadata:
    creationTimestamp: 2019-10-18T07:00:59Z
    generateName: tools-bitbucket-backup-prv2-20191018090027-
    generation: 1
    labels:
      velero.io/pod-uid: 0a33afb5-f175-11e9-967b-005056b9b6b7
      velero.io/restore-name: tools-bitbucket-backup-prv2-20191018090027
      velero.io/restore-uid: f7012bc8-f174-11e9-bf99-005056b9c7f4
    name: tools-bitbucket-backup-prv2-20191018090027-6dkgf
    namespace: ok24-velero
    ownerReferences:
    - apiVersion: velero.io/v1
      controller: true
      kind: Restore
      name: tools-bitbucket-backup-prv2-20191018090027
      uid: f7012bc8-f174-11e9-bf99-005056b9c7f4
    resourceVersion: ""853596""
    selfLink: /apis/velero.io/v1/namespaces/ok24-velero/podvolumerestores/tools-bitbucket-backup-prv2-20191018090027-6dkgf
    uid: 0a35ffec-f175-11e9-967b-005056b9b6b7
  spec:
    backupStorageLocation: cluster-tools
    pod:
      kind: Pod
      name: bitbucket-postgresql-5-vvwsm
      namespace: ok101-bitbucket-pr
      uid: 0a33afb5-f175-11e9-967b-005056b9b6b7
    repoIdentifier: s3:http://oca-miniolb.oca.local/velero/tools/restic/ok101-bitbucket-pr
    snapshotID: 4bd49d6e
    volume: bitbucket-postgresql-data
  status:
    completionTimestamp: 2019-10-18T07:01:31Z
    message: """"
    phase: Completed
    progress:
      bytesDone: 83536468
      totalBytes: 83536468
    startTimestamp: 2019-10-18T07:01:10Z
- apiVersion: velero.io/v1
  kind: PodVolumeRestore
  metadata:
    creationTimestamp: 2019-10-18T07:00:59Z
    generateName: tools-bitbucket-backup-prv2-20191018090027-
    generation: 1
    labels:
      velero.io/pod-uid: 0a3b5638-f175-11e9-967b-005056b9b6b7
      velero.io/restore-name: tools-bitbucket-backup-prv2-20191018090027
      velero.io/restore-uid: f7012bc8-f174-11e9-bf99-005056b9c7f4
    name: tools-bitbucket-backup-prv2-20191018090027-gghc8
    namespace: ok24-velero
    ownerReferences:
    - apiVersion: velero.io/v1
      controller: true
      kind: Restore
      name: tools-bitbucket-backup-prv2-20191018090027
      uid: f7012bc8-f174-11e9-bf99-005056b9c7f4
    resourceVersion: ""853188""
    selfLink: /apis/velero.io/v1/namespaces/ok24-velero/podvolumerestores/tools-bitbucket-backup-prv2-20191018090027-gghc8
    uid: 0a3d1615-f175-11e9-967b-005056b9b6b7
  spec:
    backupStorageLocation: cluster-tools
    pod:
      kind: Pod
      name: bitbucket-server-14-ddrpp
      namespace: ok101-bitbucket-pr
      uid: 0a3b5638-f175-11e9-967b-005056b9b6b7
    repoIdentifier: s3:http://oca-miniolb.oca.local/velero/tools/restic/ok101-bitbucket-pr
    snapshotID: e5a5986e
    volume: bitbucket-server-data
  status:
    completionTimestamp: null
    message: """"
    phase: """"
    startTimestamp: null
kind: List
metadata:
  resourceVersion: """"
  selfLink: """"

```

** Environment **

velero version
Client:
Version: v1.1.0
Git commit: a357f21
Server:
Version: v1.1.0

oc v3.11.0+0cbc58b
kubernetes v1.11.0+d4cacc0
openshift v3.11.0+bdd37ad-314
kubernetes v1.11.0+d4cacc0

The namespace does not exist before the restore so every resources is new on the cluster 


Any idea ?

Thanks a lot",,,skriss,"
--
hmm, based on the following lines:
```
W1018 07:05:36.261218       1 reflector.go:302] github.com/vmware-tanzu/velero/pkg/cmd/cli/restic/server.go:197: watch of *v1.Secret ended with: The resourceVersion for the provided watch is too old.
W1018 07:05:52.312131       1 reflector.go:302] github.com/vmware-tanzu/velero/pkg/generated/informers/externalversions/factory.go:117: watch of *v1.PodVolumeBackup ended with: The resourceVersion for the provided watch is too old.
```
it looks like there might be an issue with the informer caches.

Could you try deleting all of the restic daemonset pods, letting them get re-created, and then trying another restore? (you'll want to delete the target namespace as well before kicking off the new restore)
--

--
@Stolr i'm not exactly sure what's going on, but I do see that during the backup, the pod that's being backed up is `bitbucket-server-14-ddrpp`, and then during/after restore, you end up with pod `bitbucket-server-15-6528q`. I do see in the Velero server log that during the restore, pod `bitbucket-server-14-ddrpp` is restored, but it seems like it's probably being deleted and replaced with `bitbucket-server-15-6528q`.

I'm not super-familiar with OpenShift's deploymentconfigs and (apparently) their use of replication controllers, but in plain vanilla Kubernetes, the way this would work is we'd restore pod ""14"", then restore the replicaset controlling it, and that replicaset would see pod ""14"" and ""adopt"" it. It seems like possibly, something about the deploymentconfig/replicationcontroller is preventing this ""adoption"" from happening, and triggering the creating of a new pod ""15"".

Does this ring any bells for you? Maybe we can figure it out together :)
--

--
@sseago @dymurray do you guys have any thoughts on what's going on here? (https://github.com/vmware-tanzu/velero/issues/1981#issuecomment-544709044)
--

--
> If a pod is managed by another resource the restic restore will generally fail since both the pod and the managing resource is restored which causes the initial pod (with the restic annotation) to be overwritten. I could have sworn there was an open issue on this but I can't seem to find it right now.

We haven't seen this, at least not with pods managed by replicasets/deployments. Per my comment (https://github.com/vmware-tanzu/velero/issues/1981#issuecomment-544709044), during a restic restore, we first restore the pod & trigger a restic restore, then restore the owning replicaset and deployment. The pod is successfully ""adopted"" by the replicaset, since the pod's spec matches the pod template spec from the replicaset.

If that behavior were different, then I agree it would likely cause problems with restic restores, which seems to be what we're seeing here. Can you shed any more light onto why the DeploymentConfig restore is triggering the creation of a new pod, rather than adopting the existing one?
--

--
open to ideas on how to improve this. the data populator KEP that's making the rounds upstream may be relevant/useful, though AFAIK it's only for PVs, not any pod volume.
--
",Stolr,"
--
Hi @skriss 

Thanks for the answer.


I already try this.

I'm making a restore on another cluster. It is a fresh one so there should not be any cache ?

Can this be the issue ? ( Restoring to another cluster)

I'm not able to try it until monday, but not sure it will fix the issue since I already try on a fresh instance.

Any other idea ? :)


--

--
Hey ,

So I made a new fresh install to test this and make sure this is not a cache issue.

Here is  the whole procedure to help you debugging ( I install the restic DS before velero because I adapted for okd , this can be the issue maybe.)


**Instalation on Cluster Tools  && Cluster Tools-B :**  

```
kubectl create ns velero
namespace/velero created

oc annotate namespace velero openshift.io/node-selector=""""
namespace/velero annotated

oc adm policy add-scc-to-user privileged system:serviceaccount:velero:velero
scc ""privileged"" added to: [""system:serviceaccount:velero:velero""]

oc apply -f serviceAccount.yaml
serviceaccount/velero created

kubectl apply -f daemonSetrestic.yaml
daemonset.extensions/restic created

velero install \
    --provider aws \
    --bucket velero \
    --use-restic \
    --secret-file ./credentials-velero  \
    --use-volume-snapshots=false \
    --backup-location-config region=minio,s3ForcePathStyle=""true"",s3Url=http://oca-miniolb.oca.local/ 
	
CustomResourceDefinition/schedules.velero.io: attempting to create resource
CustomResourceDefinition/schedules.velero.io: created
CustomResourceDefinition/deletebackuprequests.velero.io: attempting to create resource
CustomResourceDefinition/deletebackuprequests.velero.io: created
CustomResourceDefinition/podvolumerestores.velero.io: attempting to create resource
CustomResourceDefinition/podvolumerestores.velero.io: created
CustomResourceDefinition/volumesnapshotlocations.velero.io: attempting to create resource
CustomResourceDefinition/volumesnapshotlocations.velero.io: created
CustomResourceDefinition/backups.velero.io: attempting to create resource
CustomResourceDefinition/backups.velero.io: created
CustomResourceDefinition/downloadrequests.velero.io: attempting to create resource
CustomResourceDefinition/downloadrequests.velero.io: created
CustomResourceDefinition/podvolumebackups.velero.io: attempting to create resource
CustomResourceDefinition/podvolumebackups.velero.io: created
CustomResourceDefinition/resticrepositories.velero.io: attempting to create resource
CustomResourceDefinition/resticrepositories.velero.io: created
CustomResourceDefinition/backupstoragelocations.velero.io: attempting to create resource
CustomResourceDefinition/backupstoragelocations.velero.io: created
CustomResourceDefinition/serverstatusrequests.velero.io: attempting to create resource
CustomResourceDefinition/serverstatusrequests.velero.io: created
CustomResourceDefinition/restores.velero.io: attempting to create resource
CustomResourceDefinition/restores.velero.io: created
Waiting for resources to be ready in cluster...
Namespace/velero: attempting to create resource
Namespace/velero: already exists, proceeding
Namespace/velero: created
ClusterRoleBinding/velero: attempting to create resource
ClusterRoleBinding/velero: created
ServiceAccount/velero: attempting to create resource
ServiceAccount/velero: already exists, proceeding
ServiceAccount/velero: created
Secret/cloud-credentials: attempting to create resource
Secret/cloud-credentials: created
BackupStorageLocation/default: attempting to create resource
BackupStorageLocation/default: created
Deployment/velero: attempting to create resource
Deployment/velero: created
DaemonSet/restic: attempting to create resource
DaemonSet/restic: already exists, proceeding
DaemonSet/restic: created
Velero is installed! ⛵ Use 'kubectl logs deployment/velero -n velero' to view the status.	

```


**Cluster Tools Backup location** 
```
apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
  creationTimestamp: 2019-10-21T06:14:03Z
  generation: 1
  labels:
    component: velero
  name: default
  namespace: velero
  resourceVersion: ""48443311""
  selfLink: /apis/velero.io/v1/namespaces/velero/backupstoragelocations/default
  uid: fb1ae7ac-f3c9-11e9-843a-005056b9cf2b
spec:
  config:
    region: minio
    s3ForcePathStyle: ""true""
    s3Url: http://oca-miniolb.oca.local/
  objectStorage:
    bucket: velero
    prefix: ""tools""
  provider: aws
```



**Cluster Tools-B Backup location** 
```
apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
  creationTimestamp: 2019-10-21T06:16:31Z
  generation: 1
  labels:
    component: velero
  name: default
  namespace: velero
  resourceVersion: ""1743001""
  selfLink: /apis/velero.io/v1/namespaces/velero/backupstoragelocations/default
  uid: 53050648-f3ca-11e9-8991-005056b92845
spec:
  config:
    region: minio
    s3ForcePathStyle: ""true""
    s3Url: http://oca-miniolb.oca.local/
  objectStorage:
    bucket: velero
    prefix: ""tools-b""
  provider: aws
  

 velero backup-location create cluster-tools \
    --provider aws \
    --bucket velero \
    --access-mode ReadOnly  \
    --config region=minio,s3ForcePathStyle=""true"",s3Url=http://oca-miniolb.oca.local/
 ```

And then I edited BackupLocation cluster-tools to add ""tools"" Prefix.

Now everything is running fine :
```
 kubectl get po
NAME                     READY     STATUS    RESTARTS   AGE
restic-2s7n8             1/1       Running   0          6m
restic-2zcr7             1/1       Running   0          6m
restic-7wbrt             1/1       Running   0          6m
restic-8zn8n             1/1       Running   0          6m
restic-cfq77             1/1       Running   0          6m
restic-djvrj             1/1       Running   0          6m
restic-kpnvt             1/1       Running   0          6m
restic-n58w2             1/1       Running   0          6m
restic-n5c6x             1/1       Running   0          6m
restic-ssvpp             1/1       Running   0          6m
restic-wjxj7             1/1       Running   0          6m
restic-wsj94             1/1       Running   0          6m
restic-xgxjt             1/1       Running   0          6m
restic-zvfvj             1/1       Running   0          6m
velero-df87fbb89-m2tbh   1/1       Running   2          6m
```

**Cluster Tools :** 

Creating the backup

```
kubectl -n ok101-bitbucket-pr annotate pod/bitbucket-postgresql-5-vvwsm backup.velero.io/backup-volumes=bitbucket-postgresql-data
kubectl -n ok101-bitbucket-pr annotate pod/bitbucket-server-14-ddrpp backup.velero.io/backup-volumes=bitbucket-server-data

velero backup create tools-bitbucket-backup --include-namespaces=ok101-bitbucket-pr
```
Backup get 

 velero backup get
NAME                     STATUS                       CREATED                          EXPIRES   STORAGE LOCATION   SELECTOR
tools-bitbucket-backup   PartiallyFailed (2 errors)   2019-10-21 08:23:26 +0200 CEST   29d       default            <none>

Velero Logs :

https://gist.github.com/Stolr/23d0dd11b301150ccb336a12b77107a1


**Backup description**
```
velero backup describe tools-bitbucket-backup --details
```
https://gist.github.com/Stolr/9b862178df8f951cbd9b50357bd502c8

**Backup logs**
```
velero backup logs tools-bitbucket-backup
```
https://gist.github.com/Stolr/293051c52536541fec55f924f76386be


I can see there is 2 error, but it says my restic are completed. It should not be relevant. This is probably due to some pods not correct in that namespace. First time didn't have that error but the  restic issue was here.


**Now , On Cluster Tools-B**

```
velero backup get
NAME                     STATUS                       CREATED                          EXPIRES   STORAGE LOCATION   SELECTOR
tools-bitbucket-backup   PartiallyFailed (2 errors)   2019-10-21 08:23:26 +0200 CEST   29d       cluster-tools      <none>

velero restore create --include-namespaces=ok101-bitbucket-pr --from-backup tools-bitbucket-backup

Restore request ""tools-bitbucket-backup-20191021083745"" submitted successfully.
Run `velero restore describe tools-bitbucket-backup-20191021083745` or `velero restore logs tools-bitbucket-backup-20191021083745` for more details.
```


Same issue :

The restore Stay in progress because of the restic not restored

```
 velero restore get
NAME                                    BACKUP                   STATUS       WARNINGS   ERRORS   CREATED                          SELECTOR
tools-bitbucket-backup-20191021083745   tools-bitbucket-backup   InProgress   0          0        2019-10-21 08:37:45 +0200 CEST   <none>

```
```
 kubectl get po -n ok101-bitbucket-pr
NAME                                              READY     STATUS            RESTARTS   AGE
bitbucket-postgresql-5-vvwsm                      0/1       PodInitializing   0          39s
bitbucket-pr-data-backup-1571351700-gfhf4         0/1       Pending           0          39s
bitbucket-pr-data-backup-1571438100-jb2dv         0/1       Pending           0          39s
bitbucket-pr-postgresql-backup-1571436000-rtsf7   0/1       Pending           0          39s
bitbucket-server-15-6528q                         1/1       Running           0          34s
```

**Restic Logs**

https://gist.github.com/Stolr/dabac536a5235b87ecd184045ab2e7b5

**Velero Logs**

https://gist.github.com/Stolr/5dfc2f7fea9c63f0ddbd61d9276ac984


**Restore Logs**

No available
Logs for restore ""tools-bitbucket-backup-20191021083745"" are not available until it's finished processing. Please wait until the restore has a phase of Completed or Failed and try again.

**PodVolumeRestore**

```
kubectl -n velero get podvolumerestores -l velero.io/restore-name=tools-bitbucket-backup-20191021083745 -o yaml
apiVersion: v1
items:
- apiVersion: velero.io/v1
  kind: PodVolumeRestore
  metadata:
    creationTimestamp: 2019-10-21T06:37:46Z
    generateName: tools-bitbucket-backup-20191021083745-
    generation: 1
    labels:
      velero.io/pod-uid: 4ae0fee4-f3cd-11e9-bf99-005056b9c7f4
      velero.io/restore-name: tools-bitbucket-backup-20191021083745
      velero.io/restore-uid: 4a83ec78-f3cd-11e9-bf99-005056b9c7f4
    name: tools-bitbucket-backup-20191021083745-9pm46
    namespace: velero
    ownerReferences:
    - apiVersion: velero.io/v1
      controller: true
      kind: Restore
      name: tools-bitbucket-backup-20191021083745
      uid: 4a83ec78-f3cd-11e9-bf99-005056b9c7f4
    resourceVersion: ""1747867""
    selfLink: /apis/velero.io/v1/namespaces/velero/podvolumerestores/tools-bitbucket-backup-20191021083745-9pm46
    uid: 4b5e4bfa-f3cd-11e9-bf99-005056b9c7f4
  spec:
    backupStorageLocation: cluster-tools
    pod:
      kind: Pod
      name: bitbucket-server-14-ddrpp
      namespace: ok101-bitbucket-pr
      uid: 4ae0fee4-f3cd-11e9-bf99-005056b9c7f4
    repoIdentifier: s3:http://oca-miniolb.oca.local/velero/tools/restic/ok101-bitbucket-pr
    snapshotID: d17de56d
    volume: bitbucket-server-data
  status:
    completionTimestamp: null
    message: """"
    phase: """"
    startTimestamp: null
- apiVersion: velero.io/v1
  kind: PodVolumeRestore
  metadata:
    creationTimestamp: 2019-10-21T06:37:46Z
    generateName: tools-bitbucket-backup-20191021083745-
    generation: 1
    labels:
      velero.io/pod-uid: 4adbf402-f3cd-11e9-bf99-005056b9c7f4
      velero.io/restore-name: tools-bitbucket-backup-20191021083745
      velero.io/restore-uid: 4a83ec78-f3cd-11e9-bf99-005056b9c7f4
    name: tools-bitbucket-backup-20191021083745-mlrmd
    namespace: velero
    ownerReferences:
    - apiVersion: velero.io/v1
      controller: true
      kind: Restore
      name: tools-bitbucket-backup-20191021083745
      uid: 4a83ec78-f3cd-11e9-bf99-005056b9c7f4
    resourceVersion: ""1748261""
    selfLink: /apis/velero.io/v1/namespaces/velero/podvolumerestores/tools-bitbucket-backup-20191021083745-mlrmd
    uid: 4b5ecc1e-f3cd-11e9-bf99-005056b9c7f4
  spec:
    backupStorageLocation: cluster-tools
    pod:
      kind: Pod
      name: bitbucket-postgresql-5-vvwsm
      namespace: ok101-bitbucket-pr
      uid: 4adbf402-f3cd-11e9-bf99-005056b9c7f4
    repoIdentifier: s3:http://oca-miniolb.oca.local/velero/tools/restic/ok101-bitbucket-pr
    snapshotID: 01fc7cc8
    volume: bitbucket-postgresql-data
  status:
    completionTimestamp: 2019-10-21T06:38:20Z
    message: """"
    phase: Completed
    startTimestamp: 2019-10-21T06:37:59Z
kind: List
metadata:
  resourceVersion: """"
  selfLink: """"
```

My bitbucket data is not restored. No init container is created. But the postgres one is working as espected.


Do you find something in all theses logs that can explain this ?

Thanks for your help !

--

--
@skriss  Wow thanks !!
You are right , the deployment number is not the same.

For some reason , openshift trigger a new deploy. Probably because of all resource beeing restore.  No way to return on the 14 even with a rollback.

I'm not super familiar also with Openshift
I got rid of that deploymentConfig ( no point using it ) , and adapt everything using normal deployment.

Everything is working as espected using deployment.


@yashbhutwala  : I Might try this when i will be able. Thanks for your answer.


Thanks both for your help. Since this issue is related to Openshift , you can close the issue if you  want or rename it.

Best Regards

Thanks again for helping me getting through this. 
--
",yashbhutwala,"
--
@Stolr @skriss sorry to bump into conversation, just a thought.  Instead of annotating the pod itself, can you try annotating the pod template spec of the parent controller, i.e.: Deployment or ReplicationController?
--

--
@dymurray not sure if this covers all of what you say, but I made an issue a month ago facing a similar issue.  See: #1919 
--
",sseago,"
--
Off the top of my head, I'm not sure what's going on, although I haven't looked at the logs in detail yet. The redeployment of a new pod may well be affecting things here, since the new pod probably won't have the restic annotation. For the work my group has been doing, we actually do a two-phase backup/restore, in part to eliminate as much complexity as possible from the environment Restic is working in. We create a full backup without any restic annotations, and then a limited backup with just the PVs/PVCs and pods which mount them with the restic annotations. Then, on restore we first restore the restic backup (pods only, no deployments, deploymentconfigs, etc.) -- this is when the restic copies happen. Then those restored pods are deleted and we do the full restore (without restic annotations). I don't know that all of this is necessary for a basic backup/restore -- in our case we're using it for app migration from one cluster to another, with the possibility of running the restic/PV migration more than once before the final migration. In any case, if you're restoring deploymentconfigs which are then rolling out new pods post-restore, that could definitely interfere with restic. I don't know what the appropriate general-purpose answer is here -- our approach has been for a very specific migration use case. I wonder whether the same issue comes up with non-OpenShift resources. Daemonsets, Deployments, etc. Annotating the pod template spec, as suggested above (in addition to annotating the pod) may be the way to go here. I""m not sure whether it will resolve this issue completely or not, though.
--

--
From what I've seen with DeploymentConfigs they don't *always* trigger new pods, but sometimes they do. I believe they actually do (initially) adopt the restored pod, as expected, but if there's a ConfigChange trigger registered, then the restore event on the DeploymentConfig will sometimes trigger that if the restore process looks like a configuration change. Most of my experience here is in restoring resources to a different cluster than the backup came from, with some spec params modified by a plugin on restore (""image"" references, for example, if the image is located in an in-cluster registry). The pod as restored will run for a short amount of time, but will terminate as soon as the ConfigChange triggered replacement is ready. Most recently, this week I've restored a couple DeploymentConfigs to the *same* cluster as the backup was run in, and in that case I did not see a replacement being created post-restore.
--
",dymurray,"
--
To add on to what Scott said, yes we hit this same problem very early on. This is a problem that extends beyond OCP specific restores, my understanding is that any pod which is managed by another resource faces this risk.

If a pod is managed by another resource the restic restore will generally fail since both the pod and the managing resource is restored which causes the initial pod (with the restic annotation) to be overwritten. I could have sworn there was an open issue on this but I can't seem to find it right now.
--

--
So I spent some time digging into this, and based on what I've learned I can say that yes the method Velero is currently taking with restic restores has it's shortcomings. Currently, we are lucky that a deployment doesn't trigger a new generation of the pod in 99% of the restore use cases. If you specifically trigger a redeploy during the restic restore then things will break as shown in #1919 . 

With deploymentconfigs, there are a number of triggers you can set which will trigger the redeploy of a pod, but the bigger issue is that currently with DCs the pod is restored first with the restic annotation and then later adopted to the DC controller and redeployed wiping the annotation out. If a plugin is used to not restore a pod if it's managed by a DC in conjunction with placing the annotation on the DC pod template spec then the restic restore has a good chance of succeeding, but the same concern that Kubernetes could trigger a new deployment for deployments and deploymentconfigs during restore is a larger problem that needs to be solved.
--
",dejwsz,"
--
Well, I had just the same problem! Restore completed, no errors in logs but the PV is completely empty! Sucks.
--

--
I wanted to restore only PVC with PV itself and did it:

velero restore create --from-backup  daily-20200528020046 --include-namespaces test-project --include-resources persistentvolumeclaims,persistentvolumes --restore-volumes=true

Completed, no errors. But there is no data at all.
I did not suspect that. Is there any way to make it working with restic?
I have DeploymentConfig but ""replicas"" is set to 0 and I removed ConfigChange from triggers.
--

--
What is interesting I tested it before but only after removing a whole project and then it was ok and even data was there. So it works only during restoring of whole projects? It is not possible to restore just a volume?
--

--
I can confirm - I can restore volumes only restoring a whole project. So a whole namespace - it must be empty.
You cannot restore volumes if there are some objects like deployments or other things. You cannot restore PVC with PV themselves separately using restic.

So in my case, I needed to restore to a mapped temporary namespace. Then go there and scale everything down. Then spin a new POD just to attach PV and rsync data out of the volume to my host. Then I deleted temporary namespace. I run the helper POD again in my original project and I needed there to connect to PV and rsync all the data there. Later I did chown with the user ID of the container. Removed helper POD and then finally scale up the deployment. And it worked and data was there from the backup snapshot. But the process is very inconvenient in such cases, very clumsy.


--
"
1980,OPEN,Schedule creation thru API triggers backup regardless of cron settings,Bug; Help wanted; P3 - Wouldn't it be nice if...,2020-05-18 19:12:47 +0000 UTC,funkycode,Opened,,"I think it's known issue as according to comment in code ( https://github.com/vmware-tanzu/velero/blob/e3d64d9dd9bf4d550a660e60088d20c866fc5972/pkg/controller/schedule_controller.go#L279 )

```
func getNextRunTime(schedule *api.Schedule, cronSchedule cron.Schedule, asOf time.Time) (bool, time.Time) {
    // get the latest run time (if the schedule hasn't run yet, this will be the zero value which will trigger
    // an immediate backup)
    var lastBackupTime time.Time
    if schedule.Status.LastBackup != nil {
        lastBackupTime = schedule.Status.LastBackup.Time
    }
    nextRunTime := cronSchedule.Next(lastBackupTime)

```
**What steps did you take and what happened:**
Here is the simple code to create scheduler we use
`	schedulerBuilder := builder.ForSchedule(
		c.context, name,
	).CronSchedule(cronString).Template(backupBuilder.Result().Spec).ObjectMeta(
		builder.WithLabels(defaults.TufinReleaseVersionLabel, tosVersion))
	scheduler, err = c.client.Schedules(c.context).Create(schedulerBuilder.Result())`

**What did you expect to happen:**
One of the following:
1) Creation Date would be taken into account to calculate next run if schedule.Status.LastBackup.Time value is lower than creation date
2) LastBackup.Time value would be set to creation date if not specified on creation of object




",,,skriss,"
--
xref #2433 
--
",,,,,,,,,,
1978,OPEN,"Restore logs are written to ""read-only"" backup storage locations",Bug; Help wanted; P2 - Long-term important,2019-12-13 21:25:25 +0000 UTC,asaf-erlich,Opened,,"**Describe the problem/challenge you have**
I start up a new cluster which has a default backup storage location. I add a second read-only backup storage location to restore from. In IAM I want to give it only read permissions such as GetObject (we use multiple clusters and I want to be able to restore from a different cluster's bucket, but only give write permission to one cluster per backup bucket). But unfortunately when I run a restore it tries to write logs to the backup storage location that is read-only. It fails with this error in the logs:

`time=""2019-10-18T03:01:05Z"" level=error msg=""Error uploading restore results to backup storage"" controller=restore error=""rpc error: code = Unknown desc = error putting object restores/customerdata-2019-10-18-02-59-06-restore/restore-customerdata-2019-10-18-02-59-06-restore-results.gz: AccessDenied: Access Denied\n\tstatus code: 403, request id: D1CC034F0FC007C1, host id: RTA7KuHx/Ypg3WLvsubDvkMWglBuK1sKL+UnH9iIroSLAeTI1R75UC/CW4Eg4ydEg9/pbZd3W4k="" error.file=""/go/src/github.com/heptio/velero/pkg/cloudprovider/aws/object_store.go:204"" error.function=""github.com/heptio/velero/pkg/cloudprovider/aws.(*ObjectStore).PutObject"" logSource=""pkg/controller/restore_controller.go:494""`

This is pointing to this line in the code base: https://github.com/vmware-tanzu/velero/blob/master/pkg/controller/restore_controller.go#L479 (this is master, not sure why the line number isn't 494 as in the error, but it's the same error message).

**Describe the solution you'd like**
Provide restore create command the same flag that backup create has: `--storage-location string                         location in which to store the backup`. Then I could point it at a read-only storage location but write the output to the storage location of my choosing. This way I could keep s3 write permissions on the bucket to only that cluster which needs it.


**Anything else you would like to add:**
This backup storage location is defined as access mode ReadOnly. It feels like this is a violation of that configuration value or it's at least an unclear experience.


**Environment:**

- Velero version (use `velero version`): `1.1.0`
- Kubernetes version (use `kubectl version`): 
```kubectl version
Client Version: version.Info{Major:""1"", Minor:""15"", GitVersion:""v1.15.3"", GitCommit:""2d3c76f9091b6bec110a5e63777c332469e0cba2"", GitTreeState:""clean"", BuildDate:""2019-08-19T12:36:28Z"", GoVersion:""go1.12.9"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.3"", GitCommit:""5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0"", GitTreeState:""clean"", BuildDate:""2019-06-06T01:36:19Z"", GoVersion:""go1.12.5"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kubernetes installer & version: `kubeadm same version as kubernetes cluster above: v1.14.3`
- Cloud provider or hardware configuration: `aws`
- OS (e.g. from `/etc/os-release`): `Amazon Linux 2`
",,,skriss,"
--
Ah, yeah I agree this doesn't make a lot of sense. We could consider writing restore logs to the default storage location, rather than the location where the backup being restored is located.
--
",asaf,"
--
Yes that would also solve the problem.
--
",,,,,,,,
1977,OPEN,Allow for a flag to cleanup all objects not included in backup,Enhancement/User; Needs Product; ZD3851,2020-10-22 21:47:11 +0000 UTC,poidag,In progress,,"**Describe the problem/challenge you have**
Restoring to a namespace does not provide the capability of a `cleanup` to restore everything taken in that snapshot and remove all other existing objects in that namespace.


**Describe the solution you'd like**
A flag on the restore function, or perhaps a flag on a backup function to specify the type of backup or restore whereby any object that is not a part of the inital backup will be removed when the restore occurs.
 
**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): `n/a`
- Kubernetes version (use `kubectl version`): `n/a`
- Kubernetes installer & version: `n/a`
- Cloud provider or hardware configuration: `n/a`
- OS (e.g. from `/etc/os-release`): `n/a`
",,,poidag,"
--
/label ZD3851
--
",skriss,"
--
somewhat related to #469 

right now, Velero as a policy does not delete items from the cluster during a restore, but we've heard (per #469, along with this request) that this is sometimes desirable.

one thing we'd need to figure out is what the right ""scope"" is for deleting objects -- i.e. backups aren't always limited to a single namespace, they may have resource exclusions, label-selectors, etc., and restores can have the same -- so if executing a restore, which objects in the cluster should be considered part of the ""scope"" of the backup/restore and therefore would be eligible for deletion?
--

--
I share the concerns @prydonius raises - @pickledrick, can you provide some more info on why having the user delete the relevant resources/namespace prior to restore is insufficient?
--
",prydonius,"
--
I'm curious about the motivation for this. I'm concerned that deleting objects in the cluster is dangerous due to the scope issue @skriss outlines and it seems like this is better solved out-of-band. For example, simply deleting the namespace and then performing the restore with Velero is a good way of achieving this for one particular namespace (and only requires one more step).
--
",nrb,"
--
Something I've recently thought about - how would Velero behave if a finalizer prevented deletion? Simply wait? Timeout?

There's a more general problem in Kubernetes where if a namespace is deleting and some object has a finalizer blocking the deletion, it is very hard to find out what object is holding up the process. Velero might be able to report it, but I don't feel comfortable just removing finalizers.
--
",debianmaster,"
--
i think this a much required feature.      with basic features like --conflict-strategy=delete 
--
",,
1975,OPEN,Support for dual-stack clusters and running Velero in IPv6,Enhancement/User; P1 - Important; P2 - Long-term important,2021-03-06 00:01:53 +0000 UTC,VMmore,Opened,,"**Describe the problem/challenge you have**
I would like to ensure that Velero runs successfully in a dual-stack cluster generally, and connecting with IPv6 locations for backups and snapshots.


**Describe the solution you'd like**
Velero should run successfully in a dual stack cluster with ipFamily set to IPv4 or IPv6. Velero should accept and successfully backup and restore to locations that have IPv6 endpoints.


**Environment:**

- Velero version (use `velero version`): >v1.2
- Kubernetes version (use `kubectl version`): >=Kubernetes v1.16",,,,,,,,,,,,,,
1970,OPEN,Velero restore with include-resources to restore only namespace resource is not working,Bug; P2 - Long-term important,2021-01-31 02:07:58 +0000 UTC,Frank51,In progress,,"**What steps did you take and what happened:**
Velero user could use '--include-resources' flag to specify which kind of resources need to be restored from backup. But when  '--include-resources' only contains 'namespaces', the backup namespace could not be restored successfully to the Kubernetes cluster. If adding 'namespaces' and 'services' into the  'include resources', the backup could be restored correctly at this time. 

```
velero get backups
NAME        STATUS      CREATED                         EXPIRES   STORAGE LOCATION   SELECTOR
yelb-back   Completed   2019-09-20 12:39:34 -0700 PDT   3d        default            <none>

velero describe backup yelb-back
Name:         yelb-back
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  <none>

Phase:  Completed

Namespaces:
  Included:  yelb-ns
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  default

Snapshot PVs:  auto

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  1

Started:    2019-09-20 12:39:34 -0700 PDT
Completed:  2019-09-20 12:39:35 -0700 PDT

Expiration:  2019-10-20 12:39:34 -0700 PDT

Persistent Volumes: <none included>


kubectl get ns
NAME              STATUS   AGE
default           Active   5h6m
kube-node-lease   Active   5h6m
kube-public       Active   5h6m
kube-system       Active   5h6m
velero            Active   152m

velero restore create --from-backup yelb-back --include-resources namespaces
Restore request ""yelb-back-20191016140424"" submitted successfully.
Run `velero restore describe yelb-back-20191016140424` or `velero restore logs yelb-back-20191016140424` for more details.

kubectl get ns
NAME              STATUS   AGE
default           Active   5h7m
kube-node-lease   Active   5h7m
kube-public       Active   5h7m
kube-system       Active   5h7m
velero            Active   152m

# yelb-ns is not restored from the backup.
# However, when I restore both the namespace and service, yelb-ns is restored:

velero restore create --from-backup yelb-back --include-resources namespaces,services
Restore request ""yelb-back-20191016140932"" submitted successfully.
Run `velero restore describe yelb-back-20191016140932` or `velero restore logs yelb-back-20191016140932` for more details.

kubectl get ns
NAME              STATUS   AGE
default           Active   5h12m
kube-node-lease   Active   5h12m
kube-public       Active   5h12m
kube-system       Active   5h12m
velero            Active   157m
yelb-ns           Active   4s

```

**What did you expect to happen:**
When running 'velero restore create --from-backup <backupname>  --include-resources namespaces ', velero should restore only the namespace resource from the backup on the Kubernetes cluster successfully. 

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

* `kubectl logs deployment/velero -n velero`
https://gist.github.com/Frank51/5c18363b55bb593fc780068134684da7
* `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`

```
Name:         yelb-back
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  <none>

Phase:  Completed

Namespaces:
  Included:  yelb-ns
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  default

Snapshot PVs:  auto

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  1

Started:    2019-09-20 12:39:34 -0700 PDT
Completed:  2019-09-20 12:39:35 -0700 PDT

Expiration:  2019-10-20 12:39:34 -0700 PDT

Persistent Volumes: <none included>

```

* `velero backup logs <backupname>`
https://gist.github.com/Frank51/62aaa98ea6a38418eb159b926e101e17
* `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`

```
Name:         yelb-back-20191016140424
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:  Completed

Backup:  yelb-back

Namespaces:
  Included:  *
  Excluded:  <none>

Resources:
  Included:        namespaces
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io
  Cluster-scoped:  auto

Namespace mappings:  <none>

Label selector:  <none>

Restore PVs:  auto

```

* `velero restore logs <restorename>`
https://gist.github.com/Frank51/3e2bbdc44a5c1f1cdee91e0d65d70dd4
**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): v1.0.0
- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`):  (Client Version : v1.16.0), (Server Version :v1.14.5 )
- Kubernetes installer & version:
- Cloud provider or hardware configuration: AWS
- OS (e.g. from `/etc/os-release`): Ubuntu
",,,skriss,"
--
Yeah, this is likely being caused by https://github.com/vmware-tanzu/velero/blob/master/pkg/restore/restore.go#L403-L407. This will take a little bit of investigation as I don't remember exactly why we treated namespaces that way, though likely has to do with included/excluded namespaces, namespace mappings, etc.
--

--
@Frank51 sorry, I remember you asked me for this before in Slack :) I'll try to add more context tomorrow - I'll have to jog my own memory as to why it's implemented this way.
--

--
So I think there are a couple of things to consider:
- if we're restoring a namespaced resource, we first need to ensure that the namespace itself exists, so the resource can be restored into it
- when restoring `Namespace` resources themselves, we need to consider the namespace includes/excludes, as well as the namespace mappings

I believe that the implementation approach of lazily restoring `Namespace` resources only if we are restoring a resource into it was the easiest way to deal with the above two items. That said, alternate approaches would work fine as long as all of the permutations of the above are properly dealt with.

Hope that helps!
--

--
@Frank51 were you interested in working on a fix here?
--
",Frank51,"
--
> Yeah, this is likely being caused by https://github.com/vmware-tanzu/velero/blob/master/pkg/restore/restore.go#L403-L407. This will take a little bit of investigation as I don't remember exactly why we treated namespaces that way, though likely has to do with included/excluded namespaces, namespace mappings, etc.

Hi, Kriss. Could you give me more detail related to this namespace restore issue? Because I am a little bit confused about that part of the code. Why the namespace will be treated in this way? But when restoring namespace with other resources like PV, PVC, it works. 
--

--
> @Frank51 sorry, I remember you asked me for this before in Slack :) I'll try to add more context tomorrow - I'll have to jog my own memory as to why it's implemented this way.

Hi, Kriss. Sorry to disturb you. Do you have any idea about the implementation of the restore of namespace at that time?
--

--
> @Frank51 were you interested in working on a fix here?

Sure, I am quite interested in it. 
--
",ashish,"
--
/assign @Frank51 
--
",nrb,"
--
This also has implications for users wanting to write RestoreItemActions for namespaces. Right now, since namespaces aren't created as part of the normal restore process but rather directly with a client call, writing a RestoreItemAction plugin for them will not get invoked.
--
",,,,
1952,OPEN,[Azure] Add resource_group and subscription_id labels to Prometheus metrics,Area/Cloud/Azure; Enhancement/User,2019-10-26 08:06:42 +0000 UTC,boxcee,In progress,,"**Describe the problem/challenge you have:**
With #1895, cross subscription backups/restore were enabled. In order to monitor backups cross subscription and resource group it is required to add some labels to the metrics.


**Describe the solution you'd like:**
Add more labels to prometheus metrics. E.g. resource_group and subscription_id.


**Anything else you would like to add:**
I am happy to add them by myself. However, I noticed that you are in the progress of separating velero core and provider relevant code. I would need some advice on how to start then (where to put the code).",,,skriss,"
--
@boxcee yes, we're going to be moving the cloud-provider code into separate repos. That work is in progress as we speak - we should soon have https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure repo that contains most of the code that's currently in `pkg/cloudprovider/azure`. If you can hold off on implementing this until we do that migration, that would probably work out best for all of us. Thanks!
--

--
@boxcee we've finished moving the Azure code into its own repo, so it should be fine to proceed with this issue now if you're still interested.
--
",boxcee,"
--
Thanks for letting me know! Will add.
--
",,,,,,,,
1948,OPEN,RFE: show diff when object already exists in cluster/is different from backed-up version,Enhancement/User; P3 - Wouldn't it be nice if...,2019-10-15 15:30:20 +0000 UTC,ajohnstone,Opened,,"**What steps did you take and what happened:**
Create a backup on same cluster and restore to the same cluster produces warnings of ""not restored: %s and is different from backed up version.""

https://github.com/vmware-tanzu/velero/blob/v1.1.0/pkg/restore/restore.go#L1082

**What did you expect to happen:**
No warnings to be produced and/or object to be applied.
If different then at least a diff of the object in storage and clusters current resource or the resource versions to show why.

**The output of the following commands will help us better understand what's going on**:


* `kubectl logs deployment/velero -n velero`
* `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
* `velero backup logs <backupname>`
* `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
* `velero restore logs <restorename>`


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`):  v1.1.0
- Velero features (use `velero client config get features`): 
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):
",,,skriss,"
--
Currently, as a policy, Velero doesn't overwrite objects in-cluster if they already exist. We have #469 to optionally support this.

I'll update this issue to cover the feature request of showing a diff when objects are different.
--
",,,,,,,,,,
1944,OPEN,Consider running plugins as sidecar containers,Area/Plugins; Enhancement/Dev; P3 - Wouldn't it be nice if...,2019-10-07 16:10:08 +0000 UTC,skriss,Opened,,"There may be some benefits to running plugins as sidecar containers, rather than copying the binaries into the main velero container and running them there. For example, if a plugin has additional dependencies or environment requirements, they'd be easier to set up in their own container image.",,,,,,,,,,,,,,
1929,OPEN,Documentation: Dealing with K8S CA certificates,Area/Documentation; Help wanted; P2 - Long-term important,2019-10-07 18:50:06 +0000 UTC,dpkirchner,In progress,,"**Describe the problem/challenge you have**

Some providers, such as Google, don't allow you to create a cluster with a specific CA (at least as far as I can tell, been looking at docs for a while). That means any restored certificates created using k8s's built in CSR tooling will not work in the new cluster.

**Describe the solution you'd like**

I'm suggesting/requesting that this issue be documented on the Restore Reference and Disaster Recovery pages as it may not be an obvious issue people will face. I don't think there's a way to resolve the underlying issue itself so this is really just a ""heads up!"" that people can be aware of when testing and documenting their Velero backup solution.

**Anything else you would like to add:**

If I'm wrong, then this issue becomes a request for documentation describing how to restore the cluster's CA as well.",,,nrb,"
--
Can you tell me a little bit more about your Google clusters? Are you using GKE? I haven't witnessed this with GKE, but I haven't personally tested on top of GCP compute VMs.
--
",dpkirchner,"
--
I'm using GKE and I'm running cockroachdb in the cluster. The cockroachdb helm chart uses k8s CertificateSigningRequests to generate certificates for intra-node communication, and you can use those for client communication as well. This issue isn't limited to crdb, it's more of a general problem, but I'm mentioning it to show a practical use case. See https://github.com/helm/charts/blob/master/stable/cockroachdb/templates/cluster-init.yaml

Here's a high level overview of the steps required to reproduce the issue. I can go in to greater detail if you want (it'd be a few pages of scripts).

- Create a key, use openssl to generate a csr, then use that in a `CertificateSigningRequest` object
- Approve the certificate with `kubectl certificate approve`
- Store the certificate in a `Secret` so it can be mounted by pods
- Run a backup
- Download the backup and untar the k8s parts
- Take a look at `resources/secrets/namespaces/default/secret-name.json` -- you'll see the certificate and if you decode it you'll see it was signed by the cluster's CA (using `openssl verify`)

If you create a new cluster, GKE generates a brand new CA. The certificate stored in the `Secret` is no longer valid -- it won't verify -- because the CA that was used for signing is not used in the new cluster. You'll need to delete and create new certificates.

~~It might also be worth noting somewhere that `CertificateSigningRequest` resources are not included in the backups. If they were, you may be able to re-approve them, update your secrets, and then carry on with a working cluster. I think that will probably need to be a manual process for security and practical reasons, although someone could script it if they wanted.~~ I was wrong -- I was 100% sure that my cluster had CSRs (I saw them while working on this) but now I see it doesn't. Dang. Sorry about that.
--
",,,,,,,,
1928,OPEN,Make backup/restore logs more user-friendly,Enhancement/User; P2 - Long-term important,2019-10-07 16:51:46 +0000 UTC,mqsoh,Opened,,"**Describe the problem/challenge you have**
I just want to share some frustration I just had with a failed cluster migration. The restore information looks like this:

```
NAME                                 BACKUP                STATUS            WARNINGS   ERRORS   CREATED                         SELECTOR
influxdb-migration2-20191002133625   influxdb-migration2   PartiallyFailed   4          1        2019-10-02 13:36:25 +0000 UTC   <none>
```

4 warnings and 1 error but every line has `level=info` and there are literally no lines with the words ""warn"" or ""error"".

```
$ velero restore logs influxdb-migration2-20191002133625 | grep -i warn
1 $ velero restore logs influxdb-migration2-20191002133625 | grep -i error
1 $
```

Every line has redundant and unhelpful information, i.e. `logSource` (I don't know your code.) and `restore` (literally the thing I'm looking at). I tried a stupid sed to reduce it to the information I needed (`sed 's/time=""//; s/"" level=info msg=""/ /; s/"" logSource=.\+//'`) which works for most lines but not all.

In this specific case I'm missing one volume among 6. I have pairs like ""attempting to restore""/""successfully restored"" for 5 of them but the 6 just isn't mentioned until later when it says the PV is already there (but it's not?).

**Describe the solution you'd like**
My own application logs look like this. I know what they mean; each entry in the log refers to a specific code path and I can dig through the code and spot bugs pretty quickly. When I open a bug report for my actual issue, I'm sure you'll be able to do the same.

I think there should be logs tailored for the end user. Non-structured messages that describe the state as the program currently sees it, the changes that it thinks it needs, and the words ""error"" and ""warning"" *in* those types of messages.


**Anything else you would like to add:**
If one of my coworkers wasn't out of town, I think they would see the problem immediately. I'm not very good at anything, so maybe take it with a grain of salt.

Also, I'm not asking for help with my issue. I'll make a ticket once I feel I've done my due dilligence. I'm making a suggestion about the friendliness of the logs.


**Environment:**

- Velero version (use `velero version`): 1.0.0
- Kubernetes version (use `kubectl version`): 1.12.9
- Kubernetes installer & version: migrating from 1.11.x to 1.12.x
- Cloud provider or hardware configuration: AWS
- OS (e.g. from `/etc/os-release`): ubuntu 18.04
",,,,,,,,,,,,,,
1927,OPEN,Restic restores of statically provisioned PVs,Enhancement/User; P2 - Long-term important; Restic,2019-10-14 15:25:02 +0000 UTC,brokencode64,Opened,,"**What steps did you take and what happened:**
We have many persistent volumes that are classless nfs disks created vs ""persistent volume"" yamls.  It seems that after annotating the pods with the correct notations to have them backed up the actual data for the volumes are backed up to s3; however, upon trying to restore them the Persistent volumes are not automatically recreated.  If I create the persistent volume manually the data will be restored.


**What did you expect to happen:**
The volumes should have been backed up along with the rest of the deployments and restored. Upon running a restore the persistent volumes should be automatically restored with the data.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

**Yamls**
For example, we create most of our persistent volumes for our on-prem environment and some of our aws volumes like this:
```
---
apiVersion: v1
kind: PersistentVolume
metadata:
 name: prometheus-prometheus-alertmanager
 labels:
   type: nfs
   name: prometheus-prometheus-alertmanager
spec:
 capacity:
   storage: 10Gi
 accessModes:
   - ReadWriteMany
 persistentVolumeReclaimPolicy: Retain
 nfs:
   path: /system/prometheus-prometheus-alertmanager
   server: x.x.x.x.1.amazonaws.com
```
That persistent volume claim would be used like this:
```
`---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: prometheus-prometheus-alertmanager
  namespace: monitor
spec:
  storageClassName: """"
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  selector:
    matchLabels:
      type: nfs
      name: prometheus-prometheus-alertmanager
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: prometheus-alertmanager
  name: prometheus-prometheus-alertmanager
  namespace: monitor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-alertmanager
  template:
    metadata:
      labels:
        app: prometheus-alertmanager
      annotations:
        velero.io/backup-name: monitor
        backup.velero.io/backup-volumes: storage-volume
        prometheus.io/path: /metrics
        prometheus.io/port: ""9093""
        prometheus.io/scrape: ""true""
    spec:
      containers:
      - args:
        - --config.file=/etc/config/alertmanager.yml
        - --storage.path=/data
        - --web.external-url=http://alertmanager.tools.kube-pet.appdev.io
        image: prom/alertmanager:v0.16.1
        imagePullPolicy: IfNotPresent
        name: prometheus-alertmanager
        resources:
          limits:
            memory: 256Mi
          requests:
            memory: 128Mi
        ports:
        - containerPort: 9093
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /#/status
            port: 9093
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 30
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/config
          name: config-volume
        - mountPath: /data
          name: storage-volume
      - args:
        - --volume-dir=/etc/config
        - --webhook-url=http://localhost:9093/-/reload
        image: jimmidyson/configmap-reload:v0.1
        imagePullPolicy: IfNotPresent
        name: prometheus-alertmanager-configmap-reload
        resources:
          limits:
            memory: 128Mi
          requests:
            memory: 64Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/config
          name: config-volume
          readOnly: true
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          defaultMode: 420
          name: prometheus-prometheus-alertmanager
        name: config-volume
      - name: storage-volume
        persistentVolumeClaim:
          claimName: prometheus-prometheus-alertmanager
---
```
**Environment:**
- Velero version: V1.1.0 
- Kubernetes version : V1.12.4
- Cloud provider or hardware configuration: Two clusters AWS, Three on-prem (hosted on vmware vms)
- OS (e.g. from `/etc/os-release`): AWS hosts ubuntu based, on-prem RHEL 7.5.
",,,skriss,"
--
Ah, yes, when restoring with restic, Velero assumes it can dynamically provision an empty PV to restore into by just restoring the PVC.

In your case, presumably if we directly restored the PV, the NFS path would stay the same - and the contents of it would be non-empty, correct?
--
",lumatijev,"
--
I have the same problem. When restoring PVs that are using NFS and are backed up using restic Velero is stuck at InProgress because PVCs are waiting for PVs to be created/restored.

Could we somehow mark these PVs so Velero can restore them directly without assuming that they can be dynamically provisioned?
--
",,,,,,,,
1921,OPEN,Support backing up just the volumes with restic,Restic,2020-03-21 18:52:25 +0000 UTC,yashbhutwala,Opened,,"**Describe the problem/challenge you have**
[A description of the current limitation/problem/challenge that you are experiencing.]
Currently, Velero with Restic, you cannot just backup solely the restic data due to restic support relying on the pod volumes. This means you have to back up the pods in order to back up the volumes.  It'd be nice if velero allowed to backup just the persistent volumes.

i.e.: `velero backup create yb-backup --include-namespaces yb --include-resources pods`

Also, `velero backup create yb-backup-just-pv --include-resources persistentvolumes` does not work with restic, because of the implementation as I mention above, but this should be documented.	


**Environment:**

- Velero version (use `velero version`): `v1.1.0`
- Kubernetes version (use `kubectl version`): `1.13`
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):
",,,schaze,"
--
I would really like to have that too. Or at least make it possible to only restore the PVC/PV including volume data from a restic backup. Sometimes we need to change the structure of the workloads either when migrating or doing blue/green approach but keep them working with their existing data. Could the restore process not just pull in the volume with a dummy deployment so it is mounted which gets deleted afterwards again?
--
",,,,,,,,,,
1919,OPEN,Scaling down deployment prevents a restore with restic,Area/Documentation; Restic,2019-12-13 21:29:42 +0000 UTC,yashbhutwala,In progress,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)

PV restore with restic hangs `InProgress` when scaling down deployment:
```
backup create
scale down postgres deployment
delete pvc, pv
restore create --from-backup
scale up postgres deployment
```

But works with:
```
backup create
delete deployment + delete pvc, pv
restore create --from-backup
```

**What did you expect to happen:**
Scaling down deployment should still result in a successful restore.
",,,yashbhutwala,"
--
Perhaps due to the controller wanting to put the annotation: 
`velero.io/backup-name: yb-backup-0
 velero.io/restore-name: yb-backup-0-20190927125644`

Before the backup, I only patch the pod to have the annotation, I scale down the deployment, but the deployment does not have annotation.  Velero controller wants to patch the deployment to add the backup and restore annotation and it can't ...that's where I suspect the bug is...if this helps in the analysis.
--

--
If it's the above case, it should be made explicit in the docs to annotate the deployment instead: https://velero.io/docs/v1.1.0/restic/

although I do see: `This annotation can also be provided in a pod template spec if you use a controller to manage your pods.`
--
",nrb,"
--
As discussed in slack, `velero.io/backup-name` isn't the same annotation that's used to identify restic volumes. `velero.io/backup-volumes` is the annotation used for this. The same annotation can be placed directly on a pod or within a pod template spec in an object like a Deployment, DaemonSet, or StatefulSet.

To explain some of the things happening here, if the deployment is scaled to 0 and exists in the cluster, Velero won't actually try to restore that resource, as it already exists. If Velero restored the requested pods, the reconciliation loops that manage Deployments would delete the running pods because the scale is set to be 0, so Kubernetes will ensure that's the case. Until the pods are created and remain running, the current logic around restic restores won't work.

This is a consequence of how we implement restic support, and I think the current best approach will be to document this behavior.
--
",,,,,,,,
1918,OPEN,Make explicit the diff between backup describe and restore describe,Good first issue; Help wanted; Size/S,2019-09-27 20:25:28 +0000 UTC,yashbhutwala,Opened,,"**Describe the problem/challenge you have**
[A description of the current limitation/problem/challenge that you are experiencing.]

There is a diff between namespaces included as well as resources excluded in `backup describe` and `restore describe` that makes it unclear to the user.  It'd be nice to add some notes in the restore to make it explicit that some resources are exclueded by default in restore. 3 of these are Velero resources - backups, restores, and resticrepositories.  Also, add a message to `included namespaces` in restore.

Result of backup describe
```
Name:         yb-backup-0
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  <none>

Phase:  Completed

Namespaces:
  Included:  yb-onprem
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  default

Snapshot PVs:  auto

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  1

Started:    2019-09-27 11:32:35 -0400 EDT
Completed:  2019-09-27 11:32:50 -0400 EDT

Expiration:  2019-10-27 11:32:35 -0400 EDT

Persistent Volumes: <none included>

Restic Backups:
  Completed:
    yb-backup/postgresql-675cc47d87-brx5t: postgresdb
```

Result of restore describe:
```
Name:         yb-backup-0-20190927114106
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:  InProgress

Backup:  yb-backup-0

Namespaces:
  Included:  *
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io
  Cluster-scoped:  auto

Namespace mappings:  <none>

Label selector:  <none>

Restore PVs:  auto

Restic Restores:
  New:
    yb/postgresql-675cc47d87-brx5t: postgresdb
```",,,nrb,"
--
There's a subtle difference between the field on a backup and a restore.

On a backup, `Included Namespaces: *` means ""back up all namespaces found in the cluster.""

On a restore, `Included Namespaces: *` means ""restore all namespaces found in the backup.""

I think clarifying the wording on the restore description to be something like ""All namespaces found in backup"" or something may help reduce confusion versus using `*`.
--
",,,,,,,,,,
1917,OPEN,Add ability to specify order of backup and restore of Persistent Volumes,Needs Product,2020-10-22 17:35:49 +0000 UTC,yashbhutwala,In progress,,"**Describe the problem/challenge you have**
I would like to be able to specify the order of backup and restore of Persistent Volumes.  For my use-case, the order in which the state of two different databases is captured is very important.  
",,,nrb,"
--
This would be an interesting capability to add, but would like input for @VMmore on roadmap timing.
--

--
@phuongatemc Does your ordering enhancement fix this issue?
--

--
@yashbhutwala See https://github.com/vmware-tanzu/velero/pull/2724 for how to specify an ordering for resources on backup now. This is not yet supported on restore, though.
--
",yashbhutwala,"
--
The workaround I'm doing now, is creating a separate backup with 1st level of PVs, and a separate backup for the second step.  And then applying the restore in reverse order.
--
",phuongatemc,"
--
@nrb Yes, the Velero 1.5.1 work for for us.  
--
",,,,,,
1902,OPEN,Back up roles/role bindings from other namespaces that relate to a given service account,Enhancement/User; Needs Product,2020-02-07 19:58:34 +0000 UTC,fei922,In progress,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)
I deployed [k8s service-catalog](https://github.com/kubernetes-sigs/service-catalog) on clusterA. Now I want to backup  service-catalog and restore it to clusterB.

When the restore is complete，service catalog pod's status is CrashLoopBackOff. Because serviceaccount and secrets on cluster A does not have permission to access resources of cluster B.
```
root@master-1:~/velero-v1.1.0# kubectl get po -n public 
NAME                                               READY   STATUS             RESTARTS   AGE
catalog-catalog-apiserver-5fbc5fd76c-6rzg9         1/2     CrashLoopBackOff   365        31h
root@master-1:~/velero-v1.1.0# 
root@master-1:~/velero-v1.1.0# 
root@master-1:~/velero-v1.1.0# 
root@master-1:~/velero-v1.1.0# kubectl logs -f --tail 10 -n public catalog-catalog-apiserver-5fbc5fd76c-6rzg9 -c apiserver
I0924 16:50:14.235256       1 round_trippers.go:386] curl -k -v -XGET  -H ""Accept: application/json, */*"" -H ""User-Agent: service-catalog/v0.1.37 (linux/amd64) kubernetes/c6a2cef"" 'https://10.43.0.1:443/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication'
I0924 16:50:14.242177       1 round_trippers.go:405] GET https://10.43.0.1:443/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication 403 Forbidden in 6 milliseconds
I0924 16:50:14.242199       1 round_trippers.go:411] Response Headers:
I0924 16:50:14.242203       1 round_trippers.go:414]     Content-Type: application/json
I0924 16:50:14.242206       1 round_trippers.go:414]     X-Content-Type-Options: nosniff
I0924 16:50:14.242209       1 round_trippers.go:414]     Content-Length: 403
I0924 16:50:14.242212       1 round_trippers.go:414]     Date: Tue, 24 Sep 2019 08:50:14 GMT
I0924 16:50:14.242410       1 request.go:942] Response Body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""configmaps \""extension-apiserver-authentication\"" is forbidden: User \""system:serviceaccount:public:service-catalog-apiserver\"" cannot get resource \""configmaps\"" in API group \""\"" in the namespace \""kube-system\"""",""reason"":""Forbidden"",""details"":{""name"":""extension-apiserver-authentication"",""kind"":""configmaps""},""code"":403}
W0924 16:50:14.242715       1 authentication.go:246] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLE_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
Error: configmaps ""extension-apiserver-authentication"" is forbidden: User ""system:serviceaccount:public:service-catalog-apiserver"" cannot get resource ""configmaps"" in API group """" in the namespace ""kube-system""

```
**so How to handle backup migration with serviceaccount and secret ？**

**What did you expect to happen:**
When I restore, serviceaccount can become available in cluster b, or is there any other scheme to generate usable serviceaccount in cluster B?


**Environment:**

- Velero version (use `velero version`):  v1.1.0
- Kubernetes version (use `kubectl version`):  v1.13.5
",,,skriss,"
--
@fei922 have you tried restoring the RBAC objects  as well (roles+bindings, clusterroles+bindings)? The ones used by your service account should be backed up automatically when you back up the service account, so you'd just need to ensure they got restored as well.
--

--
`Error: configmaps ""extension-apiserver-authentication"" is forbidden: User ""system:serviceaccount:public:service-catalog-apiserver"" cannot get resource ""configmaps"" in API group """" in the namespace ""kube-system""`

^^ There must be some missing RBAC that allows your SA to access configmaps in the `kube-system` namespace - can you track down which role that is in your source cluster?
--

--
Hmm, interesting. So this is a `Role` and `RoleBinding` in a different namespace from the service account. We don't currently have code to pull these in -- we only have code that pulls in related `ClusterRoles`/`ClusterRoleBindings`.

This scenario can get tricky -- on backup, if you specify `--include-namespaces public`, would it be appropriate to pull in roles/rolebindings from another namespace? What about on restore? We'll definitely have to think about this some more.

In the interim, you could: create a second backup that captures these RBAC objects, and restore it alongside your main backup; label all of the objects that you want to back up, and then use a label selector rather than a namespace selector to capture everything; or write a custom backup item action plugin that adds these roles/rolebindings to the backup.
--
",fei922,"
--
@skriss Thanks.  The  RBAC objects  used by service account looks like backing up automatically when I back up the service account.  There are RBAC objects in restoring cluster.
```
root@master-1:~/velero-v1.1.0# kubectl get roles.rbac.authorization.k8s.io -n public -l velero.io/backup-name=test-public
NAME                                                      AGE
servicecatalog.k8s.io:cluster-info-configmap              3d15h
servicecatalog.k8s.io:leader-locking-controller-manager   3d15h
root@master-1:~/velero-v1.1.0# 
root@master-1:~/velero-v1.1.0# kubectl get rolebindings.rbac.authorization.k8s.io -n public -l velero.io/backup-name=test-public
NAME                                                 AGE
service-catalog-controller-manager-cluster-info      3d15h
service-catalog-controller-manager-leader-election   3d15h
root@master-1:~/velero-v1.1.0# 
root@master-1:~/velero-v1.1.0# kubectl get clusterroles.rbac.authorization.k8s.io -n public -l velero.io/backup-name=test-public
NAME                                       AGE
servicecatalog.k8s.io:apiserver            3d15h
servicecatalog.k8s.io:controller-manager   3d15h
root@master-1:~/velero-v1.1.0# 
root@master-1:~/velero-v1.1.0# kubectl get clusterrolebindings.rbac.authorization.k8s.io -n public -l velero.io/backup-name=test-public
NAME                                             AGE
servicecatalog.k8s.io:apiserver                  3d15h
servicecatalog.k8s.io:apiserver-auth-delegator   3d15h
servicecatalog.k8s.io:controller-manager         3d15h
```
--

--
@skriss thanks,  I found that the following resource object is missing. 
```
kind: RoleBinding
metadata:
  name: servicecatalog.k8s.io:apiserver-authentication-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: service-catalog-apiserver
  namespace: public
```

**Why is this resource object not backed up together?**
--

--
Thank you, I will try it.
--
",carlisia,"
--
Hey @fei922, please let us know if the instructions were helpful. For now I'll go ahead and close this ticket, we can always reopen it if the problem persisted.
--

--
Ok, reopening because I just noticed the ""needs product"" label.
--
",,,,,,
1891,OPEN,Support Schedules for restores,Enhancement/User; Needs Product; P3 - Wouldn't it be nice if...,2020-08-05 08:29:06 +0000 UTC,mshivanna,Opened,,"**Describe the problem/challenge you have**
[A description of the current limitation/problem/challenge that you are experiencing.]
Currently velero schedules are only for backups. Good to have schedules for restores too.

**Describe the solution you'd like**
[A clear and concise description of what you want to happen.]
One valid use case i can think of is to clone production data and resources to a staging environment for testing. Since we need to have up to date data/resources synced with production schedules for restores will help achieve this goal.",,,skriss,"
--
Related to this, I would love to see ""Schedules"" leverage Kubernetes' native `CronJob` - where the CronJob would simply trigger a backup or restore on the specified schedule. I'm not sure if this means the Velero Schedule CRD would go away, or if it would become a lightweight wrapper/controller for the CronJob CRD. Using this approach could make implementing this feature easier.
--
",chasebolt,"
--
Also would :heart: to have this feature. Great way to validate backups by restoring production to staging nightly.
--
",panpan0000,"
--
+1.   with `schedule`  for restore, it would make cross-cluster DR(main/standby cluster ) much more easier.
for example: I could schedule a backup in main cluster,  and schedule restore in standby cluster.
Simple cronjob only is not enough , you would be required to choose one among many backup points.(default latest one). And more over, the schedule restore may also include the spec ""override"" for standby cluster infrastructure differences.

but anyway, `kasten`  has a feature : “restore after import” option. Aka, we can schedule `import` backup data from another cluster , and then `restore` it automatically.  

In many enterprise use cases, main/standby cluster strategy is popular.  this feature would be  ~loved~.

last but not least, I feel so impressive to find velero, a cloud-native tools and such a small team to make this great achievement.   
--
",,,,,,
1889,OPEN,Create an upgrade script for updating plugins,Enhancement/User; P2 - Long-term important,2020-04-27 15:37:18 +0000 UTC,carlisia,Opened,,See https://github.com/heptio/velero/pull/1870#discussion_r323783953.,,,skriss,"
--
Related to this, we'll also need instructions for upgrading from v1.1 to v1.2 -- users will need to add the plugins for the first time.
--

--
I'm going to move this to v1.3 since it won't be an issue until then.
--
",,,,,,,,,,
1874,OPEN,Wildcards don't work for namespaces,Bug; P2 - Long-term important,2019-09-13 14:40:32 +0000 UTC,kaxil,Opened,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)
Tried to create a backup for all namespaces that matches `*-helm` regex
 
`velero backup create wp-airflow-helm --include-namespaces ""*-helm""`


**What did you expect to happen:**
Backup created for just that specific namespaces that matched regex


**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

* `kubectl logs deployment/velero -n velero`
* `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`:
```
> velero backup describe wp-airflow-helm2
Name:         wp-airflow-helm2
Namespace:    velero
Labels:       velero.io/storage-location=default
Annotations:  <none>

Phase:  PartiallyFailed (run `velero backup logs wp-airflow-helm2` for more information)

Errors:    1
Warnings:  0

Namespaces:
  Included:  *-helm
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Storage Location:  default

Snapshot PVs:  auto

TTL:  720h0m0s

Hooks:  <none>

Backup Format Version:  1

Started:    2019-09-12 20:10:25 +0100 BST
Completed:  2019-09-12 20:10:26 +0100 BST

Expiration:  2019-10-12 20:10:25 +0100 BST

Persistent Volumes: <none included>
```

* `velero backup logs <backupname>`:
```
time=""2019-09-12T19:10:26Z"" level=error msg=""Error getting namespace"" backup=velero/wp-airflow-helm2 error=""namespaces \""*-helm\"" not found"" error.file=""/go/src/github.com/heptio/velero/pkg/backup/resource_backupper.go:186"" error.function=""github.com/heptio/velero/pkg/backup.(*defaultResourceBackupper).backupResource"" group=v1 logSource=""pkg/backup/resource_backupper.go:186"" namespace=""*-helm"" resource=namespaces

```


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): v1.1.0
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.6"", GitCommit:""96fac5cd13a5dc064f7d9f4f23030a6aeface6cc"", GitTreeState:""clean"", BuildDate:""2019-08-19T11:13:49Z"", GoVersion:""go1.12.9"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""13+"", GitVersion:""v1.13.7-gke.8"", GitCommit:""7d3d6f113e933ed1b44b78dff4baf649258415e5"", GitTreeState:""clean"", BuildDate:""2019-06-19T16:37:16Z"", GoVersion:""go1.11.5b4"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kubernetes installer & version:
- Cloud provider or hardware configuration: GKE
- OS (e.g. from `/etc/os-release`): MacOS
",,,skriss,"
--
Around https://github.com/heptio/velero/blob/master/pkg/backup/resource_backupper.go#L164, we're assuming that the namespace includes/excludes are ""resolved"" namespaces, which obviously doesn't hold with wildcards. We'd need to add some logic here to first get a full list of namespaces from the cluster, then check each one to see if it should be included, and then proceed with backing up those namespaces.
--
",,,,,,,,,,
1872,OPEN,Consider making it easier to add fsfreeze or other pre/post hooks to pods,Enhancement/User; Needs Product; P3 - Wouldn't it be nice if...; Restic,2019-10-02 20:30:06 +0000 UTC,skriss,Opened,,"**Describe the problem/challenge you have**
Right now, if a user wants to use an fsfreeze or other pre/post hook for their pod backup, they need to manually add annotations to the pod and possibly add a container to the pod spec as well (i.e. the `gcr.io/heptio-images/fsfreeze` image). This can be burdensome for users.

**Describe the solution you'd like**
Per #1586, one option would be to add a webhook to automatically add this to new pods. Another option would be a CLI command that makes this easier (e.g. `velero hook add`).",,,,,,,,,,,,,,
1868,OPEN,restic-timeouts are currently based on time for PodVolumeBackups to complete,Enhancement/User; P3 - Wouldn't it be nice if...; Restic,2019-10-04 16:10:58 +0000 UTC,mshivanna,In progress,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)
Tried creating a backup on a namespace which has 6GB to be backed up.
velero create backup monitoring --include-namespace monitoring

Backup was created with partial failures. When looked at the logs there was an error logged 
```level=error msg=""Error backing up item"" backup=velero/monitoring error=""timed out waiting for all PodVolumeBackups to complete"" error.file=""/go/src/github.com/heptio/velero/pkg/restic/backupper.go:165"" error.function=""github.com/heptio/velero/pkg/restic.(*backupper).BackupPodVolumes"" group=v1 
logSource=""pkg/backup/resource_backupper.go:264"" name=prometheus-prometheus-prometheus-oper-prometheus-0 namespace=monitoring resource=pods```
**What did you expect to happen:**

Backup to be completed without errors.
**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

* `kubectl logs deployment/velero -n velero`
* `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
Restic Backups:
  Completed:
    monitoring/alertmanager-prometheus-prometheus-oper-alertmanager-0: alertmanager-prometheus-prometheus-oper-alertmanager-db
    monitoring/prometheus-grafana-69777855-l427t: storage
    monitoring/prometheus-prometheus-prometheus-oper-prometheus-0: prometheus-prometheus-prometheus-oper-prometheus-db 
* `velero backup logs <backupname>`
level=error msg=""Error backing up item"" backup=velero/monitoring error=""timed out waiting for all PodVolumeBackups to complete"" error.file=""/go/src/github.com/heptio/velero/pkg/restic/backupper.go:165"" error.function=""github.com/heptio/velero/pkg/restic.(*backupper).BackupPodVolumes"" group=v1 
logSource=""pkg/backup/resource_backupper.go:264"" name=prometheus-prometheus-prometheus-oper-prometheus-0 namespace=monitoring resource=pods`
* `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
* `velero restore logs <restorename>`


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]
 There shouldn’t be a timeout as long as the PodVolumeBackup is progressing and the timeout should be based on restic status( if it does not make any progress) and error should be logged if restic hangs during backup or restore

**Environment:**

- Velero version (use `velero version`): 
velero version
Client:
	Version: v1.1.0
	Git commit: -
Server:
	Version: v1.1.0
- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.0"", GitCommit:""ddf47ac13c1a9483ea035a79cd7c10005ff21a6d"", GitTreeState:""clean"", BuildDate:""2018-12-03T21:04:45Z"", GoVersion:""go1.11.2"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.3"", GitCommit:""435f92c719f279a3a67808c80521ea17d5715c66"", GitTreeState:""clean"", BuildDate:""2018-11-26T12:46:57Z"", GoVersion:""go1.10.4"", Compiler:""gc"", Platform:""linux/amd64""}

",,,mshivanna,"
--
 There shouldn’t be a timeout as long as the PodVolumeBackup is progressing and the timeout should be based on restic status( if it does not make any progress) and error should be logged if restic hangs during backup or restore

--
",,,,,,,,,,
1824,OPEN,Delay in noticing backup storage location credentials changes,Enhancement/User; IAM,2020-06-11 14:37:29 +0000 UTC,alexander-demichev,Opened,,"When credentials file is kept as secret and secret gets updated, it takes about 2 minutes for velero to notice these changes. Reason for this delay is kubelet sync period + ttl of cache(https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#mounted-configmaps-are-updated-automatically). This delay causes backups to fail.

My proposed solution is to keep credentials file in a normal volume, because secret volumes are read-only. Then velero can track the secret changes and update credentials file. I prototyped a solution that works for my installation - https://gist.github.com/alexander-demichev/1575abc5a1dd9b2c150f81abab1c477c , code was placed in `pkg/cmd/server/server.go` - https://github.com/fusor/velero/blob/fusor-dev/pkg/cmd/server/server.go#L312 . 


I would like to get some opinions on this, does the concept look ok for improving it and opening a PR?",,,skriss,"
--
xref #1655 
--
",,,,,,,,,,
1799,OPEN,EBS VolumeSnapshotter fails to create volume from snapshot if the snapshot is still pending,Bug; Needs Product; P2 - Long-term important,2021-01-15 17:01:45 +0000 UTC,sseago,In progress,,"
**What steps did you take and what happened:**
Restoring a volume with an EBS snapshot sometimes fails if the snapshot is still pending. Most of the time I don't hit it, but it happened at least once. The case where it's likely to come up is where the backup is happening from one cluster and the restore is happening into another cluster immediately following, since there is no enforced downtime around namespace/volume deletion.

**What did you expect to happen:**
I expected the volume  creation to succeed.

**The output of the following commands will help us better understand what's going on**:
Error message from restore: `error executing PVAction for persistentvolumes/pvc-89e0cd19-c53c-11e9-9f8d-0a0e2c8f9874: rpc error: code = Unknown desc = IncorrectState: Snapshot is in invalid state - pending`

One possible solution is for the aws volume_snapshotter to check for the snapshot state before moving on. This actually solves two potential errors:
1) if the volume state is 'error', then we can just return an error without attempting to create
2) if the volume state is 'pending', then wait and try again

Here's an idea of what I'm suggesting. As a first approximation, just make a Sleep call and try again. The real answer would need to include a timeout and possibly a better way of waiting, but this gets the concept down.

Basically replace the lines here:
https://github.com/heptio/velero/blob/master/pkg/cloudprovider/aws/volume_snapshotter.go#L104-L106
With something like this (timeout, error checking, etc. omitted to just show the concept):
```
	if count := len(snapRes.Snapshots); count != 1 {
		return """", errors.Errorf(""expected 1 snapshot from DescribeSnapshots for %s, got %v"", snapshotID, count)
	}
	for (snapRes.Snapshots[0].State == ""pending"" {
		time.Sleep(time.Second * 5)
		snapRes, _ = b.ec2.DescribeSnapshots(snapReq)
	}
	if snapRes.Snapshots[0].State == ""error"" {
		return """", errors.Errorf(""Snapshot is in 'error' state"")
	}
```",,,prydonius,"
--
@sseago's suggestion sgtm, but an alternative way might be to have the controller check at the start of the restore if the snapshots are ready. If they aren't, then the restore can be requeued (with some backoff) and retried later.
--
",skriss,"
--
Prioritized Backlog


--

--
Yep, that's right. I might go with something like `snapshotCreationTimeout` as the name. If we end up wanting something different down the road, we could always support reading both in for awhile.

I do wonder if long-term, we want the status of the snapshots to be reflected in the *Backup* status - so the backup controller could be periodically checking the state of the snapshots after they've been triggered, and update the Backup's phase once they've been completed. That way, the restore controller would just need to ensure the backup is in a certain phase.
--

--
I think a new phase that indicates we're waiting for the snapshot data to replicate makes sense. And we'll still want that ""wait for snapshot completion"" step to be non-blocking, so subsequent backups can proceed.

Couple related things we'll want to consider:
- we'd still want any post hooks to run immediately after the snapshot was ""created"", rather than waiting until they're completed/replicated
- for some storage platforms (typically on-prem), it may be possible to restore from a local snapshot after it's created, *before* the data is replicated off to some durable backend. so, we may want some provider-specific way to indicate whether the snapshot/backup is restorable yet, independent of whether it's completed replicating its data somewhere
--
",sseago,"
--
I'm thinking that we may need to add another element to the VolumeSnapshotLocation CRD here. While we can set a reasonable timeout limit for how long to wait for pending snapshots to something between 10 and 30 minutes, large volumes could take much longer than this. If a user is restoring a 1TB volume, for example, it could take several hours for the snapshot to hit the ready state. We need a way for a user who has larger-than-normal volumes (or who has previously failed restores due to timeouts on large volumes) to specify a longer timeout. Given that this is all happening in a plugin, and we're already passing in the VSL.Spec.Config struct, the obvious place to put this configuration is in the VSL itself. 

Should we add a new field volumeSnapshotLocation.Spec.Config.Timeout? I'm not sure if there's a better more specific name. RestoreTimeout, since we're applying it on the restore side, or CreationTimeout, since we're specifically talking about waiting until the snapshot is fully-created and ready to use. If it's empty, we'll use a default (30 minutes? 20? 10?)

For the blocking case (the short-term fix I'm working on for just our local build), this would be pretty easy to use -- just pass it in as an argument to `wait.PollImmediate` (added to volume_snapshotter). For the requeuing solution, it would be trickier to apply. Assuming the solution here is to add a new method to the plugin API to call prior to starting restore -- `readyToRestore` or something like that, this method would need to figure out whether to return ""yes, ready"", ""no, requeue"", or ""don't bother; fail the migration"". Basically a `bool, err` return val, where in the error case we'd pass an error, either ""Snapshot %s is in an error state"" (or not found, or whatever), or ""Snapshot timeout reached"", and with no error, `true` would mean ""ready to restore"" and `false` would mean ""requeue"". What's less obvious is how to apply the timeout in this situation. Perhaps the first time the plugin method is called for a given restore would set the initial time, and then on subsequent calls, time elapsed vs. the vsl timeout would be compared. Also, in the requeue case, the method would have to look at *all* snapshots for a given restore, while in the blocking case, we're looking at one snapshot at a time -- this means that the actual meaning of the timeout would be somewhat different in these cases -- in the requeue case, it's a single timeout for the restore, while in the blocking case, it's per snapshot, but since the blocking case is a short-term fix that will go away once the upstream fix is done, I'm less worried about it.
--

--
Actually, I guess it wouldn't really requre a CRD change, since `Spec.Config` is a map. It would just be another entry in the map that the AWS plugin would look for.
--

--
Handling it on the backup actually makes more sense -- really, the backup isn't complete until the snapshots are done. Would you just keep the backup InProgress until the post-backup plugin api call indicated that the volume snapshots were complete, or would there be some new state ""WaitingOnSnapshot"", that would move to ""Completed"" once it's done. Either way, the configuration should be the same (so whatever short-term solution I put in place right now on the restore should still work with the same creation timeout configuration when we drop it in favor of whatever upstream does for the long-term fix). Also, as long as the backup doesn't go into the Completed state until after the snapshotter says it's done, our existing code that waits for backup completion before attempting a restore shouldn't require any changes.
--

--
""for some storage platforms (typically on-prem), it may be possible to restore from a local snapshot after it's created, before the data is replicated off to some durable backend. so, we may want some provider-specific way to indicate whether the snapshot/backup is restorable yet, independent of whether it's completed replicating its data somewhere""
That makes sense, although it gets tricky in terms of how to translate this into backup state. If it's (locally) restorable but not restorable from remote data, then ""Completed"" doesn't seem right. Perhaps a second (optional) state would be needed. So `InProgress` -> `WaitForSnapshot` -> (only where supported) `LocalSnapshotCompleted` -> `Completed`, but as you say, once InProgress changes to `WaitForSnapshot`, the next backup continues and it's only a background polling process that would update the backup to the next state (or two). Also, I'm assuming in the case where either the snapshot was fast enough to complete before the first post-backup API call or in cases where there are no snapshots to wait for, we'd go straight to `Completed` just like we do now.
--

--
We just discovered that GCP has the same problem (for the same reasons). This isn't unexpected, and I think everything discussed above still applies. When the plugin API is updated to allow post-backup snapshot status checks, then both GCP and AWS (and probably Azure too) will need to check snapshot status, taking into account the `snapshotCreationTimeout` in the VSL.
--
",nrb,"
--
CSI snapshots represent some of this via the [`ReadyToUse`](https://github.com/kubernetes-csi/external-snapshotter/blob/master/pkg/apis/volumesnapshot/v1beta1/types.go#L135) flag, which more or less is left to the storage system's meaning of ""ready."" On AWS and GCP, it would mean pushing to object storage, and on Ceph or vSphere it would probably mean replication to different nodes then copying outside the cluster.

From what I've seen of the CSI snapshotting implementations, they return before `ReadyToUse` is set to `true` in order to handle other operations, but the common controller won't use the snapshot for a restore/clone until the field is set.

I think keeping a way for individual storage systems to communicate their requirements makes sense, since they can operate differently. I'm not sure at the moment if that means multiple backup states, multiple fields, or something else. I do think we might lose some fidelity as we move to CSI, though, if we rely only on the `ReadyToUse` field.
--
",arianitu,"
--
@nrb Do you know if this actually waits with CSI snapshots? We are on GCP and we have no way to know that a backup is ready other than doing a `time.Sleep(3 * time.Minute)`. 

Does velero wait for CSI snapshots to be complete before marking the backup as complete?
--

--
@nrb Just following up again. We have patched the velero-plugin-gcp to wait for the volume snapshot to be ready. But long-term, it seems like CSI is the future.

Do you know if the CSI plugin correctly handles restoring a snapshot if it's still pending. For example:

1. Velero makes a backup and creates a snapshot (with CSI driver)
2. The user initiates a Restore immediately after the Backup is in the Complete state
3. Velero starts the restore

When Velero does Step 3, does the system wait for the snapshot to be ready, or does Kubernetes handle it? Right now, for velero-plugin-gcp, it will fail because it attempts to use a snapshot that is not ready, so we added logic to wait for it.

Do we have to do that for CSI? An answer would be really appreciated!


--

--
If anyone is curious, we talked to the Google folks managing the gke-csi diver and the CSI driver will handle this case for you. This may be relevant for you @nrb since I don't think velero needs to wait for snapshots to be ready, so I guess you guys do not need to do anything.

Basically, the PVC provisioner will retry forever until the `VolumeSnapshot` is `readyToUse`. So in theory, the EBS and GKE volume snapshotter bugs are no longer relevant if you use the CSI snapshot.



--
",,
1785,OPEN,backup_sync_controller doesn't update VSLs like it does BSLs,Enhancement/User; Needs Product; P2 - Long-term important,2019-12-13 18:25:04 +0000 UTC,sseago,In progress,,"I've noticed that when restoring to a different cluster than the
backup was taken from, the BSL gets updated to match the appropriate
BSL resource in the new cluster, while the VSL does not. Digging into
this, I see that Velero updates the BSL reference (Spec.StorageLocation)
in backup_sync_controller.go:
  https://github.com/heptio/velero/blob/master/pkg/controller/backup_sync_controller.go#L214

Since velero is getting the backup metadata *from* a BSL on the new
cluster, it's obvious that this reference must be modified to match
whatever BSL it's pulling the data from.

The issue is that Spec.VolumeSnapshotLocations has a similar
problem. The corresponding VSL resources in the new cluster may not
have the same name as the old ones. The problem is that there's no way
for Velero to know what the VSL name is in the new cluster. When I run
a restore into a different-from-the-backup cluster with a snapshot to
deal with, I get this error on restore:

Cluster:  error executing PVAction for persistentvolumes/pvc-da28f6f9-c435-11e9-a5b9-0a0e2c8f9874: volumesnapshotlocation.velero.io ""migstorage-sample-stnpt"" not found

I have a VSL in the new cluster that is appropriate, but the name is
different. From the look of it, I see 3 possible solutions:

1) Modify my controller code to pre-generate unique names for VSLs for
all clusters that will use the same base VSL definition (i.e. don't
use metadata.GenerateName when defining it)

2) Modify Velero to accept a backup annotation which will modify
Spec.VolumeSnapshotLocations in the backup_sync_controller (possibly
with some check to make sure that it only modifies the field if the
VSLs referenced in the backup itself don't exist in this cluster)/

3) Modify the Restore API to add an optional Spec.VolumeSnapshotLocations,
and modify the restore controller to use this field, if it exists,
instead of the corresponding field from the backup.

Of the three, 3) seems like the most appropriate general-purpose
solution to the problem. 2) feels like a hack that would only work for
a subset of possible use cases. 1) would work for our purposes, but
it's a bit of a clumsy workaround.

**What steps did you take and what happened:**
1) Create a backup which contains a snapshot
2) Restore the backup into a different cluster with BSL and VSL created which point to the same underlying storage as the backup but with different kubernetes names in the cluster
3) I get this error on restore:

`Cluster:  error executing PVAction for persistentvolumes/pvc-da28f6f9-c435-11e9-a5b9-0a0e2c8f9874: volumesnapshotlocation.velero.io ""migstorage-sample-stnpt"" not found`

I have a VSL in the new cluster that is appropriate, but the name is
different.


**What did you expect to happen:**
I expected Velero to find the VSL.


**Environment:**

- Velero version (use `velero version`): 1.1 beta2
- Kubernetes version (use `kubectl version`): 1.13
- Kubernetes installer & version:
- Cloud provider or hardware configuration: aws
- OS (e.g. from `/etc/os-release`):
",,,carlisia,"
--
@skriss should we add a cli flag something like `vsl-mappings`?
--
",nrb,"
--
Thanks for the report @sseago!

Can you talk a little more about why the VSL names are different in each cluster? Do they include some sort of cluster identifier in their names? I'm fairly sure our assumptions in writing this code were that the VSL would have the same name regardless of which cluster it was in.
--

--
Using generateName is a totally reasonable approach. I think if we put a VSL field on a restore object, it should also be exposed in the CLI as Carlisia mentioned. I'll have to think about the different approaches.

Regardless I think this is a good bugfix candidate for v1.2.
--
",sseago,"
--
Actually I wonder whether there might be a fourth option. Perhaps backup_sync_controller could iterate over the existing VSLs in the cluster and if it finds a match based on spec (config/provider), it could update it automatically, without any user config needed.
--

--
@nrb sure. So in our case it's because our migration controller generally creates resources using the kubernetes generateName metadata field rather than setting Name explicitly. The names are similar, but differ in the stub at the end. Your code does assume that the name *could* be different since it does update the BSL name -- see the linked line above.
--

--
And yes, I could rewrite our controller code to *not* use generateName, which would also resolve it, but it would be better if Velero could handle the name difference. Rewriting our code is the fallback approach if changing this upstream doesn't make sense.
--

--
So adding it to restore (and CLI) seems like the most flexible approach. Inspecting VSLs in-cluster and comparing with the backup VSL list would require less config on our end, but it's may be a bit too fragile to be reliable in all cases.
--
",skriss,"
--
`#3` seems like the preferred approach - need some more research here.

Prioritized Backlog
--
",,,,
1784,OPEN,Display percentage progress for restic operations when using velero create backup/restore --wait,Enhancement/User; P3 - Wouldn't it be nice if...; Restic,2020-02-21 21:06:03 +0000 UTC,prydonius,Opened,,"Once #1749 has been implemented, we will have information about the restic backup/restore progress. We can periodically query the progress and display it when using the `--wait` flag for `velero create restore/backup`.

cc @nrb ",,,,,,,,,,,,,,
1766,OPEN,Document using Velero with two remote clusters,Area/Documentation; P3 - Wouldn't it be nice if...,2019-08-23 15:54:32 +0000 UTC,rbankston,Opened,,"**Describe the problem/challenge you have**
Document the configuration needed for backing up a remote cluster including the need to use the CLI to point to the remote cluster to initiate and control the backup along with a minimum set of manifests for the local and remote backup.


**Describe the solution you'd like**
Documented steps to configure Velero to take backups in a remote cluster


**Anything else you would like to add:**
Useful for systems like Gardener or OpenStack with a remote cluster native to the design

",,,,,,,,,,,,,,
1755,OPEN,Feature Request: Ability to restore volumes+PVCs in a different AWS AZ,Area/Cloud/AWS; Enhancement/User,2020-10-02 15:37:29 +0000 UTC,emayssat,Opened,,"Velero is awesome to backup and restore kubernetes objects.
That being said, beyond simple backup and restore operations, we investigated whether velero could help in the case of an AWS AZ failure. Unfortunately, with the current implementation, it does not appear to be supported.

Indeed, in the backup metadata, I found that the AZ is hardcoded in the volumesnapshot file, e.g. nginx-backup-20190810055750-volumesnapshots.json.gz and in the corresponding PVC manifests, stored and compressed in the manifest archive, e.g. nginx-backup-20190810055750.tar.gz

**Describe the solution you'd like**
Although velero does not support it, I edited the volumesnapshot and manifest archives manually, and was successfully able to restore my stateful sets in another AZ (us-east-2b) than the one they originated from (us-east-2a). 

At this time, I do not see anything in the 'velero restore create' command that would support the feature. But that's where I would expect to see the command line option.

Could it be possible for velero to support the feature?
Yes! 
In my case, the manual metadata changes I executed were rather simple.
Nothing in my installation is specific, therefore I believe a generic solution is possible.  

**Anything else you would like to add:**
Awesome tool!

**Environment:**

- Velero version (use `velero version`): v1.1.0-beta.1
- Kubernetes version (use `kubectl version`): EKS 1.13
- Kubernetes installer & version: eksctl
- Cloud provider or hardware configuration: AWS
- OS (e.g. from `/etc/os-release`): AWS AMIs, generic cluster deployed with eksctl. 
",,,mo,"
--
So as if version 1.2.0, this is still not implemented and the ""hack"" above works. Just change the az in both the files specified. Please notice that you need to change in the persistentvolume (not the persistentvolumeclaim) resource. 
--
",,,,,,,,,,
1752,OPEN,Feature Request: Annotate pods as critical during backups to resolve issues with descheduler/HPA,Enhancement/User,2021-03-10 08:03:57 +0000 UTC,echel0n,In progress,,"**Describe the problem/challenge you have**
Scheduled backups on a cluster that uses descheduler or HPA will sometimes result in not being able to locate the pod volume due to the pod being re-scheduled.


**Describe the solution you'd like**
Add in the ability to set ""scheduler.alpha.kubernetes.io/critical-pod"" annotation prior to performing pod backup then remove after backup has been completed.


**Environment:**

- Velero version (use `velero version`): v1.1.0-beta.1
- Kubernetes version (use `kubectl version`): v1.15.0
- Kubernetes installer & version: kubeadm / v1.15.0
- Cloud provider or hardware configuration: 3 node cluster
- OS (e.g. from `/etc/os-release`): Ubuntu 18.04.2 LTS
",,,skriss,"
--
@echel0n are you looking to be able to add this annotation to your workload's pods? or to the velero pods? or both?
--

--
xref #1770 for another example of how pods might be terminated mid-backup causing issues in Velero.
--
",echel0n,"
--
@skriss both would be good so descheduler doesn't kill the pods during backup or restore, need to also remove afterwards so descheduler can continue working as intended.
--
",,,,,,,,
1712,OPEN,Removing backups with kubectl does not trigger velero-server backup removal,Area/Documentation; Good first issue; Help wanted; P2 - Long-term important,2020-11-05 16:18:14 +0000 UTC,invidian,In progress,,"**What steps did you take and what happened:**
- create full backup with `velero backup create test`
- wait for backup to complete
- remove backup using kubectl: `kubectl delete backups test -n velero`

**What did you expect to happen:**
- Azure snapshot and data in blob storage should be removed

**The output of the following commands will help us better understand what's going on**:

Velero server does not produce any logs on delete operation


**Anything else you would like to add:**
Creating subsequent backups fails with following message:
```
time=""2019-07-31T12:55:48Z"" level=error msg=""backup failed"" controller=backup error=""backup already exists in object storage"" error.file=""/go/src/github.com/heptio/velero/pkg/controller/backup_controller.go:484"" error.function=""github.com/heptio/velero/pkg/controller.(*backupController).runBackup"" key=velero/test logSource=""pkg/controller/backup_controller.go:230""
```

Snapshots are still visible in Azure, so manual cleanup is required.

I also wasn't able to find any information in the docs, that using `kubectl` for taking backups is not recommended.

**Environment:**

- Velero version (use `velero version`): 
```
Client:
	Version: v1.0.0
	Git commit: 72f5cadc3a865019ab9dc043d4952c9bfd5f2ecb
Server:
	Version: v1.0.0
``
- Kubernetes version (use `kubectl version`):
```
Client Version: version.Info{Major:""1"", Minor:""15"", GitVersion:""v1.15.0"", GitCommit:""e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529"", GitTreeState:""clean"", BuildDate:""2019-06-19T16:40:16Z"", GoVersion:""go1.12.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.7"", GitCommit:""4683545293d792934a7a7e12f2cc47d20b2dd01b"", GitTreeState:""clean"", BuildDate:""2019-06-06T01:39:30Z"", GoVersion:""go1.11.5"", Compiler:""gc"", Platform:""linux/amd64""}
```
- Kubernetes installer & version: AKS
- Cloud provider or hardware configuration: Azure
",,,skriss,"
--
@invidian yes - if you want to delete the backup including all data in object/block storage, use `velero backup delete`.  As you said, using `kubectl delete` will only delete the custom resource.

I agree this is probably not documented well, and we should probably add something to explain the hows and whys.
--

--
Creates via `kubectl` are fine. You can also `velero backup create ... -o yaml` to generate a YAML spec for a backup, which you can later apply using `kubectl`.
--
",invidian,"
--
>@invidian yes - if you want to delete the backup including all data in object/block storage, use velero backup delete. As you said, using kubectl delete will only delete the custom resource.

Thanks for confirmation @skriss. And do you know if there are any implications from **creating** backups, schedules etc using `kubectl`?
--
",turkenh,"
--
What I understand from the [source code](https://github.com/vmware-tanzu/velero/blob/master/pkg/cmd/cli/backup/delete.go#L117), deletions are triggered via another crd called [`DeleteBackupRequest`](https://github.com/vmware-tanzu/velero/blob/master/pkg/apis/velero/v1/delete_backup_request.go#L57).

So, if you want to use kubectl (or go client) to delete a backup, you need to create `DeleteBackupRequest` instead of deleting `Backup` resource.
--
",framled,"
--
there are no documentation about those resources.
--
",,,,
1687,OPEN,Allow restoration of the `status` metadata field,Enhancement/User; Kubernetes Resources; Needs Product,2020-10-26 18:24:09 +0000 UTC,rayanebel,In progress,,"**Describe the problem/challenge you have**
To manage our clusters, we are using Rancher. Rancher run on top of Kubernetes and save all the data in the same ETCD server _(it's like a big operator)_. We tried to use `velero` to be able to restore our Rancher server in case of disaster recovery.

Backup is running perfectly but we are facing to an issue during the restore phase because Rancher save some state information inside `status` part and velero clean all status and metadata before restoring all the objects. So currently, we use velero for the backup and restore the file with multiple `kubectl` command line.

**Describe the solution you'd like**
I don't have the perfect solution but, I saw in velero code the function `resetMetadataAndStatus`

https://github.com/heptio/velero/blob/e371ba78b0844b28359f902076d470ae5a5de0b9/pkg/restore/restore.go#L1117

Maybe as a first step we can add a simple flag `--include-status` or something else to let the choice to the user to execute this function.

Maybe you have another solution or another idea ?

**Anything else you would like to add:**

We also open a ticket in Rancher side : https://github.com/rancher/rancher/issues/21647

**Environment:**

- Velero version
  - Client: v1.0.0
  - Server: v0.11.0
- Kubernetes:
  - Client: v1.14
  - Server: v1.12.8
- Kubernetes installer & version: AKS
- Cloud provider or hardware configuration: Azure AKS/Rancher (2.2.5)",,,prydonius,"
--
Thanks for filing the issue upstream with Rancher, I do think that it's odd for Rancher to store state in status. That said, I don't think it would hurt to add an annotation or flag (my preference would be to annotate items with something like `velero.io/restore-status: true`). It's possible that other Operators might use status in some meaningful way, so this could be generally useful.
--
`). It's possible that other Operators might use status in some meaningful way, so this could be generally useful.
--
author:	prydonius
association:	contributor
edited:	false
status:	none
--
wdyt @skriss @carlisia @nrb?
--

--
hello @skriss 

As Rancher work with custom resources definitions (xxxxxx.cattle.io) for all its object. I think It will be great to restore status at the CRD level (for all item in CRD with a wildcard for example).

With Rancher for example, when we create a cluster we have an object of kind `clusters.management.cattle.io` which are added in kubernetes with the following content :

```
apiVersion: management.cattle.io/v3
kind: Cluster
metadata:
  labels:
    cattle.io/creator: norman
  name: c-cpcnd
  resourceVersion: ""11616341""
  selfLink: /apis/management.cattle.io/v3/clusters/c-cpcnd
  uid: e34c0df9-cefe-11e9-8978-067a6cd806b3
spec:
  description: """"
  desiredAgentImage: """"
  desiredAuthImage: """"
  displayName: pp-test-aks2
  dockerRootDir: /var/lib/docker
  enableClusterAlerting: false
  [...]
status:
  agentImage: rancher/rancher-agent:v2.2.8
  allocatable:
    cpu: 5793m
    memory: 14003528Ki
    pods: ""330""
  apiEndpoint: https://xxxxxxxxxxxxxxxxxxxxxxxx:443
  appliedEnableNetworkPolicy: false
  appliedPodSecurityPolicyTemplateId: """"
  appliedSpec:
    description: """"
    desiredAgentImage: """"
    desiredAuthImage: """"
    displayName: pp-test-aks2
    dockerRootDir: /var/lib/docker
    enableClusterAlerting: false
    enableClusterMonitoring: false
    enableNetworkPolicy: false
    internal: false
    localClusterAuthEndpoint:
      enabled: false
  authImage: """"
  caCert: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
  capabilities:
    loadBalancerCapabilities:
      enabled: true
      healthCheckSupported: true
      protocolsSupported:
      - TCP
      - UDP
      provider: Azure L4 LB
  capacity:
    cpu: ""6""
    memory: 21339464Ki
    pods: ""330""
  componentStatuses:
  - conditions:
    - message: 'Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect:
        connection refused'
      status: ""False""
      type: Healthy
    name: controller-manager
  - conditions:
    - message: '{""health"": ""true""}'
      status: ""True""
      type: Healthy
    name: etcd-0
  - conditions:
    - message: 'Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect:
        connection refused'
      status: ""False""
      type: Healthy
    name: scheduler
  conditions:
  - status: ""True""
    type: Pending
  - lastUpdateTime: ""2019-09-04T10:50:03Z""
    status: ""True""
    type: Provisioned
  - lastUpdateTime: ""2019-09-04T10:50:19Z""
    status: ""True""
    type: Waiting
  - lastUpdateTime: ""2019-09-04T10:29:34Z""
    status: ""True""
    type: BackingNamespaceCreated
  - lastUpdateTime: ""2019-09-04T10:29:34Z""
    status: ""True""
    type: DefaultProjectCreated
  - lastUpdateTime: ""2019-09-04T10:29:34Z""
    status: ""True""
    type: SystemProjectCreated
  - lastUpdateTime: ""2019-09-04T10:29:34Z""
    status: ""True""
    type: InitialRolesPopulated
  - lastUpdateTime: ""2019-09-04T10:29:40Z""
    status: ""True""
    type: CreatorMadeOwner
  - lastUpdateTime: ""2019-09-04T10:31:35Z""
    status: ""True""
    type: NoDiskPressure
  - lastUpdateTime: ""2019-09-04T10:31:35Z""
    status: ""True""
    type: NoMemoryPressure
  - lastUpdateTime: ""2019-09-04T10:50:05Z""
    status: ""True""
    type: GlobalAdminsSynced
  - lastUpdateTime: ""2019-09-04T10:50:05Z""
    status: ""False""
    type: AlertingEnabled
  - lastUpdateTime: ""2019-09-04T10:50:19Z""
    status: ""True""
    type: Ready
  - lastUpdateTime: ""2019-09-04T10:50:25Z""
    status: ""True""
    type: SystemAccountCreated
  - lastUpdateTime: ""2019-09-04T10:50:26Z""
    status: ""True""
    type: AgentDeployed
  - lastUpdateTime: ""2019-09-04T10:50:26Z""
    status: ""False""
    type: PrometheusOperatorDeployed
  - lastUpdateTime: ""2019-09-04T10:50:49Z""
    status: ""True""
    type: Updated
  - lastUpdateTime: ""2019-09-04T10:50:43Z""
    status: ""True""
    type: ServiceAccountMigrated
  driver: aks2
  limits:
    cpu: 700m
    memory: 3140Mi
    pods: ""0""
  requested:
    cpu: 905m
    memory: 1114Mi
    pods: ""17""
  serviceAccountToken: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
  version:
    buildDate: ""2019-08-19T11:05:16Z""
    compiler: gc
    gitCommit: 96fac5cd13a5dc064f7d9f4f23030a6aeface6cc
    gitTreeState: clean
    gitVersion: v1.14.6
    goVersion: go1.12.9
    major: ""1""
    minor: ""14""
    platform: linux/amd64
```

And for example Rancher need to retrieve data like `status.serviceAccountToken` or `status.caCert` or `status.apiEndpoint`
--
author:	prydonius
association:	contributor
edited:	false
status:	none
--
Somewhat related to this, there are discussions just starting in SIG Apps to standardise the `.status` field which probably involves assumptions about not storing state in `.status`. 
--
",skriss,"
--
wdyt @skriss @carlisia @nrb?
--
author:	skriss
association:	member
edited:	false
status:	none
--
I believe there's another issue here, which is that `status` is actually ignored by the API server when a resource is created or updated, since it's not intended to be specified by the user.  To restore status, I believe we'd have to explicitly capture it and write it to the `/status` subresource for whatever resource we are operating on.  CRDs haven't always supported the `/status` subresource, so that may not be true for some older clusters/CRDs, but going forward it should be. I'm not sure off the top of my head exactly how we would implement this - there are likely complexities around not being able to create the `spec` and `status`  atomically, and possibly other issues.
--

--
I believe there's another issue here, which is that `status` is actually ignored by the API server when a resource is created or updated, since it's not intended to be specified by the user.  To restore status, I believe we'd have to explicitly capture it and write it to the `/status` subresource for whatever resource we are operating on.  CRDs haven't always supported the `/status` subresource, so that may not be true for some older clusters/CRDs, but going forward it should be. I'm not sure off the top of my head exactly how we would implement this - there are likely complexities around not being able to create the `spec` and `status`  atomically, and possibly other issues.
--
author:	skriss
association:	member
edited:	false
status:	none
--
We probably need to discuss as a team and see if this is something we want to invest further time in to investigate/prototype.
--

--
For various reasons, our application was developed in this way, and changing it is not easy at this point. Why does Velero add a restriction to Kubernetes objects which is explicitly not enforced by Kubernetes itself? In Kubernetes, a CRD which doesn't explicitly define a ""/status"" subresource is able to use the Status section like our application does. Velero's assumption breaks our flow, while removing this assumption will not hurt any use case as far as we can see no ?
--
author:	skriss
association:	member
edited:	false
status:	none
--
@rayanebel @i300543 could you provide a couple specific examples of the types of information that are stored in `status` that need to be restored?

Also, in terms of UX - would you want to enable/disable restoring `status` on a per-CRD level (i.e. for all items of a given CRD type), or at some other level of granularity?
--
",i300543,"
--
We probably need to discuss as a team and see if this is something we want to invest further time in to investigate/prototype.
--
author:	i300543
association:	none
edited:	false
status:	none
--
Hello, 
we are also trying to use velero for our cluster backups, and faced with similar issue.
We would rather prefer to keep the state in the status field in case it was not defined as a subresource:

1. We would like to keep some state in the object that is visible to the user as status, but not modifiable by the user
2. If the CRD object doesn't explicitly define /status as a subresource, then a backup/recover operation should not remove the status. Doing so means that objects that rely on it will fail, while not doing so would mean that objects that don't rely on it would still work correctly.
Nothing is required of the Velero code other than ""don't modify the backup as it was made.

Please let us know if and when you reach a decision.
Thanks!

SAP Webide dev team
--

--
Thanks for adding your use case here @i300543.

Originally this code was designed under the assumption that `status` would be recoverable based on the `spec`. Can you help me understand why that's not the case with your CRDs?
--
author:	i300543
association:	none
edited:	false
status:	none
--
For various reasons, our application was developed in this way, and changing it is not easy at this point. Why does Velero add a restriction to Kubernetes objects which is explicitly not enforced by Kubernetes itself? In Kubernetes, a CRD which doesn't explicitly define a ""/status"" subresource is able to use the Status section like our application does. Velero's assumption breaks our flow, while removing this assumption will not hurt any use case as far as we can see no ?
--
",nrb,"
--
Hello, 
we are also trying to use velero for our cluster backups, and faced with similar issue.
We would rather prefer to keep the state in the status field in case it was not defined as a subresource:

1. We would like to keep some state in the object that is visible to the user as status, but not modifiable by the user
2. If the CRD object doesn't explicitly define /status as a subresource, then a backup/recover operation should not remove the status. Doing so means that objects that rely on it will fail, while not doing so would mean that objects that don't rely on it would still work correctly.
Nothing is required of the Velero code other than ""don't modify the backup as it was made.

Please let us know if and when you reach a decision.
Thanks!

SAP Webide dev team
--
author:	nrb
association:	member
edited:	false
status:	none
--
Thanks for adding your use case here @i300543.

Originally this code was designed under the assumption that `status` would be recoverable based on the `spec`. Can you help me understand why that's not the case with your CRDs?
--
",rayanebel,"
--
@rayanebel @i300543 could you provide a couple specific examples of the types of information that are stored in `status` that need to be restored?

Also, in terms of UX - would you want to enable/disable restoring `status` on a per-CRD level (i.e. for all items of a given CRD type), or at some other level of granularity?
--
author:	rayanebel
association:	none
edited:	true
status:	none
--
hello @skriss 

As Rancher work with custom resources definitions (xxxxxx.cattle.io) for all its object. I think It will be great to restore status at the CRD level (for all item in CRD with a wildcard for example).

With Rancher for example, when we create a cluster we have an object of kind `clusters.management.cattle.io` which are added in kubernetes with the following content :

```
apiVersion: management.cattle.io/v3
kind: Cluster
metadata:
  labels:
    cattle.io/creator: norman
  name: c-cpcnd
  resourceVersion: ""11616341""
  selfLink: /apis/management.cattle.io/v3/clusters/c-cpcnd
  uid: e34c0df9-cefe-11e9-8978-067a6cd806b3
spec:
  description: """"
  desiredAgentImage: """"
  desiredAuthImage: """"
  displayName: pp-test-aks2
  dockerRootDir: /var/lib/docker
  enableClusterAlerting: false
  [...]
status:
  agentImage: rancher/rancher-agent:v2.2.8
  allocatable:
    cpu: 5793m
    memory: 14003528Ki
    pods: ""330""
  apiEndpoint: https://xxxxxxxxxxxxxxxxxxxxxxxx:443
  appliedEnableNetworkPolicy: false
  appliedPodSecurityPolicyTemplateId: """"
  appliedSpec:
    description: """"
    desiredAgentImage: """"
    desiredAuthImage: """"
    displayName: pp-test-aks2
    dockerRootDir: /var/lib/docker
    enableClusterAlerting: false
    enableClusterMonitoring: false
    enableNetworkPolicy: false
    internal: false
    localClusterAuthEndpoint:
      enabled: false
  authImage: """"
  caCert: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
  capabilities:
    loadBalancerCapabilities:
      enabled: true
      healthCheckSupported: true
      protocolsSupported:
      - TCP
      - UDP
      provider: Azure L4 LB
  capacity:
    cpu: ""6""
    memory: 21339464Ki
    pods: ""330""
  componentStatuses:
  - conditions:
    - message: 'Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect:
        connection refused'
      status: ""False""
      type: Healthy
    name: controller-manager
  - conditions:
    - message: '{""health"": ""true""}'
      status: ""True""
      type: Healthy
    name: etcd-0
  - conditions:
    - message: 'Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect:
        connection refused'
      status: ""False""
      type: Healthy
    name: scheduler
  conditions:
  - status: ""True""
    type: Pending
  - lastUpdateTime: ""2019-09-04T10:50:03Z""
    status: ""True""
    type: Provisioned
  - lastUpdateTime: ""2019-09-04T10:50:19Z""
    status: ""True""
    type: Waiting
  - lastUpdateTime: ""2019-09-04T10:29:34Z""
    status: ""True""
    type: BackingNamespaceCreated
  - lastUpdateTime: ""2019-09-04T10:29:34Z""
    status: ""True""
    type: DefaultProjectCreated
  - lastUpdateTime: ""2019-09-04T10:29:34Z""
    status: ""True""
    type: SystemProjectCreated
  - lastUpdateTime: ""2019-09-04T10:29:34Z""
    status: ""True""
    type: InitialRolesPopulated
  - lastUpdateTime: ""2019-09-04T10:29:40Z""
    status: ""True""
    type: CreatorMadeOwner
  - lastUpdateTime: ""2019-09-04T10:31:35Z""
    status: ""True""
    type: NoDiskPressure
  - lastUpdateTime: ""2019-09-04T10:31:35Z""
    status: ""True""
    type: NoMemoryPressure
  - lastUpdateTime: ""2019-09-04T10:50:05Z""
    status: ""True""
    type: GlobalAdminsSynced
  - lastUpdateTime: ""2019-09-04T10:50:05Z""
    status: ""False""
    type: AlertingEnabled
  - lastUpdateTime: ""2019-09-04T10:50:19Z""
    status: ""True""
    type: Ready
  - lastUpdateTime: ""2019-09-04T10:50:25Z""
    status: ""True""
    type: SystemAccountCreated
  - lastUpdateTime: ""2019-09-04T10:50:26Z""
    status: ""True""
    type: AgentDeployed
  - lastUpdateTime: ""2019-09-04T10:50:26Z""
    status: ""False""
    type: PrometheusOperatorDeployed
  - lastUpdateTime: ""2019-09-04T10:50:49Z""
    status: ""True""
    type: Updated
  - lastUpdateTime: ""2019-09-04T10:50:43Z""
    status: ""True""
    type: ServiceAccountMigrated
  driver: aks2
  limits:
    cpu: 700m
    memory: 3140Mi
    pods: ""0""
  requested:
    cpu: 905m
    memory: 1114Mi
    pods: ""17""
  serviceAccountToken: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
  version:
    buildDate: ""2019-08-19T11:05:16Z""
    compiler: gc
    gitCommit: 96fac5cd13a5dc064f7d9f4f23030a6aeface6cc
    gitTreeState: clean
    gitVersion: v1.14.6
    goVersion: go1.12.9
    major: ""1""
    minor: ""14""
    platform: linux/amd64
```

And for example Rancher need to retrieve data like `status.serviceAccountToken` or `status.caCert` or `status.apiEndpoint`
--
",ChrisHaPunkt,"
--
Somewhat related to this, there are discussions just starting in SIG Apps to standardise the `.status` field which probably involves assumptions about not storing state in `.status`. 
--
author:	ChrisHaPunkt
association:	none
edited:	true
status:	none
--
Same here. In our migration scenario, Rancher is recreating the System and Default project, because it is not aware of the fact that these were already created und restored by the backup.
So we are ending up with two System and two Default projects.
![grafik](https://user-images.githubusercontent.com/4389395/67276059-2dc28180-f4c4-11e9-993d-a2201d6370ba.png)

--
"
1655,OPEN,Velero need to support custom credential provider,Enhancement/User; IAM; Needs Product; P3 - Wouldn't it be nice if...; Security,2020-10-22 21:47:04 +0000 UTC,taojwmware,Opened,,"**Describe the problem/challenge you have**

Currently, Velero can only use long-lived credentials or kube2iam in order to access storage location. However, kube2iam only works for AWS and using long-lived credentials have negative security implication.  We would like Velero uses temporary security credentials instead.

**Describe the solution you'd like**

We'd like Velero be able to inject a credential provider and get credentials on-demand and the credentials provider can provide temporary credentials instead.


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`):
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):
",,,ashish,"
--
Implementing a credential provider as a plugin seems to be the right solution to this problem. However that will require a major revamp of our plugin framework. Requiring the the storage/ volumesnapshot plugins to talk to the credential provider plugin. This revamp will take a longer time to have a solution to this problem. 
Based on the discussions with @nrb and others, we might be better of keeping this lightweight and potentially reuse existing tools for credentials management. E.g. credhub https://www.youtube.com/watch?v=uevyVtnPOwI
--

--
The other approach may be to delegate the credential management to each of the plugins. This would couple credential management with the plugins that may use it. It will also give each individual plugins the ability to support their own credential management at their own pace and flexibility in their implementation. 
IMO, this approach is a nice middle ground between having an out of band credential getter and a credential plugin that works across provider implementations.
This approach however, will force plugins to run with only one way of managing credentials, in the case where multiple may be available.
--
",nrb,"
--
> The other approach may be to delegate the credential management to each of the plugins.

Which plugin types? ObjectStore and VolumeSnapshotter?
--
",,,,,,,,
1624,OPEN,Add support for restoring to another zone [GCP],Area/Cloud/GCP; Enhancement/User; Needs Product; P2 - Long-term important,2020-08-20 11:56:18 +0000 UTC,kanuahs,In progress,,"**Describe the solution you'd like**
I've been evaluating velero for disaster recovery. One of the use cases is restoring a failed cluster in another availability zone. I want to take a backup of a GKE cluster which is running in one zone (eg. us-east1-b) and restore it in another zone in the same region (eg. us-east1-c).
Apart from Statefulsets & PVs, everything seems to be working.
The PVs get provisioned in the first zone (us-east1-b) and can't be mounted.

When restoring PVs from snapshots, the AZ of the previous volume is used. I'd like to overrride this when restoring the backup.

**Anything else you would like to add:**
Persistent disk snapshots in GCP are regional or multi-regional, so it is possible to restore them into another zone.

**Environment:**

- Velero version (use `velero version`):
Client:
	Version: v1.0.0
	Git commit: 72f5cadc3a865019ab9dc043d4952c9bfd5f2ecb
Server:
	Version: v1.0.0

- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.2"", GitCommit:""66049e3b21efe110454d67df4fa62b08ea79a19b"", GitTreeState:""clean"", BuildDate:""2019-05-16T16:23:09Z"", GoVersion:""go1.12.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""11+"", GitVersion:""v1.11.10-gke.5"", GitCommit:""5aa3a95d828fe45aab3611dfc4ebdc0341fe1507"", GitTreeState:""clean"", BuildDate:""2019-05-29T17:25:39Z"", GoVersion:""go1.10.8b4"", Compiler:""gc"", Platform:""linux/amd64""}

- Kubernetes installer & version: GKE
- Cloud provider or hardware configuration: 4x n1-standard-4
- OS (e.g. from `/etc/os-release`): Container-Optimized OS 
",,,skriss,"
--
Thanks for the request @kanuahs! We'll evaluate and update with more detail/questions here.
--

--
xref #1755 
--

--
Cool, thanks for that @stevegore!
--
",nrb,"
--
I believe this is already possible using [regional disks in Kubernetes](https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/regional-pd). PR #1200 fixed a bug that prevented it, but it should work as long as a regional disk is used.
--

--
That makes a whole lot of sense to me, @dijitali.

Yes, addressing #103 is what would enable this from the Velero side. It was in the planning stages but was de-prioritized. The `BackupStorageLocation` and `VolumeSnapshotLocation` CRDs were introduced as prerequisites for the work.

@VMmore and @skriss, we should revisit where we think replication should go in the long term roadmap. I still believe it's likely to a major release.
--
",dijitali,"
--
@nrb - unfortunately regional disks have a minimum size of 200GB:

> Regional persistent disk limits are similar to zonal persistent disks. However, regional standard persistent disks have a 200 GB size minimum.

From [Regional persistent disks (Standard and SSD): Limits](https://cloud.google.com/compute/docs/disks/#repds)

This puts the cheapest EU disk at $68/month each (using `europe-west1`).  This adds a significant cost to storage when smaller disks would suffice (we currently use several PersistentVolumes with <20GB disks, costing $3.4/month).

I realise this is a restriction incurred by GCP, not Velero, but as a result it makes this solution a bit untenable for our use case.

It sounds like this might be addressed by the work in #103 though?
--
",kanuahs,"
--
GCP PersistentDisk snapshots are regional(us-east) or multi-regional(us) even for zonal PersistentDisks.
They can be restored into another zone without doing any replication.

The node affinity field in PersistentVolume templates is causing the restored GCP PersistentDisks to be provisioned in the wrong zone (the source cluster's zone).

I think this issue could be fixed for GCP by either removing the node affinity and letting it get auto-generated when restoring or adding an option to override it with another zone/region before restoring PVs.

I was able to get velero to restore into another zone by downloading the compressed archives from the bucket and changing the NodeAffinity field in PersistentVolume templates from source cluster's zone (us-east1-b) to target cluster's zone (us-east1-c) as well as the volumeAZ field in the VolumeSnapshots.json file.

@nrb @skriss 
--
",mizunos,"
--
I am experiencing this issue when trying to restore PVC to a cluster in another GCP project and region.  A couple of questions about the work around you mentioned about i.e NodeAffinity field in the PersistentVolume templates :1 -  is it being done to the backup zip files before the restore then restore? 2 - I can't seem to find any persistent volume template in the backup file (there are pvc objects that are backed up),  can you give more specific directory names>.  Thanks
--
",stevegore,"
--
I'll share the script I created to resolve my own issue.

I needed to do this all in a Docker container as my Mac kept inserting `._` files.

1. Spin up a container with the GCP SDK: `docker run --rm -it google/cloud-sdk:alpine`
2. Authenticate: `gcloud auth login`
3. Run the below

```bash
export PROJECT_NAME=yourvalueshere
export BACKUP_NAME=yourvalueshere
export BUCKET_NAME=yourvalueshere
export OLD_REGION=yourvalueshere
export OLD_ZONE=yourvalueshere
export NEW_REGION=yourvalueshere
export NEW_ZONE=yourvalueshere

gcloud config set project ""$PROJECT_NAME""

mkdir fix
cd fix

# Volume snapshots
gsutil cp gs://$BUCKET_NAME/backups/$BACKUP_NAME/$BACKUP_NAME-volumesnapshots.json.gz  .
gunzip $BACKUP_NAME-volumesnapshots.json.gz
sed -i ""s/$OLD_ZONE/$NEW_ZONE/g"" $BACKUP_NAME-volumesnapshots.json
sed -i ""s/$OLD_REGION/$NEW_REGION/g"" $BACKUP_NAME-volumesnapshots.json
gzip $BACKUP_NAME-volumesnapshots.json

# Bigger file
gsutil cp gs://$BUCKET_NAME/backups/$BACKUP_NAME/$BACKUP_NAME.tar.gz .
mkdir $BACKUP_NAME-temp
tar xzf $BACKUP_NAME.tar.gz -C $BACKUP_NAME-temp
cd $BACKUP_NAME-temp
find . -name \*.json -exec sh -c ""sed -i 's/$OLD_ZONE/$NEW_ZONE/g' {}"" \;
find . -name \*.json -exec sh -c ""sed -i 's/$OLD_REGION/$NEW_REGION/g' {}"" \;
tar czf ../$BACKUP_NAME.tar.gz *
gsutil cp ../$BACKUP_NAME.tar.gz gs://$BUCKET_NAME/backups/$BACKUP_NAME
```
--

--
> Most notably, it seems like your version doesn't copy the gzipped volumesnapshots.json back to the bucket.

Looks like I made some mistakes in trying to make my script ""GitHub ready"" :). Regardless yours is way better - thank you!
--
"
1619,OPEN,Consider downloading all metadata files as part of `velero backup download`,Enhancement/User; P2 - Long-term important,2019-07-02 16:53:39 +0000 UTC,skriss,Opened,,"**Describe the solution you'd like**
[A clear and concise description of what you want to happen.]

Currently, `velero backup download` only downloads the backup tarball. We should consider also downloading all of the metadata files (the backup CR YAML, volumesnapshots, etc), so that in theory the complete file set could be re-uploaded to object storage and restored by another Velero instance.",,,,,,,,,,,,,,
1540,OPEN,Replace the in-process restic repository lock management with restic’s built-in lock,Enhancement/Dev; Restic,2020-12-08 01:52:42 +0000 UTC,carlisia,In progress,,,,,skriss,"
--
Restic has a concept of repository locks.  All restic operations create either an exclusive or non-exclusive lock on the repository they're operating on. A non-exclusive lock can be created *only* if there are no **exclusive** locks that currently exist; an exclusive lock can be created *only* if there are **no** locks that currently exist.  If a restic CLI command is run and the necessary lock cannot be acquired, the restic CLI exits with an error.

Velero currently has an in-process lock manager that provides a coordination layer on top of restic's locks. It uses a `sync.RWMutex` since the semantics are the same as restic's exclusive/non-exclusive locks.  The benefit of this component is that Velero code will **wait** to run restic operations until the necessary lock can be acquired, rather than just running the command,  potentially getting an error back, and having to retry over and over.

The problem with Velero's lock manager is that it's in-process, i.e. in the Velero deployment pod.  If/when we move to running backups/restores in separate pods, they won't have any way to interact with this lock manager.  It could potentially be moved out-of-process (e.g. by using a Kubernetes ConfigMap to maintain the state of the locks) but implementing a distributed read-write lock is non-trivial.

I propose that we eliminate the Velero lock manager, and modify our code that makes restic CLI calls to check for returned errors that indicate a lock could not be acquired, and to retry with backoff if that happens.  This way we're only using restic's built-in lock management.  
--

--
As we look at #1511, let's consider whether there's any overlap between this issue and that one. If not, then we'll move this issue to v1.2.
--

--
Moved out of the v1.1 backlog.
--

--
I've been thinking that maybe we should defer this until we tackle #1653, which may not be in the next release or two given all the other priorities. Otherwise, implementing a fix for this issue is fairly disruptive and doesn't buy us much.
--

--
@carlisia @nrb @prydonius @VMmore interested in your thoughts on deferring this until we tackle #1653.
--

--
Yeah, sorry, my wording was unclear. This issue remains a prereq for #1653. I'm just proposing bundling it with #1653 (which isn't yet scheduled), since doing this issue itself doesn't really provide benefit until we go to tackle #1653. So, e.g., if #1653 gets scheduled for 2.0, then we do this issue in 2.0 as well (or just prior).
--
",prydonius,"
--
@skriss from the description of this, it sounds like this would be a pre-requisite if backups/restores were to be moved out to separate pods? Or did you just mean to defer until we want to tackle #1653 in the same release, but still implement before?
--
",carlisia,"
--
> The problem with Velero's lock manager is that it's in-process, i.e. in the Velero deployment pod. If/when we move to running backups/restores in separate pods, they won't have any way to interact with this lock manager. 

From that ^ statement, sounds like it would be needed before, like @prydonius pointed out. Even if for some reason this is not the case, I wonder if it wouldn't be easier to make this change and test it while we are not yet running on multiple workers.
--

--
Yes, it makes sense to bundle those two since there's no gain to be had in doing it now other than removing code, which can wait.
--

--
Un-assigning myself since I'm not actively working on this.
--
",,,,,,
1539,OPEN,[Epic] Encryption at rest support,Enhancement/User; Epic; Needs Product; Security,2020-06-29 23:19:40 +0000 UTC,VMmore,Opened,,"Velero should support Encryption at rest as an option for users.
* Backup data should be optionally encrypted in object storage
* Snapshots should be stored as encrypted when supported by the storage provider
* Restic backup data should be encrypted in the Restic repository
* Users should be able to provide a unique encryption key to be used for their backups
* Velero should support key rotation and management and/or KMSes",,,skriss,"
--
xref #434 
--
",,,,,,,,,,
1531,OPEN,[RFE] support running restic backups concurrently,Enhancement/User; Performance; Restic,2020-06-09 20:49:12 +0000 UTC,king-jam,Opened,,"**Describe the solution you'd like**
When a single request is made to Velero to back up multiple applications/pods (ie: backing up an entire namespace), resources within the backup job are backed up sequentially, rather than backing up all resources in parallel (concurrently). This is an issue when the list of resources contains large PVs, because the backup job takes longer than desired. Want to make the job execute with workers if possible.

**Environment:**

- Velero version (use `velero version`): master/1.0.0
- Kubernetes version (use `kubectl version`): master/v1.14
- Kubernetes installer & version: N/A
- Cloud provider or hardware configuration: Restic/Minio
- OS (e.g. from `/etc/os-release`): Ubuntu/CentOS
",,,skriss,"
--
Thanks for logging this @king-jam. We think we may be able to up the number of workers running for each controller in the restic daemonset and get the desired parallelism here without any material code changes - we'll definitely queue this up, as we're planning on doing a bunch of work with the restic integration over the next release or two.
--

--
@duyanghao the # of workers is set at https://github.com/vmware-tanzu/velero/blob/master/pkg/cmd/cli/restic/server.go#L174 and https://github.com/vmware-tanzu/velero/blob/master/pkg/cmd/cli/restic/server.go#L191, but as @king-jam noted, this would only get us parallelism across multiple volumes within a single pod, not parallelism across pods. 
--

--
@stephbman I think this would be more around improving the performance of a _single_ backup, since it would involve parallelizing the operations _within_ a Velero backup.

Re: technical design, we could consider using a worker pods-like approach here but I'm not sure it's actually necessary; the existing restic daemonset can probably already handle running multiple operations simultaneously, so it'd just be a matter of having Velero trigger them in parallel rather than sequentially.
--
",king,"
--
So if I'm reading the code correctly, we will have to up the number of workers but that won't resolve the issue.

I believe the restic daemonset would be able to do parallel backups but the itemBackup code still gets executed sequentially so the PVs would still get processed in a synchronous sequential way. 

I think the solution is to make the itemBackup code concurrent (# of workers) AND the code for PVs concurrent. This handles multiple pods with a single PV attached to each and the case of a single pod with many PVs attached.
--
",ac,"
--
Are there any timelines as to when these improvements to restic will be implemented?
--
",duyanghao,"
--
> Are there any timelines as to when these improvements to restic will be implemented?

+1
--

--
> Thanks for logging this @king-jam. We think we may be able to up the number of workers running for each controller in the restic daemonset and get the desired parallelism here without any material code changes - we'll definitely queue this up, as we're planning on doing a bunch of work with the restic integration over the next release or two.

@skriss How to up the number of workers running for each controller in the restic daemonset? Is there any arguments?
--
",ThoTischner,"
--
Any news on this? I think we are having scaling issues because of the sequential restic backups.
--
",stephbman,"
--
@skriss based on review of some of the issues, my feeling that this may need to be linked with #1653 - what are  your thoughts?
--
"
1519,OPEN,[Epic] Two stage API for the snapshot and backup/upload snapshot,Area/Plugins; Enhancement/User; Epic; Needs Product,2020-10-22 02:19:42 +0000 UTC,satyamz,In progress,,"Hi Velero community :wave: 

We have written a velero plugin which implements all the BlockStore APIs aka VolumeSnapshotter APIs. Today, we only have a `CreateSnapshot` API where we take a snapshot and upload it in one go because, openebs velero plugin implements an API `CreateSnapshot` which takes the snapshot and uploads data. 

When we have an application like mysql, we need to apply pre-hooks and post-hooks to freeze the io's coming to the application, so that, we can take an application consistent snapshot. However, in today's velero implementation, we apply the pre-hook on the Pod, then find PVC, PV and once PV is found, take the snapshot using plugin and apply post-hook. 

In most of the blockStore plugin cases, plugins do the upload data, as a part of the `CreateSnapshot` API execution. This creates an unecessary down time for an application. Application io's will be frozen for the time equivalent to the time requires to upload the data. Velero should provide a separate API to upload the backup which will be executed as soon as snapshot is taken and post-hooks are executed.


Thanks!!
",,,skriss,"
--
@satyamz thanks for filing this issue! really helpful to get this kind of feedback from plugin authors. we have definitely already had some thoughts and discussions along these lines.
--

--
@satyamz sorry for the delay in getting back here. At first glance this makes a lot of sense, and I think would help not just for your plugin but for others as well.

In your POC, what did the signature for `UploadSnapshot` look like, and were there any changes necessary for `CreateSnapshot`?
--

--
Also, in terms of execution flow - could `UploadSnapshot` execute in the background while other resources are being backed up? I'm guessing yes, and we'd just want to wait for all of the `UploadSnapshot` calls to complete before completing the overall backup. 
--

--
cc @VMmore 
--

--
👍 thanks for the info. One thing we'll need to think about is whether adding this function to the interface would be considered a breaking change/necessitate a 2.0, or whether we can gracefully handle plugins that don't implement the function. This isn't really something we've tackled in Velero before.
--

--
I played around with this some, and it looks like if we add a new method to the VolumeSnapshotter interface, existing plugin implementations that don't implement it will return a grpc error with a code of `Unimplemented` when the call is attempted. So we could check for this type of error in the Velero code and ignore it if encountered, which would maintain our backwards-compatibility.
--

--
I'd like to have a brief design doc written up containing the proposal, that we can review with the community and especially other plugin authors before moving on to execution. I just opened a PR this morning with a proposed design doc template (https://github.com/heptio/velero/pull/1636) - the idea is that you'd copy the template, fill out the details, then open a PR against this repo with your design doc for review/comment. Specifically I'd like to lock down the signature of the new function and how the backup execution flow changes (scope of hooks, any phase changes, etc).

Once the design doc is created, it'd be awesome if you could present it at a Velero community meeting.

Does that sound reasonable?
--

--
cc @nrb @carlisia @prydonius 
--

--
hi @satyamz - just wondering if you were able to start work on a design doc here? let me know if I can provide any input!
--
",satyamz,"
--
Hi @skriss , 

We are very much interested in discussing how this can be solved. We tried to resolve this by adding an additional method to the `VolumeSnapshotter` interface called `UploadSnapshot` which would be called just after the Post hook of the Pod is executed. We actually got it working for taking snapshot using `CreateSnapshot` and uploading the backup / snapshot data using `UploadSnapshot`. I want to validate this approach. 
--

--
Hi @skriss, Thanks for considering this. :slightly_smiling_face: 


> At first glance this makes a lot of sense, and I think would help not just for your plugin but for others as well. 

Yes! It can be useful for all the storage providers. 

> In your POC, what did the signature for UploadSnapshot look like, and were there any changes necessary for CreateSnapshot? 

```go
func UploadSnapshot(volumeID string) error {
...
}
```
AFAIK, There are two kind of storage snapshots. first one where snapshot is present as a file (copy of the data) on the disk (Like restic does for the localPV) and second type where snapshot is CoW which is logical entity altogether. In both the cases, `volumeID` can help us identify the volume, in our case i.e. `openebs/velero-plugin` we need `backupName` as well as `volumeId`. We find the `backupName` with help of `volumeID`. Need of having `backupName` for upload method differs from the provider to provider. There's no change in `CreateSnapshot` method.

> Also, in terms of execution flow - could UploadSnapshot execute in the background while other resources are being backed up? 

Yes, that's correct. Also, we should call `UploadSnapshot` as soon as post hook on **Pod** is released. We might also need to create multiple `go routines` calling `UploadSnapshot` method for multiple volumes. Implementation for this is not quite clear to me. We tried it for the single volume. And it worked!! 

> we'd just want to wait for all of the UploadSnapshot calls to complete before completing the overall backup. 

Yes! We should mark backup as completed only when all `UploadSnapshot` methods are returned.

--

--
> One thing we'll need to think about is whether adding this function to the interface would be considered a breaking change/necessitate a 2.0, or whether we can gracefully handle plugins that don't implement the function.

Good point :+1:  

We need to add a method to the `volumesnapshotter` interface, hence there'll be changes in the   `volumesnapshotter.proto` which means we need to generate the gRPC code and update `GRPCServer` and `GRPCClient` implementation for the `UploadSnapshot`  method in the Velero. These steps need to be followed for the **Plugin** too. So yes, this is something which is required on both the side Velero and plugin. 
I had figured this out a few months back. Let me check if I can find some time get it to work for velero 1.0 .
--

--
Sounds good! How can we proceed further? 
--

--
Hi @skriss

> Once the design doc is created, it'd be awesome if you could present it at a Velero community meeting.
Does that sound reasonable?

Yes, our overall approach is remarkable. I will start writing the design document and open a PR for the same with the problem statement and approach to solving the problem by covering topics discussed here.

Thank you!
 

--

--
Hi @skriss, Apologies. I was dragged into something else. I should send PR this week. 
--
",carlisia,"
--
+1. @satyamz and we can do a review prior to you presenting this if you'd like to present, which would be super useful.
--

--
> So we could check for this type of error in the Velero code and ignore it

I think this is completely harmless and since it's temporary until we can make a breaking release, I'm on board. Maybe logging should be done alongside ignoring the error.
--
",,,,,,
1512,OPEN,Explicit NodePort values defined in Helm charts are deleted during restoration,Bug; Needs Product,2019-09-30 18:08:23 +0000 UTC,Rathgore,In progress,,"**What steps did you take and what happened:**

1. Deploy a Helm chart containing a K8s `Service` with explicitly defined NodePort port values.
1. Create a Velero backup of the service.
1. Delete the service.
1. Restore the backup of the service.
1. Note that the explicit NodePort port values are removed, and new, random port numbers are assigned.

**What did you expect to happen:**

Explicitly defined NodePort values should be restored from the backup.

**Anything else you would like to add:**

This appears to be caused by the `deleteNodePorts` function (see below). As written it looks for the presence of the `kubectl`-specific annotation `kubectl.kubernetes.io/last-applied-configuration` which it uses to restore the port values. Only `kubectl` creates this annotation so the port restoration fails when using other deployment methods such as Helm:

https://github.com/heptio/velero/blob/a111eed2af8c0bb37ecd6d682b3faad116fbad2f/pkg/restore/service_action.go#L70-L100

A potential solution would be to support reading a custom annotation on K8s services that would direct Velero to preserve and not delete any custom port values.

**Possible workarounds:**

* Use `helm template` and pipe through `kubectl apply`. This won't work for us as we depend on the release management features of Helm.
* Create a _Restore Item Action_ Velero plugin to force the restoration of the NodePort values. It is unclear to me if this plugin would execute at the right stage so that these values would be preserved and not just removed again in the final restore.

**Environment:**

- Velero version (use `velero version`): 

```
Client:
	Version: v1.0.0
	Git commit: 72f5cadc3a865019ab9dc043d4952c9bfd5f2ecb
Server:
	Version: v1.0.0
```
- Kubernetes version (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""13"", GitVersion:""v1.13.4"", GitCommit:""c27b913fddd1a6c480c229191a087698aa92f0b1"", GitTreeState:""clean"", BuildDate:""2019-02-28T13:37:52Z"", GoVersion:""go1.11.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""10"", GitVersion:""v1.10.13"", GitCommit:""954ff68d59e9dc62fa8252ffa9023a90ff8a358c"", GitTreeState:""clean"", BuildDate:""2019-02-13T11:03:32Z"", GoVersion:""go1.9.3"", Compiler:""gc"", Platform:""linux/amd64""}
```
",,,skriss,"
--
@Rathgore thanks for the issue report - your analysis looks correct. The challenge is knowing whether or not the NodePort value was explicitly specified by the user, or auto-assigned by the system. In the absence of being able to figure this out systematically for all cases, I agree that giving the user some kind of control over whether these get restored or not probably makes sense.
--
",Rathgore,"
--
Thanks @skriss. I am tempted to pursue writing a plugin as a workaround. Would a _Restore Item Action_ plugin work in this case? Does it execute after `deleteNodePorts`?
--
",bernardomk,"
--
Same issue here. I don't know how velero restore works but the port  looks fine in the json, so it's really weird that this is only happening with services created by helm charts.

As a workaround we're patching the service as we have a fixed port, but it's an additional step we rather not have in our code.
--
",,,,,,
1508,OPEN,Support filtering by multiple labels/resources,Enhancement/User,2019-05-20 19:12:34 +0000 UTC,tillepille,In progress,,"**Describe the solution you'd like**

Currently 
```
velero create backup mulit -l 'backup=foo' -l 'backup=bar'

or

velero create backup multi --include-resources 'secrets' --include-resources 'configmaps'
```
means, that the flag gets overwritten.  
So only configmaps / things with the label `backup=bar` will be backed up.

It would be great if i could specify multiple resource types or label combinations. 

 
**Anything else you would like to add:**
I know that i can specify multiple labels in one string like `-l 'backup=foo,shouldbackup=true'`
but this is an *AND* not an *OR* 

In my usecase its not possible to add one label to all the things i want to include into the backup.

**Environment:**

- Velero version (use `velero version`): v1.0.0
- Kubernetes version (use `kubectl version`):v1.14.2
- Kubernetes installer & version:v1.14.0
- Cloud provider or hardware configuration: hetzner (backup into aws s3)
- OS (e.g. from `/etc/os-release`): Client debian:stretch-slim (container) / Server Ubuntu:18.04
",,,skriss,"
--
@tillepille thanks for reporting the issue, this is a known limitation.  If it's not possible to specify in one set of flags, for now your best bet is to set up multiple backups, each with their own filter. 
--
",tillepille,"
--
@skriss Exactly, multiple backups are my current setup, but this kind of eliminates the benefits of using velero.

Do you have a starting point in the point for me? 
I'm probably to bad at Go but i would like to see if i can do something, or at least better understand the limitation.

Thanks for your fast reply!

--
",,,,,,,,
1492,OPEN,add support for uploading/restoring a local backup,Enhancement/User; Needs Product; P2 - Long-term important,2019-07-08 18:44:43 +0000 UTC,zencircle,Opened,,"**What steps did you take and what happened:**
[A clear and concise description of what the bug is, and what commands you ran.)
velero backup download nginx-backup
Backup nginx-backup has been successfully downloaded to /USERHOMEDIR/nginx-backup-data.tar.gz

velero restore create --from-backup nginx-backup-data.tar.gz
An error occurred: backups.velero.io ""nginx-backup-data.tar.gz"" not found
**What did you expect to happen:**
Restore from file should work

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

* `kubectl logs deployment/velero -n velero`
https://gist.github.com/zencircle/c0d1f9ec81ba89716f19bf65c408d5ec
* `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`
velero backup describe nginx-backup 
An error occurred: backups.velero.io ""nginx-backup"" not found
* `velero backup logs <backupname>`
* `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml`
* `velero restore logs <restorename>`
Note: I manually deleted the backup to test from file

**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]


**Environment:**

- Velero version (use `velero version`): 
velero version
Client:
	Version: 0.11.0
	Git commit: -
Server:
	Version: v0.11.0

- Kubernetes version (use `kubectl version`):
Client Version: version.Info{Major:""1"", Minor:""11"", GitVersion:""v1.11.3"", GitCommit:""a4529464e4629c21224b3d52edfe0ea91b072862"", GitTreeState:""clean"", BuildDate:""2018-09-10T11:44:26Z"", GoVersion:""go1.11"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.1"", GitCommit:""b7394102d6ef778017f2ca4046abbaa23b88c290"", GitTreeState:""clean"", BuildDate:""2019-04-08T17:02:58Z"", GoVersion:""go1.12.1"", Compiler:""gc"", Platform:""linux/amd64""}
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
AWS
- OS (e.g. from `/etc/os-release`):
Red Hat Enterprise Linux Server release 7.6 (Maipo)
",,,nrb,"
--
Thanks for this report! This is a known issue currently. The `--from-backup` parameter takes the name of a backup, then looks it up in object storage. We do not currently have a way to upload it to the Velero server to restore from, as the Velero server isn't exposed outside the kubernetes cluster via a service (all interactions are via creating/reading custom resources via the API server).

However, I think this is definitely a use case for us to consider, so I'm going to mark this as an enhancement request for us to track.
--
",,,,,,,,,,
1477,OPEN,velero restore describe xxx --details provides no additional snapshot details,Area/Plugins; Enhancement/User; P2 - Long-term important,2020-10-22 17:26:34 +0000 UTC,cormachogan,Opened,,"**What steps did you take and what happened:**
When working with Velero 1.0.0-RC and Portworx RC plugin, I noticed that the command *velero restore describe xxx* and *velero restore describe xxx --details* return exactly the same information


**What did you expect to happen:**
As per the *velero backup describe xxx* and *velero backup describe xxx --details* commands, I expected the --details to provide additional snapshot related information.


**The output of the following commands will help us better understand what's going on**:
`cormac@pks-cli:~$ velero restore describe cassandra-restore
Name:         cassandra-restore
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:  Completed

Backup:  cassandra

Namespaces:
  Included:  *
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io
  Cluster-scoped:  auto

Namespace mappings:  <none>

Label selector:  <none>

Restore PVs:  auto
cormac@pks-cli:~$ velero restore describe cassandra-restore --details
Name:         cassandra-restore
Namespace:    velero
Labels:       <none>
Annotations:  <none>

Phase:  Completed

Backup:  cassandra

Namespaces:
  Included:  *
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io
  Cluster-scoped:  auto

Namespace mappings:  <none>

Label selector:  <none>

Restore PVs:  auto
`

**Anything else you would like to add:**
I discussed this with @nrb in slack. Nolan walked the code and saw the problem - the details are only used for the podvolumerestores portion, which wouldn't be available with my Portworx deployment. 

Nolan recommended filing an issue to help track it, though it may be marked as a known issue for the 1.0 release


**Environment:**

- Velero version (use `velero version`):  1.0.0-RC
- Kubernetes version (use `kubectl version`): 1.14.1
- Cloud provider or hardware configuration: vSphere
- OS (e.g. from `/etc/os-release`): 6.7U3 (pre-release)
- Portworx version: 2.0.3.4-0c0bbe4
- Portworx plugin for Velero: portworx/velero-plugin:1.0.0-rc1
",,,nrb,"
--
This is also present with vSphere volumes now, as Velero specifically only looks at PodVolumeBackups, our Snapshots file, and CSI VolumeSnapshots/VolumeSnapshotContents. This should be expanded to allow description of snapshots from various plugin sources.
--
",,,,,,,,,,
1476,OPEN,Support both snapshot and Restic backup for the same volume,Enhancement/User; P2 - Long-term important; Restic,2021-01-20 18:21:29 +0000 UTC,vitobotta,Opened,,"I would like to be able to quickly restore a volume from a local snapshot if something happens, as well as easily migrate to another cluster/provider with Restic if needed. 

On Slack @carlisia explained to me that

>You CAN use Velero’s native snapshot AND restic in the same Velero instance, but not for the same PV,. If both are configured for a PV, restic wins over the native snapshots.

So at the moment it seems I can't have both.  Ideally I would have frequent snapshots and less frequent or on demand backups with Restic for the purpose mentioned.

Thanks!",,,skriss,"
--
xref #957 
--
",danielsand,"
--
out of curiosity ... is this still planed ?
--
",,,,,,,,
1456,OPEN,Ability to rename backups,Enhancement/User; P2 - Long-term important,2019-05-07 15:54:47 +0000 UTC,carlisia,Opened,,"Since we preclude the user from creating a backup if it already exists, they have to delete it first before creating one with that same name. Maybe they want to keep it around and could just rename it if we give them this ability.",,,,,,,,,,,,,,
1444,OPEN,Restic backup stuck in progress if node becomes unavailable during backup,P2 - Long-term important; Restic,2019-06-06 17:33:48 +0000 UTC,vitobotta,In progress,,"**What steps did you take and what happened:**

During a test, I had to forcefully reboot a node while Velero was backing up with Restic. The pod that was using the volumes being backed up was on that node, and even when the node was back up and running again, the backup remained still stuck ""InProgress"". This seems to be somehow related to #1437 because in `velero backup describe <backup name>` shows that Restic is in progress backing up for a pod with a name that no longer exists because the pod was recreated on another node with a different name. I tried to delete the backup stuck a few times unsuccessfully, then I redeployed both restic and velero pods and tried to delete again, now the backup seems to be stuck ""deleting"". The volume affected contains around 25 GB of data.

**What did you expect to happen:**

Ideally, Restic could resume the backup automatically when the volume is attached again to a pod in the deployment being backed up. Or, at least, the backup should fail right away if the volume becomes inaccessible or something like that. It shouldn't remain stuck in progress etc.

**The output of the following commands will help us better understand what's going on**:

* `kubectl logs deployment/velero -n velero`

https://ybin.me/p/5e9e64bd0e51f5ae#BK+GzsUkbGKXP8E9D8Id3GoOnXiz1JiZWQWI7u64Xog=

* `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`

https://ybin.me/p/f19b1d16453f08ea#0eiODHo7ErgNfX90z8z3H29B9Lwx5XEjbz5xD3oL2Ig=

* `velero backup logs <backupname>`

Not available - backup not completed.

**Environment:**

- Velero version (use `velero version`): 

```
Client:
	Version: 0.11.0
	Git commit: -
Server:
	Version: v0.11.0

```

- Kubernetes version (use `kubectl version`):

```
Client Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.0"", GitCommit:""641856db18352033a0d96dbc99153fa3b27298e5"", GitTreeState:""clean"", BuildDate:""2019-03-26T00:04:52Z"", GoVersion:""go1.12.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Server Version: version.Info{Major:""1"", Minor:""14"", GitVersion:""v1.14.1-k3s.4"", GitCommit:""52f3b42401c93c36467f1fd6d294a3aba26c7def"", GitTreeState:""clean"", BuildDate:""2019-04-15T22:13+00:00Z"", GoVersion:""go1.12.1"", Compiler:""gc"", Platform:""linux/amd64""}

```

- Kubernetes installer & version:

Rancher K3S 0.4.0

- Cloud provider or hardware configuration:

Hetzner Cloud

- OS (e.g. from `/etc/os-release`):

Ubuntu 18.04",,,nrb,"
--
This is a known issue, but unfortunately it wasn't documented. I've created https://github.com/heptio/velero/pull/1445 for it.
--
",vitobotta,"
--
Thanks @nrb 

I used `velero backup delete` to delete the backup but it got stuck at ""Deleting"". I have now used `kubectl delete backup <name> -n velero` and it seems to have deleted it. Do you think it would be possible to ensure at least that the backup fails quickly when it cannot access the volume or is this complicated at the moment? Thanks!
--
",,,,,,,,
1404,OPEN,Restic case study: Mongo diff snapshots take longer than init snapshot,Restic,2019-05-20 13:55:27 +0000 UTC,agolomoodysaada,In progress,,"**What steps did you take and what happened:**
Hey Velero team, 

I’m using the new `v1.0.0-alpha.2` with Restic with GCS (bucket and VM in the same region) and I’m having trouble wrapping my head around the performance. Everytime I run incremental snapshots for MongoDB, they take longer than the initial snapshot.

Annotations on pod

```
kubectl annotate pod mongodb-mongodb-replicaset-2 \
    --overwrite \
    pre.hook.backup.velero.io/command='[""/bin/sh"", ""-c"", ""mongo --eval \""rs.stepDown()\"" ; mongo --eval \""db.fsyncLock()\"" ""]' \
    pre.hook.backup.velero.io/container=mongodb-replicaset \
    post.hook.backup.velero.io/command='[""/bin/sh"", ""-c"", ""mongo --eval \""db.fsyncUnlock()\"" ""]' \
    post.hook.backup.velero.io/container=mongodb-replicaset \
    backup.velero.io/backup-volumes=configdir,datadir,workdir
```



```sh
velero backup create backup1 -l statefulset.kubernetes.io/pod-name=mongodb-mongodb-replicaset-2
velero backup create backup2 -l statefulset.kubernetes.io/pod-name=mongodb-mongodb-replicaset-2
...
```

```
# backup1
Started:    2019-04-24 19:16:32 -0400 EDT
Completed:  2019-04-24 19:52:10 -0400 EDT
Time taken: ~36mins
`gsutil du -h` output:  163.88G

# backup2 (diff)
Started:    2019-04-25 08:40:48 -0400 EDT
Completed:  2019-04-25 09:24:54 -0400 EDT
Time taken: ~44mins
`gsutil du -h` output: 170.48 GiB
```

There's only ~7G of added data in backup2. It took 45mins to transfer ~7G. I think it should take <5 mins for such a diff to be transferred based on GCS’s 60MB/s average

**What did you expect to happen:**

I expect the performance to be faster on the following snapshot runs since restic only transfers the diff between each snapshot. However, that’s not what I’m observing… it seems to be getting slower or remain the same.

**The output of the following commands will help us better understand what's going on**:

The second snapshot logs show the following which is not very useful as **it doesn't show Restic specific actions**.

```
time=""2019-04-25T12:40:55Z"" level=info msg=""Executing takePVSnapshot""
time=""2019-04-25T12:40:55Z"" level=info msg=""Skipping Persistent Volume snapshot because volume has already been backed up.""
# post-backup hook
time=""2019-04-25T13:24:52Z"" level=info msg=""running exec hook""
time=""2019-04-25T13:24:52Z"" level=info msg=""stdout: ...........""
```

## Metrics for Backup2
Bucket ingress traffic:
<img width=""387"" alt=""Screen Shot 2019-04-25 at 10 41 40 AM"" src=""https://user-images.githubusercontent.com/29459870/56751135-50f9b780-6753-11e9-8ebf-bb7df73aef94.png"">
<img width=""600"" alt=""Screen Shot 2019-04-25 at 11 21 43 AM"" src=""https://user-images.githubusercontent.com/29459870/56751138-50f9b780-6753-11e9-9b62-5392b2b634bc.png"">
Seems like most of the time of the backup was spent compute diffs of modified files rather that uploading data to GCS.

CPU and Memory
<img width=""543"" alt=""Screen Shot 2019-04-25 at 11 18 33 AM"" src=""https://user-images.githubusercontent.com/29459870/56751136-50f9b780-6753-11e9-80d9-3264deb1c4a0.png"">
<img width=""543"" alt=""Screen Shot 2019-04-25 at 11 18 38 AM"" src=""https://user-images.githubusercontent.com/29459870/56751137-50f9b780-6753-11e9-93d7-194812b65690.png"">
<img width=""1432"" alt=""Screen Shot 2019-04-25 at 11 31 47 AM"" src=""https://user-images.githubusercontent.com/29459870/56751139-50f9b780-6753-11e9-814b-1582d6c889e4.png"">

We can see from the container memory metrics that the working set remains under 1G but the cache memory just eats up the OS memory without releasing it.

Exec-ing into a restic pod allowed me to confirm that snapshots are in fact incremental using:

```sh
velero restic repo get -o yaml | grep resticIdentifier
restic -r <repo-identifier> snapshots --json
```

I was also able to run a `restic -r <repo-identifier> diff <snapshot-id-1> <snapshot-id-2>` and got

```
Files:           4 new,     4 removed,    48 changed
Dirs:            0 new,     0 removed
Others:          0 new,     0 removed
Data Blobs:  18151 new, 18703 removed
Tree Blobs:      2 new,     2 removed
  Added:   9.064 GiB
  Removed: 9.360 GiB
```

**Anything else you would like to add:**

Reference to findings can be found from [conversations with the Velero team on slack](https://kubernetes.slack.com/archives/C6VCGP4MT/p1556196254051000) @skriss and @nrb . 

Thank you so much to the team for their help here.

**Environment:**

- Velero version (use `velero version`): `v1.0.0-alpha.2`
- Kubernetes version (use `kubectl version`): `v1.11.7-gke.12`
- Cloud provider or hardware configuration: `beta.kubernetes.io/instance-type=n1-highmem-16`
- OS (e.g. from `/etc/os-release`): `cloud.google.com/gke-os-distribution=ubuntu`

# Suggested actions

- [ ] check if restic can be configured to release memory after completing a backup or limit its memory cache size
- [ ] `velero install` and helm chart should have sane defaults for restic daemonset pods as they eat up cache memory without releasing it. `memory: 1Gi` seems to be a sane default from above metrics for both requests and limits.
- [ ] document cases where restic might not be efficient at backup time such as this case study for MongoDB. Restic is slow at backup time, but still minimizes storage cost and restore performance.
- [ ] make it easy to find restic specific logs. Maybe expose a new `velero restic logs` command? Or make it easy to find out which restic pod, ran which backup, for which volume.",,,skriss,"
--
thanks so much for this detailed report @agolomoodysaada! really helpful.
--

--
@agolomoodysaada can you provide the YAML for the restic daemonset that you're using? thanks.
--

--
Can you provide a rough file count of this volume, and the sizes of the top ~10 files? Trying to understand if most of the data is in one large file.
--

--
OK, so definitely a couple big files that dominate the size of the data dir (as expected).  My sense from looking at the restic code is that restic is not particularly efficient when this is the case -- it's much more efficient when it has lots of files that it can process in parallel.  I'll look into some of the open issues/PRs and see if there's any work in progress on this particular area.
--
",agolomoodysaada,"
--
Hey @skriss , I used `velero install --use-restic ...`
--

--
Restic diff

```
M    /WiredTiger.backup
M    /WiredTiger.turtle
M    /WiredTiger.wt
M    /collection-0--4937860834407901523.wt
M    /collection-14-4691893134753119794.wt
M    /collection-2--4937860834407901523.wt
M    /collection-29--4937860834407901523.wt
M    /collection-31--4937860834407901523.wt
M    /collection-33--4937860834407901523.wt
M    /collection-35--4937860834407901523.wt
M    /collection-4-4691893134753119794.wt
M    /collection-42--4937860834407901523.wt
M    /collection-50--4937860834407901523.wt
-    /diagnostic.data/metrics.2019-04-12T19-55-53Z-00000
M    /diagnostic.data/metrics.2019-04-24T13-47-20Z-00000
+    /diagnostic.data/metrics.2019-04-25T03-30-48Z-00000
M    /diagnostic.data/metrics.interim
M    /index-105--4937860834407901523.wt
M    /index-110--4937860834407901523.wt
M    /index-112--4937860834407901523.wt
M    /index-113--4937860834407901523.wt
M    /index-124--4937860834407901523.wt
M    /index-126--4937860834407901523.wt
M    /index-140--4937860834407901523.wt
M    /index-147--4937860834407901523.wt
M    /index-153--4937860834407901523.wt
M    /index-164--4937860834407901523.wt
M    /index-169--4937860834407901523.wt
M    /index-172--4937860834407901523.wt
M    /index-174--4937860834407901523.wt
M    /index-177--4937860834407901523.wt
M    /index-179--4937860834407901523.wt
M    /index-3--4937860834407901523.wt
M    /index-30--4937860834407901523.wt
M    /index-32--4937860834407901523.wt
M    /index-34--4937860834407901523.wt
M    /index-36--4937860834407901523.wt
M    /index-37--4937860834407901523.wt
M    /index-38--4937860834407901523.wt
M    /index-39--4937860834407901523.wt
M    /index-4--4937860834407901523.wt
M    /index-43--4937860834407901523.wt
M    /index-44--4937860834407901523.wt
M    /index-45--4937860834407901523.wt
M    /index-46--4937860834407901523.wt
M    /index-47--4937860834407901523.wt
M    /index-48--4937860834407901523.wt
M    /index-49--4937860834407901523.wt
M    /index-51--4937860834407901523.wt
-    /journal/WiredTigerLog.0000003833
-    /journal/WiredTigerLog.0000003834
+    /journal/WiredTigerLog.0000003861
+    /journal/WiredTigerLog.0000003862
-    /journal/WiredTigerPreplog.0000000070
+    /journal/WiredTigerPreplog.0000000097
M    /sizeStorer.wt

Files:           4 new,     4 removed,    48 changed
Dirs:            0 new,     0 removed
Others:          0 new,     0 removed
Data Blobs:  18703 new, 18151 removed
Tree Blobs:      2 new,     2 removed
  Added:   9.360 GiB
  Removed: 9.064 GiB
```

File sizes

```
total 161G
-rw-rw---- 1 mongodb mongodb 130G Apr 29 11:26 collection-31--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  17G Apr 29 11:26 collection-14-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb  12G Apr 29 11:26 collection-33--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 465M Apr 29 11:26 index-124--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 440M Apr 29 11:26 index-153--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 434M Apr 29 11:26 index-169--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 422M Apr 29 11:26 index-179--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 421M Apr 29 11:26 index-174--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 246M Apr 29 11:26 index-177--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 192M Apr 29 11:26 index-172--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 179M Apr 29 11:26 index-32--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 154M Apr 29 11:26 index-126--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 149M Apr 29 11:26 index-164--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  64M Apr 29 11:26 index-112--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  59M Apr 29 11:26 index-140--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  57M Apr 29 11:26 index-147--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  47M Apr 29 11:26 collection-29--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  43M Apr 29 11:26 index-34--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  26M Apr 29 11:26 index-105--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  24M Apr 29 11:26 index-110--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  23M Apr 29 11:26 index-113--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  14M Apr 29 10:58 collection-50--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 7.6M Apr 29 10:04 collection-42--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 6.2M Apr 29 11:26 index-30--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 2.3M Apr 25 14:23 collection-52--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 1.5M Apr 29 02:34 collection-35--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 1.4M Apr 29 02:34 index-39--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 1.1M Apr 29 11:26 index-3--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 924K Apr 29 02:34 index-37--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 824K Apr 23 10:22 index-41--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 500K Apr 23 10:23 collection-40--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 500K Apr 29 02:34 index-36--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 416K Apr 29 11:26 WiredTiger.wt
-rw-rw---- 1 mongodb mongodb 312K Apr 29 11:26 collection-2--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 300K Apr 29 10:58 index-51--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 268K Apr 29 10:02 index-43--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 156K Apr 23 10:23 collection-54--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 124K Apr 25 14:23 index-53--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 116K Apr 29 02:34 index-38--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  84K Apr 27 11:53 collection-56--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  84K Apr 29 10:02 index-47--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  84K Apr 29 10:04 index-49--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  68K Apr 29 10:02 index-44--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  68K Apr 29 10:02 index-45--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  68K Apr 29 10:02 index-46--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  68K Apr 29 10:02 index-48--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  64K Apr 29 11:26 index-4--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  52K Apr 27 11:53 index-60--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  44K Apr 23 10:23 _mdb_catalog.wt
-rw-rw---- 1 mongodb mongodb  44K Apr 29 11:26 sizeStorer.wt
-rw-rw---- 1 mongodb mongodb  36K Apr 29 11:26 collection-0--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  36K Apr 23 10:24 collection-0-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb  36K Apr 23 10:23 collection-180--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  36K Apr 29 11:26 collection-4-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb  36K Apr 24 23:16 collection-5--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  36K Apr 24 19:54 collection-69--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  36K Apr 24 19:56 collection-76--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  36K Apr 23 10:24 index-1-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb  36K Apr 27 11:53 index-63--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  36K Apr 24 19:54 index-70--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  32K Apr 23 10:23 collection-15-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb  32K Apr 23 10:23 index-16-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb  32K Apr 23 10:22 index-181--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  32K Apr 23 10:23 index-182--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  24K Apr 23 10:22 index-55--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  20K Apr 23 10:24 index-66--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:23 collection-17-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:23 collection-2-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:23 collection-6-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:23 collection-71--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:23 collection-73--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:23 collection-78--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:23 collection-8-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:23 collection-80--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:23 index-1--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr  8 20:24 index-18-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb  16K Apr  8 20:24 index-3-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb  16K Apr  8 20:24 index-5-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:24 index-57--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 26 00:01 index-59--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:22 index-6--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:22 index-68--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:23 index-7-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:22 index-72--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:22 index-74--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:22 index-75--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 11:13 index-77--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:22 index-79--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr 23 10:22 index-81--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb  16K Apr  8 20:24 index-9-4691893134753119794.wt
-rw------- 1 mongodb mongodb 4.0K Apr 23 10:23 WiredTigerLAS.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-101--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-103--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-183--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-19-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-192--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-197--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-200--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-203--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-82--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-84--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-86--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-88--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-90--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-92--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-94--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-96--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:23 collection-98--4937860834407901523.wt
drwxrwS--- 2 mongodb mongodb 4.0K Apr 29 11:26 diagnostic.data
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:52 index-100--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:52 index-102--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:44 index-104--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-184--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:39 index-185--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:39 index-186--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:39 index-187--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:39 index-188--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:39 index-189--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:39 index-190--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:39 index-191--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-193--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:39 index-194--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:39 index-195--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:39 index-196--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-198--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:39 index-199--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 24 22:08 index-20-4691893134753119794.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-201--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:39 index-202--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 04:02 index-204--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:39 index-205--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-58--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-61--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 12:01 index-62--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-64--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-65--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 24 22:12 index-67--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-83--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-85--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-87--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-89--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-91--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-93--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 23 10:22 index-95--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 12:39 index-97--4937860834407901523.wt
-rw-rw---- 1 mongodb mongodb 4.0K Apr 21 11:54 index-99--4937860834407901523.wt
-rw------- 1 mongodb mongodb 1.2K Apr 29 11:26 WiredTiger.turtle
-rw-rw---- 1 mongodb mongodb  114 Apr  8 20:24 storage.bson
drwxrwS--- 2 mongodb mongodb  110 Apr 29 11:03 journal
-rw-rw---- 1 mongodb mongodb   45 Apr  8 20:24 WiredTiger
-rw-rw---- 1 mongodb mongodb   21 Apr  8 20:24 WiredTiger.lock
-rw-rw---- 1 mongodb mongodb    2 Apr 23 10:23 mongod.lock
```
--

--
@vitobotta , In my case, I use Grafana to view the metrics scraped from cAdvisor by Prometheus. I'm not showing any Velero metrics in this report. If you wanted to access Velero metrics, you should be able to go to the prometheus UI or Grafana and and type something like `velero` and autocomplete should kick in... 
--
",vitobotta,"
--
> Metrics for Backup2

Hi, I hope you don't mind this little off-topic. How do you visualize the metrics? I have installed Prometheus Operator and then Velero with the metrics enabled. Where/how do I see the graphs etc? Thanks
--

--
Thanks @agolomoodysaada 
--
",,,,,,
1350,OPEN,Race condition when passing AdditionalItems from restore item action,Area/Plugins; Enhancement/User,2020-08-26 14:04:46 +0000 UTC,sseago,In progress,,"**What steps did you take and what happened:**
We have a restore plugin which returns resources via `AdditionalItems` when those items must be restored before the current item. This happens when there is a dependency relationship between two items of the same resource type. In our particular case, for the `imagestreamtags.image.openshift.io` resource, in some cases some imagestream tags reference other imagestreamtags, and a restore for `imageStreamTagA` will fail if it references `imageStreamTagB` which does not exist. The plugin returns `imageStreamTagB` in the `AdditionalItems` slice.

In Restore.go, after running Execute on the plugin, for any `AdditionalItems` returned, `restoreItem` is called on each one.

From looking at the logs, everything seems to be happening in the right order. The plugin action runs for the resource that references the other imagestream tag, it passes the AdditionalItem reference, and then restore is called on that resource, including calling its restore plugins. However, when I look at the target namespace in the cluster after restore, I see the returned AdditionalItem resource, but I am not seeing the first resource restored at all. I know that if we attempt to restore an imagestreamtag with a non-existent reference, it will fail to restore, which is the expected behavior (i.e. if we have an alias tag for alpine:3.x which is supposed to track alpine 3.2, if 3.2 does not exist, the tracking tag will not be restored).

In this case, however, what seems to be happening is that `restoreItem` for the referenced resource returns before the resource is fully created in the cluster, so when the `resourceClient.Create` call is made, the referenced resource does not yet exist, according to the cluster.

I just modified my local velero checkout to add `time.Sleep(10 * time.Second)` immediately following the restoreItem call (linked below), and everything works as expected.
https://github.com/heptio/velero/blob/master/pkg/restore/restore.go#L938

**What did you expect to happen:**
My expectation was that both of the resources would be restored correctly.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

(I don't really think anything in my logs will give any more useful information than is already in the description above)

**Anything else you would like to add:**
I'm not sure exactly how to resolve this. If there was some ""after restore plugin action"", then for cases like this, I could have a plugin that blocked until the resource appeared in the cluster. In this case, on the returned `AdditionalItems` resource, the after restore plugin would wait until it was created to return, that way the initial imagestream tag which references it wouldn't actually have its `resourceClient.Create` call made until the cluster initialization of the resource was done.

**Environment:**

- Velero version (use `velero version`): Server version is a build off the master branch
- Kubernetes version (use `kubectl version`): 1.12+
- Kubernetes installer & version:
- Cloud provider or hardware configuration: openshift 4 on aws
- OS (e.g. from `/etc/os-release`):
",,,nrb,"
--
Instead of a sleep, could a watch happen on the resource? We do something similar when checking if a PV or related PVC are deleted ([code](https://github.com/heptio/velero/blob/master/pkg/restore/restore.go#L611)), and I'm working on similar logic in the install command. Similar behavior also happens in Helm when instances of CRDs are made before the definitions are ready, like in #1195.

The problem is that plugins don't receive a client (though they could generate one, I suppose), and the ready conditions for resources are different.

Would it make sense for the restore item action plugin interface to include some sort of Ready function? I'm not sure if this would be per item or not. @skriss @carlisia PTAL at the issue and share any ideas you might have.
--

--
>  If there were some plugin that got called after the resourceClient.Create call, that plugin could watch the resource.

That was kind of my thinking with a `Ready` function - it might be part of the same plugin, but the `Ready` function would be invoked after the `Create` call, effectively pausing restore til `Ready` returns true or a timeout is reached. If the functions not defined, move on as we do now.

And yeah, we'd need to think about how it might link between items like you said.
--

--
Revisiting this, I think while they're generally the same problem (race conditions waiting for something to be ready), this is waiting at a different point in the code - namely for arbitrary AdditionalItems like Scott pointed out, not just CRDs.

I still think designing some sort of mechanism in to say that there needs to be a readiness check on the returned AdditionalItems makes sense. Very roughly, it could be a boolean added to the field on `RestoreItemActionExecuteOutput`, similar to `SkipRestore`, and if it's `true`, then invoke the plugin's `Wait` method, polling for readiness on the `AdditionalItems`.
--

--
@sseago Yes, there would be a field on `RestoreItemActionExecuteOutput` and a method on the `RestoreItemAction` interface, operating in tandem as you described it.

I'm not sure about using `AppliesTo` on the `Wait()` function; I was envisioning it to be the exact same plugin, so I suppose it would be with same `AppliesTo` criteria that had already been applied...but if there's a chain of plugins, I need to think a little bit more about how things might get selected. Right now, I believe `AdditionalItems` are excluded from any kind of filtering logic, so including a PV from a PVC, which came from a Pod will get restore even if you explicitly excluded cluster-scoped resources.

The timeouts are tricky too -  there is a global timeout now, but we actually have different numbers for waiting for PVs & associated objects to delete (in case someone deletes something and wants to immediately restore it) and for CRDs to be ready. Loads of command line flags certainly per type isn't an option.  We could use [ConfigMaps for plugin settings](https://velero.io/docs/v1.3.1/custom-plugins/#plugin-configuration), and a [core plugin](https://velero.io/docs/v1.3.1/restore-reference/#changing-pv-pvc-storage-classes) that already does it.
--
",carlisia,"
--
Not sure yet about the how (let's discuss) but initially I'm thinking we might want the behavior of one resource waiting (watching?) multiple resources.
--
",sseago,"
--
@nrb Sure -- I never intended sleeping as a solution -- I just used it to confirm that the only reason for the failure was that the newly-created resource returned in the AdditionalItems slice wasn't there yet. Sleeping after calling restoreItem on AdditionalItems made the failure go away. Watching until a resource is ready could work, but it can't be done in the restore item action plugin, since that plugin is called *before* the restore happens in the cluster. If there were some plugin that got called *after* the resourceClient.Create call, that plugin could watch the resource. It's probably something we'd want to do within a plugin action which would only wait if it was required, since we don't want to always wait for one item to finish before going onto the next across all resource types. Also, it would need to be available per item, since in the case I'm hitting, the two dependent items are of the same resource type -- one imagestreamtag is dependent on another imagestreamtag.
--

--
@skriss Circling back to this. I don't think it's an example of #964 since this applies to waiting for the creation of resources other than CRDs. The concern here is that AdditionalItems needed by the restore aren't necessarily ready when this item's restore is processed. For example, say you're restoring an ScheduledAppointment resource that requires the existence of the User first. Creating a ScheduledAppointment resource with a reference to a non-existent user will fail, so the restore item action plugin passes the appropriate User in the AdditionalItems list, but if this User resource gets restored but isn't ready yet, the ScheduledAppointment will still fail. This is why in my original investigation, adding a sleep call made the race condition go away (even though, as I mentioned, that's not the right solution here).
--

--
This makes sense. I think you almost have to do this via the plugin itself, since there's no way for Velero to know how to determine whether some arbitrary resource is ready, especially considering that the resource in question may have custom actions taken on it in a plugin. One consequence of this is you may have a situation where a plugin is returning AdditionalItems of a resource type that doesn't have its own plugin today, since there may be no need for custom actions on it, but since we need to know when that non-customized resource restore is done, we may need to add a plugin for that resource that has an empty/no-op Execute method but has a Wait method which determines when it's done. The Wait approach would also work for the use case I've run into, where in some cases we just need the Velero-restored item to show up in the cluster (so waiting for client.<Resource>... to return the appropriate item), and in other cases we need the action carried out by the resource's plugin to complete successfully (perhaps a docker copy, or some other operation).
--

--
@nrb If I'm understanding you correctly,  you're proposing adding a new method (`Wait()`)to the RestoreItemAction interface, in conjunction with adding a `Wait` field to `RestoreItemActionExecuteOutput`. If `Wait` is false, then current behavior is unchanged. If `Wait` is true, then after restoring the resource, call `Wait()` and don't move on to the next action (either the next `additionalItems` element, if we're there, or the resource item on the list if we're in a normal resource restore) until `Wait()` returns. As for the new plugin func, I'm assuming the same `AppliesTo` would be used to determine which Wait methods to call (perhaps more than one). If there's a generic plugin that runs on all resources (for setting annotations, etc.), then that plugin will probably have an empty Wait method, while a plugin which does processing specific to that resource type will implement the actual readiness check. For a resource that we don't currently have a plugin but which we may have other plugins which pull resources in, we may need to add a new plugin which would basically only return the original item with the `Wait` bool set to true, and then would have a `Wait()` method implemented which makes sure the resource is created.
--

--
The other issue is what about timeouts? Would velero call Wait() with a global timeout setting? Would it be per restore, or would the plugin itself define it, so different resources might have different timeout values?
--

--
@nrb Right. We're using additional items to pull in cluster-scoped resources in some cases. In the case where this race condition has hit us, though, the AdditionalItems are actually resources of the same type but which this resource references. I guess the question is whether we want to call wait on the plugin that returned the AdditionalItems (even if it's a different resource type), or whether we'd want to call Wait on the chain of plugins that normally applies to the additional item's resource type. If it's a PVC calling out a PV, for example, would we want the PVC's plugin to call wait (being passed an ObjectReference for the PV), which would have to deal with the full range of resource types that it is able to pass via AdditionalItems? Alternatively, we might want to call Wait on each AdditionalItem's normal plugins, based on the AppliesTo logic. In this case, the normal PV plugin would implement wait for itself -- or if we don't already have a PV plugin, we'd need to write one which had a no-op Execute method and a filled-in Wait method. 

In our ImageStreamTag use case, the distinction won't matter so much, since the case here is in the process of restoring tag1 we recognize that this tag contains an object reference to tag2, so we have to have tag2 restored before tag1, so we pass tag2 as an additional item. It gets restored, then wait is called on this plugin again with a tag2 reference -- same plugin whether it's via the appliesTo chain or because velero remembered that this plugin was the one that returned the additional item.
--

--
I've created a PR for a design for this here: https://github.com/vmware-tanzu/velero/pull/2867
--
",skriss,"
--
This looks like a specific instance of #964 
--

--
@nrb @sseago I'm pretty sure we can close this out, now that we've closed #964 via #1937 - I'm going to close, but please reopen if you disagree with my assessment.
--
",,,,
1333,OPEN,velero_backup_success_total metric keeps being reported for deleted schedules,Metrics,2020-09-18 14:45:52 +0000 UTC,multi-io,Opened,,"**What steps did you take and what happened:**

Create a schedule, let it perform a few backups, then delete the schedule.

The /metrics endpoint keeps reporting the velero_backup_success_total{schedule=""<the schedule>""} metric, even after all the backups have expired, until velero itself is restarted.

**What did you expect to happen:**

The metric should vanish when the schedule is deleted or after all its backups have expired, or it should stay around indefinitely (which I wouldn't prefer). In any case, the fact that it goes away when the velero pod is restarted suggests that this an artifact of the implementation rather than desired behaviour, and it does make it harder to implement certain alerting rules.",,,maberny,"
--
Same thing happens with the ark_backup_failure_total metric.
--
",h4wkmoon,"
--
Hi, 
I really love velero. It does really great for its principal purpose : backups and restores.

But, exposing nothing at all is better than incorrect metrics.
Can you, either : 
* advertise that your metrics are not entireiy ok,
* correct or remove those which are not


Thanks.


--
",,,,,,,,
1309,OPEN,"Add ""snapshots size"" prometheus metric",Enhancement/User; Help wanted; Metrics,2019-12-20 14:23:13 +0000 UTC,skriss,In progress,,"xref #1059 #1077 

> Another metric which can be valuable for me (but maybe questionable for you, I can move it to another issue for discussion or 'reject'):
>
>[gauge] size of all PV snapshots (labeled with a schedule, volumeSnapshotLocation) - to track how much space do I use.

Splitting this request out into its own issue.",,,cpanato,"
--
@skriss I would like to try this one, as a newbie here, can you help me and guide where should i change/add, or some examples in the code.
thanks!
--

--
@skriss I agree because I don't have much experience with Velero, however if we get the scope done and some walkthrough I can try that 😄 
--
",skriss,"
--
@cpanato it'd be great if you could help out with this! Let me look into it and see if I can offer some guidance.
--

--
I took a quick look here, and there are definitely some questions to figure out before we can implement this:

- what do we mean by snapshot size? there are a couple of possibilities here:
    -  use the size of the disk that was snapshotted -- so if it's a 100GB disk, we report that as the snapshot size, even if there was only 1GB of data on it. 
    - ask the storage provider for the snapshot size -- however, it looks like there is some inconsistency between providers here -- for example, AWS will only report the disk size, whereas GCE has both disk size and snapshot size.
- how do we get the stat we're looking for?
    - we could get PV size from the Kubernetes API, if that's what we're interested in
    - if we want to get it from the storage provider, we likely need to add a new VolumeSnapshotter plugin function
    - CSI snapshots have a size field that we could use

I'm not sure off the top of my head what the best answers are to all of these questions, we need to do some thinking and solicit feedback. @cpanato I'm thinking maybe this isn't the best issue to pick up for a new contributor, unless it's something you're particularly interested in :)
--
",,,,,,,,
1271,OPEN,Merging of service accounts cannot handle generated names,Bug,2019-08-29 20:17:05 +0000 UTC,dymurray,In progress,,"**Describe the solution you'd like**
[A clear and concise description of what you want to happen.]
Since OpenShift creates new namespaces with default service accounts (`default`, `builder`, `deployer`), we would like the ability for the user to specify a white/blacklist of the service accounts they wish to not merge on restore.

Since a user might have customized `imagePullSecrets` which are attached to the service accounts, we would prefer that this list exists on the custom resource that is created on restore. Something like:
```
---
apiVersion: ark.heptio.com/v1
kind: Restore
metadata:
  name: nginx-backup
  namespace: heptio-ark
spec:
  backupName: nginx-backup
  excludedResources:
  - node
  - events
  - events.events.k8s.io
  - backups.ark.heptio.com
  - restores.ark.heptio.com
  includedNamespaces:
  - '*'
  blacklistServiceAccountMerge:
  - default
  - deployer
  - builder
```


**Anything else you would like to add:**
[Miscellaneous information that will assist in solving the issue.]
I saw some previous discussion around this here: https://github.com/heptio/velero/issues/110, which I am curious about the last comment around using ""conflict resolution"" on the default service accounts. When I restore a basic application in OpenShift, I expect the service accounts to have 1 `dockercfg` secret associated with it, and one `token` secret:
```
[dymurray@pups ocp-velero-ansible]$ oc get sa builder -o yaml
apiVersion: v1
imagePullSecrets:
- name: builder-dockercfg-pqlf9
kind: ServiceAccount
metadata:
  creationTimestamp: 2019-03-11T18:16:41Z
  name: builder
  namespace: test2
  resourceVersion: ""16223""
  selfLink: /api/v1/namespaces/test2/serviceaccounts/builder
  uid: d1c0f304-4429-11e9-a2f3-02f7573ba654
secrets:
- name: builder-token-jds7f
- name: builder-dockercfg-pqlf9
```

On restore, I instead get:
```
[dymurray@pups ocp-velero-ansible]$ oc get sa builder -o yaml                                                                                                                                                      
apiVersion: v1                                                                                                                                                                                                     
imagePullSecrets:                                                                                                                                                                                                  
- name: builder-dockercfg-f52hs                                                                                                                                                                                    
- name: builder-dockercfg-c67wm                                                                                                                                                                                    
- name: builder-dockercfg-8dnw8                                                                                                                                                                                    
kind: ServiceAccount                                                                                                                                                                                               
metadata:                                                                                                                                                                                                          
  annotations:                                                                                                                                                                                                     
    openshift.io/imagestream-restore-plugin: ""1""                                                                                                                                                                   
  creationTimestamp: 2019-03-11T18:13:54Z                                                                                                                                                                         
  name: builder                                                                                                                                                                                                   
  namespace: test                                                                                                                                                                                                 
  resourceVersion: ""14490""                                                                                                                                                                                 
  selfLink: /api/v1/namespaces/test/serviceaccounts/builder                                                                                                                                                       
  uid: 6e352c03-4429-11e9-a2f3-02f7573ba654                                                                                                                                                   
secrets:                                                                                                                                                                            
- name: builder-token-4vn7d                                                                                                                                                                                        
- name: builder-dockercfg-f52hs                                                                                                                                          
- name: builder-dockercfg-c67wm                                                                                                                                            
- name: builder-dockercfg-8dnw8  
```

**Environment:**

- Velero version (use `velero version`): v0.11
- Kubernetes version (use `kubectl version`): v1.12

",,,nrb,"
--
I'm not sure I understand this statement when followed by the example of blacklisting:

>Since a user might have customized imagePullSecrets which are attached to the service accounts, we would prefer that this list exists on the custom resource that is created on restore.

That was the original goal with service account merging - kubernetes would create the service account before Velero could restore the resource, but we still wanted the image pull secrets from the backup. If these are blacklisted then they would not receive the old secret. Do the `default`, `deployer`, and `builder` SAs not have any image pull secrets associated in OpenShift? Or are you proposing that, since these accounts may have image pull secrets, users should be able to allow the restore of those accounts.

Also, I think the observed behavior is expected - we were conservative and only allowed merging on the `default` account, if I remember correctly.
--
",dymurray,"
--
>Or are you proposing that, since these accounts may have image pull secrets, users should be able to allow the restore of those accounts.

Yes I am proposing that for the case if a user has customized their imagePullSecret in the previous environment and want that secret carried over to the new environment. 

>Also, I think the observed behavior is expected - we were conservative and only allowed merging on the default account, if I remember correctly.

Okay, so I might have been confused when filing this issue then. Basically what I want is the ability to prevent the merging of stale service account data into the new environment. You can see that the `default` service account has two `imagePullSecrets`, one of them being a secret that does not exist on restore. I believe this is coming from the backup definition of the service account, and the `mergeServiceAccounts` function is simply appending it to the list. My idea of a ""blacklist"" was to prevent the restore process from mounting the stale data.

```
[dymurray@pups ocp-velero-ansible]$ oc get sa default -o yaml
apiVersion: v1
imagePullSecrets:
- name: default-dockercfg-szxfs
- name: default-dockercfg-5hrfc
kind: ServiceAccount
metadata:
  creationTimestamp: 2019-03-13T14:47:33Z
  name: default
  namespace: test
  resourceVersion: ""38167""
  selfLink: /api/v1/namespaces/test/serviceaccounts/default
  uid: ef93b0bc-459e-11e9-a55a-02ddec4ad0d2
secrets:
- name: default-token-9wfqc
- name: default-dockercfg-szxfs
- name: default-dockercfg-5hrfc
[dymurray@pups ocp-velero-ansible]$ oc get secret
NAME                       TYPE                                  DATA      AGE
builder-dockercfg-l8lkn    kubernetes.io/dockercfg               1         24h
builder-token-25sxt        kubernetes.io/service-account-token   3         24h
builder-token-sgtwx        kubernetes.io/service-account-token   3         24h
default-dockercfg-szxfs    kubernetes.io/dockercfg               1         24h
default-token-9wfqc        kubernetes.io/service-account-token   3         24h
default-token-d8p88        kubernetes.io/service-account-token   3         24h
deployer-dockercfg-tcbmt   kubernetes.io/dockercfg               1         24h
deployer-token-fmgkh       kubernetes.io/service-account-token   3         24h
deployer-token-pwk2w       kubernetes.io/service-account-token   3         24h
```

So my idea of a ""blacklist"" was essentially to prevent `default`, `builder`, and `deployer` from including the `imagePullSecrets` that no longer exist. I also think this is happening for more than the default service account, as I see the same behavior for `default`, `builder`, and `deployer`.

If I am missing some information please correct me.
--

--
Looking at the code, the reason the merge is not working for us is that the code only compares the same of the imagePullSecrets and skips merging if they are the exact same name. Since OCP uses generated names for these resources they will always have an id at the end of the name that will change. I'm trying to determine the best way to handle this in an agnostic way that will prevent merging the stale secrets.
--
",,,,,,,,
1263,OPEN,SSE-KMS for restic backups,Enhancement/User; Restic; Security,2019-08-29 19:51:44 +0000 UTC,ac-hibbert,Opened,,"**What steps did you take and what happened:**

I have enabled SSE-KMS using kmsKeyId field in BackupStorageLocation. The Backups folder (K8S resources) is encrypted but it seems that the data in the restic folder is not


**What did you expect to happen:**

SSE-KMS for restic
",,,skriss,"
--
Thanks for the issue report @hibbert.  It looks like `restic` does not support SSE-KMS natively.  It might be possible to get support for it if we switch to using [rclone](https://restic.readthedocs.io/en/stable/030_preparing_a_new_repo.html#other-services-via-rclone) as the backend for `restic`. 
--
",ac,"
--
Thanks for the reply. Looking for some increased security around restic backups this and/or https://github.com/heptio/velero/issues/1053 would be good
--
",,,,,,,,
1258,OPEN,Limit Velero's access to snapshots/volumes within an account based on tags,Area/Cloud/AWS; Enhancement/User; IAM,2019-09-13 15:59:17 +0000 UTC,muvaf,Opened,,"**Describe the solution you'd like**
The documentation provides an AWS policy where bucket permissions are limited to 1 bucket. However, snapshot and volume actions allow ark to operate on any volume and snapshot in the given AWS account. We should be able to limit the resources that Ark would like to operate on using tags. For example, it should only be able to create/describe/delete snapshots and volumes if an X label exists in the resource tags.

I am creating many kubernetes clusters in the same account from a central cluster. If it was possible to run ark server on that orchestrator cluster, I could give all the access. However, I don't want an AWS IAM user that can operate on others' resources to exist in customer cluster.

**Environment:**

- Velero version (use `velero version`): 0.1.11
- Kubernetes version (use `kubectl version`): 1.11
- Cloud provider or hardware configuration: AWS
",,,skriss,"
--
@bymafmaf this is a great idea. We do already attach a couple of tags to snapshots, and they get copied to the volume on a restore - would that be sufficient? (I'm not super-familiar with tag-based IAM policies).  The standard tags are `velero.io/backup` which has the backup name, and `velero.io/pv` which has the PV name

--
",,,,,,,,,,
1255,OPEN,allow service clusterIP to be preserved on restore,Enhancement/User; Help wanted; P2 - Long-term important,2020-10-02 22:47:04 +0000 UTC,nareshtank,In progress,,"**What steps did you take and what happened:**
I created Service with fixed ClusterIP

```
apiVersion: v1
kind: Service
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {""apiVersion"":""v1"",""kind"":""Service"",""metadata"":{""annotations"":{},""name"":""portal"",""namespace"":""portal""},""spec"":{""clusterIP"":""10.96.0.51"",""ports"":[{""name"":""portal-https"",""port"":443},{""name"":""portal-ssh"",""port"":2020}],""selector"":{""run"":""portal""},""type"":""ClusterIP""}}
  creationTimestamp: 2019-03-05T06:33:29Z
  labels:
    ark-restore: fbbac0f17e4049bcbea3c0521c31175f-1551767531000
    ark.heptio.com/backup-name: fbbac0f17e4049bcbea3c0521c31175f-1551766587000
    ark.heptio.com/restore-name: fbbac0f17e4049bcbea3c0521c31175f-1551767531000
  name: portal
  namespace: portal
  selfLink: /api/v1/namespaces/portal/services/portal
  uid: 970aab73-3f10-11e9-b30b-aaeeba615cd7
spec:
  clusterIP: 10.96.0.51
  ports:
  - name: portal-https
    port: 443
    protocol: TCP
    targetPort: 443
  selector:
    run: portal
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

After ark backup JSON looks correct:

```
{
  ""apiVersion"": ""v1"",
  ""kind"": ""Service"",
  ""metadata"": {
    ""annotations"": {
      ""kubectl.kubernetes.io/last-applied-configuration"": ""{\""apiVersion\"":\""v1\"",\""kind\"":\""Service\"",\""metadata\"":{\""annotations\"":{},\""name\"":\""portal\"",\""namespace\"":\""portal\""},\""spec\"":{\""clusterIP\"":\""10.96.0.51\"",\""ports\"":[{\""name\"":\""portal-https\"",\""port\"":443},{\""name\"":\""portal-ssh\"",\""port\"":2020}],\""selector\"":{\""run\"":\""portal\""},\""type\"":\""ClusterIP\""}}\n""
    },
    ""creationTimestamp"": ""2019-03-05T06:15:11Z"",
    ""name"": ""portal"",
    ""namespace"": ""portal"",
    ""resourceVersion"": ""483713"",
    ""selfLink"": ""/api/v1/namespaces/portal/services/portal"",
    ""uid"": ""08942fb5-3f0e-11e9-b30b-aaeeba615cd7""
  },
  ""spec"": {
    ""clusterIP"": ""10.96.0.51"",
    ""ports"": [
      {
        ""name"": ""portal-https"",
        ""port"": 443,
        ""protocol"": ""TCP"",
        ""targetPort"": 443
      },
      {
        ""name"": ""portal-ssh"",
        ""port"": 2020,
        ""protocol"": ""TCP"",
        ""targetPort"": 2020
      }
    ],
    ""selector"": {
      ""run"": ""portal""
    },
    ""sessionAffinity"": ""None"",
    ""type"": ""ClusterIP""
  },
  ""status"": {
    ""loadBalancer"": {}
  }
}
```

I then deleted the service and restore it with ark: the ClusterIP was random
```
apiVersion: v1
kind: Service
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {""apiVersion"":""v1"",""kind"":""Service"",""metadata"":{""annotations"":{},""name"":""portal"",""namespace"":""portal""},""spec"":{""clusterIP"":""10.96.0.51"",""ports"":[{""name"":""portal-https"",""port"":443},{""name"":""portal-ssh"",""port"":2020}],""selector"":{""run"":""portal""},""type"":""ClusterIP""}}
  creationTimestamp: 2019-03-05T06:33:29Z
  labels:
    ark-restore: fbbac0f17e4049bcbea3c0521c31175f-1551767531000
    ark.heptio.com/backup-name: fbbac0f17e4049bcbea3c0521c31175f-1551766587000
    ark.heptio.com/restore-name: fbbac0f17e4049bcbea3c0521c31175f-1551767531000
  name: portal
  namespace: portal
  resourceVersion: ""485930""
  selfLink: /api/v1/namespaces/portal/services/portal
  uid: 970aab73-3f10-11e9-b30b-aaeeba615cd7
spec:
  clusterIP: 10.110.171.41
  ports:
  - name: portal-https
    port: 443
    protocol: TCP
    targetPort: 443
  - name: portal-ssh
    port: 2020
    protocol: TCP
    targetPort: 2020
  selector:
    run: portal
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```


**What did you expect to happen:**

clusterIP should remain same when restore.
I restored the service from backup JSON manually, it created the service with same clusterIP.

**The output of the following commands will help us better understand what's going on**:
(Pasting long output into a [GitHub gist](https://gist.github.com) or other pastebin is fine.)

* `kubectl logs deployment/ark -n heptio-ark`

```
time=""2019-03-05T06:33:29Z"" level=info msg=""Restoring resource 'services' into namespace 'portal' from: /tmp/301133352/resources/services/namespaces/portal"" backup=fbbac0f17e4049bcbea3c0521c31175f-1551766587000 logSource=""pkg/restore/restore.go:592"" restore=heptio-ark/fbbac0f17e4049bcbea3c0521c31175f-1551767531000
time=""2019-03-05T06:33:29Z"" level=info msg=""Getting client for /v1, Kind=Service"" backup=fbbac0f17e4049bcbea3c0521c31175f-1551766587000 logSource=""pkg/restore/restore.go:652"" restore=heptio-ark/fbbac0f17e4049bcbea3c0521c31175f-1551767531000
time=""2019-03-05T06:33:29Z"" level=info msg=""Executing item action for services"" backup=fbbac0f17e4049bcbea3c0521c31175f-1551766587000 logSource=""pkg/restore/restore.go:764"" restore=heptio-ark/fbbac0f17e4049bcbea3c0521c31175f-1551767531000
time=""2019-03-05T06:33:29Z"" level=info msg=""Restoring Service: portal"" backup=fbbac0f17e4049bcbea3c0521c31175f-1551766587000 logSource=""pkg/restore/restore.go:796"" restore=heptio-ark/fbbac0f17e4049bcbea3c0521c31175f-1551767531000
```


* `velero backup describe <backupname>` or `kubectl get backup/<backupname> -n velero -o yaml`

```
Phase:  Completed

Validation errors:  <none>

```

No warnings,
No errors

* `velero backup logs <backupname>`

(Whole backup have 3 sets of docker secret, pv, pvc, deployments, service)
https://gist.github.com/nareshtank/97023f9934e186c6249a95f7008a933f


* `velero restore describe <restorename>` or `kubectl get restore/<restorename> -n velero -o yaml
```
Phase:  Completed

Validation errors:  <none>

Warnings:  <none>
Errors:    <none>`
```

* `velero restore logs <restorename>`

https://gist.github.com/nareshtank/a57529c811d464768c82972656a45bc6


*`Notes`*
I have three services in backup 2 with fixed clusterIP and 1 with Random clusterIp but fixed NodePort.
On restore NodePorts are not changing, get info from [issue 989](https://github.com/heptio/velero/issues/989).

But ClusterIp in all 3 services are changing, but in Issue:989 I saw the logs reported It has the same clusterIP even after restore.

**Environment:**

- Ark version (use `ark version`):
 
Version: v0.10.1
Git commit: 7e4fca428d73899cbbdb00da8d340d44984975d8

- Kubernetes version (use `kubectl version`):

Client Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.3"", GitCommit:""435f92c719f279a3a67808c80521ea17d5715c66"", GitTreeState:""clean"", BuildDate:""2018-11-26T12:57:14Z"", GoVersion:""go1.10.4"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.4"", GitCommit:""f49fa022dbe63faafd0da106ef7e05a29721d3f1"", GitTreeState:""clean"", BuildDate:""2018-12-14T06:59:37Z"", GoVersion:""go1.10.4"", Compiler:""gc"", Platform:""linux/amd64""}


- Kubernetes installer & version:

kubeadm version: &version.Info{Major:""1"", Minor:""12"", GitVersion:""v1.12.3"", GitCommit:""435f92c719f279a3a67808c80521ea17d5715c66"", GitTreeState:""clean"", BuildDate:""2018-11-26T12:54:02Z"", GoVersion:""go1.10.4"", Compiler:""gc"", Platform:""linux/amd64""}

- Cloud provider or hardware configuration:
On premises - Fedora 26",,,nareshtank,"
--
[Line 54](https://github.com/heptio/velero/blob/c006d9246f3ecc5c332a03a33c2b9e98cce96933/pkg/restore/service_action.go#L54) of Execute method in restore/service_action.go file clears the clusterIp.

can we skip this by adding some kind on flag in service labels?
--

--
We need fixed clusterIP to exposed the service on Edge via revers proxy, In nginx reverse proxy we gave appliance team a fixed IP to create that reverse proxy so for on mass deployments no need to change the IP on each edge box. 
In other cases some daemons which are running on edge requires to communicate with the POD services, In that case the host daemon knows the IP of the service which is fixed during service creation.
--

--
Yes we always do ""`kubectl apply`"". and the solution similar to nodeports will work for us. 
But I prefer some annotation based logic, like you do for the Volumes backup in POD. For other users, they may not want the fixed clusterIP.
--
",skriss,"
--
@nareshtank we might be able to add logic similar to what's done for `NodePorts` that looks to see if a `ClusterIP` was specified during a `kubectl apply`, and if so, retain it during restore. Can you tell us a little bit more about your use case for a fixed `ClusterIP`?
--

--
Do you always create your initial Service via `kubectl apply`? If so, then adding some code that looked there to see if a `ClusterIP` was explicitly specified (similar to how we do for nodeports) would work for you. If not, we'd need to do something else.
--
",mowings,"
--
Came here to say we have a similar use-case for preserving clusterIP.  We also use `kubectl apply` for these services 
--
",WaterKnight1998,"
--
Any updates on this issue? Has it been merged?
--
",,,,
1229,OPEN,Support non-object store backends (e.g. NFS),Enhancement/User; Needs Product,2021-02-10 12:55:47 +0000 UTC,ipochi,In progress,,"Hi,

I would like to know if the requirement for having an S3 compatible storage bucket for backup metadata is an inflexible hard constraint.

If it's an inflexible hard constraint, I'd very much like to understand the reasons behind and see If I can see from that same angle  and twist my use case to fit the same.

If it's not a hard constraint I'd like to know How to accomplish the removal of dependency on object storage for backup metadata. I'd like to understand the code and possibly be able to push in a PR for the same.

In my ideal use case I'd like to be able to take a backup using Restic without having any dependency on the object storage or any cloud storage provider for the VolumeSnapshot and Backup metadata.

I understand Ark + Restic right now supports only File level backup on s3 compatible object storage
There is an issue #1178  that talks about providing full array of support for other Restic supported backends. Any plans on implementing of the said issue ?

 ",,,skriss,"
--
@ipochi what kind of storage backend are you interested in using?
--

--
Velero expects to store both metadata backups and restic backups in ""object storage"" (more on this below).

As you mentioned, there's an open issue to allow additional [restic backends](https://restic.readthedocs.io/en/stable/030_preparing_a_new_repo.html). We're not actively working on this.

Velero has a [plugin architecture](https://heptio.github.io/velero/v0.11.0/plugins) to allow additional ""Object Storage"" backends to be implemented. @nrb's second paragraph was referring to the fact that you could probably create an implementation of this interface that didn't actually use an object-storage system behind the scenes. The interface you'd need to implement is:
```go
// ObjectStore exposes basic object-storage operations required
// by Velero.
type ObjectStore interface {
	// Init prepares the ObjectStore for usage using the provided map of
	// configuration key-value pairs. It returns an error if the ObjectStore
	// cannot be initialized from the provided config.
	Init(config map[string]string) error

	// PutObject creates a new object using the data in body within the specified
	// object storage bucket with the given key.
	PutObject(bucket, key string, body io.Reader) error

	// GetObject retrieves the object with the given key from the specified
	// bucket in object storage.
	GetObject(bucket, key string) (io.ReadCloser, error)

	// ListCommonPrefixes gets a list of all object key prefixes that start with
	// the specified prefix and stop at the next instance of the provided delimiter.
	//
	// For example, if the bucket contains the following keys:
	//		a-prefix/foo-1/bar
	// 		a-prefix/foo-1/baz
	//		a-prefix/foo-2/baz
	// 		some-other-prefix/foo-3/bar
	// and the provided prefix arg is ""a-prefix/"", and the delimiter is ""/"",
	// this will return the slice {""a-prefix/foo-1/"", ""a-prefix/foo-2/""}.
	ListCommonPrefixes(bucket, prefix, delimiter string) ([]string, error)

	// ListObjects gets a list of all keys in the specified bucket
	// that have the given prefix.
	ListObjects(bucket, prefix string) ([]string, error)

	// DeleteObject removes the object with the specified key from the given
	// bucket.
	DeleteObject(bucket, key string) error

	// CreateSignedURL creates a pre-signed URL for the given bucket and key that expires after ttl.
	CreateSignedURL(bucket, key string, ttl time.Duration) (string, error)
}
```

Most of these functions are relatively easy to create an implementation of using a file system - @nrb provided a link to an example plugin that does just that. The `CreateSignedURL` function would be the trickiest one, I think.  This is used for viewing backup/restore logs from a client, and downloading backup tarballs to a client. You might be able to get by without implementing this, depending on your requirements, or you could stand up some kind of web server front-end.


--

--
@JimBugwadia i don't think it would work out of the box since I believe support for the `file` scheme is disabled by default in the go `http` package. It would also need to be reachable by any client that ran `velero logs`. Possibly an option to explore, though
--

--
One idea for providing backup/restore logs to the user: as needed, we could run a pod that fetches the log file from the NFS PV, streams it to stdout, and completes. The user would then effectively be running a `kubectl logs` to get the backup/restore log.  This avoids the need for a separate API/server to serve the logs to the user.
--

--
@SDBrett Velero actually already has a plugin architecture for both storage and custom backup/restore logic - see https://velero.io/docs/v1.2.0/overview-plugins/  and https://velero.io/docs/v1.2.0/custom-plugins/ for some basic info :)  

For storage, we currently have `ObjectStore` and `VolumeSnapshotter` plugin types.  As you can tell from the name, though, the `ObjectStore` plugin is coupled to the idea of using object storage as a backend, so it's not easily extensible to a generic file system, database, or anything else.

To support something like NFS as a backend, we'd need to either redefine the `ObjectStore` plugin type to be more generic, e.g. `BackupStore`, and remove the object store-specific assumptions baked into the interface definition, *or* add an alternate plugin type, e.g. `FileSystemStore` or something along those lines, that could be used *instead of* an `ObjectStore` as backend storage for backups.

Definitely interested in continuing to think through what this could look like with your input!
--
",ipochi,"
--
@skriss 

NFS for example for both VolumeSnapshot and Backup metadata.

Ideally I don't want to setup cloud provider object storage or minio if on-premise for any backup related thing.

Is this possible in Velero ? 
--

--
@nrb Thanks for replying.

I didn't quite understand the 2nd paragraph , could you please elaborate.

Is my understand correct ? 
  1. VolumeSnapshot location can be different other location apart from object storage [ example other plugins such as openebs-ark-plugin]
  2. but backup of metadata needs an object storage ? 

Your possible solution in second paragraph talks about point 1 or 2 ?


--

--
@nrb @skriss  any info on this ?
--
",nrb,"
--
Currently, Velero is designed to send the backed up Kubernetes objects to object storage, and it is fairly hardcoded right now. That said, I don't think we'd be opposed to expanding it, but we'd likely have to revisit our plugin interfaces to accommodate that.

Another solution I can think of is to implement an ObjectStore plugin that saves to NFS instead of an HTTP endpoint. We currently have and example filesystem plugin that does something similar, though it's mostly an example, and not production grade. It can be found at https://github.com/heptio/velero-plugin-example/blob/master/velero-examples/file.go
--

--
@SDBrett I think that's a fairly point as Kubernetes becomes more prevalent in on-prem environments. Velero was originally designed for cloud environments, so it went with a more ""cloudy"" set up with the object storage.

I think there's room for the introduction of a `filePath` storage solution, but we need time to think through a design. My initial thoughts are that it would be a new type of storage plugin that might require mounting a volume into the Velero container, and it uploads the K8s resources there. But it will require design and probably some prototyping.
--

--
>add an alternate plugin type, e.g. FileSystemStore or something along those lines, that could be used instead of an ObjectStore as backend storage for backups.

This is what I was thinking of with my `filePath` suggestion - a new plugin type that was distinct from our ObjectStore plugins, not necessarily having a hierarchy. But I think a discussion around the design is worth having, because I'm sure there's trade offs to each approach that I've not thought through.
--
",JimBugwadia,"
--
@nrb @skriss @ipochi Thanks! Having NFS support is very interesting! 

Can the CreateSignedURL function return a 'file://' URL that points to a file system path?
--
",SDBrett,"
--
+1

The use of NFS storage is widespread and adding support would be very helpful to many companies. Companies who are just starting to adopt K8S on-prem may not have an object store available, or budget to implement one.
--

--
> Here's an example of backing up to NFS that might be helpful. Might not be production quality:
> 
> http://www.rafaelbrito.com/2019/11/project-velero-12-on-openshift-311.html

Thanks for the link. The lack of production quality is one of the issues with a solution such as this.

The customers that I work with (mainly service providers) will also raise an objection to yet another item to manage. 
--

--
Before starting, I understand that the suggestion below is a major change and probably feature breaking.

I've been thinking about this over the weekend and I keep coming back to how Terraform decoupled modules from the core project (I think in v.12). Perhaps a similar approach would enable new options for plugin development and improve flexibility.

The idea is to decouple core backup logic from storage plugins entirely.

**Velero Core** Core backup logic, interaction with the API server etc

**Storage Plugin** Plugin for communication with a storage provider. At a high level this is like a storage driver.

Plugins will provided by the 'vendor' as a binary which are mounted into the Velero pod to be used by Velero as needed.
Plugins would need to adhere to a set of standards for mandatory inputs, but additional inputs could be specified with backup objects.
--
",carlisia,"
--
Here's an example of backing up to NFS that might be helpful. Might not be production quality:

http://www.rafaelbrito.com/2019/11/project-velero-12-on-openshift-311.html
--
"
1217,OPEN,Manage resources from other namespaces,Enhancement/User; Needs Product; P3 - Wouldn't it be nice if...,2020-12-08 01:52:28 +0000 UTC,renat1sakenov,Opened,,"Hello everyone,

I have a question/feature request.
Would it be possible to enable Velero to listen across all namespaces for new backup/restore/schedule resources? Teams working on different projects could manage their backup routines themselves, without having to ask the cluster admin / having access to the heptio-ark namespace.

Or is there already a solution for this case? Currently I create templates that start an ark-pod, from another namespace, with a given task. However, this still gives everyone the possibility to change/add to the template and mess with backups of others.

Thank you.",,,rosskukulinski,"
--
Hi @renat1sakenov.  We've explicitly locked down Velero to only listen in one namespace because it functions as `root` in the cluster.  Any user that can create backup/restore objects could have an escalation attack against the cluster.

That said, the workflow you're describing (teams should have the ability to do backup/restores _inside their namespace_) is a good one.  This likely won't happen for our 1.0 release, but I could see Velero borrowing from Contour's [Delegation/DAG model](https://github.com/heptio/contour/blob/master/docs/ingressroute.md#ingressroute-delegation) to enable safe multi-team backup/restore behaviors.
--
",ncdc,"
--
Dupe of #18?
--
",skriss,"
--
see #2415 for another instance of this request.
--
",marccampbell,"
--
Our use case is a slight variation on the above. We have an application that manages applications in multiple namespaces. The application manager exists in namespace A, and it might be managing applications in namespaces B and C. If we require that the application manager have write access to the `velero` namespace, that's work for the user to configure that RBAC policy. If instead, the application manager was able to write Backup resources to its own namespace (""A"" in this case), then the application manager only needs RBAC permissions for itself and the namespaces that it manages.

Additionally, if we currently deploy a Backup resource to a different namespace, it's hard to debug. Nothing happens and there's nothing happening in the cluster, leaving users a little confused about what happened.
--
",,,,
1178,OPEN,RFE: Allow other restic repository backends,Enhancement/User; Needs Product; P3 - Wouldn't it be nice if...; Restic,2019-08-28 15:18:14 +0000 UTC,ncdc,Opened,,"**Describe the solution you'd like**
Our code expects that restic snapshots are stored in the BackupStorageLocation used by the backup. Our server code hard-codes logic to determine restic URLs for AWS, Azure, and GCP ObjectStores.

We should consider expanding the supported restic backends to everything restic supports. For example, one user on Slack requested the ability to use NFS (I'm guessing this would just be restic's local directory support). If we do this, we'd probably need to amend BackupStorageLocation to allow the user to specify the restic repo location 100% by hand (and prevent the controller from overwriting it).

We should also probably add a method to ObjectStore to return a restic-compatible URL for the repository location.

Finally, we'd need to explore how to configure credentials for all of the different restic backends.

**Anything else you would like to add:**
Somewhat related to #939


**Environment:**

- Ark version (use `ark version`):
- Kubernetes version (use `kubectl version`):
- Kubernetes installer & version:
- Cloud provider or hardware configuration:
- OS (e.g. from `/etc/os-release`):
",,,skriss,"
--
xref #1253 for rclone backend support
--
",,,,,,,,,,
3137,OPEN,Allow users to specify a Storage Class when backing up to S3,Area/Cloud/AWS; Enhancement/User; Good first issue; Help wanted,2021-01-12 18:40:56 +0000 UTC,cullenmcdermott,In progress,,"**Describe the solution you'd like**
I would like the ability to specify a [storage class](https://aws.amazon.com/s3/storage-classes/) in order to avoid having to setup additional lifecycle rules for objects in my bucket(s). 


**Anything else you would like to add:**
Any metadata associated with the backup that is kept in the S3 bucket would probably need to be kept in the 'standard' class. Otherwise S3 costs could balloon if Ark were to frequently try to read from it when it was in the IA tier for example. Also extra consideration would need to be taken for restores from Glacier since it can take up to a few hours to be available and is only available for a certain amount of time. 



",,,nrb,"
--
Thanks for this request, and apologies about the delayed response.

I haven't done a code dive on this yet, but I think we should be able to add this as an entry to a BackupStorageLocation.ObjectStorage.config map, and maybe some validation on the storage class name.

@skriss @rosskukulinski Any other thoughts?
--
",skriss,"
--
@cullenmcdermott are you envisioning different storage classes for the different files stored in S3 (there's `ark-backup.json`, the backup tarball itself, a log file, etc.)?  Also - are you envisioning Ark/Velero changing these over time?  Or is a single static storage class, configurable by you, for each object that gets uploaded sufficient?
--
",cullenmcdermott,"
--
I think a single static storage class would be fine in most cases. The only potential issue I'd see with that is if there are any files that get uploaded that store metadata that are retrieved before a backup is performed then it could be expensive to retrieve them regularly in the case of IA or could cause the Ark backup to fail in the case of Glacier. 
--
",sladyn98,"
--
@nrb Would like to tackle this issue. Any tips on how to begin since this is my first time contributing here and looks like a good first issue.
--
",,,,
1150,OPEN,Pre/Post Backup & Pre/Post Restore Plugins,Enhancement/User; Needs Product,2020-05-01 15:39:25 +0000 UTC,king-jam,Opened,,"**Describe the solution you'd like**
I'd like to be able to use the same plugin framework but guarantee ordering based on the registration call within the plugin framework. This is for scenarios where ""rehydration"" of data is required via an application specific hook.

The current example that I'm working with is backup/restore of a MySQL environment. PV Snapshots only provide a crash consistent approach whereas leveraging `mysqldump` provides a point-in-time application consistent backup. The existing plugin framework does not encounter issues during backup but during restore is unable to guarantee that the plugin will execute after the service endpoint is up.

The workaround is to use the restore plugin action as a timing sequence to start a separate job that can remotely rehydrate the database but that results in the ark restore reporting success before the database is actually restored. This could create an issue with how statuses are conveyed to a user or an automation system that wraps the ark CLI.


**Anything else you would like to add:**
N/A


**Environment:**

- Ark version (use `ark version`): v0.10.0
- Kubernetes version (use `kubectl version`): v1.13.0
- Kubernetes installer & version: Minikube, PKS, KubeSpray
- Cloud provider or hardware configuration: N/A
- OS (e.g. from `/etc/os-release`): Ubuntu 18.04.1 LTS
",,,ncdc,"
--
Our backup and restore item action `Execute` methods are executed pre-backup (before persisting in the tarball) and pre-restore (before creating the item in Kubernetes). I'm maybe thinking that post-backup doesn't make much sense (do you have any use cases)? But post-restore definitely would give you what you need here. WDYT?
--

--
Let's walk through the scenarios in full detail.

# Backup
- A custom backup item action plugin that you're writing calls `mysqldump` and uploads the output to the Ark ObjectStore

# Restore
1. Ark restores pod
1. A custom restore item action plugin runs post-restore for the pod
    1. It waits for the pod to be running
    1. It restores the data backed up by `mysqldump`

# Post-restore, pod restart and/or scale-up
1. Pod starts up, data has already been restored

I'm not sure an initContainer would work, because you'd need the main `mysqld` process up and running to be able to restore the backed up sql, and that's in a regular container, not an initContainer.

Instead, I think you could use an exec-based liveness and/or readiness probe on the mysql pod. It could wait for the presence of marker file (placed by the restore item action plugin) before saying live/ready. You'd have to be careful to get it in a path that survives restarts, so it would need to be in a volume instead of something like `/tmp`. Do you think this approach could work?
--

--
How are you planning on making your restore item action plugin talk to mysql? If you're going through the `Service`, then the probes would make that challenging. But you could exec into the pod itself and run the command there, perhaps.
--

--
The pod's init container would not have access to the credentials to talk to object storage, though?

I would like to spend some more time thinking about how we can allow users to store arbitrary data in object storage. And then retrieve it when restoring. That's going to be a real need, isn't it?
--

--
@king-jam thanks for the update. We are knee-deep in project renaming work, but we should be able to come up for air starting next week. Thanks for being patient!
--
",king,"
--
Definitely agree. Post Restore would be the priority scenario. The rest may be generalized under the feature add but aren't required immediately.
--

--
After thinking about it further, The Post Restore hooks would need a caveat.....mainly in a scenario of restoring a database/etc, the hooks needs to happen just before exposing the Service so higher level apps don't start trying to execute transactions before data is fully restored.

An init container can solve this. It just leaves breadcrumbs that we might not want every time.
--

--
Your scenario is accurate. Would the probe also remove our ability to connect to the service to write the dump back in?
--

--
Just approaching that now. Probably can't use the `Service` moving forward.
--

--
So the pattern can also match that of Restic. An init container with the mysql PVC mounted can start `mysqld` and run an io.Reader stream from GetObject directly into the `mysql` client @ localhost. Then when the ""real"" `mysqld` instance starts, the data will all be present in a ""consistent"" state. The Post Restore piece alludes to how Restic has it's ordering hardcoded into multiple places. 

Thoughts?
--

--
I agree....I tried to figure out if I could steal your plugin registry and create a gRPC client to talk to the object store but in the end have opted for pulling code from the `cmd` package to peak the cloud credentials and inject environment variables/secrets to the init container. Essentially reimplemented the persistence pkg.

The fact that the svc account is available to a plugin also means I can create k8s objects and opted to just skip the init and instead pause the entire backup progress by inspecting runtime.Unstructured and creating a copy temporary Pod. It mounts the same PVC so the actual Pod will see the data as if it took a restart when it finally comes up.
--

--
So following back up @ncdc 

We got everything working, backup, restore, object storage handling, etc. Hid it behind a nice extensible interface; all that good stuff.

I would like to revisit this in terms of hooking the object store and the plugin concepts you discussed, it is something else to maintain and need to stay in sync with the BackupStorageLocations object.

The Post Restore piece does require a lot of thought since we want Ark to restore the Pod to a functional state, then do a check for plugins registered for Post Restore hooks that have an AppliesTo of `pods`. I think it's just another map of plugins and a for loop in the restore package but I fear I may be oversimplifying it.
--
",connorearl,"
--
I'm in the same boat on this one. I'm running into issues with mariadb-galera and having crash consistent backups. Any news on this issue?
--
",,,,,,
1144,OPEN,split permissions for viewing/downloading logs from downloading backups,Enhancement/User; Needs Product,2019-09-18 19:32:25 +0000 UTC,GAHila,Opened,,"**Describe the solution you'd like**
Allow certain users to be able to run ark backup logs but not be able to run ark backup download.


**Anything else you would like to add:**
When running:

`ark backup logs backupname`

in production operators groups gets:

`An error occurred: downloadrequests.ark.heptio.com is forbidden: User ""username"" cannot create downloadrequests.ark.heptio.com in the namespace ""heptio-ark""`

When we add downloadrequests.ark.heptio.com as an allowed resource it also allows them to run 
ark backup download backupname which would allow them to look at the secrets in production and that is not acceptable. For now we are blocking both at the expense of not being able to view the logs for backup failure.

`ark backup describe `and `ark backup get` show only the failed status but not the reason it failed. 

In our case the failure was caused by S3 SnapshotCreationPerVolumeRateExceeded error but we had to dig in to AWS Cloudtrail logs and relate those logs to the time that the backup failed.

**Environment:**

- Ark version (use `ark version`): 0.9.9
- Kubernetes version (use `kubectl version`):v1.11.5-eks-6bad6d
- Kubernetes installer & version: v1.11.1
- Cloud provider or hardware configuration: AWS EKS
- OS (e.g. from `/etc/os-release`):
",,,,,,,,,,,,,,
1135,OPEN,Need to handle provider rate limit issues (retry; etc),Bug; Needs Product; P2 - Long-term important,2021-01-25 07:50:13 +0000 UTC,jaxxstorm,Opened,,"**What steps did you take and what happened:**
In the course of a general backup, the API can rate limit the requests, which means the backup fails


**What did you expect to happen:**
The backup should at least retry

**The output of the following commands will help us better understand what's going on**:

```
time=""2018-12-20T13:05:54Z"" level=info msg=""Backup completed with errors: [error taking snapshot of volume: rpc error: code = Unknown desc = RequestLimitExceeded: Request limit exceeded.\n\tstatus code: 503, request id: 4650996e-445d-44c4-9462-f586863e949e, error getting volume info: rpc error: code = Unknown desc = RequestLimitExceeded: Request limit exceeded.\n\tstatus code: 503, request id: 416e3b3c-cc8a-43fa-8a08-1bcaa6417725, error getting volume info: rpc error: code = Unknown desc = RequestLimitExceeded: Request limit exceeded.\n\tstatus code: 503, request id: a84464d5-451c-480a-becc-8bfd970ef7f3, error getting volume info: rpc error: code = Unknown desc = RequestLimitExceeded: Request limit exceeded.\n\tstatus code: 503, request id: 93e78772-3a85-43c9-b5e8-622d36683699, error getting volume info: rpc error: code = Unknown desc = RequestLimitExceeded: Request limit exceeded.\n\tstatus code: 503, request id: 5b62f061-582c-4ce7-81fc-0931eea13cdf]"" backup=heptio-ark/full-hourly-20181220130007 logSource=""pkg/backup/backup.go:289""
```

CloudTrail event:

```
{
  ""Events"": [
    {
      ""EventId"": ""3559ed3e-e613-40d3-972b-3b2b1a9e3658"",
      ""EventName"": ""DescribeVolumes"",
      ""ReadOnly"": ""true"",
      ""AccessKeyId"": ""XXXXXXXXXXX"",
      ""EventTime"": 1545310889,
      ""EventSource"": ""ec2.amazonaws.com"",
      ""Username"": ""XXXXXXXXX"",
      ""Resources"": [],
      ""CloudTrailEvent"": ""{\""eventVersion\"":\""1.05\"",\""userIdentity\"":{\""type\"":\""AssumedRole\"",\""principalId\"":\""XXXXXXXXX:xxxxxxxxxx\"",\""arn\"":\""arn:aws:sts::<accountid>assumed-role/role/rolename\"",\""accountId\"":\""<accountid>\"",\""accessKeyId\"":\""XXXXXXXXXX\"",\""sessionContext\"":{\""attributes\"":{\""mfaAuthenticated\"":\""false\"",\""creationDate\"":\""2018-12-20T13:00:14Z\""},\""sessionIssuer\"":{\""type\"":\""Role\"",\""principalId\"":\""XXXXXXXXXX\"",\""arn\"":\""arn:aws:iam::<accountid>:role/role\"",\""accountId\"":\""XXXXXXXXXXX\"",\""userName\"":\""<username>\""}}},\""eventTime\"":\""2018-12-20T13:01:29Z\"",\""eventSource\"":\""ec2.amazonaws.com\"",\""eventName\"":\""DescribeVolumes\"",\""awsRegion\"":\""us-west-2\"",\""sourceIPAddress\"":\""xx.xx.xx.xx.xx\"",\""userAgent\"":\""aws-sdk-go/1.13.12 (go1.10.5; linux; amd64)\"",\""errorCode\"":\""Client.RequestLimitExceeded\"",\""errorMessage\"":\""Request limit exceeded.\"",\""requestParameters\"":{\""volumeSet\"":{\""items\"":[{\""volumeId\"":\""vol-00d6620bc08be02d9\""}]},\""filterSet\"":{}},\""responseElements\"":null,\""requestID\"":\""5b62f061-582c-4ce7-81fc-0931eea13cdf\"",\""eventID\"":\""3559ed3e-e613-40d3-972b-3b2b1a9e3658\"",\""eventType\"":\""AwsApiCall\"",\""recipientAccountId\"":\""<account>\""}""
    }
  ]
}
```
",,,ncdc,"
--
@rosskukulinski 
--
",seemonicago,"
--
@rosskukulinski  @ncdc @skriss Requesting another look at prioritizing this as we are also running into this same issue.
--
",skriss,"
--
:+1: we'll take a look at this for v1.3 planning
--
",pingoleon108,"
--
Is this feature already developed?
Thanks
--

--
Yes but I found a workaround, thanks
--

--
I think that I asked AWS to increase the limit but this is not always approved.
--
",nrb,"
--
@pingoleon108 Not yet - the priority has been moved around. Have you been affected?
--
",kaldorn,"
--
@pingoleon108 May I ask what your workaround was? I've recently started seeing these errors.
--
"
1089,OPEN,Define expectations with client/server version skew,P2 - Long-term important,2020-12-08 01:52:41 +0000 UTC,ncdc,Opened,,e.g. should the server reject/fail new backups/restores if they come from an older client / are missing certain fields?,,,,,,,,,,,,,,
1087,OPEN,Add ability to upgrade velero version via CLI,Area/CLI; Enhancement/User; Needs Product; P2 - Long-term important,2020-12-08 01:52:41 +0000 UTC,ncdc,Opened,,"- check for missing CRDs and add them
- ideally preserve upgrade logic for each version, to support upgrading from _n_ to _n+2_ without having to do _n+1_ first.",,,skriss,"
--
xref #2034 #2028 
--

--
xref #2508 
--

--
cc @yogeek. I've updated this issue title to be more general so we don't presume that upgrade should be a separate command from install. Please see #2508 for more context on @yogeek's preference for this to function more like `kubectl apply` rather than having a separate command for install vs. upgrade.
--
",,,,,,,,,,
1072,OPEN,Backup checksums,Enhancement/User,2021-01-02 11:01:52 +0000 UTC,ncdc,Opened,,"We need to store checksums of the various files we produce and upload to backup storage. These can then be verified to ensure no tampering.

We may also want/need to add the option to sign the checksum files for additional verification.",,,nrb,"
--
@skriss This seems like a security measure to me, guaranteeing the consistency of the data at rest.
--

--
I don't think this is for snapshots/restic contents specifically. It's for the JSON Kubernetes manifests, so that an attacker doesn't upload a modified JSON document that has something malicious in it (like an added container image as a sidecar to a valid application).
--
",VMmore,"
--
Before closing, we should check what features that restic has around this. This is a common feature of backup software and given that restic is passing the file system we have an opportunity to record the checksums.

For snapshots, I don't think it makes as much sense, we should rely on the snapshot provider to ensure consistency.
--
",skriss,"
--
Removed ""Candidate for close"" label - we'll need to decide how to prioritize.
--
",jnaulty,"
--
**Describe the problem/challenge you have**
Integrity of backup object is not guaranteed before restore.

**Why is this a Problem?**
Velero treats object storage as the source of truth. This could be abused by interfering with the object server. A compromise of this could affect the client and the server module if malicious input can trigger potential vulnerabilities in these

**Describe the solution you'd like**
When velero performs a backup of an object, velero acquires a checksum of that object, which it then stores in a custom resource along with the UUID of the object.

When velero performs a restore of an object, it will check the checksum of the object _before_ updating the kubernetes object. 


**Environment:**

- Velero version (use `velero version`): v1.2.0

--

--
**Info on Backup Objects**
Based on the documentation for [output-file-format](https://velero.io/docs/v1.2.0/output-file-format/), there are two files that would be useful to checksum:

backup artifact
> A backup is a gzip-compressed tar file whose name matches the Backup API resource's metadata.name

`velero-backup.json`
> JSON file lists all information about your associated Backup resource, including any default values.

```
rootBucket/
    backup1234/
        velero-backup.json
        backup1234.tar.gz
```

A checksum should be done after the creation of each artifact.

**Checksum Creation**

backup artifact:
execute a checksum in the Backup function for the gzipped tar archive
https://github.com/vmware-tanzu/velero/blob/dcca3c3d2bd5379a26d72a4737aef124b07eabdc/pkg/backup/backup.go#L201-L280


`velero-backup.json`
execute a checksum for the json file when it is written to disk in `persistBackup` function:
https://github.com/vmware-tanzu/velero/blob/dcca3c3d2bd5379a26d72a4737aef124b07eabdc/pkg/controller/backup_controller.go#L563-L569

The following parts need planning:
- storage of checksum in velero server state (or as a kubernetes resource)
- validation of checksum on restore

--
",invidian,"
--
>storage of checksum in velero server state (or as a kubernetes resource)

I guess the checksum could be stored in Backup CR. And in case of restoring to a different cluster, Velero should not record a checksum from the backup storage. Then user could either provide a valid checksum from the other cluster or accept insecure restore.

Also, alternatively to checksums one could use some sort of signatures, so Velero would sign the backup while it's being created and verify the signature before restoring. This would allow easy way to verify backups if Velero servers share the trusted public keys.
--
",,
1068,OPEN,[Epic] Item selection; inclusion; exclusion,Epic; Needs Product,2020-11-12 00:23:24 +0000 UTC,ncdc,Opened,,,,,,,,,,,,,,,,
1053,OPEN,Enable users to set restic repo passwords,Enhancement/User; Needs Product; P2 - Long-term important; Restic; Security,2021-03-17 17:16:22 +0000 UTC,skriss,Opened,,"Currently, Ark uses a static encryption password for restic repositories it creates.  Ark assumes that the data is protected at rest by IAM policies on the object storage bucket where the data is stored.

Some users would like to be able to explicitly set an encryption password for these Ark-managed restic repositories.  This issue serves as a placeholder for discussion on the whys/hows/whens.",,,dharmab,"
--
Really looking forward to this. IAM policies be changed, and backups can be downloaded to operator devices where they are vulnerable to theft or malware. This would add another layer of security.
--
",sh0rez,"
--
Ark is already creating a secret containing the restic key (`ark-restic-credentials`, value is `static-passw0rd`). I believe it would be a good first step to allow users to supply their own secret instead so the key would at least differ across individual ark install. And if the user does not supply one then maybe randomize the key and put it in the secret .. This is still better than using the same password for all users everywhere!
--

--
@ncdc if the next version of ark would silently start encrypting with randomized tokens stored in a secret, allow users to provide their own secrets and continue to run with the old static secret if already existent this change would not be breaking so there is no need for a major release, or do I miss something?
--
",ncdc,"
--
@rosskukulinski might want to consider for 1.0?
--
",rosskukulinski,"
--
@ncdc I would be supportive of having this part of the workstream, but I do wonder if it requires a breaking change.  1.x might be more reasonable from a timeline perspective?
--
",ac,"
--
Hi there, we've taken a second looks at the code and worked out that we can create our own restic repo passwords using the secret ""velero-restic-credentials"" and data ""repository-password"" and when it initializes the repo it will use this. In addition to this it can also be changed if you move sideways the restic folder in s3 and delete restic repositories found in resticrepositories.velero.io. You can still get at the old restic repository using the old password and native restic command. It would be good if this was included in docs as according to the code it will only use the static password if this secret isn't created, this could be useful for people.

As we want to be able to able to run restic to restore single files sideways (see #1210) we would need to be able to find out the password in order to do this
--
",ThoTischner,"
--
We also create the secret on our ci/cd via helm before the velero helm chart will be applied.
But this should be just a workaround, one local helm chart with just one resource (the secret) does not make much sense.
--
"
3151,OPEN,support azure file snapshot,Area/Cloud/Azure; Enhancement/User; Help wanted; Volumes,2020-12-07 23:59:24 +0000 UTC,andyzhangx,In progress,,"Currently azure cloud provide only supports azure disk snapshot, and [azure file plugin](https://kubernetes.io/docs/concepts/storage/volumes/#azurefile) is also a common storage layer which could provide multiple read/write.
Azure file snapshot doc:
https://docs.microsoft.com/en-us/azure/storage/files/storage-snapshots-files

@rosskukulinski 

I could also do the code review work of all azure part since I have been working on k8s on azure storage for a long time.",,,ncdc,"
--
@andyzhangx what would the mechanics be around restores? With Azure managed disks, for example, we create a new managed disk from a snapshot and associate that with the Kubernetes PersistentVolume. Could we do something similar with Azure files - can we create a new ""share"" from a snapshot and associate that with a PV? Or how would you envision this working? Thanks!
--
",andyzhangx,"
--
Yes, azure file pvc is like azure disk pvc. It's using different protocol and support multiple readwrite.
And its apiVersion requires >= 2017-04-17, see RESET api here:
https://docs.microsoft.com/en-us/rest/api/storageservices/snapshot-share
--

--
Update: we are already implementing azure file snapshot functionality in https://github.com/kubernetes-sigs/azurefile-csi-driver, currently azure file does not provide a restore sdk for this functionality, and we are expecting azure file team to provide a v2 sdk to cover this functionality. I think we could wait until that restore function is released, stay tuned.
--

--
> @andyzhangx any update on the restore SDK? There have been several folks inquiring about Azure File support lately, and they're having to default to using restic since we don't have a snapshotter plugin.

No update from azure file team now, I will let you know when that feature is released. @skriss 




--
",moebius87,"
--
Hi @ncdc , is there any good news about this issue? Thank you!
--
",skriss,"
--
@moebius87 nobody has picked this up to work on it, but it sounds like there's a good chance that a plugin could be developed for this. I'm happy to do a little R&D and provide some direction if someone wants to work on it!
--

--
@andyzhangx any update on the restore SDK? There have been several folks inquiring about Azure File support lately, and they're having to default to using restic since we don't have a snapshotter plugin.
--
",pimjansen,"
--
Also really need this! There is no way to properly snapshot/backup my azurefile at this point
--
",fredgate,"
--
Yes, it's a big lack.
--
"
1037,OPEN,can pods created by a DaemonSet be excluded from backups?,Bug; Enhancement/User,2019-03-14 17:36:10 +0000 UTC,mattnworb,In progress,,"We recently had a need to restore a cluster from a backup, which worked pretty great overall (thank you for Ark!).

We noticed after the restore that, for a DaemonSet we run for `fluentd`, twice as many pods existed than we expected - one set of pods that were created by the DaemonSet, and another set of fluentd pods that were not attached to the DaemonSet but did have the label `ark-restore: <name of backup>` indicating that those Pods were restored from the backup.

Is it possible to configure Ark to exclude Pods created by DaemonSets from being backed up, since the post-restore DaemonSet will manage to create one pod-per-node on its own?",,,ncdc,"
--
@mattnworb sorry for the delay in responding! We'll take a look and get back to you as soon as we can. Thanks for your patience!
--

--
@mattnworb 

I looked at the DaemonSet controller code in Kubernetes. If I'm reading it correctly, it's doing this:

If the pod has a `controllerRef` where the owner's kind is DaemonSet, and the owner's UID doesn't match the UID of the DaemonSet in question, it ignores that pod. I'm guessing the assumption is that ""some other"" DaemonSet is in charge.

If you're doing a true recovery, where Ark is restoring both the DaemonSet and its pod(s), we'll end up in the situation you reported. Ark isn't allowed to restore the original UID when we create the DaemonSet (it's assigned by Kubernetes), so there's no way the pod's controllerRef can point at the UID of the newly-restored DaemonSet, unless we restore the pod after we restore the DaemonSet. However, if we take that approach, it's possible/likely that the DaemonSet will create its own pod(s) around the same time Ark is creating them, ending up with extra pods that ultimately will get deleted by the DaemonSet controller.

It might make more sense to recognize there are some things, like in this situation, where Ark can't really ""win"" when recreating resources. We could maybe take a look at letting the DaemonSet create its pod(s), then find some way for Ark to update them and carry over any labels & annotations that are present in the backup. I'm not entirely sure how this would work, though - what if the backup has 5 pods for a DaemonSet - how do we map a pod in the backup to a pod in the new cluster?

This definitely needs some more thinking... 


--

--
@mattnworb what I'm struggling with the most is figuring out the best way to configure this along with all the other things we'll discover that also need tweaking. A simple fix for this could be to add a new `--include-daemonset-pods` flag (default to `false`) and have that map to a field on the `Restore` `spec` with the same name (we'd add it to the `Restore` instead of the `Backup` because our preference is to back up everything, but restore only what's necessary). I'm a bit worried that we'll find other similar needs, and I'm not sure if adding a new flag/field for each situation is appropriate, or if there's some better, more holistic approach. We're working on finalizing a stable API for 1.0, so we need to think through these issues carefully.

Once the rest of the team is back from the Thanksgiving holiday next week, we can have a more thorough discussion. Thanks again for your patience!
--
",mattnworb,"
--
Hi @ncdc thanks for the reply and looking into this!

The operation in question that led to me creating this issue was in fact a true recovery - Ark restored both the DaemonSet and all of the Pods in the (original) cluster that were created by the DaemonSet. 

What we noticed after the Ark restore was complete was that we ended up with 2x as many pods for this DaemonSet as we expected - the N pods that the restored DaemonSet created, one per node, and the N pods that had been in the backup. The latter half of these pods were not being managed by the (restored) DaemonSet as you pointed out.

In the case I am thinking of, I would be happy if Ark could be configured to not bother to backup pods that were created by DaemonSets - I'd be confident that a restored DaemonSet would create the necessary pods after being restored.
--
",,,,,,,,
1033,OPEN,Feature request: Enable backup only for selected cluster resources,Enhancement/User,2019-08-29 20:12:26 +0000 UTC,mwieczorek,In progress,,"Right now we can only set `--include-cluster-resources=true/false`. So all cluster resources or none are stored in the backup

I'd like to be able to select cluster resources (kinds) which ark will backup. 
",,,skriss,"
--
@mwieczorek you should be able to use the `--include-resources` and `--exclude-resources` to control this.  
--

--
Ah, ok, this makes sense and is a limitation we realized when we implemented the `--include-cluster-resources` flag.  Do you have any thoughts on what the UX might look like for this?
--
",mwieczorek,"
--
I didn’t describe that well.
I’d like to get backup of one (or more) selected namespace and one (or more) selected cluster resource. 
If I use —include-resources=SomeClusterResource all other namespaced resources are not in the backup.

As a workaround I just make 2 backups. 
But I was wondering if you consider such change (I know it may add some complexity...)
--

--
I'd prefer to stay with include/exclude, and not depend only on the selector. In my use-case, I'd like to backup whole namespace, regardless if some resources were labeled or someone forgot about it.

Also, I'd treat cluster-scoped and non-cluster scoped resources separately:
```
--include-cluster-resources stringArray   cluster-scoped resources to include in the backup, formatted as resource.group, such as storageclasses.storage.k8s.io (default: none, use '*' for all resources)
--include-resources stringArray    namespaced resources to include in the backup, formatted as resource.group, such as networkpolicies.networking.k8s.io (default: all resources)

--exclude-cluster-resources stringArray   cluster-scoped resources to exclude from the backup, formatted as resource.group, such as storageclasses.storage.k8s.io (default: none)
--exclude-resources stringArray    namespaced resources to exclude from the backup, formatted as resource.group, such as networkpolicies.networking.k8s.io (default: none)
```
Filtering should first process includes and then excludes (so, for example, I can set `--include-cluster-resources * --exclude-cluster-resources storageclasses.storage.k8s.io` to backup all cluster scope resources except storageclasses

Also, I like Andy's idea to reduce the number of flags and use f.e. `--cluster-resources-filter *,-storageclasses.storage.k8s.io` notation to get the same result as in the example above. 
--
",ncdc,"
--
We should also think about #589 and #590 as part of this discussion
--
",,,,,,
979,OPEN,Add a `velero status` command to report if the server is ready to create backup or not,Area/CLI; Enhancement/User; P3 - Wouldn't it be nice if...,2021-02-03 17:42:42 +0000 UTC,carlisia,In progress,,"There are two issues in this ticket, one is addressed with https://github.com/vmware-tanzu/velero/issues/1967 (BSL/VSL readiness).

Will use this to track adding the `status` command to report on the readiness via the CLI.

Original report:
---
I purposefully started the ark server w/o setting a backup location via the cli or deploying the `backupstoragelocation.yaml` file.

In the end, the backup sync kept retrying for a really long time.

I wonder if the warning at the top should be an error, if the message should be stronger, or else there is a better way to avoid the user being stuck in this scenario.

![image](https://user-images.githubusercontent.com/16508/47325521-0f0f7d80-d619-11e8-93f2-bd45ed6010f4.png)
",,,carlisia,"
--
For reference: https://github.com/heptio/ark/issues/898#issuecomment-429047146
--

--
Question: is it expected that Ark be used only for PV backup?
--

--
No, I'm asking something different. Currently we have backup storage and volume storage. Can we bootup the ark server w/o setting up/deploying configuration for the backup storage if we only want to do pv storage?

I know the answer to that ^ is no, but should we?
--

--
> What if we added an ark status command that could give you an overview of the server's status? Things like ""waiting on BSL"", ""waiting on VSL"", etc?

Yes, I'd like that. Also, could we change the message from `Checking for backup storage` to `Waiting for backup storage`? I think the later gives an idea that there's a user action that needs to be taken.
--

--
I like 1 better. I'm thinking when I want to check on a status it's bc something is happening/not happening and I want to get a quick read on the situation. Sound that there'd be a latency with option 2. For option 1, could the `ark status` not ping and check if the server is online? If not online maybe add a disclaimer saying so and providing last known status?
--

--
Adding this for future reference: we might consider adding to the `ark status` a field that indicates what the ark server looks for as far as default locations. Currently, if there's more than 1 location we can't tell which one is the default:

![image](https://user-images.githubusercontent.com/16508/47448027-d46e2800-d773-11e8-85b0-df393d3bd7cc.png)

--

--
This is different, but related: https://github.com/vmware-tanzu/velero/issues/675.
--
",ncdc,"
--
> In the end, the backup sync kept retrying for a really long time.

This is acceptable and a reasonable design, imho. It should keep trying until it has work to do.

> I wonder if the warning at the top should be an error, if the message should be stronger, of else there is a better way to avoid the user being stuck in this scenario.

What if we added an `ark status` command that could give you an overview of the server's status? Things like ""waiting on BSL"", ""waiting on VSL"", etc?
--

--
> Question: is it expected that Ark be used only for PV backup?

Are you asking if it should handle volumes that are directly referenced by the pod (e.g. `spec.volumes[i].gcePersistentDisk`)? If so, that's #408 
--

--
> No, I'm asking something different. Currently we have backup storage and volume storage. Can we bootup the ark server w/o setting up/deploying configuration for the backup storage if we only want to do pv storage?
>
> I know the answer to that ^ is no, but should we?

That is along the lines of #504. I'm interested in exploring this, but I'm not sure how/where we'd record the information (volume snapshot info).
--

--
It depends on how we implement it. I could see 3 options:

1. The `ark` server keeps a new status-y CR up to date and `ark status` would retrieve the CR's contents. This would potentially report false positives, e.g. in the event the `ark` server is offline.
1. `ark status` creates and sends a new `StatusRequest` CR, and the `ark` server has a controller that updates it. The client waits/watches for it to be processed.
1. The `ark` server exposes an HTTP endpoint that the client communicates with.

Of these, I'd probably vote for 2.
--

--
I'm afraid that if we have the server crashloop because something such as the default BSL is missing, we're going to get bug reports that the server is crashlooping. Do you agree or disagree?
--
",nrb,"
--
A bit of a tangent, but I think that the mechanism for getting this information would be somewhat similar - would `ark status` need creation of an endpoint like #770?
--

--
I like 2, too. Seems like a lot less plumbing for users to have to wrangle.
--

--
Given this wasn't included in #1052, I'm removing it from the v0.10.0 milestone
--
",rosskukulinski,"
--
> > In the end, the backup sync kept retrying for a really long time.
> 
> This is acceptable and a reasonable design, imho. It should keep trying until it has work to do.

@ncdc If Ark doesn't have a functioning backup storage location, the process should eventually exit.  If the Ark pod is running then I would expect that it's functional.  Alternatively, we could leverage a liveness probe to help let users know that Ark is not in a happy state.
--

--
@ncdc ahh! I didn't realize that this is related to a missing _default_.  Agree crashlooping for that is bad.
--
",,,,
957,OPEN,Make it possible to skip restic backup/restore,Enhancement/User; Needs Product; P2 - Long-term important; Restic,2021-02-09 16:13:26 +0000 UTC,fersingb,Opened,,"**Describe the solution you'd like**
I have set up ark to use restic and it works well. But sometimes I'd like to not backup all the data, only the cluster state. In this case it would be nice to have a flag to disable restic. 

The same applies for the restore phase. If I have a backup that has a corresponding restic snaphot, I'd like to be able to ignore that snapshot and restore only the cluster state.

Thank you

**Environment:**

- Ark version (use `ark version`): v0.9.8
- Kubernetes version (use `kubectl version`): v1.12.1
- Kubernetes installer & version: 
- Cloud provider or hardware configuration: Openstack
- OS (e.g. from `/etc/os-release`): CoreOS stable (1800.6.0)
",,,Subv,"
--
I think using `snapshotVolumes: false` in the Schedule definition solves this. When the Backup runs it won't try to snapshot the volumes, just the cluster state.

https://velero.io/docs/v1.5/api-types/schedule/#docs
--
",joshkwedar,"
--
@Subv I tested by specifying `--snapshot-volumes=false` during a backup create, yet the volume annotated for velero backup is still backed up via restic.  I saw the following in the comments of doc you've linked.  This option doesn't appear to work for restic.

```
# Whether or not to snapshot volumes. This only applies to PersistentVolumes for Azure, GCE, and
# AWS. Valid values are true, false, and null/unset. If unset, Velero performs snapshots as long as
# a persistent volume provider is configured for Velero.
```
--
",,,,,,,,
929,OPEN,Ability to specify which volumes to snapshot (or not),Enhancement/User; Needs Product,2019-03-14 17:36:10 +0000 UTC,ncdc,Opened,,"**Describe the solution you'd like**
The `--snapshot-volumes` flag only enables or disables the snapshots, but I want to specify which ones to snapshot.


**Anything else you would like to add:**
There are some persistent volumes we stand up like RabbitMQ and Redis which we are interested in doing a configuration backup but are not interested in snapshotting those volumes because the data in those volumes is ephemeral. However, we do have data stores like MongoDB in which we are interested in doing a config backup AND a snapshot of the data. It would be useful to specify which `pvs` ark should make a snapshot of.

(from https://kubernetes.slack.com/archives/C6VCGP4MT/p1539287294000100)",,,,,,,,,,,,,,
915,OPEN,Support backing up and restoring Helm workloads,Enhancement/User,2020-12-08 01:52:39 +0000 UTC,thdrnsdk,In progress,,"Hello!

I am one of the users who is using ark now.
The ability to back up objects in kubernetes is great.
By the way, our system distributes the application using helm and uses helm history as the lifecycle of the application.

Do you plan to do ark backup and restore in helm's deploy unit instead of kubernetes object?
",,,nrb,"
--
Hi @thdrnsdk. There is an Ark Helm chart available at https://github.com/helm/charts/tree/master/stable/ark, but it is not currently maintained by the Ark team.

Ark also doesn't have any logic to handle Helm resources differently than other Kubernetes objects at the moment.

Can you provide a bit more detail on what behavior you're looking for?
--
",thdrnsdk,"
--
oh it seems to be misunderstood. I'm sorry my poor English.

i know ark is deployed by helm chart.

my question is 

Is there any way to backup and restore k8s objects deployed using helm?

for example i'm deployed ""nginx"" use for helm chart. 

$ helm install --name=nginx nginx

and 

$ helm ls

show the list of helm deploy. next update object use for helm cli 

$ helm update nginx XXXXX

in this state when i backup the objects, and restore the other cluster it can not tell how the nginx that I restored was deployed and updated with helm.

Well I do not know if my question is understandable. If you do not know, I'll show you a detailed example later.

thank you
--

--
hello @ncdc 
i'm already test it. 

The namespace with tiller and the namespace to backup are different, so we could not create one ""backups.ark.heptio.com""CRD object.

If I have same namespace of the tiller that I want to backup, it is not a problem, but as you know, deploying a tiller in every namespace seems a bit wasted.

So I asked if there was any other way.

--
",ncdc,"
--
My understanding is there is a ConfigMap that Helm uses to keep track of the things it's deployed. For the time being, you would need to make sure that your backup includes the relevant Helm ConfigMaps, and that they're included when restoring. I don't think we've tried this, so we'd be interested to hear how it goes if you attempt it.
--

--
> The namespace with tiller and the namespace to backup are different, so we could not create one ""backups.ark.heptio.com""CRD object.

A single Ark `Backup` can back up multiple namespaces. If you're using the cli: `ark backup create my-backup --include-namespaces myapp,tiller`.

However, maybe we can try to be more efficient:
1. Have the Ark server know about a default namespace for tiller (`ark server --tiller-namespace foo`)
1. When backing up, allow the user to override the default tiller namespace (`ark backup create my-backup --tiller-namespace bar`)
1. When backing up, if Ark can identify an item as belonging to a Helm chart deployment (is there a label for this?), also back up the appropriate `ConfigMap` from the tiller namespace.
1. When restoring, restore the `ConfigMap` along with all the other items (pods, etc).

Do you think something like this could work?
--

--
@Evesy does Helm automatically label all the items it creates (pods, services, etc) with a common label, or is that entirely up to the chart itself?
--

--
Not aligning with the ConfigMap is fine, assuming the ConfigMap has a naming convention we can discern... But it sounds like there's no way to 100% guarantee that we could automate collecting all resources created by a chart + its associated ConfigMap. Is that accurate?
--

--
Does the ConfigMap list everything that Helm created for the chart?
--
",Evesy,"
--
Hi, This is something we've been thinking about too. We (generally) namespace our applications and in the event we need to restore one, we need to restore the relevant ConfigMap's for the release too.

Helm does label the ConfigMaps as per the below:
```
    MODIFIED_AT: ""1540987831""
    NAME: platform-unsee
    OWNER: TILLER
    STATUS: SUPERSEDED
    VERSION: ""3""
```

When we've had to do this previously for an individual application we've restored in two phases:
`ark restore create --from-backup=<> --include-namespaces <APP_NAMESPACE>`
`ark restore create --from-backup=<> --include-resources configmaps -l NAME=<APP_NAME>`

One option is to ensure the resources created via your helm release all have the matching  `NAME` label applied, but a multi-selector restore would also be useful (i.e. This namespace && these labels in a different namespace) -- I think that extends what label selectors are capable of though.

We haven't come across this issue on the `Backup` side of things since we do full cluster backups, but  I can see it definitely being sought after in the future
--

--
@ncdc Unfortunately not. There's a bit of a convention of having the below labels, but even those don't align with Helm ConfigMap's:

```
app
chart:
release:
heritage:
```
--

--
Not that I can see, either way you'd be relying on naming conventions to match resources with a helm release.
The only way to discern exactly what was created by a helm release would be to inspect the currently deployed release versions ConfigMap to identify which resources are managed by that Helm release.
--

--
It'll include all 'standard' resources that Helm has created, however CRD's created by `crd-install` hooks, as well as any actions taken with other pre/post install hooks won't be in there, so even that isn't reliable :(
--
",Dennor,"
--
I actually wrote a [plugin](https://github.com/Dennor/velero-plugin-helm) which does some basic helm release backup including resources created by hooks. The only thing I'm unsure is how to tell velero to that ResourceIdentifier is cluster scoped (for example CRD).
--

--
Yeah, scratch my question. The first thing I tried was blank Namespace but apparently I messed something up somewhere else. Now it works.

Thanks.
--
",skriss,"
--
@Dennor I believe you just need to leave Namespace blank - does that not work?
--

--
btw, awesome on the plugin! I'll check it out :)
--
"
904,OPEN,RFE: ability to restore one or more items by name,Enhancement/User; Needs Product,2020-12-08 01:52:39 +0000 UTC,dirtbag,In progress,,"This is more of a design question. I know that ark is designed to be a disaster recovery product, but we are wondering if there is a way to pick and choose kubernetes objects to restore? Say for instance, somehow one of our ingresses gets deleted/damaged and we do a restore, It will try to restore the entire cluster (or just the namespace that we specify). Is there a way to restore individual pieces like an ingress?  I found the   ""--include-resources""  in the restore docs, but I couldn't find a complete list of whats allowed to be specified there.   Also, is there something that documents how ark tries to do the restore when objects are already there? I did some restore tests after deleting certain parts of the pod like secrets or other objects and it seems like ark will not restore the object if it is already existing.  

thanks/regards,
db",,,wwitzel3,"
--
Hi db,

Ark has a `--include-namespaces` flag which lets you choose which namespaces from the backup to restore. But there is no way to selectively restore specific objects. As for what Ark does if some of the objects are present, it will skip and log, an example of what that output might look like is in the [restore debugging guide](https://heptio.github.io/ark/v0.9.0/debugging-restores).

My current recommendation would be to have a set of backups that only include what you might find yourself wanting to selectively restore instead of one large backup that holds everything. This has the added benefit of not having to remember which command flags to use, since that information was coded in to the each backup when it was created.

That said, I think we can consider per-item restoration as an enhancement to Ark.
--
",ncdc,"
--
> I found the ""--include-resources"" in the restore docs, but I couldn't find a complete list of whats allowed to be specified there

It's anything you can specify to `kubectl get` - e.g. `pods`, `deployments`, `secrets`, and so on.
--
",dirtbag,"
--
Thanks, that is helpful!

-db

--
",skriss,"
--
xref #2043 
--
",jnaulty,"
--
a useful argument when creating velero backups is the `--selector`
```
velero backup create --help | grep selector                                                                                                   
  -l, --selector labelSelector                          only back up resources matching this label selector (default <none>)
```
You can label a resource you want backed up and use the selector argument to specify it in the `velero backup` command.
--
",hmz,"
--
@jnaulty @ncdc Yes that is an option but I can not label dinamically provisioned disks on creation time -> https://github.com/kubernetes/kubernetes/issues/58413 so having that kind of an enhancement in Velero would realy help.
--
"
872,OPEN,Add tests for our CLI commands,Area/CLI; Enhancement/Dev; Good first issue; Help wanted; Size/M; kind/tech-debt,2020-05-01 17:51:06 +0000 UTC,carlisia,In progress,,"When we create a new cli command, the way to test it is manually by running the binary. What if instead we were to test that the commands we expect are all there and are responding to and with what we expect, programmatically?

We can follow this example:
https://github.com/kubernetes/kubernetes/blob/2981fb7a0171798184b4f860b07870de0ff7b234/cmd/kubeadm/test/cmd/util.go

https://github.com/kubernetes/kubernetes/blob/2981fb7a0171798184b4f860b07870de0ff7b234/cmd/kubeadm/app/cmd/phases/util_test.go

This should be broken down into 1 issue per command as it is looking like a large endeavor. The first one would be the biggest since all/most of the framework for the test cases would need to be put together.",,,nrb,"
--
What kinds of tests are you looking for? End-to-end?
--

--
Ok - can you add a little bit of a description? I'm not 100% what you're looking for here :)
--
",carlisia,"
--
Not end-to-end, scoped to the cli. Many of the cli cmds have a lot of logic in them.
--

--
@nrb thanks for probing me for a description! Let me know if I can expand on what I have so far.
--

--
@nzoueidi thank you, and I have updated the links. Let me know if you'd like to start with this or with #574 and I'll assign the issue to myself while you work on it.
--

--
Go for it!
--

--
Hey @skhalash yes it is! 🎉 
--
",nzoueidi,"
--
@carlisia thanks a lot for opening this issue. I was wondering why there is no tests for Velero binary. 
More like logic testing. I would like to help with implementing these tests and I would like also to update the second link in the issue. 
https://github.com/kubernetes/kubernetes/blob/master/cmd/kubeadm/app/cmd/phases/util_test.go
--

--
Sure! thanks @carlisia. I would like to start with this issue. :)
--
",skhalash,"
--
Is it still relevant? If yes, I would like to do it!
--

--
Hey @carlisia! Just want to clarify something before I start implementing it. I read the discussion in the thread and it's not really clear, what kind of test you have in mind. Most of the Velero CLI commands access a k8s cluster, so do you mean:
1) Unit testing the CLI logic by stubbing out/faking the client factory. If yes, are there any existing test utils for that?
2) More of e2e tests with a real cluster (e.g. a local kind cluster which is getting spinned up every time a test suite is run)?
In the kubeadm exampe you posted they only seem to run negative tests.
--
",,,,
729,OPEN,[Epic] Velero CLI UX Consistency,Epic,2020-12-08 01:52:39 +0000 UTC,rosskukulinski,Opened,,"As an Ark user, I would like for the CLI flags for Ark to be consistent in behavior.",,,jrnt30,"
--
This was a bit of a conversation when Ark was first launched and to appeal to a few parties both `ark <verb> <noun>` and `ark <noun> <verb>` were supported.  

Is this ""in scope"" for this Epic or does this cover something else?
--
",,,,,,,,,,
726,OPEN,[Epic] Backup ownership and sharing,Breaking change; Enhancement/User; Epic,2020-02-19 17:04:53 +0000 UTC,rosskukulinski,Opened,,"**User Stories**

As a cluster administrator, I would like to be able to safely backup from Cluster A and restore into Cluster B.  E.g. 
 * Production cluster backing up hourly, staging cluster restoring from the production backups to maintain a ""real-world"" environment
 * Production cluster A backing up, hot-standby Production Cluster B restoring on a regular basis

**Features**
- [ ] Add/enforce ownership of backup locations
- [ ] Resolve naming conflicts when importing backups from multiple clusters
- [ ] [Update FAQ re: sharing buckets](https://github.com/heptio/ark/blob/master/docs/faq.md#im-using-ark-in-multiple-clusters-should-i-use-the-same-bucket-to-store-all-of-my-backups)

**Non-Goals**
* Integration with IAM or other authentication/authorization systems

**Notes**

This behavior is currently supported through a `restoreOnly` mode which is prone to user error. ",,,,,,,,,,,,,,
675,OPEN,FE: Create a `velero debug` command for gathering troubleshooting information,Enhancement/User; P1 - Important,2021-02-22 22:28:00 +0000 UTC,nrb,In progress,,"**Describe the solution you'd like**
Provide an `ark debug` subcommand that could output the following information:

* ark client and server versions
* ark pod logs (should probably include restic logs, too)
* ark config
* If a backup/restore name is provided:
** relevant backup or restore logs
** backup and/or restore YAML

Additionally, the `ark debug` command should provide a way to filter out sensitive information like:

* bucket names
* secrets
* More?

This command would make it easier for users to file bug reports and get answers in a timely manner.",,,rosskukulinski,"
--
Great suggestion @nrb.  We could also pair this command with https://github.com/heptio/ark/issues/578
--
",nrb,"
--
Oh nice, I had missed that issue.

I definitely think linking the two would be a good idea.
--

--
I think for 1.7.0, we can start with this list:

 * Kubernetes version
 * Velero client and server versions
 * Velero pod logs, including restic
 * Velero Deployment
 * List of plugins
 * If a backup/restore name is provided:
   * relevant backup or restore logs
   * backup and/or restore YAML

These can be provided locally in a gzip or zip file as a first pass, allowing users to scrub data with their own tools. We can iterate on scrubbing within the functionality after that.

Ideally, this would be written for the client side, and could run against any version of Velero the server side.
--

--
After some experimentation, I think we can use https://github.com/vmware-tanzu/crash-diagnostics for this info. Here's a sample crashd script:

```skylark
ns = ""velero""
# Working dir for writing during script execution
crshd = crashd_config(workdir=""{0}/crashd"".format(os.home))
# Read the default kubeconfig, like velero
set_defaults(kube_config(path=""{0}/.kube/config"".format(os.home)))
capture_local(cmd=""velero version"")
# These need to go into functions due to Starklark limitations
# if args.backup:
    # backupLogsCmd = ""velero backup logs {}"".format(args.backup)
    # capture_local(cmd=backupLogsCmd)
# if args.restore:
    # restoreLogsCmd = ""velero restore logs {}"".format(args.restore)
    # capture_local(cmd=restoreLogsCmd)
kube_capture(what=""logs"", namespaces=[ns])
kube_capture(what=""objects"", kinds=[""customresourcedefinitions""])
archive(output_file=""diagnostics.tar.gz"", source_paths=[crshd.workdir])
```

Some concerns around using crashd:
 * We'll need to figure out how to make sure Velero users have this, easily. Use packaging dependencies? Fetch it at runtime?
 * What do we do with offline installs?
--
",metadave,"
--
I was thinking that `ark debug` and `ark bug` could be two separate things (or a set of flags on one command):

- As an open source user, having a convenience command like `ark bug` to populate some info in a Github issue would be nice, but I wouldn't want to expose my config, logs, or sensitive data without me manually editing/copying/pasting into the issue.

- As a customer paying for support, I could see `ark debug` generating a tarball of info that could be attached to a private helpdesk ticket. It's fine to try and scrub sensitive data, but you can't guarantee that the code won't miss a secret or two. We used a command like this to support Riak back in the day, the support team loved it.

Perhaps `ark bug` could include a `--verbose` and/or `--tarball` flag(s) to generate the more complete debug file.



--
",ncdc,"
--
I agree they should be 2 separate things. `ark bug` must never expose confidential data and is really a convenience for getting you to a GitHub issue, possibly with some information filled in. `ark debug` may or may not contain confidential data - we should have a sufficient number of flags to allow you to control what is included in the debug tarball.
--
",carlisia,"
--
Wanted to drop the output of Tilt's `tilt doctor` cmd:

```
❯ tilt doctor
Tilt: v0.17.12, built 2020-11-19
System: darwin-amd64
---
Docker
- Host: [default]
- Version: 1.40
- Builder: 2
---
Kubernetes
- Env: kind-0.6+
- Context: kind-development
- Cluster Name: kind-development
- Namespace: velero
- Container Runtime: containerd
- Version: v1.18.2
- Cluster Local Registry: none
---
Thanks for seeing the Tilt Doctor!
Please send the info above when filing bug reports. 💗

The info below helps us understand how you're using Tilt so we can improve,
but is not required to ask for help.
---
Analytics Settings
--> (These results reflect your personal opt in/out status and may be overridden by an `analytics_settings` call in your Tiltfile)
- User Mode: opt-in
- Machine: b01f29c71f7ed63d15c1a67509c7c06d
- Repo: Z6GQn0TgYuYG6BNNif2f/A==
```
--
",,
605,OPEN,Make it easy to mark/unmark a volume for restic backup,Enhancement/User; P2 - Long-term important; Restic,2020-03-31 02:06:26 +0000 UTC,ncdc,Opened,,"The current mechanism to mark a volume for a restic backup is
```
kubectl -n YOUR_POD_NAMESPACE annotate pod/YOUR_POD_NAME backup.ark.heptio.com/backup-volumes=YOUR_VOLUME_NAME_1,YOUR_VOLUME_NAME_2,...
```

It would be nice to make this easier with an `ark` subcommand. We should also make sure we can support various resources that contain pod specs (deployments, statefulsets, replicasets, jobs, replication controllers, pods themselves, ...)",,,agolomoodysaada,"
--
As a good starting point, I created this snippet that helps set up annotation commands for you. Install [fx](https://github.com/antonmedv/fx) and then run

```sh
kubectl get pods --all-namespaces -o json | fx 'l => l.items' 'i => i.reduce((acc, p) => {acc.push(""kubectl -n "" + p.metadata.namespace + "" annotate pod/"" + p.metadata.name + "" backup.ark.heptio.com/backup-volumes="" + p.spec.volumes.map(v => v.name).join("","")); return acc;}, [])'
```

Which outputs the following structure

```sh
[
...
  ""kubectl -n redis annotate pod/redis-master-4 backup.ark.heptio.com/backup-volumes=redis-data,redis-conf,default-token-lqmpv""
...
]
```

It would be awesome if ark itself could help manage that out of the box and maybe take a selector as an argument. For example `ark restic annotate -n mynamespace -l app=mongodb --all-volumes`
--
",zakkg3,"
--
I wrote a cli for this, just run `python3 annotate.py namespace1 namespace2`
https://github.com/zakkg3/podannotator
requires python3
It will annotate all pods with PVC in the given namespace(s).

--
",carlisia,"
--
Related: https://github.com/vmware-tanzu/velero/issues/1871.
--

--
I added this topic to our community meeting tomorrow (3/31): https://hackmd.io/Jq6F5zqZR7S80CeDWUklkA?both#March-31-2020.
--
",,,,,,
599,OPEN,Verify logrus hooks don't have race conditions,Enhancement/Dev; P2 - Long-term important,2020-12-08 01:52:38 +0000 UTC,ncdc,Opened,,"Check to make sure that multiple goroutines logging messages don't have race conditions, especially with the log location hook.",,,skriss,"
--
The version of logrus we're currently using does have a concurrency bug around multiple goroutines logging to the same `*logrus.Entry` and using hooks that mutate the data in the entry.  It *appears* to be fixed in a later version of logrus - doing more testing now.
--

--
Nope, looks like there's still an issue, although some adjacent fixes have been made. Filed an issue: https://github.com/sirupsen/logrus/issues/953

Here's my test that reproduces this issue (intermittently):

```go
func TestLogging(t *testing.T) {
	logger := DefaultLogger(logrus.InfoLevel)
	// logger.ReportCaller = true

	log := logger.WithField(""foo"", ""bar"")

	var wg sync.WaitGroup
	for i := 0; i < 10; i++ {
		go func(i int) {
			wg.Add(1)
			defer wg.Done()

			switch i {
			case 0:
				log.Infof(""Here %d"", i)
				// log.WithField(""i"", i).Infof(""Here %d"", i)
			case 1:
				log.Infof(""Here %d"", i)
				// log.WithField(""i"", i).Infof(""Here %d"", i)
			case 2:
				log.Infof(""Here %d"", i)
				// log.WithField(""i"", i).Infof(""Here %d"", i)
			case 3:
				log.Infof(""Here %d"", i)
				// log.WithField(""i"", i).Infof(""Here %d"", i)
			case 4:
				log.Infof(""Here %d"", i)
				// log.WithField(""i"", i).Infof(""Here %d"", i)
			case 5:
				log.Infof(""Here %d"", i)
				// log.WithField(""i"", i).Infof(""Here %d"", i)
			case 6:
				log.Infof(""Here %d"", i)
				// log.WithField(""i"", i).Infof(""Here %d"", i)
			case 7:
				log.Infof(""Here %d"", i)
				// log.WithField(""i"", i).Infof(""Here %d"", i)
			case 8:
				log.Infof(""Here %d"", i)
				// log.WithField(""i"", i).Infof(""Here %d"", i)
			case 9:
				log.Infof(""Here %d"", i)
				// log.WithField(""i"", i).Infof(""Here %d"", i)
			}
		}(i)
	}

	wg.Wait()
	<-time.After(100 * time.Millisecond)
}
```
--

--
Somewhat related, newer versions of logrus have built-in support for capturing log locations, which we should switch to. That code doesn't seem to be impacted by this race condition. It won't totally solve the issue for us, though, as we have other mutating hooks.
--

--
Moving this back to ""New Issues"" as there's no active work being done on it. We should follow up on the upstream bug if/when this becomes critical.
--
",,,,,,,,,,
596,OPEN,Define & document policy for which Kubernetes versions and features we support,Area/Documentation; P2 - Long-term important; Survey,2020-12-08 01:52:38 +0000 UTC,ncdc,In progress,,"Define & document a policy for which Kubernetes versions and features we support:

1. What is our minimum required Kubernetes version
1. Do we attempt to do graceful degradation for older Kubernetes versions?
    1. json-schema validation
    1. /status subresources for CRDs
    1. CRD field defaulting",,,Bradamant3,"
--
Can we get started on this one for 0.10.0? Maybe say something about 0.9.1 for backward compatibility, at least? Changing milestone provisionally.
--

--
Milestone reverted; oneliner for 0.10 added (see PR #1023 )
--
",ncdc,"
--
xref #529
--
",,,,,,,,
584,OPEN,Define how to enable/disable and configure plugins,Area/Plugins; Breaking change; Enhancement/User; P2 - Long-term important,2020-12-08 01:52:38 +0000 UTC,ncdc,In progress,,"For example, some users might not want to enable the plugins that automatically walk from pod to pvc to pv.",,,ncdc,"
--
Let's say we want to enable/disable and configure the ""foo.com/pod"" BackupItemAction. Possible options:

- Create a ConfigMap in the heptio-ark namespace, the name can be whatever:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: foo-com-pod-config
  labels:
    ark.heptio.com/plugin-config: foo.com/pod
data:
  enabled: false
  addAnnotations:
    color: green
    size: small
```

Let's say we just want to disable a plugin that has no configuration:

- add `--enable-plugins NAME(s)` and/or `--disable-plugins NAME(s)` flags to the `ark server`
- use ConfigMaps and Ark is responsible for checking for an enable/disable annotation:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: foo-com-pod-config
  labels:
    ark.heptio.com/plugin-config: foo.com/pod
    ark.heptio.com/plugin-enabled: false
```


Proposed approach:
1. Admin uses `--enable-plugins` and/or `--disable-plugins` flag for the `ark server` to control which plugins are enabled
1. Admin and/or users can create ConfigMaps to configure their plugins, assuming plugins support configuration
--
",carlisia,"
--
Un-assigned myself since I'm not currently working on this.
--
",,,,,,,,
549,OPEN,Review all error handling in Velero CLI,Area/CLI; Area/Documentation; Enhancement/User; P2 - Long-term important; kind/tech-debt,2020-12-08 01:52:38 +0000 UTC,rosskukulinski,Opened,,"As we lead up to 1.0, I think it would be helpful to do a comprehensive review of all error handling in the Ark CLIs to ensure we have friendly error messages and associated troubleshooting documentation.

Spun out of #492 and other error related issues.

- [ ] https://github.com/heptio/ark/issues/492",,,skriss,"
--
Not sure if we should lump this together or not, but we should also review our server log messages & levels. It would be nice for info-level server logging to have a consistent level of information across all controllers.
--
",,,,,,,,,,
536,OPEN,Create example Grafana dashboard,Enhancement/User; Good first issue; Help wanted,2020-10-21 08:14:25 +0000 UTC,rosskukulinski,In progress,,"Once Ark has exposed prometheus metrics (#84 / #531), it would be great if we had an example Grafana dashboard for visualizing the state of Ark.

In the simplest form, this could be checked into the Ark repo.  It could also be published to the Grafana dashboard community (https://grafana.com/dashboards).

Variables:
* Backup Name
* Schedule Name

Potential end-user graphs:
* Gauge showing number of active backups
* Gauge showing number of active restores
* Backup success rate (completions / attempts) over time
* Restore success rate (completions / attempts) over time
* Rate of backups over time
* Rate of restores over time
* {99, 95, 50}% duration for Restores over time
* {99, 95, 50}% duration for Backups over time
* {99, 95, 50} percentiles of backup byte size over time
* {99, 95, 50} percentiles of restore byte size over time
",,,rosskukulinski,"
--
@ashish-amarnath if you're working on #531 and find yourself creating a grafana dashboard to test with, here's an initial take at what some useful graphs might be.
--
",ashish,"
--
Good idea!
--

--
@mtritabaugh You can work on this if you would like. I am not finding a way to assign this to you either :) 
--
",vitobotta,"
--
Hi! Does a Grafana dashboard for Velero/Ark exist yet? I managed to have Prometheus operator scrape the Velero metrics but I don't know how to use them. Thanks
--

--
Hi @HaveFun83 ! Thanks for the dashboard, very useful. What is ""Backup Time""? I thought it's the duration but it only shows a flat bar on the zero for me. Also, what does ""Backup Success"" show in case there are failed backups? Thanks!
--
",skriss,"
--
@vitobotta we don't have a sample one, but maybe another user has something they can share.
--
",bryanlarsen,"
--
see also #1136 
--

--
![Backups   Grafana](https://user-images.githubusercontent.com/32073/61084116-4e763800-a3fb-11e9-95fe-d7c210302d16.png)
![Backups   Grafana(1)](https://user-images.githubusercontent.com/32073/61084121-50d89200-a3fb-11e9-9f73-854ca874695a.png)

--
",HaveFun83,"
--
Hi i created a velero dashboard but the following metrics are missing:
velero_restore_duration_seconds_bucket
velero_restore_tarball_size_bytes

Maybe it will be useful as blueprint. Any suggestions are welcome.

https://gist.github.com/HaveFun83/57b41e85fde4249daab74a9850885f6a#file-kubernetes-_-addons-_-velero-stats-1568113703354-json
--

--
> > Hi i created a velero dashboard but the following metrics are missing:
> > velero_restore_duration_seconds_bucket
> > velero_restore_tarball_size_bytes
> > Maybe it will be useful as blueprint. Any suggestions are welcome.
> > https://gist.github.com/HaveFun83/57b41e85fde4249daab74a9850885f6a#file-kubernetes-_-addons-_-velero-stats-1568113703354-json
> 
> Hi @HaveFun83 , thanks for a dashboard!
> I see you calculate 'Active backup' with `sum(rate(velero_backup_attempt_total[15m])) / sum(rate(velero_backup_success_total[15m]))`
> May I kindly ask you to spread some light on that? Thanks in advnace!

the graph should represent @rosskukulinski suggestion:
Gauge showing number of active backups

But you are right this expression makes no sense i changed it but currently only active scheduled backups will be count
--

--
> Hi @HaveFun83 ! Thanks for the dashboard, very useful. What is ""Backup Time""? I thought it's the duration but it only shows a flat bar on the zero for me. Also, what does ""Backup Success"" show in case there are failed backups? Thanks!

Backup Time shows the ""velero_backup_duration_seconds_bucket"" metric can you check your Prometheus if there is any data available?

Backup Success rate must be 1 if its below something is wrong.
Failed backups should be visible in ""Backup Total Count""


--
"
529,OPEN,Support /status subresource on our CRDs when it's beta (v1.11),Enhancement/Dev; P2 - Long-term important,2020-12-08 01:52:38 +0000 UTC,ncdc,In progress,,https://github.com/kubernetes/features/issues/571,,,nikhita,"
--
@ncdc out of curiosity, with which version of Ark will v1.11 be used?
--

--
@ncdc thanks!
--
",ncdc,"
--
We're still determining our version matches between Ark and Kubernetes - #596 
--
",,,,,,,,
505,OPEN,Support for volume snapshot groups,Breaking change; Enhancement/User; Needs Product; P2 - Long-term important; Volumes,2020-10-22 15:45:54 +0000 UTC,ncdc,In progress,,Some storage providers have the ability to coordinate a snapshot across multiple volumes simultaneously. It would be nice if Ark could support this for those providers. We'll need to determine how to model this as well as what the UI/UX looks like.,,,ncdc,"
--
cc @disrani-px @jbeda @dhanmm @emuetzel 
--

--
I know portworx does. There's an RFE to add it to ceph. I think dotmesh might.

Ark would need to somehow find all the PVs in a group and then issue a provider-specific command to tell the provider to snapshot them as a group. It's an atomic, consistent, single action instead of 1 per PV.
--

--
Yes, any time when you'd want to coordinate an atomic point-in-time snapshot of multiple volumes.
--
",rosskukulinski,"
--
Which storage providers currently offer this feature?

How is this snapshot process different from Ark doing backups across PVs with labelSelector?
--

--
So this would enable atomic backups for things like scale-out databases (mongo, elastic, etc) and/or multi-tier apps that have data consistency problems (e.g. wordpress content + mysql data)?
--

--
Ok - I think we should hold off on this until we have more than one storage provider offering a similar feature.  I want to make sure we don't build anything that is specific to one implementation.
--
",dymurray,"
--
Is this issue a good place to track the Data Protection Working Groups attempt to solve the problem consistency groups?

I am curious if Velero intends to provide first class support for the ability to take a backup that includes an applications entire state at a single moment in time. This is being worked on upstream and I'm wondering if that is the best place to solve this problem or if it's something Velero is interested in solving.
--
",skriss,"
--
@dymurray late in responding here, but ideally I think we want to use/contribute to upstream solutions as much as possible. We definitely do not have a plan/design in mind for this in Velero right now.
--
",,,,
504,OPEN,design for data-only restores,Breaking change; Enhancement/User; P2 - Long-term important,2021-01-22 09:09:58 +0000 UTC,ncdc,In progress,,"If you have pods running in a cluster and you're using Ark to take backups of their PVCs, it would be nice to be able to do a data-only restore. In other words, for pods owned by deployments, replica sets, etc., create a new PV from a backup snapshot, associate it with a PVC, and update the deployment to use the new volume.",,,ncdc,"
--
cc @jbeda @dhanmm @emuetzel for input
--
",rosskukulinski,"
--
This feature would be a great-use case for Ark.  We will need to spend some time thinking about the CLI UX & data models that would enable this capability.  I think this is a strong contender for new functionality for 1.0.0 or earlier.
--
",skriss,"
--
Tentatively moving this into v1.3 - at a minimum, we should try to do the design work.
--
",nrb,"
--
For anyone doing the design, consider differences between Velero-native snapshots and CSI volume snapshots
--
",onedr0p,"
--
I am just chiming in to say this would be great from a GitOps perspective, all I care about is backing up and restoring the PVCs to my deployments.
--
",dejwsz,"
--
I need it too! I think it is a must-have functionality. I described something similar here: https://github.com/vmware-tanzu/velero/issues/2598
My case relates to a restic daemon which is used to get volumes' content backup. What I need is to have an ability to restore old content of some volume leaving other project's objects in place. I think this is a quite common use case. I was not aware it is missing in Velero, I assumed it must be there but it turned out it isn't.
--
"
499,OPEN,Remove third_party/kubenetes/pkg/kubectl/cmd/util/shortcut_expander.go,P2 - Long-term important; kind/tech-debt,2020-04-27 15:38:38 +0000 UTC,ncdc,In progress,,"When we upgrade to the version of client-go that corresponds to k8s v1.11, we can remove our forked copy of shortcut_expander.go now that https://github.com/kubernetes/kubernetes/pull/63507 merged.",,,skriss,"
--
@ncdc I believe we can do this now?  Should just be a matter of changing https://github.com/heptio/ark/blob/master/pkg/discovery/helper.go#L104 to use https://github.com/kubernetes/client-go/blob/master/restmapper/shortcut.go#L40, correct?
--

--
Note that a naive swap here will result in a performance regression, most easily observable as a significant (30-60s) pause at the start of a restore operation. See https://github.com/vmware-tanzu/velero/blob/master/third_party/kubernetes/pkg/kubectl/cmd/util/shortcut_expander.go#L40-L42 for details on this.
--
",ncdc,"
--
@skriss yes, I think so, but I want to lock down #596 first
--
",,,,,,,,
487,OPEN,[Epic] support running multiple velero backups/restores concurrently,Enhancement/User; Epic; P1 - Important; Performance,2021-02-22 22:50:01 +0000 UTC,ncdc,In progress,,"We need to account for a few things:

1. If 2 server pods run simultaneously, they both might get the same Backup or Restore in a New state. They would both attempt to change the state to InProgress, they would likely both succeed, resulting in undesirable behavior.
1. If a Backup or Restore is InProgress and the server terminates for any reason (scaled down, crashes, terminates normally), we ideally need to be able to have the replacement server process pick up whatever was in progress instead of having it linger.",,,rosskukulinski,"
--
From a use-ability perspective, I think this is a particularly important issue for us to tackle.  There are a number of reasons why two Server pods might be running simultaneously and we need to handle that gracefully.

In addition, there's lots of ways an InProgress backup get stuck because a server exits.  Again, we need to handle this gracefully.  Metrics in #84 may be able to help (gauge displaying the current number of backups InProgress or Failed) visualize these issues, but it won't fix them.
--

--
This needs a quick test (from code) to trigger:

* get a backup thats new
* patch it to inprogress
* patch again to inprogress (this should fail, since already in progress)
--
",ncdc,"
--
A backup/restore won't get stuck during normal shutdown operations because the Ark server waits for all in-progress work to complete before terminating. If that work takes too long and exceeds the pod's deletion grace period, then Kubernetes would forcefully kill the container, and that would interrupt the in-progress work before it had a chance to finish.

There are, however, plenty of situations where the Ark server could exit while doing work:
- exceeding the grace period on a normal shutdown
- OOM killed
- a bug of some sort that causes a crash

This is definitely something we need to handle
--

--
I had a thought about how to implement this.

Each ark server process is assigned a unique identifier - the name of the pod (we can get the value using the downward API and pass it to `ark server` as a flag).

Each controller worker is also assigned a unique identifier.

When a new item (backup, restore) is processed by a controller, the first thing the controller attempts to do is set `status.arkServerID` and `status.workerID`. Assuming that succeeds without a conflict, the worker can proceed to do its work.

When a worker sees an InProgress item, it checks `status.arkServerID`
- If there are no running pods matching that name, the worker resets the status back to New for reprocessing.
- If there is a running pod matching that name, and it matches this ark server, reset the status to New if there are no active workers matching `status.workerID`

The controller would also need add event handlers for pods. Upon a change, we'd want to reevaluate all InProgress items to see if they need to be taken over.

There's probably a lot more to flesh out here, but I wanted to write this down before I forgot it. 
--
",xmath279,"
--
It would be interesting to either be able to limit the number of concurrent tasks or have the option to use it as it is right now (backup queue).

It would be better for me, because I would like to limit the load on my shared file servers (Ceph RBD / CephFS) while backups are being taken. This way, I can ensure that workloads are not impacted too much by the backup tasks.
--
",nrb,"
--
@xmath279 Thanks for that feedback!
--
",dsu,"
--
Will be done after design is finished - https://github.com/vmware-tanzu/velero/issues/2601
--
",,
474,OPEN,Add Generic Find/Replace Plugin for Restores,Enhancement/User; Needs Product; P2 - Long-term important,2020-12-08 01:52:37 +0000 UTC,jordanwilson230,In progress,,"First of all, this is an incredible tool.

Issue: When restoring a backup onto a new cluster using e.g.,```--namespace-mappings staging:staging-test``` or ```--namespace-mappings staging:staging```,  any ingress or LBs that are deployed will overwrite dns entries that are currently in place for the cloned cluster. This isn't necessarily undesirable (i.e., for disaster recovery), but it prevents ark from being used easily for cloning into new test environments.  Is it possible to search and replace these resource specific fields and replace them with a value specified with e.g., --hosted-zone=test.example.com or --hosted-zone=staging-test.example.com?

I tried downloading the backup locally, unpacking it, and running sed replacements, but there were issues after restoring from the raw json via kubectl. I can't remember what the issues were, but I suspect it had to do with the ordering in which resources were created (I was pretty lazy in issuing a kubectl apply -f on all directories).

This may be something that cannot be easily resolved, but I thought it still worth asking.  If not possible, the best method might be to exclude those resources and manually create them after a restore...I will test that as well.",,,ncdc,"
--
@jordanwilson230 could you please give us a bit more information about what specific fields need to have their values changed? An example with yaml/json would be quite helpful. Thank you!
--

--
@jordanwilson230 ok this is much clearer now, thanks. Would something like a generic find/replace plugin work for you, where you could configure it somewhat like this?
```
changes:
- resource: ingresses
  field: spec.rules[].host
  find: *.kubernetes.bitbrew.com
  replace: xyz...
```
--

--
This is a hypothetical configuration for a restore item action plugin that doesn't currently exist 😄.
--

--
We'll work with @bryanl & team to figure out the best way to implement this.
--

--
@jordanwilson230 we have not implemented this yet. We'd greatly appreciate your feedback as to how you think users should specify the transformations. This is something where the UI/UX really needs to be clear and easy.
--

--
I do think this can and should be a plugin. I'd like it to be generic enough that it meets the needs of as close to 100% of users as possible.

Do you anticipate that the same set of transformations would apply to every restore in a cluster? My guess is ""no"" but I'd love to hear the community's thoughts on this.

If the answer is ""no"", then we need a way to tell a restore which set of transformations to use. I think the easiest way to do this is probably with label selectors. We could store the transformation configurations as `configmaps`, and one of the pieces of data in each configuration would be a label selector to match against restores. The plugin would load all the transformation configmaps into memory, and then do selector matching against the labels on the restore.
--

--
We'd love to hear your thoughts on the structure of the configuration. In a previous comment, I suggested something like this:

```yaml
changes:
- resource: ingresses
  field: spec.rules[].host
  find: *.kubernetes.bitbrew.com
  replace: xyz...
```

We'd probably want to take advantage of Kubernetes api machinery so it would probably look more like:

```yaml
apiVersion: ark.heptio.com/v1
kind: RestoreTransformation
metadata:
  namespace: heptio-ark
  name: my-transformation-1
selector:
  matchLabels:
    color: green
changes:
- resource: ingresses
  field: spec.rules[].host
  find: *.kubernetes.bitbrew.com
  replace: xyz...
```

I'm primarily interested in brainstorming on the `changes` section:
- How do we specify the name of the field to transform
    - What if there's an array in the path?
    - What if there's a map in the path?
    - Where do we allow wildcard matching?
- Should `find` be optional? (my vote is yes)
- etc
--

--
In the same namespace as ark

On Wed, Oct 31, 2018 at 6:43 AM neith00 <notifications@github.com> wrote:

> In my understanding you are proposing to store transformation in
> configmaps, but I don""t see how it could operate. You want to store
> configmaps with transformation in the same namespace as ark or in the
> destination namespace?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/heptio/ark/issues/474#issuecomment-434640774>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AAABYiTFZkwI53CJxojq2e52LFWwHBkdks5uqX7kgaJpZM4TxR2N>
> .
>

--

--
If you want to match a specific ingress & its domain name and make a change, you would need to use find. Perhaps what I wrote above was confusing when I said find should be optional?
--

--
cc @bryanl - would love to get your input & we should chat about this soon-ish
--
",jordanwilson230,"
--
@ncdc No problem, and thanks for looking at it.  Here is an example for an ingress we're using:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: management-ingress
  annotations:
    kubernetes.io/ingress.allow-http: ""false""
spec:
  tls:
    - secretName: kubernetes.bitbrew.com
  rules:
  **- host: api-service-staging.kubernetes.bitbrew.com**
    http:
      paths:
      - path: /v1/*
        backend:
          serviceName: management-svc
          servicePort: 80
  **- host: api-management-staging.hub.bitbrew.com**
    http:
      paths:
      - path: /v1/*
        backend:
          serviceName: management-svc
          servicePort: 80
```

Here is an example of a LB we're using:
```
---
apiVersion: v1
kind: Service
metadata:
  name: rabbitmq-lb
  annotations:
    **external-dns.alpha.kubernetes.io/hostname: staging-rabbitmq.kubernetes.bitbrew.com.**
spec:
  type: LoadBalancer
  ports:
  - name: client-port
    protocol: TCP
    port: 4443
    targetPort: 5672
  - name: srmqp-mgmt-port
    protocol: TCP
    port: 15671
    targetPort: 15671
  - name: sclient-port
    protocol: TCP
    port: 5671
    targetPort: 5671
  selector:
    app: rabbitmq
```
Note that for the last example, we're using an annotation for external-dns to pick up which of course is different than the ingress.  It'd be great to have, but I don't expect support for anything outside of the core K8s resources :)

Ideally, something such as ```--hosted-zone=test.kubernetes.bitbrew.com``` would replace (in the above example) all dns hostname fields i.e., staging-api.kubernetes.bitbrew.com with staging-api.test.kubernetes.bitbrew.com. This way, the cluster we restore into will not overwrite the DNS records for the cluster we've cloned from (which is still running).  Of course, this assumes that the hosted zone, test.kubernetes.bitbrew.com has been created ahead of time by us, which is fine.


--

--
@ncdc Yes, that would work, but how that example you gave applied?  I've taken a stab at building from source (working with collections in the service-restore.go file to do a replace), and I've looked at the plugin examples, but I'm not sure where that falls in. Also, I've never looked at Golang until now, so I'm not so proficient. Is the snippet you provided part of a k8 merge definition or something?  Thanks again.
--

--
Right now I'm using a pretty nasty looking sed command as part of the script I'm using. ...After downloading a backup:
```
    tar xzf ${BACKUP}-data.tar.gz
    rm ${BACKUP}-data.tar.gz
# Search for DNS/hostnames in the backup and prefix with user specified value (e.g., test to create test.kubernetes.bitbrew.com)
    read -p ""Enter a name to prefix to kubernetes.bitbrew.com and hub.bitbrew.com (i.e., test): "" hosted_zone
    files=($(grep -lrie '.bitbrew.com' ./resources ))
    for file in ${files[@]} ; do cat $file | jq '.' | sed -e 's|last-applied-configuration"":\(.*\)\\n""\(.*\)|last-applied-configuration"":""{\\""\\""}""\2|g; s|\(.*\)\.\(.*\).bitbrew.com\(.*\)|\1.'${hosted_zone}'.\2.bitbrew.com\3|g' | jq -r '. | @json' > ${file}.copy ; done
    for file in ${files[@]}; do mv ${file}.copy ${file} ; done
    tar czf ${BACKUP}.tar.gz resources
    gsutil cp ${BACKUP}.tar.gz gs://${BUCKET}/${BUCKET}/
```
It seems to work, but I certainly prefer efficiency and simplicity!
--

--
Any update on this? We're at the point of deciding whether to implement an internal backup/restore process for spinning up test environments, but would greatly prefer using ark and the features that come with it!  Thanks!
--

--
Perhaps this functionality would a good fit at the plugin level?   If it would be possible to provide examples specific for transforming (perhaps an example for each resource type)? 

This plugin, for example: https://github.com/heptio/ark-plugin-example/blob/de9801def1466e73a72a62dd5c1a71dc479117b2/ark-restoreitemaction-log-and-annotate/myrestoreplugin.go#L45 shows how to add an annotation via k8s.io/apimachinery/pkg/api/meta and k8s.io/apimachinery/pkg/runtime.  For those of us that don't know GO or are inexperienced with k8s api, it would be awesome if you guys were able to add other examples:

- Rather than adding an annotation, searching for and replacing an annotation
- Same as above, but an example outside meta...i.e., altering something at the spec.container level. Perhaps changing the value of some environment variable that was set at deployment, such as PRODUCTION: ""false"" instead of PRODUCTION: ""true"". 

Plugins might offer users more flexibility and in-house customization without adding complexity to ark core.  In that case, perhaps all that is needed are some code examples for search/replacing in https://github.com/heptio/ark-plugin-example

@ncdc, @rosskukulinski, what are your thoughts on using plugins for this? 
--

--
@ncdc I think pinning the transformations to labels is a great idea.  I too would like to hear from others' thoughts @Evesy @neith00 

To answer your question, you're right; for it to be applicable to the larger community,  I think users will eventually find a need to apply different sets of transformation (as you mentioned, for example, via config).  

Im starting to read more on the basics of Golang in an effort to help, but if anyone at Heptio eventually has the time for this feature, that'd be awesome! Love your guys' work.
--
",neith00,"
--
I have the same use case.
2 clusters: 1 for Dev, 1 for Prod. 2 different domains for ingresses.
Would like to clone env from Prod cluster to Dev cluster for debugging purpose for example.
The ideal case would be ark modifying ingresses domain on the fly when restoring.
Could be applicable to storageClass the day it support it.
It's not our primary way to deploy resources on k8s of course. We usually do it through our CI/CD pipeline.
I was wondering if there are any way to use ksonnet engine to do this in ark.
--

--
In my understanding you are proposing to store transformation in configmaps, but I don""t see how it could operate. You want to store configmaps with transformation in the same namespace as ark or in the destination namespace?
--

--
how would you treat the case where you only want to change the domain name in an ingress with your example without using find?
--

--
got it you meant the field find is optionnal. now I get it
--

--
@ncdc I think https://github.com/google/kasane could be useful for replacing complex values
--
",rosskukulinski,"
--
This is a great customer use-case for resources that are typically tied to a specific environment.  External-dns as described here as one example, I would think another might be kube-lego/cert-manager are other workloads that are highly dependent on fields like these.

+1 for unlocking new use cases!

Suggest renaming title of issue to `Add Generic Find/Replace plugin for restores`
--
",Evesy,"
--
A generic find/replace plugin would be great for us too. Thinking about certain applications deployed that query GCP resources, these often reference specific zones/regions.

In the event of a region outage, we may choose to deploy a cluster (and other GCP resources) into another region. Any application that has this region set via config will now be incorrect; a find/replace would allow us to use common arguments/environment variables for these settings, and replace them to the correct values on a restore
--
",jrnt30,"
--
If you squint, this is somewhat similar to the ""general"" problem we have of customizing manifests for deployment in the first place.  Having the ability to compose those transformations via Plug-Ins would be very powerful indeed. 

@neith00 's suggestion of Kasane was very similar to where my mind was going with this problem with something like https://github.com/kubernetes-sigs/kustomize

There are several projects that attempt to support patching/overlay of the resources and provide some of the ""Hard Work"" around this work.  
--
"
469,OPEN,RFE: option to delete & recreate objects that already exist when restoring,Enhancement/User; Needs Product,2020-12-08 01:52:37 +0000 UTC,gianrubio,In progress,,"I set up ark 0.8.1 to make backups of my cluster, after that I was testing the restore just to make sure that ark restore will work. I got some warning and errors so I'm wondering if they are expected or I'm doing something wrong. 

This is a warning, not sure why it's failing? I'd expect to ark replace this resource even if the resource already exist, maybe a ark flag to force restore could solve this issue.
``` 
kube-system:  not restored: configmaps ""cert-manager-controller"" already exists and is different from backed up version.
```

This is an error: 
```
error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-scheduler-ip-10-50-84-142.eu-west-1.compute.internal.json: Pod ""kube-scheduler-ip-10-50-84-142.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""d5ec5961f20e838394c13c9314b9d39d"": must set spec.nodeName if mirror pod annotation is set
```

## Full ark restore output

```
Giancarlos-MBPro:.ssh grubio$ ark restore describe logging-multiple-hostnames-20180501104707
Name:         logging-multiple-hostnames-20180501104707
Namespace:    heptio-ark
Labels:       <none>
Annotations:  <none>

Backup:  logging-multiple-hostnames

Namespaces:
  Included:  *
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io
  Cluster-scoped:  auto

Namespace mappings:  <none>

Label selector:  <none>

Restore PVs:  auto

Phase:  Completed

Validation errors:  <none>

Warnings:
  Ark:        <none>
  Cluster:  not restored: persistentvolumes ""pvc-138f24f1-431c-11e8-ac10-02c73bc75f00"" already exists and is different from backed up version.
            not restored: persistentvolumes ""pvc-13b0f8f2-431c-11e8-ac10-02c73bc75f00"" already exists and is different from backed up version.
            not restored: persistentvolumes ""pvc-13d14da2-431c-11e8-ac10-02c73bc75f00"" already exists and is different from backed up version.
            not restored: persistentvolumes ""pvc-13f6562d-431c-11e8-ac10-02c73bc75f00"" already exists and is different from backed up version.
            not restored: persistentvolumes ""pvc-37a6990b-430e-11e8-ac10-02c73bc75f00"" already exists and is different from backed up version.
            not restored: persistentvolumes ""pvc-37c27b62-430e-11e8-ac10-02c73bc75f00"" already exists and is different from backed up version.
            not restored: persistentvolumes ""pvc-37c9b935-430e-11e8-ac10-02c73bc75f00"" already exists and is different from backed up version.
            not restored: persistentvolumes ""pvc-6c54e367-430e-11e8-ac10-02c73bc75f00"" already exists and is different from backed up version.
  Namespaces:
    default:      not restored: services ""kubernetes"" already exists and is different from backed up version.
    ingress:      not restored: configmaps ""intern-intern"" already exists and is different from backed up version.
                  not restored: services ""ingress-nginx-ingress-intern-controller-metrics"" already exists and is different from backed up version.
                  not restored: services ""ingress-nginx-ingress-intern-controller-stats"" already exists and is different from backed up version.
                  not restored: services ""ingress-nginx-ingress-intern-controller"" already exists and is different from backed up version.
                  not restored: services ""ingress-nginx-ingress-intern-default-backend"" already exists and is different from backed up version.
                  not restored: services ""ingress-oauth-proxy"" already exists and is different from backed up version.
    kube-system:  not restored: configmaps ""cert-manager-controller"" already exists and is different from backed up version.
                  not restored: configmaps ""ingress-shim-controller"" already exists and is different from backed up version.
                  not restored: configmaps ""monitoring.v69"" already exists and is different from backed up version.
                  not restored: endpoints ""kube-controller-manager"" already exists and is different from backed up version.
                  not restored: endpoints ""kube-scheduler"" already exists and is different from backed up version.
                  not restored: jobs.batch ""kube-system-cert-manager-cronjob-1524473820"" already exists and is different from backed up version.
                  not restored: jobs.batch ""kube-system-cert-manager-cronjob-1524473880"" already exists and is different from backed up version.
                  not restored: jobs.batch ""kube-system-cert-manager-cronjob-1524473940"" already exists and is different from backed up version.
                  not restored: jobs.batch ""kube-system-cert-manager-cronjob-1524488340"" already exists and is different from backed up version.
                  not restored: jobs.batch ""kube-system-cert-manager-job"" already exists and is different from backed up version.
                  not restored: services ""heapster"" already exists and is different from backed up version.
                  not restored: services ""kube-dns"" already exists and is different from backed up version.
                  not restored: services ""kube-system-kubernetes-dashboard"" already exists and is different from backed up version.
                  not restored: services ""tiller-deploy"" already exists and is different from backed up version.
    logging:      not restored: configmaps ""intern-logging-intern-logging"" already exists and is different from backed up version.
                  not restored: services ""cerebro-logging-cluster"" already exists and is different from backed up version.
                  not restored: services ""elasticsearch-discovery-logging-cluster"" already exists and is different from backed up version.
                  not restored: services ""elasticsearch-logging-cluster"" already exists and is different from backed up version.
                  not restored: services ""es-data-svc-logging-cluster"" already exists and is different from backed up version.
                  not restored: services ""kibana-logging-cluster"" already exists and is different from backed up version.
                  not restored: services ""logging-nginx-ingressintern-controller-metrics"" already exists and is different from backed up version.
                  not restored: services ""logging-nginx-ingressintern-controller-stats"" already exists and is different from backed up version.
                  not restored: services ""logging-nginx-ingressintern-controller"" already exists and is different from backed up version.
                  not restored: services ""logging-nginx-ingressintern-default-backend"" already exists and is different from backed up version.
    monitoring:   not restored: configmaps ""monitoring-kube-prometheus"" already exists and is different from backed up version.
                  not restored: endpoints ""alertmanager-operated"" already exists and is different from backed up version.
                  not restored: endpoints ""prometheus-operated"" already exists and is different from backed up version.
                  not restored: services ""monitoring-prometheus-pushgateway"" already exists and is different from backed up version.

Errors:
  Ark:        <none>
  Cluster:    <none>
  Namespaces:
    kube-system:  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/etcd-server-events-ip-10-50-105-102.eu-west-1.compute.internal.json: Pod ""etcd-server-events-ip-10-50-105-102.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""69f1831d34b8a772e16fe4b53dfde156"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/etcd-server-events-ip-10-50-79-139.eu-west-1.compute.internal.json: Pod ""etcd-server-events-ip-10-50-79-139.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""2f971a1dcd6eb045c364011a4cd3eb0b"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/etcd-server-events-ip-10-50-84-142.eu-west-1.compute.internal.json: Pod ""etcd-server-events-ip-10-50-84-142.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""a78c3a37fa41e2979affd20e9b8e0111"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/etcd-server-ip-10-50-105-102.eu-west-1.compute.internal.json: Pod ""etcd-server-ip-10-50-105-102.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""1e7be17cb58e298472eb0bcf5529d4ca"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/etcd-server-ip-10-50-79-139.eu-west-1.compute.internal.json: Pod ""etcd-server-ip-10-50-79-139.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""7b2a70d4cf5b688ab13ddbe564ef527e"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/etcd-server-ip-10-50-84-142.eu-west-1.compute.internal.json: Pod ""etcd-server-ip-10-50-84-142.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""0e92292cb0f619d5a229297600d7bb97"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-apiserver-ip-10-50-105-102.eu-west-1.compute.internal.json: Pod ""kube-apiserver-ip-10-50-105-102.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""d454a354dcb2cb12783fa49f2386b6ba"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-apiserver-ip-10-50-79-139.eu-west-1.compute.internal.json: Pod ""kube-apiserver-ip-10-50-79-139.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""d454a354dcb2cb12783fa49f2386b6ba"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-apiserver-ip-10-50-84-142.eu-west-1.compute.internal.json: Pod ""kube-apiserver-ip-10-50-84-142.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""d454a354dcb2cb12783fa49f2386b6ba"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-controller-manager-ip-10-50-105-102.eu-west-1.compute.internal.json: Pod ""kube-controller-manager-ip-10-50-105-102.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""1526b1178ede071d84be82486333151e"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-controller-manager-ip-10-50-79-139.eu-west-1.compute.internal.json: Pod ""kube-controller-manager-ip-10-50-79-139.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""1526b1178ede071d84be82486333151e"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-controller-manager-ip-10-50-84-142.eu-west-1.compute.internal.json: Pod ""kube-controller-manager-ip-10-50-84-142.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""1526b1178ede071d84be82486333151e"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-proxy-ip-10-50-103-41.eu-west-1.compute.internal.json: Pod ""kube-proxy-ip-10-50-103-41.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""5963c325107b331ab635aad75b94927b"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-proxy-ip-10-50-105-102.eu-west-1.compute.internal.json: Pod ""kube-proxy-ip-10-50-105-102.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""bac2cc1636847764a0815d26720c8cd7"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-proxy-ip-10-50-107-213.eu-west-1.compute.internal.json: Pod ""kube-proxy-ip-10-50-107-213.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""377aa0ca81598973093dac679d794bba"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-proxy-ip-10-50-68-173.eu-west-1.compute.internal.json: Pod ""kube-proxy-ip-10-50-68-173.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""4b96cd34114ce182fb895b5851df1076"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-proxy-ip-10-50-71-34.eu-west-1.compute.internal.json: Pod ""kube-proxy-ip-10-50-71-34.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""f97f3000e965824d1fbf2f5e271c5dcb"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-proxy-ip-10-50-79-139.eu-west-1.compute.internal.json: Pod ""kube-proxy-ip-10-50-79-139.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""5092b3704cad1cae1ba58baa1f89c044"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-proxy-ip-10-50-81-61.eu-west-1.compute.internal.json: Pod ""kube-proxy-ip-10-50-81-61.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""41e604c2a05ff59d4ca71eae2650b77b"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-proxy-ip-10-50-82-127.eu-west-1.compute.internal.json: Pod ""kube-proxy-ip-10-50-82-127.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""8ad729bc65359d65c67211a9c8cad910"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-proxy-ip-10-50-84-142.eu-west-1.compute.internal.json: Pod ""kube-proxy-ip-10-50-84-142.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""92a7e3e865f9d8fefcc21e84377b4f40"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-scheduler-ip-10-50-105-102.eu-west-1.compute.internal.json: Pod ""kube-scheduler-ip-10-50-105-102.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""d5ec5961f20e838394c13c9314b9d39d"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-scheduler-ip-10-50-79-139.eu-west-1.compute.internal.json: Pod ""kube-scheduler-ip-10-50-79-139.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""d5ec5961f20e838394c13c9314b9d39d"": must set spec.nodeName if mirror pod annotation is set
                  error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-scheduler-ip-10-50-84-142.eu-west-1.compute.internal.json: Pod ""kube-scheduler-ip-10-50-84-142.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""d5ec5961f20e838394c13c9314b9d39d"": must set spec.nodeName if mirror pod annotation is set
Giancarlos-MBPro:.ssh grubio$ 
```",,,ncdc,"
--
Hi @gianrubio 

> kube-system:  not restored: configmaps ""cert-manager-controller"" already exists and is different from backed up version.

This type of message is a warning and it indicates that there is an item with the same name that already exists in the cluster. Ark examined the backed up copy and compared it to the in-cluster copy, and there were differences, so Ark records a warning so you're aware that it wasn't able to restore the item.

> error restoring /tmp/318897152/resources/pods/namespaces/kube-system/kube-scheduler-ip-10-50-84-142.eu-west-1.compute.internal.json: Pod ""kube-scheduler-ip-10-50-84-142.eu-west-1.compute.internal"" is invalid: metadata.annotations[kubernetes.io/config.mirror]: Invalid value: ""d5ec5961f20e838394c13c9314b9d39d"": must set spec.nodeName if mirror pod annotation is set

This is #428 

--

--
> Does it make sense to ask not restore the object even if it’s not the same?

I'm not sure what you mean?

> How does ark compare the object?

Ark clears out fields that would differ such as `.metadata.uid` and then checks for equality using `reflect.DeepEqual()`.
--

--
@gianrubio for the warnings such as `kube-system: not restored: configmaps ""cert-manager-controller"" already exists and is different from backed up version.`, do you believe the items are identical and that Ark is not comparing them correctly?

Is there anything else you need for this issue, or would it be ok to close it?
--

--
I think we'd need to provide a control for that behavior and let the user doing the restore decide if Ark should delete & recreate or no-op.
--

--
Should we repurpose this issue as ""RFE: option to delete & recreate objects that already exist when restoring""?
--

--
cc @jbeda 
--

--
I'm thinking maybe something like `--conflict-strategy` with options `replace` (delete what's in the cluster and create what's in the backup), `preserve` (keep what's in the cluster and record a warning as we're doing now). (All names TBD)
--

--
Yes, the flow would be
1. Try to create the object
1. If it failed because the item of the same name already exists
    1. If the item in the backup and the item in the cluster are the same, no-op
    1. Otherwise, check the conflict strategy and proceed with delete/create or logging a warning

I also don't think we'd ever want to delete a PV or PVC. We have another issue open for cloning preexisting PVs into a cluster (#192). We'll need to make sure we special case things like PVs/PVCs here.
--

--
@rosskukulinski can we talk about this soon?
--

--
Sounds good. 
--
",gianrubio,"
--
> This type of message is a warning and it indicates that there is an item with the same name that already exists in the cluster. Ark examined the backed up copy and compared it to the in-cluster copy, and there were differences, so Ark records a warning so you're aware that it wasn't able to restore the item.

Does it make sense to ask not restore the object even if it’s not the same? How does ark compare the object?
--

--
> @gianrubio for the warnings such as kube-system: not restored: configmaps ""cert-manager-controller"" already exists and is different from backed up version., do you believe the items are identical and that Ark is not comparing them correctly?

The items are probably not equal but I'd expect ark to replace them.
--

--
Yes, that was my point, maybe a flag like `--force` could solve this behaviour, WDYT?
--

--
The proposal sounds good, I have only one thought. Deleting the object before applying will rescheduled all the object, doing it in big cluster can cause issues. I'd rather delete objects that have failed to apply the changes, big warning on deleting volumes and PVCs
--
",rosskukulinski,"
--
User story:

`As a cluster operator, I want to use Ark as a mechanism to keep two clusters in sync.  This might be Prod A and Prod B, or alternatively every night mirror Production to Staging so that we have a fresh environment for testing/staging.`

For stateless apps, this sounds like a healthy feature for us to add.  I agree with Andy that we _probably_ don't want to delete PV/PVC by default.

That said, if the use-case is mirroring Production to Staging, I don't want to keep around my old staging PV/PVCs.  Perhaps we need another CLI flag for PV/PVC specifically?  `--conflict-strategy-volumes`?
--

--
@heptio/ark-team I'd like to propose resolving this as part of v0.11.0.  We should discuss during the v0.11.0 planning meeting what might have to get pushed back to let this in.

Adding Needs Product label.
--

--
@ncdc sure! Maybe Tuesday?
--

--
@skriss I spoke with our CRE team and this issue is not a customer priority. Feel free to push out of v0.11 - and maybe the P1 label can come off
--
",skriss,"
--
@rosskukulinski I know we've gone back and forth on this a number of times. Is this actually a priority to solve and something we need to do as part of v0.11?
--

--
Thanks @rosskukulinski.  Moved to New Issues with a 1.x milestone and removed the P1 label.
--

--
@valentin-krasontovitsch yes - in the current state, the resources would need to not exist in your cluster before being restorable.  We are still interested in having the option to delete/recreate or update objects during restore to match up the restored state, but we haven't been able to get to working on it yet. And I'm afraid the documentation around this is probably less than clear.  If you have time, we'd love a PR to the docs to add some context! We'll keep this issue open for continuing to track the feature request. 
--

--
@archmangler yes, you could first manually delete the objects that you want to restore. It's possible you'd run into issues where you couldn't delete the PV/PVC because they were being used by a pod, though.
--

--
@debianmaster we don't have any kind of built-in diff, no. It would obviously be easiest if you could get away with deleting the entire namespace before restoring it, but that may not be feasible depending on your use case.

Perhaps you could use something like https://github.com/weaveworks/kubediff to diff the contents of the backup tarball vs. the in-cluster config?
--

--
Some complications to think through:
- pods/other objects owned by controllers (they'll be recreated if we delete them in prep for a restore)
- PVCs/PVs that are in use by a pod (deletes will be disallowed due to the PVC/PV in-use protection finalizer)
--
",gidesh,"
--
Would be really great if this feature comes in v0.11.0. We actually wanted the resource to applied from backup during restore if it is already present in the cluster.
--
",valentin,"
--
We've run into this issue as well - we expected that velero would restore a pod to backup state, even it the pod still exists. Do I gather correctly that when one wants to restore a pod to backed up state, one has to delete the pod in question (or deployment, if there is one managing the pod) first, and then do a restore?

in case that this is already documented - would you point me to the docs please? If it's not documented - it wasn't obvious at least for us... Many examples one finds online simulate disaster by deleting the example namespace and restoring afterwards, but we did not expect that deletion is actually necessary.
--
"
448,OPEN,Restore dry run option,Enhancement/User; P2 - Long-term important,2020-12-08 01:52:37 +0000 UTC,rocketraman,Opened,,"A `dry-run` option to `ark restore create` would be awesome. Given that we don't have any easy visibility as to what's inside a backup (as per issue #396), doing a partial ark restore is a bit of a hair-raising / fingers crossed event, because the right values we need to pass to `--include-resources` and `--exclude-resources` are not at all obvious.

A dry-run option would list the actions ark would take / print the restore log given the parameters passed but not actually carry out those actions.",,,rosskukulinski,"
--
Yes! I think this is a critical user experience feature enhancement.  This should probably be in-place before we go to implement #469.
--

--
I'd like to propose having a backup AND restore `--dry-run` that will capture what resources are to be captured/snapshotted.  Having the backup dry-run would help in situations like #579 where the user isn't sure what will be backed up.
--
",ncdc,"
--
We need to figure out the technical means to achieve this. Ark is not an aggregated API server, so we don't have the ability for the `ark` client to send an http request to the Ark pod and wait for a response. What we've done so far for ""request-response"" semantics is to have the `ark` client create a custom resource and wait for it to be updated with the response information.

There is a challenge to that: etcd has a 1.5MB entry size limit, so we can't stuff the response into a single custom resource because we might exceed the limit.

We could upload the response to object storage, then give the client a pre-signed URL it can use to download the response (this is what we do for `ark backup download`, `ark backup logs`, and `ark restore logs`).

We could create a series of small custom resources (like Kubernetes events). I worry, though, that that might overwhelm the apiserver at some point.
--

--
Including info from 654: make sure we allow plugins to know if this is a dry-run or real.

--

--
Hi @jprecuch, I don't work on Velero any more, but @carlisia @nrb @ashish-amarnath should be able to help you
--
",jprecuch,"
--
@ncdc is there something for this ongoing or was this put on hold for now? 
--
",,,,,,
434,OPEN,Encrypt backups at rest,Area/Cloud/GCP; Breaking change; Enhancement/User; Needs Product; Security,2021-02-02 15:09:39 +0000 UTC,marpaia,In progress,,"Right now, when backups are created via `ark backup create`, sensitive objects are stored unencrypted at rest. In Google Cloud, there is excellent Go support for encrypting Google Cloud Storage objects with [Google Cloud KMS](https://cloud.google.com/kms/docs/encrypt-decrypt#kms-howto-encrypt-go) in a way that [works rather transparently to the caller](https://godoc.org/cloud.google.com/go/storage#ObjectHandle.Key).

I would love a way to configure the KMS key to use when storing backups. The best practice is to have a separate project for the key and grant IAM permissions from there, which I could easily do for the `heptio-ark` SA that already is required when setting up access to the bucket.

At @kolide, we have an internal tool that works like that which I'd like to potentially replace with Ark, so if there is a reasonable integration point within Ark for this kind of thing, perhaps some of our existing code can be upstreamed for this use-case.",,,ncdc,"
--
@marpaia thanks for this request. Could you please explain in a bit more detail what the flow looks like between Ark and the separate project that contains the key? Would you specify the project and the key in the `backupStorageProvider` config, and would Ark retrieve the key from the specified project?
--

--
Agreed, @mattmoyer let us know if you have some time to discuss
--

--
@erasmus74 thanks for your idea! @mattmoyer WDYT?
--

--
@erasmus74 are you describing using kubeseal to seal the entire backup tarball, or just the secrets contained in the tarball?
--
",marpaia,"
--
Encrypting backups at rest is for sure not something that is unique to GCP, but in GCP you need three bits of information to encrypt/decrypt a blob:

- project which contains the KMS key-ring
- name of the key-ring
- name of the key (each key-ring can have multiple keys)

I think Ark is already 1:1 tied with a GCP Project, but it's worth noting that GCP has a [Separation of Duties](https://cloud.google.com/kms/docs/separation-of-duties) document which outlines the best practice of storing your KMS key-ring in an isolated project.

The code that I have now is an implementation of the following interface for storing a `*corev1.Secret` in GCS encrypted at rest via KMS:

```go
package secret

// Store is the interface which defines the controllers interactions with an
// arbitrary exo-cluster secret storage mechanism.
type Store interface {
	Get(ctx context.Context, namespace string, name string) (*corev1.Secret, error)
	List(ctx context.Context, namespace string) ([]*corev1.Secret, error)
	Put(ctx context.Context, s *corev1.Secret) error
	Delete(ctx context.Context, namespace, name string) error
}
```

I don't think my implementation would be super useful to you because you can current only use the KMS API to encrypt/decrypt chunks of data which are 64KiB. From [the docs](https://cloud.google.com/kms/docs/secret-management):

> Cloud KMS can handle secrets up to 64 KiB in size. If you need to encrypt larger secrets, it is recommended that you use a key hierarchy, with a locally-generated data encryption key (DEK) to encrypt the secret, and a key encryption key (KEK) in Cloud KMS to encrypt the DEK. To learn more about DEKs, see [Envelope Encryption](https://cloud.google.com/kms/docs/envelope-encryption).

This envelope encryption song and dance is kind of annoying. KMS has one job IMO: decrypt/encrypt my damn stuff. Anyway, we avoid it entirely by just implementing an interface like the above. This allows us to just deals with individual secrets, so they're all smaller than 64KiB in our environment.

Let me know if some of our KMS snippets would be helpful and I can share them privately @ncdc.
--

--
It's also worth noting that [the link I posted above](https://godoc.org/cloud.google.com/go/storage#ObjectHandle.Key) also allows you to encrypt an object with a single 32-byte AES-256 key. Rather than using envelope encryption to encrypt the tar, it would probably be easiest to use KMS to encrypt the key (ala-DEK) and store that in GCS as well. Decrypt just the key via KMS and use this API to encrypt the entire tar with the one key. The key rotation and access control story is not as good with this solution, but it's a much simpler solution in general.
--
",rosskukulinski,"
--
It might be get to get @mattmoyer's thoughts on this as I don't fully understand the chain of trust using KMS and the impact on Ark & security.  How does something like [vault](https://github.com/hashicorp/vault) or [sealed-secrets](https://github.com/bitnami-labs/sealed-secrets) play into this?

We should also look at the other major cloud providers (and any bare-metal equivalents) to make sure this feature will work across a variety of platforms.
--

--
There was also a suggestion from #ark-dr to use https://github.com/mozilla/sops/.
--
",erasmus74,"
--
Hows this? Allow using kubeseal for secrets, just add optional params, '--kubeseal' which now requires '--secret' '--controller' etc. The tool then encrypts the backup using a generated key as a secret, secret is sealed, exists as 'secret-name' then for restore, it would only need to use that same secret for decryption.

Might need fineseing but I currently have to install components on the pod and do Etcdctl snapshot so it'd be awesome to have it running as a k8s batch job.

I'll help if I can, not a go programmer YET.
--
",kzap,"
--
Do we want to encrypt everything or just the secrets? if just secrets then wouldnt running encryption at rest suffice?
--

--
Thank you for clarifying, is there a way to encrypt the full backup before storing in in Storage? Can restic take care of this part for us?
--
",prydonius,"
--
@kzap Velero uses the Kubernetes API server to backup resources, instead of backing up from etcd directly. This means that even if encryption at rest is enabled, Velero will backup the plaintext Secret because it is decrypted by the Kubernetes API server.

From the Velero perspective, I imagine it would be easiest to encrypt the full backup.

In the meantime, you could use something like [sealed-secrets](https://github.com/bitnami-labs/sealed-secrets) and only backup the encrypted SealedSecret resources instead of Secrets.
--

--
Unfortunately not really, the backups with restic are encrypted with a static key (see https://github.com/vmware-tanzu/velero/issues/1053). Even if that work is done though, restic is only used to store volume snapshots, so the resources that were backed up also needs to be encrypted and stored.
--
"
317,OPEN,Conditional PV backup opt-in.,Breaking change; Enhancement/User; Needs Product; P2 - Long-term important; Volumes,2020-10-22 15:44:35 +0000 UTC,jacobstr,Opened,,"### Feature Suggestion

The thought has come up that we may want users to opt-in to PV backups e.g. via a label or annotation. We advise folks to use PV's in general to guarantee disk space for things such as local scratch space. We do a lot of image processing, which results in large, fungible cache PV's.

Host disk is currently unbounded by resource limits ala cpu/memory. The effort to improve this is under way in https://github.com/kubernetes/community/pull/306. 

 
",,,ncdc,"
--
@jbeda would appreciate any input you might have on this one
--

--
@rosskukulinski I don't think the existing selector flag sounds sufficient for this use case. It sounds like there needs to be an additional flag that selects just PVCs/PVs.
--

--
@rosskukulinski another data point - let's say you have the following items:
1. pod `mysql`, labeled `app=mysql`, using PVC `data`
1. pvc `data`, labeled `backup=false`, referencing PV `pvc-23423-23-2342-34-23`
1. PV `pvc-23423-23-2342-34-23`, dynamically provisioned, no labels

If you do `ark backup create --selector app=mysql`, Ark:
1. Uses the label selector and gets pod `mysql`
1. Runs backup item actions for the pod, specifically `podAction`, which sees the pod has the PVC `data` and tells Ark to make sure to back up that PVC
1. Ark immediately processes the PVC `data`, runs backup item actions for it, specifically `backupPVAction`, which finds the PV `pvc-23423-23-2342-34-23` associated with the PVC and tells Ark to back it up
1. Ark immediately processes PV `pvc-23423-23-2342-34-23` and backs it up (snapshot or restic)

This is our approach to making sure that if you use a label selector to back up things (such as pods), we make sure to include the associated PVs. Our original documentation & example before we added this logic required the user to manually label the PVC (if needed) and PV so they'd be backed up. This is especially true for dynamically provisioned PVs.

xref #591 
--
",rosskukulinski,"
--
Hi @jacobstr - the use-case you're describing makes sense.  Do you have additional details around how PV/PVCs tend to be provisioned in your environments?  

I'm wondering whether the existing --selector flag (with or without [negation](https://github.com/heptio/ark/pull/501)) or maybe a [--ignore-selector](https://github.com/heptio/ark/issues/404) would be sufficient for these uses.

Teams could label their PVs with `backup: false` and then define a negative selector on your backup.
--
",skriss,"
--
xref #929 
--

--
@mithuns it is not currently prioritized. If you're interested in this feature, could you provide some more information on your use case? Thanks!
--
",stanislavb,"
--
I have a hard time excluding a PV auto-provisioned from a PVC. Label based filtering on pod and PVC level does not help since the PV does not inherit the label. Ideally for my use case, label based filtering of the PVC would affect whether the associated PV would be backed up or not, but looking at test code currently that is not a supported feature.
--
",karlkfi,"
--
We have some related use cases:
- Some jobs use PVCs for scratch disks that are larger than the node disk size and/or to isolate between jobs to allow higher pod per node destiny. These get deleted when the pod exist and never need to be backed up by Velero.
- Some jobs use PVs for caching that needs to be shared between consecutive or concurrent jobs. These caches are giant but can be recreated easily, so they also never need to be backed up by Velero.

I don't want Velero to spend tebibytes of backup space and hours of time backing up disks we don't need backed up, but we have plenty of other disks that do need backing up.
--
",mithuns,"
--
Building on the example scenario given by @ncdc above.
What can be done in the case when pvc needs to be backed-up but pv needs to be excluded ?
We have an operator that creates STS/PVC/PV but since PVs dont inherit labels from PVC we are not able to exclude PVs. 
Also, to have another operator that deals with PV only and applies labels to them directly, is also not the best solution because PV are cluster scoped , so we don't want to make an operator that has cluster scope.
Any suggestions would be welcome.

**Update**: I think I got the answer from here
#2151

--

--
Just out of curiosity , is this on some roadmap or planned yet ? 
--

--
So, as described above by @karlkfi  and @stanislavb , there are multiple cases where auto-provisioned pv is hard to exclude or be applied velero exclude label.
In my use-case , my application is just such in nature that the pv created is never really needed to be backed up, (the entire cluster state needs to be backedup in a timely way) 
Thus, I had to create a kubernetes operator just to look for pv creation and pvc updation events, so then pv (from certain pvc with certain labels on them) can be marked to be excluded from backups.

Ideally if there is a way to setup such rules in velero itself, that would be great. Kind of like a matrix (rule book, config whatever we wish to call it),
1) PV created from pvc with these labels will automatically be marked for exclusion.
2) PVC created from StatefulSets with these labels will automatically be marked for exclusion.
3) StatefulSets created from Custom Resources with these specific labels will automatically be marked for exclusion. 
I think velero already is running at a cluster scope (some might argue that velero shouldnt listen for more cluster-scoped or namespace-scoped events) , might be that can be a turn off/on feature with a flag ?

--
"
310,OPEN,Possible issue with multi-zone statefulsets,Bug; P3 - Wouldn't it be nice if...,2020-12-08 01:52:37 +0000 UTC,ncdc,Opened,,"1. Create a GKE cluster (1.8.7 was the reported version), make sure it spans 2 zones
1. Deploy a statefulset that has nodes in both zones, using pvc claim templates (e.g. https://github.com/kubernetes/charts/tree/master/stable/mongodb-replicaset)
1. Take a backup
1. Delete everything related to the statefulset
1. Restore (should work fine)
1. Restore again

At this point, we should probably hit #192. We have a report that 2 of the 3 volumes in the statefulset restored successfully, while the 3rd briefly showed up in the cloud console, then disappeared.
",,,unguiculus,"
--
The cluster is in `europe-west3-a` with `europe-west3-b` as additional zone. The statefulset had three volumes, two in `europe-west3-b`, one in `europe-west3-a`. The volumes in `europe-west3-b` always restored, that in `europe-west3-a` always failed to restore. It quickly showed up with an error in the GCP console and then disappeared again. See screenshot.

![image](https://user-images.githubusercontent.com/925412/36036070-23e236e0-0db9-11e8-8621-180d689f1c15.png)




--
",,,,,,,,,,
305,OPEN,When a backup or restore fails; provide the user information or instructions to find out root cause,Enhancement/User; Needs Product,2020-12-08 01:52:37 +0000 UTC,rdodev,Opened,,"Presently a backup can fail for a number reasons. If the user runs `ark backup describe` they will see it the backup failed, but no reason, logs or anything that would help them understand the problem and how to fix it:

```

[centos@ip ark]$ ./ark backup describe nginx-001
Name:         nginx-001
Namespace:    heptio-ark
Labels:       <none>
Annotations:  <none>

Namespaces:
  Included:  *
  Excluded:  <none>

Resources:
  Included:        *
  Excluded:        <none>
  Cluster-scoped:  auto

Label selector:  <none>

Snapshot PVs:  auto

TTL:  720h0m0s

Hooks:  <none>

Phase:  Failed

Backup Format Version:  1

Expiration:  2018-03-09 19:50:47 +0000 UTC

Validation errors:  <none>

Persistent Volumes: <none included>
```

Having a `Logs` section there or else instructions to find out what happen would be greatly helpful.",,,ncdc,"
--
Example error only seen in ark server pod log:

```
time=""2018-02-14T00:50:28Z"" level=error msg=""backup failed"" error=""rpc error: code = Unknown desc = error putting object mybackup/ark-backup.json: AccessDenied: Access Denied\n\tstatus code: 403, request id: 096E12AAC407F5B7, host id: ..../....="" key=heptio-ark/mybackup logSource=""pkg/controller/backup_controller.go:258""
```
--

--
@donbecker if you wouldn't mind, please join us in #ark-dr on the Kubernetes slack for real-time troubleshooting, or create a new issue. This issue is an RFE to provide more details to the user why a backup failed. Thanks!
--

--
@rosskukulinski the `status` field is currently supported; with k8s 1.11 we gain the ability to use the `/status` subresource in the http request.
--

--
FYI, events have a TTL and are automatically deleted after they expire. It's usually a pretty short amount of time - 1 to 2 hours by default, iirc. Also, each event is its own resource in etcd, so you would probably want to avoid having thousands of events for each backup/restore. Finally, the default client-side event broadcasting code that's in client-go has a ""spam filter"" to make sure that a single component + object target isn't overloading the system. The defaults are fairly low - I think it's something like 10 or 25 events in a minute, if you exceed the threshold, the events that you generated are silently dropped and it's really confusing trying to figure out why they're disappearing into thin air.
--
",donbecker,"
--
Have just set up ark and am getting this, using AWS and the nginx example (non PV). Have I overlooked troubleshooting steps?
--

--
`ark backup logs <backup name>`
--
",rosskukulinski,"
--
This is super important from a usability perspective.  While the ark server log can provide debugging information, it might be hard to hunt down, especially because users creating backups may not have access to the Ark server logs.

One possibility would be to leverage the`status` field in Backup CRD to reflect error/failure details (may require k8s 1.11 - https://github.com/heptio/ark/issues/529) or alternatively leverage Kubernetes Events API to track backup or restore failure events.  This is also likely related to backup progress: https://github.com/heptio/ark/issues/20
--

--
Product question: What are the the common errors/error states that we want to be able to resolve.

Sources that can help piece together what happened:
* per-backup / per-restore logs
* restores have warnings/errors file
* ark server log
* restic pod logs

Restores (Related: #286)
* per-restore log
* errors file
* warnings file
--
",carlisia,"
--
When we are helping users debug Velero, we often ask for the output from `describe` as well as the logs for the backup/restore. With the logs, it usually it's not completely helpful unless they can reproduce the failure after setting the log level to `debug`, which increases the amount of logging to sort thru.  And for backups stuck in ""InProgress"" or for more complicated cases, we have to dig thru the output of the entire Velero log.

One alternative to make debugging easier and faster, and to potentially address this request:

We know at every step of the way what activity the backup/restore is performing. We could keep a running list of these ""events"" and add them to the describe output, the way Kubernetes does. Knowing where in the process the failure occurred could itself be a hint for how to fix the issue, but otherwise it is a great starting point for where in the logs to start looking.

--

--
Thanks for the explanation. Yes, I have noticed that events go away, but didn't know why, this is helpful.

Maybe events is not what we need, sounds like overkill. Trying again, w/o using a vocabulary that has any k8s meaning: We have a limited number of 'steps"" that happen from running ""create"" until the backup reaches its final phase of ""Complete"". I submit that it would be helpful to list these steps in the describe output. And I think they are not so many that would be unreasonable. So, instead of ""Events"", we would have our own ""Steps"" section.
--
",,,,
286,OPEN,Consider consolidating per-restore logs into a single file? (Similarly for backups),Breaking change; Enhancement/Dev; Enhancement/User; P2 - Long-term important,2020-12-08 01:52:37 +0000 UTC,skriss,In progress,,"Right now, for restores, we log:
- validation errors into the restore API object (.status.validationErrors)
- info-level logs to <restore-name>-logs.gz in object storage, using logrus
- warnings/errors to <restore-name>-results.gz in object storage, as JSON

We follow a similar pattern for backups, except we don't have the third item (yet). 

I wonder whether it would make sense to consolidate this down into just a single per-backup/per-restore log file (e.g. <restore-name>-logs.gz). Since we're now using logrus, we have the ability to use differing log levels as well as other structured logging fields to differentiate between different types of log output. The main benefit that I see of consolidating is that a user now only needs to look in a single place for the full set of results from a backup or restore.  So, for a restore:


We could retain the view that we currently provide in `ark restore describe` by adding logic to that command to parse the log file and separate out the log statements into different sections within the command output.

Interested to see if other folks think this makes sense/is useful.",,,skriss,"
--
See #225 for some related discussion re: backup warnings/errors.
--

--
That is what I was thinking. We could add a unique log field to identify them as such if we want to retain the distinction between them and errors during/after execution.

I'm not 100% convinced this is the right idea yet, so certainly interested in alternate opinions.
--

--
^^ yeah, I was thinking along the same lines re: schema validation vs. logical validation.
--

--
Summarizing discussion from above, here's my straw-man proposal:

- Backups and Restores use the following phases: 
  - `New`: the object has not yet been processed by the Ark server
  - `Processing`: the object is currently being processed by the Ark server
  - `Processed`: the object has been processed by the Ark server and has reached a terminal state (e.g. for backups, everything that could be backed up was backed up and the tarball/etc have uploaded to object storage; for restores, everything that could be restored was restored and the log has been uploaded to object storage)
  - `Failed`: there was a fatal error preventing the object from reaching a normal terminal state (e.g. there was an error uploading the tarball/logs to object storage)
- All info/warn/error outputs for an individual backup/restore go to a single leveled per-item log that gets uploaded to object storage
- `WarningCount` and `ErrorCount` fields are added to the status for both Backups and Restores to store counts of the warnings/errors logged during the backup/restore; backups and restores can now be displayed in `ark backup get` / `ark restore get` as e.g. `Processed (2 warnings, 1 error)`
- `ark backup describe` and `ark restore describe` display warning/error counts by default.  If `--details` is specified, the per-item log is fetched from backup storage, and the warning/error lines are displayed as part of the output
- For both Backups and Restores, validation errors (i.e. `.status.validationErrors`) go away; any information currently reported as a validation error gets logged at `error` level to the per-item log file, and instead of using the `FailedValidation` phase, objects will be `Processed` with >0 errors.
- `FailureReason` field is added to the status for both Backups and Restores, to provide the user information about why their backup/restore is `Failed` rather than `Processed` (this is particularly useful for when the log fails to upload, and the only other place that errors can be logged currently is to the server log)

All comments welcome -- just wanted to get a draft proposal out!
--

--
One thing I'm still thinking about is how to make it more clear that a backup that's failed validation has no data uploaded to object storage.  E.g. a backup with a single validation error that therefore never backed up anything might be displayed as `Processed (1 error)` which could be the same as for a full-cluster backup that got one error backing up a single PV.  We could:
- keep `FailedValidation` as a phase
- `FailedValidation` -> `Failed`, rather than `Processed` 
- store a count of objects backed up as a summary stat on the API object and display it, e.g. `Processed (0 items backed up, 1 error)`
- something else
--

--
FWIW - was thinking about https://github.com/heptio/velero/issues/286#issuecomment-440386348 again, and I think `FailedValidation` should become `Failed` (rather than processed).
--

--
So, after starting to dig into this, I realized that this could turn into a pretty big ugly PR if tackled wholesale. I thought about how to break it down and here's what I came up with:

**For v1.0:**
- [x] move `RestoreResult` https://github.com/heptio/velero/blob/master/pkg/apis/velero/v1/restore.go#L118 out of the API package, since it's not actually part of the API, just a struct that gets JSON'ed and uploaded to object storage. This frees us up to drop it in a 1.x release.
- [x] in `velero restore describe`, show warning/error counts right next to the status (so it'd show something like `Phase: Completed (2 errors, 1 warning)`. Possibly also update `velero restore get` output to combine the phase/warnings/errors columns.
- [x] add Warnings and Errors count fields to Backup's status to be consistent with Restores.
- [x] make Backups consistent with Restores re: whether it ends as `Failed` or `Completed`.  `Failed` should mean ""failed to start"" or ""failed to upload tarball"", whereas `Completed` (with errors) would mean individual items failed to backup.

**For v1.x:**
- [ ] enhance `velero backup describe` or `velero backup logs` to more easily view errors, by adding filtering, grouping, formatting. Do this for backups first, since we already have leveled logs.
- [ ] drop the `RestoreResult` type for restores, move warning/error reporting into the restore log, and enable them to be shown in the same way as for Backups

**For v2.0:**
- [ ] Once all of the above is working well, drop the `ValidationErrors` field on Backups/Restores (these will go into the log as regular errors), and drop the `FailedValidation` phase (this will become `Failed`).

@nrb @carlisia does this seem reasonable? I tried to focus v1.0 work on (a) make things consistent between Backups/Restores so it's more obvious for the user, and (b) set us up for making additional changes later.
--

--
@nrb @carlisia would appreciate any input you have on the above - I'd like to get started on this soon.
--

--
Let's chat about this in person next week, I think some discussion will be helpful.
--

--
@carlisia @nrb responses to your comments. We should still discuss in person.

> You didn't mention FailureReason , does this no longer make sense to add? It sounded like a good idea.

Yeah, this still makes sense, and can be added for v1.0.

> For the v1.0 milestone, how does the user then get this combined stream? velero restore logs? So describe only displays counts and no other information?

Based on the latest proposal, not much changes for v1.0 in terms of seeing the results. `velero restore logs` plus `velero restore describe` would still be used to see the overall picture. The work to combine into a single stream would be done in a `v1.x`.

Per https://github.com/heptio/velero/issues/286#issuecomment-479014480, the main work item that I actually want to tackle for v1.0 is deciding on the semantics of `Completed` vs. `Failed`, and then making it consistent across backups + restores.  Specifically:
  - does `Completed` (or `Processed`) mean ""Completed with no errors""? Or can it mean ""Completed with some errors backing up/restoring individual items""? 
  - conversely, does `Failed` mean ""There were >0 errors encountered during backup/restore""? Or does it mean ""Fatal error, such as unable to upload/download backup tarball""?
--

--
We could also decide to go to `Completed`, `CompletedWithErrors`, and `Failed` as the phase set, to differentiate.
--
",nrb,"
--
I think this makes sense. Consolidating everything in one place makes for a better user experience, and I don't think altering the commands to parse the logs should be too onerous.
--

--
> Would this add any value for end-users?

Currently, end users asking for help have to look in the 3 locations specified to see the full picture of what happened if a backup or restore didn't work as expected, so I'd say it does have end user impact.
--

--
This proposal makes sense to me. I think `FailureReason` for cases where the failure is to upload a log makes a lot of sense in terms of visibility.
--

--
For the v1.0 milestone, how does the user then get this combined stream? `velero restore logs`? So `describe` only displays counts and no other information?
--

--
I like `Failed` as ""fatal error, couldn't perform a key operation to start the restore,"" and `CompletedWithErrors` or `PartialCompletion` for a restore where some resources can't be restored. The overlap I see there is if we failed to restore anything into k8s, but velero operations had no error, is that `Failed`, or `CompletedWithErrors`/`PartialCompletion`? If number of artifacts restore == 0, I could see how the latter would be confusing.
--
",ncdc,"
--
Would you move validation errors from the restore resource to the log file?
--

--
The ""nice"" (maybe?) thing about validation errors is you don't have to go to object storage to see them, as they're directly on the restore in kube.

Maybe once we have #155, we could drop validation errors entirely. We'd still need some code to check for things like invalid overlapping includes and excludes, but those could just be ""errors"".
--

--
And we could have our phases be New, InProgress, Failed (we couldn't start at all), Processed (maybe has errors and/or warnings, but we did some work and we've done as much as we could with it).
--

--
@jbeda this is an item we'd like to discuss soon
--
",rosskukulinski,"
--
My read on this would be that this is an internal or developer focused improvement.  Would this add any value for end-users?
--

--
I'd also like to raise a discussion on prioritization for this. @ncdc I see you made it P1 - would love to know more, or we can discuss in the context of the larger 1.0 roadmap planning.
--

--
Ah - I understand now. Thanks @nrb. So I'm lumping this under UX / debuggability.

Related to #305 


--
",carlisia,"
--
I'll look at it in a little bit.
--

--
You didn't mention `FailureReason `, does this no longer make sense to add? It sounded like a good idea.

I like this proposal as is. I especially like rolling the `FailedValidation` into the `Failed` phase. Ultimately, I think the priority should be to have 1 phase that unequivocally means ""success"". If this is `Processed`, then we definitely should go for ""FailedValidation -> Failed, rather than Processed"". And I don't think it hurts if this change gets postponed to v2.0.

I'm confused about this `Completed (2 errors, 1 warning)`. How does `Phase: Completed` relate to `Phase: Processed`? I found `completed` in the code but having trouble finding `processed`. In addition, in the code it says `BackupPhaseCompleted means the backup has run successfully without errors.` but your proposal says to display in the `describe`: `Phase: Completed (2 errors, 1 warning)`. If there are errors, how could it be completed?

I'm asking to make sure we don't have discrepancy in meaning with our phases but also for my understanding.

Other then that tho, I like it that you broke out part of this effort into future releases, that is great. 👍 

--

--
For me it hinges on how useful the output of a `CompletedWithErrors` would be. If 100% unusable, then it should be a plain `Failed`. I suspect there's a spectrum and maybe we can't even detect that.
--
",,
275,OPEN,Document restore flow,Area/Documentation; P2 - Long-term important,2020-12-08 01:52:36 +0000 UTC,ncdc,Opened,,"Include information about downloading tarball, iterating through api groups, resources, items. Cluster-scoped vs namespace-scoped. Custom actions. Conflicts. Errors/warnings.",,,,,,,,,,,,,,
274,OPEN,Document backup flow,Area/Documentation; P2 - Long-term important,2020-12-08 01:52:36 +0000 UTC,ncdc,Opened,,"Include info about discovery, api groups, resources, items, pre/post hooks, custom actions, additional items, etc.",,,ddebrunner,"
--
Is there any initial documentation about the flow currently, or where to look to try and figure out the flow (e.g. when are pre/post hooks executed when multiple pods exist)?
--
",a,"
--
@nrb and I were just talking about making improvements to the backup docs, https://velero.io/docs/v1.5/how-velero-works/#backup-workflow, to make them more clear. We should make sure that the velero docs cover backups in enough detail
--
",,,,,,,,
262,OPEN,Support retrieving chunked lists of items when backing up,P1 - Important; Performance,2021-02-01 18:48:23 +0000 UTC,ncdc,In progress,,Kuberentes 1.9 adds support for chunked lists. We should do this. See https://github.com/kubernetes/website/pull/6540/files#diff-dd99945fd91d7ea569799a45d59669f0R90.,,,ncdc,"
--
Need to use `k8s.io/client-go/tools/pager` from client-go v6. I am doing the work to upgrade us to v6, so this can come afterwards.
--

--
No, I believe the helper in client-go retrieves the full list if paging isn't supported by the server.
--
",nrb,"
--
Will this force the minimum required Kubernetes version up?
--
",rosskukulinski,"
--
This falls under the performance improvement category.  Until we have better metrics (#531) to understand the performance of Ark on larger clusters, I recommend deprioritizing this effort.

That said, it could be interesting to test Ark against some of the large Gimbal clusters that are being used for performance testing. [cc: @alexbrand ]
--
",,,,,,
251,OPEN,Ability to keep a backup from being garbage collected,Enhancement/User,2020-12-08 01:52:36 +0000 UTC,ncdc,In progress,,"Add the ability to preserve a backup. This could mean one of a few things:

- be able to specify a ttl of 0 == never expire
- be able to change a backup's expiration after it's been created
- be able to mark a backup as pinned/preserved so that the expiration is ignored",,,skriss,"
--
Personally I like the UX of pin/unpin. We might want to reset the expiration upon unpin, otherwise unpinning an expired backup could result in a near-immediate GC, which might not be desirable.
--

--
@bastoker we haven't implemented this feature yet, no.

You *can* always manually edit the TTL/expiration of a backup, e.g. `kubectl -n velero edit backup.velero.io <NAME>`. Note that the `status.expiration` field is the one that Velero will actually look at to determine when to delete expired backups.
--
",lypht,"
--
Feature request: The ability to ship a preserved backup to a source of record separate from the primary backup location for audits/reference.
--
",ncdc,"
--
That will be covered by #103 
--
",rosskukulinski,"
--
Yes! Simple is best here.

I think the key flag here is the “Pin / Unpin” action.  Pinning a backup should apply a label to the backup, such that a replication policy can be configured to match pinned backups.

Specifying a ttl of 0 automatically sets the Pin flag

Changing a backup’s expiration after it’s been created feels like a different feature/issue to me.

--
",bastoker,"
--
Is there any update on the pin/unpin front? I'm curious if I can change or delete the TTL and expiration of an existing backup?
--
",,
131,OPEN,Move validation from controllers to pkg/backup; pkg/restore,Enhancement/Dev; kind/tech-debt,2021-02-03 17:40:22 +0000 UTC,ncdc,Opened,,Make the validation the first thing we do in Backup() and Restore(). WDYT @skriss?,,,skriss,"
--
yeah, makes sense to me to put it in the library
--

--
Here's an initial mini-design for this issue:

**Goals**
- Move all backup/restore-processing logic into `pkg/backup`, `pkg/restore` respectively so there's a single entrypoint for running a backup/restore which can be more easily tested, vendored, etc.
- Make controllers ""dumb"" - only responsible for watching for new/updated custom resources, enqueuing them, and dequeueing them and passing them to the appropriate packages for work

**Non-Goals**
- Full refactoring of `pkg/backup` & `pkg/restore`.  The goal of this initial effort is just to draw a clear boundary around the end-to-end backup/restore processes.  From there, integration tests can be written against this boundary to provide guardrails for refactoring the implementations.
- Similar refactoring for other controllers.  This should probably be undertaken later, but first priority is for core backup and restore flows.

**Design**
Entrypoints to packages should be very simple:
```go
// <pkg/backup>
type Pipeline interface {
    Execute(namespace, name string) error
}
```
- Backup pipeline will:
  - Ensure API object exists
  - Ensure API object has the correct phase for processing (blank or `New`)
  - Default unspecified API object fields
    - `.spec.ttl`
    - `.spec.storageLocation`
    - `.spec.volumeSnapshotLocations`
    - `.status.version`
  - Validate API object spec
    - included/excluded resources
    - included/excluded namespaces
    - backup storage location
    - volume snapshot locations
  - Update API object's status to either `FailedValidation` (with errors) or `InProgress`
  - Instantiate all the things
  - Run API-scraping & related actions/hooks
  - Upload artifacts (archive, log, volumesnapshots file) to storage
  - Update API object's final status
- Restore pipeline will work similarly (to be detailed after going through backup pipeline & learning from it) 

cc @carlisia @nrb @wwitzel3
--
",rosskukulinski,"
--
Internal development enhancement, but this is P1 because right now we have backup and restore logic split in 2 different packages. In addition, if anyone wants to vendor ark to do backups/restores, it's not pretty. We want a self-contained package for each.
--
",,,,,,,,
103,OPEN,[Epic] Backup Replication,Breaking change; Enhancement/User; Epic,2020-12-08 01:52:27 +0000 UTC,jrnt30,Opened,,"**User Stories**
As a cluster administrator, I would like to define a replication policy for my backups which will ensure that copies exist in other availability zones or regions. This will allow me to restore a cluster in case of an AZ or region failure.

**Non-Goals**
1. Cross-cloud replication of backups
1. Cross-account replication of backups

**Features**
- [ ] ?

---
_*Original Issue Description*_

There are a few different dimensions of a DR strategy that may be worth consideration.  For AWS deployments the trade-offs the complexity of running Multi-AZ are fairly negligible if you stay in the same region. As such the Single Region/Multi-AZ deployment is extremely common. 

An additional requirement often is having the ability to restore in another region with more relaxed RTO/RPO in the case of an entire region going down.  

Looking over #101 brought a few things to mind, and a large wish list might include: 

- Ability to specify additional block storage providers for syncing to additional regions (or a different type of block storage provider that would simply execute the clone to a different region) 
- Ability to map AZs for a restoration (maybe similar to Namespaces but preferably just transparently for the user) to allow for something like `us-east-1a -> us-west-2b`. 
- Writing backup data to an additional bucket in alternate region

Some of these are certainly available today to users (copying snapshots and s3 data) but require additional external integrations to function properly.  As a user it would be more convenient if this were able to be done in a consolidated way. 
",,,ncdc,"
--
@jbeda some of what @jrnt30 is describing sounds similar to your idea of ""backup targets""
--

--
@jimzim this is definitely something we need to spec out and do! We've been kicking around the idea of a ""backup target"", which would replace the current `Config` kind. You could define as many targets as you wish, and when you perform a backup, you would then specify which target to use. There are some UX issues to reason through here...
--

--
Sounds great!

On Wed, Nov 29, 2017 at 5:59 PM Jim Zimmerman <notifications@github.com>
wrote:

> @ncdc <https://github.com/ncdc> Maybe we can discuss this briefly at
> KubeCon? I have begun to make this work on Azure, but before I go too much
> further it would be good to talk about what your planned architecture is.
>
> —
> You are receiving this because you were mentioned.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/heptio/ark/issues/103#issuecomment-348025540>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AAABYoh9BCcUoAc0fXI8AEDsX5VP9gqjks5s7eHsgaJpZM4PingG>
> .
>

--
",jimzim,"
--
I just was going to post this as a feature request. :)

I just tried to do this from eastus to westus in Azure and started to think about how we could copy the snapshot and create the disk in the correct region.  We could possibly have a restore target config?  I also like the idea of creating multiple backups to other regions in case a region goes down or a cluster and its resources get deleted.
--

--
@ncdc Maybe we can discuss this briefly at KubeCon?  I have begun to make this work on Azure, but before I go too much further it would be good to talk about what your planned architecture is.
--
",jbeda,"
--
This is very much what i'm thinking.  We need to think about backup targets, restore sources and ways to munge stuff with a pipeline.  Sounds like we are all thinking similar things.
--
",rocketraman,"
--
On Azure, you can create a snapshot into a different resource group than the one that the persistent disk is on, which means the snapshots could be created directly into the `AZURE_BACKUP_RESOURCE_GROUP` instead of `AZURE_RESOURCE_GROUP`.

Then, cross-RG restores should be quite simple as the source of the data will always be consistent and there should be no refs to `AZURE_RESOURCE_GROUP`.

I'm not sure if same-Location is a limitation of this -- I've only tried this on two resource groups that are in the same Azure Location.

The command/output I used to test this:

```
az snapshot create --name foo --resource-group Ark_Dev-Kube --source '/subscriptions/xxx/resourceGroups/my-Dev-Kube1/providers/Microsoft.Compute/disks/devkube1-dynamic-pvc-0bbf7e11-9e82-11e7-a717-000d3af4357e'
  DiskSizeGb  Location    Name    ProvisioningState    ResourceGroup    TimeCreated
------------  ----------  ------  -------------------  ---------------  --------------------------------
           5  canadaeast  foo     Succeeded            Ark_Dev-Kube     2018-01-09T16:21:58.398476+00:00
```

and the `foo` snapshot was created in `Ark_Dev-Kube` even though the disk is in `my-Dev-Kube1`.
--
",rosskukulinski,"
--
For reference, [this](https://docs.google.com/document/d/1hFl8MPFWM3JVwS-e5PlmBzROgvNPfZRnllm6Ts04Y4Q/edit) is the current Ark Backup Replication design.
--
",nrb,"
--
We've created a [document of scenarios](https://docs.google.com/document/d/1Ct-0bt1aCs_GRC0ecpn64sBl3JCQ2ykN7LCk_EsYOc4/) that we'll use to inform the design decisions for this project.

We also have a document where we're [discussing more detailed changes](https://docs.google.com/document/d/1vGz53OVAPynrgi5sF0xSfKKr32NogQP-xgXA1PB6xMc/) to the Ark codebase from which we'll generate a list of specific work items.

Members of the heptio-ark@googlegroups.com google group have comment access to both of these documents for anyone who would like to share their thoughts on these.
--
"
18,OPEN,design for multi-tenancy support,Breaking change; Enhancement/User; P2 - Long-term important,2020-04-27 15:35:21 +0000 UTC,kstewart,Opened,,"Allow users other than cluster-admin to use Ark. This has security implications:

- need to ensure a user can only back up/restore PVs/snapshots they have access to
- need to ensure a user can only back up kube resources they have access to
- need to find a way to have the restore ""run as"" the user, so there's no privilege escalation",,,ncdc,"
--
@yastij is also interested in helping out here
--
",yastij,"
--
+1
--
",dhawal55,"
--
+1. I'm willing to help too. I will start by getting familiar with the codebase and come up with a proposal

--
",abessifi,"
--
Guys, do you please know in which upcomming release this feature will be available ?
--

--
>The biggest concern for us is that once a non-admin user is granted permissions which allow them to either 1. access the Velero namespace, or 2. create Velero resources, then we have broken the security model.

I expect users to interract with Velero only through CRDs. This is the best way to abstract the access to Velero and it's underlying infra (Restic in this case). I think the challenge here is more about redesigning Velero to support multi-tenancy out-of-the-box which may lead to a lot of code refactoring.
--
",skriss,"
--
@abessifi we haven't slated it for a release yet - we are still in the stage of needing to do some R&D/design work to figure out how this would work. we'll update here as/when it progresses!

If you have any input on this item - please feel free to add comments on this issue about your use case, that would be helpful for the team.
--

--
Moving this into v1.3 for tracking purposes, as we have some community members who are interested in helping to design/implement.
--

--
@bgagnon yeah, I have been thinking about something like that as well. Another challenge is how cluster-scoped resources are dealt with - Persistent Volumes being the first one that comes to mind.  If you have a constrained service account per namespace, then it needs to be able to GET these cluster-scoped resources in order to back them up.
--
",nzoueidi,"
--
I am interested on helping with this feature. Maybe we could take this up to the velero community meeting and discuss it with all the other interested community members? 
--
"
