Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Component/s,Due Date,Votes,Description,Environment,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Incorporates),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Required),Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Lucene Fields),Custom field (Lucene Fields),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Tags),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Make sure to account for ScoreMode.TOP_DOCS in queries,LUCENE-9628,13343658,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Minor,,,jtibshirani,jtibshirani,01/Dec/20 23:05,15/Dec/20 15:39,18/Feb/21 10:07,,,,,,core/search,,1,"I noticed a few places where we are directly check the {{ScoreMode}} type that should perhaps be generalized. These could affect whether numeric sort optimization is applied:
 * In {{BooleanWeight#bulkScorer}}, we check if score mode is {{TOP_SCORES}} and if so, force non-bulk scoring. Should we expand this to include modes like {{TOP_DOCS}}?
 * In {{ConstantScoreQuery}}, we create the delegate weight with a hardcoded {{COMPLETE_NO_SCORES}}. I'm not sure it actually causes problems, but it seems like this doesn't handle {{TOP_DOCS}} correctly.

Apologies this issue isn’t more precise – I am not up-to-speed on the numeric sort optimization but wanted to raise these in case they’re helpful.",,"mayya-sharipova opened a new pull request #2126:
URL: https://github.com/apache/lucene-solr/pull/2126


   Account for ScoreMode.TOP_DOCS and TOP_DOCS_WITH_SCORES in boolean queries
   
   LUCENE-9280 introduced new ScoreModes of TOP_DOCS and TOP_DOCS_WITH_SCORES,
   but some boolean queries were not considering these score modes.
   This patch fixes some of these cases and also enhances definition for
   these score modes.
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Dec/20 16:52;githubbot;600","jtibshirani commented on a change in pull request #2126:
URL: https://github.com/apache/lucene-solr/pull/2126#discussion_r539662454



##########
File path: lucene/core/src/java/org/apache/lucene/search/DisjunctionScorer.java
##########
@@ -46,8 +46,8 @@ protected DisjunctionScorer(Weight weight, List<Scorer> subScorers, ScoreMode sc
       final DisiWrapper w = new DisiWrapper(scorer);
       this.subScorers.add(w);
     }
-    this.needsScores = scoreMode != ScoreMode.COMPLETE_NO_SCORES;
-    if (scoreMode == ScoreMode.TOP_SCORES) {
+    this.needsScores = scoreMode.needsScores();
+    if (scoreMode == ScoreMode.TOP_SCORES || scoreMode == ScoreMode.TOP_DOCS_WITH_SCORES) {

Review comment:
       In the spirit of making the checks general, could this instead be `if (needsScores && scoreMode.isExhaustive() == false)` ?

##########
File path: lucene/core/src/test/org/apache/lucene/search/TestFieldSortOptimizationSkipping.java
##########
@@ -115,6 +115,96 @@ public void testLongSortOptimization() throws IOException {
     dir.close();
   }
 
+  public void testSortOptWithBooleanQueries() throws IOException {
+    try (
+      Directory dir = newDirectory();
+      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig());
+    ) {
+      final int numDocs = atLeast(10000);
+      for (int i = 0; i < numDocs; ++i) {
+        final Document doc = new Document();
+        doc.add(new NumericDocValuesField(""field1"", i));
+        doc.add(new IntPoint(""field1"", i));
+        doc.add(new StringField(""field2"", random().nextBoolean() ? ""value1"" : ""value2"", Field.Store.NO));
+        writer.addDocument(doc);
+        if (i == 7000) writer.flush(); // two segments
+      }
+
+      try (IndexReader reader = DirectoryReader.open(writer)) {
+        IndexSearcher searcher = new IndexSearcher(reader);
+        searcher.setQueryCache(null);
+        final SortField sortField = new SortField(""field1"", SortField.Type.INT);
+        final Sort sort = new Sort(sortField);
+        final int numHits = 3;
+        final int totalHitsThreshold = 3;
+
+        { // test that sort optimization is enabled in a boolean query FILTER
+          BooleanQuery.Builder bq = new BooleanQuery.Builder();
+          bq.add(IntPoint.newRangeQuery(""field1"", 10, Integer.MAX_VALUE), BooleanClause.Occur.FILTER);
+          bq.add(new TermQuery(new Term(""field2"", ""value1"")), BooleanClause.Occur.FILTER);
+          Query query = bq.build();
+          TotalHitCountCollector countCollector = new TotalHitCountCollector();
+          searcher.search(query, countCollector);
+          final int totalHitsCount = countCollector.getTotalHits();
+
+          TopFieldCollector topCollector = TopFieldCollector.create(sort, numHits, null, totalHitsThreshold);
+          searcher.search(query, topCollector);
+          TopDocs topDocs = topCollector.topDocs();
+          assertTrue(topCollector.isEarlyTerminated());
+          assertTrue(topDocs.totalHits.value < totalHitsCount);
+        }
+
+        { // test that sort optimization is enabled in a boolean query MUST
+          BooleanQuery.Builder bq = new BooleanQuery.Builder();
+          bq.add(IntPoint.newRangeQuery(""field1"", 10, Integer.MAX_VALUE), BooleanClause.Occur.MUST);
+          bq.add(new TermQuery(new Term(""field2"", ""value1"")), BooleanClause.Occur.MUST);
+          Query query = bq.build();
+          TotalHitCountCollector countCollector = new TotalHitCountCollector();
+          searcher.search(query, countCollector);
+          final int totalHitsCount = countCollector.getTotalHits();
+
+          TopFieldCollector topCollector = TopFieldCollector.create(sort, numHits, null, totalHitsThreshold);
+          searcher.search(query, topCollector);
+          TopDocs topDocs = topCollector.topDocs();
+          assertTrue(topCollector.isEarlyTerminated());
+          assertTrue(topDocs.totalHits.value < totalHitsCount);
+        }
+
+        { // test that sort optimization is enabled in a boolean query SHOULD
+          BooleanQuery.Builder bq = new BooleanQuery.Builder();
+          bq.add(IntPoint.newRangeQuery(""field1"", 10, Integer.MAX_VALUE), BooleanClause.Occur.SHOULD);
+          bq.add(new TermQuery(new Term(""field2"", ""value1"")), BooleanClause.Occur.SHOULD);
+          Query query = bq.build();
+          TotalHitCountCollector countCollector = new TotalHitCountCollector();
+          searcher.search(query, countCollector);
+          final int totalHitsCount = countCollector.getTotalHits();
+
+          TopFieldCollector topCollector = TopFieldCollector.create(sort, numHits, null, totalHitsThreshold);
+          searcher.search(query, topCollector);
+          TopDocs topDocs = topCollector.topDocs();
+          assertTrue(topCollector.isEarlyTerminated());
+          assertTrue(topDocs.totalHits.value < totalHitsCount);
+        }
+
+        { // test that sort optimization is enabled in a boolean query with MUST_NOT
+          BooleanQuery.Builder bq = new BooleanQuery.Builder();

Review comment:
       We could also test a `ConstantScoreQuery`?

##########
File path: lucene/core/src/java/org/apache/lucene/search/ScoreMode.java
##########
@@ -73,4 +82,16 @@ public boolean needsScores() {
   public boolean isExhaustive() {
     return isExhaustive;
   }
+
+  /**
+   * Converts the given scoreMode to a corresponding scoreMode that doesn't need scores.
+   */
+  public static ScoreMode convertToNoScores(ScoreMode scoreMode) {

Review comment:
       Small suggestion: the name `withNoScores` could be more concise.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Dec/20 22:03;githubbot;600","jpountz commented on a change in pull request #2126:
URL: https://github.com/apache/lucene-solr/pull/2126#discussion_r542360145



##########
File path: lucene/core/src/java/org/apache/lucene/search/ScoreMode.java
##########
@@ -73,4 +82,16 @@ public boolean needsScores() {
   public boolean isExhaustive() {
     return isExhaustive;
   }
+
+  /**
+   * Converts the given scoreMode to a corresponding scoreMode that doesn't need scores.
+   */
+  public static ScoreMode convertToNoScores(ScoreMode scoreMode) {

Review comment:
       what about making it a method on score mode so that consumers could do `scoreMode.withNoScores()` instead of `ScoreMode.withNoScores(scoreMode)`?

##########
File path: lucene/core/src/java/org/apache/lucene/search/DisjunctionScorer.java
##########
@@ -46,8 +46,8 @@ protected DisjunctionScorer(Weight weight, List<Scorer> subScorers, ScoreMode sc
       final DisiWrapper w = new DisiWrapper(scorer);
       this.subScorers.add(w);
     }
-    this.needsScores = scoreMode != ScoreMode.COMPLETE_NO_SCORES;
-    if (scoreMode == ScoreMode.TOP_SCORES) {
+    this.needsScores = scoreMode.needsScores();
+    if (scoreMode == ScoreMode.TOP_SCORES || scoreMode == ScoreMode.TOP_DOCS_WITH_SCORES) {

Review comment:
       Why do we need to call advanceShallow() with `TOP_DOCS_WITH_SCORES`? I thought this would only be needed with `TOP_SCORES`.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Dec/20 12:54;githubbot;600","mayya-sharipova commented on pull request #2126:
URL: https://github.com/apache/lucene-solr/pull/2126#issuecomment-744672083


   @jtibshirani @jpountz Thank you for your review. I've address your feedback in 7e04bdce818c3. Can you please continue the review. 


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Dec/20 19:53;githubbot;600","jbampton commented on a change in pull request #2126:
URL: https://github.com/apache/lucene-solr/pull/2126#discussion_r543452721



##########
File path: lucene/core/src/java/org/apache/lucene/search/ScoreMode.java
##########
@@ -73,4 +112,5 @@ public boolean needsScores() {
   public boolean isExhaustive() {
     return isExhaustive;
   }
+

Review comment:
       ```suggestion
   ```

##########
File path: lucene/core/src/test/org/apache/lucene/search/TestFieldSortOptimizationSkipping.java
##########
@@ -115,6 +115,109 @@ public void testLongSortOptimization() throws IOException {
     dir.close();
   }
 
+  public void testSortOptWithBooleanQueriesAndConstantScoreQuery() throws IOException {
+    try (
+      Directory dir = newDirectory();
+      IndexWriter writer = new IndexWriter(dir, new IndexWriterConfig());
+    ) {
+      final int numDocs = atLeast(10000);
+      for (int i = 0; i < numDocs; ++i) {
+        final Document doc = new Document();
+        doc.add(new NumericDocValuesField(""field1"", i));
+        doc.add(new IntPoint(""field1"", i));
+        doc.add(new StringField(""field2"", random().nextBoolean() ? ""value1"" : ""value2"", Field.Store.NO));
+        writer.addDocument(doc);
+        if (i == 7000) writer.flush(); // two segments
+      }
+
+      try (IndexReader reader = DirectoryReader.open(writer)) {
+        IndexSearcher searcher = new IndexSearcher(reader);
+        searcher.setQueryCache(null);
+        final SortField sortField = new SortField(""field1"", SortField.Type.INT);
+        final Sort sort = new Sort(sortField);
+        final int numHits = 3;
+        final int totalHitsThreshold = 3;
+
+        { // test that sort optimization is enabled in a boolean query FILTER
+          BooleanQuery.Builder bq = new BooleanQuery.Builder();
+          bq.add(IntPoint.newRangeQuery(""field1"", 10, Integer.MAX_VALUE), BooleanClause.Occur.FILTER);
+          bq.add(new TermQuery(new Term(""field2"", ""value1"")), BooleanClause.Occur.FILTER);
+          Query query = bq.build();
+          TotalHitCountCollector countCollector = new TotalHitCountCollector();
+          searcher.search(query, countCollector);
+          final int totalHitsCount = countCollector.getTotalHits();
+
+          TopFieldCollector topCollector = TopFieldCollector.create(sort, numHits, null, totalHitsThreshold);
+          searcher.search(query, topCollector);
+          TopDocs topDocs = topCollector.topDocs();
+          assertTrue(topCollector.isEarlyTerminated());
+          assertTrue(topDocs.totalHits.value < totalHitsCount);
+        }
+
+        { // test that sort optimization is enabled in a boolean query MUST
+          BooleanQuery.Builder bq = new BooleanQuery.Builder();
+          bq.add(IntPoint.newRangeQuery(""field1"", 10, Integer.MAX_VALUE), BooleanClause.Occur.MUST);
+          bq.add(new TermQuery(new Term(""field2"", ""value1"")), BooleanClause.Occur.MUST);
+          Query query = bq.build();
+          TotalHitCountCollector countCollector = new TotalHitCountCollector();
+          searcher.search(query, countCollector);
+          final int totalHitsCount = countCollector.getTotalHits();
+
+          TopFieldCollector topCollector = TopFieldCollector.create(sort, numHits, null, totalHitsThreshold);
+          searcher.search(query, topCollector);
+          TopDocs topDocs = topCollector.topDocs();
+          assertTrue(topCollector.isEarlyTerminated());
+          assertTrue(topDocs.totalHits.value < totalHitsCount);
+        }
+
+        { // test that sort optimization is enabled in a boolean query SHOULD
+          BooleanQuery.Builder bq = new BooleanQuery.Builder();
+          bq.add(IntPoint.newRangeQuery(""field1"", 10, Integer.MAX_VALUE), BooleanClause.Occur.SHOULD);
+          bq.add(new TermQuery(new Term(""field2"", ""value1"")), BooleanClause.Occur.SHOULD);
+          Query query = bq.build();
+          TotalHitCountCollector countCollector = new TotalHitCountCollector();
+          searcher.search(query, countCollector);
+          final int totalHitsCount = countCollector.getTotalHits();
+
+          TopFieldCollector topCollector = TopFieldCollector.create(sort, numHits, null, totalHitsThreshold);
+          searcher.search(query, topCollector);
+          TopDocs topDocs = topCollector.topDocs();
+          assertTrue(topCollector.isEarlyTerminated());
+          assertTrue(topDocs.totalHits.value < totalHitsCount);
+        }
+
+        { // test that sort optimization is enabled in a boolean query with MUST_NOT
+          BooleanQuery.Builder bq = new BooleanQuery.Builder();
+          bq.add(IntPoint.newRangeQuery(""field1"", 20, Integer.MAX_VALUE), BooleanClause.Occur.MUST);
+          bq.add(new TermQuery(new Term(""field2"", ""value1"")), BooleanClause.Occur.MUST_NOT);
+          Query query = bq.build();
+          TotalHitCountCollector countCollector = new TotalHitCountCollector();
+          searcher.search(query, countCollector);
+          final int totalHitsCount = countCollector.getTotalHits();
+
+          TopFieldCollector topCollector = TopFieldCollector.create(sort, numHits, null, totalHitsThreshold);
+          searcher.search(query, topCollector);
+          TopDocs topDocs = topCollector.topDocs();
+          assertTrue(topCollector.isEarlyTerminated());
+          assertTrue(topDocs.totalHits.value < totalHitsCount);
+        }
+
+        { // test that sort optimization is enabled in a ConstantScoreQuery
+          Query query = new ConstantScoreQuery(IntPoint.newRangeQuery(""field1"", 20, Integer.MAX_VALUE));
+          TotalHitCountCollector countCollector = new TotalHitCountCollector();
+          searcher.search(query, countCollector);
+          final int totalHitsCount = countCollector.getTotalHits();
+
+          TopFieldCollector topCollector = TopFieldCollector.create(sort, numHits, null, totalHitsThreshold);
+          searcher.search(query, topCollector);
+          TopDocs topDocs = topCollector.topDocs();
+          assertTrue(topCollector.isEarlyTerminated());
+          assertTrue(topDocs.totalHits.value < totalHitsCount);
+        }
+      }
+    }
+  }
+

Review comment:
       ```suggestion
   ```




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;15/Dec/20 15:39;githubbot;600",,,,,,,,,,,,,,,,,,,,,0,3000,,,0,3000,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-12-02 18:14:50.496,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 07 17:04:09 UTC 2020,,New,,,,,,,"0|z0l3y0:",9223372036854775807,,,,,,,,,,,,,,,"02/Dec/20 18:14;jpountz;bq. In BooleanWeight#bulkScorer, we check if score mode is TOP_SCORES and if so, force non-bulk scoring. Should we expand this to include modes like TOP_DOCS?

I think so. DefaultBulkScorer seems to be the only bulk scorer which knows how to deal with {{LeafCollector#competitiveIterator}}, so we seem to be disabling the numeric sort optimization with boolean queries today? Let's switch to ScoreMode#isExhaustive? What do you think [~mayyas]?

bq. In ConstantScoreQuery, we create the delegate weight with a hardcoded COMPLETE_NO_SCORES. I'm not sure it actually causes problems, but it seems like this doesn't handle TOP_DOCS correctly.

I suspect that this could be a problem if the wrapped query uses the ScoreMode as an indication of whether it will need to handle {{LeafCollector#competitiveIterator}} or not, which seems to be something we'd like to do for boolean queries since BS1 (BooleanScorer) only really makes sense if we know we're going to collect all matches.

I think it'd be helpful if we improved ScoreMode javadocs to be more explicit regarding the expectations we have on scorers. TOP_SCORES mentions the relationship with {{Scorer#setMinCompetitiveScore}}, we should add something similar to TOP_DOCS and TOP_DOCS_WITH_SCORES regarding bulk scorers and {{LeafCollector#competitiveIterator}}?","03/Dec/20 12:13;mayyas;[~jtibshirani] Thank you for raising these very good issues.

I agree with you and [~jpountz] about switching to a check of  `ScoreMode#isExhaustive == false` instead of just `scoreMode == ScoreMode.TOP_SCORES` in `BooleanWeight`. I will address this.

---

About `ConstantScoreQuery`:
 * I will see how we can addressed `TOP_DOCS` case as well without using `COMPLETE_NO_SCORES` for `innerWeight` all the time.
 * agree with [~jpountz] that we can improve javadoc of `ScoreMode`","03/Dec/20 23:21;julietibs;bq. I think it'd be helpful if we improved ScoreMode javadocs to be more explicit regarding the expectations we have on scorers.

This would be great. Adding one last spot I noticed as it may be easy to cover in a broader fix: {{DisjunctionScorer}} checks whether scores are required through {{scoreMode != ScoreMode.COMPLETE_NO_SCORES}}, which could just be {{scoreMode.needsScores()}} now.","07/Dec/20 17:04;mayyas;I've create a PR to address the discussed points.

There are other occurrences of checks of scoreMode in boolean scores that we should probably improve to account for other score modes, but I don't know this code well enough to make this change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use RandomMockMergePolicy more often,LUCENE-9469,13323498,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,simonw,simonw,19/Aug/20 07:19,19/Aug/20 15:03,18/Feb/21 10:07,,,,,,,,0,"During the work on LUCENE-8962 we had some discussions about testability of the feature and how well we can guarantee that it's tested. One argument was that we can rely to a certain extend on the randomization and enabling these features by swapping in RandomMockMergePolicy. 
I ran a test which throws an assertion error every time the feature is explicitly used which required MockRandomMergePolicy to be used. Unfortunately I had two entire test runs without any failure except of the tests that explicitly enabled it ie. the ones that I wrote for the feature. 
I think we are not using this MP often enough in our tests and I want to propose to use it way more frequently. It's certainly not a replacement for dedicated unit tests, I wrote a dedicated one for every random failure I found which should be common practice but it would be great to increase coverage by swapping in MockRandomMergePolicy more often. Maybe something as simple as this would do it:

{noformat}
--- a/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
@@ -1059,7 +1059,7 @@ public abstract class LuceneTestCase extends Assert {
   }
 
   public static MergePolicy newMergePolicy(Random r, boolean includeMockMP) {
-    if (includeMockMP && rarely(r)) {
+    if (includeMockMP && r.nextBoolean()) {
       return new MockRandomMergePolicy(r);
     } else if (r.nextBoolean()) {
       return newTieredMergePolicy(r);
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-08-19 15:03:22.059,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 19 15:03:22 UTC 2020,,New,,,,,,,"0|z0hwnc:",9223372036854775807,,,,,,,,,,,,,,,"19/Aug/20 15:03;jpountz;I think it has the potential to slow tests down significantly? If it doesn't +1 to do this change but otherwise we might need to make MockRandomMP find more reasonable merges as the index grows?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 TestLongBitSet.testHugeCapacity sometimes fails on nightly,LUCENE-9419,13313741,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Minor,,,simonw,simonw,27/Jun/20 10:27,27/Jun/20 10:35,18/Feb/21 10:07,,,,,,,,0,"I see this once in a while on our CI with  -Dtests.nightly=true 
{noformat}
00:05:30    [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestLongBitSet -Dtests.method=testHugeCapacity -Dtests.seed=FB56CEB40315E44E -Dtests.nightly=true -Dtests.slow=true -Dtests.badapples=true -Dtests.locale=lu -Dtests.timezone=Africa/Casablanca -Dtests.asserts=true -Dtests.file.encoding=UTF-8
00:05:30    [junit4] ERROR   0.08s J1 | TestLongBitSet.testHugeCapacity <<<
00:05:30    [junit4]    > Throwable #1: java.lang.OutOfMemoryError: Java heap space
00:05:30    [junit4]    > 	at __randomizedtesting.SeedInfo.seed([FB56CEB40315E44E:8116844FEAE5C24A]:0)
00:05:30    [junit4]    > 	at org.apache.lucene.util.ArrayUtil.growExact(ArrayUtil.java:323)
00:05:30    [junit4]    > 	at org.apache.lucene.util.ArrayUtil.grow(ArrayUtil.java:332)
00:05:30    [junit4]    > 	at org.apache.lucene.util.LongBitSet.ensureCapacity(LongBitSet.java:54)
00:05:30    [junit4]    > 	at org.apache.lucene.util.TestLongBitSet.testHugeCapacity(TestLongBitSet.java:343)
00:05:30    [junit4]    > 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
00:05:30    [junit4]    > 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
00:05:30    [junit4]    > 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
00:05:30    [junit4]    > 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
00:05:30    [junit4]   2> NOTE: test params are: codec=Asserting(Lucene86): {}, docValues:{}, maxPointsInLeafNode=246, maxMBSortInHeap=6.097958740443044, sim=Asserting(RandomSimilarity(queryNorm=false): {}), locale=lu, timezone=Africa/Casablanca
00:05:30    [junit4]   2> NOTE: Linux 4.12.14-122.26-default amd64/Oracle Corporation 11.0.2 (64-bit)/cpus=32,threads=1,free=346124224,total=362807296
00:05:30    [junit4]   2> NOTE: All tests run in this JVM: [TestSynonymQuery, TestCachingCollector, TestLucene86PointsFormat, TestSizeBoundedForceMerge, TestSimpleWKTShapeParsing, TestMergeRateLimiter, TestAxiomaticSimilarity, TestReaderClosed, TestSpanExplanations, TestSingleInstanceLockFactory, TestIndexSorting, TestIndexInput, TestBasicModelIn, TestPoint2D, TestAtomicUpdate, TestOneMergeWrappingMergePolicy, TestTragicIndexWriterDeadlock, TestRollingUpdates, TestCodecHoldsOpenFiles, TestLucene50StoredFieldsFormatHighCompression, TestFieldsReader, TestAxiomaticF2EXP, TestPointValues, TestIndependenceStandardized, TestTwoPhaseCommitTool, TestLongBitSet]
00:05:30    [junit4] Completed [87/566 (1!)] on J1 in 3.30s, 11 tests, 1 error <<< FAILURES!
{noformat}

my suspicion is that some other test is holding on to memory here. I am trying to record all failures here maybe we find some tests in common. The test itself needs about 300MB to run so that should be ok with a 512M JVM
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2020-06-27 10:27:19.0,,New,Patch Available,,,,,,"0|z0g8rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tests for corruptions caused by byte flips,LUCENE-9356,13302436,Test,Reopened,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Minor,,,jpountz,jpountz,02/May/20 13:45,06/Jul/20 11:53,18/Feb/21 10:07,,,,,,,,0,We already have tests that file truncation and modification of the index headers are caught correctly. I'd like to add another test that flipping a byte in a way that modifies the checksum of the file is always caught gracefully by Lucene.,,"jpountz opened a new pull request #1569:
URL: https://github.com/apache/lucene-solr/pull/1569


   Opening a reader and then calling checkIntegrity must throw a `CorruptIndexException` or an `IndexFormatToo(Old|New)Exception`.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jun/20 11:54;githubbot;600","msokolov commented on a change in pull request #1569:
URL: https://github.com/apache/lucene-solr/pull/1569#discussion_r438751988



##########
File path: lucene/core/src/test/org/apache/lucene/index/TestAllFilesDetectBitFlips.java
##########
@@ -0,0 +1,139 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.index;
+
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collections;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.LineFileDocs;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressFileSystems;
+import org.apache.lucene.util.TestUtil;
+
+/**
+ * Test that the default codec detects bit flips at open or checkIntegrity time.
+ */
+@SuppressFileSystems(""ExtrasFS"")
+public class TestAllFilesDetectBitFlips extends LuceneTestCase {
+
+  public void test() throws Exception {
+    doTest(false);
+  }
+
+  public void testCFS() throws Exception {
+    doTest(true);
+  }
+
+  public void doTest(boolean cfs) throws Exception {
+    Directory dir = newDirectory();
+
+    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
+    conf.setCodec(TestUtil.getDefaultCodec());
+
+    if (cfs == false) {
+      conf.setUseCompoundFile(false);
+      conf.getMergePolicy().setNoCFSRatio(0.0);
+    }
+
+    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, conf);
+    // Use LineFileDocs so we (hopefully) get most Lucene features
+    // tested, e.g. IntPoint was recently added to it:
+    LineFileDocs docs = new LineFileDocs(random());
+    for (int i = 0; i < 100; i++) {
+      riw.addDocument(docs.nextDoc());
+      if (random().nextInt(7) == 0) {
+        riw.commit();
+      }
+      if (random().nextInt(20) == 0) {
+        riw.deleteDocuments(new Term(""docid"", Integer.toString(i)));
+      }
+      if (random().nextInt(15) == 0) {
+        riw.updateNumericDocValue(new Term(""docid"", Integer.toString(i)), ""docid_intDV"", Long.valueOf(i));
+      }
+    }
+    if (TEST_NIGHTLY == false) {
+      riw.forceMerge(1);
+    }
+    riw.close();
+    checkBitFlips(dir);
+    dir.close();
+  }
+  
+  private void checkBitFlips(Directory dir) throws IOException {
+    for(String name : dir.listAll()) {
+      if (name.equals(IndexWriter.WRITE_LOCK_NAME) == false) {
+        corruptFile(dir, name);
+      }
+    }
+  }
+  
+  private void corruptFile(Directory dir, String victim) throws IOException {
+    try (BaseDirectoryWrapper dirCopy = newDirectory()) {
+      dirCopy.setCheckIndexOnClose(false);
+
+      long victimLength = dir.fileLength(victim);
+      long flipOffset = TestUtil.nextLong(random(), 0, victimLength - 1);
+
+      if (VERBOSE) {
+        System.out.println(""TEST: now corrupt file "" + victim + "" by changing byte at offset "" + flipOffset + "" (length= "" + victimLength + "")"");
+      }
+
+      for(String name : dir.listAll()) {
+        if (name.equals(victim) == false) {
+          dirCopy.copyFrom(dir, name, name, IOContext.DEFAULT);
+        } else {
+          try (IndexOutput out = dirCopy.createOutput(name, IOContext.DEFAULT);
+              IndexInput in = dir.openInput(name, IOContext.DEFAULT)) {
+              out.copyBytes(in, flipOffset);
+              out.writeByte((byte) (in.readByte() + TestUtil.nextInt(random(), 0x01, 0xFF)));
+              out.copyBytes(in, victimLength - flipOffset - 1);
+          }
+          try (IndexInput in = dirCopy.openInput(name, IOContext.DEFAULT)) {
+            try {
+              CodecUtil.checksumEntireFile(in);
+              System.out.println(""TEST: changing a byte in "" + victim + "" did not update the checksum)"");

Review comment:
       curious if you saw this much?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jun/20 12:43;githubbot;600","jpountz commented on a change in pull request #1569:
URL: https://github.com/apache/lucene-solr/pull/1569#discussion_r438754619



##########
File path: lucene/core/src/test/org/apache/lucene/index/TestAllFilesDetectBitFlips.java
##########
@@ -0,0 +1,139 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.index;
+
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collections;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.LineFileDocs;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressFileSystems;
+import org.apache.lucene.util.TestUtil;
+
+/**
+ * Test that the default codec detects bit flips at open or checkIntegrity time.
+ */
+@SuppressFileSystems(""ExtrasFS"")
+public class TestAllFilesDetectBitFlips extends LuceneTestCase {
+
+  public void test() throws Exception {
+    doTest(false);
+  }
+
+  public void testCFS() throws Exception {
+    doTest(true);
+  }
+
+  public void doTest(boolean cfs) throws Exception {
+    Directory dir = newDirectory();
+
+    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
+    conf.setCodec(TestUtil.getDefaultCodec());
+
+    if (cfs == false) {
+      conf.setUseCompoundFile(false);
+      conf.getMergePolicy().setNoCFSRatio(0.0);
+    }
+
+    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, conf);
+    // Use LineFileDocs so we (hopefully) get most Lucene features
+    // tested, e.g. IntPoint was recently added to it:
+    LineFileDocs docs = new LineFileDocs(random());
+    for (int i = 0; i < 100; i++) {
+      riw.addDocument(docs.nextDoc());
+      if (random().nextInt(7) == 0) {
+        riw.commit();
+      }
+      if (random().nextInt(20) == 0) {
+        riw.deleteDocuments(new Term(""docid"", Integer.toString(i)));
+      }
+      if (random().nextInt(15) == 0) {
+        riw.updateNumericDocValue(new Term(""docid"", Integer.toString(i)), ""docid_intDV"", Long.valueOf(i));
+      }
+    }
+    if (TEST_NIGHTLY == false) {
+      riw.forceMerge(1);
+    }
+    riw.close();
+    checkBitFlips(dir);
+    dir.close();
+  }
+  
+  private void checkBitFlips(Directory dir) throws IOException {
+    for(String name : dir.listAll()) {
+      if (name.equals(IndexWriter.WRITE_LOCK_NAME) == false) {
+        corruptFile(dir, name);
+      }
+    }
+  }
+  
+  private void corruptFile(Directory dir, String victim) throws IOException {
+    try (BaseDirectoryWrapper dirCopy = newDirectory()) {
+      dirCopy.setCheckIndexOnClose(false);
+
+      long victimLength = dir.fileLength(victim);
+      long flipOffset = TestUtil.nextLong(random(), 0, victimLength - 1);
+
+      if (VERBOSE) {
+        System.out.println(""TEST: now corrupt file "" + victim + "" by changing byte at offset "" + flipOffset + "" (length= "" + victimLength + "")"");
+      }
+
+      for(String name : dir.listAll()) {
+        if (name.equals(victim) == false) {
+          dirCopy.copyFrom(dir, name, name, IOContext.DEFAULT);
+        } else {
+          try (IndexOutput out = dirCopy.createOutput(name, IOContext.DEFAULT);
+              IndexInput in = dir.openInput(name, IOContext.DEFAULT)) {
+              out.copyBytes(in, flipOffset);
+              out.writeByte((byte) (in.readByte() + TestUtil.nextInt(random(), 0x01, 0xFF)));
+              out.copyBytes(in, victimLength - flipOffset - 1);
+          }
+          try (IndexInput in = dirCopy.openInput(name, IOContext.DEFAULT)) {
+            try {
+              CodecUtil.checksumEntireFile(in);
+              System.out.println(""TEST: changing a byte in "" + victim + "" did not update the checksum)"");

Review comment:
       I haven't seen a single occurrence of it (fortunately! :) )




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jun/20 12:45;githubbot;600","jpountz commented on pull request #1569:
URL: https://github.com/apache/lucene-solr/pull/1569#issuecomment-642621848


   > So we now check checksums on every file when opening
   
   We only verify checksums of meta file when opening (those that we read entirely anyway). Checksums only get verified on other files when `LeafReader#checkIntegrity` is called.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jun/20 12:46;githubbot;600","mikemccand commented on a change in pull request #1569:
URL: https://github.com/apache/lucene-solr/pull/1569#discussion_r438774512



##########
File path: lucene/core/src/test/org/apache/lucene/index/TestAllFilesDetectBitFlips.java
##########
@@ -0,0 +1,139 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.index;
+
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collections;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.LineFileDocs;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressFileSystems;
+import org.apache.lucene.util.TestUtil;
+
+/**
+ * Test that the default codec detects bit flips at open or checkIntegrity time.
+ */
+@SuppressFileSystems(""ExtrasFS"")
+public class TestAllFilesDetectBitFlips extends LuceneTestCase {
+
+  public void test() throws Exception {
+    doTest(false);
+  }
+
+  public void testCFS() throws Exception {
+    doTest(true);
+  }
+
+  public void doTest(boolean cfs) throws Exception {
+    Directory dir = newDirectory();
+
+    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
+    conf.setCodec(TestUtil.getDefaultCodec());
+
+    if (cfs == false) {
+      conf.setUseCompoundFile(false);
+      conf.getMergePolicy().setNoCFSRatio(0.0);
+    }
+
+    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, conf);
+    // Use LineFileDocs so we (hopefully) get most Lucene features

Review comment:
       Woohoo!

##########
File path: lucene/core/src/test/org/apache/lucene/index/TestAllFilesDetectBitFlips.java
##########
@@ -0,0 +1,139 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.index;
+
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collections;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.LineFileDocs;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressFileSystems;
+import org.apache.lucene.util.TestUtil;
+
+/**
+ * Test that the default codec detects bit flips at open or checkIntegrity time.
+ */
+@SuppressFileSystems(""ExtrasFS"")
+public class TestAllFilesDetectBitFlips extends LuceneTestCase {
+
+  public void test() throws Exception {
+    doTest(false);
+  }
+
+  public void testCFS() throws Exception {
+    doTest(true);
+  }
+
+  public void doTest(boolean cfs) throws Exception {
+    Directory dir = newDirectory();
+
+    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
+    conf.setCodec(TestUtil.getDefaultCodec());
+
+    if (cfs == false) {
+      conf.setUseCompoundFile(false);
+      conf.getMergePolicy().setNoCFSRatio(0.0);
+    }
+
+    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, conf);
+    // Use LineFileDocs so we (hopefully) get most Lucene features
+    // tested, e.g. IntPoint was recently added to it:
+    LineFileDocs docs = new LineFileDocs(random());
+    for (int i = 0; i < 100; i++) {
+      riw.addDocument(docs.nextDoc());
+      if (random().nextInt(7) == 0) {
+        riw.commit();
+      }
+      if (random().nextInt(20) == 0) {
+        riw.deleteDocuments(new Term(""docid"", Integer.toString(i)));
+      }
+      if (random().nextInt(15) == 0) {
+        riw.updateNumericDocValue(new Term(""docid"", Integer.toString(i)), ""docid_intDV"", Long.valueOf(i));
+      }
+    }
+    if (TEST_NIGHTLY == false) {
+      riw.forceMerge(1);
+    }
+    riw.close();
+    checkBitFlips(dir);
+    dir.close();
+  }
+  
+  private void checkBitFlips(Directory dir) throws IOException {
+    for(String name : dir.listAll()) {
+      if (name.equals(IndexWriter.WRITE_LOCK_NAME) == false) {
+        corruptFile(dir, name);

Review comment:
       Do you maybe need to exclude `extraN` files for when `ExtraFS` randomly strikes?
   
   Edit: oh, nevermind, I see we suppress `ExtraFS` above :)




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jun/20 13:21;githubbot;600","jpountz commented on a change in pull request #1569:
URL: https://github.com/apache/lucene-solr/pull/1569#discussion_r438831834



##########
File path: lucene/core/src/test/org/apache/lucene/index/TestAllFilesDetectBitFlips.java
##########
@@ -0,0 +1,139 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.index;
+
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collections;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.LineFileDocs;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressFileSystems;
+import org.apache.lucene.util.TestUtil;
+
+/**
+ * Test that the default codec detects bit flips at open or checkIntegrity time.
+ */
+@SuppressFileSystems(""ExtrasFS"")
+public class TestAllFilesDetectBitFlips extends LuceneTestCase {
+
+  public void test() throws Exception {
+    doTest(false);
+  }
+
+  public void testCFS() throws Exception {
+    doTest(true);
+  }
+
+  public void doTest(boolean cfs) throws Exception {
+    Directory dir = newDirectory();
+
+    IndexWriterConfig conf = newIndexWriterConfig(new MockAnalyzer(random()));
+    conf.setCodec(TestUtil.getDefaultCodec());
+
+    if (cfs == false) {
+      conf.setUseCompoundFile(false);
+      conf.getMergePolicy().setNoCFSRatio(0.0);
+    }
+
+    RandomIndexWriter riw = new RandomIndexWriter(random(), dir, conf);
+    // Use LineFileDocs so we (hopefully) get most Lucene features

Review comment:
       This is actually copy-pasted from `TestAllFilesDetectTruncation` :)




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jun/20 14:33;githubbot;600","jpountz merged pull request #1569:
URL: https://github.com/apache/lucene-solr/pull/1569


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jun/20 16:09;githubbot;600",,,,,,,,,,,,,,,,,,,0,4200,,,0,4200,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-06-11 16:09:24.789,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 06 11:53:23 UTC 2020,,New,,,,,,,"0|z0ebhc:",9223372036854775807,,,,,,,,,,,,,,,"11/Jun/20 11:58;jpountz;Thanks to LUCENE-7822 and LUCENE-9359, Lucene now always throws a CorruptIndexException or an IndexFormatToo(Old|New)Exception when opening and then calling checkIntegrity on an index. The attached PR adds a test.","11/Jun/20 16:09;jira-bot;Commit 36109ec36216141cb0fbf9fb09e9d74721a78bda in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=36109ec ]

LUCENE-9356: Add a test that verifies that Lucene catches bit flips. (#1569)

","11/Jun/20 16:11;jira-bot;Commit d3c74a305ff95f087a6e88953d1ef34e7d71f06f in lucene-solr's branch refs/heads/branch_8x from Adrien Grand
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=d3c74a3 ]

LUCENE-9356: Add a test that verifies that Lucene catches bit flips. (#1569)

","11/Jun/20 16:25;jira-bot;Commit 8d95a2ee582da04edf419e6b39756fdde55503fc in lucene-solr's branch refs/heads/branch_8x from Adrien Grand
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=8d95a2e ]

LUCENE-9356: Make FST throw the correct exception upon incorrect input type.
","11/Jun/20 16:26;jira-bot;Commit 8d95a2ee582da04edf419e6b39756fdde55503fc in lucene-solr's branch refs/heads/branch_8x from Adrien Grand
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=8d95a2e ]

LUCENE-9356: Make FST throw the correct exception upon incorrect input type.
","11/Jun/20 16:54;jpountz;I had beasted many iterations but the Elastic CI found a failing seed right after I pushed that is due to the FST constructor, which throws an IllegalStateException when an unexpected byte is read for the input type, so I changed it for a CorruptIndexException.","12/Jun/20 07:17;jira-bot;Commit 38adf09ca2ba9620940fc279cc12760cc355a361 in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=38adf09 ]

LUCENE-9356: Make FST throw the correct exception upon incorrect input type.
","12/Jun/20 07:17;jira-bot;Commit cf8f83cef95c03767f83602ff99345979dd0808b in lucene-solr's branch refs/heads/master from Adrien Grand
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=cf8f83c ]

LUCENE-9356: Disable test, some corruptions are still not detected as corruptions.
","12/Jun/20 07:19;jpountz;I reverted the test, there are still some cases that wouldn't cause a CorruptIndexException and that do not seem straightforward to fix, such as checking the suffix in the index header, which might throw an EOF. I'll think more about what can be done.","12/Jun/20 08:24;jira-bot;Commit 21d08e4b725a4a28196eb36ed1e402f8e19d1be2 in lucene-solr's branch refs/heads/branch_8x from Adrien Grand
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=21d08e4 ]

LUCENE-9356: Disable test, some corruptions are still not detected as corruptions.
","06/Jul/20 10:48;broustant;[8.6 release manager] Is this issue resolved [~jpountz]? I'm checking to prepare 8.6 RC tomorrow.","06/Jul/20 11:53;jpountz;It's not resolved, but it doesn't need to delay 8.6. I'll clear the fixVersion field.",,,,,,,,,,,,,,,,,,,,,,
TestXYMultiPolygonShapeQueries test failures,LUCENE-9139,13279552,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,ivera,ivera,15/Jan/20 10:21,17/Jan/20 10:36,18/Feb/21 10:07,,,,,,,,0,"We recently have two failures on CI from the test method TestXYMultiPolygonShapeQueries. The reproduction lines are:

 
{code:java}
ant test  -Dtestcase=TestXYMultiPolygonShapeQueries -Dtests.method=testRandomMedium -Dtests.seed=F1E142C2FBB612AF -Dtests.multiplier=3 -Dtests.slow=true -Dtests.badapples=true -Dtests.locale=el -Dtests.timezone=EST5EDT -Dtests.asserts=true -Dtests.file.encoding=US-ASCII{code}
{code:java}
ant test  -Dtestcase=TestXYMultiPolygonShapeQueries -Dtests.method=testRandomMedium -Dtests.seed=363603A0428EC788 -Dtests.multiplier=3 -Dtests.slow=true -Dtests.badapples=true -Dtests.locale=sv-SE -Dtests.timezone=America/Yakutat -Dtests.asserts=true -Dtests.file.encoding=UTF-8{code}
 

I dug into the failures and there seem to be due to numerical errors in the GeoUtils.orient method. The method is detecting intersections of two very long lines when it shouldn't. For example:

Line 1: 
{code:java}
double ax = 3.314395000050712E38;
double ay = -1.4151510014141656E37;
double bx = 3.4028234663852886E38;
double by = 9.641030236797581E20;{code}
Line 2:
{code:java}
double cx = 3.4028234663852886E38;
double cy = -0.0;
double dx = 3.4028234663852886E38;
double dy = -2.7386422951137726E38;{code}
My proposal to prevent those numerical errors is to modify the shape generator to prevent creating shapes that expands more than half the float space.

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-01-17 09:48:11.553,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 17 10:36:24 UTC 2020,,New,,,,,,,"0|z0aj28:",9223372036854775807,,,,,,,,,,,,,,,"17/Jan/20 09:48;jpountz;The subtraction by-ay is indeed not accurate already in spite of the promotion from floats to doubles since their exponents differ by more than the number of mantissa bits of a double. And things might get worse with the multiplications. I wonder if your proposal would actually address the problem though, the problem is not that much the absolute values of the coordinates, but rather their relative values. For instance I believe you could have the same issue if we had some coordinates that are close but not equal to zero?

I haven't looked at the test, but how does it know that these lines don't intersect, is it using better logic?","17/Jan/20 10:36;ivera;You are totally right, my proposal does not solve the issue.

What I have done to check that lines do not intersects is to change the intersect logic to use BigDecimals instead of doubles. In that case lines do not intersect (and test don't fail for those seeds).

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
extend TopGroups.merge test coverage,LUCENE-9010,13262696,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Minor,,,cpoerschke,cpoerschke,16/Oct/19 18:22,12/Nov/19 16:57,18/Feb/21 10:07,,,,,,,,0,"This -sub--task proposes to add test coverage for the {{TopGroups.merge}} method, separately from but as preparation for LUCENE-8996 fixing the 'maxScore is sometimes missing' bug.

edit: converted from LUCENE-8996 bug-fix sub-task to test-type task of its own so that its life can extend beyond the LUCENE-8996 bug-fix ticket's life for further test coverage extensions",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/19 10:47;cpoerschke;LUCENE-9010.patch;https://issues.apache.org/jira/secure/attachment/12983268/LUCENE-9010.patch","16/Oct/19 18:23;cpoerschke;LUCENE-9010.patch;https://issues.apache.org/jira/secure/attachment/12983203/LUCENE-9010.patch",,,,2.0,,,,,,,,,,,,,,,,,,,,2019-10-16 23:43:18.958,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 29 16:35:37 UTC 2019,,New,,,,,,,"0|z07nuw:",9223372036854775807,,,,,,,,,,,,,,,"16/Oct/19 18:23;cpoerschke;The attached proposed patch tries to reduce the ""amount of numbers"" in the test e.g. instead of integer group values 1 and 2 there's string group values ""red"" and ""blue"" and a narrative and local variable names (redAntScore, blueDragonflyScore, blueDragonflySize, redSquirrelScore, blueWhaleScore) try to make it easier to work out what the {{expectedMaxScore}} value is.","16/Oct/19 23:43;lucenesolrqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m 10s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red}  0m 46s{color} | {color:red} lucene_grouping generated 4 new + 107 unchanged - 0 fixed = 111 total (was 107) {color} |
| {color:green}+1{color} | {color:green} Release audit (RAT) {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} Check forbidden APIs {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} Validate source patterns {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  1m  3s{color} | {color:green} grouping in the patch passed. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}  5m 10s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | LUCENE-9010 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12983203/LUCENE-9010.patch |
| Optional Tests |  compile  javac  unit  ratsources  checkforbiddenapis  validatesourcepatterns  |
| uname | Linux lucene2-us-west.apache.org 4.4.0-112-generic #135-Ubuntu SMP Fri Jan 19 11:48:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | ant |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-LUCENE-Build/sourcedir/dev-tools/test-patch/lucene-solr-yetus-personality.sh |
| git revision | master / ebc720c |
| ant | version: Apache Ant(TM) version 1.9.6 compiled on July 20 2018 |
| Default Java | LTS |
| javac | https://builds.apache.org/job/PreCommit-LUCENE-Build/210/artifact/out/diff-compile-javac-lucene_grouping.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-LUCENE-Build/210/testReport/ |
| modules | C: lucene/grouping U: lucene/grouping |
| Console output | https://builds.apache.org/job/PreCommit-LUCENE-Build/210/console |
| Powered by | Apache Yetus 0.7.0   http://yetus.apache.org |


This message was automatically generated.

","17/Oct/19 16:06;lucenesolrqa;| (/) *{color:green}+1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} master Compile Tests {color} ||
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 21s{color} | {color:green} master passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} Release audit (RAT) {color} | {color:green}  0m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} Check forbidden APIs {color} | {color:green}  0m 18s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} Validate source patterns {color} | {color:green}  0m 18s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:green}+1{color} | {color:green} unit {color} | {color:green}  0m 17s{color} | {color:green} grouping in the patch passed. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}  2m 37s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | LUCENE-9010 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12983268/LUCENE-9010.patch |
| Optional Tests |  compile  javac  unit  ratsources  checkforbiddenapis  validatesourcepatterns  |
| uname | Linux lucene2-us-west.apache.org 4.4.0-112-generic #135-Ubuntu SMP Fri Jan 19 11:48:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | ant |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-LUCENE-Build/sourcedir/dev-tools/test-patch/lucene-solr-yetus-personality.sh |
| git revision | master / 63e9bcf |
| ant | version: Apache Ant(TM) version 1.9.6 compiled on July 20 2018 |
| Default Java | LTS |
|  Test Results | https://builds.apache.org/job/PreCommit-LUCENE-Build/211/testReport/ |
| modules | C: lucene/grouping U: lucene/grouping |
| Console output | https://builds.apache.org/job/PreCommit-LUCENE-Build/211/console |
| Powered by | Apache Yetus 0.7.0   http://yetus.apache.org |


This message was automatically generated.

","22/Oct/19 15:37;jira-bot;Commit f8292f5372502598dc8cabc50642c4f783e1c811 in lucene-solr's branch refs/heads/master from Christine Poerschke
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=f8292f5 ]

LUCENE-9010: extend TopGroups.merge test coverage
","23/Oct/19 00:30;jira-bot;Commit f8292f5372502598dc8cabc50642c4f783e1c811 in lucene-solr's branch refs/heads/jira/SOLR-13822 from Christine Poerschke
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=f8292f5 ]

LUCENE-9010: extend TopGroups.merge test coverage
","23/Oct/19 11:52;jira-bot;Commit 516f607618a501596272bbfdcb533277072e485e in lucene-solr's branch refs/heads/branch_8x from Christine Poerschke
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=516f607 ]

LUCENE-9010: extend TopGroups.merge test coverage
","23/Oct/19 12:08;jira-bot;Commit 6beac1dc39d3ebadf01335e11f71a19da92a0da3 in lucene-solr's branch refs/heads/branch_8_3 from Christine Poerschke
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=6beac1d ]

LUCENE-9010: extend TopGroups.merge test coverage
","29/Oct/19 16:35;jira-bot;Commit 6beac1dc39d3ebadf01335e11f71a19da92a0da3 in lucene-solr's branch refs/heads/jira/SOLR-13101 from Christine Poerschke
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=6beac1d ]

LUCENE-9010: extend TopGroups.merge test coverage
",,,,,,,,,,,,,,,,,,,,,,,,,,
TestTessellator#testLinesIntersect failure,LUCENE-8744,13224596,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Trivial,,,ivera,ivera,28/Mar/19 14:00,28/Mar/19 14:04,18/Feb/21 10:07,,,,,,,,0,"Reproduce with:
{code:java}
ant test  -Dtestcase=TestTessellator -Dtests.method=testLinesIntersect -Dtests.seed=D8AE5A1A4CA3A81D -Dtests.slow=true -Dtests.badapples=true -Dtests.locale=ar-IQ -Dtests.timezone=Europe/Sarajevo -Dtests.asserts=true -Dtests.file.encoding=UTF-8{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/19 14:03;ivera;LUCENE-8744.patch;https://issues.apache.org/jira/secure/attachment/12964038/LUCENE-8744.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 28 14:04:28 UTC 2019,,New,,,,,,,"0|z017g0:",9223372036854775807,,,,,,,,,,,,,,,"28/Mar/19 14:04;ivera;Test fails because the generated lines are degenerated, values difference of the points are sub-atomic.

The fix proposed changes the visibility of GeoEncodingUtils#LAT_DECODE and  GeoEncodingUtils#LON_DECODE and use that values to check if the provided edges are degenerated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
standardise test class naming,LUCENE-8626,13194968,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,cpoerschke,cpoerschke,29/Oct/18 20:43,22/Jan/21 17:39,18/Feb/21 10:07,,,,,,,,0,"This was mentioned and proposed on the dev mailing list. Starting this ticket here to start to make it happen?

History: This ticket was created as https://issues.apache.org/jira/browse/SOLR-12939 ticket and then got JIRA-moved to become https://issues.apache.org/jira/browse/LUCENE-8626 ticket.",,"MarcusSorealheis opened a new pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026


   <!--
   _(If you are a project committer then you may remove some/all of the following template.)_
   
   Before creating a pull request, please file an issue in the ASF Jira system for Lucene or Solr:
   
   * https://issues.apache.org/jira/projects/LUCENE
   * https://issues.apache.org/jira/projects/SOLR
   
   You will need to create an account in Jira in order to create an issue.
   
   The title of the PR should reference the Jira issue number in the form:
   
   * LUCENE-####: <short description of problem or changes>
   * SOLR-####: <short description of problem or changes>
   
   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.
   
   Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->
   
   
   # Description
   
   Janitor here, moving a closer to a standard test file naming convention, starting with Lucene. 
   
   # Solution
   
   Janitorial work based on the work that @cpoerschke started a couple years ago in the ticket from the title.Almost every change was identical. More to come.
   
   # Tests
   
   These are all tests.
   
   # Checklist
   
   Please review the following and check all that apply:
   
   - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability.
   - [ ] I have created a Jira issue and added the issue ID to my pull request title.
   - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended)
   - [ ] I have developed this patch against the `master` branch.
   - [ ] I have run `./gradlew check`.
   - [ ] I have added tests for my changes.
   - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Oct/20 07:37;githubbot;600","ErickErickson commented on pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026#issuecomment-716144601


   LGTM. We can check these in piecemeal rather than do them all at once, as you say the PRs get massive...


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Oct/20 13:01;githubbot;600","dweiss commented on pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026#issuecomment-716183934


   Wouldn't it be better to add test convention enforcement first, followed by gradual rename of all convention-excluded files? I filed a suggestion on how this could be done - it doesn't have to be that particular one; can be anything but would be better than nothing?


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Oct/20 17:40;githubbot;600","MarcusSorealheis commented on pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026#issuecomment-716184493


   @dweiss I'm of the opinion that they could happen in parallel because we know in which direction we are going given the current state. The enforcement will obviously be a plus, but both the enforcement and the renaming are both in flight and gradually landing.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Oct/20 17:44;githubbot;600","dweiss commented on pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026#issuecomment-716184631


   ok.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;25/Oct/20 17:45;githubbot;600","anshumg commented on a change in pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026#discussion_r511735403



##########
File path: lucene/spatial-extras/src/test/org/apache/lucene/spatial/TestQueryEqualsHashCode.java
##########
@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.spatial;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+import org.apache.lucene.spatial.bbox.BBoxStrategy;
+import org.apache.lucene.spatial.composite.CompositeSpatialStrategy;
+import org.apache.lucene.spatial.prefix.RecursivePrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.TermQueryPrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.tree.GeohashPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.QuadPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.SpatialPrefixTree;
+import org.apache.lucene.spatial.query.SpatialArgs;
+import org.apache.lucene.spatial.query.SpatialOperation;
+import org.apache.lucene.spatial.serialized.SerializedDVStrategy;
+import org.apache.lucene.spatial.vector.PointVectorStrategy;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+import org.locationtech.spatial4j.context.SpatialContext;
+import org.locationtech.spatial4j.shape.Shape;
+
+public class TestQueryEqualsHashCode extends LuceneTestCase {

Review comment:
       Did you intend to add this as a duplicate file?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Oct/20 06:08;githubbot;600","MarcusSorealheis commented on a change in pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026#discussion_r512109714



##########
File path: lucene/spatial-extras/src/test/org/apache/lucene/spatial/TestQueryEqualsHashCode.java
##########
@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.spatial;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+import org.apache.lucene.spatial.bbox.BBoxStrategy;
+import org.apache.lucene.spatial.composite.CompositeSpatialStrategy;
+import org.apache.lucene.spatial.prefix.RecursivePrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.TermQueryPrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.tree.GeohashPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.QuadPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.SpatialPrefixTree;
+import org.apache.lucene.spatial.query.SpatialArgs;
+import org.apache.lucene.spatial.query.SpatialOperation;
+import org.apache.lucene.spatial.serialized.SerializedDVStrategy;
+import org.apache.lucene.spatial.vector.PointVectorStrategy;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+import org.locationtech.spatial4j.context.SpatialContext;
+import org.locationtech.spatial4j.shape.Shape;
+
+public class TestQueryEqualsHashCode extends LuceneTestCase {

Review comment:
       what is the duplicate?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Oct/20 16:47;githubbot;600","MarcusSorealheis commented on a change in pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026#discussion_r512111030



##########
File path: lucene/spatial-extras/src/test/org/apache/lucene/spatial/TestQueryEqualsHashCode.java
##########
@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.spatial;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+import org.apache.lucene.spatial.bbox.BBoxStrategy;
+import org.apache.lucene.spatial.composite.CompositeSpatialStrategy;
+import org.apache.lucene.spatial.prefix.RecursivePrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.TermQueryPrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.tree.GeohashPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.QuadPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.SpatialPrefixTree;
+import org.apache.lucene.spatial.query.SpatialArgs;
+import org.apache.lucene.spatial.query.SpatialOperation;
+import org.apache.lucene.spatial.serialized.SerializedDVStrategy;
+import org.apache.lucene.spatial.vector.PointVectorStrategy;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+import org.locationtech.spatial4j.context.SpatialContext;
+import org.locationtech.spatial4j.shape.Shape;
+
+public class TestQueryEqualsHashCode extends LuceneTestCase {

Review comment:
       I intended to remove the previous file `lucene/spatial-extras/src/test/org/apache/lucene/spatial/QueryEqualsHashCodeTest.java`




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Oct/20 16:48;githubbot;600","MarcusSorealheis commented on a change in pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026#discussion_r512115584



##########
File path: lucene/spatial-extras/src/test/org/apache/lucene/spatial/TestQueryEqualsHashCode.java
##########
@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.spatial;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+import org.apache.lucene.spatial.bbox.BBoxStrategy;
+import org.apache.lucene.spatial.composite.CompositeSpatialStrategy;
+import org.apache.lucene.spatial.prefix.RecursivePrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.TermQueryPrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.tree.GeohashPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.QuadPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.SpatialPrefixTree;
+import org.apache.lucene.spatial.query.SpatialArgs;
+import org.apache.lucene.spatial.query.SpatialOperation;
+import org.apache.lucene.spatial.serialized.SerializedDVStrategy;
+import org.apache.lucene.spatial.vector.PointVectorStrategy;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+import org.locationtech.spatial4j.context.SpatialContext;
+import org.locationtech.spatial4j.shape.Shape;
+
+public class TestQueryEqualsHashCode extends LuceneTestCase {

Review comment:
       I see, I didn't remove the original in that case good catch




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Oct/20 16:52;githubbot;600","MarcusSorealheis commented on a change in pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026#discussion_r512116132



##########
File path: lucene/spatial-extras/src/test/org/apache/lucene/spatial/TestQueryEqualsHashCode.java
##########
@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.spatial;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+import org.apache.lucene.spatial.bbox.BBoxStrategy;
+import org.apache.lucene.spatial.composite.CompositeSpatialStrategy;
+import org.apache.lucene.spatial.prefix.RecursivePrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.TermQueryPrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.tree.GeohashPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.QuadPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.SpatialPrefixTree;
+import org.apache.lucene.spatial.query.SpatialArgs;
+import org.apache.lucene.spatial.query.SpatialOperation;
+import org.apache.lucene.spatial.serialized.SerializedDVStrategy;
+import org.apache.lucene.spatial.vector.PointVectorStrategy;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+import org.locationtech.spatial4j.context.SpatialContext;
+import org.locationtech.spatial4j.shape.Shape;
+
+public class TestQueryEqualsHashCode extends LuceneTestCase {

Review comment:
       that was a `git fart` on my end. 

##########
File path: lucene/spatial-extras/src/test/org/apache/lucene/spatial/TestQueryEqualsHashCode.java
##########
@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.spatial;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+import org.apache.lucene.spatial.bbox.BBoxStrategy;
+import org.apache.lucene.spatial.composite.CompositeSpatialStrategy;
+import org.apache.lucene.spatial.prefix.RecursivePrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.TermQueryPrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.tree.GeohashPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.QuadPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.SpatialPrefixTree;
+import org.apache.lucene.spatial.query.SpatialArgs;
+import org.apache.lucene.spatial.query.SpatialOperation;
+import org.apache.lucene.spatial.serialized.SerializedDVStrategy;
+import org.apache.lucene.spatial.vector.PointVectorStrategy;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+import org.locationtech.spatial4j.context.SpatialContext;
+import org.locationtech.spatial4j.shape.Shape;
+
+public class TestQueryEqualsHashCode extends LuceneTestCase {

Review comment:
       that's why I am keeping these small




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Oct/20 16:53;githubbot;600","MarcusSorealheis commented on a change in pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026#discussion_r512116132



##########
File path: lucene/spatial-extras/src/test/org/apache/lucene/spatial/TestQueryEqualsHashCode.java
##########
@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.spatial;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+import org.apache.lucene.spatial.bbox.BBoxStrategy;
+import org.apache.lucene.spatial.composite.CompositeSpatialStrategy;
+import org.apache.lucene.spatial.prefix.RecursivePrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.TermQueryPrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.tree.GeohashPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.QuadPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.SpatialPrefixTree;
+import org.apache.lucene.spatial.query.SpatialArgs;
+import org.apache.lucene.spatial.query.SpatialOperation;
+import org.apache.lucene.spatial.serialized.SerializedDVStrategy;
+import org.apache.lucene.spatial.vector.PointVectorStrategy;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+import org.locationtech.spatial4j.context.SpatialContext;
+import org.locationtech.spatial4j.shape.Shape;
+
+public class TestQueryEqualsHashCode extends LuceneTestCase {

Review comment:
       that was a `git fart` on my end. 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Oct/20 06:29;githubbot;600","MarcusSorealheis commented on a change in pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026#discussion_r512116365



##########
File path: lucene/spatial-extras/src/test/org/apache/lucene/spatial/TestQueryEqualsHashCode.java
##########
@@ -0,0 +1,119 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.lucene.spatial;
+
+import java.util.ArrayList;
+import java.util.Collection;
+
+import org.apache.lucene.spatial.bbox.BBoxStrategy;
+import org.apache.lucene.spatial.composite.CompositeSpatialStrategy;
+import org.apache.lucene.spatial.prefix.RecursivePrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.TermQueryPrefixTreeStrategy;
+import org.apache.lucene.spatial.prefix.tree.GeohashPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.QuadPrefixTree;
+import org.apache.lucene.spatial.prefix.tree.SpatialPrefixTree;
+import org.apache.lucene.spatial.query.SpatialArgs;
+import org.apache.lucene.spatial.query.SpatialOperation;
+import org.apache.lucene.spatial.serialized.SerializedDVStrategy;
+import org.apache.lucene.spatial.vector.PointVectorStrategy;
+import org.apache.lucene.util.LuceneTestCase;
+import org.junit.Test;
+import org.locationtech.spatial4j.context.SpatialContext;
+import org.locationtech.spatial4j.shape.Shape;
+
+public class TestQueryEqualsHashCode extends LuceneTestCase {

Review comment:
       that's why I am keeping these small




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/Oct/20 06:29;githubbot;600","CaoManhDat merged pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/20 06:35;githubbot;600","CaoManhDat commented on pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026#issuecomment-719286855


   Thank you @MarcusSorealheis 


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/20 06:36;githubbot;600","CaoManhDat edited a comment on pull request #2026:
URL: https://github.com/apache/lucene-solr/pull/2026#issuecomment-719286855


   Thank you @MarcusSorealheis and everyone


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/20 06:36;githubbot;600","msokolov merged pull request #2053:
URL: https://github.com/apache/lucene-solr/pull/2053


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Nov/20 13:13;githubbot;600","MarcusSorealheis opened a new pull request #2220:
URL: https://github.com/apache/lucene-solr/pull/2220


   <!--
   _(If you are a project committer then you may remove some/all of the following template.)_
   
   Before creating a pull request, please file an issue in the ASF Jira system for Lucene or Solr:
   
   * https://issues.apache.org/jira/projects/LUCENE
   * https://issues.apache.org/jira/projects/SOLR
   
   You will need to create an account in Jira in order to create an issue.
   
   The title of the PR should reference the Jira issue number in the form:
   
   * LUCENE-####: <short description of problem or changes>
   * SOLR-####: <short description of problem or changes>
   
   LUCENE and SOLR must be fully capitalized. A short description helps people scanning pull requests for items they can work on.
   
   Properly referencing the issue in the title ensures that Jira is correctly updated with code review comments and commits. -->
   
   
   # Description
   
   I've added the final set of standardizations to test files in Lucene so that they conform to the new pattern.
   
   # Solution
   
   Lead with Test instead of `{something}Test.java`
   
   # Tests
   
   All files here are test classes. 
   
   # Checklist
   
   Please review the following and check all that apply:
   
   - [ ] I have reviewed the guidelines for [How to Contribute](https://wiki.apache.org/solr/HowToContribute) and my code conforms to the standards described there to the best of my ability.
   - [ ] I have created a Jira issue and added the issue ID to my pull request title.
   - [ ] I have given Solr maintainers [access](https://help.github.com/en/articles/allowing-changes-to-a-pull-request-branch-created-from-a-fork) to contribute to my PR branch. (optional but recommended)
   - [ ] I have developed this patch against the `master` branch.
   - [ ] I have run `./gradlew check`.
   - [ ] I have added tests for my changes.
   - [ ] I have added documentation for the [Ref Guide](https://github.com/apache/lucene-solr/tree/master/solr/solr-ref-guide) (for Solr changes only).
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jan/21 07:22;githubbot;600","MarcusSorealheis commented on pull request #2220:
URL: https://github.com/apache/lucene-solr/pull/2220#issuecomment-762655240


   also, for a bit of fun to show the dirty work is done: 
   ![image](https://user-images.githubusercontent.com/2353608/105001369-84c74700-59e4-11eb-9b6d-3a68fe2257cb.png)
   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jan/21 07:26;githubbot;600","MarcusSorealheis commented on pull request #2220:
URL: https://github.com/apache/lucene-solr/pull/2220#issuecomment-762850831


   I'll fix the test failure


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Jan/21 13:51;githubbot;600","erikhatcher commented on pull request #2220:
URL: https://github.com/apache/lucene-solr/pull/2220#issuecomment-763715268


   lgtm!


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jan/21 15:36;githubbot;600","MarcusSorealheis commented on pull request #2220:
URL: https://github.com/apache/lucene-solr/pull/2220#issuecomment-764035674


   > also, for a bit of fun to show the dirty work is done:
   > ![image](https://user-images.githubusercontent.com/2353608/105001369-84c74700-59e4-11eb-9b6d-3a68fe2257cb.png)
   
   You will notice this file returned is not actually a test class.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jan/21 23:48;githubbot;600","MarcusSorealheis commented on pull request #2220:
URL: https://github.com/apache/lucene-solr/pull/2220#issuecomment-764035674


   > also, for a bit of fun to show the dirty work is done:
   > ![image](https://user-images.githubusercontent.com/2353608/105001369-84c74700-59e4-11eb-9b6d-3a68fe2257cb.png)
   
   You will notice this file returned is not actually a test class.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/21 05:31;githubbot;600","msokolov commented on a change in pull request #2220:
URL: https://github.com/apache/lucene-solr/pull/2220#discussion_r562792732



##########
File path: lucene/queryparser/src/test/org/apache/lucene/queryparser/surround/query/Test02Boolean.java
##########
@@ -46,11 +46,11 @@ public void setUp() throws Exception {
   SingleFieldTestDb db1;
 
   public void normalTest1(String query, int[] expdnrs) throws Exception {
-    BooleanQueryTst bqt =
-        new BooleanQueryTst(
+    TestBooleanQuery tbq =

Review comment:
       ooh fancy




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/21 17:36;githubbot;600","msokolov merged pull request #2220:
URL: https://github.com/apache/lucene-solr/pull/2220


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Jan/21 17:39;githubbot;600",,0,14400,,,0,14400,,,SOLR-13096,,,"29/Oct/18 20:44;cpoerschke;SOLR-12939.01.patch;https://issues.apache.org/jira/secure/attachment/12946106/SOLR-12939.01.patch","30/Oct/18 21:04;cpoerschke;SOLR-12939.02.patch;https://issues.apache.org/jira/secure/attachment/12946291/SOLR-12939.02.patch","28/Dec/18 17:24;cpoerschke;SOLR-12939.03.patch;https://issues.apache.org/jira/secure/attachment/12953255/SOLR-12939.03.patch","31/Oct/18 17:17;hossman;SOLR-12939_hoss_validation_groovy_experiment.patch;https://issues.apache.org/jira/secure/attachment/12946415/SOLR-12939_hoss_validation_groovy_experiment.patch",,4.0,,,,,,,,,,,,,,,,,,,,2018-10-29 21:33:56.578,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 22 17:39:16 UTC 2021,,,,,,,,,"0|i3zrxb:",9223372036854775807,,,,,,,,,,,,,,,"29/Oct/18 20:45;cpoerschke;The current {{TestFooBar : FooBarTest}} ratio is approximately 2:1 - so would it make sense to standardise on the former i.e. to prefer {{TestFooBar.java}} over {{FooBarTest.java}}?

Attached SOLR-12939.01.patch is one way we could incrementally standardise directories (and then keep them standardised going forward).","29/Oct/18 21:33;gus;At the risk of venturing into a tabs vs spaces sort of thing, I have found that many online examples use the Test suffix rather than prefix.  For example:
http://www.mkyong.com/unittest/junit-4-tutorial-1-basic-usage/

Also the junit team themselves used *Test 
https://github.com/junit-team/junit4/tree/master/src/test/java/org/junit/internal

When I first had contact with the code I was shocked to see Test* classes. YMMV here's hoping I don't start a silly holy war :)","30/Oct/18 21:03;cpoerschke;bq. ... the Test suffix rather than prefix. ...

I have no strong views either way. Instead of choosing one or the other a hybrid solution could also be to permit both but to enforce that within a given directory/sub-package/package there is consistency.

Illustrative SOLR-12939.02.patch patch to follow on how {{ant validate-source-patterns}} (which runs as part of {{ant precommit}}) could incrementally enforce that.","31/Oct/18 16:06;dsmiley;I concur with Gus.  If we're going to standardize, I've seen explicit code styles and other projects that have Test as the suffix and I think we should follow suite.  Though I admit it may not matter what we standardize on.

Thanks for working on the patch Christine.","31/Oct/18 17:26;hossman;a few thoughts...

# if we're going to have a standard naming convention for tests, then it seems like it needs to be across the entire codebase, so this should probably be a LUCENE level issue, not a SOLR issue?
# i'm not sure if a ""source"" level validation will go far enough for enforcing this -- to be really useful, whatever ""rule"" we bikeshe as far as how tests will be named needs to be enforcible to the point that anything matching that rule *must* be a runnable test (ie: not an abstract base class for tests, and not a collection of helper utility methods)

I'm attaching a patch where i breifly experimented with a groovy script to walk all compiled class files and then use reflection to try and validate the properties of the class against expecations of it's name. 

I think the results are promosing, it works fine on things like lucene-core, but i've run into some sort of classpath/classloading related problem with some of the solr tests variable references between tests and code ...  probably some silly classpath mistake i'm making, or something i'm missunderstanding about how ant/groovy sets up the classpath the shell runs in ... either way it's probably solvable.

In any case ... i wanted to put this out there as a POC for thought/discussion in case anyone wants to pick it up and run with it ... i probably won't be focusing much time on this specifically.

","28/Dec/18 17:26;cpoerschke;bq. ... across the entire codebase, so this should probably be a LUCENE level issue, not a SOLR issue?

Makes sense, I'll try to JIRA-move the ticket from SOLR to LUCENE, that _should_ work including forwarding of any SOLR-12939 links.

bq. ... i'm not sure if a ""source"" level validation will go far enough for enforcing this ...

Sure, beyond (or indeed instead of) test class name standardisation more things are possible. My intention here was purely to start somewhere.

The latest patch (SOLR-12939.03.patch) does away with the idea of explicitly enforcing a convention for the parts of the code base that already follow the convention but instead it simply overall counts un-conventionality (anti patterns). If the overall count is 'too high' (e.g. likely new test files don't follow the convention) then {{ant validate-source-patterns}} will fail e.g.
{code}
...
validate-source-patterns:

BUILD FAILED
/Users/cpoerschke/lucene-solr/build.xml:128: Found too many (1607 > 1606) anti patterns (prefer FooBarTest.java over TestFooBar.java for new tests).
...
{code}

Could such logic be helpful (in avoiding the addition of new {{TestFooBar.java}} test files) and/or might the fuzziness of the logic cause too much confusion?

bq. ... I'm attaching a patch where i breifly experimented with a groovy script ... i wanted to put this out there as a POC for thought/discussion in case anyone wants to pick it up and run with it ...

Thanks for sharing!","06/Aug/20 06:07;rcmuir;I'm gonna reiterate my numbers: 2/3 of all tests use the style starting with Test*:
{noformat}
lucene-solr[master]$ find . -name ""Test*.java""  | wc -l
1739
lucene-solr[master]$ find . -name ""*Test.java""  | wc -l
891
{noformat}

Within the lucene test suite, it is nearly 90%. It is really only a few activists using a different style:
{noformat}
think:lucene[master]$ find . -name ""Test*.java"" | wc -l
1215
think:lucene[master]$ find . -name ""*Test.java"" | wc -l
157
{noformat}

Within the solr test suite, things are less consistent:
{noformat}
think:solr[master]$ find . -name ""Test*.java"" | wc -l
524
think:solr[master]$ find . -name ""*Test.java"" | wc -l
734
{noformat}

So I don't think we should use ""Test"" as a suffix with no scientific reason when most tests (especially the ones that work consistently) are doing the opposite.
","06/Aug/20 07:55;cpoerschke;bq. This was mentioned and proposed on the dev mailing list. ...

The ""Test Harness behaviour on a package run"" [thread| https://lists.apache.org/thread.html/3a8e38e6ca9abe2e4be0c12cfd23d103cf60d0891c54df45e9c7bf18%40%3Cdev.lucene.apache.org%3E] had led to the ticket here at the end of 2018.

Great to see the interest and effort resume via the ""Standardize Leading Test or Trailing Test"" [thread|https://lists.apache.org/thread.html/rde0276272a86582c5e6f9456ad592233c9ed575579a0f812d88be486%40%3Cdev.lucene.apache.org%3E] now.","06/Aug/20 16:51;sokolov;Personally, I prefer -prefixing- suffixing (um suffix is the one that comes after, right?), but more than that, I'd value consistency. Still, without automated enforcement, we won't get that either. So, I'd be -0 to this change unless it comes along with enforcement (banning files with the nonstandard naming scheme). Otherwise we'll just be back here again in a year...  uh, actually reading the thread now I see we do have an enforcement mechanism, OK that's great. If we can come to some consensus here, then rename away!","06/Aug/20 17:01;erickerickson;Contrariwise, I prefer suffixing on the theory that when looking at a directory, 
TestFoo
TestBar
TestBlivet

requires me to read past the ""Test"" before being able to see the class name, whereas FooTest, BarTest, BlivetTest is easier on the eyes. 

That said, I'll abide by whatever the person leading the charge decides, Michael Sokolov's comment about valuing consistency is germane.

","06/Aug/20 21:03;erickerickson;Hmmm, one other random thought. While I'd prefer some kind of enforcement, I'd claim that if we just changed the test file names however we agree, the fact that they'd all be consistent would make it less likely that new test classes are created with the abandoned pattern. I think it's worth making the change first, then worrying about enforcement.","06/Aug/20 23:01;marcussorealheis;[~cpoerschke] Thank you very much for kicking this effort off back in 2018. This issue has many implications for problems. And for what I am working on, there is a detriment to my productivity because of it. I'm sure I am not alone. 

If you don't mind, I'd like to take this effort a bit further and to completion via a PR. ","06/Aug/20 23:03;marcussorealheis;There are many many areas where I am looking to improve the developer experience and the code hygiene. I'm not some guru of clean code or anything, but I am starting go through my laundry list of things that drive me (and others) nuts and reduces the overall quality of the project. 

I intend to add a pre-commit check to enforce this and other standards as they come through.","07/Aug/20 15:54;cpoerschke;{quote}Thank you very much for kicking this effort off back in 2018. ... If you don't mind, I'd like to take this effort a bit further and to completion via a PR.
{quote}
No problem. I didn't and don't have the bandwidth to fully see this through to completion myself. My hopes for the ticket were and are simply for it be to a collaborative open space where multiple folks could contribute:
 * analysis of what we have now
 * thoughts on whether or not to standardise
 * thoughts on what to standardise to and why
 * independent incremental test renaming
 ** if now renaming a small number of classes helps remove active productivity detriments in a particular area of code then that could be worth doing independently
 ** if then later renaming of a larger number of classes (for the overall standardisation) renames a few classes back to their previous name then so be it
 * code snippets (groovy, shell script, algorithm ideas) on how standardisation might be enforced
 * other stuff","07/Aug/20 15:55;cpoerschke;bq. Within the lucene test suite, it is nearly 90%. ...
bq. Within the solr test suite, things are less consistent: ...

What are people's thoughts on ""lucene"" and ""solr"" standardising to the same naming, or not?

To me it seems ""lucene"" standardising on X and ""solr"" standardising on Y could be confusing but it would still be clearer than neither of them being standardised.","07/Aug/20 16:35;erickerickson;Both should be the same IMO.","10/Aug/20 18:29;dweiss;bq. Still, without automated enforcement

For LuceneTestCase subclasses an automatic enforcement of this is trivial: add a test rule (or before class hook) that checks test class name (it can go up the chain of superclasses but doesn't have to). The benefit of doing this vs. file name checks is that actual test suites would be verified - not any other class that isn't a test suite.

It would also work across all projects. Including those that import lucene-test-framework (which may be problematic for people?).","12/Aug/20 05:29;dsmiley;I request that the Solr side be delayed some, and maybe get its own issue.  The reason for my request is [~markrmiller@gmail.com]'s massive branch ""reference_impl"" which he writes about SOLR-14636.  It's unclear how/when master & this branch will somehow get reconciled, but presently I suggest using caution when doing very wide-scale changes, _especially_ for renames or moves.","12/Aug/20 06:47;marcussorealheis;Great ideas [~dweiss] and [~dsmiley]!","12/Aug/20 07:32;dweiss;Pushed a PR for consideration on how this can be done incrementally (but enforced for anything added from now on).","12/Aug/20 07:34;dweiss;As for the convention itself, I'm myself more of a suffix-style kind of guy (so that prefix class searches work  for both the class and its test in any environment). I'll follow what's agreed upon though.","12/Aug/20 13:07;erickerickson;+100 to David's comment. We should refrain from wholesale changes like this until we figure out what to do with Mark's reference impl. I've been hoping that all the files I changed with the SuppressWarnings won't be a problem in that regard, I didn't realize at the time that the reference impl diverge this long....","12/Aug/20 14:53;cpoerschke;Looking for tests where we currently have both {{TestFooBar.java}} and {{FooBarTest.java}} present via something like

{code}
(find . -name ""*Test.java"" ; find . -name ""Test*.java"") | sed 's/Test//g' | sort | uniq -c | sort -nr | grep -v 1
{code}

finds 4 matches

{code}
   2 ./solr/core/src/test/org/apache/solr/core/DirectoryFactory.java
   2 ./solr/core/src/test/org/apache/solr/cloud/ConfigSetsAPI.java
   2 ./solr/core/src/test/org/apache/solr/SolrCaseJ4.java
   2 ./lucene/core/src/test/org/apache/lucene/search/MultiCollector.java
{code}

and for the {{MultiCollector}} one I've just opened https://github.com/apache/lucene-solr/pull/1745 PR. Haven't looked yet if/how the other 3 might be handled.
","12/Aug/20 19:38;mikemccand;{quote}I'm myself more of a suffix-style kind of guy (so that prefix class searches work for both the class and its test in any environment)
{quote}
I had thought this issue was really a butter side up / Sneetches / Law of Triviality / bike shedding sort of situation, but it is not ;)

This justification makes sense to me, so I think suffix is indeed better than prefix, as long as we can 1) do the renaming at once, and 2) enforce that consistent naming going forwards.

+1 to rename all of Lucene's tests to use {{Test.java}} suffix.","12/Aug/20 20:08;dweiss;bq. 1) do the renaming at once

I pushed the PR which makes it easier to do it in smaller steps (enforces any new classes to follow the convention though). Lucene may be easy as patches are smaller; Solr may be more problematic.","12/Aug/20 21:17;mikemccand;Ahh thanks [~dweiss], yeah +1 for this approach – progress not perfection!

Hmm did you link the PR here?  OK looks like: [https://github.com/apache/lucene-solr/pull/1743]

I left a couple comments on the PR.","30/Oct/20 06:35;jira-bot;Commit 57729c9acaace26f37644c42a0b0889508e589ba in lucene-solr's branch refs/heads/master from Marcus
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=57729c9 ]

LUCENE-8626: Standardize Lucene Test Files (#2026)

","01/Nov/20 22:26;marcussorealheis;[I added another 36 tests to this ticket|https://github.com/apache/lucene-solr/pull/2053]","02/Nov/20 16:37;cpoerschke;bq. Looking for tests where we currently have both {{TestFooBar.java}} and {{FooBarTest.java}} present ...

These have now been taken care of via one append and three renames:
* https://github.com/apache/lucene-solr/pull/1745
* https://github.com/apache/lucene-solr/pull/1790
* https://github.com/apache/lucene-solr/pull/1890
* https://github.com/apache/lucene-solr/pull/2032
","02/Nov/20 16:50;cpoerschke;bq. ... the PR which makes it easier to do it in smaller steps (enforces any new classes to follow the convention though). ...

[~dweiss] - I pushed a commit to your PR -- https://github.com/apache/lucene-solr/pull/1743 -- which aims to avoid the ""both {{TestFooBar.java}} and {{FooBarTest.java}} present"" scenario being reintroduced. Hope you don't mind.","02/Nov/20 18:56;dweiss;I don't mind at all, Christine. Thank you for picking this up.","17/Nov/20 13:13;jira-bot;Commit b9a93cf6952fa785fc5bf4d95f15e978e269baf2 in lucene-solr's branch refs/heads/master from Marcus
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=b9a93cf ]

LUCENE-8626: Standardize Lucene test file naming Part 2 (#2053)

","20/Jan/21 20:51;marcussorealheis;I've added the final batch of standardized tests.  No more inconsistencies.

https://github.com/apache/lucene-solr/pull/2220","22/Jan/21 17:39;jira-bot;Commit 4bc5d51494c0c0441d58b0e030da8f843d9b9897 in lucene-solr's branch refs/heads/master from Marcus
[ https://gitbox.apache.org/repos/asf?p=lucene-solr.git;h=4bc5d51 ]

LUCENE-8626: Lucene standardize test naming part 3 and final (#2220)

"
Add a build target that beasts all of the tests modified in the last commit.,LUCENE-8547,13194172,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,markrmiller@gmail.com,markrmiller@gmail.com,25/Oct/18 15:20,18/Dec/19 13:30,18/Feb/21 10:07,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-10-25 15:20:05.0,,New,,,,,,,"0|i3zn0n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Precommit could check for any tests in the latest commit and do a cursory beasting.,LUCENE-8545,13194018,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,markrmiller@gmail.com,markrmiller@gmail.com,25/Oct/18 00:25,25/Oct/18 15:20,18/Feb/21 10:07,,,,,,,,0,"Many tests are not currently beastable with the build beast target because they are not made in a way that they can be called repeatedly with a clean env.

We should consider letting precommit lightly beast any test in the last commit - this will ensure tests can be beasted with the build beaster and gives a little protection against some of the really flakey tests that get committed.

I have a little prototype that still needs a bit of work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LUCENE-8547,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-10-25 00:25:42.0,,New,,,,,,,"0|i3zm2f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make TestRandomChains check that filters preserve positions,LUCENE-8361,13166656,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Minor,,romseygeek,jpountz,jpountz,18/Jun/18 08:50,29/Jun/18 13:30,18/Feb/21 10:07,,,,,,,,0,Follow-up of LUCENE-8360: it is a bit disappointing that we only found this issue because of a newly introduced token filter. I'm wondering that we might be able to make TestRandomChains detect more bugs by verifying that the sum of position increments is preserved through the whole analysis chain.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/18 13:08;romseygeek;LUCENE-8361.patch;https://issues.apache.org/jira/secure/attachment/12928196/LUCENE-8361.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,2018-06-18 13:13:34.743,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 29 13:30:38 UTC 2018,,New,,,,,,,"0|i3uyq7:",9223372036854775807,,,,,,,,,,,,,,,"18/Jun/18 13:13;romseygeek;Here's a patch that adds a step to {{BaseTokenStreamTestCase.checkAnalysisConsistency}}.

I've already found a failure in MinHashTokenFilter, which raises the question of whether we still expect end() to report the total number of original tokens for filters that summarise the entire stream - the same will apply to FingerprintFilter and ConcatenateGraphFilter.  I'm minded to just exclude filter chains that contain any of these from the test.","21/Jun/18 13:47;romseygeek;TestRandomChains found another failure due to this, in LimitTokenOffsetFilter.","21/Jun/18 14:08;rcmuir;Both those limit-filters are broken and buggy by default, they won't consume all the tokens unless you pass some special boolean.","29/Jun/18 11:23;romseygeek;bq. Both those limit-filters are broken and buggy by default

They're designed to short-cut tokenization (mainly for highlighting, I think) - do we have a non-buggy way of not consuming all tokens?  Because I can see that it's a valid thing to do in some circumstances.","29/Jun/18 12:35;jpountz;We should disable some checks when the analysis chain includes one of these token filters that truncate the token stream. If we made them correct so that they consumed the wrapped token stream to eg. have access to the end offset, it would defeat their purpose a bit, which is to reduce the amount of work to do.","29/Jun/18 13:08;rcmuir;Strongly disagree with making our tests wimpy because of these buggy filters. 

These filters have the ability to be correct or incorrect, the problem is that they choose to default to incorrect behavior. 
They should default to correct instead, and the ctor with the boolean option should be blacklisted in the test (since its known to be buggy).
They will still achieve their purpose, but yeah it means the user should pay more attention to where they sit in the analysis chain and so on, that's all.


","29/Jun/18 13:09;rcmuir;It is also bad they behave this way by default as I imagine it only causes problems for highlighting fields with multiple values and so on if they are used.","29/Jun/18 13:18;rcmuir;{quote}
They're designed to short-cut tokenization (mainly for highlighting, I think) - do we have a non-buggy way of not consuming all tokens? Because I can see that it's a valid thing to do in some circumstances.
{quote}

Yes, these filters have a boolean option to do this correctly. Its just not the default. This is really too bad, since somehow these bugs (which are like implementation details of how particular highlighters worked) made their way into the analysis module in such a way that its easy to put wrong offsets into your index.","29/Jun/18 13:21;jpountz;Only blacklisting the bad constructor works for me too.","29/Jun/18 13:25;rcmuir;Yes, the issue is that currently, all the constructors are bad :)","29/Jun/18 13:30;rcmuir;Even if we fix the default ctor to behave as correct=true, I think these filters will still have problems because they don't do anything with position increments? So they can easily stop at the wrong place (eg middle of a graph), cause wrong posLengths, etc.",,,,,,,,,,,,,,,,,,,,,,,
modified test for testing payload,LUCENE-7812,13068118,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Minor,,ehatcher,mgainty@hotmail.com,mgainty@hotmail.com,01/May/17 10:53,20/May/17 00:37,18/Feb/21 10:07,,6.5.1,,,,core/queryparser,,0," no specific test for SpanPayloadCheckQuery method specifically

matches = payloadToMatch.get(upto).bytesEquals(payload);

the only implementation i could locate was in collectLeaf of SpanPayloadCheckQuery

modified SpanPayloadCheckQueryTest now tests collectLeaf of SpanPayloadCheckQuery

patch for test attached

suggestions/advice are most welcome","jdk 1.8
mvn 3.3",,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,"01/May/17 10:56;mgainty@hotmail.com;TestPayloadCheckQuery.java;https://issues.apache.org/jira/secure/attachment/12865754/TestPayloadCheckQuery.java",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,2017-05-01 10:53:15.0,,New,Patch Available,,,,,,"0|i3ebav:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve random testing for GeoPointPointInPolygonQuery and DimensionalPointInPolygonQuery,LUCENE-6935,12922393,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Minor,,,nknize,nknize,16/Dec/15 14:51,01/Feb/16 17:41,18/Feb/21 10:07,,,,,,modules/spatial,,0,Currently {{TestGeoPointQuery}} and {{TestDimensionalQueries}} only create bounding box polygons to test {{PointInPolygonQuery}}. This provides coverage for testing basic functionality but lacks critical testing with true random shapes. Random shape generation needs to be added to {{BaseGeoPointTestCase}} and used in {{newPolygonQuery}} for GeoPoint and DimensionalQuery testing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-12-16 14:52:47.257,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 16 14:52:47 UTC 2015,,New,,,,,,,"0|i2pzdb:",9223372036854775807,,,,,,,,,,,,,,,"16/Dec/15 14:52;mikemccand;+1, tests are weak now for ""interesting"" polygons ...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fail tests on threadlocal leaks,LUCENE-6335,12779308,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,04/Mar/15 06:09,05/Mar/15 17:15,18/Feb/21 10:07,,,,,,,,0,"I know this requires us to do things like close our Analyzers in tests, but I think we should do it. just experimenting i found other leaks, e.g. in LineFileDocs.

{noformat}
   [junit4] ERROR   0.00s J0 | TestForceMergeForever (suite) <<<
   [junit4]    > Throwable #1: java.lang.IllegalStateException: ThreadLocal leaks were found: 
   [junit4]    > 1. thread=SUITE-TestForceMergeForever-seed#[AF7141C55A57350E]-worker value=WeakReference<HashMap<?,Analyzer$TokenStreamComponents>>
   [junit4]    > 2. thread=SUITE-TestForceMergeForever-seed#[AF7141C55A57350E]-worker value=LineFileDocs$DocState
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/15 02:02;rcmuir;LUCENE-6335.patch;https://issues.apache.org/jira/secure/attachment/12702668/LUCENE-6335.patch","04/Mar/15 07:11;rcmuir;LUCENE-6335.patch;https://issues.apache.org/jira/secure/attachment/12702395/LUCENE-6335.patch","04/Mar/15 06:12;rcmuir;LUCENE-6335.patch;https://issues.apache.org/jira/secure/attachment/12702384/LUCENE-6335.patch",,,3.0,,,,,,,,,,,,,,,,,,,,2015-03-05 08:10:46.098,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 17:15:40 UTC 2015,,New,,,,,,,"0|i26bzj:",9223372036854775807,,,,,,,,,,,,,,,"04/Mar/15 06:12;rcmuir;here is a REALLY hacky patch. I based it on tomcat's threadlocal leak detector, but with some simpler/imperfect logic as a start, since e.g. we aren't wrapping tests in custom classloader or any of that stuff.

I only fixed TestDemo and LineFileDocs so far and didnt look much more at failures.","04/Mar/15 06:21;rcmuir;A lot of the hacky stuff was to get something useful enough to track down the leak. It makes the code ugly to print ""WeakReference<HashMap<?,Analyzer$TokenStreamComponents>>"" but this is much more useful than just ""java.lang.ref.WeakReference""","04/Mar/15 07:11;rcmuir;updated patch: I started fixing some of the analyzers tests.","05/Mar/15 02:02;rcmuir;Updated patch. I fixed a few tests. There are still lots of leaks and systematic problems.

Only one real bugfix, SynonymFilterFactory leaked an analyzer it created internally. Otherwise it is just test fixes... I plan to commit these test fixes soon and just update patch with the test-framework stuff.","05/Mar/15 08:10;dweiss;I reviewed the test rule. It looks crazily bound to the default JDK implementation (all that reflection) but there seems to be no other way to get hold of the thread locals map, so be it.

Overall it looks good. I'm slightly concerned with this:
{code}
+    ClassLoader cl = clazz.getClassLoader();
+    // our crazy heuristic: threadlocal contains something from non-bootstrap loader.
+    if (cl != null) {
+      throw new ThreadLocalLeakException(getPrettyClassName(clazz));
+    }
{code}
because I planned to isolate the test runner's classes from the test classes at some point by forking a separate classloader... But there are workarounds for this and these can be applied and explored when the actual problem shows up.

+1","05/Mar/15 08:37;jpountz;bq.  I plan to commit these test fixes soon and just update patch with the test-framework stuff.

+1","05/Mar/15 12:20;rcmuir;{quote}
because I planned to isolate the test runner's classes from the test classes at some point by forking a separate classloader... But there are workarounds for this and these can be applied and explored when the actual problem shows up.
{quote}

Tomcat code is actually doing all this in a custom classloader.

But yeah, there are thousands more tests (and some real bugs) to fix before we can add the test rule. I have concerns about this craziness too, but it does seem to work at least for our purposes in lucene.","05/Mar/15 16:56;jira-bot;Commit 1664404 from [~rcmuir] in branch 'dev/trunk'
[ https://svn.apache.org/r1664404 ]

LUCENE-6335: test fixes, and one real fix to synonymfilterfactory (missing analyzer.close)","05/Mar/15 17:15;jira-bot;Commit 1664421 from [~rcmuir] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1664421 ]

LUCENE-6335: test fixes, and one real fix to synonymfilterfactory (missing analyzer.close)",,,,,,,,,,,,,,,,,,,,,,,,,
Rename TestRuleTemporaryFilesCleanup,LUCENE-6267,12776441,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,20/Feb/15 13:30,20/Feb/15 13:59,18/Feb/21 10:07,,,,,,,,0,"As mentioned on LUCENE-6264, this is no longer a good representation of what this thing does.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-02-20 13:51:00.631,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 20 13:59:45 UTC 2015,,New,,,,,,,"0|i25uuf:",9223372036854775807,,,,,,,,,,,,,,,"20/Feb/15 13:40;rcmuir;Maybe it should just be TestRuleTemporarilyFilesSetup?","20/Feb/15 13:51;dweiss;I think the mention of a ""filesystem"" should be in there since it's really what it does -- replaces the default filesystem provider. But I'm not strong on this opinion. TestRuleTemporaryFilesSetup is fine with me too.","20/Feb/15 13:59;rcmuir;Well (optionally, and sometimes) replacing the default filesystem provider is only one of many things going on here. its also still setting up cleanupQueue and all that stuff for all these to be closed when the test class ends, removing files from those temp dirs, failing if it can't do this, etc, etc so it still manages the cleanup. In general I think its good to just have our temporary file handling (whatever that is doing) consolidated here.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexAndTaxonomyReplicationClientTest.testConsistencyOnExceptions failure,LUCENE-6136,12763716,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,24/Dec/14 21:11,24/Dec/14 22:47,18/Feb/21 10:07,,,,,,,,0,"This reproduces on 4.10:

ant test  -Dtestcase=IndexAndTaxonomyReplicationClientTest -Dtests.method=testConsistencyOnExceptions -Dtests.seed=DFD235D80B3C7F11 -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=be_BY -Dtests.timezone=Atlantic/Reykjavik -Dtests.file.encoding=ISO-8859-1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 24 22:47:19 UTC 2014,,New,,,,,,,"0|i23rnb:",9223372036854775807,,,,,,,,,,,,,,,"24/Dec/14 22:47;rcmuir;exception is hit writing segments_k, but then the delete fails, twice (once in Directory.copy's finally block, then again in cleanupFilesOnFailure).

because this segments_k cannot be deleted, then checkindex goes to use it, and gets a bunch of FNFE, as expected.

the bug is not possible in 5.x because segments_N is written atomically there. in 4.x there is not much that can be done IMO.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use mock filesystem in tests,LUCENE-6072,12757280,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,23/Nov/14 06:07,26/Nov/14 17:24,18/Feb/21 10:07,,,,,,,,0,"We went through the trouble to convert to NIO.2, but we don't take advantage of it in tests...

Since everything boils down to LuceneTestCase's temp dir (which is just Path), we can wrap the filesystem with useful stuff:
* detect file handle leaks (better than mockdir: not just index files)
* act like windows (don't delete open files, case-insensitivity, etc)
* verbosity (add what is going on to infostream for debugging)

I prototyped some of this in a patch. Currently it makes a chain like this:
{code}
  private FileSystem initializeFileSystem() {
    FileSystem fs = FileSystems.getDefault();
    if (LuceneTestCase.VERBOSE) {
      fs = new VerboseFS(fs,
                new PrintStreamInfoStream(System.out)).getFileSystem(null);
    }
    fs = new LeakFS(fs).getFileSystem(null);
    fs = new WindowsFS(fs).getFileSystem(null);
    return fs.provider().getFileSystem(URI.create(""file:///""));
  }
{code}

Some things to figure out:
* I don't think we want to wrap all the time (worry about hiding bugs)
* its currently a bit lenient (e.g. these filesystems allow calling toFile, which can ""escape"" and allow you to do broken things). But only 2 or 3 tests really need File, so we could fix that.
* its currently complicated and messy (i blame the jdk api here, but maybe we can simplify it)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/14 16:09;rcmuir;LUCENE-6072.patch;https://issues.apache.org/jira/secure/attachment/12683227/LUCENE-6072.patch","23/Nov/14 15:27;rcmuir;LUCENE-6072.patch;https://issues.apache.org/jira/secure/attachment/12683225/LUCENE-6072.patch","23/Nov/14 06:08;rcmuir;LUCENE-6072.patch;https://issues.apache.org/jira/secure/attachment/12683217/LUCENE-6072.patch",,,3.0,,,,,,,,,,,,,,,,,,,,2014-11-23 19:14:36.355,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 26 17:24:12 UTC 2014,,New,,,,,,,"0|i22ozr:",9223372036854775807,,,,,,,,,,,,,,,"23/Nov/14 15:27;rcmuir;I added a wrapper to disable underlying fsync calls in tests, and added some simple unit tests for these things, randomized the wrapping so we still test against bare filesystem, and use the mockwindows only some of the time, etc.","23/Nov/14 16:09;rcmuir;I added a few more tests, and fixed a bug in mockwindows.","23/Nov/14 19:14;rjernst;This looks great! It would be cool if this could replace the windows virus checker mocking that was done recently?

+1 to the patch. My only thought is to rename ""sop"" in VerboseFS to something like ""log"" or ""notify"".","23/Nov/14 19:49;dweiss;Very cool indeed. There is a (similar delegate-based) PassThroughFileSystem.java in OpenJDK test sources; it seems to have its own separate URI. I didn't look deeply, but if you wanted to lookup how things are (should be?) done there, it's readily available. (*)

(*) Not sure about licensing issues; I assume we can look at OpenJDK code as a reference for custom implementations?","23/Nov/14 19:56;rcmuir;I didnt know about it, i assume its GPL without looking. It is sad, as i spent a ton of time trying to figure out that part :(

Each of these have their own scheme. Its required parameter for subclasses (e.g. the windows one is windows://). ","23/Nov/14 19:58;dweiss;Sadly, you are correct, it's GPLv2.
","23/Nov/14 20:14;rcmuir;Also, not to justify the craziness, but i dont think we can trust the JDK to implement these ""pass thru"" ones.

Every one they do (FilterInputStream, FilterOutputStream, FilterReader) has horrible performance / delegation bugs.

I found this out because when i initially developed this, i used FilterInputStream and FilterOutputStream, and suddenly like 10% of tests had beating hearts and were taking forever. After profiling tests, its because if you subclass these classes (even to just intercept 'close' or whatever), then suddenly all bytes are being written/read one byte at a time, and i think even boxed inside byte[1]'s and so on. ","23/Nov/14 20:17;mikemccand;+1, this looks wonderful.  I like the name ""sop"" :)

I ran tests before/after with the patch (""ant test -Dtests.seed=0"" in lucene/core) and the times were the same within noise.","23/Nov/14 22:44;uschindler;bq. After profiling tests, its because if you subclass these classes (even to just intercept 'close' or whatever), then suddenly all bytes are being written/read one byte at a time, and i think even boxed inside byte[1]'s and so on.

Commons-IO has ProxyInputStream and ProxyOutputStream to do exactly the 1:1 delegation. In fact we could fix the delegation without implementing a completely new FilterXxxStream. But your solution is also fine, very explicit :-)

Otherwise I like the whole thing!","23/Nov/14 22:48;uschindler;One small thing: Should FilterFSProvider.onClose not be protected instead of public?","23/Nov/14 22:52;rcmuir;Currently, only concrete things are public. Most of the patch is pkg private. I can fix docs and promote to public, with some experimental or internal warning. ","25/Nov/14 15:07;rcmuir;{quote}
One small thing: Should FilterFSProvider.onClose not be protected instead of public?
{quote}

Yes you are correct here. Since you should not delegate the call. close() has to be handled in this stupid strange way, because the default filesystem throws exception if you try to close it :)","25/Nov/14 15:21;jira-bot;Commit 1641632 from [~rcmuir] in branch 'dev/trunk'
[ https://svn.apache.org/r1641632 ]

LUCENE-6072: use mockfilesystem in tests","25/Nov/14 15:21;rcmuir;I committed to trunk for now. Ill give some jenkins time before backporting, its not a invasive change.","26/Nov/14 14:22;jira-bot;Commit 1641821 from [~rcmuir] in branch 'dev/trunk'
[ https://svn.apache.org/r1641821 ]

LUCENE-6072: add a way to test for too many open files","26/Nov/14 15:32;jira-bot;Commit 1641833 from [~rcmuir] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1641833 ]

LUCENE-6072: use mockfilesystem in tests","26/Nov/14 17:19;jira-bot;Commit 1641861 from [~rcmuir] in branch 'dev/branches/branch_5x'
[ https://svn.apache.org/r1641861 ]

LUCENE-6072: fix delegation of remove method","26/Nov/14 17:24;jira-bot;Commit 1641862 from [~rcmuir] in branch 'dev/trunk'
[ https://svn.apache.org/r1641862 ]

LUCENE-6072: fix delegation of remove method",,,,,,,,,,,,,,,,
Copy/Move critical pieces of Monster tests into continuous integration,LUCENE-6008,12748309,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,elyograg,elyograg,15/Oct/14 15:23,09/May/16 18:34,18/Feb/21 10:07,,5.0,6.0,5.0,6.0,general/test,,0,[~rcmuir] mentioned on LUCENE-6002 that some of the things in @Monster tests might be important enough to move into tests that are run by continuous integration (Jenkins).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-10-15 15:29:56.683,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 15 15:29:56 UTC 2014,,New,,,,,,,"0|i21787:",9223372036854775807,,,,,,,,,,,,,,,"15/Oct/14 15:23;elyograg;I don't have any idea how to actually manage this migration, creating the issue so the idea doesn't get lost.","15/Oct/14 15:29;uschindler;In fact we need more CPU power to do this :-)

Policeman Jenkins has too less RAM to allow large heap sizes and is heavy testing the different OS platforms, JVM versions, and and and. ASF Jenkins is overloaded and monster tests should not run there, as this would hold other long-running stuff like nightly.

If somebody wants to spend another Jenkins server, I would be happy to put it into the policeman cloud, so it will make use of JVM randomization. In fact this additional server should maybe run only monster tests and only on linux with all known jvm configs?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add minor-revision writers to backwards-codecs,LUCENE-5991,12745992,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,05/Oct/14 15:08,06/Oct/14 08:00,18/Feb/21 10:07,,,,,,,,0,"Today we have backwards testing almost completely isolated cleanly, and tests against each format. But we only test old major formats, not the minor ones. Before it was probably the right tradeoff, but now that its isolated I think we should test all of them.

For example the 4.1 stored fields format had two minor format changes across the 4.x release: 
{noformat}
  static final int VERSION_START = 0;
  static final int VERSION_BIG_CHUNKS = 1;
  static final int VERSION_CHECKSUM = 2;
  static final int VERSION_CURRENT = VERSION_CHECKSUM;
{noformat}

We could easily directly test these possibilities (e.g. take this as a parameter to the RW format and have 3 TestXXXStoredFieldsFormat, one for each) instead of only testing the latest one and relying on TestBackCompat to find issues, which it probably won't since the index is simplistic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-10-06 08:00:54.512,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 06 08:00:54 UTC 2014,,New,,,,,,,"0|i20tav:",9223372036854775807,,,,,,,,,,,,,,,"05/Oct/14 15:12;rcmuir;By the way, i dont think we should add complex conditional logic on the write-side. Since this is just for tests, we should focus on the authenticity of the format and just have a writer for each or something simple if we need.","06/Oct/14 08:00;mikemccand;+1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A few potential reproducibility issues,LUCENE-5896,12735430,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,simonw,simonw,20/Aug/14 15:30,09/May/16 18:31,18/Feb/21 10:07,,4.9,,4.10,6.0,general/test,,0,"I realized that passing the same seeded random instance to LuceneTestCase# newIndewWriterConfig doesn't necessarily produce the same IWC and I found a bunch of issues in that class using global random rather than local random. Yet, I went over the file to spot others but we might need to think about a more automated way to spot those...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/14 15:32;simonw;LUCENE-5896.patch;https://issues.apache.org/jira/secure/attachment/12663136/LUCENE-5896.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,2014-08-20 15:38:39.703,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 28 13:26:35 UTC 2014,,New,Patch Available,,,,,,"0|i1z4uv:",9223372036854775807,,,,,,,,,,,,,,,"20/Aug/14 15:32;simonw;here are the ones I found looking over it for 2 min I bet there are more","20/Aug/14 15:38;rcmuir;+1

maybe in the future we can use some other strategy to at least detect or avoid such bugs, maybe ThreadLocalRandom?

But please fix these for now, they just stop tests from reproducing and so on.","20/Aug/14 15:41;mikemccand;+1, nice catch.","20/Aug/14 19:36;jira-bot;Commit 1619211 from [~simonw] in branch 'dev/trunk'
[ https://svn.apache.org/r1619211 ]

LUCENE-5896: Use local random instance rather than global in LuceneTestCase","20/Aug/14 19:44;jira-bot;Commit 1619213 from [~simonw] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1619213 ]

LUCENE-5896: Use local random instance rather than global in LuceneTestCase","20/Aug/14 19:47;dweiss;Every random instance in the test framework is already assigned per-thread; there are no races there.

I think *not* using the randomized context's random is actually a mistake (the only exception being tight loops where performance is the key). Once you fork a custom random it will escape. If you consistently use the framework's Random then you should always be able to reproduce the same run with tests.seed.

Unless I'm missing something.","20/Aug/14 19:51;dweiss;So, to be clear -- I think this is the problem:
{code}
public static IndexWriterConfig newIndexWriterConfig(Random r, Analyzer a) {
{code}
It should just use the randomized context's randomness consistently instead of accepting an external Random. It would be consistent (and is not leading to any thread related randomization races).","20/Aug/14 19:53;rcmuir;Dawid: well this is exactly what i mean. We got ourselves into the situation long before that, when there was just a huge pile of code in LuceneTestCase.java, and I think a lot of that stuff is just hanging around, we should try to think of a way to do it better...","20/Aug/14 20:00;dweiss;Ok, clear. I would really encourage removing Random as a parameter in favor of grabbing a random() locally inside a method that needs it (or use adequate methods from the superclass). When there are loops that do millions of iterations and performance becomes an issue, fork off a local variable with
{code}
Random local = new Random(randomLong());
...
{code}

Ideally, it shouldn't escape outside of the method scope.","20/Aug/14 21:21;hossman;bq. ...I went over the file to spot others but we might need to think about a more automated way to spot those...

bq. It should just use the randomized context's randomness consistently instead of accepting an external Random. It would be consistent (and is not leading to any thread related randomization races).

Ironically, the best way (i can think of) to ""test the test utils"" and verify the reproducibility of these methods (along hte lines of simon's question about a ""more automated way to spot"" these types of bugs) goes out the window if you remove the Random param from these types of methods.  

As things stand right now, we can at least write a test that does this...

{code}
long localSeed = random().randomLong();
assertEquals(newIndexWriterConfig(new Random(localSeed), XXX),
             newIndexWriterConfig(new Random(localSeed), XXX));
{code}

..and be fairly confident that we have a test which will fail if anyone introduces similar bugs into {{newIndexWriterConfig}} in the future.

----

I havne't thought this through very hard, but...

Is it worth refactoring these methods into some special package w/restricted visibility (and w/o direct access to {{LuceneTestCase.random()}} so we can test them with explicit random instances, but then wrap them in instances w/o the {{Random}} arg so test cases can't exploit them?

maybe something like...

{code}
package org.apache.lucene.test-internals; // put some test-the-test test's in same package
public final class HuperDuperRandomizedObjFactories {
  // package protected, test-the-test test's in same package can vet directly
  // but most code can never call this
  static IndexWriterConfig newIndexWriterConfig(long localSeed, Analyzer a) {
    Random localRandom = new Random(localSeed);
    ...
  }
  public static IndexWriterConfig newIndexWriterConfig(random r, Analyzer a) {
    return newIndexWriterConfig(r.nextLong(), a) {
  }
  ...
}
// - - -
package org.apache.lucene;
public class LuceneTestCase {
  ...
  public static IndexWriterConfig newIndexWriterConfig(Analyzer a) {
    return HuperDuperRandomizedObjFactories.newIndexWriterConfig(random(),a);
  }
}
{code}
","21/Aug/14 06:57;dweiss;> As things stand right now, we can at least write a test that does this...

This is a good idea if you make those methods hidden from general use (for example package-private, like you said). The drawback is it'd require an implementation of equals on the returned result, which may be a pain.","28/Oct/14 13:26;simonw;[~dawidweiss] just merged a PR that might make this easier after all.... I had similar problems in Elasticsearch leading to https://github.com/carrotsearch/randomizedtesting/pull/177 which would allow methods like this:

{code}

public static IndexWriterConfig newIndexWriterConfig(long seed) {
    return RandomizedContext.current().runWithPrivateRandomness(new Randomness(seed), new Callable<IndexWriterConfig>() {
        @Override
        public IndexWriterConfig call() throws Exception {
            return newIndexWriterConfig(); // delegate to the ordinary method
        }
    });    
}

{code}

it's not the simplest solution but it's way less error prone",,,,,,,,,,,,,,,,,,,,,,
Allow choosing latest default codec (not random),LUCENE-5874,12732566,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,dsmiley,dsmiley,07/Aug/14 14:47,08/Aug/14 14:13,18/Feb/21 10:07,,,,,,modules/test-framework,,0,"Within the Lucene/Solr tests, I can see that we want to widely test the various Lucene Codecs.  And so a test can use a black-list @SuppressCodecs when certain codecs are known not to work with certain tests.  But there is no white-list nor white-list by capabilities (e.g. saying you need term vectors that support payloads).

In external applications that are using the test infrastructure, this is annoying.  An application generally wants to test with only the codec that ships as default with the particular Lucene version they use.  Now sure, this could be done by using ""-Dtests.codec=Lucene46"" , or having your \@BeforeClass explicitly do Codec.setDefault(Codec.forName(""Lucene46"")).  But in both these cases, you have to call out a specific version; you can't simply ask for whatever the latest is for the Lucene version you're using that's on your classpath.  So what I propose is two things: a whitelist annotation \@LuceneTestCase.Codecs and a value ""CURRENT"" (to align with the LUCENE_CURRENT naming convention).  In this way a test could use the latest Codec and not hit a random failure the first time around just because Lucene3x or some other old codec was chosen.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-08-07 15:40:58.915,,,false,,,,,,,,,,,,,,,,,,410594,,,Fri Aug 08 14:13:31 UTC 2014,,New,,,,,,,"0|i1yn47:",410588,,,,,,,,,,,,,,,"07/Aug/14 15:40;rcmuir;Can you comment on the proposal on LUCENE-5858 instead?

I argue there we should remove such ancient codecs from rotation: its annoying because they dont support the full feature set and just causes a bunch of wasted time debugging and false failures.

Furthermore this doesn't really bring any value anymore, it doesnt find bugs in these old codecs, it just brings pain.
Instead we have a better testing directly against old codecs, we have the mechanism to test these guys directly (various ""unit"" tests like BaseXXXFormatTestCase).

We could do this before trying to split out a jar, just as a step.","07/Aug/14 17:14;dsmiley;I don't have much of an opinion on LUCENE-5858.  It's somewhat related I guess but doesn't address what I propose in this issue -- the ability for a test (especially a test outside of Lucene/Solr) to simply get whatever the latest Codec/PostingsFormat/DocValues format there is.","07/Aug/14 19:06;rcmuir;How is it somewhat related? Its completely related if the motivation here is to have all the lucene features: all the other codecs support the full feature set.

Its just the old ones that don't.","08/Aug/14 05:40;dsmiley;I weighed in on LUCENE-5858.

bq. How is it somewhat related? Its completely related if the motivation here is to have all the lucene features: all the other codecs support the full feature set.

I think one day there's going to be some new feature only supported by a subset of the codecs in the rotation.  If the only tool we have is to say which codecs we *don't* want, then it's less ideal then expressing what you *do* want.","08/Aug/14 05:53;rcmuir;Why woudl there be? Codec is encoding and datastructures. All the current ones support all the current features.

They arent ""pluggable alternative lucenes"".","08/Aug/14 14:13;dsmiley;I'll comment here when my concern next applies in the future.  It's water under the bridge in the interim once LUCENE-5858 happens.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debug spatial test failure isWithin ,LUCENE-5549,12702984,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Minor,,dsmiley,dsmiley,dsmiley,21/Mar/14 22:06,24/Mar/14 19:43,18/Feb/21 10:07,,4.8,,,,modules/spatial,,0,"The bug doesn't appear to be serious; mostly a test bug.  It happens with the query shape is based on ShapeCollection, which has a bug in its bounding box that will turn the longitude range into a -180 to 180 range when there is a more optimal tighter range.

https://github.com/spatial4j/spatial4j/issues/77

",-Dtests.method=testWithin -Dtests.multiplier=3 -Dtests.seed=5F5294CE2E075A3E:AAD2F0F79288CA64,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-03-21 22:07:04.985,,,false,,,,,,,,,,,,,,,,,,381321,,,Mon Mar 24 19:43:44 UTC 2014,,New,,,,,,,"0|i1tp5j:",381597,,,,,,,,,,,,,,,"21/Mar/14 22:07;jira-bot;Commit 1580070 from [~dsmiley] in branch 'dev/trunk'
[ https://svn.apache.org/r1580070 ]

LUCENE-5549: punting on this test bug for now","21/Mar/14 22:09;jira-bot;Commit 1580071 from [~dsmiley] in branch 'dev/branches/branch_4x'
[ https://svn.apache.org/r1580071 ]

LUCENE-5549: punting on this test bug for now","24/Mar/14 19:43;jira-bot;Commit 1581020 from [~dsmiley] in branch 'dev/branches/lucene_solr_4_7'
[ https://svn.apache.org/r1581020 ]

LUCENE-5549: punting on this test bug for now",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
move WithNestedTests from src/test to src/test-framework,LUCENE-5403,12689165,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,16/Jan/14 04:09,16/Jan/14 04:09,18/Feb/21 10:07,,,,,,general/test,,0,This class is abstract: its useful if you want test-the-tester tests.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,368132,,,2014-01-16 04:09:52.0,,New,,,,,,,"0|i1rga7:",368437,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests for LicenseCheckTask.,LUCENE-5210,12668563,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,markrmiller@gmail.com,markrmiller@gmail.com,13/Sep/13 21:28,16/Sep/13 03:31,18/Feb/21 10:07,,,,,,general/build,,0,"While working on LUCENE-5209, I noticed the LicenseCheckTask is kind of a second class citizen - excluded from UI src folder setup and with no units tests. This was a little scary to me.

I've started adding some units tests. So far I have mainly just done the lifting of getting units tests to work as part of tools.

I have added two super simple tests - really just the start - but something to build on.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/13 02:07;markrmiller@gmail.com;LUCENE-5210.patch;https://issues.apache.org/jira/secure/attachment/12603177/LUCENE-5210.patch","13/Sep/13 21:29;markrmiller@gmail.com;LUCENE-5210.patch;https://issues.apache.org/jira/secure/attachment/12603112/LUCENE-5210.patch",,,,2.0,,,,,,,,,,,,,,,,,,,,2013-09-14 07:43:54.156,,,false,,,,,,,,,,,,,,,,,,348497,,,Mon Sep 16 03:31:35 UTC 2013,,New,,,,,,,"0|i1o3b3:",348794,,,,,,,,,,,,,,,"13/Sep/13 21:31;markrmiller@gmail.com;This patch is not complete, but to show the current direction. There will be at least one more.","13/Sep/13 21:36;markrmiller@gmail.com;Also note: the test.jar files (fake) that belong in the test resources folder did not get picked up by the patch.","14/Sep/13 02:07;markrmiller@gmail.com;Here is a more complete patch.","14/Sep/13 07:43;uschindler;Hi,
to make tests easy there is antunit. I use this to test forbidden-apis. It is registered as a separate task in ANT and you write a build file that works as test. It can check inner tests and apply assertions on the output, file state,...

This would reduce the 2 simple tests to one build file that is executed as a subant task from the main build file. No Junit needed.

See as example: [https://code.google.com/p/forbidden-apis/source/browse/trunk#trunk%2Fsrc%2Ftest%2Fantunit]
It is called from here: [https://code.google.com/p/forbidden-apis/source/browse/trunk/build.xml?spec=svn207&r=207#346]

I can help with implementing this (unfortunately I am busy next week), it should be mostly copy/paste of the test setup in forbidden's build.xml and reuse some simple tests.","14/Sep/13 09:48;uschindler;In addition: I disagree with adding the LicenseCheckTask (and all tools folders around Lucene) to Eclipse setup. With a good antunit test this is also not needed.","14/Sep/13 10:50;uschindler;I while ago I was also discussing with Dawid and Robert about the LicenseChecker in general. At the time it was written, we had not yet experience with using scripting inside ANT.

We should maybe discover removing the java code completely and create the license checker as a macro like the svn working copy checks. We already have the framework in common-buil.xml to load the groovy scripting engine, porting over the java code to be a macro should be very simple (it is mostly copy/paste and some ANT magic). We can then also test the checker with Antunit, if we like. The simplicity of this checker does not rectify having the compile/classpath/ANT overhead. A simple ""resolve-grovy"" and a macro should do the work.

Can we keep this issue open for a while until I am back so I can check?

About your patch: To me the patch looks a little bit incorrect regarding resources: It creates a src/resources folder, which is per convention only for resources shipped with the JAR file. Test resources are currently next to the Test .java files. This is not an issue for the tool, because we dont package them as JAR file, but its still wrong, because the resources are available to the ANT task when its running ""in production"".","14/Sep/13 12:26;markrmiller@gmail.com;bq. In addition: I disagree with adding the LicenseCheckTask (and all tools folders around Lucene) to Eclipse setup. With a good antunit test this is also not needed.

How is that? When it's not in the eclipse setup, editing the code is annoying and hard - when it is, it is easy. Does antunit make it nice to edit the code? I'm not sure how it could.

In any case, thanks for the tip.

bq. We should maybe discover removing the java code completely and create the license checker as a macro like the svn working copy checks.

Personally, I think doing anything in ant over java is a terrible idea. Ant is terrible when it's complicated and more and more of us have to be experts in it to do anything. Ant is so obtuse for anything that is not super straightfoward. Mixing in Groovy as well really makes my stomach turn :)

bq. To me the patch looks a little bit incorrect regarding resources

I'm not sure that matters, but it can just be a test resources folder as I intended instead.

Basically, I think being anal about what we end up with should be a clear step 2. Having *any* testing at all for our license checker should be the clear step one. Once we have any testing, it will make me sad if you make it all ant and really hard for anyone else to figure out, but I don't really care too much as long as our very important LiceneCheckTask has tests - right now it's flying blind.","16/Sep/13 02:59;rcmuir;I have no opinion on the eclipse/ant/antunit stuff.

I just want to say currently there is no test, so I think we should start with a test and then improve it.

My one suggestion about testing and the jars: if the test is in java, it can easily create jars on the fly in temp dirs so we dont have to package them (with fake licenses). This is done in ResourceLoaderTest in solr for example:

{code}
  public void testClassLoaderLibs() throws Exception {
    File tmpRoot = _TestUtil.getTempDir(""testClassLoaderLibs"");

    File lib = new File(tmpRoot, ""lib"");
    lib.mkdirs();

    JarOutputStream jar1 = new JarOutputStream(new FileOutputStream(new File(lib, ""jar1.jar"")));
    jar1.putNextEntry(new JarEntry(""aLibFile""));
    jar1.closeEntry();
    jar1.close();
    ...
{code}
","16/Sep/13 03:14;rcmuir;As far as current patch, i dont really have a problem with it (any other simplifications can be done later).

I have only one concern: will this make a lucene-tools module (e.g. packaged in releases, published in maven?)

It seems like it might, which separately might be a good idea so someone can use the stuff in this folder in their own project, except a few things would be off as far as packaging:
* it should probably be restructured, so that various configs used by the build are in src/resources and put inside its jar file (e.g. forbiddenApis configs and so on)
* I think this depends on ant, but there is no dependency of ant in the ivy.xml
* it would need maven configuration and so on, added in smoketester, etc.
* there might be other exclusions for tools/ in the build that are not appropriate, etc.
* as far as the name, maybe build-tools would be a better one (since its not tools for working on lucene indexes).

If smoketester passes though, I am happy: We can just make sure its excluded from the right places and not doing something we don't want wrt packaging for now, and discuss this stuff on other issues.
","16/Sep/13 03:22;markrmiller@gmail.com;Yeah, my first thought was to write out the test files to a tmp dir, but essentially I was too lazy to code it up.","16/Sep/13 03:24;markrmiller@gmail.com;Also note, prepare-release does not currently pass the way things are - something to do with a maven artifact that now tries to run on tools.","16/Sep/13 03:26;rcmuir;I think if we _just want to run tests_ for now, we should change the test target to explicitly recurse to tools, rather than modifying the 'general module macro' in common-build.

otherwise other tasks (like packaging, javadocs, maven, etc) will try to do things with tools.","16/Sep/13 03:27;markrmiller@gmail.com;bq. will this make a lucene-tools module (e.g. packaged in releases, published in maven?)

Yeah, that is what is happening currently - I'm sure that is what is causing prepare-release to have issues.","16/Sep/13 03:31;rcmuir;look at regenerate task in build.xml, it has a subant explicitly going to 'core' to run a task.

we'd just want something like that subant, call it someting like 'test-tools'

and have 'test' depend on that.",,,,,,,,,,,,,,,,,,,,
remove fieldcache weakmap or at least see what relies on GC for purging today,LUCENE-5177,12663751,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,14/Aug/13 23:01,16/Aug/13 01:04,18/Feb/21 10:07,,,,,,,,0,"If we are registering close listeners why does this need to be weak?

But really i dont care about that, here is what i said to Hoss on the solr mailing list:
{quote}
> (In any case: it looks like a WeakHashMap is still used in case the
> listeners never get called, correct?)
>

I think it might be the other way around: i think it was weakmap
before always, the close listeners were then added sometime in 3.x
series, so we registered purge events ""as an optimization"".

But one way to look at it is: readers should really get closed, so why
have the weak map and not just a regular hashmap.

Even if we want to keep the weak map (seriously i dont care, and i
dont want to be the guy fielding complaints on this), I'm going to
open with an issue with a patch that removes it and fails tests in
@afterclass if there is any entries. This way its totally clear
if/when/where anything is ""relying on GC"" today here and we can at
least look at that.
{quote}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/13 23:04;rcmuir;LUCENE-5177.patch;https://issues.apache.org/jira/secure/attachment/12598080/LUCENE-5177.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,2013-08-15 19:21:27.348,,,false,,,,,,,,,,,,,,,,,,343752,,,Fri Aug 16 01:04:27 UTC 2013,,New,,,,,,,"0|i1na4n:",344054,,,,,,,,,,,,,,,"14/Aug/13 23:04;rcmuir;patch that 
* changes WeakMap to HashMap
* removes the *completely bogus* purging that happens in tearDown() today (instead only doing this in beforeClass() so failed tests dont interfere with others)
* fails if there are any fieldCache entries in afterClass()
* adds test-the-tester class.

some tests fail. we should see what they are doing (I suspect test-utility classes making crazy readers with their own cache keys that never get closed, but maybe something else interesting)","15/Aug/13 19:21;mikemccand;+1 to cutover to non-weak HashMap.","15/Aug/13 21:04;hossman;I understand the value of the improved testing -- but is there any tangible benefit to users to converting the WeakHashMap -> HashMap?

","15/Aug/13 21:16;rcmuir;there need not be tangible benefits to users for us to make a change.

we can make a change because its refactoring our codebase and making it cleaner or many other reasons.","15/Aug/13 21:25;hossman;bq. its refactoring our codebase and making it cleaner or many other reasons.

How does this do that?

I'm not trying to be confrontational, i'm genuinely not understanding what is improved by switching away from a WeakHashMap and i just want to make sure i'm not missunderstanding something about the big picture.

(If you proposed to get rid of the Map completely and have the Caches hang directly off the readers (something i remember discussing a LOOOONG time ago that people seemed to think was a good idea but no one seemd to have bandwidth for) then i could totally understand arguments that doing so would be making the codebase cleaner -- but i'm not understanding what's clearner/better about using a global static HashMap instead of a WeakHashMap.)

","15/Aug/13 21:33;rcmuir;I don't think we should hang ""fieldcaches"" directly off readers.

Fieldcache is a broken design: users who want to sort/facet on a field should index their content correctly with docvalues instead.

Its fine for it to be wrapped underneath an UninvertingFilterReader, that also takes a Map<String,DocValuesType> up front though, but by no means should we have this broken shit tightly coupled with stuff in lucene/core","15/Aug/13 21:59;hossman;bq. by no means should we have this broken shit tightly coupled with stuff in lucene/core

ok fine, fieldcaches as a concept is fundementally broken, and the suggestion of hanging the caches of index readers is stupid in this day and age of docvalues -- forget it, then.  It was simply an aside comment.

Can you (or anyone else) please help me understand why keeping these ""broken"" caches in global static HashMaps is cleaner/better then keeping them in global static WeakHashMaps?

","15/Aug/13 22:07;yseeley@gmail.com;bq. ok fine, fieldcaches as a concept is fundementally broken, and the suggestion of hanging the caches of index readers is stupid in this day and age of docvalues 

Meh... I happen to disagree with both of those assertions.","15/Aug/13 22:11;rcmuir;Thats fine, users who want to have slow reopen times and inefficient use of memory can use an FilterReader that uninverts and exposes stuff via the AtomicReader docValues APIs.

Such a FilterReader is useful as a transition mechanism anyway: users could pass it to addIndexes to 'upgrade' to docvalues.","15/Aug/13 22:21;yseeley@gmail.com;bq. users who want to have slow reopen times and inefficient use of memory can use an FilterReader that uninverts and exposes stuff via the AtomicReader docValues APIs

As long as it's not slower / more inefficient than the field cache stuff we have today, that's fine.  Just a different API to access it. ","15/Aug/13 22:25;yseeley@gmail.com;Hoss - wrt WeakHashMap, I guess if it's not needed, lookups could be very slightly faster, debugging maybe slightly easier (if you're looking at weak references on the heap for example), and the code easier to understand (since we are not in fact relying on a weak reference to clean anything up anymore).","15/Aug/13 22:28;rcmuir;Yes, i dont think we should make it slower. but with such a filterreader, it could be implemented cleaner/differently more easily, because people can access it thru DV apis rather than thru FieldCache.DEFAULT.xxx
","15/Aug/13 22:47;elyograg;How is Solr affected by the idea of not using fieldcache?  From what I understand, docValues have the same data that would go into a stored field, not the indexed field ... so they might not behave exactly the same when it comes to sorting.  Although it doesn't make any sense to sort on a fully tokenized field, it can make sense to sort (or facet) on a field that uses the keyword tokenizer, the lowercase filter, and the trim filter.  I don't think that's possible with docValues.  
","15/Aug/13 22:51;rcmuir;Thats all solr issues, its not related here.

DocValues fields take a byte[]. you can put whatever you want in there, it doesnt have to be the same as what goes in a stored field, you can run it thru an analysis chain yourself in some solr document processor or something like that if you really want to do that... you don't need indexwriter's help.","15/Aug/13 22:58;yseeley@gmail.com;bq. How is Solr affected by the idea of not using fieldcache?

AFAIK, fieldcache-type functionality isn't going away, and any Lucene API changes (like FieldCache.DEFAULT, etc) will be hidden by Solr.
DocValues adds additional functionality, and does not take away from anything we already have.

There are currently one or two disadvantages to using docvalues in solr currently.  The major disadvantage is being forced to specify a default value in Solr, thus making them impossible to use from dynamic fields.   Ideally we'd be able to specify a ""missing"" value (i.e. the value used when there is no value for that document... or rather the ""default"" at the DocValues layer), and that would allow us to remove the currently mandated default value at the Solr layer.","15/Aug/13 23:04;rcmuir;But really this should just be an option on the solr fieldtype, you know when the value is missing, solr just puts that missing value in the dv field for that document (nowhere else like stored fields or anything like that).

same with if you want the fieldtype to run stuff thru an analyzer or anything like that. I dont think this stuff really has to do with lucene, it can just be options in solrs update process/fieldtypes.","16/Aug/13 00:04;yseeley@gmail.com;bq. But really this should just be an option on the solr fieldtype, you know when the value is missing, solr just puts that missing value in the dv field for that document.

But doing it at the solr level means that you can't define a field without using it.
Defining a non-docvalues field in solr and not using it currently incurs no overhead.  This isn't the case with docvalues, and I don't believe docvalues allows one to currently specify a default (it always defaults to 0 for missing values?)","16/Aug/13 00:10;rcmuir;why would have fields defined you arent using? I dont understnad the use case here.

and to say there is no overhead with fieldcache is wrong: it makes huge arrays even if one document has the field. so I'm really missing something: in both cases its a column-stride field, just one is built at index-time.

I dont understand why docvalues needs to allow you to specify a default, when you can just specify your own for the document with missing value (if you specify 0, its no different than if it fills that for you).","16/Aug/13 00:25;yseeley@gmail.com;bq. why would have fields defined you arent using?

Why not? Other fields take no resources if you don't use them, but docvalues do.
Dynamic fields represent an almost infinite number of fields you aren't using until you do.
BTW, this is why the only uses of docvalues in the example schema are commented out.  Who want's to incur index overhead for fields you aren't even using?

If we want anything other than 0 for a value for ""missing"", we must do it in DocValues.
","16/Aug/13 00:29;rcmuir;Thats so wrong: if you want a value other than 0 (say 20) you just set the missing value yourself by setting it in the o.a.l.Document you add to indexwriter.

There is absolutely no reason for indexwriter to do things for you that you can do yourself.","16/Aug/13 00:37;rcmuir;By the way: just to explain how it works in FieldCache when you supply these ""sort missing/first/last stuff"". 
The way that works is with a separate fieldcache ""field"" (a bitset) which marks documents that have a value.
So its really a separate fieldcache'd boolean[maxdoc] telling you if there is anything there or not for the original field.

You can easily do the exact same parallel with docvalues yourself (e.g. in a solr fieldtype) if you want to support those options (a numericdvfield only 1 or 0, takes 1 bit per document, same situation).
","16/Aug/13 00:43;yseeley@gmail.com;bq. Thats so wrong: if you want a value other than 0 (say 20) you just set the missing value yourself by setting it in the o.a.l.Document you add to indexwriter.

And how exactly could we do that with dynamic fields?
Face it - if we want any other defaults than 0 (for numerics) it needs to somehow be configurable.","16/Aug/13 00:50;rcmuir;It doesnt need to be configurable. its not configurable in fieldcache today!
It has a separate bitset cached on fieldcache indicating 1 or 0.

so you just do the same thing with docvalues, as I explained earlier.
","16/Aug/13 01:04;yseeley@gmail.com;I've opened LUCENE-5178 for this.  It's not strictly related to the FieldCache.",,,,,,,,,,
Refactor TestGrouping.java to break TestRandom into separate tests,LUCENE-5065,12653539,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Minor,,,tburtonwest,tburtonwest,18/Jun/13 18:00,23/Jun/13 15:23,18/Feb/21 10:07,,4.3.1,,,,modules/grouping,,0," lucene/grouping/src/test/org/apache/lucene/search/grouping
TestGrouping.java  combines multiple tests inside of one test: TestRandom(). 
This makes it difficult to understand or for new users to use the TestGrouping.java as an entry to understanding grouping functionality.

Either break TestRandom into separate tests or add small separate tests for the most important parts of TestRandom.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-06-23 15:23:20.554,,,false,,,,,,,,,,,,,,,,,,333816,,,Sun Jun 23 15:23:20 UTC 2013,,New,,,,,,,"0|i1ll2f:",334143,,,,,,,,,,,,,,,"23/Jun/13 15:23;martijn.v.groningen;+1! This test has really become unreadable / unmaintainable.

I would split it into separate test classes each testing a specific part:
* A test that both tests the first and second pass collector.
* Some as above but then a sharded test.
* Move the test for the all groups collector to AllGroupsCollectorTest.
* Move out the block grouping to a dedicated test.
* I think the usage of CacheCollector can be removed and perhaps this can just be tested via the 'GroupingSearchTest' class.

This is a big tasks :-) So it would be a more digestible task if this is done step by step.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertingIndexSearcher should do basic QueryUtils/etc checks on every query,LUCENE-4934,12642557,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,15/Apr/13 16:15,15/Apr/13 18:09,18/Feb/21 10:07,,,,,,,,0,"We can start with QueryUtils.check(query): which does some basic hashcode/equals checks.

Ideally we'd strengthen the checks as we fix problems: e.g. add explanations verifications (checkExplanations) and then finally the more intense check() that does more verifications with deleted docs/next/advance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/13 17:52;rcmuir;LUCENE-4934.patch;https://issues.apache.org/jira/secure/attachment/12578756/LUCENE-4934.patch","15/Apr/13 17:09;rcmuir;LUCENE-4934.patch;https://issues.apache.org/jira/secure/attachment/12578750/LUCENE-4934.patch","15/Apr/13 16:43;rcmuir;LUCENE-4934.patch;https://issues.apache.org/jira/secure/attachment/12578739/LUCENE-4934.patch","15/Apr/13 16:26;rcmuir;LUCENE-4934.patch;https://issues.apache.org/jira/secure/attachment/12578738/LUCENE-4934.patch",,4.0,,,,,,,,,,,,,,,,,,,,2013-04-15 16:21:39.186,,,false,,,,,,,,,,,,,,,,,,322971,,,Mon Apr 15 18:09:39 UTC 2013,,New,,,,,,,"0|i1jq1r:",323316,,,,,,,,,,,,,,,"15/Apr/13 16:21;jpountz;+1","15/Apr/13 16:26;rcmuir;Here's a start just doing the simplest check. I am currently running tests fixing the bugs this one found... I'm gonna see if we can do the explanations test though (at some point we will hit bugs in spans/payloads like LUCENE-4219)","15/Apr/13 16:31;rcmuir;There are hashcode/equals bugs in DrillDownQuery :)","15/Apr/13 16:43;rcmuir;updated patch: now the next bug to fix is hashcode/equals in DrillSidewaysQuery.

I don't know why it throws UOE today... we can't do stuff like this.","15/Apr/13 17:09;rcmuir;updated patch: fixes some more equals/hashcode bugs.

I also implemented the assert differently: we check both before rewrite() and after.","15/Apr/13 17:11;rcmuir;3 more queries with bugs i still havent fixed:

[junit4:junit4] Tests with failures:
[junit4:junit4]   - org.apache.lucene.queryparser.xml.TestParser.testLikeThisQueryXML
[junit4:junit4]   - org.apache.lucene.queryparser.xml.TestParser.testBoostingQueryXML
[junit4:junit4]   - org.apache.lucene.queryparser.xml.TestParser.testFuzzyLikeThisQueryXML

Its a little disturbing that tests in lucene/queries arent finding these problems, only a queryparser test. 

Maybe they arent using newSearcher.
","15/Apr/13 17:32;uschindler;+1, I also like the Term.toString() method :-) We may use a similar approach in Solr's AnalysisRequestHandler when completely binary terms are generated (currently it prints the raw binary term and the string rep next to each other). If the latter fails, it should only return the binary term in the NämedList.","15/Apr/13 17:52;rcmuir;OK updated patch. I think i fixed the big issues in all these equals/hashcodes.

BlockJoin should be revisited (in another issue: please). Its doing complicated stuff like equals based on unrewritten-query, I think this is wrong (e.g. in the case of a MTQ query rewritten from anotehr reader). But i dont want to fix this here.

I want to commit this to make some progress and then look at trying to improve the check (e.g. explains would be a nice step)","15/Apr/13 18:08;mikemccand;+1","15/Apr/13 18:09;uschindler;bq. I also implemented the assert differently: we check both before rewrite() and after.

Hey, that was my idea to move that into IS.rewrite() :-)",,,,,,,,,,,,,,,,,,,,,,,,
improve TestBackwardsCompatibility to test Lucene 4.x features,LUCENE-4085,12558463,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,29/May/12 17:29,09/May/16 18:29,18/Feb/21 10:07,,6.0,,,,general/test,,0,"Currently TestBackwardsCompatibility doesn't test any of the new features of 4.0: e.g. docvalues fields, fields with offsets in postings, etc etc.

We should improve the index generation and testcases (in 5.x) to ensure we don't break these things.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,243880,,,Tue Jun 12 20:47:15 UTC 2012,,New,,,,,,,"0|i04gh3:",23934,,,,,,,,,,,,,,,"12/Jun/12 20:47;rcmuir;I improved the situation to some extent in r1349510. We add docvalues fields of various types, a field with offsets, a field that only omits positions, etc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
factor BaseTokenStream random docs generation into LineDocs,LUCENE-3975,12550624,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,12/Apr/12 05:29,12/Apr/12 05:34,18/Feb/21 10:07,,,,,,general/test,,0,"We improved on our 'random document generation' a lot in LUCENE-3911

In fact these random docs find a lot of real bugs. Also the linedocs
driven from real data is also improved in the analyzer tests: it takes
substrings of random linedocs and makes 'partial docs'.

Really we should refactor this so that LineDocs uses a mix of real,
partial-real, and synthetic docs just like the analyzer tests.

This would help tests like term dictionary tests which are basically
static (even though they are random, the amount of documents is limited).

BaseTokenStreamTestCase would simply pull from LineDocs at that point,
but other tests would immediately see the benefits.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,235488,,,2012-04-12 05:29:20.0,,New,,,,,,,"0|i04h5z:",24046,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add infra for ""integration-test"" modules.",LUCENE-3974,12550623,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,12/Apr/12 04:59,12/Apr/12 04:59,18/Feb/21 10:07,,,,,,general/build,,0,"LUCENE-3969 is the first use case that really motivated me to open this:

Here we are building random Analysis chains from all the Tokenizers, TokenFilters, 
CharFilters, etc that we can possibly find, and kicking them around to find bugs.

But we only do this in the analyzers-common module. Really I want to combine
all the analysis components across common, phonetic, kuromoji, uima, icu, etc to 
create more interesting chains and increase test coverage.

So I think along with having a modular architecture, we have some responsibility
to test different modules combined together, as sometimes we find bugs like
LUCENE-3920 that no unit test will ever discover.

Can/Should we create some sort of test module that depends on *every lucene module* and
is a free-for-all for crazy integration tests?
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,235487,,,2012-04-12 04:59:20.0,,New,,,,,,,"0|i04h67:",24047,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add tests for JaSpell/TST/HighFrequencyDictionary to the suggest module,LUCENE-3813,12543503,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,21/Feb/12 15:36,09/May/16 18:34,18/Feb/21 10:07,,,,4.9,6.0,,,0,"Currently we are trying to refactor the suggest APIs in another issue...
I think if we had lucene-level tests for these implementations in the module it
would make things a lot easier.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-07-23 18:44:31.746,,,false,,,,,,,,,,,,,,,,,,228749,,,Wed Apr 16 12:54:50 UTC 2014,,New,,,,,,,"0|i04i53:",24204,,,,,,,,,,,,,,,"23/Jul/13 18:44;sarowe;Bulk move 4.4 issues to 4.5 and 5.0","16/Apr/14 12:54;uschindler;Move issue to Lucene 4.9.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If a test fails in beforeClass(), we don't get any debugging information",LUCENE-3611,12533299,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,30/Nov/11 15:10,30/Nov/11 15:10,18/Feb/21 10:07,,,,,,general/test,,0,At the minimum we at least need reportPartialFailureInfo(),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,219027,,,2011-11-30 15:10:03.0,,New,,,,,,,"0|i04jdr:",24405,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Refactor test classes that use assumeFalse(codec != SimpleText, Memory) to use new annotation and move the expensive methods to separate classes",LUCENE-3489,12525818,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,uschindler,uschindler,05/Oct/11 13:19,09/May/16 18:29,18/Feb/21 10:07,,4.0-ALPHA,,4.9,6.0,general/test,,0,"Folloup for LUCENE-3463.

TODO:
- Move test-methods that need the new @UseNoMemoryExpensiveCodec annotation to separate classes
- Eliminate the assumeFalse-calls that check the current codec and disable the test if SimpleText or Memory is used",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LUCENE-3463,LUCENE-3429,,"12/May/12 13:29;rcmuir;LUCENE-3489.patch;https://issues.apache.org/jira/secure/attachment/12526637/LUCENE-3489.patch","12/May/12 13:18;rcmuir;LUCENE-3489.patch;https://issues.apache.org/jira/secure/attachment/12526636/LUCENE-3489.patch","11/May/12 21:40;rcmuir;LUCENE-3489.patch;https://issues.apache.org/jira/secure/attachment/12526575/LUCENE-3489.patch",,,3.0,,,,,,,,,,,,,,,,,,,,2011-10-05 13:25:51.131,,,false,,,,,,,,,,,,,,,,,,46432,,,Wed Apr 16 12:54:35 UTC 2014,,New,,,,,,,"0|i04k4f:",24525,,,,,,,,,,,,,,,"05/Oct/11 13:25;rcmuir;you know, another similar issue we have is tests that assumeFalse(codec != PreFlex), because of things like new index statistics or byte terms or other features that it doesn't support.

Maybe there is some way we could generalize the annotation?

something like @AvoidCodecs(""SimpleText"", ""Memory""), @AvoidCodecs(""PreFlex""), and this set would be handled like the boolean today?","05/Oct/11 13:31;uschindler;Nice idea, this can be easily transformed to a annotation with param! Of course it would be per-class, too.","05/Oct/11 14:57;dweiss;While working on my presentation I've been trying to generalize the concept of ""randomized tests"". There's definitely a lot of great concepts here, but they're closely coupled with Lucene and the rest of the Solr/Lucene infrastructure. 

I have a draft code of a RandomizedTesting framework that provides very much similar functionality, although in a slightly different technical way (for example it's based on JUnit @Rules only, not on a custom runner/ base abstract class). I would really like you to peek at this and, perhaps with some effort, generalize the concepts in the test framework instead of introducing more Lucene-specific annotations.

I'll publish the code tomorrow (it still needs some, ehm, polishing) along with some thoughts that I had about the current code in LuceneTestCase/Runner. 

I'd really like this to evolve into something of a stand-alone project (even if bundled with Lucene) so that other project can benefit without necessarily rely on the rest of Lucene code. We're already using a somewhat decoupled code internally and making it really cross-project applicable is a great way of proving these concepts are generally useful.

Until tomorrow? :)","05/Oct/11 16:06;uschindler;bq. (for example it's based on JUnit @Rules only, not on a custom runner/ base abstract class)

This is a nice idea, although the reason for the LuceneTestCaseRunner and the abstract base class is more because we hate @Test annotations and dont want to add thousands of stupid Assert-includes (having this is abstract base class is more convenient). Just to mention UweSays on twitter: [@UweSays|http://twitter.com/#!/UweSays]","05/Oct/11 16:19;dweiss;Yeah, I figured that you want to keep it compact. These may be compatible because there's nothing forbidding us to keep LuceneTestCase as a base class (descending from Assert and providing Lucene-related infrastructure). I'm just trying to push all the randomization (seed initialization, reproducibility, thread controls) out of LuceneTestCase and into something more generic. So far it looks good to my eyes, but I'll be looking forward to your strict German opinion, Uwe :)

Oh, by the way -- is there any particular reason for so many things to be static (class level)? I get these are fixtures reused by tests but would people scream if they were object-level fixtures rather than class-level fixtures? It'd make things a bit easier... starting with the need for a single initial seed, for example.","05/Oct/11 16:27;rcmuir;{quote}
Oh, by the way – is there any particular reason for so many things to be static (class level)? I get these are fixtures reused by tests but would people scream if they were object-level fixtures rather than class-level fixtures? It'd make things a bit easier... starting with the need for a single initial seed, for example.
{quote}

why we have the different seeds:
One thing we do is support running a test class (test1(), test2(), test3()). If test2() fails, we want to be able to just run that method and reproduce it.
So we allow you to specify -Dtestmethod to only run a single method.

At the same time, we want to support doing things like creating indexes in beforeClass() and afterClass() for efficient tests.
We also support -Dtests.iter, where you run a single test method over and over... this is often convenient. If we only had 1 class-level seed, this would
be useless as it would just do the same thing over and over!

So the need for multiple seeds comes from the fact that some things are random at ""class-level"" and some things are at ""method level"". 
If you look at the 3 parts to the random seed, its really part1:part2:part3,
* part1 = class seed
* part2 = method seed
* part3 = runner seed (this is needed for consistent randomization of test methods)
","05/Oct/11 16:29;uschindler;bq. Yeah, I figured that you want to keep it compact. These may be compatible because there's nothing forbidding us to keep LuceneTestCase as a base class (descending from Assert and providing Lucene-related infrastructure).

Yes, I just wanted to mention this.

The other stuff in LuceneTestRunner is just to work around some limitations in JUnit's @BeforeClass: @BeforeClass does not pass the Class object to the annotated method, and you cannot find out which child class is initialized. So checking for annotations on the implementation class from the abstract LuceneTestCase base class does not work.

bq. Oh, by the way – is there any particular reason for so many things to be static (class level)? I get these are fixtures reused by tests but would people scream if they were object-level fixtures rather than class-level fixtures? It'd make things a bit easier... starting with the need for a single initial seed, for example.

The reason is simple: We want those per test-class lifetime, but JUnit allocates a new class instance for each test method. And lot's of Lucene tests use @BeforeClass to produce indexes (random) static indexes, then used by all test methods in a read-only way. Currently we have 3 seeds, one for class-level stuff, one for instance stuff and a third one for the runner (according to Mister [~rcmuir]: LUCENE-3362).

The randoms must therefore be static and initialized in @BeforeClass.","05/Oct/11 16:37;rcmuir;The need for the runner seed is explained in LUCENE-3362. One problem is the ""Test lifecyle"" of junit is ill-defined, it depends on how you are running tests!
{noformat}
The problem is that via ant, tests work like this (e.g. for 3 test classes):
computeTestMethods
beforeClass
afterClass
computeTestMethods
beforeClass
AfterClass
computeTestMethods
beforeClass
afterClass

but via an IDE or maven, if you run it from a folder like you did, then it does this:
computeTestMethods
computeTestMethods
computeTestMethods
beforeClass
afterClass
beforeClass
afterClass
beforeClass
afterClass 
{noformat}
","05/Oct/11 17:32;dweiss;Thanks. I did go through the code, so I know where the seeds are used and had a pretty much good understanding as to why. 

As for the different lifecycle - this is weird, but isn't it a direct consequence of subclassing BlockJUnit4ClassRunner and relying on what it internally does? This is what's causing the problem (superclass impl. changing over time - I think you just hit two different junit versions in that issue). 

Perhaps I was wrong and a custom runner is indeed needed, but if so then I still think a single seed (logically) would be fine. A custom runner then:
- collects methods to be executed (per-class)
- initializes the global init seed/ random. This random becomes the initial randomness source for everything that follows to make it repeatable.
- shuffles methods,
- execute any @BeforeClass rules (see note below),
- for each selected method (-Dtestmethod limits the selection and acts as a filter), repeat test.iter-times (seed changes predictably): {initialize per-method starting seed based on the current random, create a new test instance, execute}.
- execute any @AfterClass rules

The question how to randomize class-level fixtures could be answered by a static utility method that would return the per-class seed using ThreadLocal or a thread map updated by the runner. Still predictable and repeatable.

I'll chew a bit on the possibilities and report back tomorrow.","05/Oct/11 17:35;dweiss;Re-reading the above algorithm I think I'll make it clearer: my point is that you can write repeatable runner by starting from a single initial seed and assigning initial seeds to all execution start points (tests) regardless of whether they are executed or not (and how many times). Hope I'm a bit clear(er) now.","05/Oct/11 17:36;rcmuir;{quote}
This is what's causing the problem (superclass impl. changing over time - I think you just hit two different junit versions in that issue). 
{quote}

I disagree. I used the same junit version (4.7) myself in both eclipse and via ant to deal with this problem. It has nothing to do with that.

The junit test lifecycle is really undefined just as I described, its unfortunate.
","05/Oct/11 17:38;rcmuir;And just so you know, its not possible i could have used 4.8 here, because all of our tests fail with 4.8

Thats because of breaks in the lifecycle of TestWatchMan (Its initialized before the @Before's in 4.8, not in 4.7).
A separate problem, but just something to mention. currently you cannot use junit 4.8 with lucene's tests for this reason.","05/Oct/11 17:58;rcmuir;One last thing, thinking thru the simplifications Dawid is looking at doing, 
and knowing how horrible the code currently is, we could consider trying some things like:
* upgrade/fix our tests to work with latest junit? maybe there are less frustrations
* contribute some of the more general things like assume(String message, xxx) to junit to get them out of our codebase?","11/May/12 21:40;rcmuir;attached is a patch generalizing the UseNoExpensiveMemory annotation to @AvoidCodecs that takes a list of codecs to avoid.

This way, tests that cannot work with Lucene3x codec can just avoid it, using another codec, rather than assuming (in general its bad that many of the tests of actual new functionality often dont run at all because of the current assumes) ","12/May/12 02:18;uschindler;I like the annotation. Can we maybe change it to look like @SuppressWarnings? So it does not need codecs={} or if there is only one codec, no {} at all? Should be not too hard?

Otherwise strong +1!","12/May/12 02:20;uschindler;It's easy, just rename codecs to ""String[] value"" and you are done. After that you can use @AvoidCodecs(""SimpleText"") or @AvoidCodecs({""SimpleText"",""Lucene3x""})

See: http://docs.oracle.com/javase/1.5.0/docs/api/java/lang/SuppressWarnings.html","12/May/12 06:21;rcmuir;I agree, this is the main problem with the current patch. We should fix this before committing.","12/May/12 13:18;rcmuir;updated patch using value[], much less wordy. 

I will commit soon.","12/May/12 13:29;rcmuir;ok one last change, renamed to SuppressCodecs (it actually is not just funny, but better since it works the same way etc)","23/Jul/13 18:44;sarowe;Bulk move 4.4 issues to 4.5 and 5.0","16/Apr/14 12:54;uschindler;Move issue to Lucene 4.9.",,,,,,,,,,,,,
need a low-level base directory assert class,LUCENE-3383,12519311,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,18/Aug/11 15:57,18/Aug/11 15:57,18/Feb/21 10:07,,,,,,,,0,"idea is to have some base helper class in test-framework
that tests directories, so that we get better coverage out of anything sitting in contrib,
and out of our existing directories (not relying upon random testing to eventually catch stuff, and
not relying upon directories being in core lucene to have good tests).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,4112,,,2011-08-18 15:57:06.0,,New,,,,,,,"0|i04krz:",24631,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MockDirectoryWrapper should throw 'too many open files' if too many are open,LUCENE-3195,12510059,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,12/Jun/11 17:10,12/Jun/11 18:14,18/Feb/21 10:07,,,,,,,,0,"regardless of operating system.

We should take a good default value, like the lowest one we consider 'reasonable' macosX? and it should throw exception on openInput if openFiles hashmap is too large.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-06-12 17:20:34.247,,,false,,,,,,,,,,,,,,,,,,10787,,,Sun Jun 12 18:14:02 UTC 2011,,New,,,,,,,"0|i04lxz:",24820,,,,,,,,,,,,,,,"12/Jun/11 17:20;shaie;That's a great idea !

bq. and it should throw exception on openInput

and createOutput too?

bq. we should take a good default value

Let's just start w/ 1024? I doubt our tests need to open that many anyway, and it's, I think, low enough to fit any OS?

Also, and this can be added in future iterations, I think it'd be good if the exception includes a list of all test classes + count of open files, to make it easier to spot offending classes.

Hmmm ... now that I think of it, if our tests are run in different JVMs, we will fail to faithfully track the number of open file handles (I mean the *true* number)? In that case, we might want to lower the value to whatever we think a single JVM should allocate ... which gets tricky.","12/Jun/11 17:28;rcmuir;{quote}
Also, and this can be added in future iterations, I think it'd be good if the exception includes a list of all test classes + count of open files, to make it easier to spot offending classes.
{quote}

Well, the test spits this out when it fails already? it says ""tests run in this jvm: [TestX, TestY, TestZ]...""

{quote}
Hmmm ... now that I think of it, if our tests are run in different JVMs, we will fail to faithfully track the number of open file handles (I mean the true number)? In that case, we might want to lower the value to whatever we think a single JVM should allocate ... which gets tricky.
{quote}

Right, its not going to be totally accurate anyway, I guess I think the goal is not to emulate an operating system but instead to catch a runaway test on all platforms.

For example, as a start MDW could just check its own hashmap, as 99% of tests only make one directory, and a JVM only runs one test at a time.

But it would be better for MDW to have a static atomic counter, for better detection of the crazy tests (those are probably the ones opening too many files sometimes anyway).

","12/Jun/11 17:36;uschindler;Great idea!

I dont think it will help on Jenkins (as it was caused by something else), but its a good idea.","12/Jun/11 17:49;shaie;bq. Well, the test spits this out when it fails already? it says ""tests run in this jvm: [TestX, TestY, TestZ]...""

Ok. But for easier spotting the offending test, I meant it'd be more useful if at the point of exception you'd know which tests keep the open files and how many each test holds. Because ""TestX"" and ""TestY"" which run in this JVM may not keep any open files at the moment, so it's just noise that you need to filter? Are they reported even if they have already finished?","12/Jun/11 17:52;rcmuir;{quote}
Because ""TestX"" and ""TestY"" which run in this JVM may not keep any open files at the moment, so it's just noise that you need to filter?
{quote}

They had better not! or, they should fail because they had file leaks!

I think its useful to provide some debugging information, but on the other hand I also think the exception we throw should be as much as possible, exactly like the one the OS throws (isn't it FNFE or something strange?)
","12/Jun/11 17:54;rcmuir;but in line with what you said Shai, i think its hard to debug when one test goes over the limit, and fails, currently its like this chain reaction where all the tests in hudson then fail.

so if we add this atomic variable, maybe we check it with an assume() in beforeClass or something, so that if one test blows it up, the rest are @Ignored?
","12/Jun/11 18:14;shaie;bq. so that if one test blows it up, the rest are @Ignored?

That sounds good.

bq. They had better not! or, they should fail because they had file leaks!

Right :). But I meant what if N tests are run in parallel in the same JVM (do we do that today?), and one test causes the TooManyOpenFiles exception (or FNFE as you said), it'd be good to know which how many open files each test holds.

If tests are run sequentially in the same JVM though, then this is unnecessary and we can do w/ that global counter + assume().",,,,,,,,,,,,,,,,,,,,,,,,,,,
add more low-level tests for the suggest module,LUCENE-3134,12508101,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,23/May/11 20:33,23/May/11 20:33,18/Feb/21 10:07,,,,,,modules/spellchecker,,0,"The suggest module has some low-level serialization/deserialization tests,
but I think it needs some basic API tests such as... suggesting queries.

I think we can make a basic test class, and subclass it to test the different impls (FST,TST,Jaspell)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,10807,,,2011-05-23 20:33:52.0,,New,,,,,,,"0|i04mbb:",24880,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pendingCommit in IndexWriter is not thoroughly tested,LUCENE-3116,12507637,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,uschindler,uschindler,18/May/11 17:48,09/May/16 18:36,18/Feb/21 10:07,,3.2,4.0-ALPHA,4.9,6.0,core/index,,0,"When working on LUCENE-3084, I had a copy-paste error in my patch (see revision 1124307 and corrected in 1124316), I replaced pendingCommit by segmentInfos in IndexWriter, corrected by the following patch:

{noformat}
--- lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/IndexWriter.java (original)
+++ lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/IndexWriter.java Wed May 18 16:16:29 2011
@@ -2552,7 +2552,7 @@ public class IndexWriter implements Clos
         lastCommitChangeCount = pendingCommitChangeCount;
         segmentInfos.updateGeneration(pendingCommit);
         segmentInfos.setUserData(pendingCommit.getUserData());
-        rollbackSegments = segmentInfos.createBackupSegmentInfos(true);
+        rollbackSegments = pendingCommit.createBackupSegmentInfos(true);
         deleter.checkpoint(pendingCommit, true);
       } finally {
         // Matches the incRef done in startCommit:
{noformat}

This did not cause any test failure.

On IRC, Mike said:

{quote}
[19:21]	mikemccand: ThetaPh1: hmm
[19:21]	mikemccand: well
[19:22]	mikemccand: pendingCommit and sis only differ while commit() is running
[19:22]	mikemccand: ie if a thread starts commit
[19:22]	mikemccand: but fsync is taking a long time
[19:22]	mikemccand: and another thread makes a change to sis
[19:22]	ThetaPh1: ok so hard to find that bug
[19:22]	mikemccand: we need our mock dir wrapper to sometimes take a long time syncing....
{quote}

Maybe we need such a test, I feel bad when such stupid changes don't make any test fail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-05-18 17:52:24.577,,,false,,,,,,,,,,,,,,,,,,2959,,,Wed Apr 16 12:54:54 UTC 2014,,New,,,,,,,"0|i04mf3:",24897,,,,,,,,,,,,,,,"18/May/11 17:52;mikemccand;It's great you caught this on backport Uwe!  And, yes, spooky no tests failed...

It'll be challenging to have a test catch this.  Fixing MockDirWrapper to sometimes take ""unusually"" long time to do the fsync is a great start.  What this change would have caused is .rollback() would roll back to a wrong copy of the sis, ie not a commit point but rather a commit point plus some additional flushes.","03/Jun/11 16:40;rcmuir;bulk move 3.2 -> 3.3","06/Mar/12 02:27;rcmuir;its easy to add the sleep, but we dont even have good multithreaded tests with rollback() [except testing how exceptions are handled and not really asserting anything?]

Can we push this out to 4.0?","06/Mar/12 16:29;mikemccand;I think we can push to 4.0...","23/Jul/13 18:44;sarowe;Bulk move 4.4 issues to 4.5 and 5.0","16/Apr/14 12:54;uschindler;Move issue to Lucene 4.9.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Randomize indexed collation key testing,LUCENE-2798,12492071,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Minor,,sarowe,sarowe,sarowe,04/Dec/10 16:57,09/May/16 18:39,18/Feb/21 10:07,,3.1,4.0-ALPHA,4.9,6.0,modules/analysis,,0,"Robert Muir noted on #lucene IRC channel today that Lucene's indexed collation key testing is currently fragile (for example, they had to be revisited when Robert upgraded the ICU dependency in LUCENE-2797 because of Unicode 6.0 collation changes) and coverage is trivial (only 5 locales tested, and no collator options are exercised).  This affects both the JDK implementation in {{modules/analysis/common/}} and the ICU implementation under {{modules/icu/}}.

The key thing to test is that the order of the indexed terms is the same as that provided by the Collator itself.  Instead of the current set of static tests, this could be achieved via indexing randomly generated terms' collation keys (and collator options) and then comparing the index terms' order to the order provided by the Collator over the original terms.

Since different terms may produce the same collation key, however, the order of indexed terms is inherently unstable.  When performing runtime collation, the Collator addresses the sort stability issue by adding a secondary sort over the normalized original terms.  In order to directly compare Collator's sort with Lucene's collation key sort, a secondary sort will need to be applied to Lucene's indexed terms as well. Robert has suggested indexing the original terms in addition to their collation keys, then using a Sort over the original terms as the secondary sort.

Another complication: Lucene 3.X uses Java's UTF-16 term comparison, and trunk uses UTF-8 order, so the implemented secondary sort will need to respect that.

From #lucene:
{quote}
rmuir__: so i think we have to on 3.x, sort the 'expected list' with Collator.compare, if thats equal, then as a tiebreak use String.compareTo
rmuir__: and in the index sort on the collated field, followed by the original term
rmuir__: in 4.x we do the same thing, but dont use String.compareTo as the tiebreak for the expected list
rmuir__: instead compare codepoints (iterating character.codepointAt, or comparing .getBytes(""UTF-8""))
{quote}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LUCENE-2797,,,"11/Apr/11 15:19;sarowe;LUCENE-2798.patch;https://issues.apache.org/jira/secure/attachment/12476008/LUCENE-2798.patch","11/Apr/11 07:25;sarowe;LUCENE-2798.patch;https://issues.apache.org/jira/secure/attachment/12475983/LUCENE-2798.patch",,,,2.0,,,,,,,,,,,,,,,,,,,,2010-12-05 12:33:49.811,,,false,,,,,,,,,,,,,,,,,,11058,,,Wed Apr 16 12:54:35 UTC 2014,,New,,,,,,,"0|i04od3:",25212,,,,,,,,,,,,,,,"05/Dec/10 12:33;rcmuir;Steven, before working too hard on the jdk collation tests, i just had this idea:

Are we sure we shouldn't deprecate the jdk collation functionality (remove in trunk) and only offer ICU?

I was just thinking that the JDK Collator integration is basically a RAM trap due to its aweful keysize, etc:
http://site.icu-project.org/charts/collation-icu4j-sun

","08/Dec/10 17:53;sarowe;{quote}
Are we sure we shouldn't deprecate the jdk collation functionality (remove in trunk) and only offer ICU?

I was just thinking that the JDK Collator integration is basically a RAM trap due to its aweful keysize, etc:
http://site.icu-project.org/charts/collation-icu4j-sun
{quote}

I don't like this idea, because it removes the choice.

If there were some way to perform deprecation without eventual removal, I'd be okay with it.  The issue, as I see it, is documentaiton.  Here is an excerpt from the current class-level javadoc for {{CollationKeyFilter}}:

{quote}
The <code>ICUCollationKeyFilter</code> in the icu package of Lucene's contrib area uses ICU4J's Collator, which makes its version available, thus allowing collation to be versioned independently from the JVM.  ICUCollationKeyFilter is also significantly faster and generates significantly shorter keys than CollationKeyFilter.  See http://site.icu-project.org/charts/collation-icu4j-sun for key generation timing and key length comparisons between ICU4J and java.text.Collator over several languages.
{quote}

So an attempt is already being made to inform potential victims of the choice they're making - it even links to the same web page you mentioned.

Maybe if we move the JDK variant out of core and into a module, rather than on trunk, it would at least send a message that it's on par with the ICU variant.
","11/Apr/11 07:25;sarowe;work in progress: JDK-only Analyzer-only test: {{TestCollationKeyAnalyzer.testRandomizedCollationKeySort()}}.

The test succeeds most of the times I run it, but sometimes fails, e.g. for these seeds:

* 3253903552510972177:-5236779063463918718
* 1469913545269555695:-7929666046197505961

Robert, would you please take a look at the code and see if you can figure out why the test fails?","11/Apr/11 11:08;rcmuir;just a glance: 

it may be the use of _TestUtil.randomUnicodeString here.
it is not just avoiding supplementaries, but also avoiding things like U+FFFF

bottom line: there are serious bugs in this stuff, and even my current ""testThreadSafe"" i think is not completely avoiding them (I seem to trigger a OOM from the jre impl every few days)

I've thought about @Ignore'ing our current testThreadSafe for this reason... I don't like dancing around known bugs in a test like this, it makes the test stupid. Somehow this stuff needs to get fixed in ICU/OpenJDK.
","11/Apr/11 14:02;sarowe;bq. it may be the use of _TestUtil.randomUnicodeString here.

It may, but the first above-listed seed produces this mismatch (strings are printed out as arrays of codepoints):

{noformat}
java.lang.AssertionError: -----------
Indexed string #45: [141]
 Sorted string #45: [141]
-----------
Indexed string #46: [32]
 Sorted string #46: [28, 777]
-----------
Indexed string #47: [28, 777]
 Sorted string #47: [32]

Collator strength: SECONDARY  Collator decomposition: CANONICAL_DECOMPOSITION
{noformat}

#46 and #47 include neither supplementary chars nor problematic BMP chars.

I wrote a test including just [32] and [28,777] as indexed strings, and the same mismatch occurs for random locales, regardless of collator decomposition, and for all collator strengths except PRIMARY.
","11/Apr/11 14:16;rcmuir;{quote}
I wrote a test including just [32] and [28,777] as indexed strings, and the same mismatch occurs for random locales, regardless of collator decomposition, and for all collator strengths except PRIMARY.
{quote}

Without looking too hard (are these hex values?) in your debugging it would be useful to print the sort key as well. Are the sort keys the same?

But FYI the bugs i found in collation, somehow corrupted the internal state of RuleBasedCollator, so the exact strings you are looking at might simply be a symptom.
","11/Apr/11 14:49;sarowe;bq. Without looking too hard (are these hex values?) 

No, it's just the output from Arrays.toString(int[]), which outputs decimal.

bq. in your debugging it would be useful to print the sort key as well.

Agreed. Here's the output:

{quote}
java.lang.AssertionError: -----------
Indexed string #0: [32]
Indexed collation key: [0, 0, 0, 119, 0, 0]
 Sorted string #0: [28, 777]
Sorted collation key: [0, 0, 0, -101, 0, 0]
-----------
Indexed string #1: [28, 777]
Indexed collation key: [0, 0, 0, -101, 0, 0]
 Sorted string #1: [32]
Sorted collation key: [0, 0, 0, 119, 0, 0]

Collator strength: SECONDARY  Collator decomposition: NO_DECOMPOSITION
{quote}

(again with the Arrays.toString() for the byte array from the collation keys - obviously not ideal in that they're first converted to signed integers...)

bq. Are the sort keys the same?

No.","11/Apr/11 15:10;rcmuir;also i don't see any check that preflex codec isn't in use for this test?

","11/Apr/11 15:16;sarowe;bq. also i don't see any check that preflex codec isn't in use for this test?

{{TestCollationKeyAnalyzer.setUp()}} handles it:
{code:java}
  @Override
  public void setUp() throws Exception {
    super.setUp();
    assumeFalse(""preflex format only supports UTF-8 encoded bytes"", ""PreFlex"".equals(CodecProvider.getDefault().getDefaultFieldCodec()));
  }
{code}

And in practice, the test gets skipped 25% of the time as a result of this.
","11/Apr/11 15:19;sarowe;Added two-term collation sort test; added collation key debug printing.","23/Jul/13 18:44;sarowe;Bulk move 4.4 issues to 4.5 and 5.0","16/Apr/14 12:54;uschindler;Move issue to Lucene 4.9.",,,,,,,,,,,,,,,,,,,,,,
improve test coverage for omitNorms and omitTFAP,LUCENE-2738,12479077,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,04/Nov/10 10:54,09/May/16 18:37,18/Feb/21 10:07,,,,4.9,6.0,general/build,,0,"just expands on what lucenetestcase does already...

if you say Analyzed_NO_NORMS, we might set norms anyway.
in the same sense, if you say Index.NO, we might index it anyway, and might set omitTFAP etc.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/10 12:06;rcmuir;LUCENE-2738.patch;https://issues.apache.org/jira/secure/attachment/12458810/LUCENE-2738.patch","04/Nov/10 10:58;rcmuir;LUCENE-2738.patch;https://issues.apache.org/jira/secure/attachment/12458806/LUCENE-2738.patch","04/Nov/10 10:56;rcmuir;LUCENE-2738.patch;https://issues.apache.org/jira/secure/attachment/12458805/LUCENE-2738.patch",,,3.0,,,,,,,,,,,,,,,,,,,,2013-07-23 18:44:39.033,,,false,,,,,,,,,,,,,,,,,,2919,,,Wed Apr 16 12:54:42 UTC 2014,,New,,,,,,,"0|i04oqf:",25272,,,,,,,,,,,,,,,"04/Nov/10 10:56;rcmuir;here's the start to a patch.

worried about one fail, either i made a mistake here i don't see, or the test shouldn't be failing:
{noformat}
ant test-core -Dtestcase=TestIndexWriter -Dtestmethod=testTermVectorCorruption2 -Dtests.seed=8395558104679823604:-6279799097172774748
{noformat}","04/Nov/10 10:58;rcmuir;nevermind, that was my problem.... heres a fixed patch.

my coffee IV is not fully running yet.
","04/Nov/10 12:06;rcmuir;ok, here's a final patch... all tests pass (at least a few times).

I also improved some of the better tests, if they dont need norms to use _NO_NORMS,
and to explicitly randomly set OmitTFAP","17/Jan/11 11:47;rcmuir;Mike just reminded me about this one:
My concern for not committing is that we would actually reduce test coverage,
because most tests will create say field ""foobar"" in a loop like this:
{noformat}
for (....) {
   newField(""foobar""....);
}
{noformat}

So because removing norms/omitTFAP is infectious, i think we will end out
only testing certain cases... unless we change the patch so that this random value
is ""remembered"" per field name during the length of the test... i think thats the
right solution (adding hashmap)","23/Jul/13 18:44;sarowe;Bulk move 4.4 issues to 4.5 and 5.0","16/Apr/14 12:54;uschindler;Move issue to Lucene 4.9.",,,,,,,,,,,,,,,,,,,,,,,,,,,,
simulate disk fulls in copyBytes,LUCENE-2726,12478572,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,,rcmuir,rcmuir,28/Oct/10 13:24,09/May/16 18:35,18/Feb/21 10:07,,,,4.9,6.0,general/build,,0,"In LUCENE-2637, i disabled copyBytes optimization (so it just calls writeBytes), but i mentioned there,
that I think it would be good to try to beef up tests if we ever wanted to have an optimization like this.

the problem was that when there was an index corruption bug, it was very difficult to detect with our tests.
So I think for safety, we should do this, even though its redundant with our current impl, since we look
for this in writeBytes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/10 13:26;rcmuir;LUCENE-2726.patch;https://issues.apache.org/jira/secure/attachment/12458248/LUCENE-2726.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,2013-07-23 18:44:54.292,,,false,,,,,,,,,,,,,,,,,,2924,,,Wed Apr 16 12:54:53 UTC 2014,,New,Patch Available,,,,,,"0|i04ot3:",25284,,,,,,,,,,,,,,,"28/Oct/10 13:26;rcmuir;here's a patch, uses the same logic to check as writeBytes does.","28/Oct/10 13:34;rcmuir;So the question here, is how to account for the fact that copyBytes might call writeBytes (like it does today), and not double-count the bytes for disk full.
","23/Jul/13 18:44;sarowe;Bulk move 4.4 issues to 4.5 and 5.0","16/Apr/14 12:54;uschindler;Move issue to Lucene 4.9.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some tests catch Exceptions in separate threads and just print a stack trace - the test does not fail,LUCENE-2338,12459796,Test,Reopened,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Major,,uschindler,uschindler,uschindler,21/Mar/10 16:12,09/May/16 18:40,18/Feb/21 10:07,,,,4.9,6.0,general/build,,0,"Some tests catch Exceptions in separate threads and just print a stack trace - the test does not fail. The test should fail. Since LUCENE-2274, the LuceneTestCase(J4) class installs an UncaughtExceptionHandler, so this type of catching and solely printing a Stack trace is a bad idea. Problem is, that the run() method of threads is not allowed to throw checked Exceptions.

Two possibilities:
- Catch checked Exceptions in the run() method and wrap into RuntimeException or call Assert.fail() instead
- Use Executors",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LUCENE-2274,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2010-03-24 18:23:41.436,,,false,,,,,,,,,,,,,,,,,,3696,,,Wed Apr 16 12:54:51 UTC 2014,,New,,,,,,,"0|i04ren:",25705,,,,,,,,,,,,,,,"24/Mar/10 18:23;sanjoy;Hi Uwe,

I can work on this bug.  Do you have any test cases to reproduce this behavior?

Sanjoy","24/Mar/10 18:48;uschindler;Hi Sanjoy,

just grep on ""printStackTrace("" to find test cases that simply print out stacktraces and ignore exceptions. Possible examples can be seen also in related issues like LUCENE-1814.

Thanks for your interest in fixing this!","20/Mar/12 15:38;mikemccand;Our test framework fails tests w/ errant exceptions from threads now...","20/Mar/12 15:41;uschindler;Were all tests already converted to not supress exceptions in threads? This is why the issue is still open...","20/Mar/12 16:26;mikemccand;bq. Were all tests already converted to not supress exceptions in threads? This is why the issue is still open...

Oh, woops: I don't know!

Reopening...","23/Mar/12 20:40;hossman;Issue is marked 3.6 and actively being discussed but has no assignee - assigning to most active committer contributing patches/discussion so far to triage wether this can/should be pushed to 4.0 or not.","23/Mar/12 21:29;uschindler;I remove 3.x branch. I chacked all tests using e.printStackTrace() and all have some (outdated) logic to report the failure. In trunk we should rewrite those tests to simply fail()/rethrow as RuntimeEx on any Exception in threads. This way the failures are reported consistently.","11/Jul/12 23:03;hossman;bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment","07/Aug/12 03:41;rcmuir;rmuir20120906-bulk-40-change","23/Jul/13 18:44;sarowe;Bulk move 4.4 issues to 4.5 and 5.0","16/Apr/14 12:54;uschindler;Move issue to Lucene 4.9.",,,,,,,,,,,,,,,,,,,,,,,
Add missing tests for PayloadXxxQuery,LUCENE-2264,12456262,Test,Open,LUCENE,Lucene - Core,software,ehatcher,"Java Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform.",http://lucene.apache.org/core/,Minor,,ehatcher,uschindler,uschindler,14/Feb/10 12:49,20/May/17 00:43,18/Feb/21 10:07,,,,4.9,6.0,core/search,,0,"This is a followup for  LUCENE-1941 and the discussion in IRC. The Payload queries have no real working tests, esp they are missing for the Min/Max/Avg functions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LUCENE-1941,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2013-07-23 18:44:36.137,,,false,,,,,,,,,,,,,,,,,,11525,,,Sat May 20 00:43:02 UTC 2017,,New,,,,,,,"0|i04rv3:",25779,,,,,,,,,,,,,,,"23/Jul/13 18:44;sarowe;Bulk move 4.4 issues to 4.5 and 5.0","16/Apr/14 12:54;uschindler;Move issue to Lucene 4.9.","20/May/17 00:43;ehatcher;[~thetaphi] I think the tests are sufficient now for the payload queries.  Any gaps you see now?  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
