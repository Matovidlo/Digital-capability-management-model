Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Container),Outward issue link (Container),Outward issue link (Container),Outward issue link (Container),Outward issue link (Container),Outward issue link (Container),Outward issue link (Container),Outward issue link (Container),Outward issue link (Container),Outward issue link (Container),Outward issue link (Container),Outward issue link (Incorporates),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Tags),Custom field (Tags),Custom field (Target Version/s),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Version Upgrade Issue,HDFS-15606,13329639,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,,abpatwa,abpatwa,27/Sep/20 16:58,29/Sep/20 12:54,18/Feb/21 10:08,,,,,,,,,0,,,,,"{{020-09-28 02:11:28,114 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB}}
{{2020-09-28 02:11:28,115 INFO org.apache.hadoop.util.GSet: capacity = 2^15 = 32768 entries}}
{{2020-09-28 02:11:28,184 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /opt/hadoop/dfs/name/in_use.lock acquired by nodename 4221@Node01}}
{{2020-09-28 02:11:28,190 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage}}
{{java.io.IOException: NameNode is not formatted.}}
{{ at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:235)}}
{{ at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:978)}}
{{ at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:685)}}
{{ at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:585)}}
{{ at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:645)}}
{{ at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:819)}}
{{ at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:803)}}
{{ at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1500)}}
{{ at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1566)hdfs namenode -importCheckpoint}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2020-09-29 12:34:33.104,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 29 12:54:29 UTC 2020,,,,,,,"0|z0iyh4:",9223372036854775807,,,,,,,,,,,,,,,,,,"29/Sep/20 12:34;elek;Based on the stack trace it's an HDFS issue, not an Ozone issue. Moved to the HDFS project.","29/Sep/20 12:54;ayushtkn;What is the version of hadoop? What operation you executed.

Can you share the steps to repro?",,,,,,,,,,,,,,,
Kerberize JournalNodeSyncer unit test,HDFS-14261,13214502,Test,Reopened,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,smeng,smeng,smeng,08/Feb/19 01:16,11/Apr/20 18:24,18/Feb/21 10:08,,3.1.2,3.2.0,,journal-node,security,test,,0,,,,,This jira is an addition to HDFS-14140. Making the unit tests in TestJournalNodeSync run on a Kerberized cluster.,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-14140,,"08/Feb/19 01:41;smeng;HDFS-14261.001.patch;https://issues.apache.org/jira/secure/attachment/12957996/HDFS-14261.001.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,2019-02-08 04:12:04.111,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 11 18:13:47 UTC 2020,,,,,,,"0|yi0rrk:",9223372036854775807,,,,,,,,,,,,,3.4.0,,,,,"08/Feb/19 04:12;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 15s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 24m  8s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  1m  9s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  1m  0s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m 22s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 13m 44s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  7s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 59s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 56s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} javac {color} | {color:red}  0m 56s{color} | {color:red} hadoop-hdfs-project_hadoop-hdfs generated 1 new + 481 unchanged - 0 fixed = 482 total (was 481) {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m 10s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 16s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 78m 52s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 31s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}142m 16s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.web.TestWebHdfsTimeouts |
|   | hadoop.hdfs.server.datanode.TestBPOfferService |
|   | hadoop.hdfs.qjournal.server.TestJournalNodeSync |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:8f97d6f |
| JIRA Issue | HDFS-14261 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12957996/HDFS-14261.001.patch |
| Optional Tests |  dupname  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 681cfb6b6f0b 4.4.0-138-generic #164~14.04.1-Ubuntu SMP Fri Oct 5 08:56:16 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 4be8735 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_191 |
| findbugs | v3.1.0-RC1 |
| javac | https://builds.apache.org/job/PreCommit-HDFS-Build/26176/artifact/out/diff-compile-javac-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/26176/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/26176/testReport/ |
| Max. process+thread count | 3498 (vs. ulimit of 10000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/26176/console |
| Powered by | Apache Yetus 0.8.0   http://yetus.apache.org |


This message was automatically generated.

","08/Feb/19 18:35;weichiu;Looks good to me. Thanks for making the patch, [~smeng]
Would you think it makes sense to add a test for SSL enabled JournalNodes? That is, a test for HDFS-12579","08/Feb/19 18:36;weichiu;Actually, the test failed. Could you take a further look?","08/Feb/19 18:38;smeng;[~weichiu] Seems that flaky TestWebHdfsTimeouts leads to TestJournalNodeSync failure due to time out.
{code}
...
[ERROR]   TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout:247 expected timeout
[ERROR]   TestWebHdfsTimeouts.testTwoStepWriteConnectTimeout:247 expected timeout
[ERROR] Errors: 
[ERROR]   TestJournalNodeSync.testJournalNodeSync:200 ? Timeout Timed out waiting for co...
[ERROR]   TestJournalNodeSync.testMultipleJournalsMissingLogs:257 ? Timeout Timed out wa...
[ERROR]   TestJournalNodeSync.testMultipleJournalsMultipleMissingLogs:285 ? Timeout Time...
[ERROR]   TestJournalNodeSync.testRandomJournalMissingLogs:294 ? Timeout Timed out waiti...
[ERROR]   TestJournalNodeSync.testSyncAfterJNdowntimeWithoutQJournalQueue:390 ? Timeout ...
[ERROR]   TestJournalNodeSync.testSyncAfterJNformat:445 ? Timeout Timed out waiting for ...
[ERROR]   TestJournalNodeSync.testSyncDuringRollingUpgrade:481 ? Timeout Timed out waiti...
[ERROR]   TestJournalNodeSync.testSyncForDiscontinuousMissingLogs:236 ? Timeout Timed ou...
[ERROR]   TestJournalNodeSync.testSyncForMultipleMissingLogs:218 ? Timeout Timed out wai...
...
{code}

Tests passed locally. I think we should trigger jenkins again.","11/Feb/19 19:01;weichiu;+1","11/Feb/19 19:03;weichiu;Pushed to trunk. Thanks for the patch!","11/Feb/19 19:15;hudson;SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #15929 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/15929/])
HDFS-14261. Kerberize JournalNodeSyncer unit test. Contributed by Siyao (weichiu: rev 5c10630ad8c976380491adec8e2d9f3e49ea8fa9)
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournalNodeSync.java
","01/Mar/19 03:46;ayushtkn;Hi [~smeng] [~weichiu]

Seems Like TestJournalNodeSync is failing after this in trunk.
Some authentication problem I guess. Doesn't seem to be related with fix, should be in UT itself. Mind giving a check

{noformat}
javax.net.ssl.SSLHandshakeException: Error while authenticating with endpoint: https://localhost:43003/getJournal?jid=ns1&segmentTxId=5&storageInfo=-65%3A1841012157%3A1551342985924%3AtestClusterID&inProgressOk=false
	at sun.reflect.GeneratedConstructorAccessor76.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.wrapExceptionWithMessage(KerberosAuthenticator.java:232)
	at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:216)
	at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:348)
	at org.apache.hadoop.hdfs.web.URLConnectionFactory.openConnection(URLConnectionFactory.java:219)
{noformat}

FYI. It seems pretty independent to web hdfs timeouts. It failed here independently too.

https://builds.apache.org/job/PreCommit-HDFS-Build/26359/testReport/","03/Mar/19 01:03;weichiu;Reopened. The test is failing almost consistently. [~smeng] could you please look into the test failure? I reverted the test in the meantime.","03/Mar/19 01:17;hudson;SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #16110 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/16110/])
Revert ""HDFS-14261. Kerberize JournalNodeSyncer unit test. Contributed (weichiu: rev a54839ef99b0a47e85c41931102c554a3412ce5d)
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournalNodeSync.java
","04/Mar/19 08:15;hudson;SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #16113 (See [https://builds.apache.org/job/Hadoop-trunk-Commit/16113/])
Revert ""HDFS-14261. Kerberize JournalNodeSyncer unit test. Contributed (aajisaka: rev 6c4d566955084f7ea5e8b0208db7d54c6ae52ef1)
* (edit) hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournalNodeSync.java
","04/Mar/19 23:53;smeng;[~ayushtkn] Yeah I also noted a lot of time outs due to Kerberized JournalNodeSync unit tests lately. Tests passed locally but need to look into why it fails so often under heavy load (jenkins). I asked [~weichiu] to revert it at the moment. Thanks!","11/Apr/20 18:13;brahmareddy;Removing the fix version as this is reverted.",,,,
testUpgradeCommand fails following tests in TestDFSAdminWithHA on Windows,HDFS-13630,13162457,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,,huanbang1993,huanbang1993,29/May/18 00:22,29/May/18 00:32,18/Feb/21 10:08,,,,,,,,,0,,,,,"32 tests in [TestDFSAdminWithHA|https://builds.apache.org/job/hadoop-trunk-win/479/testReport/org.apache.hadoop.hdfs.tools/TestDFSAdminWithHA/] fail on Windows after testUpgradeCommand with error message:
Could not format one or more JournalNodes. 1 exceptions thrown:
{color:#d04437}127.0.0.1:58098: Directory F:\short\hadoop-trunk-win\s\hadoop-hdfs-project\hadoop-hdfs\target\test\data\1\dfs\journalnode-0\ns1 is in an inconsistent state: Can't format the storage directory because the current directory is not empty.
 at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.checkEmptyCurrent(Storage.java:600)
 at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.analyzeStorage(Storage.java:683)
 at org.apache.hadoop.hdfs.qjournal.server.JNStorage.format(JNStorage.java:210)
 at org.apache.hadoop.hdfs.qjournal.server.Journal.format(Journal.java:236)
 at org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.format(JournalNodeRpcServer.java:181)
 at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.format(QJournalProtocolServerSideTranslatorPB.java:148)
 at org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:27399)
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
 at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
 at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
 at java.security.AccessController.doPrivileged(Native Method)
 at javax.security.auth.Subject.doAs(Subject.java:422)
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1687)
 at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682){color}

Restarting NN1 with -upgrade option seems to keep the journalnode directory from being released after testUpgradeCommand.
{code:java}
    // Start NN1 with -upgrade option
    dfsCluster.getNameNodeInfos()[0].setStartOpt(
        HdfsServerConstants.StartupOption.UPGRADE);
    dfsCluster.restartNameNode(0, true);
{code}

branch-2 does not have this issue, because there is no testUpgradeCommand in branch-2.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-05-29 00:32:43.96,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 29 00:32:43 UTC 2018,,,,,,,"0|i3u8kn:",9223372036854775807,,,,,,,,,,,,,,,,,,"29/May/18 00:30;huanbang1993;I have not figured out the root cause to this, but would suggest randomize the base directory for MiniDFSCluster to isolate tests from each other.
[~elgoiri] any thoughts here?","29/May/18 00:32;elgoiri;Yes, let's go with the randomized path.",,,,,,,,,,,,,,,
TestHDFSFileSystemContract#testAppend times out on Windows on the first run,HDFS-13624,13162177,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,,huanbang1993,huanbang1993,25/May/18 23:16,14/Jun/18 18:17,18/Feb/21 10:08,,,,,,,,,0,,,,,"Sometimes, TestHDFSFileSystemContract#testAppend takes a long time for the first time in WindowsSelectorImpl$poll0:
{code:java}
private native int poll0(long var1, int var3, int[] var4, int[] var5, int[] var6, long var7);
{code}
Second run of the test does not time out.",,,,,,,,,,,,,,,,,,,,,,,,HDFS-13620,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2018-05-25 23:16:01.0,,,,,,,"0|i3u6uf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HDFS memory issues,HDFS-13597,13160533,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,Exam Dumps,Exam Dumps,19/May/18 19:52,19/May/18 19:55,18/Feb/21 10:08,,2.7.4,,HDFS-7285,balancer & mover,,,,0,performance,,,,Please help me with this issue asap,PROD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Sat May 19 19:55:21 UTC 2018,,,,,,,"0|i3tx4f:",9223372036854775807,,,,,,,,,,,,,3.0.2,,,,,"19/May/18 19:55;Exam Dumps;We are looking into this issue and will get back to you soon.",,,,,,,,,,,,,,,,
Add test to validate dfs used and no of blocks when blocks are moved across volumes,HDFS-13089,13134924,Test,Patch Available,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,ajayydv,ajayydv,ajayydv,30/Jan/18 23:24,05/Mar/18 18:45,18/Feb/21 10:08,,,,,,,,,0,,,,,Add test to validate dfs used and no of blocks when blocks are moved across volumes,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/18 23:26;ajayydv;HDFS-13089.000.patch;https://issues.apache.org/jira/secure/attachment/12908454/HDFS-13089.000.patch","27/Feb/18 23:06;ajayydv;HDFS-13089.001.patch;https://issues.apache.org/jira/secure/attachment/12912341/HDFS-13089.001.patch",,,,2.0,,,,,,,,,,,,,,,,,,,,2018-01-31 02:44:34.206,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 05 18:45:50 UTC 2018,,,,,,,"0|i3pkh3:",9223372036854775807,,,,,,,,,,,,,,,,,,"31/Jan/18 02:44;genericqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 18s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 15m 11s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 35s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 54s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m 57s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 50s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 48s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:red}-1{color} | {color:red} mvninstall {color} | {color:red}  1m 10s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 50s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 31s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  1s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green}  9m 54s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 47s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 46s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red}147m 22s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 21s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}193m 49s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA |
|   | hadoop.hdfs.web.TestWebHdfsTimeouts |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure |
|   | hadoop.tools.TestHdfsConfigFields |
|   | hadoop.hdfs.TestReadStripedFileWithMissingBlocks |
|   | hadoop.hdfs.server.namenode.TestDecommissioningStatus |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HDFS-13089 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12908454/HDFS-13089.000.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux 5568ef2dedcb 4.4.0-64-generic #85-Ubuntu SMP Mon Feb 20 11:50:30 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / f9dd5b6 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| mvninstall | https://builds.apache.org/job/PreCommit-HDFS-Build/22896/artifact/out/patch-mvninstall-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/22896/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/22896/testReport/ |
| Max. process+thread count | 3812 (vs. ulimit of 5000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/22896/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

","22/Feb/18 22:25;ajayydv;{{TestDistributedFileSystem#testTotalDfsUsed}} does check DFS used at filesystem level but this test is more granular .","27/Feb/18 18:31;arp;Hi [~ajayydv], looks like this patch needs to be rebased to current trunk.","27/Feb/18 18:33;arp;+1 for the change once its rebased, with one minor comment. Let's increase the test timeout from 1minute to 10 minutes, to avoid spurious failures.","27/Feb/18 23:07;ajayydv;[~arpitagarwal], thanks for review. patch v1 rebased for trunk and increased timeout.","27/Feb/18 23:09;arp;+1 pending Jenkins.","28/Feb/18 01:36;genericqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 21s{color} | {color:blue} Docker mode activated. {color} |
|| || || || {color:brown} Prechecks {color} ||
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
|| || || || {color:brown} trunk Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 20m  3s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 56s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 39s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  2s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 12m  6s{color} | {color:green} branch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 53s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 56s{color} | {color:green} trunk passed {color} |
|| || || || {color:brown} Patch Compile Tests {color} ||
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 36s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 57s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} shadedclient {color} | {color:green} 11m 10s{color} | {color:green} patch has no errors when building and testing our client artifacts. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 52s{color} | {color:green} the patch passed {color} |
|| || || || {color:brown} Other Tests {color} ||
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 90m  6s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 23s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}145m 53s{color} | {color:black} {color} |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=17.05.0-ce Server=17.05.0-ce Image:yetus/hadoop:5b98639 |
| JIRA Issue | HDFS-13089 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12912341/HDFS-13089.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  shadedclient  findbugs  checkstyle  |
| uname | Linux bc4514088e25 3.13.0-139-generic #188-Ubuntu SMP Tue Jan 9 14:43:09 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 727c033 |
| maven | version: Apache Maven 3.3.9 |
| Default Java | 1.8.0_151 |
| findbugs | v3.1.0-RC1 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/23231/artifact/out/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/23231/testReport/ |
| Max. process+thread count | 4003 (vs. ulimit of 10000) |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/23231/console |
| Powered by | Apache Yetus 0.8.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

","05/Mar/18 18:45;ajayydv;test failure due to OOM is unrelated to patch.",,,,,,,,,
HDFS unit test failure in AArch64. TestDirectoryScanner.testThrottling: Throttle is too permissive,HDFS-12822,13118790,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,,guangming,guangming,16/Nov/17 03:23,17/Nov/17 05:26,18/Feb/21 10:08,,3.1.0,,,test,,,,1,dtest,easyfix,maven,test,"Description:  Hi,  When I ran the HDFS unit test and got a failure in TestDirectoryScanner.java test case :
TestDirectoryScanner.testThrottling:624 Throttle is too permissive
detail:
                Running org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner
                Tests run: 7, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 227.046 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner
                testThrottling(org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner)  Time elapsed: 198.014 sec  <<< FAILURE!
                java.lang.AssertionError: Throttle is too permissive
                                                at org.junit.Assert.fail(Assert.java:88)
                                                at org.junit.Assert.assertTrue(Assert.java:41)
                                                at org.apache.hadoop.hdfs.server.datanode.TestDirectoryScanner.testThrottling(TestDirectoryScanner.java:624)

And below is the failure part of source code TestDirectoryScanner.java:

{code:java}
      ...........
      while ((retries > 0) && ((ratio < 7f) || (ratio > 10f))) {
        scanner = new DirectoryScanner(dataNode, fds, conf);
        ratio = runThrottleTest(blocks);
        retries -= 1;
      }

      // Waiting should be about 9x running.
      LOG.info(""RATIO: "" + ratio);
      assertTrue(""Throttle is too restrictive"", ratio <= 10f);
      assertTrue(""Throttle is too permissive"", ratio >= 7f);
    ............
    private float runThrottleTest(int blocks) throws IOException {
      scanner.setRetainDiffs(true);
      scan(blocks, 0, 0, 0, 0, 0);
      scanner.shutdown();
      assertFalse(scanner.getRunStatus());
      return (float)scanner.timeWaitingMs.get() / scanner.timeRunningMs.get();
    }
   .............

{code}

The ratio in my test is 6.0578866, which is smaller than 7f in the code. So the code thrown out an assertTrue failure.
My questions are: 
1. Why the ratio was set between 7f and 10f, is it a empirical value?
       2. The ratio is smaller than 7f in AArch64 platform, is this value within normal range?

Could anyone help? Thanks a lot. ","ARMv8 AArch64, Ubuntu16.04",604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-11-17 05:23:21.35,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Fri Nov 17 05:23:21 UTC 2017,,,,,,,"0|i3mupz:",9223372036854775807,,,,,,,,,,,,,,,,,,"17/Nov/17 05:23;Eugene.Xie;That puzzles me as well. How came the expected ratio?",,,,,,,,,,,,,,,,
Upgrade JUnit from 4 to 5 in hadoop-hdfs,HDFS-12254,13092129,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,ajayydv,aajisaka,aajisaka,03/Aug/17 09:01,17/Aug/17 09:50,18/Feb/21 10:08,,,,,test,,,,0,,,,,Feel free to create sub-tasks for each module.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-08-09 17:32:05.272,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 17 09:50:22 UTC 2017,,,,,,,"0|i3id53:",9223372036854775807,,,,,,,,,,,,,,,,,,"09/Aug/17 17:32;ajayydv;hi [~ajisakaa] , As i understand , we can do it in two ways.
# 1. Update the junit dependency in hadoop-main to junit5 with junit-jupiter-engine. which will require changes in most of the test cases to move them to new junit5 api and platform. (More riskier)
# 2. Update the junit dependency in hadoop-main to junit5 while maintaining backward compatibility to test cases built in junit4 using (junit-vintage-engine). As a next step we can create new test cases using junit 5 api and move old test cases to junit5 in steps. This will be incremental change with less risk to breaking old test cases.

Any ideas, suggestions on this?
","17/Aug/17 09:50;aajisaka;+1 for the option 2. Thanks.",,,,,,,,,,,,,,,
TestCacheDirectives.testWaitForCachedReplicas failed intermittently ,HDFS-11632,13062283,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,xyao,xyao,06/Apr/17 22:51,09/Apr/17 06:52,18/Feb/21 10:08,,,,,,,,,0,,,,,"This was found in recent Jenkins [run|https://builds.apache.org/job/PreCommit-HDFS-Build/18997/testReport/org.apache.hadoop.hdfs.server.namenode/TestCacheDirectives/testWaitForCachedReplicas/]

{code}
Error Message

expected:<16384> but was:<20480>
Stacktrace

java.lang.AssertionError: expected:<16384> but was:<20480>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives.testWaitForCachedReplicas(TestCacheDirectives.java:965)
Standard Output
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-04-09 06:50:53.565,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 09 06:50:53 UTC 2017,,,,,,,"0|i3dbx3:",9223372036854775807,,,,,,,,,,,,,,,,,,"09/Apr/17 06:50;linyiqun;Thanks [~xyao] for reporting this.
I am thinking the root cause of this is that the cache blocks info is not completely reported to namenode when Jenkins is busy. In addition, I see the cache report interval time is 10s by default. That means the second cache report must be sent after 10s from the first report time. So I think there is a chance to cause the test failure.
Hi [~xyao], any comments for this? Will attach the patch after your further comments. Thanks.",,,,,,,,,,,,,,,,
[Umbrella] Additional testing for striped erasure coding,HDFS-11349,13036022,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,andrew.wang,andrew.wang,18/Jan/17 22:38,18/Jan/17 23:55,18/Feb/21 10:08,,3.0.0-alpha1,,,erasure-coding,test,,,0,,,,,Umbrella for testing work for striped erasure coding.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-01-18 22:38:13.0,,,,,,,"0|i38wen:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding unit test for SNN checkpoint with unfinalized rollingUpgrade,HDFS-11341,13034856,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,xyao,xyao,13/Jan/17 23:42,13/Jan/17 23:42,18/Feb/21 10:08,,,,,,,,,0,,,,,"This is a followup work for HDFS-11209. 

The repro of HDFS-11209 is a bit tricky with MiniDFSCluter as we need to run old version of NN with """"hdfs dfsadmin -rollingUpgrade prepare"" to create a fsiamge with the old layoutversion. Then do the upgrade and run the primary namenode(new software layout version) with ""-rollingUpgrade started"" option and secondary namenode (new software layout version) as normal.

The software layout version is determined by static method from LayoutVersion class which is not supported with mockito. It is possible to do that with powermock + mockito. 

This ticket is opened to add unit test for the scenarios that is fixed by HDFS-11209.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2017-01-13 23:42:43.0,,,,,,,"0|i38p8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Balancer behavior when some block moves are slow,HDFS-11051,13015147,Test,Patch Available,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,linyiqun,zhz,zhz,25/Oct/16 17:15,27/Jan/17 07:46,18/Feb/21 10:08,,,,,balancer & mover,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-11015,,,,"26/Oct/16 02:26;linyiqun;HDFS-11051.001.patch;https://issues.apache.org/jira/secure/attachment/12835241/HDFS-11051.001.patch","17/Nov/16 10:37;linyiqun;HDFS-11051.002.patch;https://issues.apache.org/jira/secure/attachment/12839340/HDFS-11051.002.patch",,,,2.0,,,,,,,,,,,,,,,,,,,,2016-10-26 00:33:54.421,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 27 07:46:38 UTC 2017,,,,,,,"0|i35dfj:",9223372036854775807,,,,,,,,,,,,,,,,,,"26/Oct/16 00:33;linyiqun;Hi [~zhz], I am ready to work on this. But before I do that, I have one question: Is this just mean that we should set a timeout in balancer as HDFS-11015 has mentioned? Do you have any other suggestions? Thanks!","26/Oct/16 02:26;linyiqun;I make a code glance for the balancer tests and add the block-move timeout in related tests. Since that some tests reuse the method {{TestBalancer#initConf}}, I don't need to modify each test. Attach a initial patch.

Hi [~zhz], if you think the block-move timeout is not a suitable value here or you have other suggestions, you can just let me know. Thanks!","26/Oct/16 03:42;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 13s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 3 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  6m 44s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 43s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 28s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 51s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 12s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 41s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  0m 44s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 41s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 41s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 25s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  0m 48s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m  9s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  1m 46s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 37s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 56m  6s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 17s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 74m  9s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.datanode.TestFsDatasetCache |
| Timed out junit tests | org.apache.hadoop.hdfs.server.namenode.ha.TestPendingCorruptDnMessages |
|   | org.apache.hadoop.hdfs.server.namenode.TestINodeFile |
|   | org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot |
|   | org.apache.hadoop.cli.TestErasureCodingCLI |
|   | org.apache.hadoop.hdfs.server.namenode.ha.TestLossyRetryInvocationHandler |
|   | org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA |
|   | org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover |
|   | org.apache.hadoop.hdfs.server.namenode.ha.TestQuotasWithHA |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:9560f25 |
| JIRA Issue | HDFS-11051 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12835241/HDFS-11051.001.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux e3b56c45cfe8 3.13.0-95-generic #142-Ubuntu SMP Fri Aug 12 17:00:09 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / d88dca8 |
| Default Java | 1.8.0_101 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/17287/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/17287/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/17287/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

","12/Nov/16 02:56;zhz;Thanks [~linyiqun] for working on this. When creating the JIRA I was thinking about creating a new test to emulate the scenario where some block moves are slow. This will test our Balancer patches such as HDFS-11015.","17/Nov/16 10:37;linyiqun;Sorry for the delay response, thanks for your concrete explanation for this JIRA, [~zhz]. Attach a initial patch for this. Kindly review.","17/Nov/16 12:24;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m 34s{color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green}  0m  0s{color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green}  0m  0s{color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  9m 46s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 56s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green}  0m 37s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  8s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 17s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m 14s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 49s{color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green}  1m  1s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green}  0m 55s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green}  0m 55s{color} | {color:green} the patch passed {color} |
| {color:orange}-0{color} | {color:orange} checkstyle {color} | {color:orange}  0m 32s{color} | {color:orange} hadoop-hdfs-project/hadoop-hdfs: The patch generated 5 new + 206 unchanged - 0 fixed = 211 total (was 206) {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green}  1m  3s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green}  0m 13s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green}  0m  0s{color} | {color:green} The patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green}  2m  9s{color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green}  0m 47s{color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 80m 53s{color} | {color:red} hadoop-hdfs in the patch failed. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green}  0m 20s{color} | {color:green} The patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black}105m 53s{color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| Failed junit tests | hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:a9ad5d6 |
| JIRA Issue | HDFS-11051 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12839340/HDFS-11051.002.patch |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux cbddfe11f6a1 3.13.0-96-generic #143-Ubuntu SMP Mon Aug 29 20:15:20 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / aab9737 |
| Default Java | 1.8.0_111 |
| findbugs | v3.0.0 |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/17598/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/17598/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs.txt |
|  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/17598/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/17598/console |
| Powered by | Apache Yetus 0.4.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

","27/Jan/17 07:46;linyiqun;Hi [~zhz], could you take a look for the latest patch? I have make a test to test the timeout scenario of balancer. Thanks.",,,,,,,,,,
Improve Testing for NN Lock Logging,HDFS-10829,13002184,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,xkrogen,xkrogen,xkrogen,01/Sep/16 22:20,02/Jun/18 17:45,18/Feb/21 10:08,,,,,logging,namenode,,,0,,,,,Currently the testing for the namenode logging when locks are held for a long time is a little bit lacking. Specifically some of the multithreaded tests from HDFS-10817 did not end up making it in. We should get these tests back into the build and do similar things for the write locking and testing the throttling (HDFS-10713). ,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-10817,HDFS-10713,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-07-28 00:32:36.559,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 20 02:25:15 UTC 2018,,,,,,,"0|i335nb:",9223372036854775807,,,,,,,,,,,,,2.7.8,,,,,"28/Jul/17 00:32;shv;Moving target version to 2.7.5 due to 2.7.4 release.","15/Dec/17 20:27;shv;Moving target version to 2.7.6 due to 2.7.5 release.","20/Apr/18 02:25;shv;Moving target version to 2.7.7 due to 2.7.6 release.",,,,,,,,,,,,,,
MiniDFSCluster#restartDataNode does not always stop DN before start DN,HDFS-10371,12965320,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,xiaobingo,xyao,xyao,05/May/16 16:38,06/May/16 03:27,18/Feb/21 10:08,,,,,test,,,,0,,,,,"This could cause intermittent port binding problem if the keep the same port option is chosen as evident in the recent [Jenkins|https://builds.apache.org/job/PreCommit-HDFS-Build/15366/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_91.txt]

{code}
Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 53.772 sec <<< FAILURE! - in org.apache.hadoop.hdfs.TestDecommissionWithStriped
testDecommissionWithURBlockForSameBlockGroup(org.apache.hadoop.hdfs.TestDecommissionWithStriped)  Time elapsed: 6.946 sec  <<< ERROR!
java.net.BindException: Problem binding to [localhost:52957] java.net.BindException: Address already in use; For more details see:  http://wiki.apache.org/hadoop/BindException
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.apache.hadoop.ipc.Server.bind(Server.java:530)
	at org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:793)
	at org.apache.hadoop.ipc.Server.<init>(Server.java:2592)
	at org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:958)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:563)
	at org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:538)
	at org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:800)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initIpcServer(DataNode.java:932)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1297)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:479)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2584)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2472)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2519)
	at org.apache.hadoop.hdfs.MiniDFSCluster.restartDataNode(MiniDFSCluster.java:2242)
	at org.apache.hadoop.hdfs.TestDecommissionWithStriped.testDecommissionWithURBlockForSameBlockGroup(TestDecommissionWithStriped.java:254)

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-05-06 01:36:56.904,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 06 01:36:57 UTC 2016,,,,,,,"0|i2x7t3:",9223372036854775807,,,,,,,,,,,,,,,,,,"05/May/16 16:42;xyao;Many tests such as {{TestDecommissionWithStriped.testDecommissionWithURBlockForSameBlockGroup}} use the one is subject to the similar binding problem.
{code}
  public synchronized boolean restartDataNode(DataNodeProperties dnprop, boolean keepPort)
{code}

This ticket is opened to fix them with the new one introduced by HDFS-7886, which stops DN before restart.
{code}
public synchronized boolean restartDataNode(
      int idn, boolean keepPort, boolean expireOnNN) throws IOException {
    DataNodeProperties dnprop = stopDataNode(idn);
    if(expireOnNN) {
      setDataNodeDead(dnprop.datanode.getDatanodeId());
    }
    if (dnprop == null) {
      return false;
    } else {
      return restartDataNode(dnprop, keepPort);
    }
  }
{code}
","06/May/16 01:36;linyiqun;I suggest that we should change the param {{expireOnNN}} from false to true in these two {{restartDataNode}}. So the dn will be stopped and restart.
{code}
  /*
   * Restart a particular datanode, use newly assigned port
   */
  public boolean restartDataNode(int i) throws IOException {
    return restartDataNode(i, false);
  }

  /*
   * Restart a particular datanode, on the same port if keepPort is true
   */
  public synchronized boolean restartDataNode(int i, boolean keepPort)
      throws IOException {
    return restartDataNode(i, keepPort, false);
  }
{code}","06/May/16 01:36;linyiqun;I suggest that we should change the param {{expireOnNN}} from false to true in these two {{restartDataNode}}. So the dn will be stopped and restart.
{code}
  /*
   * Restart a particular datanode, use newly assigned port
   */
  public boolean restartDataNode(int i) throws IOException {
    return restartDataNode(i, false);
  }

  /*
   * Restart a particular datanode, on the same port if keepPort is true
   */
  public synchronized boolean restartDataNode(int i, boolean keepPort)
      throws IOException {
    return restartDataNode(i, keepPort, false);
  }
{code}",,,,,,,,,,,,,,
Introduce unit tests framework for HDFS UI,HDFS-10184,12951837,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,wheat9,wheat9,21/Mar/16 00:24,22/Mar/16 10:14,18/Feb/21 10:08,,,,,,,,,0,,,,,The current HDFS UI is based on HTML5 and it does not have unit tests yet. Occasionally things break and we can't catch it. We should investigate and introduce unit test frameworks such as Mocha for the UI.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2016-03-21 12:08:14.186,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 22 10:14:01 UTC 2016,,,,,,,"0|i2uxpj:",9223372036854775807,,,,,,,,,,,,,,,,,,"21/Mar/16 12:08;stevel@apache.org;can  you see if you can get away with HtmlUnit first? That uses the JVM's own JS engine & so doesn't need the browser —it'll be able to run under Jenkins. As it's designed to run under JUnit, it'll be maintainable by the current set of java developers, without anyone having to learn node.js &c","21/Mar/16 17:21;wheat9;Unless we move to Java 8 this is a non starter. The UI is heavily driven by HTML 5. Rhino, the JavaScript engine in Java 7, is at least 10x slower than Nashorn and node.js.

I have written a JavaScript version of dfsadmin using Rhino and the performance is very unsatisfactory. Given the scope of the tests I'm not fully convinced it is a good idea.

I believe that the new Web UI in YARN is adopiting npm / node.js -- these tools will be integrated with the current Jenkins workflow so the integration should not be an issue.","22/Mar/16 10:14;stevel@apache.org;OK. How well would this work on java 8? Is the limitation JVM or something more fundamental about test validity?",,,,,,,,,,,,,,
Erasure Coding: need a way to test multiple EC policies,HDFS-9962,12950194,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,tasanuma,lirui,lirui,15/Mar/16 06:12,16/Oct/17 03:02,18/Feb/21 10:08,,,,,,,,,0,hdfs-ec-3.0-nice-to-have,,,,"Now that we support multiple EC policies, we need a way test it to catch potential issues.

*UPDATE:*
Since many existing EC unit tests need some changes to use multiple EC policies, it would be good to create a jira for every few tests. Based on some discussion in HDFS-7866 and HDFS-9962, this task follows the below idea.

* Default policy (RS-6-3) always gets tested.
* Create a random ec policy test if the test is long-running (e.g. using minicluster many times).
* If the test is not long, all policies test with parametrizing would be better.

If you are interested in this work, please join it and create a jira under this jira.
",,,,,,,,,,,,,,,,,,,,,,,HDFS-11823,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2017-01-11 02:05:05.419,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 06 07:52:06 UTC 2017,,,,,,,"0|i2unkf:",9223372036854775807,,,,,,,,,,,,,,,,,,"15/Mar/16 06:18;lirui;As I proposed in HDFS-7866, one possible solution is to randomly choose a policy in each test. Kai also suggested to make sure the default policy always gets tested.
[~drankye], [~zhz], let's use this JIRA to reach an agreement. Would like to know your opinions. Thanks.","11/Jan/17 02:05;andrew.wang;It looks like we still lack complete test coverage, I see that RS (10,4), RS (6,3), and XOR 2,1 are covered by subclasses of TestDFSStripedInputStream but we're still missing RS (3,2). Not sure how comprehensive other tests are either.","21/Mar/17 08:58;tasanuma;Hi, thank you for creating this jira , [~lirui], and thank you for organizing ec jiras lately, [~andrew.wang]!

Do you plan to work on this topic? If not, can I work on this jira? :)","22/Mar/17 02:15;lirui;[~tasanuma0829], feel free to take the JIRA. Thanks.","22/Mar/17 03:21;tasanuma;[~lirui], thanks for your reply. I'd like to take it.","20/Apr/17 08:19;Sammi;Hi [~tasanuma0829], do you still plan to work on this? ","26/Apr/17 05:13;tasanuma;Hi [~Sammi], yes I still plan to do. Sorry for late.","15/May/17 08:09;tasanuma;Created HDFS-11823 for extending {{TestDFSStripedIutputStream}}/{{TestDFSStripedOutputStream}}/{{TestDFSStripedOutputStreamWithFailure}}.","02/Jun/17 09:38;tasanuma;It seems many existing EC unit tests need some changes if they use non-default EC policies. I think it would be good to create a jira for every few tests. I made this jira the umbrella jira. If you are interested in this work, please join it and create a jira under this jira. If you have any opinions, please let me know.","05/Oct/17 13:15;tasanuma;Hello [~andrew.wang] and [~drankye],

While writing the patches for the subtasks of this jira, I thought it is better to check all EC policy every time by parametrized test, rather than choosing an EC policy randomly. Comparing to the random policy test (see HDFS-12547), the parametrized test (see HDFS-12587) keeps idempotency and simple code. As a disadvantage, the parameterized test may become more flakey (especially when using minicluster).

I want to hear your opinions. Which do you think is better?","06/Oct/17 01:05;andrew.wang;Hi [~tasanuma0829], thanks for working on this,

Is your concern about flakiness related to test timeouts? We've been struggling with that recently, since increasing the cell size from 64k to 1MB greatly increased test runtimes. The approach we've been taking is to split long running tests into separate classes, or ""parameterizing"" by creating a lot of subclasses.

We randomized the policy tests to reduce the runtime of the test suite, under the belief that there wasn't much incremental benefit from running all of them every time. I think that's still true, and would prefer we randomize any long-running tests. If the tests run quickly, then it's okay to be exhaustive.","06/Oct/17 07:52;tasanuma;Thanks for you comments, [~andrew.wang]. Yes, test timeouts may increase if we use parametrized tests.

I understand. I will use random policy tests for long-running tests, and using all ec policies by the parameterized test when it doesn't use minicluster.",,,,,
Add more SnapshotDiffReport unit tests,HDFS-9633,12928885,Test,Patch Available,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,jzhuge,jzhuge,jzhuge,09/Jan/16 18:16,25/Mar/16 17:53,18/Feb/21 10:08,,,,,,,,,0,,,,,"Add more behavior-based unit tests to verify and document SnapshotDiffReport API behaviors:
* DeleteRecreateSameName: Delete a file, then create a new file of the same
  name. Expect a DELETE and a CREATE entry instead of one MODIFY entry.
* CircularRenames: Rename file ""a"" to ""tmp"", rename ""b"" to ""a"", then rename
  ""tmp"" to ""b"". Expect RENAME(a->b) and RENAME(b->a) entry.

These unit tests will prevent any future NN change from accidentally breaking SnapshotDiffReport API.

Has anyone built incremental copy/migration tools based on command {{hdfs snapshotDiff}} or directly on the API?",,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/16 19:14;jzhuge;HDFS-9633-001.patch;https://issues.apache.org/jira/secure/attachment/12781892/HDFS-9633-001.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,2016-01-12 22:29:53.402,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 24 14:19:58 UTC 2016,,,,,,,"0|i2r2xr:",9223372036854775807,,,,,,,,,,,,,,,,,,"12/Jan/16 19:21;jzhuge;Diff report for testDiffReportWithDeleteCreateSameName:
{noformat}
Difference between snapshot s0 and snapshot s1 under directory /:
M       .
+       ./foo
-       ./foo
{noformat}

Diff report for testDiffReportWithCircularRenames:
{noformat}
Difference between snapshot s0 and snapshot s1 under directory /:
M       .
R       ./bar -> ./foo
R       ./foo -> ./bar
{noformat}

SnapshotDiffReport works as expected. I will verify distcp handles these cases properly.","12/Jan/16 22:29;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 0s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 7m 20s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 38s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 40s {color} | {color:green} trunk passed with JDK v1.7.0_91 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 16s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 51s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 1m 51s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 4s {color} | {color:green} trunk passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 47s {color} | {color:green} trunk passed with JDK v1.7.0_91 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 45s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 37s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 37s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 39s {color} | {color:green} the patch passed with JDK v1.7.0_91 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 39s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 15s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 0m 48s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 11s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 0s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 4s {color} | {color:green} the patch passed with JDK v1.8.0_66 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 44s {color} | {color:green} the patch passed with JDK v1.7.0_91 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 53m 50s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.8.0_66. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 49m 49s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.7.0_91. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 20s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 129m 14s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_66 Failed junit tests | hadoop.hdfs.tools.TestDFSZKFailoverController |
|   | hadoop.hdfs.server.datanode.TestBlockScanner |
|   | hadoop.hdfs.server.namenode.TestStartup |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure030 |
| JDK v1.7.0_91 Failed junit tests | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure160 |
|   | hadoop.hdfs.server.namenode.TestStartup |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:0ca8df7 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12781892/HDFS-9633-001.patch |
| JIRA Issue | HDFS-9633 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux a6a12853dad1 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 30c7dfd |
| Default Java | 1.7.0_91 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_66 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_91 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/14106/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_66.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/14106/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_91.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/14106/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_66.txt https://builds.apache.org/job/PreCommit-HDFS-Build/14106/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_91.txt |
| JDK v1.7.0_91  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/14106/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Max memory used | 75MB |
| Powered by | Apache Yetus 0.2.0-SNAPSHOT   http://yetus.apache.org |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/14106/console |


This message was automatically generated.

","24/Mar/16 02:00;yzhangal;Hi [~jzhuge],

Thanks for reporting and working on the jira. Would you please elaborate what ""strange scenario""? Is it only about  confusion or that you are trying to fix something (your uploaded patch only touched the test code)?

Thanks.

","24/Mar/16 02:03;jzhuge;No, I am not trying fix any problem, just adding more test cases.","24/Mar/16 14:19;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 20s {color} | {color:blue} Docker mode activated. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 11m 8s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 32s {color} | {color:green} trunk passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 10s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 34s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 26s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 22s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 51s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 2m 5s {color} | {color:green} trunk passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 3m 4s {color} | {color:green} trunk passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 1m 14s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 32s {color} | {color:green} the patch passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 32s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 1m 8s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 1m 8s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 29s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvnsite {color} | {color:green} 1m 20s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 19s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 3m 8s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 57s {color} | {color:green} the patch passed with JDK v1.8.0_74 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 3m 12s {color} | {color:green} the patch passed with JDK v1.7.0_95 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 132m 17s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.8.0_74. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 90m 32s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.7.0_95. {color} |
| {color:green}+1{color} | {color:green} asflicense {color} | {color:green} 0m 32s {color} | {color:green} Patch does not generate ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 265m 58s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_74 Failed junit tests | hadoop.hdfs.server.datanode.TestDirectoryScanner |
|   | hadoop.hdfs.server.namenode.ha.TestHAAppend |
|   | hadoop.hdfs.server.namenode.TestEditLog |
|   | hadoop.hdfs.server.namenode.TestDecommissioningStatus |
|   | hadoop.hdfs.qjournal.TestSecureNNWithQJM |
|   | hadoop.hdfs.server.namenode.ha.TestEditLogTailer |
|   | hadoop.hdfs.server.datanode.TestDataNodeUUID |
|   | hadoop.hdfs.security.TestDelegationTokenForProxyUser |
|   | hadoop.hdfs.TestDFSUpgradeFromImage |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
| JDK v1.7.0_95 Failed junit tests | hadoop.hdfs.server.datanode.TestDirectoryScanner |
|   | hadoop.hdfs.server.namenode.TestNameNodeMetadataConsistency |
|   | hadoop.hdfs.server.namenode.ha.TestHAAppend |
|   | hadoop.hdfs.TestMissingBlocksAlert |
|   | hadoop.hdfs.server.namenode.TestNamenodeCapacityReport |
|   | hadoop.metrics2.sink.TestRollingFileSystemSinkWithSecureHdfs |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
\\
\\
|| Subsystem || Report/Notes ||
| Docker |  Image:yetus/hadoop:fbe3e86 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12781892/HDFS-9633-001.patch |
| JIRA Issue | HDFS-9633 |
| Optional Tests |  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  |
| uname | Linux 7d9e35a49a75 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /testptch/hadoop/patchprocess/precommit/personality/provided.sh |
| git revision | trunk / 19b645c |
| Default Java | 1.7.0_95 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_74 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_95 |
| findbugs | v3.0.0 |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/14923/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_74.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/14923/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_95.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/14923/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_74.txt https://builds.apache.org/job/PreCommit-HDFS-Build/14923/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_95.txt |
| JDK v1.7.0_95  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/14923/testReport/ |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/14923/console |
| Powered by | Apache Yetus 0.2.0   http://yetus.apache.org |


This message was automatically generated.

",,,,,,,,,,,,
Modify tests that initialize MiniDFSNNTopology with hard-coded ports to use ServerSocketUtil#getPorts,HDFS-9587,12923541,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,xiaochen,xiaochen,xiaochen,21/Dec/15 19:55,21/Dec/15 19:55,18/Feb/21 10:08,,,,,,,,,0,,,,,"In tests with HA we have to {{setHttpPort}} to non-ephemeral ports when creating {{MiniDFSNNTopology}}. But hard-coding it may result in failures if the hard-coded port is in use in the env.
We should use {{ServerSocketUtil#getPorts}} to reduce the probability of such failures. This jira is to propose to update all {{MiniDFSNNTopology#setHttpPort}} usages.  (Currently there's only {{ServerSocketUtil#getPort}}, but HDFS-9444 is adding the ability to get multiple free ports)",,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-9444,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-12-21 19:55:25.0,,,,,,,"0|i2q6g7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simulated slow disk in SimulatedFSDataset,HDFS-9341,12909063,Test,Patch Available,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,zhz,zhz,zhz,29/Oct/15 22:46,06/Jan/17 21:49,18/Feb/21 10:08,,2.7.1,,,test,,,,0,,,,,"Besides simulating the byte content, {{SimulatedFSDataset}} can also simulate the scenario where disk is slow when accessing certain bytes. The slowness can be random or controlled at certain bytes. It can also be made configurable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/15 23:34;zhz;HDFS-9341.00.patch;https://issues.apache.org/jira/secure/attachment/12769894/HDFS-9341.00.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,2015-10-31 05:30:21.303,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 06 21:49:25 UTC 2017,,,,,,,"0|i2npv3:",9223372036854775807,,,,,,,,,,,,,,,,,,"30/Oct/15 23:33;zhz;Maybe we can dedicate this JIRA to adding random slowness. Attaching patch with that scope.","31/Oct/15 05:30;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue} 0m 6s {color} | {color:blue} docker + precommit patch detected. {color} |
| {color:green}+1{color} | {color:green} @author {color} | {color:green} 0m 0s {color} | {color:green} The patch does not contain any @author tags. {color} |
| {color:green}+1{color} | {color:green} test4tests {color} | {color:green} 0m 0s {color} | {color:green} The patch appears to include 1 new or modified test files. {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 3m 10s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 30s {color} | {color:green} trunk passed with JDK v1.8.0_60 {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 31s {color} | {color:green} trunk passed with JDK v1.7.0_79 {color} |
| {color:green}+1{color} | {color:green} checkstyle {color} | {color:green} 0m 15s {color} | {color:green} trunk passed {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} trunk passed {color} |
| {color:red}-1{color} | {color:red} findbugs {color} | {color:red} 1m 52s {color} | {color:red} hadoop-hdfs-project/hadoop-hdfs in trunk cannot run convertXmlToText from findbugs {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 6s {color} | {color:green} trunk passed with JDK v1.8.0_60 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 48s {color} | {color:green} trunk passed with JDK v1.7.0_79 {color} |
| {color:green}+1{color} | {color:green} mvninstall {color} | {color:green} 0m 39s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 31s {color} | {color:green} the patch passed with JDK v1.8.0_60 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 31s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} compile {color} | {color:green} 0m 31s {color} | {color:green} the patch passed with JDK v1.7.0_79 {color} |
| {color:green}+1{color} | {color:green} javac {color} | {color:green} 0m 31s {color} | {color:green} the patch passed {color} |
| {color:red}-1{color} | {color:red} checkstyle {color} | {color:red} 0m 15s {color} | {color:red} Patch generated 2 new checkstyle issues in hadoop-hdfs-project/hadoop-hdfs (total was 386, now 388). {color} |
| {color:green}+1{color} | {color:green} mvneclipse {color} | {color:green} 0m 14s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} whitespace {color} | {color:green} 0m 0s {color} | {color:green} Patch has no whitespace issues. {color} |
| {color:green}+1{color} | {color:green} findbugs {color} | {color:green} 2m 2s {color} | {color:green} the patch passed {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 8s {color} | {color:green} the patch passed with JDK v1.8.0_60 {color} |
| {color:green}+1{color} | {color:green} javadoc {color} | {color:green} 1m 47s {color} | {color:green} the patch passed with JDK v1.7.0_79 {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 72m 5s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.8.0_60. {color} |
| {color:red}-1{color} | {color:red} unit {color} | {color:red} 89m 42s {color} | {color:red} hadoop-hdfs in the patch failed with JDK v1.7.0_79. {color} |
| {color:red}-1{color} | {color:red} asflicense {color} | {color:red} 0m 23s {color} | {color:red} Patch generated 58 ASF License warnings. {color} |
| {color:black}{color} | {color:black} {color} | {color:black} 181m 47s {color} | {color:black} {color} |
\\
\\
|| Reason || Tests ||
| JDK v1.8.0_60 Failed junit tests | hadoop.hdfs.server.blockmanagement.TestBlockManager |
|   | hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes |
|   | hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation |
|   | hadoop.hdfs.TestReplication |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure090 |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
|   | hadoop.hdfs.TestReadStripedFileWithMissingBlocks |
|   | hadoop.hdfs.server.namenode.ha.TestPipelinesFailover |
|   | hadoop.hdfs.server.datanode.TestNNHandlesCombinedBlockReport |
|   | hadoop.hdfs.server.namenode.TestAddStripedBlocks |
|   | hadoop.hdfs.server.namenode.TestDecommissioningStatus |
|   | hadoop.hdfs.TestReadStripedFileWithDecoding |
|   | hadoop.hdfs.server.namenode.TestRecoverStripedBlocks |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure170 |
|   | hadoop.hdfs.TestFileCreation |
|   | hadoop.hdfs.server.datanode.TestBlockHasMultipleReplicasOnSameDN |
|   | hadoop.hdfs.server.datanode.TestDataNodeMetrics |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure200 |
|   | hadoop.hdfs.server.namenode.TestAddBlockRetry |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure110 |
|   | hadoop.hdfs.TestRecoverStripedFile |
|   | hadoop.hdfs.server.namenode.ha.TestHAAppend |
|   | hadoop.hdfs.TestQuota |
|   | hadoop.hdfs.server.namenode.TestDiskspaceQuotaUpdate |
|   | hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes |
|   | hadoop.hdfs.server.blockmanagement.TestPendingReplication |
|   | hadoop.hdfs.TestDFSStripedOutputStream |
|   | hadoop.hdfs.server.blockmanagement.TestReplicationPolicyWithNodeGroup |
|   | hadoop.hdfs.server.namenode.TestStripedINodeFile |
|   | hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks |
|   | hadoop.hdfs.TestReplaceDatanodeOnFailure |
|   | hadoop.hdfs.server.balancer.TestBalancer |
|   | hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure210 |
|   | hadoop.hdfs.TestCrcCorruption |
|   | hadoop.hdfs.server.namenode.TestFSImageWithSnapshot |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure |
|   | hadoop.hdfs.TestWriteReadStripedFile |
|   | hadoop.hdfs.server.namenode.TestBlockPlacementPolicyRackFaultTolerant |
|   | hadoop.hdfs.TestDecommission |
|   | hadoop.hdfs.server.namenode.TestFileTruncate |
|   | hadoop.hdfs.protocol.datatransfer.sasl.TestSaslDataTransfer |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure070 |
|   | hadoop.hdfs.shortcircuit.TestShortCircuitCache |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
|   | hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewerWithStripedBlocks |
|   | hadoop.hdfs.TestBlockStoragePolicy |
|   | hadoop.hdfs.TestInjectionForSimulatedStorage |
|   | hadoop.hdfs.TestFileAppendRestart |
|   | hadoop.hdfs.server.namenode.TestMetaSave |
|   | hadoop.hdfs.TestEncryptedTransfer |
|   | hadoop.hdfs.server.namenode.TestFsck |
|   | hadoop.hdfs.server.blockmanagement.TestPendingInvalidateBlock |
|   | hadoop.hdfs.server.blockmanagement.TestOverReplicatedBlocks |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure050 |
|   | hadoop.hdfs.TestGetFileChecksum |
|   | hadoop.hdfs.TestSafeModeWithStripedFile |
|   | hadoop.hdfs.server.datanode.TestDeleteBlockPool |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure000 |
|   | hadoop.hdfs.server.namenode.ha.TestSeveralNameNodes |
|   | hadoop.hdfs.server.mover.TestMover |
|   | hadoop.hdfs.server.blockmanagement.TestReplicationPolicy |
| JDK v1.8.0_60 Timed out junit tests | org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFSStriped |
| JDK v1.7.0_79 Failed junit tests | hadoop.hdfs.server.blockmanagement.TestBlockManager |
|   | hadoop.hdfs.server.datanode.TestReadOnlySharedStorage |
|   | hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes |
|   | hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure090 |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
|   | hadoop.hdfs.server.namenode.ha.TestStandbyIsHot |
|   | hadoop.hdfs.TestReadStripedFileWithMissingBlocks |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure020 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure130 |
|   | hadoop.hdfs.server.blockmanagement.TestNodeCount |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestDatanodeRestart |
|   | hadoop.hdfs.server.namenode.ha.TestPipelinesFailover |
|   | hadoop.hdfs.server.datanode.TestNNHandlesCombinedBlockReport |
|   | hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFSStriped |
|   | hadoop.hdfs.server.namenode.TestAddStripedBlocks |
|   | hadoop.hdfs.server.namenode.TestDecommissioningStatus |
|   | hadoop.hdfs.TestReadStripedFileWithDecoding |
|   | hadoop.hdfs.server.namenode.TestRecoverStripedBlocks |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure170 |
|   | hadoop.hdfs.TestFileCreation |
|   | hadoop.hdfs.server.datanode.TestBlockHasMultipleReplicasOnSameDN |
|   | hadoop.hdfs.server.namenode.TestAddBlockRetry |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure110 |
|   | hadoop.hdfs.TestRecoverStripedFile |
|   | hadoop.hdfs.server.namenode.ha.TestHAAppend |
|   | hadoop.hdfs.TestQuota |
|   | hadoop.hdfs.server.namenode.TestDiskspaceQuotaUpdate |
|   | hadoop.hdfs.web.TestWebHdfsWithMultipleNameNodes |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure150 |
|   | hadoop.hdfs.server.blockmanagement.TestPendingReplication |
|   | hadoop.hdfs.TestFileAppend2 |
|   | hadoop.hdfs.server.namenode.ha.TestDNFencing |
|   | hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics |
|   | hadoop.hdfs.TestDFSStripedOutputStream |
|   | hadoop.hdfs.server.namenode.TestAddBlock |
|   | hadoop.hdfs.server.blockmanagement.TestReplicationPolicyWithNodeGroup |
|   | hadoop.hdfs.server.blockmanagement.TestReplicationPolicyWithUpgradeDomain |
|   | hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks |
|   | hadoop.hdfs.TestPread |
|   | hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage |
|   | hadoop.hdfs.TestReplaceDatanodeOnFailure |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure010 |
|   | hadoop.hdfs.server.balancer.TestBalancer |
|   | hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS |
|   | hadoop.hdfs.TestCrcCorruption |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure100 |
|   | hadoop.hdfs.server.namenode.TestFSImageWithSnapshot |
|   | hadoop.hdfs.server.namenode.snapshot.TestINodeFileUnderConstructionWithSnapshot |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure080 |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure |
|   | hadoop.hdfs.TestWriteReadStripedFile |
|   | hadoop.hdfs.server.namenode.TestBlockPlacementPolicyRackFaultTolerant |
|   | hadoop.hdfs.TestDecommission |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestSpaceReservation |
|   | hadoop.hdfs.server.namenode.TestFileTruncate |
|   | hadoop.hdfs.protocol.datatransfer.sasl.TestSaslDataTransfer |
|   | hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure190 |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
|   | hadoop.hdfs.TestBlockStoragePolicy |
|   | hadoop.hdfs.TestEncryptedTransfer |
|   | hadoop.hdfs.server.namenode.TestFsck |
|   | hadoop.hdfs.server.blockmanagement.TestPendingInvalidateBlock |
|   | hadoop.hdfs.server.blockmanagement.TestOverReplicatedBlocks |
|   | hadoop.hdfs.TestGetFileChecksum |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure140 |
|   | hadoop.hdfs.TestSafeModeWithStripedFile |
|   | hadoop.hdfs.server.mover.TestMover |
|   | hadoop.hdfs.server.datanode.TestHSync |
|   | hadoop.hdfs.server.blockmanagement.TestReplicationPolicy |
| JDK v1.7.0_79 Timed out junit tests | org.apache.hadoop.hdfs.TestReplication |
\\
\\
|| Subsystem || Report/Notes ||
| Docker | Client=1.7.1 Server=1.7.1 Image:test-patch-base-hadoop-date2015-10-31 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12769894/HDFS-9341.00.patch |
| JIRA Issue | HDFS-9341 |
| Optional Tests |  asflicense  javac  javadoc  mvninstall  unit  findbugs  checkstyle  compile  |
| uname | Linux 3ddd48b386e9 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Build tool | maven |
| Personality | /home/jenkins/jenkins-slave/workspace/PreCommit-HDFS-Build/patchprocess/apache-yetus-e77b1ce/precommit/personality/hadoop.sh |
| git revision | trunk / a4a6b5b |
| Default Java | 1.7.0_79 |
| Multi-JDK versions |  /usr/lib/jvm/java-8-oracle:1.8.0_60 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_79 |
| findbugs | v3.0.0 |
| findbugs | https://builds.apache.org/job/PreCommit-HDFS-Build/13311/artifact/patchprocess/branch-findbugs-hadoop-hdfs-project_hadoop-hdfs.html |
| checkstyle | https://builds.apache.org/job/PreCommit-HDFS-Build/13311/artifact/patchprocess/diff-checkstyle-hadoop-hdfs-project_hadoop-hdfs.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/13311/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_60.txt |
| unit | https://builds.apache.org/job/PreCommit-HDFS-Build/13311/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_79.txt |
| unit test logs |  https://builds.apache.org/job/PreCommit-HDFS-Build/13311/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.8.0_60.txt https://builds.apache.org/job/PreCommit-HDFS-Build/13311/artifact/patchprocess/patch-unit-hadoop-hdfs-project_hadoop-hdfs-jdk1.7.0_79.txt |
| JDK v1.7.0_79  Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/13311/testReport/ |
| asflicense | https://builds.apache.org/job/PreCommit-HDFS-Build/13311/artifact/patchprocess/patch-asflicense-problems.txt |
| modules | C: hadoop-hdfs-project/hadoop-hdfs U: hadoop-hdfs-project/hadoop-hdfs |
| Max memory used | 227MB |
| Powered by | Apache Yetus   http://yetus.apache.org |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/13311/console |


This message was automatically generated.

","01/Dec/15 23:35;eddyxu;Hi, [~zhz]. Thanks for working on this. This is an interesting patch and it looks good overall. A few small comments

* {code}
byte readSimulatedByte(Block b, long offsetInBlk, boolean slowness) ;
{code}

Should it be private? also {{slowness}} is already a member of {{SimulatedInputStream}}, perhaps we do not need to pass it here.

* {code}
 SimulatedInputStream(long l, Block b, boolean slow) {
{code}
Could you update the comments of {{SimulatedInputStream(...)}} to reflect the parameters. 

* {{private boolean randomDiskSlowness;}} should be final.

* Should {{SLOWNESS_PROBABILITY}} and {{SLOWNESS_DELAY_MS }} be configurable? For example, control ""how slow"" it is? We can do it in a following JIRA. You mentioned that

bq. The slowness can be random or controlled at certain bytes.

Are you going to implement these features in this patch?

Last, a general question: using {{random}} to build slow I/Os is non-deterministic. Would it cause problems in unit tests?

Thanks!","06/Jan/17 21:49;hadoopqa;| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:blue}0{color} | {color:blue} reexec {color} | {color:blue}  0m  0s{color} | {color:blue} Docker mode activated. {color} |
| {color:red}-1{color} | {color:red} patch {color} | {color:red}  0m  3s{color} | {color:red} HDFS-9341 does not apply to trunk. Rebase required? Wrong Branch? See https://wiki.apache.org/hadoop/HowToContribute for help. {color} |
\\
\\
|| Subsystem || Report/Notes ||
| JIRA Issue | HDFS-9341 |
| JIRA Patch URL | https://issues.apache.org/jira/secure/attachment/12769894/HDFS-9341.00.patch |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/18056/console |
| Powered by | Apache Yetus 0.5.0-SNAPSHOT   http://yetus.apache.org |


This message was automatically generated.

",,,,,,,,,,,,,
Erasure Coding: System Test of Namenode with EC files,HDFS-8267,12824181,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,rakeshr,demongaorui,demongaorui,27/Apr/15 07:25,03/Jul/15 09:21,18/Feb/21 10:08,,HDFS-7285,,,,,,,0,EC,test,,,"1. Namenode startup with EC:
   1.1. Safemode
   1.2. BlockReport
2. Namenode HA with EC: 
   2.1. Fsimage and editlog test
   2.2. Hot restart and recovery of Active NameNode after failure
   2.3. Hot restart and recovery of Standby NameNode after failure
   2.4. Restart and recovery of both Active and Standby NameNode fail in the same time
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-04-27 10:58:36.798,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 03:22:30 UTC 2015,,,,,,,"0|i2dsg7:",9223372036854775807,,,,,,,,,,,,,,,,,,"27/Apr/15 10:58;rakeshr;Thanks [~demongaorui] for creating this task. I've written few HA related EC zone test cases, but couldn't able to complete as HDFS-7859 is open. I'm happy to take up this task, shall I?

I could see there are couple of unit testing raised recently. IMHO its good to group all these EC related testing tasks together. One way is to create separate umbrella task and move everything under this. What others opinion?","12/May/15 02:31;demongaorui;[~rakeshr]Thank you very much for picking this up. Sorry for my late reply, I just come back to work from a very long holiday.  Thanks for your advice about organisation of testing jiras. Umbrella jira is created as {{HDFS-8267}}, almost all the EC system testing related jiras were moved under it. Please feel free to let me know if you have any others questions. ","12/May/15 02:33;demongaorui;Sorry, I made a mistake about Jira number. The Umbrella jira is {{HDFS-8197}}.","12/May/15 03:22;rakeshr;OK, Thats Great!

IMO NN restart case is depending on the schema persistent logic, probably I will take up this once HDFS-7859 is in.",,,,,,,,,,,,,
Erasure Coding: System Test of snapshot with EC files,HDFS-8266,12824179,Test,Patch Available,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,rakeshr,demongaorui,demongaorui,27/Apr/15 07:19,03/Jul/15 09:21,18/Feb/21 10:08,,HDFS-7285,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/15 14:53;rakeshr;HDFS-8266-HDFS-7285-00.patch;https://issues.apache.org/jira/secure/attachment/12732875/HDFS-8266-HDFS-7285-00.patch","01/Jun/15 14:22;rakeshr;HDFS-8266-HDFS-7285-01.patch;https://issues.apache.org/jira/secure/attachment/12736561/HDFS-8266-HDFS-7285-01.patch","01/Jun/15 10:05;rakeshr;HDFS-8266-HDFS-7285-01.patch;https://issues.apache.org/jira/secure/attachment/12736536/HDFS-8266-HDFS-7285-01.patch",,,3.0,,,,,,,,,,,,,,,,,,,,2015-05-14 15:16:10.05,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 09:06:32 UTC 2015,,,,,,,"0|i2dsfr:",9223372036854775807,,,,,,,,,,,EC test,,HDFS-7285,,,,,"14/May/15 15:16;rakeshr;Hi [~demongaorui], I've tried few snapshot related test cases and attached the patch in this jira. Please have a look at it. I could see there is already a jira raised HDFS-8373 to handle {{trash}} case. IMHO this jira can focus on snapshot specific unit testing and we could add {{trash}} testing as part of HDFS-8373. Do you agree with me?
","14/May/15 15:17;rakeshr;Hi All,

I've one observation while writing snapshot related unit test case. {{ErasureCodingZoneManager#getECZoneInfo()}} function is not properly resolving the {{inode.getFullPathName()}}, its returning the path including the snapshot name {{/zone/.snapshot/snap1}}. I feel we could improve this by returning only path {{/zone}}. Shall I go ahead by changing this logic like,

{code}
 return new ErasureCodingZoneInfo(dir.getInode(inode.getId()).getFullPathName(), schema);
{code}","18/May/15 07:18;vinayakumarb;I see in your tests you are trying to get the zone info for the snapshot path, NOT the original path. {{snap1Zone}} below will result as {{/zone/.snapshot/snap1}}.

{code} final Path snap1Zone = new Path(snap1, zone.getName()); 
    assertEquals(""Got unexpected erasure zone path"", zone.toString(), fs
        .getErasureCodingZoneInfo(snap1Zone).getDir().toString());{code}

It would be wrong to return original path zone information for the snapshot path.

Agree?","18/May/15 07:43;rakeshr;Thanks [~vinayrpet] for the reply. Could you please take a look at [Encryption_zone logic|https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java#L1151]. It looks like there the structure of file path details doesn't have {{/.snapshot/snap1}}. It would be good to make both EncryZone and ECZone logic in sync, does this makes sense to you?","18/May/15 09:42;vinayakumarb;Oh, 
I observe that, the resultant zone dir will come with '.snapshot' only when the zone dir itself is the snapshottable dir. If the parent of the zone is snapshottable, then zonedir will come without '.snapshot'.

I think it makes sense to include this change.
[~rakeshr], please go ahead and post a patch as well.","19/May/15 05:35;rakeshr;I've raised HDFS-8420 and put a patch over there to handle this case separately.","01/Jun/15 13:17;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |   5m 46s | Findbugs (version ) appears to be broken on HDFS-7285. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 41s | There were no new javac warning messages. |
| {color:red}-1{color} | release audit |   0m 13s | The applied patch generated 1 release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 38s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 35s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 33s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   3m 43s | The patch appears to introduce 1 new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | native |   1m 35s | Pre-build of native portion |
| {color:red}-1{color} | hdfs tests | 165m 27s | Tests failed in hadoop-hdfs. |
| | | 187m 15s | |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-hdfs |
| Failed unit tests | hadoop.hdfs.TestRead |
|   | hadoop.hdfs.TestClientReportBadBlock |
|   | hadoop.hdfs.TestAppendSnapshotTruncate |
|   | hadoop.hdfs.TestFileAppendRestart |
|   | hadoop.cli.TestErasureCodingCLI |
|   | hadoop.hdfs.protocol.TestBlockListAsLongs |
|   | hadoop.hdfs.TestErasureCodingZones2 |
|   | hadoop.TestRefreshCallQueue |
|   | hadoop.hdfs.TestListFilesInDFS |
|   | hadoop.hdfs.TestEncryptedTransfer |
|   | hadoop.hdfs.TestMiniDFSCluster |
|   | hadoop.security.TestPermissionSymlinks |
|   | hadoop.hdfs.TestDFSRollback |
|   | hadoop.hdfs.TestFileAppend2 |
|   | hadoop.security.TestRefreshUserMappings |
|   | hadoop.hdfs.TestRecoverStripedFile |
|   | hadoop.hdfs.server.namenode.TestAuditLogs |
|   | hadoop.hdfs.crypto.TestHdfsCryptoStreams |
|   | hadoop.hdfs.TestBlockStoragePolicy |
|   | hadoop.hdfs.TestCrcCorruption |
|   | hadoop.hdfs.server.blockmanagement.TestBlockInfo |
|   | hadoop.hdfs.TestDFSUpgrade |
|   | hadoop.hdfs.TestLeaseRecovery2 |
|   | hadoop.cli.TestXAttrCLI |
|   | hadoop.hdfs.TestDFSShell |
|   | hadoop.hdfs.protocol.datatransfer.sasl.TestSaslDataTransfer |
|   | hadoop.hdfs.protocol.TestAnnotations |
|   | hadoop.hdfs.server.namenode.TestFileTruncate |
|   | hadoop.security.TestPermission |
|   | hadoop.hdfs.TestFileCreationDelete |
|   | hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS |
|   | hadoop.hdfs.TestAppendDifferentChecksum |
|   | hadoop.hdfs.TestRemoteBlockReader |
|   | hadoop.hdfs.TestRestartDFS |
|   | hadoop.cli.TestHDFSCLI |
|   | hadoop.hdfs.TestHDFSFileSystemContract |
|   | hadoop.cli.TestAclCLI |
|   | hadoop.hdfs.TestDFSPermission |
|   | hadoop.hdfs.TestHDFSServerPorts |
|   | hadoop.cli.TestCryptoAdminCLI |
|   | hadoop.hdfs.TestDFSStartupVersions |
|   | hadoop.hdfs.TestWriteBlockGetsBlockLengthHint |
|   | hadoop.hdfs.TestAbandonBlock |
|   | hadoop.hdfs.TestFetchImage |
|   | hadoop.cli.TestCacheAdminCLI |
|   | hadoop.hdfs.TestDatanodeDeath |
|   | hadoop.hdfs.TestDFSClientExcludedNodes |
|   | hadoop.hdfs.TestDFSOutputStream |
|   | hadoop.hdfs.TestDFSStripedInputStream |
|   | hadoop.hdfs.TestReservedRawPaths |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12736536/HDFS-8266-HDFS-7285-01.patch |
| Optional Tests | javac unit findbugs checkstyle |
| git revision | HDFS-7285 / 1299357 |
| Release Audit | https://builds.apache.org/job/PreCommit-HDFS-Build/11182/artifact/patchprocess/patchReleaseAuditProblems.txt |
| Findbugs warnings | https://builds.apache.org/job/PreCommit-HDFS-Build/11182/artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html |
| hadoop-hdfs test log | https://builds.apache.org/job/PreCommit-HDFS-Build/11182/artifact/patchprocess/testrun_hadoop-hdfs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/11182/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/11182/console |


This message was automatically generated.","01/Jun/15 14:24;rakeshr;Unfortunately there are many test failures but those are not related to the patch. I'm re-attaching the previous patch again to get a clean Hadoop QA report.","01/Jun/15 17:45;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |   5m 45s | Findbugs (version ) appears to be broken on HDFS-7285. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 1 new or modified test files. |
| {color:green}+1{color} | javac |   7m 43s | There were no new javac warning messages. |
| {color:red}-1{color} | release audit |   0m 13s | The applied patch generated 1 release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 37s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  0s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 36s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 32s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   3m 26s | The patch appears to introduce 1 new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | native |   1m 22s | Pre-build of native portion |
| {color:red}-1{color} | hdfs tests | 173m 26s | Tests failed in hadoop-hdfs. |
| | | 194m 45s | |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-hdfs |
| Failed unit tests | hadoop.hdfs.server.namenode.TestFileTruncate |
|   | hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS |
|   | hadoop.hdfs.TestEncryptedTransfer |
|   | hadoop.hdfs.server.namenode.TestAuditLogs |
|   | hadoop.hdfs.TestDFSStripedInputStream |
|   | hadoop.hdfs.TestRecoverStripedFile |
|   | hadoop.hdfs.server.blockmanagement.TestBlockInfo |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12736561/HDFS-8266-HDFS-7285-01.patch |
| Optional Tests | javac unit findbugs checkstyle |
| git revision | HDFS-7285 / 1299357 |
| Release Audit | https://builds.apache.org/job/PreCommit-HDFS-Build/11187/artifact/patchprocess/patchReleaseAuditProblems.txt |
| Findbugs warnings | https://builds.apache.org/job/PreCommit-HDFS-Build/11187/artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html |
| hadoop-hdfs test log | https://builds.apache.org/job/PreCommit-HDFS-Build/11187/artifact/patchprocess/testrun_hadoop-hdfs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/11187/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf904.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/11187/console |


This message was automatically generated.","01/Jun/15 18:20;rakeshr;The test case failure is unrelated to my patch. All the three unit test cases added in the patch are passing [jenkins_test_report|https://builds.apache.org/job/PreCommit-HDFS-Build/11187/testReport/org.apache.hadoop.hdfs/TestErasureCodingZones2/].

Note: I've added new test class {{TestErasureCodingZones2}} because the existing {{TestErasureCodingZones}} is not starting GROUP_SIZE number of Datanodes.

","02/Jun/15 09:06;demongaorui;Hi [~rakeshr], I have some advice for the patch [^HDFS-8266-HDFS-7285-01.patch]:

(1).code style consistency.
{code:java}
    // create erasure zone
    fs.createErasureCodingZone(zone, null, 0);
    DFSTestUtil.createFile(fs, zoneFile, len, (short) 1, 0xFEED);
    String contents = DFSTestUtil.readFile(fs, zoneFile);
    final Path snap1 = fs.createSnapshot(zoneParent, ""snap1"");
    final Path snap1Zone = new Path(snap1, zone.getName());
    assertEquals(""Got unexpected erasure zone path"", zone.toString(), fs
        .getErasureCodingZone(snap1Zone).getDir().toString());
{code} 
and 
{code:java}
 // Create the erasure zone again
    fs.createErasureCodingZone(zone, null, 0);
    final Path snap3 = fs.createSnapshot(zoneParent, ""snap3"");
    final Path snap3Zone = new Path(snap3, zone.getName());
    // Check that snap3's EZ has the correct settings
    ErasureCodingZone ezSnap3 = fs.getErasureCodingZone(snap3Zone);
    assertEquals(""Got unexpected erasure zone path"", zone.toString(), ezSnap3
        .getDir().toString());
{code}
shared the same actions but were written in different style. Maybe we can change all the similar codes written in the former style to the latter.  like:
{code:java}
    // create erasure zone
    fs.createErasureCodingZone(zone, null, 0);
    DFSTestUtil.createFile(fs, zoneFile, len, (short) 1, 0xFEED);
    String contents = DFSTestUtil.readFile(fs, zoneFile);
    final Path snap1 = fs.createSnapshot(zoneParent, ""snap1"");
    final Path snap1Zone = new Path(snap1, zone.getName());
    ErasureCodingZone ezSnap1 = fs.getErasureCodingZone(snap1Zone);
    assertEquals(""Got unexpected erasure zone path"", zone.toString(), ezSnap1
        .getDir().toString());
{code} 
It might make the future reading and revising of the code easier. 

(2).
In code like bellow: 
{code:java}
 fs.getErasureCodingZone(snap1Zone).getDir().toString()
{code} 
the return type of {{fs.getErasureCodingZone(snap1Zone).getDir()}} is already {{String}}, so I think we don't need to call {{.toString()}}. 

(3).
{code:java}
    // Check that older snapshots still have the old EZ settings
    ErasureCodingZone ezSnap1 = fs.getErasureCodingZone(snap1Zone);
    assertEquals(""Got unexpected erasure zone path"", zone.toString(), ezSnap1
        .getDir().toString());
{code}
I think we could not confirm the older snapshots still have the old EZ settings here, cause {{snap1Zone}} has the exactly same value as {{snap3Zone}}. Maybe, we can make different settings of old EZ and new EZ, so that we can confirm the older snapshot keep the old EZ settings.

(4).
In method testSnapshotsOnECZoneDir(), ECZone was created based on {{zone(value: ""/zone"")}}. When we call {{fs.getErasureCodingZone()}} respectively with parameter {{snap1Zone(value:""/zone/.snapshot/snap1/zone"")}}and{{snap1(value:""/zone/.snapshot/snap1"")}}, I think {{fs.getErasureCodingZone(snap1)}} actually returns the ECZone set to {{zone(value:""/zone""), and {{fs.getErasureCodingZone(snap1Zone)}} returns the ECZone set in the process of snapshot, right?  If it's right, then ECZone can be nested, so does {{fs.getErasureCodingZone()}} return the nearest parent dir which is ECZone of the Path parameter (or the Path parameter itself if the Path parameter is ECZone)? 


Actually, this jira is contained by the umbrella jira [HDFS-8197], we created this jira for EC system test, not unit test. But, I think that's great to finish both the unit test and the system test related to snapshot with EC files. When it comes to system test, we have the image of testing snapshot functionality by using it like an end user(of cause can be imitated by testing scripts).
",,,,,,
Erasure Coding: System Test of reading EC file,HDFS-8259,12824157,Test,Patch Available,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,xinwei,demongaorui,demongaorui,27/Apr/15 05:28,24/Jul/15 01:35,18/Feb/21 10:08,,HDFS-7285,,,,,,,0,,,,,"1. Normally reading EC file(reading without datanote failure and no need of recovery)
2. Reading EC file with datanode failure.
3. Reading EC file with data block recovery by decoding from parity blocks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/15 06:46;xinwei;HDFS-8259-HDFS-7285.001.patch;https://issues.apache.org/jira/secure/attachment/12744151/HDFS-8259-HDFS-7285.001.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,2015-07-08 06:49:32.053,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 24 01:35:02 UTC 2015,,,,,,,"0|i2dsb3:",9223372036854775807,,,,,,,,,,,EC,,,,,,,"08/Jul/15 06:49;xinwei;Attached the patch to review. About reading a file with some datanodes failure or some blocks corrupted or some blocks deleted.","08/Jul/15 08:05;xinwei;In this patch, the test methods {{testReadCorruptedData*()}} will fail to read a EC file with corrupted blocks, I have created HDFS-8732 to address it.","08/Jul/15 11:20;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | pre-patch |   5m 47s | Findbugs (version ) appears to be broken on HDFS-7285. |
| {color:green}+1{color} | @author |   0m  0s | The patch does not contain any @author tags. |
| {color:green}+1{color} | tests included |   0m  0s | The patch appears to include 4 new or modified test files. |
| {color:green}+1{color} | javac |   7m 42s | There were no new javac warning messages. |
| {color:red}-1{color} | release audit |   0m 13s | The applied patch generated 1 release audit warnings. |
| {color:green}+1{color} | checkstyle |   0m 38s | There were no new checkstyle issues. |
| {color:green}+1{color} | whitespace |   0m  1s | The patch has no lines that end in whitespace. |
| {color:green}+1{color} | install |   1m 38s | mvn install still works. |
| {color:green}+1{color} | eclipse:eclipse |   0m 31s | The patch built with eclipse:eclipse. |
| {color:red}-1{color} | findbugs |   3m 31s | The patch appears to introduce 4 new Findbugs (version 3.0.0) warnings. |
| {color:green}+1{color} | native |   1m 20s | Pre-build of native portion |
| {color:red}-1{color} | hdfs tests |  68m 33s | Tests failed in hadoop-hdfs. |
| | |  89m 58s | |
\\
\\
|| Reason || Tests ||
| FindBugs | module:hadoop-hdfs |
| Failed unit tests | hadoop.hdfs.TestFileAppend4 |
|   | hadoop.hdfs.TestRead |
|   | hadoop.hdfs.server.namenode.TestNameNodeRetryCacheMetrics |
|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshot |
|   | hadoop.hdfs.server.namenode.TestSecondaryWebUi |
|   | hadoop.hdfs.server.namenode.TestNNStorageRetentionFunctional |
|   | hadoop.hdfs.server.namenode.TestFavoredNodesEndToEnd |
|   | hadoop.hdfs.server.namenode.ha.TestHAStateTransitions |
|   | hadoop.hdfs.server.datanode.TestRefreshNamenodes |
|   | hadoop.hdfs.TestHdfsAdmin |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureReporting |
|   | hadoop.hdfs.server.datanode.TestBlockHasMultipleReplicasOnSameDN |
|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshotFileLength |
|   | hadoop.hdfs.server.namenode.ha.TestLossyRetryInvocationHandler |
|   | hadoop.hdfs.TestClientReportBadBlock |
|   | hadoop.hdfs.server.namenode.TestNamenodeCapacityReport |
|   | hadoop.hdfs.server.namenode.TestNamenodeRetryCache |
|   | hadoop.hdfs.server.namenode.TestFSEditLogLoader |
|   | hadoop.hdfs.server.namenode.TestAuditLogger |
|   | hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlocks |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistPolicy |
|   | hadoop.hdfs.server.blockmanagement.TestDatanodeManager |
|   | hadoop.hdfs.server.datanode.TestDataNodeMetrics |
|   | hadoop.hdfs.TestAppendSnapshotTruncate |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistFiles |
|   | hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaPlacement |
|   | hadoop.hdfs.server.namenode.TestNameNodeRpcServer |
|   | hadoop.hdfs.TestSafeModeWithStripedFile |
|   | hadoop.hdfs.TestFileAppendRestart |
|   | hadoop.hdfs.server.namenode.TestSecondaryNameNodeUpgrade |
|   | hadoop.cli.TestErasureCodingCLI |
|   | hadoop.hdfs.server.namenode.snapshot.TestSetQuotaWithSnapshot |
|   | hadoop.hdfs.server.namenode.ha.TestHAFsck |
|   | hadoop.hdfs.server.namenode.snapshot.TestNestedSnapshots |
|   | hadoop.hdfs.protocol.TestBlockListAsLongs |
|   | hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot |
|   | hadoop.hdfs.server.namenode.TestHDFSConcat |
|   | hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics |
|   | hadoop.TestRefreshCallQueue |
|   | hadoop.hdfs.TestListFilesInDFS |
|   | hadoop.hdfs.server.namenode.TestAddBlock |
|   | hadoop.hdfs.server.datanode.TestDnRespectsBlockReportSplitThreshold |
|   | hadoop.hdfs.TestEncryptedTransfer |
|   | hadoop.hdfs.server.namenode.TestNameEditsConfigs |
|   | hadoop.hdfs.TestMiniDFSCluster |
|   | hadoop.hdfs.server.mover.TestMover |
|   | hadoop.security.TestPermissionSymlinks |
|   | hadoop.hdfs.TestDFSRollback |
|   | hadoop.hdfs.server.namenode.snapshot.TestUpdatePipelineWithSnapshots |
|   | hadoop.hdfs.server.namenode.ha.TestHASafeMode |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestWriteToReplica |
|   | hadoop.hdfs.TestFileConcurrentReader |
|   | hadoop.hdfs.TestFileAppend2 |
|   | hadoop.hdfs.server.datanode.TestDataNodeExit |
|   | hadoop.hdfs.server.namenode.ha.TestXAttrsWithHA |
|   | hadoop.hdfs.TestGetFileChecksum |
|   | hadoop.hdfs.server.namenode.TestSnapshotPathINodes |
|   | hadoop.hdfs.TestReadStripedFileWithDecoding |
|   | hadoop.security.TestRefreshUserMappings |
|   | hadoop.hdfs.server.namenode.TestNameNodeRespectsBindHostKeys |
|   | hadoop.hdfs.server.namenode.TestMetadataVersionOutput |
|   | hadoop.hdfs.server.namenode.ha.TestHAMetrics |
|   | hadoop.hdfs.TestRecoverStripedFile |
|   | hadoop.hdfs.server.namenode.TestFsckWithMultipleNameNodes |
|   | hadoop.hdfs.server.namenode.TestAllowFormat |
|   | hadoop.hdfs.server.namenode.ha.TestDNFencing |
|   | hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA |
|   | hadoop.hdfs.server.namenode.TestFSImageWithSnapshot |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl |
|   | hadoop.hdfs.server.namenode.TestDeadDatanode |
|   | hadoop.hdfs.server.namenode.TestQuotaWithStripedBlocks |
|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport |
|   | hadoop.hdfs.server.namenode.TestAuditLogs |
|   | hadoop.hdfs.crypto.TestHdfsCryptoStreams |
|   | hadoop.hdfs.server.namenode.snapshot.TestDisallowModifyROSnapshot |
|   | hadoop.hdfs.server.blockmanagement.TestAvailableSpaceBlockPlacementPolicy |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestDatanodeRestart |
|   | hadoop.hdfs.server.namenode.TestFSDirectory |
|   | hadoop.hdfs.server.datanode.TestTriggerBlockReport |
|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion |
|   | hadoop.hdfs.server.blockmanagement.TestOverReplicatedBlocks |
|   | hadoop.hdfs.server.datanode.TestIncrementalBlockReports |
|   | hadoop.hdfs.TestBlockStoragePolicy |
|   | hadoop.hdfs.server.namenode.snapshot.TestCheckpointsWithSnapshots |
|   | hadoop.hdfs.TestDFSStripedOutputStreamWithFailure |
|   | hadoop.hdfs.server.balancer.TestBalancerWithMultipleNameNodes |
|   | hadoop.hdfs.server.datanode.TestReadOnlySharedStorage |
|   | hadoop.hdfs.TestCrcCorruption |
|   | hadoop.hdfs.server.namenode.TestLargeDirectoryDelete |
|   | hadoop.hdfs.TestDFSUpgrade |
|   | hadoop.hdfs.server.namenode.ha.TestPipelinesFailover |
|   | hadoop.hdfs.server.namenode.TestSecurityTokenEditLog |
|   | hadoop.hdfs.server.namenode.TestEditLogJournalFailures |
|   | hadoop.hdfs.server.namenode.TestDeleteRace |
|   | hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication |
|   | hadoop.hdfs.server.blockmanagement.TestNameNodePrunesMissingStorages |
|   | hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation |
|   | hadoop.hdfs.server.datanode.TestIncrementalBrVariations |
|   | hadoop.hdfs.TestLeaseRecovery2 |
|   | hadoop.hdfs.server.namenode.ha.TestHAConfiguration |
|   | hadoop.hdfs.server.namenode.TestAclConfigFlag |
|   | hadoop.cli.TestXAttrCLI |
|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshotRename |
|   | hadoop.hdfs.server.blockmanagement.TestBlockManager |
|   | hadoop.hdfs.server.blockmanagement.TestComputeInvalidateWork |
|   | hadoop.hdfs.server.namenode.ha.TestStateTransitionFailure |
|   | hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes |
|   | hadoop.hdfs.server.balancer.TestBalancerWithSaslDataTransfer |
|   | hadoop.hdfs.server.balancer.TestBalancer |
|   | hadoop.hdfs.server.namenode.TestDefaultBlockPlacementPolicy |
|   | hadoop.hdfs.server.namenode.TestFSImageWithAcl |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailureToleration |
|   | hadoop.hdfs.server.namenode.ha.TestBootstrapStandbyWithQJM |
|   | hadoop.hdfs.server.namenode.ha.TestBootstrapStandby |
|   | hadoop.hdfs.TestDFSShell |
|   | hadoop.hdfs.client.impl.TestLeaseRenewer |
|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshotStatsMXBean |
|   | hadoop.hdfs.protocol.datatransfer.sasl.TestSaslDataTransfer |
|   | hadoop.hdfs.server.namenode.TestFSImageWithXAttr |
|   | hadoop.hdfs.server.balancer.TestBalancerWithEncryptedTransfer |
|   | hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA |
|   | hadoop.hdfs.protocol.TestAnnotations |
|   | hadoop.hdfs.server.namenode.TestEditLog |
|   | hadoop.hdfs.server.datanode.TestDiskError |
|   | hadoop.hdfs.server.namenode.TestCacheDirectives |
|   | hadoop.hdfs.server.datanode.TestDataNodeECN |
|   | hadoop.hdfs.server.blockmanagement.TestReplicationPolicyWithNodeGroup |
|   | hadoop.hdfs.server.namenode.web.resources.TestWebHdfsDataLocality |
|   | hadoop.hdfs.server.namenode.TestFileTruncate |
|   | hadoop.security.TestPermission |
|   | hadoop.hdfs.server.namenode.snapshot.TestXAttrWithSnapshot |
|   | hadoop.hdfs.server.datanode.TestDataNodeInitStorage |
|   | hadoop.hdfs.server.namenode.snapshot.TestFileContextSnapshot |
|   | hadoop.hdfs.server.namenode.ha.TestNNHealthCheck |
|   | hadoop.hdfs.TestIsMethodSupported |
|   | hadoop.hdfs.server.namenode.TestAddStripedBlocks |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistLockedMemory |
|   | hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations |
|   | hadoop.hdfs.TestFileCreationDelete |
|   | hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot |
|   | hadoop.hdfs.TestErasureCodingZones |
|   | hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS |
|   | hadoop.hdfs.server.datanode.TestNNHandlesCombinedBlockReport |
|   | hadoop.hdfs.TestDFSStorageStateRecovery |
|   | hadoop.hdfs.util.TestStripedBlockUtil |
|   | hadoop.hdfs.server.blockmanagement.TestHostFileManager |
|   | hadoop.hdfs.server.namenode.TestXAttrConfigFlag |
|   | hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA |
|   | hadoop.hdfs.server.namenode.TestCheckPointForSecurityTokens |
|   | hadoop.hdfs.TestFileStatusWithECschema |
|   | hadoop.hdfs.server.namenode.ha.TestStandbyBlockManagement |
|   | hadoop.hdfs.server.blockmanagement.TestNodeCount |
|   | hadoop.hdfs.server.namenode.ha.TestEditLogsDuringFailover |
|   | hadoop.hdfs.TestAppendDifferentChecksum |
|   | hadoop.hdfs.TestRemoteBlockReader |
|   | hadoop.hdfs.server.namenode.metrics.TestNNMetricFilesInGetListingOps |
|   | hadoop.hdfs.TestRestartDFS |
|   | hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes |
|   | hadoop.cli.TestHDFSCLI |
|   | hadoop.hdfs.server.datanode.TestNNHandlesBlockReportPerStorage |
|   | hadoop.hdfs.server.namenode.ha.TestFailureToReadEdits |
|   | hadoop.hdfs.server.namenode.TestDecommissioningStatus |
|   | hadoop.hdfs.server.namenode.ha.TestStandbyIsHot |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestScrLazyPersistFiles |
|   | hadoop.hdfs.server.datanode.TestBlockScanner |
|   | hadoop.hdfs.server.mover.TestStorageMover |
|   | hadoop.hdfs.TestHDFSFileSystemContract |
|   | hadoop.hdfs.server.datanode.TestFsDatasetCacheRevocation |
|   | hadoop.hdfs.server.namenode.TestClusterId |
|   | hadoop.cli.TestAclCLI |
|   | hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints |
|   | hadoop.hdfs.server.namenode.TestStripedINodeFile |
|   | hadoop.hdfs.TestDFSPermission |
|   | hadoop.hdfs.TestHDFSServerPorts |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistReplicaRecovery |
|   | hadoop.hdfs.security.TestClientProtocolWithDelegationToken |
|   | hadoop.hdfs.server.namenode.ha.TestEditLogTailer |
|   | hadoop.cli.TestCryptoAdminCLI |
|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshotBlocksMap |
|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshotNameWithInvalidCharacters |
|   | hadoop.hdfs.server.datanode.TestStorageReport |
|   | hadoop.hdfs.server.namenode.ha.TestQuotasWithHA |
|   | hadoop.hdfs.server.namenode.TestNameNodeResourceChecker |
|   | hadoop.hdfs.server.datanode.TestDirectoryScanner |
|   | hadoop.hdfs.server.datanode.TestDeleteBlockPool |
|   | hadoop.hdfs.server.blockmanagement.TestBlockInfoStriped |
|   | hadoop.hdfs.TestBlockReaderLocalLegacy |
|   | hadoop.hdfs.server.namenode.snapshot.TestRenameWithSnapshots |
|   | hadoop.hdfs.server.namenode.TestEditLogAutoroll |
|   | hadoop.hdfs.security.token.block.TestBlockToken |
|   | hadoop.hdfs.server.namenode.TestGetBlockLocations |
|   | hadoop.hdfs.server.namenode.TestSecureNameNode |
|   | hadoop.hdfs.server.namenode.TestNameNodeXAttr |
|   | hadoop.hdfs.server.datanode.web.dtp.TestDtpHttp2 |
|   | hadoop.hdfs.server.blockmanagement.TestUnderReplicatedBlockQueues |
|   | hadoop.hdfs.server.datanode.TestBlockRecovery |
|   | hadoop.hdfs.server.blockmanagement.TestPendingInvalidateBlock |
|   | hadoop.hdfs.TestDFSStartupVersions |
|   | hadoop.hdfs.TestWriteBlockGetsBlockLengthHint |
|   | hadoop.hdfs.TestAbandonBlock |
|   | hadoop.hdfs.TestFetchImage |
|   | hadoop.hdfs.server.blockmanagement.TestReplicationPolicy |
|   | hadoop.cli.TestCacheAdminCLI |
|   | hadoop.hdfs.server.namenode.snapshot.TestINodeFileUnderConstructionWithSnapshot |
|   | hadoop.hdfs.server.namenode.TestFSNamesystemMBean |
|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshotListing |
|   | hadoop.hdfs.server.blockmanagement.TestSequentialBlockId |
|   | hadoop.hdfs.server.namenode.ha.TestHAAppend |
|   | hadoop.hdfs.TestDatanodeDeath |
|   | hadoop.hdfs.server.datanode.TestFsDatasetCache |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestInterDatanodeProtocol |
|   | hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA |
|   | hadoop.hdfs.server.datanode.TestDataNodeVolumeFailure |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestRbwSpaceReservation |
|   | hadoop.hdfs.security.TestDelegationToken |
|   | hadoop.hdfs.server.datanode.TestCachingStrategy |
|   | hadoop.hdfs.TestDFSClientExcludedNodes |
|   | hadoop.hdfs.server.namenode.TestFsck |
|   | hadoop.hdfs.server.namenode.ha.TestInitializeSharedEdits |
|   | hadoop.hdfs.TestSnapshotCommands |
|   | hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks |
|   | hadoop.hdfs.server.datanode.TestHSync |
|   | hadoop.hdfs.server.namenode.ha.TestDelegationTokensWithHA |
|   | hadoop.hdfs.server.namenode.TestProcessCorruptBlocks |
|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshottableDirListing |
|   | hadoop.hdfs.server.blockmanagement.TestHeartbeatHandling |
|   | hadoop.hdfs.TestDFSInputStream |
|   | hadoop.hdfs.server.namenode.TestNNThroughputBenchmark |
|   | hadoop.hdfs.TestDFSOutputStream |
|   | hadoop.hdfs.server.namenode.TestNameNodeAcl |
|   | hadoop.hdfs.server.namenode.TestQuotaByStorageType |
|   | hadoop.hdfs.server.datanode.TestDataNodeMXBean |
|   | hadoop.hdfs.server.namenode.ha.TestPendingCorruptDnMessages |
|   | hadoop.hdfs.server.datanode.TestBlockReplacement |
|   | hadoop.hdfs.server.namenode.TestBlockUnderConstruction |
|   | hadoop.hdfs.TestDFSInotifyEventInputStream |
|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshotMetrics |
|   | hadoop.hdfs.server.namenode.snapshot.TestSnapshotReplication |
|   | hadoop.hdfs.server.namenode.TestNameNodeMXBean |
|   | hadoop.hdfs.server.blockmanagement.TestReplicationPolicyConsiderLoad |
|   | hadoop.hdfs.security.TestDelegationTokenForProxyUser |
|   | hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade |
|   | hadoop.hdfs.server.namenode.ha.TestFailureOfSharedDir |
|   | hadoop.hdfs.server.datanode.TestTransferRbw |
|   | hadoop.hdfs.server.namenode.TestListCorruptFileBlocks |
|   | hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyWriter |
|   | hadoop.hdfs.server.namenode.TestFileContextXAttr |
|   | hadoop.hdfs.server.namenode.ha.TestFailoverWithBlockTokensEnabled |
|   | hadoop.hdfs.server.namenode.TestHostsFiles |
|   | hadoop.hdfs.TestReservedRawPaths |
|   | hadoop.hdfs.server.namenode.TestNameNodeRecovery |
|   | hadoop.hdfs.server.blockmanagement.TestPendingReplication |
|   | hadoop.hdfs.TestFileCorruption |
|   | hadoop.hdfs.server.namenode.TestStorageRestore |
|   | hadoop.hdfs.server.namenode.TestDiskspaceQuotaUpdate |
|   | hadoop.hdfs.server.namenode.TestINodeAttributeProvider |
| Timed out tests | org.apache.hadoop.hdfs.server.namenode.TestINodeFile |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12744151/HDFS-8259-HDFS-7285.001.patch |
| Optional Tests | javac unit findbugs checkstyle |
| git revision | HDFS-7285 / 2c494a8 |
| Release Audit | https://builds.apache.org/job/PreCommit-HDFS-Build/11622/artifact/patchprocess/patchReleaseAuditProblems.txt |
| Findbugs warnings | https://builds.apache.org/job/PreCommit-HDFS-Build/11622/artifact/patchprocess/newPatchFindbugsWarningshadoop-hdfs.html |
| hadoop-hdfs test log | https://builds.apache.org/job/PreCommit-HDFS-Build/11622/artifact/patchprocess/testrun_hadoop-hdfs.txt |
| Test Results | https://builds.apache.org/job/PreCommit-HDFS-Build/11622/testReport/ |
| Java | 1.7.0_55 |
| uname | Linux asf903.gq1.ygridcore.net 3.13.0-36-lowlatency #63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/11622/console |


This message was automatically generated.","24/Jul/15 01:35;demongaorui;Hi [~xinwei], this jira share the same idea of HDFS-8260 .",,,,,,,,,,,,,
[umbrella] System tests for EC feature,HDFS-8197,12822536,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,lewuathe,kaisasak,kaisasak,21/Apr/15 02:44,18/Jan/17 22:40,18/Feb/21 10:08,,HDFS-7285,,,,,,,0,system-tests,test,,,"This is umbrella JIRA for system test of EC feature.
All sub-tasks and test cases are listed under this ticket. All items which are assumed to be tested are here.

* Create/Delete EC File
* Create/Delete ECZone
* teragen against EC files
* terasort against EC files
* teravalidate against EC files
",,,,,,,,,,,,HDFS-8198,HDFS-8199,HDFS-8259,HDFS-8260,HDFS-8261,HDFS-8262,HDFS-8263,HDFS-8264,HDFS-8265,HDFS-8267,HDFS-8266,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-04-22 04:28:58.785,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 06 20:53:33 UTC 2016,,,,,,,"0|i2dinj:",9223372036854775807,,,,,,,,,,,,,,,,,,"22/Apr/15 04:28;zhz;Thanks for initiating the work Kai! I think the timing is good since we already have basic I/O functionalities (for reading we can always use pread now). Could you move the 2 sub tasks to HDFS-7285? You can use this umbrella JIRA to _incorporate_ them. Do you plan to work on the two sub tasks?","22/Apr/15 06:48;kaisasak;[~zhz] How can I move these sub-tasks under HDFS-7285? It seems that I cannot convert these tasks as sub-task under HDFS-7285. ","22/Apr/15 07:21;kaisasak;Sorry I can do that. 

I have a plan to work on some of these test JIRA and also I assume these are not sufficient to do comprehensive system test. Please add any test cases and assign to yourself including these two tasks. Thank you.","22/Apr/15 18:38;zhz;Thanks Kai for clarifying!","24/Jul/15 22:48;zhz;Moving system test JIRAs as follow-ons.","06/Dec/16 20:53;andrew.wang;Hi folks, is there any action planned on these JIRAs? It'd be good to improve our test coverage.",,,,,,,,,,,
Fix HDFS unit test failures on Windows,HDFS-7947,12782728,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,xyao,xyao,xyao,18/Mar/15 00:08,02/Apr/15 18:27,18/Feb/21 10:08,,,,,,,,,0,,,,,This is the umbrella JIRA to fix HDFS unit test failures on Windows.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 18:27:08 UTC 2015,,,,,,,"0|i26w73:",9223372036854775807,,,,,,,,,,,,,,,,,,"02/Apr/15 18:27;xyao;Link to fix for Hadoop unit test failures.",,,,,,,,,,,,,,,,
Add more unit tests to TestDFSOutputStream,HDFS-7762,12773675,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,libo-intel,libo-intel,libo-intel,10/Feb/15 03:25,10/Feb/15 11:17,18/Feb/21 10:08,,,,,hdfs-client,,,,0,,,,,"Currently TestDFSOutputStream just contains one test. There exists some projects that may extends the functionality of DFSOutputStream, such as HDFS-7729. Enough test units should be provided in TestDFSOutputStream to guarantee the changes to DFSOutputStream don't influence its original logic.",,,,,,,,,,,,,,,,,,,,,,,,HDFS-7729,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-02-10 03:25:38.0,,,,,,,"0|i25ehb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestPipelinesFailover#testFailoverRightBeforeCommitSynchronization sometimes fails in Java 8 build,HDFS-7576,12764214,Test,Reopened,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,,yuzhihong@gmail.com,yuzhihong@gmail.com,30/Dec/14 15:32,09/Jun/15 05:13,18/Feb/21 10:08,,,,,,,,,0,,,,,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/54/ :
{code}
REGRESSION:  org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testFailoverRightBeforeCommitSynchronization

Error Message:
test timed out after 30000 milliseconds

Stack Trace:
java.lang.Exception: test timed out after 30000 milliseconds
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
        at org.apache.hadoop.test.GenericTestUtils$DelayAnswer.waitForCall(GenericTestUtils.java:226)
        at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testFailoverRightBeforeCommitSynchronization(TestPipelinesFailover.java:386)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2015-02-13 20:34:56.334,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 05:13:09 UTC 2015,,,,,,,"0|i23unr:",9223372036854775807,,,,,,,,,,,,,,,,,,"13/Feb/15 20:34;kihwal;I saw this in https://builds.apache.org/job/PreCommit-HDFS-Build/9574//testReport/ today.","09/Jun/15 05:13;xyao;I saw the same test failed but with a different stack from this https://builds.apache.org/job/PreCommit-HDFS-Build/11273/artifact/patchprocess/testrun_hadoop-hdfs.txt today.


{code}
Tests run: 8, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 84.396 sec <<< FAILURE! - in org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover
testFailoverRightBeforeCommitSynchronization(org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover)  Time elapsed: 2.029 sec  <<< ERROR!
java.io.IOException: Cannot obtain block length for LocatedBlock{BP-316490051-67.195.81.148-1433795731197:blk_1073741825_1001; getBlockSize()=2048; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:59303,DS-1e89e854-057a-4e6a-8fb4-4f5ee9966e66,DISK], DatanodeInfoWithStorage[127.0.0.1:58885,DS-a1d1439d-3e34-4ca1-b03e-e1aff5b873ea,DISK], DatanodeInfoWithStorage[127.0.0.1:44884,DS-ec633801-467c-4f8e-b827-fcbd04dea5d7,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:394)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:336)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:272)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:263)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1184)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:308)
	at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:304)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:304)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:768)
	at org.apache.hadoop.hdfs.DFSTestUtil.getFirstBlock(DFSTestUtil.java:747)
	at org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover.testFailoverRightBeforeCommitSynchronization(TestPipelinesFailover.java:353)

{code}",,,,,,,,,,,,,,,
TestDFSAdminWithHA#testRefreshSuperUserGroupsConfiguration fails against Java 8,HDFS-7464,12758924,Test,Reopened,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,,yuzhihong@gmail.com,yuzhihong@gmail.com,02/Dec/14 15:51,29/Nov/15 15:41,18/Feb/21 10:08,,,,,,,,,0,,,,,"From https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/23/ :
{code}
REGRESSION:  org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA.testRefreshSuperUserGroupsConfiguration

Error Message:
refreshSuperUserGroupsConfiguration: End of File Exception between local host is: ""asf908.gq1.ygridcore.net/67.195.81.152""; destination host is: ""localhost"":12700; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException expected:<0> but was:<-1>

Stack Trace:
java.lang.AssertionError: refreshSuperUserGroupsConfiguration: End of File Exception between local host is: ""asf908.gq1.ygridcore.net/67.195.81.152""; destination host is: ""localhost"":12700; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException expected:<0> but was:<-1>
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.failNotEquals(Assert.java:743)
        at org.junit.Assert.assertEquals(Assert.java:118)
        at org.junit.Assert.assertEquals(Assert.java:555)
        at org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA.testRefreshSuperUserGroupsConfiguration(TestDFSAdminWithHA.java:228)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2014-12-02 19:37:16.403,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 19 14:53:43 UTC 2015,,,,,,,"0|i22yxb:",9223372036854775807,,,,,,,,,,,,,,,,,,"02/Dec/14 19:37;airbots;Hi [~ted_yu], thank you for reporting this problem. I tried to reproduce it using java 1.8.0_25 against trunk on my mac but it passed.
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA
-------------------------------------------------------------------------------
Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 19.104 sec - in org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA ","02/Dec/14 22:06;yuzhihong@gmail.com;Can you take a look at https://builds.apache.org/job/Hadoop-Hdfs-trunk-Java8/23/testReport/junit/org.apache.hadoop.hdfs.tools/TestDFSAdminWithHA/testRefreshSuperUserGroupsConfiguration to see if you can find some clue ?","19/Oct/15 14:53;weichiu;I am seeing this today with Java 7
java version ""1.7.0_79""
Java(TM) SE Runtime Environment (build 1.7.0_79-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)

Running org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA
Tests run: 11, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 15.205 sec <<< FAILURE! - in org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA
testRefreshSuperUserGroupsConfiguration(org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA)  Time elapsed: 0.808 sec  <<< FAILURE!
java.lang.AssertionError: refreshSuperUserGroupsConfiguration: End of File Exception between local host is: ""weichiu-MBP.local/172.16.1.61""; destination host is: ""localhost"":10872; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException expected:<0> but was:<-1>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA.testRefreshSuperUserGroupsConfiguration(TestDFSAdminWithHA.java:235)


Results :

Failed tests:
  TestDFSAdminWithHA.testRefreshSuperUserGroupsConfiguration:235 refreshSuperUserGroupsConfiguration: End of File Exception between local host is: ""weichiu-MBP.local/172.16.1.61""; destination host is: ""localhost"":10872; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException expected:<0> but was:<-1>
",,,,,,,,,,,,,,
Tests for Crypto filesystem decorating HDFS,HDFS-6537,12721299,Test,In Progress,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,hitliuyi,hitliuyi,hitliuyi,15/Jun/14 13:07,05/Aug/14 00:14,18/Feb/21 10:08,,fs-encryption (HADOOP-10150 and HDFS-6134),,fs-encryption (HADOOP-10150 and HDFS-6134),security,,,,0,,,,,"{{CryptoFileSystem}} targets other filesystem. But currently other built-in Hadoop filesystems don't have XAttrs support, so this JIRA uses HDFS for test.",,,,,,,,,,,,,,,,,,,,,,,,HADOOP-10604,,,,"15/Jun/14 13:08;hitliuyi;HDFS-6537.patch;https://issues.apache.org/jira/secure/attachment/12650484/HDFS-6537.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,399495,,,Sun Jun 15 13:08:57 UTC 2014,,,,,,,"0|i1wrtr:",399604,,,,,,,,,,,,,fs-encryption (HADOOP-10150 and HDFS-6134),,,,,"15/Jun/14 13:08;hitliuyi;This test case extends {{CryptoFileSystemTestBase}}.",,,,,,,,,,,,,,,,
Running Httpfs UTs using MiniKDC,HDFS-6149,12703263,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,ddimatos,jwang302,jwang302,24/Mar/14 18:06,30/Mar/16 01:18,18/Feb/21 10:08,,2.2.0,,,test,,,,0,,,,,JIRA HADOOP-9866 converted hadoop-common Kerberos unit tests to use MiniKDC. This JIRA is doing the same thing for HttpFS to avoid the hassle of setting up Kerberos environment to run HttpFS with Kerberos unit tests.,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/14 08:47;jwang302;HDFS-6149.patch;https://issues.apache.org/jira/secure/attachment/12687992/HDFS-6149.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,381601,,,Thu Dec 18 08:43:27 UTC 2014,,,,,,,"0|i1tquv:",381876,,,,,,,,,,,,,,,,,,"24/Mar/14 18:16;jwang302;The test that cancels delegation token in testDelegationTokenHttpFSAccess assumes the operation does not require credentials. However the following block of code in HttpFSKerberosAuthenticationHandler shows that if there is no token, then the CANCELDELEGATIONTOKEN operation is not performed. Thus causing an assertion error as the the test expects a response code of 200 but getting 401 instead.

else if (dtOp.requiresKerberosCredentials() && token == null) {
          response.sendError(HttpServletResponse.SC_UNAUTHORIZED,
            MessageFormat.format(
              ""Operation [{0}] requires SPNEGO authentication established"",
              dtOp));
          requestContinues = false;
        }","18/Dec/14 08:43;jwang302;Updating the patch to move TestHttpFSWithKerberos to use MiniKDC rather than depending on native Kerberos environment setup for unit tests.",,,,,,,,,,,,,,,
TestnameNodeMetrics#testCorruptBlock fails intermittently,HDFS-6126,12702450,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,mitdesai,mitdesai,19/Mar/14 18:25,14/Jun/18 17:11,18/Feb/21 10:08,,2.4.0,,,,,,,0,,,,,"I get the following error
{noformat}
testCorruptBlock(org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics)  Time elapsed: 5.556 sec  <<< FAILURE!
java.lang.AssertionError: Bad value for metric CorruptBlocks expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:128)
	at org.junit.Assert.assertEquals(Assert.java:472)
	at org.apache.hadoop.test.MetricsAsserts.assertGauge(MetricsAsserts.java:190)
	at org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics.testCorruptBlock(TestNameNodeMetrics.java:247)


Results :

Failed tests: 
  TestNameNodeMetrics.testCorruptBlock:247 Bad value for metric CorruptBlocks expected:<1> but was:<0>
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2018-06-14 17:10:01.776,,,false,,,,,,,,,,,,,,,,,,380789,,,Thu Jun 14 17:10:01 UTC 2018,,,,,,,"0|i1tlwn:",381068,,,,,,,,,,,,,,,,,,"14/Jun/18 17:10;elgoiri;We are seeing this error once in a while.
For example [here|https://builds.apache.org/job/PreCommit-HDFS-Build/24436/testReport/org.apache.hadoop.hdfs.server.namenode.metrics/TestNameNodeMetrics/testCorruptBlock/].
There are many JIRAs that have targetted this: HDFS-2434, HDFS-3812, HDFS-4338, HDFS-4355.
However, this is still present.
[~zuzhan], can you take a look?",,,,,,,,,,,,,,,,
Remake org.apache.hadoop.hdfs.server.datanode.TestStartSecureDataNode on MiniKdc,HDFS-5410,12675294,Test,Patch Available,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,iveselovsky,iveselovsky,iveselovsky,23/Oct/13 17:21,12/May/16 18:11,18/Feb/21 10:08,,2.3.0,3.0.0-alpha1,,,,,,0,BB2015-05-TBR,,,,Reimplement test org.apache.hadoop.hdfs.server.datanode.TestStartSecureDataNode on MiniKdc.,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/13 17:30;iveselovsky;HDFS-5410-branch-2--N1.patch;https://issues.apache.org/jira/secure/attachment/12609882/HDFS-5410-branch-2--N1.patch","23/Oct/13 17:30;iveselovsky;HDFS-5410-trunk--N1.patch;https://issues.apache.org/jira/secure/attachment/12609883/HDFS-5410-trunk--N1.patch",,,,2.0,,,,,,,,,,,,,,,,,,,,2013-10-23 19:39:48.713,,,false,,,,,,,,,,,,,,,,,,354914,,,Sat May 02 04:42:04 UTC 2015,,,,,,,"0|i1p6qn:",355203,,,,,,,,,,,,,,,,,,"23/Oct/13 17:30;iveselovsky;Rewritten with MiniKdc.
Added code into SecureDataNodeStarter to allow test runing on >1024 ports.","23/Oct/13 17:44;iveselovsky;see also https://issues.apache.org/jira/browse/HDFS-4312","23/Oct/13 19:39;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12609883/HDFS-5410-trunk--N1.patch
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 1 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/5264//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/5264//console

This message is automatically generated.","30/May/14 21:41;cnauroth;Hello, [~iveselovsky].  This is another security testing patch that I'd like to help get committed.  Would you please rebase the patch?  Thank you for working on this.","30/May/14 21:43;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12609883/HDFS-5410-trunk--N1.patch
  against trunk revision .

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/7020//console

This message is automatically generated.","02/May/15 04:39;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12609883/HDFS-5410-trunk--N1.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10604/console |


This message was automatically generated.","02/May/15 04:42;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12609883/HDFS-5410-trunk--N1.patch |
| Optional Tests | javadoc javac unit findbugs checkstyle |
| git revision | trunk / f1a152c |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10621/console |


This message was automatically generated.",,,,,,,,,,
Add more Unit Test Case for HDFS-3701 HDFS Miss Final Block Reading when File is Open for Write,HDFS-4590,12636469,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,huned,huned,huned,11/Mar/13 23:26,15/Apr/13 01:44,18/Feb/21 10:08,,1.2.0,,,hdfs-client,,,,0,,,,,"Add more Java Unit Test Coverage for the feature HDFS-3701 where the file is opened for writing, the DFSClient calls one of the datanode owning the last block to get its size, and if this datanode is dead then test for if socket IO Exception is thrown. Add a unit test case to write to a file, shutdown the datanodes, and then try to read from the file and expect an IO Exception. On branch 1 it should throw the IO Exception as expected.",Java Unit Test Case,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/13 18:02;huned;HDFS-4590.b1.001_old.patch;https://issues.apache.org/jira/secure/attachment/12578045/HDFS-4590.b1.001_old.patch","10/Apr/13 04:34;huned;HDFS-4590.b1.002.patch;https://issues.apache.org/jira/secure/attachment/12577950/HDFS-4590.b1.002.patch",,,,2.0,,,,,,,,,,,,,,,,,,,,2013-03-12 18:15:07.991,,,false,,,,,,,,,,,,,,,,,,316961,,,Mon Apr 15 01:44:27 UTC 2013,,,,,,,"0|i1ioyf:",317303,,,,,,,,,,,,,,,,,,"12/Mar/13 02:41;huned;Added Java Unit Test Case to test scenario mentioned above.","12/Mar/13 18:15;jingzhao;The patch looks good to me. Huned, could you please post the results of ant test and test-patch?","15/Mar/13 04:31;huned; The test result is pass with the following Expected Exception being thrown which passes the test.

java.io.IOException: Failed to get block info from any of the DN in pipeline: [127.0.0.1:57372]
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.updateBlockInfo(DFSClient.java:2048)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.fetchLocatedBlocks(DFSClient.java:1989)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1944)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.<init>(DFSClient.java:1936)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:731)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:165)
	at org.apache.hadoop.hdfs.TestFileLengthOnClusterRestart.testFileLengthWithDatNodesShutDown(TestFileLengthOnClusterRestart.java:130)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
Shutting down the Mini HDFS Cluster","04/Apr/13 16:01;sureshms;Comments:
# Nit: typo trting
# Some changes you could do in that part of the code, unrelated to your patch:
#* verifyNNIsInSafeMode method declares throwing unnecessary IOException
#* ""NN might not started"" -> ""NN might not be started""
","10/Apr/13 04:34;huned;Hi,
I edited according to the suggestions given previously. Please let me know if this patch looks better.","10/Apr/13 06:58;sureshms;Did you delete the old patch? ","10/Apr/13 07:02;huned;Hi Suresh,
Yes, I deleted the old patch, I made the changes and upload a new patch.","10/Apr/13 07:16;sureshms;If you delete the old patch, the comments I previously posted have no context. Please leave the old patches for which comments have been posted as is.","10/Apr/13 18:02;huned;Hi,
I did save a copy of the old patch on my local drive, so I will add it back for reference if needed.","15/Apr/13 01:44;sureshms;+1 for the patch.",,,,,,,
TestLocalDirAllocator and TestDiskError need to be reviewed for Windows compatibility and either re-enabled or re-documented.,HDFS-4554,12635376,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Trivial,,,cnauroth,cnauroth,05/Mar/13 17:37,05/Mar/13 17:39,18/Feb/21 10:08,,,,,,,,,0,,,,,"Currently, these tests are skipped on Windows, and there are comments explaining an incompatibility with Cygwin.  We need to review these to see if we can re-enable them using the current winutils setup.  If they cannot pass on Windows, then the comments need to be updated to state that the tests are incompatible with ""Windows"" instead of ""Cygwin"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,315869,,,2013-03-05 17:37:29.0,,,,,,,"0|i1ii7z:",316212,,,,,,,,,,,,,trunk-win,,,,,,,,,,,,,,,,,,,,,
Add unit tests for HTTP-based filesystems against secure MiniDFSCluster,HDFS-4237,12618125,Test,Patch Available,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,schu,schu,schu,29/Nov/12 09:20,06/May/15 03:34,18/Feb/21 10:08,,2.0.0-alpha,,,security,test,webhdfs,,0,BB2015-05-TBR,,,,"Now that we can start a secure MiniDFSCluster (HADOOP-9004), we need more security unit tests.

A good area to add secure tests is the HTTP-based filesystems (WebHDFS, HttpFs).",,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/May/15 17:44;aw;HDFS-4237.008.patch;https://issues.apache.org/jira/secure/attachment/12729960/HDFS-4237.008.patch","17/Dec/12 21:27;schu;HDFS-4237.patch.001;https://issues.apache.org/jira/secure/attachment/12561356/HDFS-4237.patch.001","21/Jan/13 21:57;schu;HDFS-4237.patch.007;https://issues.apache.org/jira/secure/attachment/12565859/HDFS-4237.patch.007","23/Jan/13 00:19;schu;HDFS-4237.patch.008;https://issues.apache.org/jira/secure/attachment/12566056/HDFS-4237.patch.008",,4.0,,,,,,,,,,,,,,,,,,,,2012-12-17 21:42:02.06,,,false,,,,,,,,,,,,,,,,,,292735,,,Sat May 02 20:00:33 UTC 2015,,,,,,,"0|i0sbqv:",163379,,,,,,,,,,,,,2.0.3-alpha,,,,,"17/Dec/12 21:27;schu;I've attached a patch for secure WebHDFS unit tests.

This patch edits MiniDFSCluster to allow starting multiple secure DataNodes. This involves finding free ports under 1024. Earlier, from HADOOP-9004, we could only start one secure DataNode.

SecureWebHdfsFileSystemContract is ignored because it extends TestCase (JUnit3) but I use JUnit4's Assume to check if the external KDC properties are set. TestCase and Assume don't work together: http://issues.gradle.org/browse/GRADLE-1879

The test cases from SecureWebHdfsFileSystemContract are run in TestSecureWebHDFS, which also runs TestWebHDFS's _testLargeFile_.




","17/Dec/12 21:42;atm;Marking ""patch available"" for Stephen so that test-patch runs.

Also, Stephen - not a big deal, but please don't set the ""fix version"" until the patch is actually committed. Until then, please just set the ""target versions"" field to indicate which branch you think this bug should be fixed on.","17/Dec/12 22:08;schu;Thanks, ATM. Will do.","17/Dec/12 23:40;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12561356/HDFS-4237.patch.001
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 7 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3675//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3675//console

This message is automatically generated.","11/Jan/13 01:11;schu;Is anyone available to review the patch? It'd be much appreciated!","11/Jan/13 02:48;adi2;A few tab characters crept into your patch.  Please take them out.

{code}
+// We ignore this test class because the test cases should be run only
+// when external Kdc parameters are specified. TestSecureWebHDFS
+// checks if these settings are set using JUnit4 Assume, which does
+// not work with TestCase (JUnit 3.x). Read GRADLE-1879 for more
+// information.
{code}
Given that this is going into hadoop-hdfs, I don't see why we need to support jUnit 3.

{code}
+    TestWebHDFS.largeFileTest(conf, 200L << 20, secureUgi); //200MB file length
{code}
I'd rather see it written as 200 * 1024 * 1024 rather than using a bitshift.

Other than those issues, it seems reasonable.  Presumably this requires special permissions to run as root to get ports<1023, can we document that process somewhere?","21/Jan/13 21:57;schu;Thank you for the review, Andy.	I've uploaded a new patch. In it...

I've removed the tab characters.

I used ""200 * 1024 * 1024"" instead of the bitshift.

I converted FileSystemContractBaseTest (and the classes that extend it) to JUnit4. Previously, it was written in Junit3 style (extends TestCase), but Junit3 TestCase and Junit4 Assume are incompatible, e.g. HDFS-3966.

How does me adding a section on running/developing secure unit tests in the Developer Documentation in http://wiki.apache.org/hadoop/ sound? Is there a better place for documentation?","21/Jan/13 23:57;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12565859/HDFS-4237.patch.007
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 11 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:red}-1 core tests{color}.  The patch failed these unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs:

                  org.apache.hadoop.hdfs.web.TestSecureWebHdfsFileSystemContract

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3861//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3861//console

This message is automatically generated.","22/Jan/13 00:03;schu;Woops, I forgot the Assume check in TestSecureWebHdfsFileSystemContract.","22/Jan/13 00:26;adi2;{noformat}
+      String address = ""127.0.0.1:"" + port;
{noformat}
this line grew some trailing whitespace.

{{SecureHdfsTestUtil.java}} license comment has trailing whitespace.

{noformat}
+ * Our unit tests use 127.0.0.1/localhost to address the host running
+ * the tests. However, WebHDFS secure authentication using localhost is
+ * not allowed (kerberos authentication will complain it can't find
+ * the server). The actual hostname must be used. Therefore, to run
+ * the secure WebHDFS tests in your test environment, make 127.0.0.1
+ * resolve to the actual hostname.
{noformat}

I'm not sure this is an acceptable requirement, but let's go ahead and get it checked in as is.  Worst case we just back out this code.
(It would be better to teach the tests how to run in a reasonable environment where the hostname resolves to the actual eth0 address or similar.  This may mean that it's impossible to do jUnit style tests of Kerberized security.)

bq. How does me adding a section on running/developing secure unit tests in the Developer Documentation in http://wiki.apache.org/hadoop/ sound? Is there a better place for documentation?

A wiki page sounds like an excellent start.  I think it belongs on a new page but you can use your judgment if you find a page where it fits in.
","23/Jan/13 00:19;schu;Thanks, Andy.

I removed the spaces ahead of ""String address ..."" and the spaces in the license comment in SecureHdfsTestUtil.java. BTW, what do you use to catch these spaces?

Now, TestSecureWebHdfsFileSystemContract will be skipped unless the test is being run with the external KDC.

I agree that it'd be good to investigate if we can teach the tests to run on a reasonable environment.

I'll add a new wiki page about how to develop and run secure unit tests. ","23/Jan/13 00:36;adi2;bq. BTW, what do you use to catch these spaces?

I open the patch file in less and search for space.  Less highlights matches by default.  There are many similar tools; vim has a diff mode that can highlight bad whitespace, ReviewBoard turns it bright red, several Git GUI tools such as gitk will call it out, it's easy to write a git commit hook that refuses to allow a commit with bad whitespace.","23/Jan/13 00:43;adi2;patch.008 LGTM. +1 (non-binding).","23/Jan/13 02:18;hadoopqa;{color:green}+1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12566056/HDFS-4237.patch.008
  against trunk revision .

    {color:green}+1 @author{color}.  The patch does not contain any @author tags.

    {color:green}+1 tests included{color}.  The patch appears to include 11 new or modified test files.

    {color:green}+1 javac{color}.  The applied patch does not increase the total number of javac compiler warnings.

    {color:green}+1 javadoc{color}.  The javadoc tool did not generate any warning messages.

    {color:green}+1 eclipse:eclipse{color}.  The patch built with eclipse:eclipse.

    {color:green}+1 findbugs{color}.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    {color:green}+1 release audit{color}.  The applied patch does not increase the total number of release audit warnings.

    {color:green}+1 core tests{color}.  The patch passed unit tests in hadoop-common-project/hadoop-common hadoop-hdfs-project/hadoop-hdfs.

    {color:green}+1 contrib tests{color}.  The patch passed contrib unit tests.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/3868//testReport/
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/3868//console

This message is automatically generated.","02/May/15 20:00;hadoopqa;\\
\\
| (x) *{color:red}-1 overall{color}* |
\\
\\
|| Vote || Subsystem || Runtime || Comment ||
| {color:red}-1{color} | patch |   0m  0s | The patch command could not apply the patch during dryrun. |
\\
\\
|| Subsystem || Report/Notes ||
| Patch URL | http://issues.apache.org/jira/secure/attachment/12729960/HDFS-4237.008.patch |
| Optional Tests | javac unit findbugs checkstyle |
| git revision | trunk / 6ae2a0d |
| Console output | https://builds.apache.org/job/PreCommit-HDFS-Build/10722/console |


This message was automatically generated.",,
libhdfs: test_libhdfs_threaded should test non-recursive delete,HDFS-3657,12598767,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,cmccabe,cmccabe,cmccabe,13/Jul/12 23:25,07/Sep/12 21:09,18/Feb/21 10:08,,2.0.0-alpha,,,libhdfs,,,,0,,,,,libhdfs: test_libhdfs_threaded should test non-recursive delete,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,257206,,,2012-07-13 23:25:20.0,,,,,,,"0|i0jwyn:",114270,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add fuzz tester for FSImage serialization / deserializtion,HDFS-3346,12553581,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,,cmccabe,cmccabe,02/May/12 01:51,02/May/12 02:13,18/Feb/21 10:08,,,,,,,,,0,,,,,"HDFS-3134 added a fuzz test for edit log serialization.  The goal of this test was to ensure that we got no runtime errors when deserializing incorrect edit log data, only IOExceptions.  We should have a similar test for FSImage deserialization.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,237751,,,2012-05-02 01:51:21.0,,,,,,,"0|i0jwev:",114181,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
End-to-end test for making a non-HA HDFS cluster HA-enabled,HDFS-3269,12550745,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,,atm,atm,12/Apr/12 21:09,27/Apr/12 17:48,18/Feb/21 10:08,,2.0.0-alpha,,,ha,namenode,,,0,,,,,"Per Eli on HDFS-3259, it would be great if we had a test that did the following:

# Starts w/ non HA NN1
# Shutdown, enable HA on NN1, add SBN NN2
# Run initializeSharedEdits
# Start and transition to active NN1
# Run bootstrapStandby
# Confirm NN1 and NN2 are up and HA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-04-27 17:48:24.553,,,false,,,,,,,,,,,,,,,,,,235609,,,Fri Apr 27 17:48:24 UTC 2012,,,,,,,"0|i0jw9b:",114156,,,,,,,,,,,,,2.0.0-alpha,,,,,"27/Apr/12 17:48;mingjielai;Sorry. Cannot work on it in short. Will come back to finish it if no one else takes it. ",,,,,,,,,,,,,,,,
HA: Balancer should have a test with the combination of Federation and HA enabled.,HDFS-2980,12543421,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,umamaheswararao,umamaheswararao,umamaheswararao,21/Feb/12 01:34,16/Mar/17 21:30,18/Feb/21 10:08,,2.0.0-alpha,,,balancer & mover,federation,ha,,0,,,,,"Balancer should have test with Federation and HA enabled.
One scenario:
 Enable federation with 2 namespace ids and configure HA only for one namespace id and check the behaviors as expected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2012-03-03 01:37:06.594,,,false,,,,,,,,,,,,,,,,,,228667,,,Sat Mar 03 01:37:06 UTC 2012,,,,,,,"0|i0jvmv:",114055,,,,,,,,,,,,,,,,,,"03/Mar/12 01:37;atm;Converting to top-level issue with commit of HDFS-1623.",,,,,,,,,,,,,,,,
HA: multi-process MiniDFSCluster for testing ungraceful NN shutdown,HDFS-2758,12537451,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,eli,atm,eli,06/Jan/12 00:59,10/Mar/15 04:36,18/Feb/21 10:08,,2.0.0-alpha,,,ha,test,,,0,,,,,"We should test ungraceful termination of NN processes, this is generally useful for HDFS testing, but particularly needed for HA since we may do this as via fencing (send a NN a SIGILL via ssh kill -9, flip the PDU, etc). We can't currently do this with the MiniDFSCluster since everything is in one process and killing the native thread hosting the java thread terminates the whole process.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,222959,,,Sat Mar 03 01:20:46 UTC 2012,,,,,,,"0|i0jv6n:",113982,,,,,,,,,,,,,,,,,,"03/Mar/12 01:20;atm;Converting to top-level issue with commit of HDFS-1623.",,,,,,,,,,,,,,,,
HA: add tests for flaky and failed shared edits directories,HDFS-2755,12537425,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,atm,eli,05/Jan/12 22:00,10/Mar/15 04:36,18/Feb/21 10:08,,2.0.0-alpha,,,ha,test,,,0,,,,,We should test the behavior with both flaky and failed shared edits dirs. The tests should cover when name dir restore is enabled and disabled. There should be a warning and an API that we can check if all shared directories are not online.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,222933,,,Sat Mar 03 01:19:44 UTC 2012,,,,,,,"0|i0jv67:",113980,,,,,,,,,,,,,,,,,,"03/Mar/12 01:19;atm;Converting to top-level issue with commit of HDFS-1623.",,,,,,,,,,,,,,,,
 TestDFSClientExcludedNodes&TestBlocksScheduledCounter can cause for random failures iin Eclipse.  ,HDFS-2531,12529852,Test,Patch Available,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,umamaheswararao,umamaheswararao,umamaheswararao,02/Nov/11 13:28,06/May/15 03:27,18/Feb/21 10:08,,,,,test,,,,0,BB2015-05-TBR,,,,"FAILED:  org.apache.hadoop.hdfs.TestDFSClientExcludedNodes.testExcludedNodes

Error Message:
Cannot lock storage /home/jenkins/jenkins-slave/workspace/Hadoop-Hdfs-trunk/trunk/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1. The directory is already locked.

Stack Trace:
java.io.IOException: Cannot lock storage /home/jenkins/jenkins-slave/workspace/Hadoop-Hdfs-trunk/trunk/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1. The directory is already locked.
at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.lock(Storage.java:586)
at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.analyzeStorage(Storage.java:435)
at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:253)
at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:169)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:371)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:314)
at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:298)
at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:332)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/11 19:43;umamaheswararao;HDFS-2531.patch;https://issues.apache.org/jira/secure/attachment/12502018/HDFS-2531.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-11-02 17:35:09.415,,,false,,,,,,,,,,,,,,,,,,215711,,,Tue Mar 10 02:50:26 UTC 2015,,,,,,,"0|i0jusn:",113919,,,,,,,,,,,,,,,,,,"02/Nov/11 14:20;umamaheswararao;TestFileCreationNamenodeRestart also failing with the same error in trunk.

java.io.IOException: Cannot lock storage /home/jenkins/jenkins-slave/workspace/Hadoop-Hdfs-trunk/trunk/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2. The directory is already locked.
	at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.lock(Storage.java:586)
	at org.apache.hadoop.hdfs.server.common.Storage$StorageDirectory.analyzeStorage(Storage.java:435)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:253)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:169)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:371)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:314)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:298)
","02/Nov/11 14:23;umamaheswararao;
This problem will come when some tests are still having the locks on storage directories.
Looks the failures are random. 
TestDFSClientExcludedNodes is not shotting down the cluster, so the next test will fail diffinitely.

To find the cause for TestDFSClientExcludedNodes failure, need to check other test where they are not shutting down the cluster.","02/Nov/11 14:26;umamaheswararao;After analysing other tests, found that TestBlocksScheduledCounter is not shutting down the cluster. So, TestDFSClientExcludedNodes  might be the next test while runing in Hudson.","02/Nov/11 17:35;tlipcon;I think this is failing because of TestDfsOverAvroRpc timing out, at least in recent builds I've seen.","02/Nov/11 18:06;umamaheswararao;Hi Todd, Here is the latest report https://builds.apache.org/job/Hadoop-Hdfs-trunk/lastCompletedBuild/testReport/

It shows only two failures (TestDFSClientExcludedNodes and TestFileCreationNamenodeRestart ).
If TestDfsOverAvroRpc timing out, it should be listed in failures list right?
surprisingly i couldn't see TestDfsOverAvroRpc  in passed test cases also.","02/Nov/11 19:43;umamaheswararao;Updated the patch for trunk!.
This tests can create random failures .Patch fixes the problem.","04/Nov/11 02:23;umamaheswararao;As Todd pointed out, problem could be because of TestDfsOverAvroRpc as well. Fixing this current problems also can avoid some random failures.","07/Nov/11 06:21;atm;Hi Uma, the patch looks fine, and is a good change to make, but I don't see how this addresses the cause of these failures. I believe the test cases are run in separate JVMs, which should result in all file locks being released before the next test is run.

Were you able to reproduce these failures in your own environment? If so, how? And can you verify that this patch fixes the issue?","07/Nov/11 06:48;umamaheswararao;Yes Aaron, you are right. If we specify fork option, it will spawn separate JVM.
As Todd pointed, TestDfsOverAvroRpc  would be the real cause for test failures. This can be consider to ensure correct pattern in tests. When we add any new tests need not change others.","07/Nov/11 07:24;umamaheswararao;Why i recommend this change is, when debugging failures, i gave Junit test run in Eclipse on hdfs package (failures are from). There i noticed these 2 tests are creating random failures. But in Hudson, failures cause would be Avro test case.
I did not notice TestDfsOverAvroRpc itself in report, beacuse there is not timeout for this test. Todd, gave the patch for it HDFS-2532. ","12/Nov/11 00:00;atm;Could you please file a separate JIRA to fix these two tests, then?","12/Nov/11 01:39;umamaheswararao;Thanks a lot for taking a look!
This is the Jira HDFS-2532, actually to fix the random failures in trunk. ","10/Mar/15 02:50;hadoopqa;{color:red}-1 overall{color}.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12502018/HDFS-2531.patch
  against trunk revision 82db334.

    {color:red}-1 patch{color}.  The patch command could not apply the patch.

Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/9809//console

This message is automatically generated.",,,,
"improve TestOfflineEditsViewer to accomodate comment in editsStored.xml, for Apache license header",HDFS-2352,12523763,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,,mattf,mattf,20/Sep/11 18:09,20/Sep/11 18:09,18/Feb/21 10:08,,0.23.0,,,test,,,,0,,,,,"In HDFS-2344, we had to remove the license header comment from hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml in order to accomodate a binary file-equal-to-expected-results test in TestOfflineEditsViewer#testStored().

It would be a better solution to modify #filesEqual() to test the XML object equality, ignoring comments, rather than a simple-minded binary file equality.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,43836,,,2011-09-20 18:09:08.0,,,,,,,"0|i0juen:",113856,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More Coverage needed for FSDirectory,HDFS-2307,12521073,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,benoyantony,benoyantony,01/Sep/11 15:49,25/Oct/11 20:45,18/Feb/21 10:08,,0.22.0,,,test,,,,0,,,,,"The unit tests do not cover some of the symlink logic in FSDirectory. 
The impact of adding a symlink on the nameQuota is not covered.


The unit test coverage for FSDirectory is attached.  The uncovered lines are in  addToParent function. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/11 15:50;benoyantony;59.html;https://issues.apache.org/jira/secure/attachment/12492617/59.html",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,43842,,,2011-09-01 15:49:38.0,,,,,,,"0|i0jucn:",113847,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More coverage needed for FSPermissionChecker,HDFS-2274,12519464,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,benoyantony,benoyantony,19/Aug/11 20:11,19/Aug/11 21:39,18/Feb/21 10:08,,0.22.0,,,test,,,,0,,,,,"The unit test coverage for FSPermissionChecker is at 92%. Two conditions in FSPermissionChecker.checkStickyBit()  needs to be covered. Those two conditions are below:

 // If this user is the directory owner, return
        if(parent.getUserName().equals(user)) {
          return;
        }
     
        // if this user is the file owner, return
        if(inode.getUserName().equals(user)) {
          return;
        }",,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/11 20:12;benoyantony;85.html;https://issues.apache.org/jira/secure/attachment/12490998/85.html",,,,,1.0,,,,,,,,,,,,,,,,,,,,2011-08-19 21:39:21.009,,,false,,,,,,,,,,,,,,,,,,15009,,,Fri Aug 19 21:39:21 UTC 2011,,,,,,,"0|i0ju9b:",113832,,,,,,,,,,,,,,,,,,"19/Aug/11 21:39;sureshms;Can you post a patch please.",,,,,,,,,,,,,,,,
Add testcases for -c option of FSshell -tail ,HDFS-2221,12517783,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,xiexianshan,xiexianshan,03/Aug/11 11:33,25/Jun/15 03:14,18/Feb/21 10:08,,0.23.0,,,test,,,,0,,,,,add testcases for HADOOP-7494.,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-7494,,"25/Jun/15 03:14;iwasakims;HDFS-2221.002.patch;https://issues.apache.org/jira/secure/attachment/12741773/HDFS-2221.002.patch","03/Aug/11 11:36;xiexianshan;HDFS-2221.patch;https://issues.apache.org/jira/secure/attachment/12489181/HDFS-2221.patch",,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,15049,,,2011-08-03 11:33:49.0,,,,,,,"0|i0ju6v:",113821,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"TestHdfsProxy.testHdfsProxyInterface fails in Jenkins with ""org.apache.hadoop.ipc.RemoteException: hudson is not allowed to impersonate hudson""",HDFS-2217,12515940,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,mattf,mattf,01/Aug/11 18:13,22/Sep/11 20:04,18/Feb/21 10:08,,0.20.204.0,,,contrib/hdfsproxy,test,,,0,,,,,"See https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-0.20.204-Build/20/testReport/org.apache.hadoop.hdfsproxy/TestHdfsProxy/testHdfsProxyInterface/

Error Message
    hudson is not allowed to impersonate hudson
Stacktrace

org.apache.hadoop.ipc.RemoteException: hudson is not allowed to impersonate hudson
	at org.apache.hadoop.ipc.RemoteException.valueOf(RemoteException.java:102)
	at org.apache.hadoop.hdfs.HftpFileSystem$LsParser.startElement(HftpFileSystem.java:335)
	at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.startElement(AbstractSAXParser.java:501)
	at com.sun.org.apache.xerces.internal.parsers.AbstractXMLDocumentParser.emptyElement(AbstractXMLDocumentParser.java:179)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:377)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl$NSContentDriver.scanRootElementHook(XMLNSDocumentScannerImpl.java:626)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:3103)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:922)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:648)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:140)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:511)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:808)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:119)
	at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1205)
	at org.apache.hadoop.hdfs.HftpFileSystem$LsParser.fetchList(HftpFileSystem.java:376)
	at org.apache.hadoop.hdfs.HftpFileSystem$LsParser.getFileStatus(HftpFileSystem.java:387)
	at org.apache.hadoop.hdfs.HftpFileSystem.getFileStatus(HftpFileSystem.java:416)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:210)
	at org.apache.hadoop.hdfsproxy.TestHdfsProxy.testHdfsProxyInterface(TestHdfsProxy.java:242)",Apache Hudson build servers,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,15052,,,2011-08-01 18:13:58.0,,,,,,,"0|i0ju67:",113818,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Umbrella task: Clean up HDFS unit test recurring failures ,HDFS-1852,12504846,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,mattf,mattf,21/Apr/11 05:35,16/Aug/11 00:34,18/Feb/21 10:08,,0.22.0,,,test,,,,0,,,,,"Recurring failures and false positives undermine CI by encouraging developers to ignore unit test failures.  Let's clean these up!

Some are intermittent due to timing-sensitive conditions.  The unit tests for background thread activities (such as block replication and corrupt replica detection) often use ""wait while"" or ""wait until"" loops to detect results.  The quality and robustness of these loops vary widely, and common usages should be moved to DFSTestUtil.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,15524,,,Mon May 23 18:58:50 UTC 2011,,,,,,,"0|i0jthj:",113707,,,,,,,,,,,,,,,,,,"21/Apr/11 05:58;mattf;Here are some proposed rules for such unit test elements:

* All ""wait for condition"" loops must have timeouts, which throw TimeoutException, and provide a useful message regarding the current value of the condition variables being observed when the timeout occurs.
* Timeout values may be fixed (as a static final), or parameterized (as an argument to the method containing the wait loop), whichever is appropriate for the condition being waited on.
* Whenever a condition is waited on, and then later asserted, the condition waited on must be at least as stringent as the condition later asserted.  (Counter-example:  do not ""wait while (x < FOO)"" and then assert (x == FOO).  x == FOO+1 would exit the wait loop but fail the assert.)
* When waiting for a transient condition, such as detection of a corrupt replica which will self-heal soon after being detected, either use a busy-wait or a very small sleep interval in the wait loop, or allow for the possibility of ""missing"" the condition being waited for.

Commonly repeated methods that should be refactored to and imported from DFSTestUtils include:

* waitForReplication - wait for under- or over-replication to be normalized to the expected replication factor
* waitForCorrupt - wait for a transient detection of a corrupted replica
* corruptReplica - corrupt a specified number of replicas out of a specified block

Please contribute other suggested rules and share-able methods, and open bugs against specific unit test classes that need to be improved in this way.","23/May/11 18:58;mattf;Besides the three remaining open issues above, we also have three infrequent-intermittent issues that may still exist.  All were last seen in build 594, so it possible they were addressed.

org.apache.hadoop.cli.TestHDFSCLI.testAll - v intermittent, last 566, 579, 587, 594
org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery.testErrorReplicas - v intermittent, last 559, 566, 579, 594
org.apache.hadoop.hdfs.server.datanode.TestBlockReplacement.testBlockReplacement - v intermittent, last 565, 578, 594

Recording here for watchlist purposes.",,,,,,,,,,,,,,,
auto-test intermittently doesn't trigger in HDFS project,HDFS-1830,12504076,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,mattf,mattf,12/Apr/11 16:33,12/Apr/11 17:20,18/Feb/21 10:08,,,,,test,,,,0,,,,,"For HDFS-1295, the Hudson QA test would not run on the 02/Apr/11 01:13 submission, IBR_shortcut_v3atrunk.patch [ 12475272 ], until Nicholas (szetszwo) triggered it manually for me.
Now, with the 11/Apr/11 06:59 submission of IBR_shortcut_v4atrunk.patch [ 12475977 ],
and with the re-submission at 11/Apr/11 20:26 of IBR_shortcut_v4atrunk.patch [ 12476054 ],
and finally another at 11/Apr/11 22:26 of IBR_shortcut_v4atrunk.patch [ 12476063 ], with status first set to ""Resume Work""/""In Progress"",
it is again failing to pick up the existence of the new patch candidate. Finally, Konstantin (shv) started it manually.

I have also observed the same problem on HDFS-1070.
The 08/Apr/11 17:21 submission of trunkLocalNameImage7.patch [ 12475829 ] did not get tested until I asked Nicholas to trigger it manually.
And the 11/Apr/11 21:21 submission of trunkLocalNameImage8.patch [ 12476058 ] has been sitting for 1.5 hours, and hasn't been picked up, although it is my understanding that the need should be polled every 10 minutes.

I've also observed at https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/ that it often has up to three very long-running jobs in queue. Is it perhaps the case that patches posted when the queue is full do not get picked up after the queue is emptied? Unquestionably it often DOES work, but equally clearly it doesn't always when it should.

When I originally posted this yesterday, PreCommit-HDFS-Build had been entirely idle for about 20 minutes, until build 346 was started manually.

All times are in GMT, as in Jira.
Thank you for your help. If the Jira event log indicates I'm not doing something right, please inform. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-04-12 17:20:34.132,,,false,,,,,,,,,,,,,,,,,,15567,,,Tue Apr 12 17:20:34 UTC 2011,,,,,,,"0|i0jtf3:",113696,,,,,,,,,,,,,,,,,,"12/Apr/11 17:20;atm;I've experienced this behavior as well on a few JIRAs recently.",,,,,,,,,,,,,,,,
Some Tests in TestDFSShell can not shutdown the MiniDFSCluster on any exception/assertion failure. This will leads to fail other testcases.,HDFS-1805,12503395,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,umamaheswararao,umamaheswararao,umamaheswararao,05/Apr/11 13:57,09/Mar/15 22:11,18/Feb/21 10:08,,0.23.0,,,test,,,,0,,,,,"Some test cases in TestDFSShell are not shutting down the MiniDFSCluster in finally.
If any test assertion failure or exception can result in not shutting down this cluster. Because of this other testcases will fail. This will create difficulty in finding the actual testcase failures.
So, better to shutdown the cluster in finally. 
",,,,,,,,,,,,,,,,,,,,,,,,HDFS-1812,,,,"24/May/11 14:08;umamaheswararao;HDFS-1805-1.patch;https://issues.apache.org/jira/secure/attachment/12480251/HDFS-1805-1.patch","31/May/11 15:05;umamaheswararao;HDFS-1805-2.patch;https://issues.apache.org/jira/secure/attachment/12480946/HDFS-1805-2.patch","10/Jun/11 12:03;ram_krish;HDFS-1805-3.patch;https://issues.apache.org/jira/secure/attachment/12482052/HDFS-1805-3.patch","05/Apr/11 14:59;umamaheswararao;HDFS-1805.patch;https://issues.apache.org/jira/secure/attachment/12475486/HDFS-1805.patch",,4.0,,,,,,,,,,,,,,,,,,,,2011-04-05 17:02:03.393,,,false,,,,,,,,,,,,,,,,,,140,,,Wed Jun 15 14:57:36 UTC 2011,,,,,,,"0|i0jtdb:",113688,,,,,,,,,,,,,,,,,,"05/Apr/11 17:02;mattf;Updating the unit test to use @Before, @After, and @Test, would be better than just improving the try/catch/finally around each test case.","05/Apr/11 17:20;daryn;This test is quite problematic.  A similar problem is that the storage directory isn't being properly reset, apparently even by a nn reformat, which can often lead to odd test failures until the directory is manually deleted.  Running the test after a failure can lead to errors about unable to lock the storage directory -- perhaps this is the original issue?","05/Apr/11 17:22;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12475486/HDFS-1805.patch
  against trunk revision 1087900.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 6 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.hdfs.TestFileConcurrentReader

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-HDFS-Build/315//testReport/
Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HDFS-Build/315//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://hudson.apache.org/hudson/job/PreCommit-HDFS-Build/315//console

This message is automatically generated.","05/Apr/11 17:25;umamaheswararao;Hi Matt Foley,
Thank you for comments.
 since TestDFSSHell tests were not based on junit4,So i did not introduce any @Test, @Before, @After tags.","05/Apr/11 17:28;umamaheswararao;Hi Daryn,
 Thanks for spending time and giving comments.
  Yes, what you said is correct.","05/Apr/11 17:35;daryn;TestHDFSCLI.java has similar problems.  You may want to considering fixing it up too, or file another jira.","05/Apr/11 17:39;umamaheswararao;I will file other one.In my free time i will work on that and submit the patch.","06/Apr/11 11:34;umamaheswararao;Hi Daryn,
 I raised HDFS-1812 for handling the similar problems in TestHDFSCLI.java.
 
","06/Apr/11 19:54;cos;Considering a pretty massive change made in the patch I'd suggest to perhaps have all initialization/teardowns to be done in appropriate text fixtures (such as @Before and @After). This way you will be able to eliminate certain code dups. and make the flow easier to understand/debug.

And perhaps convert the test to Junitv4 while you're at it?","11/Apr/11 13:34;daryn;Just an addendum to Konstantin: converting to junit4 would be tremendously useful but it won't be trivial since the tests use a custom framework to execute the contents of xml files.  I do think a custom junit test runner could be used to avoid a complete rewrite of the tests.","24/May/11 14:18;daryn;Is it possible to start/stop the mini cluster in @BeforeClass/@AfterClass, and then delete the fs contents in @Before?  This should greatly reduce the runtime of the tests.","24/May/11 15:55;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12480251/HDFS-1805-1.patch
  against trunk revision 1126795.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 55 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.hdfs.TestDFSShell
                  org.apache.hadoop.hdfs.TestDFSStorageStateRecovery

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/616//testReport/
Findbugs warnings: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/616//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/616//console

This message is automatically generated.","31/May/11 15:12;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12480946/HDFS-1805-2.patch
  against trunk revision 1128987.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 55 new or modified tests.

    -1 patch.  The patch command could not apply the patch.

Console output: https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/664//console

This message is automatically generated.","31/May/11 15:49;daryn;I feel it would be cleaner to not have individual tests modifying the backup stdout & stderr to avoid the potential of one test botching up subsequent tests.  I'd suggest adding the ""final"" keyword to the backups and init them to System.{out|err}.  In the @After, always restore the backup values.

Otherwise, it looks great!  How much faster is it?","10/Jun/11 13:16;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12482052/HDFS-1805-3.patch
  against trunk revision 1134170.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 79 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    -1 findbugs.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.cli.TestHDFSCLI

    +1 contrib tests.  The patch passed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://builds.apache.org/job/PreCommit-HDFS-Build/762//testReport/
Findbugs warnings: https://builds.apache.org/job/PreCommit-HDFS-Build/762//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://builds.apache.org/job/PreCommit-HDFS-Build/762//console

This message is automatically generated.","10/Jun/11 14:29;daryn;I looked at the patch a bit more closely.  I noticed a few more issues, but it really looks good.  I run this test all the time so I'm really looking forward to it's integration.

* {{cluster}} and {{conf}} should be marked with ""final"" to ensure that a test doesn't twiddle these values and risk causing subsequent tests to fail.

* Please change psBackup{Out,Err} to be static finals initialized to System.{out,err}.  Tests should not be responsible for manipulating the restore values, else it creates opportunities for errors.  Then in the @After, unconditionally restore the values (paranoia is good).  Ie.
{code}
static final PrintStream psBackupOut = System.out;
static final PrintStream psBackupErr = System.err;
...
@After
public void CleanUpResources() {
  ...
  System.setOut(psBackupOut);
  System.setErr(psBackupErr);
  ...
}
{code}

* I think deleteFromFS can be replaced with {{FileUtil.fullyDeleteContents(fs, new Path(""/""))}}

* {{testURIPaths}} is starting another {{MiniDFSCluter}} but it's never stopped.  Rather than wrap a try around the whole test, should create another class field for a 2nd cluster and tear it down in the @After if not null.  Tests can fire up a 2nd cluster if needed since it looks like just {{testURIPaths}} needs a 2nd cluster.","15/Jun/11 14:57;ram_krish;Hi Daryn,
Thanks for the comments.

Few points to be clarified,
Making the cluster and conf as final:

@BeforeClass
	public static void startCluster() throws Exception {
		conf = new HdfsConfiguration();
		cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();
	}

'conf' can be made final by moving out of this @BeforeClass api.
But 'cluster' if we try to move outside the @BeforeClass then the build() api throws exception.  So it can be moved out like 
final static MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2).build();


The FileUtil.fullyDelete(fs, new Path(""/""))
is deprecated.

If we use the FileUtil.fullyDeleteContents(File dir)  accepts specific file name or directory name.  So can we proceed with the api deleteFromDFS().
Based on the comments will prepare patch. Thanks."
Add integration test for Ganglia 3.1,HDFS-1493,12479562,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,tomwhite,tomwhite,tomwhite,10/Nov/10 06:03,02/May/13 02:29,18/Feb/21 10:08,,,,,test,,,,0,,,,,This is to add a test for GangliaContext31 (added in HADOOP-4675) which uses a MiniDFSCluster.,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-4675,,"10/Nov/10 18:05;tomwhite;HDFS-1493.patch;https://issues.apache.org/jira/secure/attachment/12459263/HDFS-1493.patch",,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,15371,,,Wed Nov 10 18:05:51 UTC 2010,,,,,,,"0|i0jspz:",113583,,,,,,,,,,,,,,,,,,"10/Nov/10 18:05;tomwhite;Patch with test from HADOOP-4675.",,,,,,,,,,,,,,,,
Add a new test for lease expiry on hard limit,HDFS-1095,12461899,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,szetszwo,szetszwo,12/Apr/10 21:53,12/Apr/10 21:54,18/Feb/21 10:08,,0.21.0,0.22.0,,,,,,0,,,,,"The following lease recovery test case seems missing:
# create a file, write some bytes but not close it.
# let hard lease expire
# append to the file right after NN lease recovery starts but before it ends; keep retrying.
#- Expecting a RecoveryInProgressException in the first few append calls and succeed as soon as the lease recovery is done.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,16259,,,2010-04-12 21:53:42.0,,,,,,,"0|i0jr1z:",113313,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Additional FileContext coverage,HDFS-934,12447067,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,eli,eli,30/Jan/10 02:59,30/Jan/10 02:59,18/Feb/21 10:08,,,,,hdfs-client,,,,0,,,,,"After porting tests to FileContext/AFS that have not already been done (HDFS-876, eg TestDFSPermission) we need to add additional tests to cover features and interfaces present in FileContext and AFS not currently in FileSystem, eg HDFS-245 and new APIs/exceptions. Some of these are already covered in the symlink tests but should be moved/added/extended in the more specific tests that currently just use FileSytem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,16722,,,2010-01-30 02:59:21.0,,,,,,,"0|i0jqhj:",113221,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Port tests over to FileContext/AFS,HDFS-876,12444856,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,eli,eli,07/Jan/10 00:50,30/Jan/10 02:59,18/Feb/21 10:08,,,,,,,,,0,,,,,The HDFS unit and functional tests need to be ported (duplicated) to use FileContext and AbstractFileSystem.,,,,,,,,,,HADOOP-6446,HDFS-934,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,16832,,,Thu Jan 07 00:59:00 UTC 2010,,,,,,,"0|i0jq7z:",113178,,,,,,,,,,,,,,,,,,"07/Jan/10 00:59;eli;As part of doing this we should move and or add tests for new FileContext APIs. eg remove symlink permission tests from TestLink and add them to TestDFSPermission.",,,,,,,,,,,,,,,,
Additional unit tests for FSDataset,HDFS-827,12443324,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,tlipcon,tlipcon,tlipcon,15/Dec/09 00:59,24/Apr/12 19:35,18/Feb/21 10:08,,0.22.0,,,datanode,test,,,0,,,,,FSDataset doesn't currently have a unit-test that tests it in isolation of the DN or a cluster. A test specifically for this class will be helpful for developing HDFS-788,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/11 22:57;cos;hdfs-827.patch;https://issues.apache.org/jira/secure/attachment/12475262/hdfs-827.patch","31/Dec/10 19:46;tlipcon;hdfs-827.txt;https://issues.apache.org/jira/secure/attachment/12467234/hdfs-827.txt",,,,2.0,,,,,,,,,,,,,,,,,,,,2011-01-02 08:07:57.188,,,false,,,,,,,,,,,,,,,,,,171,,,Sat Apr 21 06:30:32 UTC 2012,,,,,,,"0|i0jpzz:",113142,,,,,,,,,,,,,,,,,,"31/Dec/10 19:46;tlipcon;Found this patch from long ago. This adds a set of unit tests for FSDataset (runs in about a second).

There are a number of things I found suspicious in this test, marked with XXX. It's worth looking into these to see if they represent bugs.","02/Jan/11 08:07;cos;If they so fast shall they be moved to src/test/unit instead?","04/Jan/11 22:53;tlipcon;I suppose so - I never remember to look in src/test/unit, though. What's the purpose of the distinction there? We have some tests there that take much longer than a second (eg TestBlockRecovery), and many tests in src/test/hdfs that are near instant.

To be clear, I understand the distinction between unit and functional test, but not how it actually makes a difference in our build :)","16/Jan/11 08:07;cos;The difference is timing mostly and a chance to finally organize test structure to be correct ;)","19/Feb/11 20:00;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12467234/hdfs-827.txt
  against trunk revision 1072023.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 11 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery
                  org.apache.hadoop.hdfs.TestFileConcurrentReader

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-HDFS-Build/202//testReport/
Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HDFS-Build/202//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://hudson.apache.org/hudson/job/PreCommit-HDFS-Build/202//console

This message is automatically generated.","01/Apr/11 22:50;cos;Todd, the change in {{src/java/org/apache/hadoop/hdfs/server/datanode/ReplicaInPipeline.java}}
{noformat}
+    int bytesPerChunk = checksum.getBytesPerChecksum();
+    int checksumSize = checksum.getChecksumSize();
{noformat}
causes NPE if DataChecksum.newDataChecksum
","01/Apr/11 22:57;cos;I have rebased it against current trunk","19/Apr/11 07:29;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12475262/hdfs-827.patch
  against trunk revision 1094748.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 11 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs (version 1.3.9) warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed these core unit tests:
                  org.apache.hadoop.hdfs.server.datanode.TestBlockRecovery
                  org.apache.hadoop.hdfs.TestFileConcurrentReader

    -1 contrib tests.  The patch failed contrib unit tests.

    +1 system test framework.  The patch passed system test framework compile.

Test results: https://hudson.apache.org/hudson/job/PreCommit-HDFS-Build/388//testReport/
Findbugs warnings: https://hudson.apache.org/hudson/job/PreCommit-HDFS-Build/388//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Console output: https://hudson.apache.org/hudson/job/PreCommit-HDFS-Build/388//console

This message is automatically generated.","21/Apr/12 06:30;umamaheswararao;I think, we need to re-base this patch against to latest trunk! As we have re-factored the FsDataSet impls to separate package.",,,,,,,,
libhdfs test for memory leaks ,HDFS-773,12440735,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,eli,eli,15/Nov/09 04:00,13/Apr/12 17:26,18/Feb/21 10:08,,,,,,,,,0,,,,,"We should test libhdfs for memory leaks by having a target that uses instrumented allocation functions, eg runs hdfs_test under valgrind or similar. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,15984,,,2009-11-15 04:00:50.0,,,,,,,"0|i0jprz:",113106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a test to make sure that node decomission does not get blocked by underreplicated blocks in an unclosed file,HDFS-694,12437882,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,,hairong,hairong,12/Oct/09 17:12,01/Jul/10 23:55,18/Feb/21 10:08,,0.21.0,,,,,,,0,,,,,"We have a cluster that took  much longer time to decommission datanodes than usual. It turned out to be caused by an open file. In HDFS 0.20 when a file is open, NN does not schedule any block belonged to this file to replicate. So if an open file has an under-replicated block with a replica on a decommission datanode, the decommission won't complete until the file is closed.

The new append implementation changed the replication strategy that any complete block in an unclosed file will be scheduled to be replicated if it becomes under-replicated. This jira aims to add a test to make sure that node decommission does not get blocked by under-replicated blocks in an open file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,16461,,,2009-10-12 17:12:49.0,,,,,,,"0|i0jpdb:",113040,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add test for different URI.schemas in  TestGlobPaths.java to support changes in HADOOP-6286 ,HDFS-678,12437438,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,boryas,boryas,boryas,06/Oct/09 21:56,17/Aug/10 02:10,18/Feb/21 10:08,,,,,test,,,,0,,,,,"add one more test to verify correct handling of URI in filecontext.glob
Also switch from using FS to FileContext for the tests.
Also remove deprecated methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/09 17:20;boryas;HDFS-678-1.patch;https://issues.apache.org/jira/secure/attachment/12422703/HDFS-678-1.patch","07/Oct/09 17:21;boryas;HDFS-678.patch;https://issues.apache.org/jira/secure/attachment/12421545/HDFS-678.patch",,,,2.0,,,,,,,,,,,,,,,,,,,,2009-10-07 20:32:39.029,,,false,,,,,,,,,,,,,,,,,,16563,,,Tue Aug 17 02:10:38 UTC 2010,,,,,,,"0|i0jpaf:",113027,,,,,,,,,,,,,,,,,,"07/Oct/09 17:21;boryas;Created TestFCGlobPaths.java
These are mostly same tests used for FileSystem now run thru FileContext
I've added a test for HADOOP-6286 - pTestGlobTwoFileSystems() to test using FileContext with full URI
(also from different FileSystem).","07/Oct/09 20:32;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421545/HDFS-678.patch
  against trunk revision 822153.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/57/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/57/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/57/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/57/console

This message is automatically generated.","08/Oct/09 19:57;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421545/HDFS-678.patch
  against trunk revision 822153.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/19/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/19/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/19/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/19/console

This message is automatically generated.","09/Oct/09 22:59;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421545/HDFS-678.patch
  against trunk revision 822153.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/60/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/60/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/60/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/60/console

This message is automatically generated.","14/Oct/09 21:23;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12421545/HDFS-678.patch
  against trunk revision 825229.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/66/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/66/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/66/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h5.grid.sp2.yahoo.net/66/console

This message is automatically generated.","15/Oct/09 21:46;boryas;fixed some review comments:
1. checking permission in one place
2. sync on FSNamespace
3. update LAYOUT constant
4. checked preferred block size
5. quota check for the case where files are in the same directory","15/Oct/09 21:50;boryas;please ignore the previous comments (wrong JIRA).","20/Oct/09 17:20;boryas;merged with the latest trunk","20/Oct/09 21:08;hadoopqa;-1 overall.  Here are the results of testing the latest attachment 
  http://issues.apache.org/jira/secure/attachment/12422703/HDFS-678-1.patch
  against trunk revision 826905.

    +1 @author.  The patch does not contain any @author tags.

    +1 tests included.  The patch appears to include 7 new or modified tests.

    +1 javadoc.  The javadoc tool did not generate any warning messages.

    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.

    +1 findbugs.  The patch does not introduce any new Findbugs warnings.

    +1 release audit.  The applied patch does not increase the total number of release audit warnings.

    -1 core tests.  The patch failed core unit tests.

    +1 contrib tests.  The patch passed contrib unit tests.

Test results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/43/testReport/
Findbugs warnings: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/43/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html
Checkstyle results: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/43/artifact/trunk/build/test/checkstyle-errors.html
Console output: http://hudson.zones.apache.org/hudson/job/Hdfs-Patch-h2.grid.sp2.yahoo.net/43/console

This message is automatically generated.","17/Aug/10 02:10;jghoman;Unfortunately, the patch has fallen off of trunk:
{noformat}
compile-hdfs-test:
    [javac] Compiling 175 source files to /Users/jhoman/work/git/hadoop-hdfs/build/test/classes
    [javac] /Users/jhoman/work/git/hadoop-hdfs/src/test/hdfs/org/apache/hadoop/fs/TestFCGlobPaths.java:387: cannot fin
d symbol
    [javac] symbol  : method getFileContext(org.apache.hadoop.fs.FileSystem)
    [javac] location: class org.apache.hadoop.fs.FileContext
    [javac]     FileContext dfs_fc = FileContext.getFileContext(fs);
    [javac]                                     ^
    [javac] /Users/jhoman/work/git/hadoop-hdfs/src/test/hdfs/org/apache/hadoop/fs/TestFCGlobPaths.java:389: cannot fin
d symbol
    [javac] symbol  : method getFileContext(org.apache.hadoop.fs.FileSystem)
    [javac] location: class org.apache.hadoop.fs.FileContext
    [javac]     FileContext local_fc = FileContext.getFileContext(lfs);
    [javac]                                       ^
    [javac] /Users/jhoman/work/git/hadoop-hdfs/src/test/hdfs/org/apache/hadoop/fs/TestFCGlobPaths.java:475: cannot fin
d symbol
    [javac] symbol  : method getFileContext(org.apache.hadoop.fs.FileSystem)
    [javac] location: class org.apache.hadoop.fs.FileContext
    [javac]     FileContext fc = FileContext.getFileContext(aFS);
    [javac]                                 ^
    [javac] /Users/jhoman/work/git/hadoop-hdfs/src/test/hdfs/org/apache/hadoop/fs/TestFCGlobPaths.java:491: cannot fin
d symbol
    [javac] symbol  : method getFileContext(org.apache.hadoop.fs.FileSystem)
    [javac] location: class org.apache.hadoop.fs.FileContext
    [javac]     FileContext fc = FileContext.getFileContext(fs);
{noformat}
It would be good to have these tests, however.  Boris, can you update the patch?  Canceling for now.",,,,,,,
add fuse-dfs test for non sequential writes to ensure an error is returned,HDFS-434,12407954,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,,wyckoff,wyckoff,06/Nov/08 18:56,23/Feb/11 00:56,18/Feb/21 10:08,,,,,fuse-dfs,,,,0,,,,,"Simple test that writes data to a file, does a seek back and tries to write more to ensure fuse-dfs returns EIO.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,2011-02-23 00:56:16.155,,,false,,,,,,,,,,,,,,,,,,16079,,,Wed Feb 23 00:56:16 UTC 2011,,,,,,,"0|i0ivc7:",108171,,,,,,,,,,,,,,,,,,"23/Feb/11 00:56;bockelman;Mark as a Test, not a bug.",,,,,,,,,,,,,,,,
fuse-dfs fix writeTest and add writeAppendtest,HDFS-413,12407036,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Minor,,wyckoff,wyckoff,wyckoff,22/Oct/08 20:46,02/May/13 02:29,18/Feb/21 10:08,,,,,fuse-dfs,,,,0,,,,,"testWrites wasn't actually testing the results and testCat was basically commented out plus added testAppends.
This is only a test - no C code or non-test Java code.",,,,,,,,,,,,,,,,,,,,,,,,HADOOP-4494,,HADOOP-4508,,"23/Oct/08 21:31;wyckoff;HADOOP-4495.txt;https://issues.apache.org/jira/secure/attachment/12392757/HADOOP-4495.txt","22/Oct/08 20:46;wyckoff;HADOOP-4495.txt;https://issues.apache.org/jira/secure/attachment/12392679/HADOOP-4495.txt",,,,2.0,,,,,,,,,,,,,,,,,,,,2009-06-22 13:20:33.942,,,false,,,,,,,,,,,,,,,,,,16133,,,Mon Jun 22 13:20:33 UTC 2009,,,,,,,"0|i0iv1r:",108124,,,,,,,,,,,,,,,,,,"23/Oct/08 21:31;wyckoff;remove O_EXCL flag in open since libhdfs does not support it and also removed the unlink in truncate since FileSystem.create does an overwrite anyway,
","24/Oct/08 19:36;wyckoff;because FSDataOutputStream.getPos is broken for appends, I commented out fuse's sanity check for sequential writes.
","28/Oct/08 20:29;wyckoff;unit tests will not pass without fixing getPos
","22/Jun/09 13:20;tomwhite;Looks like this is still in progress.",,,,,,,,,,,,,
fsck: Check for files-under-contruction without leases,HDFS-410,12410316,Test,Open,HDFS,Hadoop HDFS,software,dhruba,Hadoop Distributed File System,https://hadoop.apache.org/,Major,,shv,chansler,chansler,09/Dec/08 23:19,20/Jun/09 07:42,18/Feb/21 10:08,,,,,,,,,0,,,,,"A file under construction should always be associated with a lease, and vice versa. If this invariant ever breaks, HDFS may get littered with files forever under construction. fsck should observe and recover from this situation. ",,,,,,,,,,,,,,,,,,,,,,,,HADOOP-4806,HADOOP-4795,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,16139,,,2008-12-09 23:19:38.0,,,,,,,"0|i0ivnr:",108223,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
